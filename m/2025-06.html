
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 639 papers. June 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ğ˜ÑĞ½ÑŒ 2025</span> | <span id="title-articles-count">639 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-05.html">â¬…ï¸ <span id="prev-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-07.html">â¡ï¸ <span id="next-date">07.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ğ˜ÑĞ½ÑŒ 2025', 'en': 'June 2025', 'zh': '6æœˆ2025å¹´'};
        let feedDateNext = {'ru': '07.2025', 'en': '07/2025', 'zh': '7æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '05.2025', 'en': '05/2025', 'zh': '5æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.05010', 'title': 'ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development', 'url': 'https://huggingface.co/papers/2506.05010', 'abstract': 'ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.', 'score': 49, 'issue_id': 4157, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '5d1aa2eb9189bc56', 'authors': ['Zhenran Xu', 'Xue Yang', 'Yiyu Wang', 'Qingli Hu', 'Zijiao Wu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Digital Commerce', 'Harbin Institute of Technology (Shenzhen)'], 'pdf_title_img': 'assets/pdf/title_img/2506.05010.jpg', 'data': {'categories': ['#multimodal', '#agents', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ°: ComfyUI-Copilot ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°', 'desc': 'ComfyUI-Copilot - ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ ComfyUI Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ComfyUI-Copilot Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑƒĞ·Ğ»Ğ°Ğ¼ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ² Ğ¾Ğ´Ğ¸Ğ½ ĞºĞ»Ğ¸Ğº. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ĞºĞ°Ğº Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ ÑƒĞ·Ğ»Ñ‹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ².'}, 'en': {'title': 'Empowering Art Creation with Intelligent Assistance', 'desc': 'ComfyUI-Copilot is a plugin that leverages a large language model and a multi-agent system to improve the ComfyUI platform for AI art creation. It addresses common challenges faced by users, such as limited documentation and complex workflows, by providing intelligent recommendations and automating workflow construction. The system features a hierarchical structure with a central assistant agent that delegates tasks to specialized worker agents, enhancing usability and efficiency. Validation through user feedback and quantitative evaluations demonstrates that ComfyUI-Copilot effectively lowers barriers for beginners while streamlining processes for experienced users.'}, 'zh': {'title': 'æ™ºèƒ½åŠ©æ‰‹ï¼Œè½»æ¾åˆ›ä½œè‰ºæœ¯', 'desc': 'ComfyUI-Copilot æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ’ä»¶ï¼Œæ—¨åœ¨æå‡ ComfyUI è¿™ä¸€å¼€æº AI è‰ºæœ¯åˆ›ä½œå¹³å°çš„å¯ç”¨æ€§å’Œæ•ˆç‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æä¾›æ™ºèƒ½èŠ‚ç‚¹å’Œæ¨¡å‹æ¨èï¼Œä»¥åŠä¸€é”®å¼è‡ªåŠ¨åŒ–å·¥ä½œæµæ„å»ºï¼Œè§£å†³äº†æ–°æ‰‹ç”¨æˆ·åœ¨ä½¿ç”¨ ComfyUI æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚å®ƒé‡‡ç”¨äº†åˆ†å±‚çš„å¤šä»£ç†æ¡†æ¶ï¼Œä¸­å¤®åŠ©æ‰‹ä»£ç†è´Ÿè´£ä»»åŠ¡åˆ†é…ï¼Œè€Œä¸“é—¨çš„å·¥ä½œä»£ç†åˆ™å¤„ç†ä¸åŒçš„ä½¿ç”¨åœºæ™¯ã€‚é€šè¿‡ç¦»çº¿å®šé‡è¯„ä¼°å’Œåœ¨çº¿ç”¨æˆ·åé¦ˆï¼Œæˆ‘ä»¬éªŒè¯äº† ComfyUI-Copilot çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå…¶èƒ½å¤Ÿå‡†ç¡®æ¨èèŠ‚ç‚¹å¹¶åŠ é€Ÿå·¥ä½œæµå¼€å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05301', 'title': 'SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training', 'url': 'https://huggingface.co/papers/2506.05301', 'abstract': 'SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step.', 'score': 48, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'b4eef08c89e4a692', 'authors': ['Jianyi Wang', 'Shanchuan Lin', 'Zhijie Lin', 'Yuxi Ren', 'Meng Wei', 'Zongsheng Yue', 'Shangchen Zhou', 'Hao Chen', 'Yang Zhao', 'Ceyuan Yang', 'Xuefeng Xiao', 'Chen Change Loy', 'Lu Jiang'], 'affiliations': ['ByteDance', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05301.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#training', '#video', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³', 'desc': 'SeedVR2 - ÑÑ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SeedVR2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´.'}, 'en': {'title': 'Efficient Video Restoration with SeedVR2: One-Step Diffusion Redefined', 'desc': 'SeedVR2 is a novel one-step diffusion-based model designed for video restoration that enhances visual quality while minimizing computational costs. It introduces an adaptive window attention mechanism that dynamically adjusts to the output resolution, addressing issues of window inconsistency in high-resolution video. The model also employs a feature matching loss to stabilize adversarial training, ensuring effective performance without compromising efficiency. Experimental results indicate that SeedVR2 outperforms or matches existing video restoration methods, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': 'SeedVR2ï¼šé«˜æ•ˆè§†é¢‘ä¿®å¤çš„æ–°é€‰æ‹©', 'desc': 'SeedVR2æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„ä¸€æ­¥è§†é¢‘ä¿®å¤æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªé€‚åº”çª—å£æ³¨æ„åŠ›æœºåˆ¶å’Œç‰¹å¾åŒ¹é…æŸå¤±ï¼Œèƒ½å¤Ÿåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°é«˜è§†è§‰è´¨é‡ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹çœŸå®æ•°æ®è¿›è¡Œå¯¹æŠ—æ€§è®­ç»ƒï¼Œè§£å†³äº†é«˜åˆ†è¾¨ç‡è§†é¢‘ä¿®å¤çš„æŒ‘æˆ˜ã€‚ä¸ºäº†é€‚åº”ä¸åŒè¾“å‡ºåˆ†è¾¨ç‡ï¼ŒSeedVR2åŠ¨æ€è°ƒæ•´çª—å£å¤§å°ï¼Œé¿å…äº†é«˜åˆ†è¾¨ç‡è§†é¢‘ä¿®å¤ä¸­çª—å£ä¸ä¸€è‡´çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒSeedVR2åœ¨å•æ­¥ä¿®å¤ä¸­èƒ½å¤Ÿè¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰è§†é¢‘ä¿®å¤æ–¹æ³•çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04308', 'title': 'RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics', 'url': 'https://huggingface.co/papers/2506.04308', 'abstract': 'Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes.', 'score': 39, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'ef5abd087929ed17', 'authors': ['Enshen Zhou', 'Jingkun An', 'Cheng Chi', 'Yi Han', 'Shanyu Rong', 'Chi Zhang', 'Pengwei Wang', 'Zhongyuan Wang', 'Tiejun Huang', 'Lu Sheng', 'Shanghang Zhang'], 'affiliations': ['Beihang University', 'Beijing Academy of Artificial Intelligence', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04308.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#robotics', '#reasoning', '#dataset', '#3d', '#training', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RoboRefer: ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'RoboRefer - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… RefSpatial Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RefSpatial-Bench Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. RoboRefer Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Gemini-2.5-Pro, Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'RoboRefer: Advancing Spatial Understanding for Robots in 3D Environments', 'desc': 'The paper introduces RoboRefer, a novel 3D-aware vision language model (VLM) designed to enhance spatial referring capabilities in robots. It achieves improved spatial understanding through a depth encoder and supervised fine-tuning (SFT), allowing for accurate interpretation of complex 3D environments. Additionally, RoboRefer employs reinforcement fine-tuning (RFT) with specialized reward functions to facilitate multi-step spatial reasoning. The authors also present RefSpatial, a comprehensive dataset and benchmark that supports the training and evaluation of RoboRefer, demonstrating its superior performance in real-world robotic tasks.'}, 'zh': {'title': 'RoboReferï¼šæå‡æœºå™¨äººç©ºé—´ç†è§£ä¸æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¨¡å‹', 'desc': 'ç©ºé—´æŒ‡å‘æ˜¯å…·èº«æœºå™¨äººä¸ä¸‰ç»´ç‰©ç†ä¸–ç•Œäº’åŠ¨çš„åŸºæœ¬èƒ½åŠ›ã€‚å°½ç®¡ç°æœ‰çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¾ˆå¼ºå¤§ï¼Œä½†å®ƒä»¬åœ¨ç†è§£å¤æ‚çš„ä¸‰ç»´åœºæ™¯å’ŒåŠ¨æ€æ¨ç†æŒ‡ç¤ºä½ç½®æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RoboReferï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ä¸‰ç»´æ„ŸçŸ¥èƒ½åŠ›çš„VLMï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é›†æˆäº†ä¸“é—¨çš„æ·±åº¦ç¼–ç å™¨ï¼Œå®ç°äº†ç²¾ç¡®çš„ç©ºé—´ç†è§£ã€‚æ­¤å¤–ï¼ŒRoboReferé€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ¨è¿›äº†å¤šæ­¥éª¤ç©ºé—´æ¨ç†ï¼Œé‡‡ç”¨é’ˆå¯¹ç©ºé—´æŒ‡å‘ä»»åŠ¡çš„åº¦é‡æ•æ„Ÿè¿‡ç¨‹å¥–åŠ±å‡½æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05284', 'title': 'Video World Models with Long-term Spatial Memory', 'url': 'https://huggingface.co/papers/2506.05284', 'abstract': "A new framework enhances video world models' long-term consistency by integrating a geometry-grounded long-term spatial memory mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce a novel framework to enhancing long-term consistency of video world models through a geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation.", 'score': 37, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '313ae4b0ffca864d', 'authors': ['Tong Wu', 'Shuai Yang', 'Ryan Po', 'Yinghao Xu', 'Ziwei Liu', 'Dahua Lin', 'Gordon Wetzstein'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.05284.jpg', 'data': {'categories': ['#video', '#dataset', '#3d', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'ĞĞ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ñ ÑĞ²Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ 3D-Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Video World Models with Long-Term Spatial Memory', 'desc': 'This paper presents a new framework that improves the long-term consistency of video world models by incorporating a geometry-grounded long-term spatial memory mechanism. Traditional autoregressive models often forget previously generated scenes due to limited temporal context, which affects their ability to maintain consistency during revisits. The proposed framework mimics human memory by allowing the model to store and retrieve spatial information effectively, enhancing the overall quality of generated video frames. Evaluations demonstrate that this approach leads to better scene consistency and longer context retention compared to existing methods.'}, 'zh': {'title': 'å¢å¼ºè§†é¢‘ä¸–ç•Œæ¨¡å‹çš„ä¸€è‡´æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡é›†æˆåŸºäºå‡ ä½•çš„é•¿æœŸç©ºé—´è®°å¿†æœºåˆ¶ï¼Œå¢å¼ºè§†é¢‘ä¸–ç•Œæ¨¡å‹çš„é•¿æœŸä¸€è‡´æ€§ã€‚ç°æœ‰çš„ä¸–ç•Œæ¨¡å‹åœ¨ç”Ÿæˆè§†é¢‘å¸§æ—¶ï¼Œå› æ—¶é—´ä¸Šä¸‹æ–‡çª—å£å¤§å°æœ‰é™ï¼Œå¸¸å¸¸éš¾ä»¥ä¿æŒåœºæ™¯çš„ä¸€è‡´æ€§ï¼Œå¯¼è‡´å¯¹å…ˆå‰ç”Ÿæˆç¯å¢ƒçš„ä¸¥é‡é—å¿˜ã€‚æˆ‘ä»¬å€Ÿé‰´äººç±»è®°å¿†æœºåˆ¶ï¼Œè®¾è®¡äº†å­˜å‚¨å’Œæ£€ç´¢é•¿æœŸç©ºé—´è®°å¿†çš„ä¿¡æ¯æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶æ•°æ®é›†æ¥è®­ç»ƒå’Œè¯„ä¼°å…·æœ‰æ˜¾å¼å­˜å‚¨3Dè®°å¿†æœºåˆ¶çš„ä¸–ç•Œæ¨¡å‹ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸ç›¸å…³åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡ã€ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡é•¿åº¦ä¸Šéƒ½æœ‰æ‰€æå‡ï¼Œä¸ºé•¿æœŸä¸€è‡´çš„ä¸–ç•Œç”Ÿæˆé“ºå¹³äº†é“è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05229', 'title': 'Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts', 'url': 'https://huggingface.co/papers/2506.05229', 'abstract': 'Transformer models struggle with long-context inference due to their quadratic time and linear memory complexity. Recurrent Memory Transformers (RMTs) offer a solution by reducing the asymptotic cost to linear time and constant memory usage. However, their memory update mechanism leads to sequential execution, causing a performance bottleneck.   We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism across segments in RMTs while preserving exact recurrence. This approach eliminates the sequential constraint, enabling efficient GPU inference even for single long-context inputs without complex batching and pipelining techniques. Because the technique is purely a run-time computation reordering, existing RMT models adopt it with no retraining.   Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential RMT implementation on 131,072-token sequences. By removing sequential bottleneck, Diagonal Batching reduces inference cost and latency, thereby strengthening RMTs as a practical solution for real-world, long-context applications.', 'score': 34, 'issue_id': 4163, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'c1c7fcda2cc6ed7a', 'authors': ['Danil Sivtsov', 'Ivan Rodkin', 'Gleb Kuzmin', 'Yuri Kuratov', 'Ivan Oseledets'], 'affiliations': ['AIRI, Moscow, Russia', 'FRC CSC RAS, Moscow, Russia', 'MBZUAI, Abu Dhabi, UAE', 'Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia', 'Skoltech, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.05229.jpg', 'data': {'categories': ['#architecture', '#inference', '#optimization', '#training', '#long_context'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (RMT), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Diagonal Batching. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² RMT, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Diagonal Batching ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ½Ğ° GPU Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaMA-1B ARMT Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 3.3 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ LLaMA-1B Ğ¸ Ğ² 1.8 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ RMT Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 131 072 Ñ‚Ğ¾ĞºĞµĞ½Ğ°.'}, 'en': {'title': 'Unlocking Parallelism for Efficient Long-Context Inference', 'desc': 'This paper addresses the limitations of Transformer models in handling long-context inference due to their high computational costs. It introduces Recurrent Memory Transformers (RMTs) that improve efficiency by reducing time complexity to linear and memory usage to constant. The authors propose a novel scheduling method called Diagonal Batching, which allows for parallel processing of segments in RMTs, overcoming the sequential execution bottleneck. By implementing this technique, they achieve significant speedups in inference times for long sequences, making RMTs more viable for practical applications.'}, 'zh': {'title': 'å¯¹è§’æ‰¹å¤„ç†ï¼šæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†æ•ˆç‡çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è°ƒåº¦æ–¹æ¡ˆï¼Œç§°ä¸ºå¯¹è§’æ‰¹å¤„ç†ï¼ˆDiagonal Batchingï¼‰ï¼Œæ—¨åœ¨è§£å†³é€’å½’è®°å¿†å˜æ¢å™¨ï¼ˆRMTsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„æ€§èƒ½ç“¶é¢ˆã€‚ä¼ ç»Ÿçš„RMTç”±äºå…¶å†…å­˜æ›´æ–°æœºåˆ¶ï¼Œå¯¼è‡´äº†é¡ºåºæ‰§è¡Œï¼Œä»è€Œå½±å“äº†æ€§èƒ½ã€‚å¯¹è§’æ‰¹å¤„ç†é€šè¿‡åœ¨RMTä¸­å®ç°å¹¶è¡Œå¤„ç†ï¼Œæ¶ˆé™¤äº†é¡ºåºé™åˆ¶ï¼Œä½¿å¾—åœ¨å•ä¸ªé•¿ä¸Šä¸‹æ–‡è¾“å…¥ä¸Šä¹Ÿèƒ½é«˜æ•ˆæ¨ç†ã€‚è¯¥æŠ€æœ¯æ— éœ€é‡æ–°è®­ç»ƒç°æœ‰æ¨¡å‹ï¼Œåº”ç”¨äºLLaMA-1B ARMTæ¨¡å‹æ—¶ï¼Œé€Ÿåº¦æå‡è¾¾3.3å€ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬å’Œå»¶è¿Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05176', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models', 'url': 'https://huggingface.co/papers/2506.05176', 'abstract': "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.", 'score': 34, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '90ebf52dd91334c2', 'authors': ['Yanzhao Zhang', 'Mingxin Li', 'Dingkun Long', 'Xin Zhang', 'Huan Lin', 'Baosong Yang', 'Pengjun Xie', 'An Yang', 'Dayiheng Liu', 'Junyang Lin', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Alibaba Group', 'Tongyi Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.05176.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#multilingual', '#open_source', '#training', '#small_models', '#low_resource'], 'emoji': 'ğŸ”', 'ru': {'title': 'Qwen3 Embedding: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen3 Embedding, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ¼ GTE-Qwen. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen3 Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ¸ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Qwen3 Embedding Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² (0.6B, 4B, 8B) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ Qwen3 Embedding Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ MTEB Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Empowering Multilingual Text Understanding with Qwen3 Embeddings', 'desc': 'The Qwen3 Embedding series represents a major improvement in text embedding and reranking, building on the capabilities of the Qwen3 foundation models. It utilizes a multi-stage training approach that combines unsupervised pre-training with supervised fine-tuning, enhancing its performance across various languages and domains. The series includes multiple model sizes, allowing users to choose between efficiency and effectiveness based on their needs. Empirical results show that the Qwen3 Embedding series achieves top performance on benchmarks, particularly in multilingual tasks, and is available for public use to encourage further research.'}, 'zh': {'title': 'Qwen3åµŒå…¥ç³»åˆ—ï¼šå¤šè¯­è¨€æ–‡æœ¬å¤„ç†çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Qwen3åµŒå…¥ç³»åˆ—ï¼Œè¿™æ˜¯åœ¨æ–‡æœ¬åµŒå…¥å’Œé‡æ’åºèƒ½åŠ›ä¸Šç›¸è¾ƒäºGTE-Qwenç³»åˆ—çš„é‡å¤§è¿›å±•ã€‚è¯¥ç³»åˆ—åŸºäºQwen3åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„å¤šè¯­è¨€æ–‡æœ¬ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆå¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒå’Œé«˜è´¨é‡æ•°æ®é›†çš„ç›‘ç£å¾®è°ƒã€‚é€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹åˆå¹¶ç­–ç•¥ï¼ŒQwen3åµŒå…¥ç³»åˆ—ç¡®ä¿äº†æ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œæä¾›äº†å¤šç§æ¨¡å‹è§„æ¨¡ä»¥æ»¡è¶³ä¸åŒçš„éƒ¨ç½²åœºæ™¯ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒQwen3åµŒå…¥ç³»åˆ—åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå°¤å…¶åœ¨å¤šè¯­è¨€è¯„ä¼°åŸºå‡†MTEBä¸Šè¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05209', 'title': 'The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text', 'url': 'https://huggingface.co/papers/2506.05209', 'abstract': 'The Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.', 'score': 27, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '18ac0c75007ddff5', 'authors': ['Nikhil Kandpal', 'Brian Lester', 'Colin Raffel', 'Sebastian Majstorovic', 'Stella Biderman', 'Baber Abbasi', 'Luca Soldaini', 'Enrico Shippole', 'A. Feder Cooper', 'Aviya Skowron', 'John Kirchenbauer', 'Shayne Longpre', 'Lintang Sutawika', 'Alon Albalak', 'Zhenlin Xu', 'Guilherme Penedo', 'Loubna Ben Allal', 'Elie Bakouch', 'John David Pressman', 'Honglu Fan', 'Dashiell Stander', 'Guangyu Song', 'Aaron Gokaslan', 'Tom Goldstein', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Tyler Murray'], 'affiliations': ['CMU', 'Cornell University', 'EleutherAI', 'Hugging Face', 'Independent', 'Lawrence Livermore National', 'Lila Sciences', 'MIT', 'Teraflop AI', 'The Allen Institute for', 'University of Maryland, College Park', 'University of Toronto Artificial Intelligence', 'Vector Institute', 'poolside'], 'pdf_title_img': 'assets/pdf/title_img/2506.05209.jpg', 'data': {'categories': ['#dataset', '#ethics', '#open_source', '#data'], 'emoji': 'ğŸ“š', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Common Pile v0.1 - 8-Ñ‚ĞµÑ€Ğ°Ğ±Ğ°Ğ¹Ñ‚Ğ½ÑƒÑ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Comma v0.1 Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ½ĞµĞ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸: Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, ĞºĞ¾Ğ´, ĞºĞ½Ğ¸Ğ³Ğ¸, ÑĞ½Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¿ĞµĞ´Ğ¸Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Openly Licensed Text for Competitive LLM Training', 'desc': "The paper introduces the Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text aimed at training large language models (LLMs). This dataset addresses ethical concerns related to using unlicensed text by providing a high-quality, diverse source of data from various domains. The authors validate the dataset's effectiveness by training two competitive 7 billion parameter LLMs, Comma v0.1-1T and Comma v0.1-2T, which demonstrate performance comparable to LLMs trained on unlicensed data. Additionally, the paper includes the release of the dataset, training code, and model checkpoints to support further research."}, 'zh': {'title': 'å¼€æ”¾è®¸å¯æ–‡æœ¬åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒ', 'desc': 'Common Pile v0.1 æ•°æ®é›†æ˜¯ä¸€ä¸ªåŒ…å« 8TB å¼€æ”¾è®¸å¯æ–‡æœ¬çš„é›†åˆï¼Œæ—¨åœ¨è®­ç»ƒå…·æœ‰ 70 äº¿å‚æ•°çš„ç«äº‰æ€§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ•°æ®é›†æ±‡é›†äº†æ¥è‡ª 30 ä¸ªä¸åŒé¢†åŸŸçš„å†…å®¹ï¼ŒåŒ…æ‹¬ç ”ç©¶è®ºæ–‡ã€ä»£ç ã€ä¹¦ç±ã€ç™¾ç§‘å…¨ä¹¦å’Œæ•™è‚²ææ–™ç­‰ã€‚é€šè¿‡åœ¨ Common Pile ä¸Šè®­ç»ƒçš„ Comma v0.1-1T å’Œ Comma v0.1-2T æ¨¡å‹ï¼ŒéªŒè¯äº†è¯¥æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸ä½¿ç”¨æœªæˆæƒæ–‡æœ¬è®­ç»ƒçš„ LLM ç›¸å½“ã€‚æ­¤ç ”ç©¶ä¸ºè§£å†³çŸ¥è¯†äº§æƒå’Œä¼¦ç†é—®é¢˜æä¾›äº†ä¸€ä¸ªé‡è¦çš„ç¬¬ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02865', 'title': 'Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights', 'url': 'https://huggingface.co/papers/2506.02865', 'abstract': 'Surfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair it with Holo1, a new open-weight collection of VLMs specialized in web navigation and information extraction. Holo1 was trained on carefully curated data sources, including open-access web content, synthetic examples, and self-produced agentic data. Holo1 tops generalist User Interface (UI) benchmarks as well as our new web UI localization benchmark, WebClick. When powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on WebVoyager, striking a Pareto-optimal balance between accuracy and cost-efficiency. To accelerate research advancement in agentic systems, we are open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.', 'score': 27, 'issue_id': 4162, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '18e38be0b8df6445', 'authors': ['Mathieu Andreux', 'Breno Baldas Skuk', 'Hamza Benchekroun', 'Emilien BirÃ©', 'Antoine Bonnet', 'Riaz Bordie', 'Matthias Brunel', 'Pierre-Louis Cedoz', 'Antoine Chassang', 'MickaÃ«l Chen', 'Alexandra D. Constantinou', "Antoine d'AndignÃ©", 'Hubert de La JonquiÃ¨re', 'AurÃ©lien Delfosse', 'Ludovic Denoyer', 'Alexis Deprez', 'Augustin Derupti', 'Michael Eickenberg', 'MathÃ¯s Federico', 'Charles Kantor', 'Xavier Koegler', 'Yann LabbÃ©', 'Matthew C. H. Lee', 'Erwan Le Jumeau de Kergaradec', 'Amir Mahla', 'Avshalom Manevich', 'Adrien Maret', 'Charles Masson', 'RafaÃ«l Maurin', 'Arturo Mena', 'Philippe Modard', 'Axel Moyal', 'Axel Nguyen Kerbel', 'Julien Revelle', 'Mats L. Richter', 'MarÃ­a Santos', 'Laurent Sifre', 'Maxime Theillard', 'Marc Thibault', 'Louis Thiry', 'LÃ©o Tronchon', 'Nicolas Usunier', 'Tony Wu'], 'affiliations': ['Company'], 'pdf_title_img': 'assets/pdf/title_img/2506.02865.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#open_source', '#agents', '#data', '#benchmark'], 'emoji': 'ğŸ„', 'ru': {'title': 'Surfer-H Ğ¸ Holo1: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Surfer-H - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ. Surfer-H Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ñ Holo1 - Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… VLM Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Holo1 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ WebClick. Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Holo1, Surfer-H Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 92.2% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ WebVoyager, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Surfer-H: Efficient Web Navigation with Vision-Language Models', 'desc': 'The paper introduces Surfer-H, a web agent designed to efficiently navigate and perform tasks using Vision-Language Models (VLMs). It is paired with Holo1, a collection of open-weight VLMs that excel in web navigation and information extraction. Holo1 is trained on diverse data sources, ensuring high performance on various benchmarks, including a new web UI localization benchmark called WebClick. Surfer-H achieves impressive results, reaching 92.2% accuracy on the WebVoyager task, while maintaining cost-efficiency, and both the evaluation dataset and model weights are made available for further research.'}, 'zh': {'title': 'é«˜æ•ˆç½‘ç»œä»£ç†ï¼Œæ™ºèƒ½å¯¼èˆªæ–°é€‰æ‹©', 'desc': 'Surfer-Hæ˜¯ä¸€ç§é«˜æ€§ä»·æ¯”çš„ç½‘ç»œä»£ç†ï¼Œç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥æ‰§è¡Œç”¨æˆ·å®šä¹‰çš„ç½‘ç»œä»»åŠ¡ã€‚å®ƒä¸Holo1é…å¯¹ï¼ŒHolo1æ˜¯ä¸€ä¸ªæ–°çš„å¼€æ”¾æƒé‡VLMé›†åˆï¼Œä¸“æ³¨äºç½‘ç»œå¯¼èˆªå’Œä¿¡æ¯æå–ã€‚Holo1åœ¨ç»è¿‡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®æºä¸Šè®­ç»ƒï¼ŒåŒ…æ‹¬å¼€æ”¾è®¿é—®çš„ç½‘ç»œå†…å®¹å’Œåˆæˆç¤ºä¾‹ï¼Œè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ç”¨æˆ·ç•Œé¢åŸºå‡†æµ‹è¯•ä¸­ã€‚é€šè¿‡Holo1ï¼ŒSurfer-Håœ¨WebVoyagerä¸Šè¾¾åˆ°äº†92.2%çš„æœ€ä½³æ€§èƒ½ï¼Œå®ç°äº†å‡†ç¡®æ€§å’Œæˆæœ¬æ•ˆç‡çš„å¸•ç´¯æ‰˜æœ€ä¼˜å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05240', 'title': 'Aligning Latent Spaces with Flow Priors', 'url': 'https://huggingface.co/papers/2506.05240', 'abstract': 'This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently regularizes the latent space via an alignment loss, which reformulates the flow matching objective to treat the latents as optimization targets. We formally prove that minimizing this alignment loss establishes a computationally tractable surrogate objective for maximizing a variational lower bound on the log-likelihood of latents under the target distribution. Notably, the proposed method eliminates computationally expensive likelihood evaluations and avoids ODE solving during optimization. As a proof of concept, we demonstrate in a controlled setting that the alignment loss landscape closely approximates the negative log-likelihood of the target distribution. We further validate the effectiveness of our approach through large-scale image generation experiments on ImageNet with diverse target distributions, accompanied by detailed discussions and ablation studies. With both theoretical and empirical validation, our framework paves a new way for latent space alignment.', 'score': 25, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'c776325b1f9f6966', 'authors': ['Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ying Shan', 'Ping Luo'], 'affiliations': ['ARC Lab, Tencent', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.05240.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#cv', '#math', '#diffusion'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ…, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞµ Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¹ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ImageNet Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Aligning Latent Spaces with Flow-Based Models', 'desc': "This paper introduces a new method for aligning latent spaces in machine learning to match specific target distributions using flow-based generative models. The approach involves pretraining a flow model to understand the target distribution, which then helps to regularize the latent space through an alignment loss. This alignment loss is designed to optimize the latent variables effectively without the need for complex likelihood calculations or solving ordinary differential equations. The authors demonstrate the method's effectiveness through experiments on image generation, showing that it can accurately approximate the target distribution's characteristics."}, 'zh': {'title': 'æ½œåœ¨ç©ºé—´å¯¹é½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨åŸºäºæµçš„ç”Ÿæˆæ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œå°†å¯å­¦ä¹ çš„æ½œåœ¨ç©ºé—´å¯¹é½åˆ°ä»»æ„ç›®æ ‡åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåœ¨ç›®æ ‡ç‰¹å¾ä¸Šé¢„è®­ç»ƒæµæ¨¡å‹ï¼Œä»¥æ•æ‰æ½œåœ¨åˆ†å¸ƒã€‚ç„¶åï¼Œè¿™ä¸ªå›ºå®šçš„æµæ¨¡å‹é€šè¿‡å¯¹é½æŸå¤±æ¥è§„èŒƒåŒ–æ½œåœ¨ç©ºé—´ï¼Œé‡æ–°æ„é€ æµåŒ¹é…ç›®æ ‡ï¼Œå°†æ½œåœ¨å˜é‡è§†ä¸ºä¼˜åŒ–ç›®æ ‡ã€‚æˆ‘ä»¬æ­£å¼è¯æ˜ï¼Œæœ€å°åŒ–è¿™ä¸ªå¯¹é½æŸå¤±å»ºç«‹äº†ä¸€ä¸ªè®¡ç®—ä¸Šå¯å¤„ç†çš„æ›¿ä»£ç›®æ ‡ï¼Œç”¨äºæœ€å¤§åŒ–æ½œåœ¨å˜é‡åœ¨ç›®æ ‡åˆ†å¸ƒä¸‹çš„å˜åˆ†ä¸‹ç•Œçš„å¯¹æ•°ä¼¼ç„¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23656', 'title': 'VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models', 'url': 'https://huggingface.co/papers/2505.23656', 'abstract': 'Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.', 'score': 24, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': 'aa7bbc7378df2b3e', 'authors': ['Xiangdong Zhang', 'Jiaqi Liao', 'Shaofeng Zhang', 'Fanqing Meng', 'Xiangpeng Wan', 'Junchi Yan', 'Yu Cheng'], 'affiliations': ['Dept. of CSE & School of AI & MoE Key Lab of Al, Shanghai Jiao Tong University', 'NetMind.AI', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.23656.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#video', '#diffusion'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°: VideoREPA ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… T2V', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ VideoREPA Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ text-to-video (T2V). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ T2V Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Token Relation Distillation (TRD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ T2V. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VideoREPA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ» Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° CogVideoX.'}, 'en': {'title': 'Bridging the Physics Gap in Text-to-Video Generation', 'desc': 'This paper introduces VideoREPA, a new framework that improves text-to-video (T2V) models by enhancing their understanding of physics. Current T2V models often produce unrealistic videos due to their limited grasp of physical principles. VideoREPA addresses this issue by distilling knowledge from advanced video understanding models, using a novel Token Relation Distillation (TRD) loss to align token-level relationships. The results show that VideoREPA significantly boosts the physics commonsense of T2V models, leading to more realistic video generation.'}, 'zh': {'title': 'VideoREPAï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„ç‰©ç†ç†è§£èƒ½åŠ›', 'desc': 'æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•ä½¿å¾—é«˜ä¿çœŸå’ŒçœŸå®çš„è§†é¢‘åˆæˆæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„T2Væ¨¡å‹åœ¨ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„å†…å®¹æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¯¹ç‰©ç†çš„ç†è§£èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç§°ä¸ºVideoREPAï¼Œé€šè¿‡å¯¹é½ä»¤ç‰Œçº§å…³ç³»ï¼Œå°†è§†é¢‘ç†è§£åŸºç¡€æ¨¡å‹ä¸­çš„ç‰©ç†ç†è§£èƒ½åŠ›æç‚¼åˆ°T2Væ¨¡å‹ä¸­ï¼Œä»è€Œç¼©å°ç‰©ç†ç†è§£çš„å·®è·ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒVideoREPAæ˜¾è‘—å¢å¼ºäº†åŸºçº¿æ–¹æ³•CogVideoXçš„ç‰©ç†å¸¸è¯†ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸ç›´è§‚ç‰©ç†ä¸€è‡´çš„è§†é¢‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05349', 'title': 'VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal\n  Understanding in Videos', 'url': 'https://huggingface.co/papers/2506.05349', 'abstract': "VideoMathQA evaluates models' ability to perform temporally extended cross-modal reasoning across various mathematical domains in video settings, addressing direct problem solving, conceptual transfer, and deep instructional comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over 920 man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: https://mbzuai-oryx.github.io/VideoMathQA", 'score': 22, 'issue_id': 4159, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '7f9f8448e60cfdb4', 'authors': ['Hanoona Rasheed', 'Abdelrahman Shaker', 'Anqi Tang', 'Muhammad Maaz', 'Ming-Hsuan Yang', 'Salman Khan', 'Fahad Khan'], 'affiliations': ['Australian National University', 'Google Research', 'LinkÃ¶ping University', 'MBZUAI', 'University of California Merced'], 'pdf_title_img': 'assets/pdf/title_img/2506.05349.jpg', 'data': {'categories': ['#math', '#multimodal', '#benchmark', '#transfer_learning', '#reasoning', '#video'], 'emoji': 'ğŸ§®', 'ru': {'title': 'VideoMathQA: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'VideoMathQA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ 10 ÑĞµĞºÑƒĞ½Ğ´ Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ñ‡Ğ°ÑĞ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ°: Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. VideoMathQA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼.'}, 'en': {'title': 'VideoMathQA: Advancing Cross-Modal Reasoning in Mathematics', 'desc': 'VideoMathQA is a benchmark that tests how well models can reason about mathematics in videos, which is more complex than in static images or text. It focuses on understanding visual information, reading text, and integrating spoken cues that are often spread out over time. The benchmark includes questions that require direct problem solving, applying learned concepts to new situations, and understanding detailed instructions. By analyzing model performance on these tasks, VideoMathQA aims to identify the strengths and weaknesses of current approaches in handling complex, multimodal reasoning in mathematical contexts.'}, 'zh': {'title': 'è§†é¢‘æ•°å­¦æ¨ç†çš„æ–°æŒ‘æˆ˜', 'desc': 'VideoMathQA æ˜¯ä¸€ä¸ªè¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘ç¯å¢ƒä¸­è¿›è¡Œè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦é¢†åŸŸã€‚å®ƒè¦æ±‚æ¨¡å‹èƒ½å¤Ÿç†è§£å¤æ‚çš„è§†è§‰ä¿¡æ¯ã€æ‰‹å†™æˆ–æ•°å­—æ–‡æœ¬ï¼Œå¹¶æ•´åˆåˆ†æ•£çš„è¯­éŸ³æç¤ºã€‚è¯¥åŸºå‡†æ¶µç›–äº†10ä¸ªä¸åŒçš„æ•°å­¦é¢†åŸŸï¼Œé—®é¢˜è®¾è®¡å›´ç»•ç›´æ¥é—®é¢˜è§£å†³ã€æ¦‚å¿µè½¬ç§»å’Œæ·±åº¦æ•™å­¦ç†è§£ä¸‰ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡è¿™ä¸ªåŸºå‡†ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æµ‹è¯•æ¨¡å‹åœ¨å¤æ‚çš„æ•°å­¦é—®é¢˜è®¾ç½®ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05345', 'title': 'Inference-Time Hyper-Scaling with KV Cache Compression', 'url': 'https://huggingface.co/papers/2506.05345', 'abstract': 'Inference-time hyper-scaling with Dynamic Memory Sparsification in Transformer LLMs allows for increased token generation within the same compute budget by compressing the key-value cache, thereby enhancing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8times compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and memory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench across compute budgets.', 'score': 21, 'issue_id': 4162, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '172bab27eb20c036', 'authors': ['Adrian ÅaÅ„cucki', 'Konrad Staniszewski', 'Piotr Nawrot', 'Edoardo M. Ponti'], 'affiliations': ['NVIDIA', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.05345.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²Ñ‹ÑˆĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (Dynamic Memory Sparsification, DMS) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. DMS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 1000 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ 8-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Boosting Token Generation with Dynamic Memory Sparsification', 'desc': 'This paper presents a method called Dynamic Memory Sparsification (DMS) to improve the efficiency of Transformer large language models (LLMs) during inference. By compressing the key-value (KV) cache, DMS allows for the generation of more tokens without increasing the computational cost, thus enhancing reasoning accuracy. The method achieves significant compression while preserving important information, which is crucial for maintaining model performance. The results show that DMS can boost accuracy across various LLMs while keeping the inference runtime and memory usage stable.'}, 'zh': {'title': 'åŠ¨æ€å†…å­˜ç¨€ç–åŒ–ï¼šæ¨ç†æ—¶çš„è¶…è§„æ¨¡æ‰©å±•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨æ¨ç†æ—¶è¿›è¡Œè¶…è§„æ¨¡æ‰©å±•çš„æ–¹æ³•ï¼Œåˆ©ç”¨åŠ¨æ€å†…å­˜ç¨€ç–åŒ–æŠ€æœ¯æ¥å‹ç¼©é”®å€¼ç¼“å­˜ï¼Œä»è€Œåœ¨ç›¸åŒçš„è®¡ç®—é¢„ç®—å†…ç”Ÿæˆæ›´å¤šçš„ä»¤ç‰Œï¼Œå¹¶æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿçš„æ¨ç†æ‰©å±•å¾€å¾€åœ¨æ•ˆç‡ä¸æ¨ç†å‡†ç¡®æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œè€Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†æˆæœ¬ä¸»è¦å—é™äºé”®å€¼ç¼“å­˜çš„å¤§å°ã€‚é€šè¿‡å‹ç¼©é”®å€¼ç¼“å­˜ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆæ›´é•¿æˆ–æ›´å¤šçš„å¹¶è¡Œåºåˆ—ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨ç›¸ä¼¼çš„æ¨ç†è¿è¡Œæ—¶é—´å’Œå†…å­˜è´Ÿè½½ä¸‹ï¼Œå‡†ç¡®æ€§å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05328', 'title': 'AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs', 'url': 'https://huggingface.co/papers/2506.05328', 'abstract': "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io.", 'score': 20, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '774ccf3fd01aa4ef', 'authors': ['Lidong Lu', 'Guo Chen', 'Zhiqi Li', 'Yicheng Liu', 'Tong Lu'], 'affiliations': ['Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05328.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#reasoning', '#dataset', '#long_context', '#training', '#video', '#rl'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CG-AV-Counting Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AV-Reasoner, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ĞºÑƒÑ€Ñ€Ğ¸ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ´Ğ½Ğ°ĞºĞ¾ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ½Ğ¾ÑÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Video Counting with CG-AV-Counting and AV-Reasoner', 'desc': 'This paper addresses the limitations of current machine learning language models (MLLMs) in performing counting tasks in videos. It introduces CG-AV-Counting, a new benchmark that includes a large set of multimodal questions and clues, designed to evaluate counting capabilities in long videos. The authors propose a model called AV-Reasoner, which utilizes gradient-based reinforcement learning and curriculum learning to enhance counting performance. Despite achieving state-of-the-art results on various benchmarks, the model struggles with out-of-domain tasks, indicating challenges in generalizing reasoning across different contexts.'}, 'zh': {'title': 'æå‡è§†é¢‘è®¡æ•°èƒ½åŠ›çš„æ–°åŸºå‡†ä¸æ¨¡å‹', 'desc': 'å°½ç®¡è§†é¢‘ç†è§£å–å¾—äº†è¿›å±•ï¼Œä½†å½“å‰çš„å¤šæ¨¡æ€å­¦ä¹ æ¨¡å‹åœ¨è®¡æ•°ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å—é™äºçŸ­è§†é¢‘ã€å°é—­å¼æŸ¥è¯¢ã€ç¼ºä¹çº¿ç´¢æ³¨é‡Šå’Œå¤šæ¨¡æ€è¦†ç›–ä¸è¶³ã€‚æœ¬æ–‡ä»‹ç»äº†CG-AV-Countingï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰‹åŠ¨æ³¨é‡Šçš„çº¿ç´¢åŸºç¡€è®¡æ•°åŸºå‡†ï¼ŒåŒ…å«1,027ä¸ªå¤šæ¨¡æ€é—®é¢˜å’Œ5,845ä¸ªæ³¨é‡Šçº¿ç´¢ï¼Œè¦†ç›–497ä¸ªé•¿è§†é¢‘ã€‚æˆ‘ä»¬æå‡ºçš„AV-Reasoneræ¨¡å‹é€šè¿‡GRPOå’Œè¯¾ç¨‹å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä»ç›¸å…³ä»»åŠ¡ä¸­æ¨å¹¿è®¡æ•°èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04633', 'title': 'Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual\n  Simulations', 'url': 'https://huggingface.co/papers/2506.04633', 'abstract': 'Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We introduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark designed to rigorously evaluate multimodal large language models on tasks better solved through multi-step visual simulation. STARE features 4K tasks spanning foundational geometric transformations (2D and 3D), integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning), reflecting practical cognitive challenges like object assembly, mechanical diagram interpretation, and everyday spatial navigation. Our evaluations show that models excel at reasoning over simpler 2D transformations, but perform close to random chance on more complex tasks like 3D cube net folding and tangram puzzles that require multi-step visual simulations. Humans achieve near-perfect accuracy but take considerable time (up to 28.9s) on complex tasks, significantly speeding up (down by 7.5 seconds on average) with intermediate visual simulations. In contrast, models exhibit inconsistent performance gains from visual simulations, improving on most tasks but declining in specific cases like tangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0 Flash), indicating that models may not know how to effectively leverage intermediate visual information.', 'score': 18, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '05d0ae7d805419c1', 'authors': ['Linjie Li', 'Mahtab Bigverdi', 'Jiawei Gu', 'Zixian Ma', 'Yinuo Yang', 'Ziang Li', 'Yejin Choi', 'Ranjay Krishna'], 'affiliations': ['Stanford University', 'Sun Yat-sen University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.04633.jpg', 'data': {'categories': ['#multimodal', '#3d', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'STARE: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº STARE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 4000 Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ 2D-Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸ Ğº ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼Ñƒ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹. Ğ›ÑĞ´Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ñ‚Ñ€Ğ°Ñ‚ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑÑÑŒ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹.'}, 'en': {'title': 'STARE: Bridging the Gap in Spatial Reasoning for AI', 'desc': 'This paper presents STARE, a new benchmark for evaluating multimodal large language models on spatial reasoning tasks that require visual simulations. Unlike existing benchmarks that focus on verbal reasoning, STARE includes 4,000 tasks involving geometric transformations and real-world spatial challenges. The results show that while models perform well on simple 2D tasks, they struggle with complex 3D tasks that require multi-step reasoning. This indicates that current models have difficulty effectively using visual information to enhance their reasoning capabilities.'}, 'zh': {'title': 'STAREï¼šè¯„ä¼°ç©ºé—´æ¨ç†çš„æ–°åŸºå‡†', 'desc': 'ç©ºé—´è®¤çŸ¥å¯¹äººç±»æ™ºèƒ½è‡³å…³é‡è¦ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡è§†è§‰æ¨¡æ‹Ÿè§£å†³é—®é¢˜ï¼Œè€Œä¸ä»…ä»…ä¾èµ–è¯­è¨€æ¨ç†ã€‚ç°æœ‰çš„äººå·¥æ™ºèƒ½åŸºå‡†ä¸»è¦è¯„ä¼°è¯­è¨€æ¨ç†ï¼Œå¿½è§†äº†éè¯­è¨€å¤šæ­¥éª¤è§†è§‰æ¨¡æ‹Ÿçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†STAREï¼ˆç©ºé—´å˜æ¢ä¸æ¨ç†è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥éª¤è§†è§‰æ¨¡æ‹Ÿä»»åŠ¡ä¸Šçš„åŸºå‡†ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ç®€å•çš„2Då˜æ¢ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸Šï¼Œå¦‚3Dç«‹æ–¹ä½“å±•å¼€å’Œæ‹¼å›¾ï¼Œè¡¨ç°æ¥è¿‘éšæœºï¼Œè¡¨æ˜æ¨¡å‹å¯èƒ½æ— æ³•æœ‰æ•ˆåˆ©ç”¨ä¸­é—´è§†è§‰ä¿¡æ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05344', 'title': 'SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs', 'url': 'https://huggingface.co/papers/2506.05344', 'abstract': 'Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.', 'score': 15, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '0175b3788ebacf29', 'authors': ['Jiahui Wang', 'Zuyan Liu', 'Yongming Rao', 'Jiwen Lu'], 'affiliations': ['Tencent Hunyuan Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05344.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#architecture', '#open_source', '#inference'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ (Ğ¼ĞµĞ½ĞµĞµ 5%) Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ SparseMM Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ KV-ĞºÑÑˆĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´ MLLM Ğ² 1,38 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 52% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ SparseMM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ KV-ĞºÑÑˆĞ°.'}, 'en': {'title': 'Optimizing Visual Understanding in MLLMs with Sparse Attention', 'desc': 'This paper explores how Multimodal Large Language Models (MLLMs) handle visual information by examining their attention mechanisms. It uncovers that only a small fraction of attention heads, known as visual heads, are crucial for understanding visual inputs. To efficiently identify these heads, the authors propose a training-free method that assesses head-level visual relevance. They also introduce SparseMM, a KV-Cache optimization technique that improves inference speed and reduces memory usage by focusing computational resources on the most relevant visual heads, achieving significant performance improvements on multimodal tasks.'}, 'zh': {'title': 'åˆ©ç”¨ç¨€ç–è§†è§‰å¤´åŠ é€Ÿå¤šæ¨¡æ€æ¨¡å‹æ¨ç†', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡æ‰©å±•é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å¢åŠ è§†è§‰èƒ½åŠ›ã€‚æˆ‘ä»¬ç ”ç©¶äº†MLLMså¦‚ä½•å¤„ç†è§†è§‰è¾“å…¥ï¼Œåˆ†æäº†å®ƒä»¬çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæƒŠäººçš„ç¨€ç–ç°è±¡ï¼šåœ¨LLMsä¸­ï¼Œåªæœ‰å°‘é‡ï¼ˆå¤§çº¦5%ä»¥ä¸‹ï¼‰çš„æ³¨æ„åŠ›å¤´ç§¯æå‚ä¸è§†è§‰ç†è§£ï¼Œè¿™äº›è¢«ç§°ä¸ºè§†è§‰å¤´ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†SparseMMï¼Œä¸€ç§KV-Cacheä¼˜åŒ–ç­–ç•¥ï¼Œæ ¹æ®è§†è§‰å¾—åˆ†ä¸ºLLMsä¸­çš„å¤´åˆ†é…ä¸å¯¹ç§°çš„è®¡ç®—é¢„ç®—ï¼Œä»è€ŒåŠ é€ŸMLLMsçš„æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04734', 'title': 'Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design', 'url': 'https://huggingface.co/papers/2506.04734', 'abstract': 'Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.', 'score': 15, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'ce810e0cc38b26e4', 'authors': ['Lin Sun', 'Weihong Lin', 'Jinzhu Wu', 'Yongfu Zhu', 'Xiaoqi Jian', 'Guangxiang Zhao', 'Change Jia', 'Linglin Zhang', 'Sai-er Hu', 'Yuhan Wu', 'Xiangzheng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.04734.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#open_source', '#training', '#math'], 'emoji': 'ğŸ¢', 'ru': {'title': 'ĞĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº: Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµÑ€Ğ¸Ğ¸ Deepseek-R1-Distill Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼ Ğ² Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ…. ĞĞ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Deepseek-R1-Distill, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ QwQ-32B. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Ensuring Reliable Evaluations for Deep Learning Models', 'desc': 'The Deepseek-R1-Distill series of reasoning models are popular in the open-source community for their strong capabilities in various fields like mathematics and programming. However, our research shows that their performance evaluations can vary greatly due to different testing conditions. These inconsistencies also appear in other models that are fine-tuned from the Deepseek-R1-Distill series, making it hard to trust their reported improvements. We propose a stricter framework for evaluating model performance to ensure more reliable and reproducible results.'}, 'zh': {'title': 'å»ºç«‹æ›´ä¸¥æ ¼çš„æ¨¡å‹è¯„ä¼°æ ‡å‡†', 'desc': 'Deepseek-R1-Distillç³»åˆ—æ¨¡å‹åœ¨æ•°å­¦ã€ç§‘å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå—åˆ°å¼€æºç¤¾åŒºçš„å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹çš„åŸºå‡†è¯„ä¼°ç»“æœå—åˆ°å¤šç§å› ç´ çš„æ˜¾è‘—æ³¢åŠ¨å½±å“ã€‚è¯„ä¼°æ¡ä»¶çš„ç»†å¾®å·®å¼‚å¯èƒ½å¯¼è‡´ç»“æœçš„é‡å¤§å˜åŒ–ã€‚ç±»ä¼¼ç°è±¡ä¹Ÿå‡ºç°åœ¨åŸºäºDeepseek-R1-Distillç³»åˆ—å¾®è°ƒçš„å…¶ä»–å¼€æºæ¨ç†æ¨¡å‹ä¸­ï¼Œå› æ­¤æˆ‘ä»¬å‘¼åå»ºç«‹æ›´ä¸¥æ ¼çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03077', 'title': 'StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs', 'url': 'https://huggingface.co/papers/2506.03077', 'abstract': "StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.  \t\t\t\t\tAI-generated summary \t\t\t\t Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.", 'score': 15, 'issue_id': 4156, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '14c578e79a6d095c', 'authors': ['Qijun Luo', 'Mengqi Li', 'Lei Zhao', 'Xiao Li'], 'affiliations': ['Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03077.jpg', 'data': {'categories': ['#training', '#long_context', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'StreamBP: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'StreamBP - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ². StreamBP Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ GPU.'}, 'en': {'title': 'StreamBP: Efficient Backpropagation for Long Sequences in Language Models', 'desc': 'StreamBP is a novel backpropagation method designed to efficiently handle long sequence lengths in language models. By decomposing the chain rule in a layer-wise manner, it significantly reduces the memory required for storing activation values during training. This method not only speeds up the backpropagation process but also allows for longer sequences compared to traditional gradient checkpointing techniques. Additionally, StreamBP can be easily integrated into existing transformer training pipelines and supports multi-GPU setups for enhanced performance.'}, 'zh': {'title': 'StreamBPï¼šé«˜æ•ˆåå‘ä¼ æ’­ï¼ŒåŠ©åŠ›é•¿åºåˆ—è®­ç»ƒ', 'desc': 'StreamBPæ˜¯ä¸€ç§é«˜æ•ˆçš„åå‘ä¼ æ’­æ–¹æ³•ï¼Œé€šè¿‡å¯¹é“¾å¼æ³•åˆ™è¿›è¡Œçº¿æ€§åˆ†è§£ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ã€‚è¿™ä½¿å¾—åœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶ï¼Œå¯ä»¥å¤„ç†æ›´é•¿çš„åºåˆ—å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä¸ä¼ ç»Ÿçš„æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ç›¸æ¯”ï¼ŒStreamBPèƒ½å¤Ÿå°†åå‘ä¼ æ’­çš„æœ€å¤§åºåˆ—é•¿åº¦æé«˜2.8åˆ°5.5å€ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼æˆ–æ›´å°‘çš„åå‘ä¼ æ’­æ—¶é—´ã€‚æ­¤å¤–ï¼ŒStreamBPè¿˜æ”¯æŒå¤šGPUè®­ç»ƒï¼Œå¢å¼ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„çµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05334', 'title': 'Search Arena: Analyzing Search-Augmented LLMs', 'url': 'https://huggingface.co/papers/2506.05334', 'abstract': "Search Arena is a large-scale human-preference dataset that analyzes user interactions with search-augmented language models, revealing insights into citation influence and source credibility.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena.", 'score': 14, 'issue_id': 4169, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '531f22c16daba821', 'authors': ['Mihran Miroyan', 'Tsung-Han Wu', 'Logan King', 'Tianle Li', 'Jiayi Pan', 'Xinyan Hu', 'Wei-Lin Chiang', 'Anastasios N. Angelopoulos', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.05334.jpg', 'data': {'categories': ['#alignment', '#data', '#multilingual', '#dataset', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼', 'desc': 'Search Arena - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°Ñ…, Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 12 000 Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unveiling User Preferences in Search-Augmented Language Models', 'desc': 'Search Arena is a comprehensive dataset designed to study user interactions with search-augmented language models (LLMs). It includes over 24,000 multi-turn interactions and 12,000 human preference votes, providing insights into how users perceive citation influence and source credibility. The findings indicate that user preferences are swayed by the number of citations, regardless of their relevance, highlighting a disconnect between perceived and actual credibility. Additionally, the dataset reveals that users favor community-driven sources over static encyclopedic ones, and it demonstrates that web search can enhance performance in various contexts, challenging the notion that LLMs should rely solely on their internal knowledge.'}, 'zh': {'title': 'æ­ç¤ºæœç´¢å¢å¼ºæ¨¡å‹ä¸­çš„ç”¨æˆ·åå¥½ä¸å¯ä¿¡åº¦', 'desc': 'Search Arenaæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„äººç±»åå¥½æ•°æ®é›†ï¼Œåˆ†æç”¨æˆ·ä¸æœç´¢å¢å¼ºè¯­è¨€æ¨¡å‹çš„äº’åŠ¨ï¼Œæ­ç¤ºäº†å¼•ç”¨å½±å“å’Œæ¥æºå¯ä¿¡åº¦çš„è§è§£ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡24,000å¯¹å¤šè½®ç”¨æˆ·äº¤äº’ï¼Œæ¶µç›–å¤šç§æ„å›¾å’Œè¯­è¨€ï¼Œå¹¶æä¾›äº†çº¦12,000ä¸ªç”¨æˆ·åå¥½æŠ•ç¥¨çš„å®Œæ•´ç³»ç»Ÿè®°å½•ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œç”¨æˆ·åå¥½å—åˆ°å¼•ç”¨æ•°é‡çš„å½±å“ï¼Œå³ä½¿è¢«å¼•ç”¨çš„å†…å®¹å¹¶ä¸ç›´æ¥æ”¯æŒæ‰€å£°ç§°çš„è§‚ç‚¹ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯¹ä¸åŒå¼•ç”¨æ¥æºçš„åå¥½å­˜åœ¨å·®å¼‚ï¼Œè¡¨æ˜ç¤¾åŒºé©±åŠ¨çš„å¹³å°é€šå¸¸æ›´å—æ¬¢è¿ï¼Œè€Œé™æ€çš„ç™¾ç§‘å…¨ä¹¦æ¥æºå¹¶ä¸æ€»æ˜¯åˆé€‚å’Œå¯é ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05287', 'title': 'EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?', 'url': 'https://huggingface.co/papers/2506.05287', 'abstract': "The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems.", 'score': 13, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '8b357b07e4b449cd', 'authors': ['Yuqian Yuan', 'Ronghao Dang', 'Long Li', 'Wentong Li', 'Dian Jiao', 'Xin Li', 'Deli Zhao', 'Fan Wang', 'Wenqiao Zhang', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05287.jpg', 'data': {'categories': ['#optimization', '#cv', '#multimodal', '#interpretability', '#benchmark', '#games'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'EOC-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ğ² ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº EOC-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. EOC-Bench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 3277 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚Ñ€Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ 11 Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. EOC-Bench Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ….'}, 'en': {'title': 'EOC-Bench: Advancing Object Cognition in Dynamic Environments', 'desc': "This paper introduces EOC-Bench, a new benchmark for evaluating multimodal large language models (MLLMs) in dynamic environments where users interact with objects. Unlike previous benchmarks that focus on static scenes, EOC-Bench assesses how well models understand objects in changing contexts by using 3,277 annotated question-answer pairs across different time frames. The benchmark includes a unique human-in-the-loop annotation framework and a multi-scale temporal accuracy metric to evaluate models' performance in real-time interactions. By providing a comprehensive evaluation tool, EOC-Bench aims to enhance the cognitive abilities of MLLMs in embodied applications."}, 'zh': {'title': 'åŠ¨æ€åœºæ™¯ä¸­çš„ç‰©ä½“è®¤çŸ¥è¯„ä¼°æ–°åŸºå‡†', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°æ¨åŠ¨äº†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰åº”ç”¨çš„çªç ´ã€‚è¿™äº›åº”ç”¨éœ€è¦å¯¹ç‰©ä½“è¿›è¡ŒæŒç»­çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç†è§£ï¼Œå› ä¸ºç”¨æˆ·åœ¨åŠ¨æ€å’Œæ‚ä¹±çš„ç¯å¢ƒä¸­ä¸å·¥å…·äº’åŠ¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä½“ç°åŸºå‡†ä¸»è¦é›†ä¸­åœ¨é™æ€åœºæ™¯æ¢ç´¢ä¸Šï¼Œå¼ºè°ƒç‰©ä½“çš„å¤–è§‚å’Œç©ºé—´å±æ€§ï¼Œè€Œå¿½è§†äº†ç”¨æˆ·äº’åŠ¨æ‰€å¸¦æ¥çš„åŠ¨æ€å˜åŒ–è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EOC-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°åŠ¨æ€è‡ªæˆ‘ä¸­å¿ƒåœºæ™¯ä¸­çš„ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„ä½“ç°è®¤çŸ¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05331', 'title': 'MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2506.05331', 'abstract': 'Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at https://github.com/xinyan-cxy/MINT-CoT', 'score': 12, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'b3e1648a048ac6b7', 'authors': ['Xinyan Chen', 'Renrui Zhang', 'Dongzhi Jiang', 'Aojun Zhou', 'Shilin Yan', 'Weifeng Lin', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2506.05331.jpg', 'data': {'categories': ['#training', '#multimodal', '#dataset', '#reasoning', '#math', '#games'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ MINT-CoT', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MINT-CoT - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought). ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ»ÑĞ±Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MINT-CoT, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 54 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MINT-CoT-7B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Math Reasoning with Visual Interleaving in LLMs', 'desc': 'This paper introduces MINT-CoT, a novel approach to enhance mathematical reasoning in Large Language Models (LLMs) by integrating visual information more effectively. The method uses Mathematical Interleaved Tokens to dynamically select and incorporate relevant visual regions into the reasoning process, overcoming limitations of previous models that relied on fixed image regions. The authors created a dataset with 54,000 math problems that align visual tokens with reasoning steps, enabling better training of the model. Experimental results show that MINT-CoT-7B significantly outperforms existing models in various math problem-solving tasks, demonstrating its effectiveness in multimodal reasoning.'}, 'zh': {'title': 'MINT-CoTï¼šæ•°å­¦æ¨ç†çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MINT-CoTï¼Œç”¨äºåœ¨å¤šæ¨¡æ€é¢†åŸŸä¸­å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚MINT-CoTé€šè¿‡å¼•å…¥æ•°å­¦äº¤é”™æ ‡è®°ï¼Œå°†ç›¸å…³çš„è§†è§‰ä¿¡æ¯åŠ¨æ€åœ°èå…¥æ–‡æœ¬æ¨ç†æ­¥éª¤ä¸­ï¼Œä»è€Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­çš„å±€é™æ€§ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«54Kæ•°å­¦é—®é¢˜çš„æ•°æ®é›†ï¼Œç¡®ä¿æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸è§†è§‰åŒºåŸŸåœ¨æ ‡è®°çº§åˆ«ä¸Šå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMINT-CoT-7Bæ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰äº¤é”™æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05327', 'title': 'Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2506.05327', 'abstract': 'Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss', 'score': 11, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '374ddd58ce0653c6', 'authors': ['Duochao Shi', 'Weijie Wang', 'Donny Y. Chen', 'Zeyu Zhang', 'Jia-Wang Bian', 'Bohan Zhuang', 'Chunhua Shen'], 'affiliations': ['GigaAI', 'MBZUAI', 'Monash University, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.05327.jpg', 'data': {'categories': ['#optimization', '#3d', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… 3D Gaussian Splatting. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ PM-Loss - Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ³Ğ»Ğ°Ğ´Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ñ‹ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ PM-Loss Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¸ ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Enhancing 3D Rendering with PM-Loss for Smooth Depth Representation', 'desc': 'This paper presents PM-Loss, a new regularization technique designed to enhance the quality of 3D point clouds generated from depth maps in 3D Gaussian Splatting (3DGS) systems. The authors address the common issue of depth discontinuities at object boundaries, which can lead to poor rendering quality due to fragmented point clouds. By utilizing a pointmap predicted by a pre-trained transformer, PM-Loss promotes geometric smoothness, improving the overall accuracy of the depth representation. The results demonstrate that incorporating PM-Loss leads to superior rendering outcomes across different architectures and scenes.'}, 'zh': {'title': 'æå‡3Dæ¸²æŸ“è´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ­£åˆ™åŒ–æŸå¤±å‡½æ•°PM-Lossï¼Œç”¨äºæ”¹å–„åŸºäºæ·±åº¦å›¾çš„3Dé«˜æ–¯ç‚¹äº‘æ¸²æŸ“ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨ç‰©ä½“è¾¹ç•Œå¤„çš„æ·±åº¦ä¸è¿ç»­æ€§ä¼šå¯¼è‡´ç‚¹äº‘ç¨€ç–ï¼Œä»è€Œå½±å“æ¸²æŸ“è´¨é‡ã€‚PM-Lossåˆ©ç”¨é¢„è®­ç»ƒçš„å˜æ¢å™¨é¢„æµ‹çš„ç‚¹å›¾ï¼Œå°½ç®¡å…¶å‡†ç¡®æ€§ä¸å¦‚æ·±åº¦å›¾ï¼Œä½†èƒ½æœ‰æ•ˆå¢å¼ºå‡ ä½•å¹³æ»‘æ€§ã€‚é€šè¿‡æ”¹è¿›æ·±åº¦å›¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒæ¶æ„å’Œåœºæ™¯ä¸­æ˜¾è‘—æå‡äº†3Dé«˜æ–¯ç‚¹äº‘çš„æ¸²æŸ“æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02620', 'title': 'FlexPainter: Flexible and Multi-View Consistent Texture Generation', 'url': 'https://huggingface.co/papers/2506.02620', 'abstract': 'FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.  \t\t\t\t\tAI-generated summary \t\t\t\t Texture map production is an important part of 3D modeling and determines the rendering quality. Recently, diffusion-based methods have opened a new way for texture generation. However, restricted control flexibility and limited prompt modalities may prevent creators from producing desired results. Furthermore, inconsistencies between generated multi-view images often lead to poor texture generation quality. To address these issues, we introduce FlexPainter, a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. A shared conditional embedding space is constructed to perform flexible aggregation between different input modalities. Utilizing such embedding space, we present an image-based CFG method to decompose structural and style information, achieving reference image-based stylization. Leveraging the 3D knowledge within the image diffusion prior, we first generate multi-view images simultaneously using a grid representation to enhance global understanding. Meanwhile, we propose a view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency. Finally, a 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. Comprehensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods in both flexibility and generation quality.', 'score': 11, 'issue_id': 4157, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '296d6abe1e32cfa9', 'authors': ['Dongyu Yan', 'Leyi Wu', 'Jiantao Lin', 'Luozhou Wang', 'Tianshuo Xu', 'Zhifei Chen', 'Zhen Yang', 'Lie Xu', 'Shunsi Zhang', 'Yingcong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Quwan'], 'pdf_title_img': 'assets/pdf/title_img/2506.02620.jpg', 'data': {'categories': ['#multimodal', '#3d', '#cv', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'FlexPainter: Ğ³Ğ¸Ğ±ĞºĞ°Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'FlexPainter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ñ‰ĞµĞµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. FlexPainter Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'FlexPainter: Revolutionizing Texture Generation with Multi-Modal Guidance', 'desc': 'FlexPainter is a new pipeline designed for generating high-quality texture maps in 3D modeling. It utilizes a shared conditional embedding space to allow for flexible multi-modal guidance, which helps in producing consistent textures from various input types. By employing an image diffusion prior and a 3D-aware model, it generates multi-view images that maintain local consistency and enhance overall quality. The framework has been shown to outperform existing methods in terms of both flexibility and the quality of the generated textures.'}, 'zh': {'title': 'FlexPainterï¼šçµæ´»é«˜æ•ˆçš„çº¹ç†ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'FlexPainteræ˜¯ä¸€ç§æ–°å‹çš„çº¹ç†ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨å…±äº«çš„æ¡ä»¶åµŒå…¥ç©ºé—´å®ç°çµæ´»çš„å¤šæ¨¡æ€å¼•å¯¼ï¼Œä»è€Œç¡®ä¿é«˜è´¨é‡å’Œä¸€è‡´æ€§çš„çº¹ç†å›¾ç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†å›¾åƒæ‰©æ•£å…ˆéªŒå’Œ3Dæ„ŸçŸ¥æ¨¡å‹ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ§åˆ¶çµæ´»æ€§å’Œæç¤ºæ¨¡æ€æ–¹é¢çš„é™åˆ¶ã€‚é€šè¿‡æ„å»ºå…±äº«çš„æ¡ä»¶åµŒå…¥ç©ºé—´ï¼ŒFlexPainterèƒ½å¤Ÿåœ¨ä¸åŒè¾“å…¥æ¨¡æ€ä¹‹é—´è¿›è¡Œçµæ´»èšåˆï¼Œæå‡ç”Ÿæˆæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlexPainteråœ¨çµæ´»æ€§å’Œç”Ÿæˆè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04209', 'title': 'Language-Image Alignment with Fixed Text Encoders', 'url': 'https://huggingface.co/papers/2506.04209', 'abstract': 'Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations.', 'score': 10, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '921137445b3be92e', 'authors': ['Jingfeng Yang', 'Ziyang Wu', 'Yue Zhao', 'Yi Ma'], 'affiliations': ['The University of Hong Kong', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.04209.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#long_context', '#alignment', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¤Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LIFT Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², ĞºĞ°Ğº Ğ² CLIP, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸. LIFT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ· LLM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'LIFT: Efficient Language-Image Alignment with Fixed Text Encoders', 'desc': 'This paper introduces a method called LIFT, which stands for Learning Language-Image alignment with a Fixed Text encoder. Instead of training both text and image encoders together, LIFT uses a pre-trained large language model (LLM) as a fixed text encoder to improve visual representation learning. The authors demonstrate that this approach outperforms traditional joint training methods like CLIP, especially in tasks requiring compositional understanding and handling long captions. Additionally, LIFT is more computationally efficient, suggesting a new way to leverage LLMs for better language-image alignment.'}, 'zh': {'title': 'ç®€åŒ–è®­ç»ƒï¼Œæå‡è§†è§‰ç†è§£çš„LIFTæ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºLIFTï¼ˆä½¿ç”¨å›ºå®šæ–‡æœ¬ç¼–ç å™¨çš„è¯­è¨€-å›¾åƒå¯¹é½ï¼‰ï¼Œæ—¨åœ¨é€šè¿‡é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥æŒ‡å¯¼è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„è”åˆè®­ç»ƒæ–¹æ³•ï¼ˆå¦‚CLIPï¼‰ç›¸æ¯”ï¼ŒLIFTåªè®­ç»ƒå›¾åƒç¼–ç å™¨ï¼Œè€Œä½¿ç”¨å›ºå®šçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä»è€Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLIFTåœ¨å¤„ç†ç»„åˆç†è§£å’Œé•¿æ–‡æœ¬æè¿°æ—¶ï¼Œè¡¨ç°ä¼˜äºCLIPï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚è¯¥ç ”ç©¶ä¸ºå¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åµŒå…¥æ¥æŒ‡å¯¼è§†è§‰å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01011', 'title': 'Autoregressive Images Watermarking through Lexical Biasing: An Approach\n  Resistant to Regeneration Attack', 'url': 'https://huggingface.co/papers/2506.01011', 'abstract': 'A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks.', 'score': 8, 'issue_id': 4158, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '5475ba18f16db6f7', 'authors': ['Siqi Hui', 'Yiren Song', 'Sanping Zhou', 'Ye Deng', 'Wenli Huang', 'Jinjun Wang'], 'affiliations': ['National University of Singapore', 'Ningbo University of Technology', 'Southwestern University of Finance and Economics', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01011.jpg', 'data': {'categories': ['#cv', '#security', '#video'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - Lexical Bias Watermarking (LBW). LBW Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¼ĞµÑ‰Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ 'Ğ·ĞµĞ»ĞµĞ½Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¸ÑĞºĞ°' Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµÑÑˆĞ¾Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ÑÑ Ğ´Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ„Ğ°ĞºÑ‚ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LBW Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."}, 'en': {'title': 'Secure Your Images with Lexical Bias Watermarking!', 'desc': 'The paper introduces Lexical Bias Watermarking (LBW), a new technique aimed at enhancing the security of autoregressive (AR) image generation models. Unlike traditional methods that embed watermarks in diffusion models, LBW integrates watermarks directly into the token selection process during image generation. This method not only improves the robustness of watermarks against regeneration attacks but also allows for easy adaptation to existing AR frameworks. The approach utilizes a randomized selection of green lists for watermarking, ensuring higher security and effective detection through statistical analysis of token distributions.'}, 'zh': {'title': 'å¢å¼ºè‡ªå›å½’æ¨¡å‹å®‰å…¨æ€§çš„æ°´å°æŠ€æœ¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ°´å°æŠ€æœ¯ï¼Œç§°ä¸ºè¯æ±‡åç½®æ°´å°ï¼ˆLexical Bias Watermarkingï¼‰ï¼Œæ—¨åœ¨å¢å¼ºè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„å®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åå‘é€‰æ‹©é¢„å®šä¹‰çš„ç»¿è‰²åˆ—è¡¨ï¼Œå°†æ°´å°åµŒå…¥åˆ°ä»¤ç‰Œé€‰æ‹©ä¸­ï¼Œä»è€Œæœ‰æ•ˆæŠµå¾¡å†ç”Ÿæ”»å‡»ã€‚ä¸ç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ°´å°æŠ€æœ¯ä¸åŒï¼ŒLBWèƒ½å¤Ÿç›´æ¥ä¸è‡ªå›å½’æ¨¡å‹é›†æˆï¼Œå¹¶æ”¯æŒåæœŸæ°´å°å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLBWåœ¨æŠµæŠ—å†ç”Ÿæ”»å‡»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æ°´å°çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05348', 'title': 'FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene\n  Reconstruction', 'url': 'https://huggingface.co/papers/2506.05348', 'abstract': 'A novel 4D representation, FreeTimeGS, enhances the modeling of dynamic 3D scenes by enabling Gaussian primitives to appear at arbitrary times and locations, improving rendering quality compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin.', 'score': 5, 'issue_id': 4162, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '41deb088324d4fce', 'authors': ['Yifan Wang', 'Peishan Yang', 'Zhen Xu', 'Jiaming Sun', 'Zhanhua Zhang', 'Yong Chen', 'Hujun Bao', 'Sida Peng', 'Xiaowei Zhou'], 'affiliations': ['Geely Automobile Research Institute', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05348.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½Ğ°Ñ…', 'desc': 'FreeTimeGS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ°Ğ¼ Ğ¿Ğ¾ÑĞ²Ğ»ÑÑ‚ÑŒÑÑ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑÑ‚Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. FreeTimeGS Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Revolutionizing Dynamic 3D Scene Modeling with FreeTimeGS', 'desc': 'This paper introduces FreeTimeGS, a new 4D representation that enhances the modeling of dynamic 3D scenes. By allowing Gaussian primitives to appear at any time and location, it provides greater flexibility compared to traditional methods that rely on fixed canonical spaces. The approach includes motion functions for each Gaussian primitive, enabling them to transition smoothly over time and reducing redundancy in the scene representation. Experimental results demonstrate that FreeTimeGS significantly improves rendering quality, outperforming existing techniques in handling complex motions.'}, 'zh': {'title': 'åŠ¨æ€3Dåœºæ™¯å»ºæ¨¡çš„æ–°çªç ´ï¼šFreeTimeGS', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„4Dè¡¨ç¤ºæ–¹æ³•FreeTimeGSï¼Œæ—¨åœ¨å¢å¼ºåŠ¨æ€3Dåœºæ™¯çš„å»ºæ¨¡èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„3Dé«˜æ–¯åŸè¯­ä¸åŒï¼ŒFreeTimeGSå…è®¸é«˜æ–¯åŸè¯­åœ¨ä»»æ„æ—¶é—´å’Œä½ç½®å‡ºç°ï¼Œä»è€Œæé«˜äº†æ¸²æŸ“è´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ºæ¯ä¸ªé«˜æ–¯åŸè¯­èµ‹äºˆè¿åŠ¨å‡½æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿéšæ—¶é—´ç§»åŠ¨åˆ°ç›¸é‚»åŒºåŸŸï¼Œå‡å°‘äº†æ—¶é—´å†—ä½™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFreeTimeGSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ¸²æŸ“è´¨é‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05333', 'title': 'Kinetics: Rethinking Test-Time Scaling Laws', 'url': 'https://huggingface.co/papers/2506.05333', 'abstract': 'Inference with small models is less efficient due to memory bottlenecks, leading to a new Kinetics Scaling Law emphasizing sparse attention for better test-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-N, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling because, unlike training, where parameter scaling saturates, test-time accuracy continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.', 'score': 5, 'issue_id': 4172, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '8462c5d73273bc69', 'authors': ['Ranajoy Sadhukhan', 'Zhuoming Chen', 'Haizhong Zheng', 'Yang Zhou', 'Emma Strubell', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05333.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#small_models'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Kinetics, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ·-Ğ·Ğ° ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ (sparse attention), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ğ´ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚.'}, 'en': {'title': 'Unlocking Efficiency: Sparse Attention for Better Model Performance', 'desc': "This paper introduces the Kinetics Scaling Law, which highlights the importance of sparse attention in improving the efficiency of smaller machine learning models during inference. It argues that previous assessments of smaller models' effectiveness have underestimated the impact of memory access bottlenecks that arise during test-time strategies. The authors demonstrate that larger models, particularly those utilizing sparse attention, can significantly enhance performance while managing resource allocation more effectively. Their empirical results show that sparse attention models outperform dense models, leading to substantial gains in accuracy across various cost regimes."}, 'zh': {'title': 'ç¨€ç–æ³¨æ„åŠ›ï¼šæå‡å°æ¨¡å‹æ¨ç†æ•ˆç‡çš„å…³é”®', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å°æ¨¡å‹åœ¨æ¨ç†æ—¶çš„æ•ˆç‡é—®é¢˜ï¼ŒæŒ‡å‡ºç”±äºå†…å­˜ç“¶é¢ˆï¼Œå¯¼è‡´å…¶æ€§èƒ½è¢«é«˜ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Kinetics Scaling Lawï¼Œå¼ºè°ƒç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨æµ‹è¯•æ—¶çš„ä¼˜åŠ¿ã€‚é€šè¿‡å¯¹ä¸åŒå‚æ•°è§„æ¨¡æ¨¡å‹çš„åˆ†æï¼Œå‘ç°æµ‹è¯•æ—¶çš„è®¡ç®—æ•ˆç‡åœ¨è¶…è¿‡ä¸€å®šé˜ˆå€¼çš„æ¨¡å‹ä¸Šæ›´ä¸ºæ˜¾è‘—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¨€ç–æ³¨æ„åŠ›æ¨¡å‹åœ¨ä½æˆæœ¬å’Œé«˜æˆæœ¬ç¯å¢ƒä¸‹å‡ä¼˜äºå¯†é›†æ¨¡å‹ï¼Œæå‡äº†é—®é¢˜è§£å†³çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00830', 'title': 'SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.00830', 'abstract': 'SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation and editing of audio-conditioned talking portraits guided by multimodal inputs, including text, images, and videos, remains under explored. In this paper, we present SkyReels-Audio, a unified framework for synthesizing high-fidelity and temporally coherent talking portrait videos. Built upon pretrained video diffusion transformers, our framework supports infinite-length generation and editing, while enabling diverse and controllable conditioning through multimodal inputs. We employ a hybrid curriculum learning strategy to progressively align audio with facial motion, enabling fine-grained multimodal control over long video sequences. To enhance local facial coherence, we introduce a facial mask loss and an audio-guided classifier-free guidance mechanism. A sliding-window denoising approach further fuses latent representations across temporal segments, ensuring visual fidelity and temporal consistency across extended durations and diverse identities. More importantly, we construct a dedicated data pipeline for curating high-quality triplets consisting of synchronized audio, video, and textual descriptions. Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics, particularly under complex and challenging conditions.', 'score': 5, 'issue_id': 4157, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '7a04cf6593bac4b3', 'authors': ['Zhengcong Fei', 'Hao Jiang', 'Di Qiu', 'Baoxuan Gu', 'Youqiang Zhang', 'Jiahua Wang', 'Jialin Bai', 'Debang Li', 'Mingyuan Fan', 'Guibin Chen', 'Yahui Zhou'], 'affiliations': ['Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.00830.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#audio', '#diffusion', '#synthetic', '#video'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'SkyReels-Audio - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ’ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºÑƒÑ€Ñ€Ğ¸ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¸Ñ†Ğ°. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ†Ğ° Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ·ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Talking Portraits with SkyReels-Audio', 'desc': 'SkyReels-Audio is a novel framework that generates high-quality talking portrait videos by using pretrained video diffusion transformers. It allows for the creation and editing of videos based on audio, text, and images, making it versatile for various multimodal inputs. The framework employs a hybrid curriculum learning strategy to ensure that the audio aligns well with facial movements, enhancing the control over video sequences. Additionally, it introduces advanced loss mechanisms to improve facial coherence and uses a sliding-window denoising technique to maintain visual quality and consistency over time.'}, 'zh': {'title': 'SkyReels-Audioï¼šéŸ³é¢‘é©±åŠ¨çš„é«˜ä¿çœŸè¯´è¯è‚–åƒç”Ÿæˆ', 'desc': 'SkyReels-Audio æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£å˜æ¢å™¨ç”Ÿæˆé«˜ä¿çœŸä¸”è¿è´¯çš„éŸ³é¢‘æ¡ä»¶ä¸‹çš„è¯´è¯è‚–åƒè§†é¢‘ã€‚è¯¥æ¡†æ¶æ”¯æŒæ— é™é•¿åº¦çš„ç”Ÿæˆå’Œç¼–è¾‘ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€è¾“å…¥å®ç°å¤šæ ·åŒ–å’Œå¯æ§çš„æ¡ä»¶è®¾ç½®ã€‚æˆ‘ä»¬é‡‡ç”¨æ··åˆè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¯¹é½éŸ³é¢‘ä¸é¢éƒ¨è¿åŠ¨ï¼Œä»è€Œå®ç°å¯¹é•¿è§†é¢‘åºåˆ—çš„ç²¾ç»†æ§åˆ¶ã€‚é€šè¿‡å¼•å…¥é¢éƒ¨æ©è†œæŸå¤±å’ŒéŸ³é¢‘å¼•å¯¼çš„æ— åˆ†ç±»å™¨æŒ‡å¯¼æœºåˆ¶ï¼ŒSkyReels-Audio åœ¨å¤æ‚æ¡ä»¶ä¸‹å±•ç°å‡ºå“è¶Šçš„å”‡åŒæ­¥ç²¾åº¦å’Œèº«ä»½ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20914', 'title': 'Geometry-Editable and Appearance-Preserving Object Compositon', 'url': 'https://huggingface.co/papers/2505.20914', 'abstract': 'General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in a disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design a dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework.', 'score': 5, 'issue_id': 4156, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'f95a4c2b427959d4', 'authors': ['Jianman Lin', 'Haojie Li', 'Chunmei Qing', 'Zhijing Yang', 'Liang Lin', 'Tianshui Chen'], 'affiliations': ['Guangdong University of Technology', 'South China University of Technology', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20914.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DGAD (Disentangled Geometry-editable and Appearance-preserving Diffusion) Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğµ. DGAD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. DGAD Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Seamless Object Integration with Geometry and Appearance Preservation', 'desc': "The paper presents a new model called Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) for integrating objects into backgrounds while maintaining their detailed appearance. DGAD uses semantic embeddings to understand the desired geometric changes and a cross-attention mechanism to align fine-grained appearance features with these changes. This approach allows for precise editing of object geometry without losing the intricate details of the object's appearance. The model builds on existing diffusion techniques and shows improved performance in object composition tasks through extensive experiments."}, 'zh': {'title': 'è§£è€¦å‡ ä½•ä¸å¤–è§‚ä¿ç•™çš„ç‰©ä½“ç»„åˆæ–°æ–¹æ³•', 'desc': 'ä¸€èˆ¬ç‰©ä½“ç»„åˆï¼ˆGOCï¼‰æ—¨åœ¨å°†ç›®æ ‡ç‰©ä½“æ— ç¼åœ°èå…¥èƒŒæ™¯åœºæ™¯ä¸­ï¼ŒåŒæ—¶ä¿æŒå…¶ç»†è‡´çš„å¤–è§‚ç»†èŠ‚ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡è¯­ä¹‰åµŒå…¥ä¸å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç»“åˆï¼Œå®ç°å‡ ä½•å¯ç¼–è¾‘çš„ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›ç´§å‡‘çš„åµŒå…¥ä»…ç¼–ç é«˜å±‚è¯­ä¹‰ä¿¡æ¯ï¼Œéš¾ä»¥ä¿ç•™ç»†è‡´çš„å¤–è§‚ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è§£è€¦å‡ ä½•å¯ç¼–è¾‘å’Œå¤–è§‚ä¿ç•™çš„æ‰©æ•£æ¨¡å‹ï¼ˆDGADï¼‰ï¼Œé€šè¿‡è¯­ä¹‰åµŒå…¥æ•æ‰å‡ ä½•å˜æ¢ï¼Œå¹¶åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¯¹é½å¤–è§‚ç‰¹å¾ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„å‡ ä½•ç¼–è¾‘å’ŒçœŸå®çš„å¤–è§‚ä¿ç•™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04598', 'title': 'Scaling Laws for Robust Comparison of Open Foundation Language-Vision\n  Models and Datasets', 'url': 'https://huggingface.co/papers/2506.04598', 'abstract': "Scaling laws are derived for CLIP and MaMMUT to compare their performance and sample efficiency across different scales and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t In studies of transferable learning, scaling laws are obtained for various important foundation models to predict their properties and performance at larger scales. We show here how scaling law derivation can also be used for model and dataset comparison, allowing to decide which procedure is to be preferred for pre-training. For the first time, full scaling laws based on dense measurements across a wide span of model and samples seen scales are derived for two important language-vision learning procedures, CLIP and MaMMUT, that use either contrastive only or contrastive and captioning text generative loss. Ensuring sufficient prediction accuracy for held out points, we use derived scaling laws to compare both models, obtaining evidence for MaMMUT's stronger improvement with scale and better sample efficiency than standard CLIP. To strengthen validity of the comparison, we show scaling laws for various downstream tasks, classification, retrieval, and segmentation, and for different open datasets, DataComp, DFN and Re-LAION, observing consistently the same trends. We show that comparison can also be performed when deriving scaling laws with a constant learning rate schedule, reducing compute cost. Accurate derivation of scaling laws provides thus means to perform model and dataset comparison across scale spans, avoiding misleading conclusions based on measurements from single reference scales only, paving the road for systematic comparison and improvement of open foundation models and datasets for their creation. We release all the pre-trained models with their intermediate checkpoints, including openMaMMUT-L/14, which achieves 80.3% zero-shot ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/scaling-laws-for-comparison.", 'score': 4, 'issue_id': 4165, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '3add105c0b3c0782', 'authors': ['Marianna Nezhurina', 'Tomer Porian', 'Giovanni Pucceti', 'Tommie Kerssies', 'Romain Beaumont', 'Mehdi Cherti', 'Jenia Jitsev'], 'affiliations': ['Eindhoven University of Technology', 'Institute of Information Science and Technologies A. Faedo - CNR Pisa', 'Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ)', 'LAION', 'Open-Î¨ (Open-Sci) Collective'], 'pdf_title_img': 'assets/pdf/title_img/2506.04598.jpg', 'data': {'categories': ['#data', '#optimization', '#benchmark', '#transfer_learning', '#training', '#dataset', '#open_source'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ CLIP Ğ¸ MaMMUT, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ MaMMUT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ CLIP. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Model Potential: Scaling Laws for Better Comparisons', 'desc': "This paper explores scaling laws for two language-vision models, CLIP and MaMMUT, to evaluate their performance and efficiency as they are trained on larger datasets. By deriving these scaling laws, the authors provide a framework for comparing different pre-training methods, highlighting MaMMUT's superior performance and sample efficiency over CLIP. The study includes various downstream tasks and datasets, ensuring that the observed trends are consistent across different scenarios. The findings aim to guide future improvements in foundation models and datasets by offering a systematic approach to model comparison."}, 'zh': {'title': 'æ¨¡å‹ä¸æ•°æ®é›†æ¯”è¾ƒçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†CLIPå’ŒMaMMUTæ¨¡å‹çš„ç¼©æ”¾è§„å¾‹ï¼Œä»¥æ¯”è¾ƒå®ƒä»¬åœ¨ä¸åŒè§„æ¨¡å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½å’Œæ ·æœ¬æ•ˆç‡ã€‚é€šè¿‡å¯¹è¿™ä¸¤ç§é‡è¦çš„è¯­è¨€-è§†è§‰å­¦ä¹ æ–¹æ³•è¿›è¡Œå…¨é¢çš„ç¼©æ”¾è§„å¾‹æ¨å¯¼ï¼Œæ­ç¤ºäº†MaMMUTåœ¨è§„æ¨¡æ‰©å¤§æ—¶çš„æ€§èƒ½æå‡å’Œæ ·æœ¬æ•ˆç‡ä¼˜äºæ ‡å‡†CLIPã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡å’Œå¼€æ”¾æ•°æ®é›†ä¸Šï¼Œç¼©æ”¾è§„å¾‹çš„ä¸€è‡´æ€§è¶‹åŠ¿ï¼Œç¡®ä¿äº†æ¯”è¾ƒçš„æœ‰æ•ˆæ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹åŠå…¶ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œä»¥æ”¯æŒåç»­ç ”ç©¶å’Œå®éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04405', 'title': 'MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale', 'url': 'https://huggingface.co/papers/2506.04405', 'abstract': 'We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '2cf822e179634776', 'authors': ['Ran Xu', 'Yuchen Zhuang', 'Yishan Zhong', 'Yue Yu', 'Xiangru Tang', 'Hang Wu', 'May D. Wang', 'Peifeng Ruan', 'Donghan Yang', 'Tao Wang', 'Guanghua Xiao', 'Carl Yang', 'Yang Xie', 'Wenqi Shi'], 'affiliations': ['Emory University', 'Georgia Tech', 'UT Southwestern Medical Center', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04405.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#dataset', '#open_source', '#training', '#rl'], 'emoji': 'ğŸ©º', 'ru': {'title': 'MedAgentGYM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'MedAgentGYM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 72 Ñ‚Ñ‹ÑÑÑ‡ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· 129 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´ Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ MedAgentGYM, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Med-Copilot-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Empowering Medical Reasoning in LLMs with MedAgentGYM', 'desc': 'MedAgentGYM is a new training environment aimed at improving the coding abilities of large language models (LLMs) in medical reasoning. It includes over 72,000 tasks from real-world biomedical situations, allowing LLMs to learn through interactive coding environments. The platform provides detailed task descriptions, feedback, and verified annotations to support effective training. Benchmarking shows that models like Med-Copilot-7B can significantly improve their performance through fine-tuning and reinforcement learning, making it a strong alternative to more expensive models like gpt-4o.'}, 'zh': {'title': 'MedAgentGYMï¼šæå‡åŒ»å­¦æ¨ç†èƒ½åŠ›çš„åˆ›æ–°å¹³å°', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†MedAgentGYMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„è®­ç»ƒç¯å¢ƒï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„åŸºäºç¼–ç çš„åŒ»å­¦æ¨ç†èƒ½åŠ›ã€‚MedAgentGYMåŒ…å«72,413ä¸ªä»»åŠ¡å®ä¾‹ï¼Œæ¶µç›–129ä¸ªç±»åˆ«ï¼Œæ¥æºäºçœŸå®çš„ç”Ÿç‰©åŒ»å­¦åœºæ™¯ã€‚æ¯ä¸ªä»»åŠ¡éƒ½åœ¨å¯æ‰§è¡Œçš„ç¼–ç ç¯å¢ƒä¸­å°è£…ï¼Œæä¾›è¯¦ç»†çš„ä»»åŠ¡æè¿°ã€äº’åŠ¨åé¦ˆæœºåˆ¶ã€å¯éªŒè¯çš„çœŸå®æ³¨é‡Šå’Œå¯æ‰©å±•çš„è®­ç»ƒè½¨è¿¹ç”Ÿæˆã€‚é€šè¿‡å¯¹30å¤šç§LLMçš„å¹¿æ³›åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å•†ä¸šAPIæ¨¡å‹ä¸å¼€æºæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04245', 'title': 'Contextual Integrity in LLMs via Reasoning and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04245', 'abstract': 'As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only sim700 examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '8716d3ca53b1d58f', 'authors': ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim'], 'affiliations': ['Microsoft', 'National University of Singapore', 'Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04245.jpg', 'data': {'categories': ['#synthetic', '#agents', '#benchmark', '#reasoning', '#dataset', '#transfer_learning', '#rl', '#leakage'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (CI) Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑƒĞ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ CI, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº PrivacyLens.'}, 'en': {'title': 'Enhancing Contextual Integrity in Autonomous Agents', 'desc': 'This paper addresses the challenge of contextual integrity (CI) in autonomous agents, focusing on how these agents decide what information to share during tasks. The authors propose that effective CI requires agents to reason about their operating context. They introduce a reinforcement learning (RL) framework that enhances this reasoning capability in language models (LLMs). Their experiments demonstrate that this approach significantly reduces inappropriate information disclosure while preserving task performance, and the improvements are validated against established benchmarks.'}, 'zh': {'title': 'ç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼Œæå‡è‡ªä¸»ä»£ç†å†³ç­–èƒ½åŠ›', 'desc': 'åœ¨è‡ªä¸»ä»£ç†ä¸ºç”¨æˆ·åšå†³ç­–çš„æ—¶ä»£ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼ˆCIï¼‰æˆä¸ºä¸€ä¸ªé‡è¦é—®é¢˜ã€‚æœ¬æ–‡æå‡ºï¼ŒCIéœ€è¦ä»£ç†åœ¨æ‰§è¡Œä»»åŠ¡æ—¶å¯¹å…¶æ“ä½œçš„ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬é¦–å…ˆè®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜ç¡®æ¨ç†CIï¼Œä»¥å†³å®šæŠ«éœ²å“ªäº›ä¿¡æ¯ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹è¿›è¡ŒCIæ‰€éœ€çš„æ¨ç†èƒ½åŠ›ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä¸å½“ä¿¡æ¯æŠ«éœ²ï¼ŒåŒæ—¶ä¿æŒäº†ä»»åŠ¡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05282', 'title': 'Rectified Point Flow: Generic Point Cloud Pose Estimation', 'url': 'https://huggingface.co/papers/2506.05282', 'abstract': 'We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/.', 'score': 3, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'c7d3c7ca688358d9', 'authors': ['Tao Sun', 'Liyuan Zhu', 'Shengyu Huang', 'Shuran Song', 'Iro Armeni'], 'affiliations': ['NVIDIA Research', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05282.jpg', 'data': {'categories': ['#dataset', '#3d', '#benchmark'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ ÑĞ±Ğ¾Ñ€ĞºĞµ Ñ„Ğ¾Ñ€Ğ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Rectified Point Flow, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ ÑĞ±Ğ¾Ñ€ĞºÑƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğº Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ±Ğ¾Ñ€ĞºĞµ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ’Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ÑÑ Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚Ğ¾Ñ‡ĞºĞ°Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unified Learning for Point Cloud Registration and Shape Assembly', 'desc': 'This paper presents Rectified Point Flow, a novel approach that combines point cloud registration and multi-part shape assembly into a single generative framework. The method learns a continuous velocity field that aligns noisy point clouds to their target configurations, allowing for the recovery of part poses. Unlike previous methods that require manual symmetry handling, this approach automatically learns assembly symmetries without needing explicit labels. By utilizing a self-supervised encoder for overlapping points, the method achieves state-of-the-art results across multiple benchmarks, enhancing accuracy through joint training on varied datasets.'}, 'zh': {'title': 'ç»Ÿä¸€ç‚¹äº‘é…å‡†ä¸å½¢çŠ¶ç»„è£…çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¿®æ­£ç‚¹æµï¼ˆRectified Point Flowï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å‚æ•°åŒ–æ–¹æ³•ï¼Œå°†æˆå¯¹ç‚¹äº‘é…å‡†å’Œå¤šéƒ¨ä»¶å½¢çŠ¶ç»„è£…è§†ä¸ºä¸€ä¸ªå•ä¸€çš„æ¡ä»¶ç”Ÿæˆé—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨æ²¡æœ‰å§¿æ€ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ ä¸€ä¸ªè¿ç»­çš„ç‚¹ä½é€Ÿåº¦åœºï¼Œå°†å™ªå£°ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®ï¼Œå¹¶ä»ä¸­æ¢å¤éƒ¨ä»¶å§¿æ€ã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰å¯¹ç§°æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œè‡ªç„¶åœ°å­¦ä¹ ç»„è£…å¯¹ç§°æ€§ã€‚é€šè¿‡ä¸“æ³¨äºé‡å ç‚¹çš„è‡ªç›‘ç£ç¼–ç å™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œä¿ƒè¿›äº†å…±äº«å‡ ä½•å…ˆéªŒçš„å­¦ä¹ ï¼Œä»è€Œæé«˜äº†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05278', 'title': 'Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning', 'url': 'https://huggingface.co/papers/2506.05278', 'abstract': 'A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.', 'score': 3, 'issue_id': 4158, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '119fed47e7d43a96', 'authors': ['Nan Huo', 'Jinyang Li', 'Bowen Qin', 'Ge Qu', 'Xiaolong Li', 'Xiaodong Li', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['BAAI', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05278.jpg', 'data': {'categories': ['#interpretability', '#rag', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Micro-Act: ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Micro-Act Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (RAG). ĞĞ½ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¼ĞµĞ»ĞºĞ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Micro-Act: Resolving Knowledge Conflicts for Better QA Accuracy', 'desc': 'The paper introduces Micro-Act, a novel framework designed to tackle Knowledge Conflicts in Retrieval-Augmented Generation (RAG) systems. Knowledge Conflicts occur when external information contradicts the knowledge embedded in large language models, negatively impacting question answering (QA) tasks. Micro-Act improves QA accuracy by adaptively breaking down knowledge sources into manageable comparisons, allowing for deeper reasoning and better conflict resolution. Experimental results demonstrate that Micro-Act outperforms existing methods across various datasets and conflict types, while also maintaining strong performance on non-conflict questions.'}, 'zh': {'title': 'Micro-Actï¼šè§£å†³çŸ¥è¯†å†²çªçš„æ™ºèƒ½æ¡†æ¶', 'desc': 'Micro-Actæ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çš„çŸ¥è¯†å†²çªé—®é¢˜ã€‚å®ƒé€šè¿‡è‡ªé€‚åº”åœ°åˆ†è§£çŸ¥è¯†æºï¼Œæ”¹å–„äº†é—®ç­”ï¼ˆQAï¼‰çš„å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒMicro-Acté‡‡ç”¨åˆ†å±‚çš„è¡ŒåŠ¨ç©ºé—´ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„å¤æ‚æ€§ï¼Œå¹¶å°†çŸ¥è¯†æºç»†åˆ†ä¸ºä¸€ç³»åˆ—ç²¾ç»†çš„æ¯”è¾ƒæ­¥éª¤ã€‚è¿™ç§æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„QAå‡†ç¡®æ€§æå‡ï¼Œå°¤å…¶åœ¨æ—¶é—´å’Œè¯­ä¹‰ç±»å‹çš„å†²çªä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04956', 'title': 'FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.04956', 'abstract': 'Synthesizing high-quality dynamic medical videos remains a significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity from self-attention, and coarse denoising guidance from timestep embeddings when handling varying noise levels. In this work, we propose FEAT, a full-dimensional efficient attention Transformer, which addresses these issues through three key innovations: (1) a unified paradigm with sequential spatial-temporal-channel attention mechanisms to capture global dependencies across all dimensions, (2) a linear-complexity design for attention mechanisms in each dimension, utilizing weighted key-value attention and global channel attention, and (3) a residual value guidance module that provides fine-grained pixel-level guidance to adapt to different noise levels. We evaluate FEAT on standard benchmarks and downstream tasks, demonstrating that FEAT-S, with only 23\\% of the parameters of the state-of-the-art model Endora, achieves comparable or even superior performance. Furthermore, FEAT-L surpasses all comparison methods across multiple datasets, showcasing both superior effectiveness and scalability. Code is available at https://github.com/Yaziwel/FEAT.', 'score': 3, 'issue_id': 4166, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'e1a303f65a6d378c', 'authors': ['Huihan Wang', 'Zhiwen Yang', 'Hui Zhang', 'Dan Zhao', 'Bingzheng Wei', 'Yan Xu'], 'affiliations': ['ByteDance Inc., Beijing 100098, China', 'Department of Biomedical Engineering, Tsinghua University, Beijing 100084, China', 'Department of Gynecology Oncology, National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing 100021, China', 'School of Biological Science and Medical Engineering, State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100191, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04956.jpg', 'data': {'categories': ['#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'FEAT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ FEAT - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. FEAT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ…, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ°Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸Ğ¼ĞµĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼ ÑˆÑƒĞ¼Ğ°. FEAT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'FEAT: Revolutionizing Medical Video Synthesis with Efficient Attention', 'desc': "This paper introduces FEAT, a novel Transformer model designed to create high-quality dynamic medical videos by effectively managing spatial and temporal information. It overcomes limitations of existing models by implementing a unified attention mechanism that captures dependencies across spatial, temporal, and channel dimensions. FEAT also features a linear-complexity attention design, which reduces computational demands while maintaining performance. Additionally, a residual value guidance module enhances the model's ability to adapt to varying noise levels, leading to superior results on benchmark tasks with fewer parameters than previous state-of-the-art models."}, 'zh': {'title': 'é«˜æ•ˆåŠ¨æ€åŒ»ç–—è§†é¢‘åˆæˆçš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å…¨ç»´é«˜æ•ˆæ³¨æ„åŠ›å˜æ¢å™¨ï¼ˆFEATï¼‰ï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€åŒ»ç–—è§†é¢‘åˆæˆä¸­çš„ç©ºé—´ä¸€è‡´æ€§å’Œæ—¶é—´åŠ¨æ€å»ºæ¨¡é—®é¢˜ã€‚FEATé€šè¿‡ä¸‰é¡¹åˆ›æ–°æ¥å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬ç»Ÿä¸€çš„æ—¶ç©ºé€šé“æ³¨æ„åŠ›æœºåˆ¶ã€çº¿æ€§å¤æ‚åº¦çš„æ³¨æ„åŠ›è®¾è®¡ä»¥åŠæ®‹å·®å€¼å¼•å¯¼æ¨¡å—ï¼Œä»¥é€‚åº”ä¸åŒçš„å™ªå£°æ°´å¹³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFEAT-Såœ¨å‚æ•°é‡ä»…ä¸ºæœ€å…ˆè¿›æ¨¡å‹Endoraçš„23%çš„æƒ…å†µä¸‹ï¼Œä»èƒ½å®ç°ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚FEAT-Låœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†æ‰€æœ‰å¯¹æ¯”æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03643', 'title': 'Images are Worth Variable Length of Representations', 'url': 'https://huggingface.co/papers/2506.03643', 'abstract': 'Most existing vision encoders map images into a fixed-length sequence of tokens, overlooking the fact that different images contain varying amounts of information. For example, a visually complex image (e.g., a cluttered room) inherently carries more information and thus deserves more tokens than a simple image (e.g., a blank wall). To address this inefficiency, we propose DOVE, a dynamic vision encoder that produces a variable number of visual tokens (i.e., continuous representation vectors) to reconstruct each image. Our results show that DOVE significantly reduces the average number of tokens while maintaining high reconstruction quality. In several linear probing and downstream multimodal tasks, it outperforms existing autoencoder-based tokenization methods when using far fewer tokens, capturing more expressive semantic features compared to fixed-length encoding. We further extend DOVE with query-conditioned tokenization. By guiding the model to focus on query-relevant regions, it achieves more efficient and targeted semantic extraction. Our code and checkpoints are available at https://dove-encoder.github.io/dove-encoder.', 'score': 3, 'issue_id': 4165, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '0581c037604119ee', 'authors': ['Lingjun Mao', 'Rodolfo Corona', 'Xin Liang', 'Wenhao Yan', 'Zineng Tang'], 'affiliations': ['University of California, Berkeley', 'University of California, San Diego', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.03643.jpg', 'data': {'categories': ['#cv', '#architecture', '#optimization', '#multimodal'], 'emoji': 'ğŸ¦…', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'DOVE - ÑÑ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, DOVE Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DOVE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Dynamic Tokenization for Enhanced Image Understanding', 'desc': 'This paper introduces DOVE, a dynamic vision encoder that adapts the number of visual tokens based on the complexity of the image being processed. Unlike traditional methods that use a fixed number of tokens, DOVE generates a variable number of tokens, allowing it to capture more information from visually complex images. The results demonstrate that DOVE not only reduces the average number of tokens needed but also maintains high-quality image reconstruction. Additionally, DOVE incorporates query-conditioned tokenization to enhance semantic extraction by focusing on relevant image regions, outperforming existing autoencoder-based methods in various tasks.'}, 'zh': {'title': 'åŠ¨æ€è§†è§‰ç¼–ç ï¼Œæå‡ä¿¡æ¯æå–æ•ˆç‡', 'desc': 'ç°æœ‰çš„è§†è§‰ç¼–ç å™¨é€šå¸¸å°†å›¾åƒæ˜ å°„ä¸ºå›ºå®šé•¿åº¦çš„æ ‡è®°åºåˆ—ï¼Œä½†ä¸åŒå›¾åƒçš„ä¿¡æ¯é‡ä¸åŒã€‚æˆ‘ä»¬æå‡ºäº†DOVEï¼Œä¸€ä¸ªåŠ¨æ€è§†è§‰ç¼–ç å™¨ï¼Œå¯ä»¥ç”Ÿæˆå¯å˜æ•°é‡çš„è§†è§‰æ ‡è®°ï¼Œä»¥é‡å»ºæ¯ä¸ªå›¾åƒã€‚DOVEåœ¨ä¿æŒé«˜é‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†å¹³å‡æ ‡è®°æ•°é‡ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„åŸºäºè‡ªç¼–ç å™¨çš„æ ‡è®°åŒ–æ–¹æ³•ã€‚é€šè¿‡æŸ¥è¯¢æ¡ä»¶æ ‡è®°åŒ–ï¼ŒDOVEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå–ä¸æŸ¥è¯¢ç›¸å…³çš„è¯­ä¹‰ç‰¹å¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02751', 'title': 'RobustSplat: Decoupling Densification and Dynamics for Transient-Free\n  3DGS', 'url': 'https://huggingface.co/papers/2506.02751', 'abstract': 'RobustSplat addresses artifacts in 3D Gaussian Splatting caused by transient objects through delayed Gaussian growth and scale-cascaded mask bootstrapping.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.', 'score': 3, 'issue_id': 4161, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '9e5cefa9df6461fd', 'authors': ['Chuanyu Fu', 'Yuqi Zhang', 'Kunbin Yao', 'Guanying Chen', 'Yuan Xiong', 'Chuan Huang', 'Shuguang Cui', 'Xiaochun Cao'], 'affiliations': ['FNii-Shenzhen', 'SSE, CUHKSZ', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02751.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸŒŸ', 'ru': {'title': 'Ğ£ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² 3D-ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²', 'desc': 'RobustSplat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑÑ†ĞµĞ½Ñ‹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, RobustSplat Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ ĞµĞ³Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Enhancing 3D Gaussian Splatting with Robust Techniques', 'desc': 'RobustSplat is a novel approach designed to improve 3D Gaussian Splatting (3DGS) by addressing artifacts caused by transient objects in rendered images. The method introduces a delayed Gaussian growth strategy that focuses on optimizing the static elements of a scene before dealing with transient disturbances, reducing the risk of overfitting. Additionally, it employs a scale-cascaded mask bootstrapping technique that starts with lower-resolution features for initial mask estimation, ensuring better semantic consistency before refining to high-resolution predictions. Through extensive testing, RobustSplat demonstrates superior performance compared to existing methods, showcasing its effectiveness in producing high-quality, artifact-free renderings.'}, 'zh': {'title': 'å¢å¼º3Dæ¸²æŸ“çš„é²æ£’æ€§', 'desc': 'RobustSplat æ˜¯ä¸€ç§é’ˆå¯¹ 3D é«˜æ–¯ç‚¹äº‘æ¸²æŸ“ä¸­å› ç¬æ€ç‰©ä½“å¼•èµ·çš„ä¼ªå½±é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡å»¶è¿Ÿé«˜æ–¯ç”Ÿé•¿å’Œå°ºåº¦çº§è”æ©ç è‡ªä¸¾æ¥ä¼˜åŒ–é™æ€åœºæ™¯ç»“æ„ï¼Œå‡å°‘å¯¹ç¬æ€ç‰©ä½“çš„è¿‡æ‹Ÿåˆã€‚é¦–å…ˆï¼Œå»¶è¿Ÿé«˜æ–¯ç”Ÿé•¿ç­–ç•¥ç¡®ä¿åœ¨å…è®¸é«˜æ–¯åˆ†è£‚ä¹‹å‰ï¼Œå…ˆä¼˜åŒ–é™æ€åœºæ™¯ã€‚å…¶æ¬¡ï¼Œå°ºåº¦çº§è”æ©ç è‡ªä¸¾æ–¹æ³•åˆ©ç”¨ä½åˆ†è¾¨ç‡ç‰¹å¾ç›¸ä¼¼æ€§è¿›è¡Œåˆæ­¥æ©ç ä¼°è®¡ï¼Œéšåå†è¿›è¡Œé«˜åˆ†è¾¨ç‡ç›‘ç£ï¼Œä»¥æé«˜æ©ç é¢„æµ‹çš„ç²¾ç¡®åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05313', 'title': 'MARBLE: Material Recomposition and Blending in CLIP-Space', 'url': 'https://huggingface.co/papers/2506.05313', 'abstract': 'MARBLE utilizes material embeddings in CLIP-space to control pre-trained text-to-image models for blending and recomposing material properties in images with parametric control over attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t Editing materials of objects in images based on exemplar images is an active area of research in computer vision and graphics. We propose MARBLE, a method for performing material blending and recomposing fine-grained material properties by finding material embeddings in CLIP-space and using that to control pre-trained text-to-image models. We improve exemplar-based material editing by finding a block in the denoising UNet responsible for material attribution. Given two material exemplar-images, we find directions in the CLIP-space for blending the materials. Further, we can achieve parametric control over fine-grained material attributes such as roughness, metallic, transparency, and glow using a shallow network to predict the direction for the desired material attribute change. We perform qualitative and quantitative analysis to demonstrate the efficacy of our proposed method. We also present the ability of our method to perform multiple edits in a single forward pass and applicability to painting.   Project Page: https://marblecontrol.github.io/', 'score': 2, 'issue_id': 4169, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'e941d4fe8c8db7a2', 'authors': ['Ta-Ying Cheng', 'Prafull Sharma', 'Mark Boss', 'Varun Jampani'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.05313.jpg', 'data': {'categories': ['#cv', '#games', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'MARBLE - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ CLIP. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-image Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². MARBLE Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ»Ğ¾Ğº Ğ² U-Net, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ·Ğ° Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² ĞºĞ°Ğº ÑˆĞµÑ€Ğ¾Ñ…Ğ¾Ğ²Ğ°Ñ‚Ğ¾ÑÑ‚ÑŒ, Ğ¼ĞµÑ‚Ğ°Ğ»Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ²ĞµÑ‡ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Blend and Recompose Materials with MARBLE!', 'desc': 'MARBLE is a novel method that enhances material editing in images by utilizing material embeddings in CLIP-space. It allows for the blending and recomposing of material properties in images through pre-trained text-to-image models. By identifying specific blocks in the denoising UNet that handle material attributes, MARBLE can manipulate fine-grained properties like roughness and transparency. The method also supports multiple edits in one pass, showcasing its efficiency and versatility in applications such as digital painting.'}, 'zh': {'title': 'MARBLEï¼šæ™ºèƒ½ææ–™ç¼–è¾‘çš„æ–°æ–¹æ³•', 'desc': 'MARBLEæ˜¯ä¸€ç§åˆ©ç”¨CLIPç©ºé—´ä¸­çš„ææ–™åµŒå…¥æ¥æ§åˆ¶é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒå¯ä»¥å®ç°å›¾åƒä¸­ææ–™å±æ€§çš„æ··åˆå’Œé‡ç»„ï¼Œå¹¶å¯¹ç»†ç²’åº¦ææ–™å±æ€§è¿›è¡Œå‚æ•°åŒ–æ§åˆ¶ã€‚é€šè¿‡åœ¨å»å™ªUNetä¸­æ‰¾åˆ°ä¸ææ–™å½’å±ç›¸å…³çš„å—ï¼ŒMARBLEæ”¹è¿›äº†åŸºäºç¤ºä¾‹çš„ææ–™ç¼–è¾‘ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­è¿›è¡Œå¤šæ¬¡ç¼–è¾‘ï¼Œå¹¶é€‚ç”¨äºç»˜ç”»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05046', 'title': 'FlowDirector: Training-Free Flow Steering for Precise Text-to-Video\n  Editing', 'url': 'https://huggingface.co/papers/2506.05046', 'abstract': 'FlowDirector, an inversion-free video editing framework, uses ODEs for spatiotemporal coherent editing and attention-guided masking for localized control, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-driven video editing aims to modify video content according to natural language instructions. While recent training-free approaches have made progress by leveraging pre-trained diffusion models, they typically rely on inversion-based techniques that map input videos into the latent space, which often leads to temporal inconsistencies and degraded structural fidelity. To address this, we propose FlowDirector, a novel inversion-free video editing framework. Our framework models the editing process as a direct evolution in data space, guiding the video via an Ordinary Differential Equation (ODE) to smoothly transition along its inherent spatiotemporal manifold, thereby preserving temporal coherence and structural details. To achieve localized and controllable edits, we introduce an attention-guided masking mechanism that modulates the ODE velocity field, preserving non-target regions both spatially and temporally. Furthermore, to address incomplete edits and enhance semantic alignment with editing instructions, we present a guidance-enhanced editing strategy inspired by Classifier-Free Guidance, which leverages differential signals between multiple candidate flows to steer the editing trajectory toward stronger semantic alignment without compromising structural consistency. Extensive experiments across benchmarks demonstrate that FlowDirector achieves state-of-the-art performance in instruction adherence, temporal consistency, and background preservation, establishing a new paradigm for efficient and coherent video editing without inversion.', 'score': 2, 'issue_id': 4170, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'a9c76743a2f7e8d6', 'authors': ['Guangzhao Li', 'Yanming Yang', 'Chenxi Song', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'Central South University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05046.jpg', 'data': {'categories': ['#games', '#video', '#multimodal', '#benchmark', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞĞ”Ğ£ Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'FlowDirector - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‹ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ (ODE) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'FlowDirector: Revolutionizing Video Editing with ODEs and Attention', 'desc': "FlowDirector is a new video editing framework that avoids the common inversion-based techniques, which can cause problems like temporal inconsistencies. Instead, it uses Ordinary Differential Equations (ODEs) to guide video edits directly in the data space, ensuring smooth transitions while maintaining the video's structure and timing. The framework also incorporates an attention-guided masking system that allows for precise control over which parts of the video are edited, preserving the areas that should remain unchanged. Additionally, it employs a guidance-enhanced strategy to improve the alignment of edits with user instructions, achieving top performance in video editing tasks."}, 'zh': {'title': 'æ— åæ¼”è§†é¢‘ç¼–è¾‘çš„æ–°èŒƒå¼', 'desc': 'FlowDirectoræ˜¯ä¸€ç§æ— åæ¼”çš„è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼Œåˆ©ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è¿›è¡Œæ—¶ç©ºä¸€è‡´æ€§ç¼–è¾‘ã€‚è¯¥æ¡†æ¶é€šè¿‡ç›´æ¥åœ¨æ•°æ®ç©ºé—´ä¸­å»ºæ¨¡ç¼–è¾‘è¿‡ç¨‹ï¼Œç¡®ä¿è§†é¢‘åœ¨å…¶å›ºæœ‰çš„æ—¶ç©ºæµå½¢ä¸Šå¹³æ»‘è¿‡æ¸¡ï¼Œä»è€Œä¿æŒæ—¶é—´ä¸€è‡´æ€§å’Œç»“æ„ç»†èŠ‚ã€‚ä¸ºäº†å®ç°å±€éƒ¨å¯æ§çš„ç¼–è¾‘ï¼ŒFlowDirectorå¼•å…¥äº†åŸºäºæ³¨æ„åŠ›çš„æ©è†œæœºåˆ¶ï¼Œè°ƒèŠ‚ODEé€Ÿåº¦åœºï¼Œä¿æŠ¤éç›®æ ‡åŒºåŸŸçš„æ—¶ç©ºç‰¹æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¢å¼ºæŒ‡å¯¼çš„ç¼–è¾‘ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†ä¸ç¼–è¾‘æŒ‡ä»¤çš„è¯­ä¹‰å¯¹é½ï¼Œç¡®ä¿äº†ç¼–è¾‘çš„å®Œæ•´æ€§å’Œç»“æ„ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04559', 'title': 'Perceptual Decoupling for Scalable Multi-modal Reasoning via\n  Reward-Optimized Captioning', 'url': 'https://huggingface.co/papers/2506.04559', 'abstract': "A reasoning-aligned reinforcement learning strategy enhances visual representations in multi-modal large language models by optimizing captions for downstream reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in slow-thinking language models (e.g., OpenAI-o1 and DeepSeek-R1) have demonstrated remarkable abilities in complex reasoning tasks by emulating human-like reflective cognition. However, extending such capabilities to multi-modal large language models (MLLMs) remains challenging due to the high cost of retraining vision-language alignments when upgrading the underlying reasoner LLMs. A straightforward solution is to decouple perception from reasoning, i.e., converting visual inputs into language representations (e.g., captions) that are then passed to a powerful text-only reasoner. However, this decoupling introduces a critical challenge: the visual extractor must generate descriptions that are both faithful to the image and informative enough to support accurate downstream reasoning. To address this, we propose Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) - a reasoning-guided reinforcement learning strategy that aligns the extractor's captioning behavior with the reasoning objective. By closing the perception-reasoning loop via reward-based optimization, RACRO significantly enhances visual grounding and extracts reasoning-optimized representations. Experiments on multi-modal math and science benchmarks show that the proposed RACRO method achieves state-of-the-art average performance while enabling superior scalability and plug-and-play adaptation to more advanced reasoning LLMs without the necessity for costly multi-modal re-alignment.", 'score': 2, 'issue_id': 4170, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'cc35ebabca1ba808', 'authors': ['Yunhao Gou', 'Kai Chen', 'Zhili Liu', 'Lanqing Hong', 'Xin Jin', 'Zhenguo Li', 'James T. Kwok', 'Yu Zhang'], 'affiliations': ['Huawei Cloud', 'Huawei Noahs Ark Lab', 'Southern University of Science and Technology', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04559.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#multimodal', '#rl', '#rag', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ RACRO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. RACRO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RACRO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°ÑƒĞºĞ°Ğ¼.'}, 'en': {'title': 'Enhancing Visual Reasoning in MLLMs with RACRO', 'desc': "This paper introduces a new reinforcement learning strategy called Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) to improve how visual information is represented in multi-modal large language models (MLLMs). The approach focuses on generating captions from visual inputs that are both accurate and informative, which helps in enhancing reasoning tasks. By using a reward-based optimization method, RACRO aligns the visual extractor's output with the reasoning goals of the model. The results show that RACRO achieves top performance on various benchmarks while allowing for easier integration with advanced reasoning models without the need for expensive retraining."}, 'zh': {'title': 'æ¨ç†é©±åŠ¨çš„è§†è§‰è¡¨ç¤ºä¼˜åŒ–ç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRACROçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–å›¾åƒæè¿°æ¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ„ŸçŸ¥ä¸æ¨ç†è§£è€¦ï¼Œå°†è§†è§‰è¾“å…¥è½¬æ¢ä¸ºè¯­è¨€è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å¼ºå¤§çš„æ–‡æœ¬æ¨ç†æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚RACROé€šè¿‡å¥–åŠ±ä¼˜åŒ–ï¼Œä½¿å¾—å›¾åƒæè¿°æ—¢å¿ å®äºå›¾åƒï¼Œåˆèƒ½æ”¯æŒå‡†ç¡®çš„æ¨ç†ä»»åŠ¡ï¼Œä»è€Œæå‡è§†è§‰å¯¹é½èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRACROåœ¨å¤šæ¨¡æ€æ•°å­¦å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04462', 'title': 'Watermarking Degrades Alignment in Language Models: Analysis and\n  Mitigation', 'url': 'https://huggingface.co/papers/2506.04462', 'abstract': 'Watermarking techniques in LLMs can degrade truthfulness, safety, and helpfulness, and alignment resampling is proposed to restore alignment while ensuring watermark detectability.  \t\t\t\t\tAI-generated summary \t\t\t\t Watermarking techniques for large language models (LLMs) can significantly impact output quality, yet their effects on truthfulness, safety, and helpfulness remain critically underexamined. This paper presents a systematic analysis of how two popular watermarking approaches-Gumbel and KGW-affect these core alignment properties across four aligned LLMs. Our experiments reveal two distinct degradation patterns: guard attenuation, where enhanced helpfulness undermines model safety, and guard amplification, where excessive caution reduces model helpfulness. These patterns emerge from watermark-induced shifts in token distribution, surfacing the fundamental tension that exists between alignment objectives.   To mitigate these degradations, we propose Alignment Resampling (AR), an inference-time sampling method that uses an external reward model to restore alignment. We establish a theoretical lower bound on the improvement in expected reward score as the sample size is increased and empirically demonstrate that sampling just 2-4 watermarked generations effectively recovers or surpasses baseline (unwatermarked) alignment scores. To overcome the limited response diversity of standard Gumbel watermarking, our modified implementation sacrifices strict distortion-freeness while maintaining robust detectability, ensuring compatibility with AR. Experimental results confirm that AR successfully recovers baseline alignment in both watermarking approaches, while maintaining strong watermark detectability. This work reveals the critical balance between watermark strength and model alignment, providing a simple inference-time solution to responsibly deploy watermarked LLMs in practice.', 'score': 2, 'issue_id': 4168, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '35a6c458e61b75e3', 'authors': ['Apurv Verma', 'NhatHai Phan', 'Shubhendu Trivedi'], 'affiliations': ['MIT', 'New Jersey Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04462.jpg', 'data': {'categories': ['#hallucinations', '#alignment', '#rlhf', '#inference'], 'emoji': 'ğŸ”’', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM), Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¸Ñ… Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸: Ğ¾ÑĞ»Ğ°Ğ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Alignment Resampling (AR), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AR ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ².'}, 'en': {'title': 'Balancing Watermarking and Model Alignment in LLMs', 'desc': 'This paper investigates how watermarking techniques in large language models (LLMs) can negatively affect their truthfulness, safety, and helpfulness. It identifies two main degradation patterns: guard attenuation, where increased helpfulness compromises safety, and guard amplification, where excessive caution limits helpfulness. To address these issues, the authors propose a method called Alignment Resampling (AR), which uses an external reward model to restore alignment during inference. The results show that AR can effectively recover alignment scores while ensuring that the watermarks remain detectable, highlighting the delicate balance between watermark strength and model performance.'}, 'zh': {'title': 'æ°´å°ä¸æ¨¡å‹å¯¹é½çš„å¹³è¡¡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ°´å°æŠ€æœ¯å¯¹è¾“å‡ºè´¨é‡çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å¯¹çœŸå®æ€§ã€å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§çš„å½±å“ã€‚ç ”ç©¶åˆ†æäº†ä¸¤ç§æµè¡Œçš„æ°´å°æ–¹æ³•â€”â€”Gumbelå’ŒKGWï¼Œå¦‚ä½•å½±å“è¿™å››ä¸ªå¯¹é½å±æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ°´å°å¼•èµ·çš„ä»£å¸åˆ†å¸ƒå˜åŒ–å¯¼è‡´äº†ä¸¤ç§ä¸åŒçš„é™çº§æ¨¡å¼ï¼šä¿æŠ¤å‡å¼±å’Œä¿æŠ¤å¢å¼ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºå¯¹é½é‡é‡‡æ ·ï¼ˆARï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å¤–éƒ¨å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†æ—¶æ¢å¤å¯¹é½ï¼ŒåŒæ—¶ä¿æŒæ°´å°çš„å¯æ£€æµ‹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02587', 'title': "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View\n  Representations", 'url': 'https://huggingface.co/papers/2506.02587', 'abstract': "BEVCALIB model uses bird's-eye view features for accurate LiDAR-camera calibration from raw data, demonstrating superior performance under various noise conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate LiDAR-camera calibration is fundamental to fusing multi-modal perception in autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully utilize the geometric information from the BEV feature, we introduce a novel feature selector to filter the most important features in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations on KITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a new state of the art. Under various noise conditions, BEVCALIB outperforms the best baseline in the literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation), respectively. In the open-source domain, it improves the best reproducible baseline by one order of magnitude. Our code and demo results are available at https://cisl.ucr.edu/BEVCalib.", 'score': 2, 'issue_id': 4160, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '8d1e0e49dea5dcad', 'authors': ['Weiduo Yuan', 'Jerry Li', 'Justin Yue', 'Divyank Shah', 'Konstantinos Karydis', 'Hang Qiu'], 'affiliations': ['University of California, Riverside', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.02587.jpg', 'data': {'categories': ['#robotics', '#cv', '#dataset', '#optimization', '#open_source'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞµ LiDAR-ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´Ğ° ÑĞ²ĞµÑ€Ñ…Ñƒ', 'desc': 'BEVCALIB - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ° ÑĞ²ĞµÑ€Ñ…Ñƒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ LiDAR-ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ° ÑĞ²ĞµÑ€Ñ…Ñƒ ĞºĞ°Ğº Ğ´Ğ»Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ LiDAR Ğ² Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². BEVCALIB Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑˆÑƒĞ¼Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ LiDAR-ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… KITTI Ğ¸ NuScenes.'}, 'en': {'title': 'Revolutionizing LiDAR-Camera Calibration with BEV Features', 'desc': "The BEVCALIB model introduces a novel approach for calibrating LiDAR and camera systems using bird's-eye view (BEV) features extracted from raw data. This method addresses the limitations of traditional calibration techniques that struggle with dynamic transformations during vehicle or robot movement. By fusing separate BEV features from both LiDAR and camera into a shared feature space, BEVCALIB enhances the accuracy of multi-modal perception in autonomous systems. The model demonstrates significant performance improvements under various noise conditions, setting a new benchmark in the field with extensive evaluations on multiple datasets."}, 'zh': {'title': 'BEVCALIBï¼šé¸Ÿç°è§†å›¾ç‰¹å¾åŠ©åŠ›æ¿€å…‰é›·è¾¾ä¸ç›¸æœºç²¾ç¡®æ ‡å®š', 'desc': 'BEVCALIBæ¨¡å‹åˆ©ç”¨é¸Ÿç°è§†å›¾ç‰¹å¾è¿›è¡Œæ¿€å…‰é›·è¾¾ä¸ç›¸æœºçš„ç²¾ç¡®æ ‡å®šï¼Œèƒ½å¤Ÿä»åŸå§‹æ•°æ®ä¸­æå–ä¿¡æ¯ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒBEVCALIBåœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººç³»ç»Ÿä¸­èåˆå¤šæ¨¡æ€æ„ŸçŸ¥æ—¶è‡³å…³é‡è¦ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ†åˆ«æå–ç›¸æœºå’Œæ¿€å…‰é›·è¾¾çš„é¸Ÿç°è§†å›¾ç‰¹å¾ï¼Œå¹¶å°†å…¶èåˆåˆ°å…±äº«çš„ç‰¹å¾ç©ºé—´ä¸­ï¼Œæ˜¾è‘—æé«˜äº†æ ‡å®šçš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¼•å…¥æ–°é¢–çš„ç‰¹å¾é€‰æ‹©å™¨ï¼ŒBEVCALIBåœ¨å‡å°‘å†…å­˜æ¶ˆè€—çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ›´å¥½çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23115', 'title': 'Diffusion-Based Generative Models for 3D Occupancy Prediction in\n  Autonomous Driving', 'url': 'https://huggingface.co/papers/2505.23115', 'abstract': 'Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as a generative modeling task using diffusion models, which learn the underlying data distribution and incorporate 3D scene priors. This approach enhances prediction consistency, noise robustness, and better handles the intricacies of 3D spatial structures. Our extensive experiments show that diffusion-based generative models outperform state-of-the-art discriminative approaches, delivering more realistic and accurate occupancy predictions, especially in occluded or low-visibility regions. Moreover, the improved predictions significantly benefit downstream planning tasks, highlighting the practical advantages of our method for real-world autonomous driving applications.', 'score': 2, 'issue_id': 4162, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '9780859f709be640', 'authors': ['Yunshen Wang', 'Yicheng Liu', 'Tianyuan Yuan', 'Yucheng Mao', 'Yingshi Liang', 'Xiuyu Yang', 'Honggang Zhang', 'Hang Zhao'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Institute for Interdisciplinary Information Sciences, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23115.jpg', 'data': {'categories': ['#diffusion', '#agents', '#3d', '#cv'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ 3D-Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğ¹ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ° Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing 3D Occupancy Prediction with Diffusion Models', 'desc': 'This paper addresses the challenge of predicting 3D occupancy grids from visual inputs for autonomous driving, particularly in the presence of noisy data and incomplete observations. The authors propose a novel approach by reframing the problem as a generative modeling task using diffusion models, which learn the data distribution and incorporate 3D scene priors. This method improves prediction consistency and robustness against noise, effectively managing the complexities of 3D spatial structures. Experimental results demonstrate that their diffusion-based models outperform traditional discriminative methods, leading to more accurate occupancy predictions that enhance downstream planning tasks in real-world driving scenarios.'}, 'zh': {'title': 'ç”Ÿæˆæ¨¡å‹æå‡3Då ç”¨é¢„æµ‹çš„å‡†ç¡®æ€§', 'desc': 'æœ¬ç ”ç©¶å°†3Då ç”¨ç½‘æ ¼çš„é¢„æµ‹è§†ä¸ºç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ï¼Œé‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¥å¤„ç†è§†è§‰è¾“å…¥ã€‚ä¸ä¼ ç»Ÿçš„åˆ¤åˆ«æ–¹æ³•ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹å™ªå£°æ•°æ®å’Œä¸å®Œæ•´è§‚æµ‹ï¼ŒåŒæ—¶æœ‰æ•ˆæ•æ‰3Dåœºæ™¯çš„å¤æ‚ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨å ç”¨é¢„æµ‹çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åˆ¤åˆ«æ–¹æ³•ï¼Œå°¤å…¶åœ¨é®æŒ¡æˆ–ä½å¯è§åº¦åŒºåŸŸè¡¨ç°æ›´ä½³ã€‚è¯¥æ–¹æ³•çš„æ”¹è¿›é¢„æµ‹æ˜¾è‘—æå‡äº†åç»­è§„åˆ’ä»»åŠ¡çš„æ•ˆæœï¼Œå±•ç¤ºäº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04996', 'title': 'PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment', 'url': 'https://huggingface.co/papers/2506.04996', 'abstract': 'PATS is a novel temporal sampling method that enhances video analysis of athletic skills by ensuring complete movement patterns are captured, outperforming existing methods across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated sports skill assessment requires capturing fundamental movement patterns that distinguish expert from novice performance, yet current video sampling methods disrupt the temporal continuity essential for proficiency evaluation. To this end, we introduce Proficiency-Aware Temporal Sampling (PATS), a novel sampling strategy that preserves complete fundamental movements within continuous temporal segments for multi-view skill assessment. PATS adaptively segments videos to ensure each analyzed portion contains full execution of critical performance components, repeating this process across multiple segments to maximize information coverage while maintaining temporal coherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses the state-of-the-art accuracy across all viewing configurations (+0.65% to +3.05%) and delivers substantial gains in challenging domains (+26.22% bouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that PATS successfully adapts to diverse activity characteristics-from high-frequency sampling for dynamic sports to fine-grained segmentation for sequential skills-demonstrating its effectiveness as an adaptive approach to temporal sampling that advances automated skill assessment for real-world applications.', 'score': 1, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'f0a38c5292395f88', 'authors': ['Edoardo Bianchi', 'Antonio Liotta'], 'affiliations': ['Faculty of Engineering Free University of Bozen-Bolzano Bozen-Bolzano, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2506.04996.jpg', 'data': {'categories': ['#benchmark', '#video'], 'emoji': 'ğŸ‹ï¸', 'ru': {'title': 'PATS: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ°', 'desc': 'PATS (Proficiency-Aware Temporal Sampling) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. PATS Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ»Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞÑ†ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ EgoExo4D Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SkillFormer, PATS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ°.'}, 'en': {'title': 'Enhancing Athletic Skill Analysis with PATS', 'desc': 'PATS, or Proficiency-Aware Temporal Sampling, is a new method designed to improve the analysis of athletic skills in videos. It captures complete movement patterns by maintaining the temporal continuity necessary for evaluating performance. This method adaptively segments videos to ensure that each analyzed part includes the full execution of key skills, enhancing the accuracy of assessments. PATS has shown to outperform existing techniques in various sports and activities, making it a significant advancement in automated skill evaluation.'}, 'zh': {'title': 'PATSï¼šæå‡è¿åŠ¨æŠ€èƒ½åˆ†æçš„æ—¶é—´é‡‡æ ·æ–°æ–¹æ³•', 'desc': 'PATSæ˜¯ä¸€ç§æ–°é¢–çš„æ—¶é—´é‡‡æ ·æ–¹æ³•ï¼Œæ—¨åœ¨æå‡è¿åŠ¨æŠ€èƒ½çš„è§†é¢‘åˆ†æã€‚å®ƒé€šè¿‡ç¡®ä¿å®Œæ•´çš„è¿åŠ¨æ¨¡å¼è¢«æ•æ‰ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é‡‡æ ·æ–¹æ³•ã€‚PATSèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ†æ®µè§†é¢‘ï¼Œç¡®ä¿æ¯ä¸ªåˆ†æéƒ¨åˆ†éƒ½åŒ…å«å…³é”®è¡¨ç°ç»„ä»¶çš„å®Œæ•´æ‰§è¡Œã€‚ç»è¿‡è¯„ä¼°ï¼ŒPATSåœ¨å¤šä¸ªé¢†åŸŸçš„å‡†ç¡®æ€§ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è‡ªåŠ¨åŒ–æŠ€èƒ½è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03238', 'title': 'Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach', 'url': 'https://huggingface.co/papers/2506.03238', 'abstract': 'OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated interpretation of CT images-particularly localizing and describing abnormal findings across multi-plane and whole-body scans-remains a significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose a comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute a dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OminiAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On benchmarks, we establish three representative evaluation tasks based on real clinical scenarios. Through extensive experiments, we show that OminiAbnorm-CT can significantly outperform existing methods on all the tasks and metrics.', 'score': 1, 'issue_id': 4155, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'f234199601bef528', 'authors': ['Ziheng Zhao', 'Lisong Dai', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['Department of Radiology, Renmin Hospital of Wuhan University', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.03238.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#healthcare', '#cv'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ˜Ğ˜-Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ĞšĞ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ²', 'desc': 'ĞœĞ¾Ğ´ĞµĞ»ÑŒ OminiAbnorm-CT Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ĞšĞ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ‚ĞµĞ»Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ²ÑĞµĞ¾Ğ±ÑŠĞµĞ¼Ğ»ÑÑ‰ĞµĞ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. OminiAbnorm-CT Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞšĞ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing CT Image Analysis with OminiAbnorm-CT', 'desc': 'OminiAbnorm-CT is a novel model designed to enhance the automated interpretation of CT images by accurately localizing and describing abnormalities. It introduces a comprehensive hierarchical classification system developed in collaboration with radiologists, covering 404 abnormal findings across various body regions. The model is trained on a large dataset of over 14.5K CT images, with detailed annotations for more than 19K abnormalities, ensuring robust performance. Through rigorous evaluation, OminiAbnorm-CT demonstrates superior accuracy compared to existing methods, making it a significant advancement in clinical radiology.'}, 'zh': {'title': 'OminiAbnorm-CTï¼šCTå›¾åƒå¼‚å¸¸è‡ªåŠ¨è§£è¯»çš„æ–°çªç ´', 'desc': 'OminiAbnorm-CTæ˜¯ä¸€ç§ç”¨äºè‡ªåŠ¨è§£è¯»CTå›¾åƒçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒèº«ä½“éƒ¨ä½ä¸­å®šä½å’Œæè¿°å¼‚å¸¸æƒ…å†µã€‚è¯¥ç ”ç©¶é€šè¿‡ä¸èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿåˆä½œï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«404ç§å¼‚å¸¸å‘ç°çš„å±‚æ¬¡åˆ†ç±»ç³»ç»Ÿã€‚æˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡14.5K CTå›¾åƒçš„æ•°æ®é›†ï¼Œå¹¶ä¸ºè¶…è¿‡19Kå¼‚å¸¸æä¾›äº†è¯¦ç»†çš„æ³¨é‡Šã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒOminiAbnorm-CTåœ¨æ‰€æœ‰ä»»åŠ¡å’ŒæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02444', 'title': 'SViMo: Synchronized Diffusion for Video and Motion Generation in\n  Hand-object Interaction Scenarios', 'url': 'https://huggingface.co/papers/2506.02444', 'abstract': "A framework combining visual priors and dynamic constraints within a synchronized diffusion process generates HOI video and motion simultaneously, enhancing video-motion consistency and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose a novel framework that combines visual priors and dynamic constraints within a synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling inter- and intra-modal dependencies. Furthermore, we introduce a vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish a closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our method's superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at https://github.com/Droliven/SViMo\\_project.", 'score': 1, 'issue_id': 4168, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'a1ed02142b4e22ee', 'authors': ['Lingwei Dang', 'Ruizhi Shao', 'Hongwen Zhang', 'Wei Min', 'Yebin Liu', 'Qingyao Wu'], 'affiliations': ['Department of Automation, Tsinghua University', 'School of Artificial Intelligence, Beijing Normal University', 'School of Software Engineering, South China University of Technology', 'Shadow AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.02444.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal', '#3d', '#games', '#diffusion'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ (HOI). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ 3D Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ğ¾Ñ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ñ 3D Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Synchronized Diffusion for Realistic HOI Generation', 'desc': 'This paper presents a new framework for generating Hand-Object Interaction (HOI) videos and motions simultaneously, improving consistency and generalization. It combines visual priors and dynamic constraints using a synchronized diffusion process, which allows for the creation of realistic interactions without relying on predefined 3D models. The method employs tri-modal adaptive modulation and 3D full-attention to align features and model dependencies effectively. Experimental results show that this approach outperforms existing methods in producing high-quality, physically plausible HOI sequences, even in new scenarios.'}, 'zh': {'title': 'åŒæ­¥æ‰©æ•£ç”Ÿæˆé«˜ä¿çœŸæ‰‹-ç‰©ä½“äº¤äº’è§†é¢‘', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè§†è§‰å…ˆéªŒå’ŒåŠ¨æ€çº¦æŸï¼Œåœ¨åŒæ­¥æ‰©æ•£è¿‡ç¨‹ä¸­åŒæ—¶ç”Ÿæˆæ‰‹-ç‰©ä½“äº¤äº’ï¼ˆHOIï¼‰è§†é¢‘å’Œè¿åŠ¨ã€‚è¿™ç§æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿ3D HOIè¿åŠ¨ç”Ÿæˆä¾èµ–é¢„å®šä¹‰æ¨¡å‹å’Œæ•è·æ•°æ®çš„å±€é™æ€§ï¼Œæå‡äº†ç”Ÿæˆçš„ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸‰æ¨¡æ€è‡ªé€‚åº”è°ƒåˆ¶å’Œ3Då…¨æ³¨æ„åŠ›æœºåˆ¶æ¥å¯¹é½ç‰¹å¾ï¼Œå¹¶å¼•å…¥è§†è§‰æ„ŸçŸ¥çš„3Däº¤äº’æ‰©æ•£æ¨¡å‹ï¼Œç›´æ¥ä»åŒæ­¥æ‰©æ•£è¾“å‡ºç”Ÿæˆ3Däº¤äº’åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜ä¿çœŸã€åŠ¨æ€åˆç†çš„HOIåºåˆ—æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå°¤å…¶åœ¨æœªè§è¿‡çš„çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00981', 'title': 'What do self-supervised speech models know about Dutch? Analyzing\n  advantages of language-specific pre-training', 'url': 'https://huggingface.co/papers/2506.00981', 'abstract': "Self-supervised Wav2Vec2 models encode Dutch linguistic features more accurately when pre-trained exclusively on Dutch data, compared to similar amounts of English or multilingual data, as shown by clustering and classification probes, and demonstrated through improved Automatic Speech Recognition performance.  \t\t\t\t\tAI-generated summary \t\t\t\t How language-specific are speech representations learned by self-supervised models? Existing work has shown that a range of linguistic features can be successfully decoded from end-to-end models trained only on speech recordings. However, it's less clear to what extent pre-training on specific languages improves language-specific linguistic information. Here we test the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the representation of Dutch linguistic features as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific advantage is well-detected by trained clustering or classification probes, and partially observable using zero-shot metrics. Furthermore, the language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition.", 'score': 1, 'issue_id': 4165, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': 'e571a309d437ed8b', 'authors': ['Marianne de Heer Kloots', 'Hosein Mohebbi', 'Charlotte Pouw', 'Gaofei Shen', 'Willem Zuidema', 'Martijn Bentum'], 'affiliations': ['Centre for Language Studies, Radboud University, Netherlands', 'Cognitive Science and Artificial Intelligence, Tilburg University, The Netherlands', 'Institute for Logic, Language and Computation, University of Amsterdam, The Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2506.00981.jpg', 'data': {'categories': ['#transfer_learning', '#multilingual', '#audio', '#low_resource'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Wav2Vec2, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ³Ğ¾Ğ»Ğ»Ğ°Ğ½Ğ´ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ»ÑƒÑ‡ÑˆĞµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ¾Ğ»Ğ»Ğ°Ğ½Ğ´ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Unlocking Dutch: The Power of Language-Specific Pre-Training', 'desc': 'This paper investigates how well self-supervised Wav2Vec2 models can learn Dutch language features when trained specifically on Dutch data. The study finds that pre-training exclusively on Dutch leads to better encoding of Dutch phonetic and lexical information compared to using English or multilingual data. The improvements are measured using clustering and classification probes, which show that the models capture language-specific characteristics more effectively. Additionally, these enhancements in linguistic representation correlate with better performance in Automatic Speech Recognition tasks.'}, 'zh': {'title': 'ä¸“æ³¨è·å…°è¯­ï¼Œæå‡è¯­éŸ³è¯†åˆ«è¡¨ç°', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹Wav2Vec2åœ¨ç¼–ç è·å…°è¯­è¯­è¨€ç‰¹å¾æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå½“æ¨¡å‹ä»…åœ¨è·å…°è¯­æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ•æ‰è·å…°è¯­çš„è¯­éŸ³å’Œè¯æ±‡ä¿¡æ¯ã€‚ä¸åœ¨è‹±è¯­æˆ–å¤šè¯­è¨€æ•°æ®ä¸Šè¿›è¡Œç›¸ä¼¼é‡çš„é¢„è®­ç»ƒç›¸æ¯”ï¼Œè·å…°è¯­ç‰¹å¾çš„è¡¨ç¤ºæ˜¾è‘—æé«˜ã€‚è¯¥è¯­è¨€ç‰¹å®šçš„ä¼˜åŠ¿é€šè¿‡èšç±»å’Œåˆ†ç±»æ¢æµ‹å™¨å¾—åˆ°äº†éªŒè¯ï¼Œå¹¶ä¸”ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„æ€§èƒ½æå‡ç›¸ä¸€è‡´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18095', 'title': 'ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation', 'url': 'https://huggingface.co/papers/2506.18095', 'abstract': "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.", 'score': 53, 'issue_id': 4496, 'pub_date': '2025-06-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ½Ñ', 'en': 'June 22', 'zh': '6æœˆ22æ—¥'}, 'hash': 'ea0d767800ce404b', 'authors': ['Junying Chen', 'Zhenyang Cai', 'Pengcheng Chen', 'Shunian Chen', 'Ke Ji', 'Xidong Wang', 'Yunjin Yang', 'Benyou Wang'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.18095.jpg', 'data': {'categories': ['#cv', '#dataset', '#open_source', '#multimodal', '#synthetic'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ShareGPT-4o-Image - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 45 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ 46 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4o. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Janus-4o, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. Janus-4o Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ¼ Janus-Pro Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 91 Ñ‚Ñ‹ÑÑÑ‡Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ShareGPT-4o-Image Ğ¸ Janus-4o Ğ±ÑƒĞ´ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Democratizing Photorealistic Image Generation with Open Datasets and Models', 'desc': 'This paper introduces ShareGPT-4o-Image, a comprehensive dataset designed to enhance photorealistic image generation aligned with user instructions. It includes 45,000 text-to-image and 46,000 text-and-image-to-image samples, all generated using the advanced capabilities of GPT-4o. The authors also present Janus-4o, a multimodal large language model that improves upon previous models by enabling both text-to-image and text-and-image-to-image generation. With only 91,000 synthetic samples and minimal training time, Janus-4o demonstrates significant advancements in generating high-quality images, promoting open research in this field.'}, 'zh': {'title': 'å¼€æ”¾ç ”ç©¶ï¼ŒçœŸå®æ„Ÿå›¾åƒç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ShareGPT-4o-Imageæ•°æ®é›†å’ŒJanus-4oæ¨¡å‹ï¼Œæ—¨åœ¨æ¨åŠ¨å¼€æ”¾ç ”ç©¶åœ¨çœŸå®æ„Ÿå›¾åƒç”Ÿæˆé¢†åŸŸçš„å‘å±•ã€‚ShareGPT-4o-ImageåŒ…å«45Kæ–‡æœ¬åˆ°å›¾åƒå’Œ46Kæ–‡æœ¬ä¸å›¾åƒåˆ°å›¾åƒçš„æ•°æ®ï¼Œåˆ©ç”¨GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›è¿›è¡Œåˆæˆã€‚Janus-4oæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬ä¸å›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ•ˆæœã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å·¥å…·èƒ½å¤Ÿä¿ƒè¿›çœŸå®æ„Ÿã€æŒ‡ä»¤å¯¹é½çš„å›¾åƒç”Ÿæˆç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19103', 'title': 'Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency\n  Models', 'url': 'https://huggingface.co/papers/2506.19103', 'abstract': 'A new framework using consistency models enhances image inversion and editing efficiency, achieving top performance with fewer steps.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in image editing with diffusion models have achieved impressive results, offering fine-grained control over the generation process. However, these methods are computationally intensive because of their iterative nature. While distilled diffusion models enable faster inference, their editing capabilities remain limited, primarily because of poor inversion quality. High-fidelity inversion and reconstruction are essential for precise image editing, as they preserve the structural and semantic integrity of the source image. In this work, we propose a novel framework that enhances image inversion using consistency models, enabling high-quality editing in just four steps. Our method introduces a cycle-consistency optimization strategy that significantly improves reconstruction accuracy and enables a controllable trade-off between editability and content preservation. We achieve state-of-the-art performance across various image editing tasks and datasets, demonstrating that our method matches or surpasses full-step diffusion models while being substantially more efficient. The code of our method is available on GitHub at https://github.com/ControlGenAI/Inverse-and-Edit.', 'score': 35, 'issue_id': 4503, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '2bce24e10f3bf399', 'authors': ['Ilia Beletskii', 'Andrey Kuznetsov', 'Aibek Alanov'], 'affiliations': ['AIRI', 'HSE University', 'Innopolis', 'Sber'], 'pdf_title_img': 'assets/pdf/title_img/2506.19103.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#open_source', '#training', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑˆĞ°Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ”Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ñ‹ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Image Editing with Consistency Models', 'desc': 'This paper presents a new framework that improves image inversion and editing by using consistency models. The proposed method allows for high-quality image editing in just four steps, making it much more efficient than traditional diffusion models. It employs a cycle-consistency optimization strategy to enhance reconstruction accuracy while balancing editability and content preservation. The results show that this approach achieves state-of-the-art performance in various image editing tasks, outperforming existing methods in both speed and quality.'}, 'zh': {'title': 'é«˜æ•ˆå›¾åƒç¼–è¾‘çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä¸€è‡´æ€§æ¨¡å‹æ¥å¢å¼ºå›¾åƒåæ¼”å’Œç¼–è¾‘çš„æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¾ªç¯ä¸€è‡´æ€§ä¼˜åŒ–ç­–ç•¥ï¼Œæé«˜äº†é‡å»ºçš„å‡†ç¡®æ€§ï¼Œä½¿å¾—é«˜è´¨é‡çš„å›¾åƒç¼–è¾‘åªéœ€å››ä¸ªæ­¥éª¤ã€‚ä¸ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå†…å®¹å®Œæ•´æ€§çš„åŒæ—¶ï¼Œå®ç°æ›´é«˜çš„å¯ç¼–è¾‘æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºå…¨æ­¥éª¤çš„æ‰©æ•£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19697', 'title': 'Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.19697', 'abstract': 'Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.', 'score': 34, 'issue_id': 4495, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '3a6d3af4578d26f6', 'authors': ['Jungwoo Park', 'Taewhoo Lee', 'Chanwoong Yoon', 'Hyeon Hwang', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19697.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#open_source'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Outlier-Safe Pre-Training (OSP). OSP Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Muon, Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ RMSNorm Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OSP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM, Ğ´ĞµĞ»Ğ°Ñ Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ….'}, 'en': {'title': 'Preventing Outliers for Better LLM Performance', 'desc': 'This paper presents Outlier-Safe Pre-Training (OSP), a novel approach to enhance the quantization performance of large language models (LLMs) by preventing extreme activation outliers during training. The authors identify that these outliers significantly impair the efficiency of LLMs when deployed on devices, and propose a proactive strategy rather than relying on post-training fixes. OSP incorporates three innovations: the Muon optimizer for efficient training, Single-Scale RMSNorm to control channel-wise amplification, and a learnable embedding projection to manage activation magnitudes. The results show that OSP-trained models achieve superior performance in quantization benchmarks while maintaining low training overhead, indicating that outliers can be effectively managed through improved training techniques.'}, 'zh': {'title': 'åˆ›æ–°è®­ç»ƒæŠ€æœ¯ï¼Œæå‡æ¨¡å‹é‡åŒ–æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºOutlier-Safe Pre-Trainingï¼ˆOSPï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡åŒ–æ€§èƒ½ã€‚OSPé€šè¿‡åˆ›æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œä¸»åŠ¨é˜²æ­¢æç«¯æ¿€æ´»å¼‚å¸¸å€¼çš„å½¢æˆï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨è®¾å¤‡ä¸Šçš„é«˜æ•ˆéƒ¨ç½²ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šMuonä¼˜åŒ–å™¨ã€å•å°ºåº¦RMSNormå’Œå¯å­¦ä¹ çš„åµŒå…¥æŠ•å½±ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¼‚å¸¸å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSPæ¨¡å‹åœ¨4ä½é‡åŒ–ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç¤ºäº†è®­ç»ƒç­–ç•¥å¯¹å¼‚å¸¸å€¼çš„å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20512', 'title': 'OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling', 'url': 'https://huggingface.co/papers/2506.20512', 'abstract': 'Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.  \t\t\t\t\tAI-generated summary \t\t\t\t Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max).', 'score': 21, 'issue_id': 4500, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '67515964a17f77dc', 'authors': ['Zengzhi Wang', 'Fan Zhou', 'Xuefeng Li', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University, SII, GAIR Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.20512.jpg', 'data': {'categories': ['#rl', '#training', '#open_source', '#optimization', '#dataset', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑÑ‹ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Llama Ğ¸ Qwen, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Stable-then-Decay, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ° Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ OctoThinker. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ñ€ÑƒĞ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğº RL ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing RL Performance with Strategic Mid-Training', 'desc': "This paper explores how mid-training strategies can improve reinforcement learning (RL) performance in language models, specifically focusing on the Qwen and Llama families. It finds that using high-quality mathematical datasets and well-structured chain-of-thought reasoning examples significantly enhances the models' reasoning capabilities. The authors introduce a two-stage mid-training approach called Stable-then-Decay, which optimizes learning rates to improve RL outcomes. The resulting model, OctoThinker, shows improved compatibility with RL tasks, bridging the performance gap with other models designed for RL."}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ€§èƒ½', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†ä¸­æœŸè®­ç»ƒç­–ç•¥å¦‚ä½•å½±å“å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ¨ç†å¯†é›†å‹ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°é«˜è´¨é‡çš„æ•°å­¦è¯­æ–™åº“å’Œæ ¼å¼è‰¯å¥½çš„é“¾å¼æ¨ç†ç¤ºä¾‹æ˜¾è‘—æå‡äº†æ¨¡å‹çš„RLæ€§èƒ½ã€‚é€šè¿‡å¼•å…¥ç¨³å®š-è¡°å‡çš„ä¸¤é˜¶æ®µä¸­æœŸè®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬å¼€å‘äº†OctoThinkeræ¨¡å‹ç³»åˆ—ï¼Œå±•ç¤ºäº†ä¸å…¶ä»–RLå‹å¥½æ¨¡å‹çš„ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨ä¸ºåŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒç­–ç•¥æä¾›æŒ‡å¯¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ æ—¶ä»£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16012', 'title': 'DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning', 'url': 'https://huggingface.co/papers/2506.16012', 'abstract': 'A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing embodied agents capable of performing complex interactive tasks in real-world scenarios remains a fundamental challenge in embodied AI. Although recent advances in simulation platforms have greatly enhanced task diversity to train embodied Vision Language Models (VLMs), most platforms rely on simplified robot morphologies and bypass the stochastic nature of low-level execution, which limits their transferability to real-world robots. To address these issues, we present a physics-based simulation platform DualTHOR for complex dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our simulator includes real-world robot assets, a task suite for dual-arm collaboration, and inverse kinematics solvers for humanoid robots. We also introduce a contingency mechanism that incorporates potential failures through physics-based low-level execution, bridging the gap to real-world scenarios. Our simulator enables a more comprehensive evaluation of the robustness and generalization of VLMs in household environments. Extensive evaluations reveal that current VLMs struggle with dual-arm coordination and exhibit limited robustness in realistic environments with contingencies, highlighting the importance of using our simulator to develop more capable VLMs for embodied tasks. The code is available at https://github.com/ds199895/DualTHOR.git.', 'score': 18, 'issue_id': 4499, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '6a45067f4e822638', 'authors': ['Boyu Li', 'Siyuan He', 'Hang Xu', 'Haoqi Yuan', 'Yu Zang', 'Liwei Hu', 'Junpeng Yue', 'Zhenxiong Jiang', 'Pengbo Hu', 'BÃ¶rje F. Karlsson', 'Yehui Tang', 'Zongqing Lu'], 'affiliations': ['AgiBot', 'Beijing Academy of Artificial Intelligence', 'BeingBeyond', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.16012.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#games', '#robotics', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'DualTHOR: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ', 'desc': 'DualTHOR - ÑÑ‚Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ€ÑƒĞºĞ¸Ñ… Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ²ÑƒÑ… Ñ€ÑƒĞº Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ²ÑƒÑ… Ñ€ÑƒĞº Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Enhancing Dual-Arm Robots with DualTHOR Simulator', 'desc': 'The paper introduces DualTHOR, a physics-based simulator designed for training dual-arm humanoid robots. It enhances Vision Language Models (VLMs) by integrating real-world assets and accounting for the complexities of low-level execution. This simulator addresses the limitations of existing platforms by incorporating a task suite for dual-arm collaboration and a contingency mechanism for potential failures. The findings indicate that current VLMs face challenges in dual-arm coordination and robustness, emphasizing the need for advanced simulation tools like DualTHOR to improve performance in real-world tasks.'}, 'zh': {'title': 'DualTHORï¼šæå‡åŒè‡‚æœºå™¨äººä»»åŠ¡èƒ½åŠ›çš„ä»¿çœŸå¹³å°', 'desc': 'DualTHORæ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒåŒè‡‚ç±»äººæœºå™¨äººçš„ç‰©ç†ä»¿çœŸå¹³å°ï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥å¹³å°ç»“åˆäº†çœŸå®ä¸–ç•Œçš„æœºå™¨äººèµ„äº§å’Œä»»åŠ¡å¥—ä»¶ï¼Œæ”¯æŒåŒè‡‚åä½œï¼Œå¹¶å¼•å…¥äº†é€†å‘è¿åŠ¨å­¦æ±‚è§£å™¨ã€‚é€šè¿‡æ¨¡æ‹Ÿä½çº§æ‰§è¡Œä¸­çš„æ½œåœ¨å¤±è´¥ï¼ŒDualTHORèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ ç°å®åœºæ™¯ä¸­çš„å¤æ‚æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒè‡‚åè°ƒå’Œåº”å¯¹ç°å®ç¯å¢ƒä¸­çš„çªå‘æƒ…å†µæ—¶è¡¨ç°ä¸ä½³ï¼Œå› æ­¤ä½¿ç”¨DualTHORè¿›è¡Œè®­ç»ƒè‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18088', 'title': 'RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation', 'url': 'https://huggingface.co/papers/2506.18088', 'abstract': 'RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Simulation-based data synthesis has emerged as a powerful paradigm for enhancing real-world robotic manipulation. However, existing synthetic datasets remain insufficient for robust bimanual manipulation due to two challenges: (1) the lack of an efficient, scalable data generation method for novel tasks, and (2) oversimplified simulation environments that fail to capture real-world complexity. We present RoboTwin 2.0, a scalable simulation framework that enables automated, large-scale generation of diverse and realistic data, along with unified evaluation protocols for dual-arm manipulation. We first construct RoboTwin-OD, a large-scale object library comprising 731 instances across 147 categories, each annotated with semantic and manipulation-relevant labels. Building on this foundation, we develop an expert data synthesis pipeline that combines multimodal large language models (MLLMs) with simulation-in-the-loop refinement to generate task-level execution code automatically. To improve sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization along five axes: clutter, lighting, background, tabletop height and language instructions, thereby enhancing data diversity and policy robustness. We instantiate this framework across 50 dual-arm tasks spanning five robot embodiments, and pre-collect over 100,000 domain-randomized expert trajectories. Empirical results show a 10.9% gain in code generation success and improved generalization to novel real-world scenarios. A VLA model fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%) on unseen scene real-world tasks, while zero-shot models trained solely on our synthetic data achieve a 228% relative gain, highlighting strong generalization without real-world supervision. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation.', 'score': 10, 'issue_id': 4499, 'pub_date': '2025-06-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ½Ñ', 'en': 'June 22', 'zh': '6æœˆ22æ—¥'}, 'hash': '92b73795da81e648', 'authors': ['Tianxing Chen', 'Zanxin Chen', 'Baijun Chen', 'Zijian Cai', 'Yibin Liu', 'Qiwei Liang', 'Zixuan Li', 'Xianliang Lin', 'Yiheng Ge', 'Zhenyu Gu', 'Weiliang Deng', 'Yubin Guo', 'Tian Nian', 'Xuanbing Xie', 'Qiangyu Chen', 'Kailun Su', 'Tianling Xu', 'Guodong Liu', 'Mengkang Hu', 'Huan-ang Gao', 'Kaixuan Wang', 'Zhixuan Liang', 'Yusen Qin', 'Xiaokang Yang', 'Ping Luo', 'Yao Mu'], 'affiliations': ['CSU', 'D-Robotics', 'FDU', 'HKU MMLab', 'HKU-SH ICRC', 'Lumina EAI', 'NEU', 'NJU', 'SJTU ScaleLab', 'SUSTech', 'SYSU', 'SZU', 'Shanghai AI Lab', 'THU', 'TeleAI', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2506.18088.jpg', 'data': {'categories': ['#transfer_learning', '#synthetic', '#dataset', '#benchmark', '#optimization', '#data', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ»Ğ¸Ğ·Ğ½ĞµÑ† Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'RoboTwin 2.0 - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ°Ğ½Ğ´Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² RoboTwin-OD Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. RoboTwin 2.0 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Bimanual Robot Manipulation with RoboTwin 2.0', 'desc': "RoboTwin 2.0 is a new simulation framework designed to improve how robots manipulate objects with both hands. It addresses the challenges of generating diverse and realistic synthetic data by using expert data synthesis and structured domain randomization. This framework creates a large-scale object library and employs advanced language models to automatically generate task execution code. The results show significant improvements in the robots' ability to perform tasks in real-world scenarios, demonstrating enhanced generalization and robustness."}, 'zh': {'title': 'RoboTwin 2.0ï¼šæå‡åŒæ‰‹æœºå™¨äººæ“ä½œçš„ä»¿çœŸæ¡†æ¶', 'desc': 'RoboTwin 2.0 æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŒæ‰‹æœºå™¨äººæ“ä½œä»¿çœŸæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸“å®¶æ•°æ®åˆæˆå’Œç»“æ„åŒ–é¢†åŸŸéšæœºåŒ–ç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„åˆæˆæ•°æ®ï¼Œä»è€Œæé«˜ä»¿çœŸåˆ°ç°å®çš„è½¬ç§»å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰åˆæˆæ•°æ®é›†åœ¨åŒæ‰‹æ“ä½œä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é«˜æ•ˆçš„æ•°æ®ç”Ÿæˆæ–¹æ³•å’Œå¤æ‚çš„ä»¿çœŸç¯å¢ƒã€‚RoboTwin 2.0 ç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œä»¿çœŸå¾ªç¯ä¼˜åŒ–ï¼Œè‡ªåŠ¨ç”Ÿæˆä»»åŠ¡æ‰§è¡Œä»£ç ï¼Œå¹¶é€šè¿‡äº”ä¸ªç»´åº¦çš„é¢†åŸŸéšæœºåŒ–å¢å¼ºæ•°æ®çš„å¤šæ ·æ€§å’Œç­–ç•¥çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä»£ç ç”ŸæˆæˆåŠŸç‡å’Œå¯¹æ–°åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œæ”¯æŒåŒæ‰‹æ“ä½œçš„å¯æ‰©å±•ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18315', 'title': 'Use Property-Based Testing to Bridge LLM Code Generation and Validation', 'url': 'https://huggingface.co/papers/2506.18315', 'abstract': 'A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in complex programming tasks, is a persistent challenge. While traditional Test-Driven Development (TDD) offers a path for code refinement, its efficacy with LLMs is often undermined by the scarcity of high-quality test cases or the pitfalls of automated test generation, including biased tests or inaccurate output predictions that can misdirect the correction process. This paper introduces Property-Generated Solver, a novel framework that leverages Property-Based Testing (PBT) to validate high-level program properties or invariants, instead of relying on specific input-output examples. These properties are often simpler to define and verify than directly predicting exhaustive test oracles, breaking the "cycle of self-deception" where tests might share flaws with the code they are meant to validate. Property-Generated Solver employs two collaborative LLM-based agents: a Generator dedicated to code generation and iterative refinement, and a Tester that manages the PBT life-cycle and formulate semantically rich feedback from property violations. The resulting comprehensive and actionable feedback then guides the Generator in its refinement efforts. By establishing PBT as the core validation engine within this iterative, closed-loop paradigm, Property-Generated Solver provides a robust mechanism for steering LLMs towards more correct and generalizable code. Extensive experimental results on multiple code generation benchmarks demonstrate that Property-Generated Solver achieves substantial pass@1 improvements, ranging from 23.1% to 37.3% relative gains over established TDD methods.', 'score': 9, 'issue_id': 4499, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'b53df58acbb9720b', 'authors': ['Lehan He', 'Zeren Chen', 'Zhe Zhang', 'Jing Shao', 'Xiang Gao', 'Lu Sheng'], 'affiliations': ['School of Software, Beihang University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.18315.jpg', 'data': {'categories': ['#plp', '#benchmark', '#optimization', '#training', '#agents'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Property-Based Testing Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Property-Generated Solver Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Property-Based Testing (PBT) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ğ²Ğ¾Ğ´Ğ°-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ”Ğ²Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM - Generator Ğ¸ Tester - ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (TDD).'}, 'en': {'title': 'Enhancing Code Generation with Property-Based Testing and LLM Collaboration', 'desc': 'This paper presents a new framework called Property-Generated Solver that enhances the correctness and generalization of code generated by Large Language Models (LLMs). It utilizes Property-Based Testing (PBT) to validate high-level program properties instead of relying solely on traditional test cases, which can be flawed or biased. The framework consists of two collaborative LLM agents: a Generator for creating and refining code, and a Tester that oversees the PBT process and provides feedback based on property violations. Experimental results show that this approach significantly improves the accuracy of code generation compared to conventional Test-Driven Development methods.'}, 'zh': {'title': 'åŸºäºå±æ€§çš„æµ‹è¯•æå‡ä»£ç ç”Ÿæˆçš„æ­£ç¡®æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºå±æ€§çš„æµ‹è¯•ï¼ˆPBTï¼‰å’Œåä½œçš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æ¥æé«˜ä»£ç ç”Ÿæˆçš„æ­£ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„æµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰åœ¨å¤„ç†LLMç”Ÿæˆçš„ä»£ç æ—¶å¸¸å¸¸é¢ä¸´é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ç¨€ç¼ºçš„é—®é¢˜ï¼Œè€ŒPBTé€šè¿‡éªŒè¯é«˜å±‚ç¨‹åºå±æ€§æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªåä½œçš„LLMä»£ç†ï¼šä¸€ä¸ªç”¨äºä»£ç ç”Ÿæˆå’Œè¿­ä»£æ”¹è¿›ï¼Œå¦ä¸€ä¸ªè´Ÿè´£ç®¡ç†PBTç”Ÿå‘½å‘¨æœŸå¹¶æä¾›æœ‰æ„ä¹‰çš„åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»£ç ç”ŸæˆåŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†é€šè¿‡ç‡ï¼Œä¼˜äºä¼ ç»Ÿçš„TDDæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20544', 'title': 'When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs', 'url': 'https://huggingface.co/papers/2506.20544', 'abstract': 'The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages.', 'score': 6, 'issue_id': 4497, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '8c4af7dcfe82a334', 'authors': ['Ammar Khairi', "Daniel D'souza", 'Ye Shen', 'Julia Kreutzer', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.20544.jpg', 'data': {'categories': ['#multilingual', '#inference', '#low_resource'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡ĞµÑ‚Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Multilingual Performance in Large Language Models', 'desc': 'This paper explores new methods for sampling and selecting outputs from large language models (LLMs) during inference to improve their performance across multiple languages and tasks. The authors highlight that traditional strategies often focus on English and specific domains, which limits their effectiveness in diverse settings. They propose innovative sampling techniques that adjust for temperature variations and selection methods tailored to different languages and tasks. The results demonstrate significant improvements in win-rates, showcasing the importance of adapting inference strategies to enhance multilingual and multi-task capabilities of LLMs.'}, 'zh': {'title': 'æå‡å¤šè¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„æ–°ç­–ç•¥', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨å¹¶æå‡ºäº†æ–°çš„é‡‡æ ·å’Œé€‰æ‹©ç­–ç•¥ï¼Œä»¥å¢å¼ºå¤šè¯­è¨€å’Œå¤šä»»åŠ¡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶é—´è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºæ¸©åº¦å˜åŒ–çš„é‡‡æ ·ç­–ç•¥å’Œé€‰æ‹©ç­–ç•¥å¿…é¡»é€‚åº”ä¸åŒé¢†åŸŸå’Œè¯­è¨€ç¯å¢ƒã€‚é€šè¿‡è¯„ä¼°ç°æœ‰çš„é€‰æ‹©æ–¹æ³•ï¼Œæˆ‘ä»¬æ­ç¤ºäº†åœ¨è‹±è¯­ä¸­æœ‰æ•ˆçš„ç­–ç•¥å¾€å¾€æ— æ³•è·¨è¯­è¨€æ¨å¹¿ã€‚æˆ‘ä»¬æå‡ºçš„åˆ›æ–°æ–¹æ³•åœ¨å¤šè¯­è¨€å’Œå¤šä»»åŠ¡æ¨ç†åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨m-ArenaHard-v2.0åŸºå‡†æµ‹è¯•ä¸­ï¼Œ8Bæ¨¡å‹çš„èƒœç‡å¹³å‡æé«˜äº†6.8ä¸ªç™¾åˆ†ç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20452', 'title': 'HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based\n  Diffusion Sampling', 'url': 'https://huggingface.co/papers/2506.20452', 'abstract': "HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWave's performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications.", 'score': 6, 'issue_id': 4499, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '470af96a76c911d8', 'authors': ['Tobias Vontobel', 'Seyedmorteza Sadat', 'Farnood Salehi', 'Romann M. Weber'], 'affiliations': ['Disney Research Studios, Switzerland', 'ETH Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2506.20452.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'HiWave: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'HiWave - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ DDIM Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚Ğ¾Ğ². HiWave Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot.'}, 'en': {'title': 'HiWave: Elevating Ultra-High-Resolution Image Synthesis with Pretrained Diffusion Models', 'desc': 'HiWave is a novel approach for enhancing ultra-high-resolution image synthesis using pretrained diffusion models without the need for retraining. It employs a two-stage pipeline that first generates a base image and then applies DDIM inversion and a wavelet-based detail enhancement technique. This method improves visual fidelity by preserving global coherence and enriching fine details, effectively reducing common artifacts like object duplication. Evaluations show that HiWave outperforms existing methods, achieving superior perceptual quality in image synthesis.'}, 'zh': {'title': 'HiWaveï¼šè¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„æ–°çªç ´', 'desc': 'HiWaveæ˜¯ä¸€ç§å¢å¼ºè¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„æŠ€æœ¯ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„æµç¨‹æ¥å®ç°ã€‚é¦–å…ˆï¼Œå®ƒä»é¢„è®­ç»ƒæ¨¡å‹ç”ŸæˆåŸºç¡€å›¾åƒï¼Œç„¶åè¿›è¡ŒåŸºäºæ³¢å½¢çš„å°å—DDIMåæ¼”æ­¥éª¤å’Œç»†èŠ‚å¢å¼ºã€‚è¯¥æ–¹æ³•æœ‰æ•ˆæé«˜äº†è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§ï¼Œå‡å°‘äº†å¸¸è§çš„è§†è§‰ä¼ªå½±ã€‚é€šè¿‡å¯¹Stable Diffusion XLçš„å¹¿æ³›è¯„ä¼°ï¼ŒHiWaveåœ¨ç”¨æˆ·ç ”ç©¶ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡80%çš„æ¯”è¾ƒä¸­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20495', 'title': 'ReCode: Updating Code API Knowledge with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.20495', 'abstract': "ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.", 'score': 5, 'issue_id': 4500, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '22cd91ad69753ea2', 'authors': ['Haoze Wu', 'Yunzhi Yao', 'Wenhao Yu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Tencent AI, Seattle Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.20495.jpg', 'data': {'categories': ['#rl', '#training', '#rlhf', '#optimization', '#dataset', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ReCode: ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ API Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ReCode Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ API Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ReCode Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ¾Ğ² Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ API. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 2000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€ÑĞ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° ÑÑ‚Ñ€Ğ¾Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ´Ğ° Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ API, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… CodeUpdateArena.'}, 'en': {'title': 'Adapting Code Generation with ReCode: Reinforcement Learning for API Updates', 'desc': "ReCode is a rule-based reinforcement learning framework designed to improve large language models' (LLMs) ability to adapt to changes in external library APIs while maintaining their general code generation skills. The framework addresses the challenge that LLMs face due to outdated API knowledge, which hinders their performance in dynamic coding environments. By creating a dataset of around 2,000 entries for training and implementing a modified string similarity metric for evaluating code, ReCode effectively enhances the LLMs' adaptation capabilities. Experimental results show that ReCode significantly improves code generation performance in scenarios with frequent API updates, outperforming traditional supervised fine-tuning methods."}, 'zh': {'title': 'ReCodeï¼šæå‡ä»£ç ç”Ÿæˆä¸APIé€‚åº”èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'ReCodeæ˜¯ä¸€ä¸ªåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹APIæ›´æ–°çš„é€‚åº”èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶ä»£ç ç”Ÿæˆçš„é€šç”¨æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«çº¦2000ä¸ªæ•°æ®æ¡ç›®çš„æ•°æ®é›†ï¼Œè®­ç»ƒLLMsè¿›è¡Œç‰ˆæœ¬è¿ç§»ï¼Œä»¥åº”å¯¹åŠ¨æ€ç¯å¢ƒä¸­çš„APIå˜åŒ–ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ä¿®æ”¹è¿‡çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦åº¦é‡ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ï¼Œä»¥è¯„ä¼°ä»£ç çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReCodeæ˜¾è‘—æå‡äº†LLMsåœ¨åŠ¨æ€APIåœºæ™¯ä¸‹çš„ä»£ç ç”Ÿæˆæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æœªè§è¿‡çš„CodeUpdateArenaä»»åŠ¡ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18674', 'title': 'Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?', 'url': 'https://huggingface.co/papers/2506.18674', 'abstract': 'Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.', 'score': 5, 'issue_id': 4497, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '8daec89854af9338', 'authors': ['Raquel Ferrando', 'Javier Conde', 'Gonzalo MartÃ­nez', 'Pedro Reviriego'], 'affiliations': ['ETSI de TelecomunicaciÃ³n Universidad PolitÃ©cnica de Madrid 28040 Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2506.18674.jpg', 'data': {'categories': ['#training', '#optimization', '#data'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5-10%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. Ğ­Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM).'}, 'en': {'title': 'Optimize Tokenizers, Save Energy!', 'desc': 'This paper explores the optimization of tokenizers specifically for chatbot conversations to enhance efficiency and reduce costs. It highlights that the computational expense of Large Language Models (LLMs) is closely tied to the number of tokens processed, making tokenizer performance crucial. By redesigning vocabularies based on a corpus of chatbot dialogues, the study demonstrates that conversation-optimized tokenizers can decrease token counts by 5% to 10%. Importantly, this optimization has little to no negative effect on the performance of the original training corpus, suggesting a dual benefit of energy savings and maintained efficiency.'}, 'zh': {'title': 'ä¼˜åŒ–åˆ†è¯å™¨ï¼ŒèŠ‚çœèƒ½æºä¸æˆæœ¬', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ä¸ºèŠå¤©æœºå™¨äººå¯¹è¯ä¼˜åŒ–åˆ†è¯å™¨çš„æ½œåœ¨å¥½å¤„ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œè®¡ç®—å’Œèƒ½æºæˆæœ¬æ€¥å‰§ä¸Šå‡ï¼Œè€Œåˆ†è¯å™¨åœ¨æ¨¡å‹æ•ˆç‡ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹èŠå¤©å¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨èƒ½å¤Ÿæ˜¾è‘—å‡å°‘å¯¹è¯ä¸­çš„æ ‡è®°æ•°é‡ï¼Œä»è€ŒèŠ‚çœ5%åˆ°10%çš„èƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶å¯¹åŸå§‹è®­ç»ƒè¯­æ–™çš„æ ‡è®°åŒ–æ•ˆç‡å½±å“è¾ƒå°æˆ–ç•¥æœ‰æ­£é¢æ•ˆæœã€‚é€šè¿‡é‡æ–°è®¾è®¡åˆ†è¯å™¨çš„è¯æ±‡ï¼Œæœ¬æ–‡å±•ç¤ºäº†åœ¨èŠå¤©æœºå™¨äººé¢†åŸŸä¼˜åŒ–åˆ†è¯å™¨çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20480', 'title': 'GPTailor: Large Language Model Pruning Through Layer Cutting and\n  Stitching', 'url': 'https://huggingface.co/papers/2506.20480', 'abstract': "A new strategy merges layers from fine-tuned model variants to compress large language models with minimal performance loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in deployment and inference. While structured pruning of model parameters offers a promising way to reduce computational costs at deployment time, current methods primarily focus on single model pruning. In this work, we develop a novel strategy to compress models by strategically combining or merging layers from finetuned model variants, which preserves the original model's abilities by aggregating capabilities accentuated in different finetunes. We pose the optimal tailoring of these LLMs as a zero-order optimization problem, adopting a search space that supports three different operations: (1) Layer removal, (2) Layer selection from different candidate models, and (3) Layer merging. Our experiments demonstrate that this approach leads to competitive model pruning, for example, for the Llama2-13B model families, our compressed models maintain approximately 97.3\\% of the original performance while removing sim25% of parameters, significantly outperforming previous state-of-the-art methods. The code is available at https://github.com/Guinan-Su/auto-merge-llm.", 'score': 3, 'issue_id': 4513, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '3d5c982e57eed2a9', 'authors': ['Guinan Su', 'Li Shen', 'Lu Yin', 'Shiwei Liu', 'Yanwu Yang', 'Jonas Geiping'], 'affiliations': ['ELLIS Institute TÃ¼bingen', 'Max Planck Institute for Intelligent Systems', 'Sun Yat-sen University', 'TÃ¼bingen AI Center', 'University of Oxford', 'University of Surrey', 'University of TÃ¼bingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.20480.jpg', 'data': {'categories': ['#inference', '#small_models', '#optimization', '#architecture', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ², Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞ»Ğ¾ĞµĞ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama2-13B ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ 97.3% Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 25% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Efficient Compression of Language Models through Layer Merging', 'desc': "This paper presents a new method for compressing large language models (LLMs) by merging layers from different fine-tuned versions of the models. The approach aims to reduce the model size while maintaining high performance, achieving about 97.3% of the original model's capabilities with a 25% reduction in parameters. It formulates the compression process as a zero-order optimization problem, allowing for layer removal, selection, and merging from various candidate models. The results show that this strategy outperforms existing model pruning techniques, making it a significant advancement in efficient LLM deployment."}, 'zh': {'title': 'æ™ºèƒ½å‹ç¼©ï¼Œæ€§èƒ½ä¸å‡ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ç­–ç•¥ï¼Œé€šè¿‡åˆå¹¶å¾®è°ƒæ¨¡å‹çš„å±‚æ¥å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘æ€§èƒ½æŸå¤±ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åºå¤§çš„æ¨¡å‹è§„æ¨¡åœ¨éƒ¨ç½²å’Œæ¨ç†æ—¶å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¼˜åŒ–å±‚çš„é€‰æ‹©å’Œåˆå¹¶ï¼Œä¿æŒäº†åŸå§‹æ¨¡å‹çš„èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†å‚æ•°æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‹ç¼©æ¨¡å‹æ—¶èƒ½å¤Ÿä¿æŒçº¦97.3%çš„åŸå§‹æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘25%çš„å‚æ•°ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20331', 'title': 'Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content', 'url': 'https://huggingface.co/papers/2506.20331', 'abstract': 'A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies.', 'score': 3, 'issue_id': 4501, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '3c4ae1c9ac9b2325', 'authors': ['Rian Touchent', 'Nathan Godey', 'Eric de la Clergerie'], 'affiliations': ['Sorbonne UniversitÃ© INRIA Paris'], 'pdf_title_img': 'assets/pdf/title_img/2506.20331.jpg', 'data': {'categories': ['#dataset', '#open_source', '#science', '#training', '#data', '#healthcare'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Biomed-Enriched: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Biomed-Enriched, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ PubMed Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ 400 Ñ‚Ñ‹ÑÑÑ‡ Ğ°Ğ±Ğ·Ğ°Ñ†ĞµĞ², Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ Ğ¸Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ, Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (ĞœĞ¯Ğœ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ğ²ĞµÑÑŒ ĞºĞ¾Ñ€Ğ¿ÑƒÑ PMC-OA. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ°Ğ±Ğ·Ğ°Ñ†ĞµĞ² Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ»ÑƒÑ‡Ğ°ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Enhancing Clinical NLP with Biomed-Enriched Dataset', 'desc': 'The paper presents Biomed-Enriched, a biomedical text dataset derived from PubMed using a two-stage annotation process. Initially, a large language model annotates 400,000 paragraphs, categorizing them by type, domain, and educational quality. This annotated data is then used to fine-tune a smaller language model, which helps in labeling the entire PMC-OA corpus. The resulting dataset, which includes high-quality clinical case paragraphs, enhances the efficiency of pretraining and improves performance in clinical NLP tasks.'}, 'zh': {'title': 'ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬æ•°æ®é›†çš„é«˜æ•ˆæ„å»ºä¸åº”ç”¨', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºBiomed-Enrichedçš„ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¥æºäºPubMedï¼Œå¹¶é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„æ³¨é‡Šè¿‡ç¨‹ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹40ä¸‡æ®µPubMedç§‘å­¦æ–‡ç« è¿›è¡Œæ³¨é‡Šï¼Œè¯„ä¼°å…¶ç±»å‹ã€é¢†åŸŸå’Œæ•™è‚²è´¨é‡ã€‚é€šè¿‡å¯¹å°å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒï¼Œè¿›ä¸€æ­¥ä¼ æ’­è¿™äº›æ ‡ç­¾ï¼Œä»è€Œæå–å‡ºé«˜è´¨é‡çš„ä¸´åºŠæ¡ˆä¾‹æ®µè½ã€‚è¯¥æ•°æ®é›†ä¸ºç”Ÿç‰©åŒ»å­¦å’Œä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†æä¾›äº†ä¸€ä¸ªå¼€æ”¾çš„èµ„æºï¼Œèƒ½å¤Ÿæé«˜é¢„è®­ç»ƒçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19502', 'title': 'MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications', 'url': 'https://huggingface.co/papers/2506.19502', 'abstract': "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.  \t\t\t\t\tAI-generated summary \t\t\t\t Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.", 'score': 2, 'issue_id': 4494, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '85d844ff6061cc93', 'authors': ['Aleksandr Algazinov', 'Matt Laing', 'Paul Laban'], 'affiliations': ['Dept. of Comp. Sci. & Tech. Tsinghua University Beijing, China', 'Dept. of Psych. & Cog. Sci. Tsinghua University Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.19502.jpg', 'data': {'categories': ['#healthcare', '#agents', '#multimodal', '#ethics', '#open_source'], 'emoji': 'â™¿', 'ru': {'title': 'MATE: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'MATE - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´Ñ‹ Ğ¸Ğ½Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸. MATE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² API Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ModCon-Task-Identifier - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Empowering Accessibility Through Intelligent Data Conversion', 'desc': 'MATE is a multimodal accessibility multi-agent system designed to convert data into formats that are understandable for users with disabilities. It addresses the limitations of existing multi-agent systems by providing customizable solutions that adapt to individual user needs. The system can perform tasks like converting images to audio descriptions, making digital content more accessible. Additionally, MATE integrates with institutional technologies and ensures user privacy by running locally, while its ModCon-Task-Identifier model excels in identifying specific modality conversion tasks.'}, 'zh': {'title': 'MATEï¼šä¸ºæ¯ä¸ªäººæä¾›æ— éšœç¢çš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'MATEæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ— éšœç¢å¤šä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·éœ€æ±‚å°†æ•°æ®è½¬æ¢ä¸ºå¯ç†è§£çš„æ ¼å¼ï¼Œä»¥æ”¯æŒå„ç§æ®‹ç–¾äººå£«ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ‰§è¡Œæ¨¡æ€è½¬æ¢ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ä¸æ•°å­—ç¯å¢ƒäº’åŠ¨ï¼Œä¾‹å¦‚å°†å›¾åƒè½¬æ¢ä¸ºéŸ³é¢‘æè¿°ï¼Œä»¥æ»¡è¶³è§†è§‰éšœç¢è€…çš„éœ€æ±‚ã€‚MATEå…·æœ‰çµæ´»æ€§ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œç¡¬ä»¶ï¼Œç¡®ä¿èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç”¨æˆ·éœ€æ±‚ï¼Œå¹¶ä¿æŠ¤æ•æ„Ÿä¿¡æ¯çš„éšç§å’Œå®‰å…¨ã€‚é€šè¿‡ä¸æœºæ„æŠ€æœ¯çš„æœ‰æ•ˆé›†æˆï¼ŒMATEèƒ½å¤Ÿæä¾›å®æ—¶çš„ç”¨æˆ·æ”¯æŒï¼Œæå‡æ— éšœç¢æœåŠ¡çš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18403', 'title': 'The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs', 'url': 'https://huggingface.co/papers/2506.18403', 'abstract': 'The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.  \t\t\t\t\tAI-generated summary \t\t\t\t The effectiveness of AI debugging follows a predictable exponential decay pattern; most models lose 60-80% of their debugging capability within just 2-3 attempts, despite iterative debugging being a critical capability for practical code generation systems. We introduce the Debugging Decay Index (DDI), a mathematical framework that quantifies when debugging becomes ineffective and predicts intervention points. Our strategic fresh start approach shifts from exploitation to exploration at strategic points in the debugging process, demonstrating that well-timed interventions can rescue the effectiveness of debugging. DDI reveals a fundamental limitation in current AI debugging and provides the first quantitative framework for optimising iterative code generation strategies.', 'score': 2, 'issue_id': 4494, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '30aa788d94afe45d', 'authors': ['Muntasir Adnan', 'Carlos C. N. Kuhn'], 'affiliations': ['Open Source Institute, University of Canberra, Bruce, Canberra, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2506.18403.jpg', 'data': {'categories': ['#optimization', '#training', '#math'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ˜Ğ˜: Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ—Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ ĞÑ‚Ğ»Ğ°Ğ´ĞºĞ¸ (DDI), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ, ÑĞ»ĞµĞ´ÑƒÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñƒ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ. DDI Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ²ĞµĞ¶ĞµĞ³Ğ¾ ÑÑ‚Ğ°Ñ€Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ¾ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Reviving AI Debugging with Strategic Interventions', 'desc': 'The Debugging Decay Index (DDI) is a new method that measures how quickly AI debugging abilities decline over time. It shows that most AI models lose a significant portion of their debugging skills after just a few attempts, which is a major issue for systems that generate code. By using DDI, developers can identify the best moments to intervene and improve the debugging process, shifting from refining existing solutions to exploring new ones. This approach not only highlights a key limitation in current AI debugging methods but also offers a way to enhance the effectiveness of iterative code generation.'}, 'zh': {'title': 'ä¼˜åŒ–AIè°ƒè¯•çš„å…³é”®ï¼šè°ƒè¯•è¡°å‡æŒ‡æ•°', 'desc': 'è°ƒè¯•è¡°å‡æŒ‡æ•°ï¼ˆDDIï¼‰é‡åŒ–å¹¶ä¼˜åŒ–äº†è¿­ä»£AIè°ƒè¯•çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡é¢„æµ‹å¹²é¢„ç‚¹æ¥æ¢å¤å’Œå¢å¼ºè°ƒè¯•èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒAIè°ƒè¯•çš„æœ‰æ•ˆæ€§éµå¾ªå¯é¢„æµ‹çš„æŒ‡æ•°è¡°å‡æ¨¡å¼ï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨ä»…ä»…2-3æ¬¡å°è¯•åå°±ä¼šå¤±å»60-80%çš„è°ƒè¯•èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºçš„DDIæ¡†æ¶å¯ä»¥é‡åŒ–è°ƒè¯•å¤±æ•ˆçš„æ—¶æœºï¼Œå¹¶é¢„æµ‹å¹²é¢„ç‚¹ï¼Œä»è€Œåœ¨è°ƒè¯•è¿‡ç¨‹ä¸­å®ç°ä»åˆ©ç”¨åˆ°æ¢ç´¢çš„æˆ˜ç•¥è½¬å˜ã€‚DDIæ­ç¤ºäº†å½“å‰AIè°ƒè¯•çš„åŸºæœ¬å±€é™æ€§ï¼Œå¹¶æä¾›äº†ä¼˜åŒ–è¿­ä»£ä»£ç ç”Ÿæˆç­–ç•¥çš„é¦–ä¸ªå®šé‡æ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19143', 'title': 'Thought Anchors: Which LLM Reasoning Steps Matter?', 'url': 'https://huggingface.co/papers/2506.19143', 'abstract': "Sentence-level attribution methods uncover critical thought anchors in large language models' reasoning processes, enhancing interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) a black-box method measuring each sentence's counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with a different meaning; (2) a white-box method of aggregating attention patterns between pairs of sentences, which identified ``broadcasting'' sentences that receive disproportionate attention from all future sentences via ``receiver'' attention heads; (3) a causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentence's tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (www.thought-anchors.com) for visualizing the outputs of our methods, and present a case study showing converging patterns across methods that map how a model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for a deeper understanding of reasoning models.", 'score': 1, 'issue_id': 4513, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '2634ea0474335d4a', 'authors': ['Paul C. Bogdan', 'Uzay Macar', 'Neel Nanda', 'Arthur Conmy'], 'affiliations': ['Aiphabet', 'Duke University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19143.jpg', 'data': {'categories': ['#inference', '#reasoning', '#interpretability', '#data', '#open_source', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜: ÑĞºĞ¾Ñ€Ñ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞœĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ñ‡ĞµÑ€Ğ½Ğ¾ÑÑ‰Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ 'ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ¼Ñ‹ÑĞ»Ğ¸' - ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Uncovering Thought Anchors in Language Model Reasoning', 'desc': "This paper explores how to better understand the reasoning processes of large language models by focusing on sentence-level analysis. It introduces three methods for attributing importance to sentences in the model's reasoning chain, which helps identify key 'thought anchors' that significantly influence the model's outputs. The methods include a black-box approach for counterfactual importance, a white-box method analyzing attention patterns, and a causal attribution technique to assess logical connections. By providing an open-source tool for visualization, the authors demonstrate that these sentence-level insights can enhance interpretability and reveal how models perform complex reasoning tasks."}, 'zh': {'title': 'å¥å­çº§åˆ†æï¼šæ­ç¤ºè¯­è¨€æ¨¡å‹æ¨ç†çš„æ€ç»´é”šç‚¹', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¥å­çº§å½’å› æ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ï¼Œæå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåˆ†ææ¨ç†è½¨è¿¹å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£æ¨¡å‹çš„æ€ç»´è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§äº’è¡¥çš„å½’å› æ–¹æ³•ï¼ŒåŒ…æ‹¬é»‘ç®±æ–¹æ³•ã€ç™½ç®±æ–¹æ³•å’Œå› æœå½’å› æ–¹æ³•ï¼Œæ­ç¤ºäº†æ€ç»´é”šç‚¹çš„å­˜åœ¨ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å¯è§†åŒ–æ¨¡å‹çš„å¤šæ­¥æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å¼ºè°ƒäº†å¥å­çº§åˆ†æåœ¨ç†è§£æ¨ç†æ¨¡å‹ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03569', 'title': 'MiMo-VL Technical Report', 'url': 'https://huggingface.co/papers/2506.03569', 'abstract': 'We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.', 'score': 56, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'cb568276c7e799cb', 'authors': ['Xiaomi LLM-Core Team', ':', 'Zihao Yue', 'Zhenru Lin', 'Yifan Song', 'Weikun Wang', 'Shuhuai Ren', 'Shuhao Gu', 'Shicheng Li', 'Peidian Li', 'Liang Zhao', 'Lei Li', 'Kainan Bao', 'Hao Tian', 'Hailin Zhang', 'Gang Wang', 'Dawei Zhu', 'Cici', 'Chenhong He', 'Bowen Ye', 'Bowen Shen', 'Zihan Zhang', 'Zihan Jiang', 'Zhixian Zheng', 'Zhichao Song', 'Zhenbo Luo', 'Yue Yu', 'Yudong Wang', 'Yuanyuan Tian', 'Yu Tu', 'Yihan Yan', 'Yi Huang', 'Xu Wang', 'Xinzhe Xu', 'Xingchen Song', 'Xing Zhang', 'Xing Yong', 'Xin Zhang', 'Xiangwei Deng', 'Wenyu Yang', 'Wenhan Ma', 'Weiwei Lv', 'Weiji Zhuang', 'Wei Liu', 'Sirui Deng', 'Shuo Liu', 'Shimao Chen', 'Shihua Yu', 'Shaohui Liu', 'Shande Wang', 'Rui Ma', 'Qiantong Wang', 'Peng Wang', 'Nuo Chen', 'Menghang Zhu', 'Kangyang Zhou', 'Kang Zhou', 'Kai Fang', 'Jun Shi', 'Jinhao Dong', 'Jiebao Xiao', 'Jiaming Xu', 'Huaqiu Liu', 'Hongshen Xu', 'Heng Qu', 'Haochen Zhao', 'Hanglong Lv', 'Guoan Wang', 'Duo Zhang', 'Dong Zhang', 'Di Zhang', 'Chong Ma', 'Chang Liu', 'Can Cai', 'Bingquan Xia'], 'affiliations': ['Xiaomi'], 'pdf_title_img': 'assets/pdf/title_img/2506.03569.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#multimodal', '#rlhf', '#benchmark', '#dataset', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜: MiMo-VL ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MiMo-VL-7B-SFT Ğ¸ MiMo-VL-7B-RL, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ MiMo-VL-7B-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Qwen2.5-VL-7B Ğ² 35 Ğ¸Ğ· 40 Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 59.4 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OlympiadBench. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 2.4 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (MORL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ² ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Vision-Language Models with MiMo-VL', 'desc': 'The paper introduces two advanced vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning tasks. MiMo-VL-7B-RL demonstrates superior performance, outperforming other models on a majority of evaluated tasks and achieving high scores on benchmark datasets. The training methodology involves a four-stage pre-training process using a massive dataset and incorporates Mixed On-policy Reinforcement Learning to enhance model performance through diverse reward signals. Additionally, the authors emphasize the significance of high-quality reasoning data and provide a comprehensive evaluation suite to facilitate reproducibility in future research.'}, 'zh': {'title': 'å¼€åˆ›è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–°æ ‡å‡†', 'desc': 'æˆ‘ä»¬å¼€æºäº†MiMo-VL-7B-SFTå’ŒMiMo-VL-7B-RLï¼Œè¿™ä¸¤ä¸ªå¼ºå¤§çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ä¸€èˆ¬è§†è§‰ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚MiMo-VL-7B-RLåœ¨40ä¸ªè¯„ä¼°ä»»åŠ¡ä¸­æœ‰35ä¸ªè¶…è¶Šäº†Qwen2.5-VL-7Bï¼Œå¹¶åœ¨OlympiadBenchä¸Šå¾—åˆ†59.4ï¼Œè¶…è¿‡äº†å‚æ•°é«˜è¾¾78Bçš„æ¨¡å‹ã€‚åœ¨GUIå®šä½åº”ç”¨ä¸­ï¼Œå®ƒåœ¨OSWorld-Gä¸Šä»¥56.1çš„åˆ†æ•°è®¾å®šäº†æ–°æ ‡å‡†ï¼Œç”šè‡³è¶…è¶Šäº†ä¸“é—¨æ¨¡å‹UI-TARSã€‚æˆ‘ä»¬çš„è®­ç»ƒç»“åˆäº†å››é˜¶æ®µçš„é¢„è®­ç»ƒï¼ˆ24ä¸‡äº¿ä¸ªæ ‡è®°ï¼‰å’Œæ··åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆMORLï¼‰ï¼Œå¹¶å¼ºè°ƒäº†åœ¨é¢„è®­ç»ƒé˜¶æ®µèå…¥é«˜è´¨é‡æ¨ç†æ•°æ®å’Œé•¿é“¾æ€ç»´çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04207', 'title': 'Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04207', 'abstract': 'Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.', 'score': 39, 'issue_id': 4135, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '61521f9ed974c930', 'authors': ['Shuang Chen', 'Yue Guo', 'Zhaochen Su', 'Yafu Li', 'Yulun Wu', 'Jiacheng Chen', 'Jiayu Chen', 'Weijie Wang', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04207.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#training', '#benchmark', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ MLLM: Ğ¾Ñ‚ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ RL', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑ‚Ğ°Ğ³Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¼ GRPO Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ RL. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ReVisual-R1, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆĞ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… 7B MLLM Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Unlocking Reasoning in MLLMs with Smart Training Strategies', 'desc': 'This paper explores how to improve reasoning in Multimodal Large Language Models (MLLMs) by analyzing their training processes. It identifies that starting with well-chosen text data can significantly boost reasoning capabilities, even before applying multimodal reinforcement learning (RL). The authors also highlight that traditional gradient-based methods in multimodal RL can lead to stagnation, negatively impacting training effectiveness. By implementing a staged training approach that combines text-only RL after multimodal RL, they introduce ReVisual-R1, which sets new performance records on various complex benchmarks.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè‰¯å¥½çš„å†·å¯åŠ¨åˆå§‹åŒ–å¯¹äºå¢å¼ºMLLMçš„æ¨ç†è‡³å…³é‡è¦ï¼Œå•ç‹¬ä½¿ç”¨ç²¾å¿ƒé€‰æ‹©çš„æ–‡æœ¬æ•°æ®å³å¯è¶…è¶Šè®¸å¤šè¿‘æœŸçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚æ ‡å‡†çš„GRPOåœ¨å¤šæ¨¡æ€RLä¸­å­˜åœ¨æ¢¯åº¦åœæ»çš„é—®é¢˜ï¼Œå½±å“äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚é€šè¿‡åˆ†é˜¶æ®µçš„è®­ç»ƒæ–¹æ³•ï¼Œç»“åˆæ–‡æœ¬å’Œå¤šæ¨¡æ€RLï¼Œæå‡ºäº†ReVisual-R1ï¼Œè¾¾åˆ°äº†å¼€æº7B MLLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ–°çŠ¶æ€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04089', 'title': 'AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment', 'url': 'https://huggingface.co/papers/2506.04089', 'abstract': 'AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.  \t\t\t\t\tAI-generated summary \t\t\t\t As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.', 'score': 39, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'e5facb42d11447ff', 'authors': ['Anastasiia Ivanova', 'Eva Bakaeva', 'Zoya Volovikova', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['AIRI, Moscow, Russia', 'LMU, Munich, Germany', 'MIPT, Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.04089.jpg', 'data': {'categories': ['#interpretability', '#agents', '#data', '#dataset', '#alignment'], 'emoji': 'ğŸ³', 'ru': {'title': 'AmbiK: ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'AmbiK - ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºÑƒÑ…Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1000 Ğ¿Ğ°Ñ€ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. AmbiK Ğ±Ñ‹Ğ» ÑĞ¾Ğ±Ñ€Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'AmbiK: A Unified Benchmark for Ambiguity Detection in Kitchen Robotics', 'desc': 'The paper introduces AmbiK, a new dataset designed to help researchers evaluate how well different methods can detect ambiguous instructions for kitchen robots. AmbiK contains 1000 pairs of ambiguous and clear tasks, categorized by types of ambiguity such as human preferences and safety concerns. This dataset is unique because it allows for standardized testing of various ambiguity detection techniques, which have previously been difficult to compare due to differing datasets. By providing a common benchmark, AmbiK aims to advance the development of more effective language models in handling real-world instructions.'}, 'zh': {'title': 'ç»Ÿä¸€æ¯”è¾ƒæ¨¡ç³Šæ€§æ£€æµ‹æ–¹æ³•çš„AmbiKæ•°æ®é›†', 'desc': 'AmbiKæ˜¯ä¸€ä¸ªé’ˆå¯¹å¨æˆ¿æœºå™¨äººæ¨¡ç³ŠæŒ‡ä»¤çš„æ–‡æœ¬æ•°æ®é›†ï¼Œæ—¨åœ¨ç»Ÿä¸€æ¯”è¾ƒæ¨¡ç³Šæ€§æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ•°æ®é›†åŒ…å«1000å¯¹æ¨¡ç³Šä»»åŠ¡åŠå…¶æ˜ç¡®å¯¹åº”ä»»åŠ¡ï¼Œæ¶µç›–äººç±»åå¥½ã€å¸¸è¯†çŸ¥è¯†å’Œå®‰å…¨ç­‰æ¨¡ç³Šæ€§ç±»å‹ã€‚AmbiKç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ååŠ©æ”¶é›†ï¼Œå¹¶ç»è¿‡äººå·¥éªŒè¯ï¼Œæä¾›ç¯å¢ƒæè¿°ã€æ¾„æ¸…é—®é¢˜åŠç­”æ¡ˆã€ç”¨æˆ·æ„å›¾å’Œä»»åŠ¡è®¡åˆ’ç­‰ä¿¡æ¯ã€‚æˆ‘ä»¬å¸Œæœ›AmbiKèƒ½å¸®åŠ©ç ”ç©¶äººå‘˜è¿›è¡Œæ¨¡ç³Šæ€§æ£€æµ‹æ–¹æ³•çš„ç»Ÿä¸€æ¯”è¾ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.16968', 'title': 'CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark', 'url': 'https://huggingface.co/papers/2505.16968', 'abstract': 'CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA leftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}, with code at https://github.com/GustavoStahl/CASS{blue{GitHub}}.', 'score': 35, 'issue_id': 4139, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': 'a069288c85761286', 'authors': ['Ahmed Heakl', 'Sarim Hashmi', 'Gustavo Bertolo Stahl', 'Seung Hun Eddie Han', 'Salman Khan', 'Abdulrahman Mahmoud'], 'affiliations': ['Australian National University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.16968.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#benchmark', '#open_source'], 'emoji': 'ğŸ”„', 'ru': {'title': 'CASS: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ GPU-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸', 'desc': 'CASS Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ GPU-ĞºĞ¾Ğ´Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°ÑÑĞµĞ¼Ğ±Ğ»ĞµÑ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ CASS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°: 95% Ğ´Ğ»Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¸ 37.5% Ğ´Ğ»Ñ Ğ°ÑÑĞµĞ¼Ğ±Ğ»ĞµÑ€Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 85% Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ CASS-Bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ GPU-ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'CASS: Bridging GPU Code Portability with High Accuracy Transpilation', 'desc': 'CASS is a groundbreaking dataset and model suite designed for GPU code transpilation, focusing on both source-level and assembly-level translations. It includes 70,000 verified code pairs that facilitate the conversion of code between different GPU architectures, addressing the challenge of low-level code portability. The CASS models achieve impressive accuracy rates, with 95% for source translation and 37.5% for assembly translation, significantly surpassing existing commercial solutions. Additionally, the generated code maintains native performance in over 85% of cases, and the accompanying CASS-Bench provides a robust evaluation framework for various GPU domains.'}, 'zh': {'title': 'CASSï¼šGPUä»£ç è½¬è¯‘çš„çªç ´æ€§è¿›å±•', 'desc': 'CASSæ˜¯ä¸€ä¸ªç”¨äºGPUä»£ç è½¬è¯‘çš„æ•°æ®é›†å’Œæ¨¡å‹å¥—ä»¶ï¼Œæ”¯æŒæºä»£ç å’Œæ±‡ç¼–çº§åˆ«çš„è½¬è¯‘ã€‚å®ƒåŒ…å«70,000å¯¹ç»è¿‡éªŒè¯çš„ä»£ç å¯¹ï¼Œè§£å†³äº†ä½çº§GPUä»£ç å¯ç§»æ¤æ€§çš„é‡è¦é—®é¢˜ã€‚é€šè¿‡è®­ç»ƒCASSç³»åˆ—ç‰¹å®šé¢†åŸŸè¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨æºä»£ç è½¬è¯‘ä¸­è¾¾åˆ°äº†95%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨æ±‡ç¼–è½¬è¯‘ä¸­è¾¾åˆ°äº†37.5%çš„å‡†ç¡®ç‡ã€‚CASSç”Ÿæˆçš„ä»£ç åœ¨è¶…è¿‡85%çš„æµ‹è¯•æ¡ˆä¾‹ä¸­ä¸æœ¬åœ°æ€§èƒ½ç›¸åŒ¹é…ï¼Œä¿æŒäº†è¿è¡Œæ—¶å’Œå†…å­˜è¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02921', 'title': 'A Controllable Examination for Long-Context Language Models', 'url': 'https://huggingface.co/papers/2506.02921', 'abstract': 'LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the "needle" and the "haystack" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: seamless context, controllable setting, and sound evaluation. This study introduces LongBioBench, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of understanding, reasoning, and trustworthiness. Our experimental evaluation, which includes 18 LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable.', 'score': 30, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '073ae66fedf9c141', 'authors': ['Yijun Yang', 'Zeyu Huang', 'Wenhao Zhu', 'Zihan Qiu', 'Fei Yuan', 'Jeff Z. Pan', 'Ivan Titov'], 'affiliations': ['Nanjing University', 'Qwen Team, Alibaba Group', 'Shanghai Artificial Intelligence Laboratory', 'University of Amsterdam', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.02921.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#long_context', '#reasoning', '#interpretability'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'LongBioBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'LongBioBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸. ĞĞ½ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¼ĞµĞ½ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'LongBioBench: A New Standard for Evaluating Long-Context Language Models', 'desc': 'LongBioBench is a new benchmark designed to evaluate long-context language models (LCLMs) using artificially generated biographies. It addresses the limitations of existing evaluation frameworks by providing a controlled environment that emphasizes understanding, reasoning, and trustworthiness. The study reveals that many LCLMs struggle with semantic understanding and reasoning as context length increases, highlighting the need for better evaluation methods. LongBioBench offers a more coherent and interpretable approach compared to previous synthetic benchmarks, making it a valuable tool for assessing LCLMs.'}, 'zh': {'title': 'LongBioBenchï¼šè¯„ä¼°é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹çš„æ–°åŸºå‡†', 'desc': 'LongBioBench æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œåˆ©ç”¨äººå·¥ç”Ÿæˆçš„ä¼ è®°æ¥è¯„ä¼°é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ï¼ˆLCLMï¼‰åœ¨ç†è§£ã€æ¨ç†å’Œå¯ä¿¡åº¦æ–¹é¢çš„è¡¨ç°ï¼Œè§£å†³äº†ç°æœ‰æ¡†æ¶çš„å±€é™æ€§ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶åˆ†ä¸ºçœŸå®ä¸–ç•Œä»»åŠ¡å’Œåˆæˆä»»åŠ¡ï¼Œä½†ä¸¤è€…éƒ½æœ‰å†…åœ¨çš„ç¼ºé™·ã€‚çœŸå®ä¸–ç•Œä»»åŠ¡å¤æ‚ä¸”æ˜“å—æ•°æ®æ±¡æŸ“ï¼Œè€Œåˆæˆä»»åŠ¡å¸¸å¸¸ç¼ºä¹è¿è´¯æ€§ï¼Œå½±å“å…¶ä½œä¸ºç°å®åº”ç”¨çš„æœ‰æ•ˆæ€§ã€‚LongBioBench æä¾›äº†ä¸€ä¸ªå—æ§ç¯å¢ƒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼° LCLM çš„èƒ½åŠ›ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå¤§å¤šæ•°æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’ŒåŸºæœ¬æ¨ç†ä¸Šä»å­˜åœ¨ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04141', 'title': "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos", 'url': 'https://huggingface.co/papers/2506.04141', 'abstract': 'A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.  \t\t\t\t\tAI-generated summary \t\t\t\t The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as "question frame") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities.', 'score': 25, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'a12411c7424fd2a7', 'authors': ['Kejian Zhu', 'Zhuoran Jin', 'Hongbang Yuan', 'Jiachun Li', 'Shangqing Tu', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao'], 'affiliations': ['School of Artificial Intelligence, University of Chinese Academy of Sciences', 'The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04141.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#long_context', '#benchmark', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'MMR-V: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMR-V Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ ÑƒĞ¿Ğ¾Ğ¼ÑĞ½ÑƒÑ‚Ñ‹Ñ… Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞµ, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ - Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ 52.5%. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñƒ Ğ˜Ğ˜.'}, 'en': {'title': 'MMR-V: Elevating Multimodal Reasoning in Videos', 'desc': 'The paper introduces MMR-V, a new benchmark designed to test the capabilities of multimodal large language models (MLLMs) in video analysis. It emphasizes the need for long-range, multi-frame reasoning, where models must analyze evidence that is not immediately adjacent to the question frame. Unlike existing benchmarks that focus on simple understanding tasks, MMR-V requires models to reason about hidden information and avoid shortcuts through distractor annotations. The findings show that current models struggle with these challenges, achieving only modest accuracy, highlighting the need for further research in enhancing multimodal reasoning skills.'}, 'zh': {'title': 'MMR-Vï¼šæ¨åŠ¨å¤šæ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†MMR-Vï¼Œæ—¨åœ¨æŒ‘æˆ˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ä¸­çš„é•¿è·ç¦»ã€å¤šå¸§æ¨ç†å’Œéšè—ä¿¡æ¯å¤„ç†èƒ½åŠ›ã€‚ç°æœ‰çš„è§†é¢‘åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ç†è§£ä»»åŠ¡ä¸Šï¼Œè€ŒMMR-Vè¦æ±‚æ¨¡å‹è¿›è¡Œæ›´å¤æ‚çš„æ¨ç†ï¼Œåˆ†æä¸é—®é¢˜å¸§ç›¸è·è¾ƒè¿œçš„è¯æ®å¸§ã€‚è¯¥åŸºå‡†åŒ…å«317ä¸ªè§†é¢‘å’Œ1257ä¸ªä»»åŠ¡ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œæœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º52.5%ã€‚æˆ‘ä»¬å¸Œæœ›MMR-Vèƒ½å¤Ÿæ¿€å‘è¿›ä¸€æ­¥ç ”ç©¶ï¼Œä»¥æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04180', 'title': 'SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models', 'url': 'https://huggingface.co/papers/2506.04180', 'abstract': 'Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.', 'score': 23, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '3f52b337c5fa3683', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqiang Hu', 'Juanzi Li', 'Roy Ka-Wei Lee'], 'affiliations': ['Singapore University of Technology and Design, Singapore', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04180.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#long_context', '#rlhf', '#benchmark', '#dataset', '#story_generation'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'SuperWriter-Agent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ 7B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SuperWriter-LM Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SuperWriter-LM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼.'}, 'en': {'title': 'Elevating Long-Form Text Generation with Structured Thinking', 'desc': 'This paper presents SuperWriter-Agent, a novel framework aimed at improving long-form text generation by large language models (LLMs). It introduces structured thinking through planning and refinement stages, which helps the model generate more coherent and logically consistent text. The framework is supported by a supervised fine-tuning dataset for training a 7B parameter model called SuperWriter-LM. Additionally, a hierarchical Direct Preference Optimization (DPO) method is employed, utilizing Monte Carlo Tree Search to enhance the quality of generated text, leading to superior performance on various benchmarks.'}, 'zh': {'title': 'æå‡é•¿æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„æ™ºèƒ½ä»£ç†', 'desc': 'é•¿æ–‡æœ¬ç”Ÿæˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´çš„é‡è¦æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒè¿è´¯æ€§ã€é€»è¾‘ä¸€è‡´æ€§å’Œæ–‡æœ¬è´¨é‡æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SuperWriter-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä»£ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é•¿æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡è§„åˆ’å’Œç²¾ç‚¼é˜¶æ®µå¼•å…¥æ˜ç¡®çš„ç»“æ„åŒ–æ€ç»´ï¼ŒæŒ‡å¯¼æ¨¡å‹éµå¾ªæ›´æœ‰æ„è¯†å’Œè®¤çŸ¥åŸºç¡€çš„è¿‡ç¨‹ï¼Œç±»ä¼¼äºä¸“ä¸šä½œå®¶çš„å†™ä½œæ–¹å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSuperWriter-LMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†åˆ†å±‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç»“æ„åŒ–æ€ç»´æ­¥éª¤çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04142', 'title': 'Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis', 'url': 'https://huggingface.co/papers/2506.04142', 'abstract': 'A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (rho) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation', 'score': 21, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '4799bf9d7a1cb57f', 'authors': ['Kejian Zhu', 'Shangqing Tu', 'Zhuoran Jin', 'Lei Hou', 'Juanzi Li', 'Jun Zhao'], 'affiliations': ['School of Artificial Intelligence, University of Chinese Academy of Sciences', 'The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04142.jpg', 'data': {'categories': ['#data', '#hallucinations', '#benchmark', '#ethics', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ‚Ñ‡Ğ¸Ğ½Ğ³ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ²-ÑˆĞ¾Ñ€Ñ‚ĞºĞ°Ñ‚Ğ¾Ğ²', 'desc': "ĞœĞµÑ‚Ğ¾Ğ´ 'shortcut neuron patching' Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹-ÑˆĞ¾Ñ€Ñ‚ĞºĞ°Ñ‚Ñ‹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ¼ MixEval, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¡Ğ¿Ğ¸Ñ€Ğ¼ĞµĞ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ 0,95. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸."}, 'en': {'title': 'Suppressing Shortcut Neurons for Trustworthy Evaluations', 'desc': 'This paper presents a method called shortcut neuron patching, which aims to identify and suppress shortcut neurons in language models to improve the reliability of evaluations. The authors highlight that current evaluation methods often suffer from data contamination, leading to unfair assessments of model performance. By analyzing the internal mechanisms of contaminated models, they find that shortcut solutions during training contribute to overestimation of model capabilities. Their proposed method effectively mitigates these issues, showing strong correlation with established trustworthy benchmarks, thus ensuring more accurate evaluations of language models.'}, 'zh': {'title': 'æŠ‘åˆ¶å¿«æ·ç¥ç»å…ƒï¼Œæå‡è¯„ä¼°å¯ä¿¡åº¦', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¿«æ·ç¥ç»å…ƒä¿®è¡¥çš„æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«å’ŒæŠ‘åˆ¶è¯­è¨€æ¨¡å‹ä¸­çš„å¿«æ·ç¥ç»å…ƒï¼Œä»¥å‡è½»æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œä»è€Œæé«˜è¯„ä¼°çš„å¯ä¿¡åº¦ã€‚å½“å‰çš„è¯„ä¼°æ–¹æ³•å¤§å¤šä¾èµ–å…¬å…±åŸºå‡†ï¼Œä½†è¿™äº›åŸºå‡†å®¹æ˜“å—åˆ°æ•°æ®æ±¡æŸ“çš„å½±å“ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å…¬å¹³ã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒå’Œå› æœåˆ†æï¼Œå‘ç°è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹å‚æ•°å¯èƒ½ä¼šè·å¾—å¿«æ·è§£å†³æ–¹æ¡ˆï¼Œä»è€Œå¯¼è‡´å¯¹æ±¡æŸ“æ¨¡å‹çš„è¿‡é«˜ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡è½»æ±¡æŸ“æ–¹é¢æœ‰æ•ˆï¼Œå¹¶ä¸”ä¸MixEvalåŸºå‡†çš„è¯„ä¼°ç»“æœå…·æœ‰å¾ˆå¼ºçš„çº¿æ€§ç›¸å…³æ€§ï¼ŒSpearmanç³»æ•°è¶…è¿‡0.95ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤ŸçœŸå®åæ˜ æ¨¡å‹çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04178', 'title': 'OpenThoughts: Data Recipes for Reasoning Models', 'url': 'https://huggingface.co/papers/2506.04178', 'abstract': 'The OpenThoughts project created open-source datasets leading to reasoning models that match or exceed state-of-the-art benchmarks in math, code, and science.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. All of our datasets and models are available on https://openthoughts.ai.', 'score': 18, 'issue_id': 4149, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '43845ce52af6bd6c', 'authors': ['Etash Guha', 'Ryan Marten', 'Sedrick Keh', 'Negin Raoof', 'Georgios Smyrnis', 'Hritik Bansal', 'Marianna Nezhurina', 'Jean Mercat', 'Trung Vu', 'Zayne Sprague', 'Ashima Suvarna', 'Benjamin Feuer', 'Liangyu Chen', 'Zaid Khan', 'Eric Frankel', 'Sachin Grover', 'Caroline Choi', 'Niklas Muennighoff', 'Shiye Su', 'Wanjia Zhao', 'John Yang', 'Shreyas Pimpalgaonkar', 'Kartik Sharma', 'Charlie Cheng-Jie Ji', 'Yichuan Deng', 'Sarah Pratt', 'Vivek Ramanujan', 'Jon Saad-Falcon', 'Jeffrey Li', 'Achal Dave', 'Alon Albalak', 'Kushal Arora', 'Blake Wulfe', 'Chinmay Hegde', 'Greg Durrett', 'Sewoong Oh', 'Mohit Bansal', 'Saadia Gabriel', 'Aditya Grover', 'Kai-Wei Chang', 'Vaishaal Shankar', 'Aaron Gokaslan', 'Mike A. Merrill', 'Tatsunori Hashimoto', 'Yejin Choi', 'Jenia Jitsev', 'Reinhard Heckel', 'Maheswaran Sathiamoorthy', 'Alexandros G. Dimakis', 'Ludwig Schmidt'], 'affiliations': ['ASU', 'BespokeLabs.ai', 'Cornell Tech', 'JSC', 'LAION', 'Lila Sciences', 'NYU', 'Open-Î¨ (Open-Sci) Collective', 'Stanford University', 'TUM', 'Toyota Research Institute', 'UC Berkeley', 'UCLA', 'UNC Chapel Hill', 'UT Austin', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.04178.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#data', '#training', '#science', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ˜Ğ˜-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…', 'desc': 'ĞŸÑ€Ğ¾ĞµĞºÑ‚ OpenThoughts ÑĞ¾Ğ·Ğ´Ğ°Ğ» Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ñ‹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°ÑƒĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ OpenThoughts2-1M, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ» Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ OpenThinker2-32B, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ DeepSeek-R1-Distill-32B Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼. Ğ”Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¾ 1,2 Ğ¼Ğ»Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OpenThinker3-7B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… AIME, LiveCodeBench Ğ¸ GPQA Diamond. Ğ’ÑĞµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ.'}, 'en': {'title': 'Open-Source Datasets for Superior Reasoning Models', 'desc': 'The OpenThoughts project focuses on creating open-source datasets to enhance reasoning models in math, code, and science. By developing the OpenThoughts2-1M dataset, they trained the OpenThinker2-32B model, which achieved performance comparable to proprietary models on standard benchmarks. The project further refined its dataset through extensive experimentation, resulting in the OpenThoughts3 dataset and the OpenThinker3-7B model. This model set new state-of-the-art results on multiple reasoning benchmarks, demonstrating the effectiveness of publicly available training data.'}, 'zh': {'title': 'å¼€æºæ•°æ®é›†åŠ©åŠ›æ¨ç†æ¨¡å‹çªç ´æé™', 'desc': 'OpenThoughtsé¡¹ç›®æ—¨åœ¨åˆ›å»ºå¼€æºæ•°æ®é›†ï¼Œä»¥è®­ç»ƒæ¨ç†æ¨¡å‹ï¼Œè¾¾åˆ°æˆ–è¶…è¿‡æ•°å­¦ã€ä»£ç å’Œç§‘å­¦é¢†åŸŸçš„æœ€æ–°åŸºå‡†ã€‚å°½ç®¡æ¨ç†æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†ä»å­˜åœ¨è®¸å¤šå…³äºæœ€ä½³è®­ç»ƒæ–¹æ³•çš„æœªè§£é—®é¢˜ã€‚é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶æ•°æ®ç”Ÿæˆæµç¨‹ï¼ŒOpenThoughtsé¡¹ç›®å¼€å‘äº†å¤šä¸ªç‰ˆæœ¬çš„æ•°æ®é›†ï¼Œæœ€ç»ˆæ¨å‡ºäº†OpenThinker3-7Bæ¨¡å‹ï¼Œå–å¾—äº†åœ¨å¤šä¸ªæ ‡å‡†æ¨ç†åŸºå‡†ä¸Šçš„æœ€ä½³æˆç»©ã€‚æ‰€æœ‰æ•°æ®é›†å’Œæ¨¡å‹å‡å¯åœ¨https://openthoughts.aiä¸Šè·å–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03150', 'title': 'IllumiCraft: Unified Geometry and Illumination Diffusion for\n  Controllable Video Generation', 'url': 'https://huggingface.co/papers/2506.03150', 'abstract': 'IllumiCraft integrates geometric cues in a diffusion framework to generate high-fidelity, temporally coherent videos from textual or image inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Although diffusion-based models can generate high-quality and high-resolution video sequences from textual or image inputs, they lack explicit integration of geometric cues when controlling scene lighting and visual appearance across frames. To address this limitation, we propose IllumiCraft, an end-to-end diffusion framework accepting three complementary inputs: (1) high-dynamic-range (HDR) video maps for detailed lighting control; (2) synthetically relit frames with randomized illumination changes (optionally paired with a static background reference image) to provide appearance cues; and (3) 3D point tracks that capture precise 3D geometry information. By integrating the lighting, appearance, and geometry cues within a unified diffusion architecture, IllumiCraft generates temporally coherent videos aligned with user-defined prompts. It supports background-conditioned and text-conditioned video relighting and provides better fidelity than existing controllable video generation methods. Project Page: https://yuanze-lin.me/IllumiCraft_page', 'score': 18, 'issue_id': 4142, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '524d1d7f47dfcf7f', 'authors': ['Yuanze Lin', 'Yi-Wen Chen', 'Yi-Hsuan Tsai', 'Ronald Clark', 'Ming-Hsuan Yang'], 'affiliations': ['Atmanity Inc.', 'Google DeepMind', 'NEC Labs America', 'UC Merced', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.03150.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#architecture', '#3d', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑĞ²ĞµÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'IllumiCraft - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: HDR-ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ 3D-Ñ‚Ñ€ĞµĞºĞ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. IllumiCraft Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'IllumiCraft: Crafting Coherent Videos with Geometric Precision', 'desc': 'IllumiCraft is a novel diffusion framework designed to create high-quality videos from text or image inputs while incorporating geometric cues. It utilizes three main inputs: HDR video maps for precise lighting control, synthetically relit frames for appearance variations, and 3D point tracks for accurate geometry representation. By merging these elements, IllumiCraft ensures that the generated videos are not only visually appealing but also maintain temporal coherence across frames. This approach enhances the fidelity of video generation compared to existing methods that lack such integrated controls.'}, 'zh': {'title': 'IllumiCraftï¼šé«˜ä¿çœŸè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'IllumiCraft æ˜¯ä¸€ä¸ªé›†æˆå‡ ä½•çº¿ç´¢çš„æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬æˆ–å›¾åƒè¾“å…¥ç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´ä¸€è‡´çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¥å—é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰è§†é¢‘å›¾åƒã€åˆæˆé‡æ–°ç…§æ˜çš„å¸§å’Œ3Dç‚¹è½¨è¿¹ï¼Œæ¥æ§åˆ¶åœºæ™¯çš„å…‰ç…§å’Œè§†è§‰å¤–è§‚ã€‚é€šè¿‡å°†å…‰ç…§ã€å¤–è§‚å’Œå‡ ä½•çº¿ç´¢æ•´åˆåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£æ¶æ„ä¸­ï¼ŒIllumiCraft ç”Ÿæˆä¸ç”¨æˆ·å®šä¹‰æç¤ºä¸€è‡´çš„æ—¶é—´è¿è´¯è§†é¢‘ã€‚ä¸ç°æœ‰çš„å¯æ§è§†é¢‘ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´å¥½çš„ä¿çœŸåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04225', 'title': 'Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation', 'url': 'https://huggingface.co/papers/2506.04225', 'abstract': 'Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world applications like video gaming and virtual reality often demand the ability to model 3D scenes that users can explore along custom camera trajectories. While significant progress has been made in generating 3D objects from text or images, creating long-range, 3D-consistent, explorable 3D scenes remains a complex and challenging problem. In this work, we present Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Unlike existing approaches, Voyager achieves end-to-end scene generation and reconstruction with inherent consistency across frames, eliminating the need for 3D reconstruction pipelines (e.g., structure-from-motion or multi-view stereo). Our method integrates three key components: 1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence 2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency, and 3) Scalable Data Engine: A video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Collectively, these designs result in a clear improvement over existing methods in visual quality and geometric accuracy, with versatile applications.', 'score': 17, 'issue_id': 4139, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '99f491949aa412fd', 'authors': ['Tianyu Huang', 'Wangguandong Zheng', 'Tengfei Wang', 'Yuhao Liu', 'Zhenwei Wang', 'Junta Wu', 'Jie Jiang', 'Hui Li', 'Rynson W. H. Lau', 'Wangmeng Zuo', 'Chunchao Guo'], 'affiliations': ['City University of Hong Kong, China', 'Harbin Institute of Technology, China', 'Southeast University, China', 'Tencent Hunyuan, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04225.jpg', 'data': {'categories': ['#games', '#dataset', '#diffusion', '#3d', '#video'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ˜ÑÑĞ»ĞµĞ´ÑƒĞ¹ 3D-Ğ¼Ğ¸Ñ€Ñ‹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°', 'desc': 'Voyager - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 3D-Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-ÑÑ†ĞµĞ½Ñ‹ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Voyager Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Voyager: Seamless 3D Scene Exploration from a Single Image', 'desc': 'Voyager is a video diffusion framework that creates 3D point-cloud sequences from a single image, allowing users to explore scenes along custom camera paths. It addresses the challenge of generating long-range, consistent 3D environments by integrating world-consistent video diffusion, which ensures alignment of RGB and depth sequences. The framework also features an efficient world cache for smooth scene exploration and a scalable data engine that automates camera pose estimation and depth prediction. Overall, Voyager enhances visual quality and geometric accuracy, making it suitable for applications in video gaming and virtual reality.'}, 'zh': {'title': 'Voyagerï¼šä»å•å›¾åƒç”Ÿæˆä¸€è‡´çš„3Dåœºæ™¯æ¢ç´¢', 'desc': 'Voyageræ˜¯ä¸€ç§è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒç”Ÿæˆä¸–ç•Œä¸€è‡´çš„3Dç‚¹äº‘åºåˆ—ï¼Œæ”¯æŒç”¨æˆ·å®šä¹‰çš„ç›¸æœºè·¯å¾„è¿›è¡Œé•¿è·ç¦»çš„3Dåœºæ™¯æ¢ç´¢ã€‚è¯¥æ–¹æ³•é€šè¿‡ç«¯åˆ°ç«¯çš„åœºæ™¯ç”Ÿæˆå’Œé‡å»ºï¼Œç¡®ä¿äº†å¸§é—´çš„ä¸€è‡´æ€§ï¼Œé¿å…äº†ä¼ ç»Ÿçš„3Dé‡å»ºæµç¨‹ã€‚Voyageré›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä¸–ç•Œä¸€è‡´çš„è§†é¢‘æ‰©æ•£ã€é•¿è·ç¦»ä¸–ç•Œæ¢ç´¢å’Œå¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œæå‡äº†è§†è§‰è´¨é‡å’Œå‡ ä½•ç²¾åº¦ã€‚è¯¥æ¡†æ¶åœ¨è§†é¢‘æ¸¸æˆå’Œè™šæ‹Ÿç°å®ç­‰åº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04158', 'title': 'Image Editing As Programs with Diffusion Models', 'url': 'https://huggingface.co/papers/2506.04158', 'abstract': 'While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.', 'score': 15, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'e5a32d484bb427f3', 'authors': ['Yujia Hu', 'Songhua Liu', 'Zhenxiong Tan', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.04158.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ˜Ğ˜-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ IEAP (Image Editing As Programs). Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Diffusion Transformer Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ DiT. IEAP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing Image Editing with Programmatic Precision', 'desc': 'This paper addresses the limitations of diffusion models in instruction-driven image editing, particularly when it comes to making significant layout changes. The authors propose a new framework called Image Editing As Programs (IEAP), which utilizes the Diffusion Transformer (DiT) architecture to break down complex editing tasks into simpler, atomic operations. Each operation is executed by a lightweight adapter that specializes in a specific type of edit, allowing for more flexible and accurate transformations. The framework shows improved performance over existing methods, achieving higher accuracy and semantic fidelity in various editing scenarios, especially for complex instructions.'}, 'zh': {'title': 'å›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•ï¼šå°†å¤æ‚æŒ‡ä»¤è½¬åŒ–ä¸ºç®€å•æ“ä½œ', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œç§°ä¸ºå›¾åƒç¼–è¾‘ä½œä¸ºç¨‹åºï¼ˆIEAPï¼‰ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹åœ¨æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒç¼–è¾‘ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚IEAPåŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¶æ„ï¼Œé€šè¿‡å°†å¤æ‚çš„ç¼–è¾‘æŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç³»åˆ—åŸå­æ“ä½œæ¥å®ç°ã€‚æ¯ä¸ªæ“ä½œç”±è½»é‡çº§é€‚é…å™¨å®ç°ï¼Œä¸“é—¨é’ˆå¯¹ç‰¹å®šç±»å‹çš„ç¼–è¾‘ï¼Œèƒ½å¤Ÿæ”¯æŒä»»æ„å’Œç»“æ„ä¸ä¸€è‡´çš„å˜æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIEAPåœ¨å„ç§ç¼–è¾‘åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤æŒ‡ä»¤æ—¶è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03295', 'title': 'Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem', 'url': 'https://huggingface.co/papers/2506.03295', 'abstract': "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs.", 'score': 15, 'issue_id': 4135, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '03df39687a4bdf5a', 'authors': ['Yubo Wang', 'Ping Nie', 'Kai Zou', 'Lijun Wu', 'Wenhu Chen'], 'affiliations': ['Independent', 'Netmind.AI', 'Shanghai AI Lab', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.03295.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Critique Fine-Tuning (CFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). CFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ñ… Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ CFT ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'Unlocking Reasoning Power with Efficient Critique Fine-Tuning', 'desc': 'This paper introduces Critique Fine-Tuning (CFT) as a method to enhance the reasoning abilities of large language models (LLMs) like Qwen and Llama. By focusing on a single problem, CFT generates critique data from various model-generated solutions, which are then used to fine-tune the models. The results show that this approach leads to significant performance improvements on reasoning tasks with much lower computational costs compared to traditional reinforcement learning methods. The findings suggest that CFT is a robust and efficient strategy for maximizing the reasoning potential of LLMs.'}, 'zh': {'title': 'æ‰¹è¯„å¾®è°ƒï¼šé«˜æ•ˆé‡Šæ”¾è¯­è¨€æ¨¡å‹æ¨ç†æ½œåŠ›çš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ‰¹è¯„å¾®è°ƒï¼ˆCritique Fine-Tuning, CFTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é«˜æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹å•ä¸€é—®é¢˜è¿›è¡Œå¾®è°ƒï¼ŒCFTèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—æˆæœ¬ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨CFTæ–¹æ³•ï¼Œæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†15%åˆ°16%ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒCFTåœ¨è®¡ç®—èµ„æºä¸Šæ›´åŠ é«˜æ•ˆï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºä¸€ç§ç®€å•ä¸”é€šç”¨çš„æ¨ç†èƒ½åŠ›æå‡ç­–ç•¥çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01320', 'title': 'Î¨-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models', 'url': 'https://huggingface.co/papers/2506.01320', 'abstract': 'We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.', 'score': 15, 'issue_id': 4136, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '6441248200df695a', 'authors': ['Taehoon Yoon', 'Yunhong Min', 'Kyeongmin Yeo', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2506.01320.jpg', 'data': {'categories': ['#inference', '#alignment', '#rlhf', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Psi-Sampler - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (SMC) Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ† Ğ¸Ğ· Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ preconditioned Crank-Nicolson Langevin (pCNL). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´.'}, 'en': {'title': 'Enhancing Reward Alignment with Psi-Sampler', 'desc': 'The paper presents Psi-Sampler, a framework that enhances reward alignment during inference by using Sequential Monte Carlo (SMC) methods. It addresses the limitations of traditional particle initialization from Gaussian priors, which often fail to capture important reward-related areas. By employing a reward-aware posterior for initialization, the framework significantly boosts sampling efficiency and alignment performance. Additionally, the introduction of the preconditioned Crank-Nicolson Langevin (pCNL) algorithm allows for effective sampling in complex, high-dimensional spaces, leading to improved results in various generative tasks.'}, 'zh': {'title': 'é«˜æ•ˆçš„å¥–åŠ±å¯¹é½ï¼šPsi-Sampleræ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPsi-Samplerçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºåºåˆ—è’™ç‰¹å¡æ´›ï¼ˆSMCï¼‰æ–¹æ³•ï¼Œå¹¶ç»“åˆäº†åŸºäºå¥–åŠ±çš„åˆå§‹ç²’å­é‡‡æ ·ï¼Œä»¥å®ç°ä¸åŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹çš„æœ‰æ•ˆæ¨ç†æ—¶é—´å¥–åŠ±å¯¹é½ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹åœ¨æ¨ç†æ—¶é—´å¥–åŠ±å¯¹é½æ–¹é¢å—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œæ ‡å¿—ç€ä»é¢„è®­ç»ƒåˆ°åè®­ç»ƒä¼˜åŒ–çš„èŒƒå¼è½¬å˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»é«˜æ–¯å…ˆéªŒåˆå§‹åŒ–ç²’å­ï¼Œè¿™ä¸è¶³ä»¥æ•æ‰ä¸å¥–åŠ±ç›¸å…³çš„åŒºåŸŸï¼Œå¯¼è‡´é‡‡æ ·æ•ˆç‡é™ä½ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»å¥–åŠ±æ„ŸçŸ¥åéªŒåˆå§‹åŒ–æ˜¾è‘—æé«˜äº†å¯¹é½æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†é¢„æ¡ä»¶Crank-Nicolson Langevinï¼ˆpCNLï¼‰ç®—æ³•ï¼Œä»¥å®ç°é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­çš„åéªŒé‡‡æ ·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03930', 'title': 'VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation', 'url': 'https://huggingface.co/papers/2506.03930', 'abstract': 'VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation.', 'score': 14, 'issue_id': 4139, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '56d407ae0e0e6b4a', 'authors': ['Yuansheng Ni', 'Ping Nie', 'Kai Zou', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'Netmind.ai', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2506.03930.jpg', 'data': {'categories': ['#story_generation', '#data', '#dataset', '#optimization', '#training'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'VisCode-200K: Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²', 'desc': 'VisCode-200K - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 200 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VisCode-200K Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VisCoder, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ğ»Ğ°ÑÑŒ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Empowering Visualization with VisCode-200K: A Leap in Plot Generation!', 'desc': 'The paper introduces VisCode-200K, a comprehensive dataset designed to enhance the performance of machine learning models in generating visualizations through improved code generation. It addresses the limitations of existing datasets by incorporating execution-grounded supervision and enabling iterative code correction, which helps models learn from their mistakes. The dataset consists of over 200,000 examples, including validated plotting code and multi-turn dialogues for code feedback. The authors demonstrate that their model, VisCoder, fine-tuned on this dataset, significantly outperforms existing open-source models and competes closely with proprietary ones in generating accurate visualizations.'}, 'zh': {'title': 'VisCode-200Kï¼šæå‡å¯è§†åŒ–ç”Ÿæˆçš„é©å‘½æ€§æ•°æ®é›†', 'desc': 'VisCode-200Kæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œä¸“æ³¨äºå¯è§†åŒ–ä»»åŠ¡ï¼Œæ—¨åœ¨æé«˜ç»˜å›¾ç”Ÿæˆçš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†ç»“åˆäº†æ‰§è¡ŒåŸºç¡€çš„ç›‘ç£å’Œè¿­ä»£ä»£ç ä¿®æ­£ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨ç»˜å›¾æ—¶çš„è„†å¼±æ€§å’Œä¸å¯é æ€§ã€‚å®ƒåŒ…å«æ¥è‡ªå¼€æºä»£ç åº“çš„æœ‰æ•ˆç»˜å›¾ä»£ç å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„é…å¯¹ï¼Œä»¥åŠå¤šè½®ä¿®æ­£å¯¹è¯ï¼Œå¸®åŠ©æ¨¡å‹ä¿®æ­£é”™è¯¯ä»£ç ã€‚é€šè¿‡åœ¨VisCode-200Kä¸Šå¾®è°ƒæ¨¡å‹ï¼ŒVisCoderåœ¨ç»˜å›¾ç”Ÿæˆæ–¹é¢æ˜¾è‘—è¶…è¶Šäº†å¼€æºåŸºçº¿ï¼Œæ¥è¿‘å•†ä¸šæ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04228', 'title': 'LayerFlow: A Unified Model for Layer-aware Video Generation', 'url': 'https://huggingface.co/papers/2506.04228', 'abstract': 'LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos for different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers.', 'score': 13, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '1e8f6532d8b54b21', 'authors': ['Sihui Ji', 'Hao Luo', 'Xi Chen', 'Yuanpeng Tu', 'Yiyang Wang', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group, China', 'Hupan Laboratory, China', 'The University of Hong Kong', 'The University of Hong Kong, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.04228.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#synthetic', '#video', '#training'], 'emoji': 'ğŸï¸', 'ru': {'title': 'LayerFlow: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼', 'desc': 'LayerFlow - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾ĞµĞ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°, Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ğ½Ğ° Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑĞ»Ğ¾ĞµĞ². LayerFlow Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LoRA Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸.'}, 'en': {'title': 'LayerFlow: Unified Layer-Aware Video Generation', 'desc': 'LayerFlow is a comprehensive framework designed for generating videos that are aware of different layers, such as foreground and background. It utilizes a text-to-video diffusion transformer to create videos based on specific prompts for each layer, allowing for various video generation tasks. The framework can decompose blended videos or generate backgrounds for given foregrounds, making it versatile. To address the challenge of limited high-quality training data, LayerFlow employs a multi-stage training strategy that begins with low-quality videos and progressively incorporates high-quality layered images.'}, 'zh': {'title': 'LayerFlowï¼šç»Ÿä¸€çš„å±‚æ„ŸçŸ¥è§†é¢‘ç”Ÿæˆæ¡†æ¶', 'desc': 'LayerFlowæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå±‚æ„ŸçŸ¥çš„è§†é¢‘ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£å˜æ¢å™¨å’Œå±‚åµŒå…¥ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬é€æ˜å‰æ™¯ã€å¹²å‡€èƒŒæ™¯å’Œæ··åˆåœºæ™¯çš„è§†é¢‘ç”Ÿæˆã€‚é€šè¿‡å°†è§†é¢‘æŒ‰å±‚ç»„ç»‡ä¸ºå­å‰ªè¾‘ï¼Œå¹¶åˆ©ç”¨å±‚åµŒå…¥æ¥åŒºåˆ†æ¯ä¸ªå‰ªè¾‘åŠå…¶å¯¹åº”çš„å±‚çº§æç¤ºï¼ŒLayerFlowå®ç°äº†å¤šç§è§†é¢‘ç”Ÿæˆå˜ä½“ã€‚ä¸ºäº†å…‹æœé«˜è´¨é‡å±‚çº§è®­ç»ƒè§†é¢‘çš„ç¼ºä¹ï¼ŒLayerFlowè®¾è®¡äº†å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç»“åˆé™æ€å›¾åƒå’Œé«˜è´¨é‡å±‚æ³¨é‡Šè¿›è¡Œè®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03139', 'title': 'SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation', 'url': 'https://huggingface.co/papers/2506.03139', 'abstract': 'SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at https://zju-real.github.io/SVGenius.', 'score': 13, 'issue_id': 4137, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'c1fdc3559598aa68', 'authors': ['Siqi Chen', 'Xinyu Dong', 'Haolei Xu', 'Xingyu Wu', 'Fei Tang', 'Hang Zhang', 'Yuchen Yan', 'Linjuan Wu', 'Wenqi Zhang', 'Guiyang Hou', 'Yongliang Shen', 'Weiming Lu', 'Yueting Zhuang'], 'affiliations': ['Zhejiang University Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03139.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#optimization', '#multimodal', '#benchmark', '#interpretability'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'SVGenius: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹', 'desc': 'SVGenius - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ SVG. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2377 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ‚Ñ€ĞµÑ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ…: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SVG. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾ 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 18 Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 24 Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ, Ğ½Ğ¾ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'SVGenius: Unveiling SVG Processing Potential in LLMs', 'desc': 'SVGenius is a benchmark designed to evaluate the performance of Large Language Models (LLMs) and Multimodal LLMs in processing Scalable Vector Graphics (SVG). It assesses models across three key dimensions: understanding, editing, and generation, using a total of 2,377 queries derived from real-world applications. The evaluation framework includes 8 task categories and 18 metrics, highlighting the strengths and weaknesses of 22 different models. Findings indicate that while proprietary models excel, all models struggle with complex tasks, suggesting a need for improved training methods, particularly in reasoning, to enhance their capabilities.'}, 'zh': {'title': 'SVGeniusï¼šå…¨é¢è¯„ä¼° SVG å¤„ç†èƒ½åŠ›çš„åŸºå‡†å·¥å…·', 'desc': 'SVGenius æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€ LLM åœ¨ SVG å¤„ç†èƒ½åŠ›çš„åŸºå‡†å·¥å…·ã€‚å®ƒé€šè¿‡ç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆä¸‰ä¸ªç»´åº¦ï¼Œä½¿ç”¨ 2,377 ä¸ªæŸ¥è¯¢æ¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡ä¸“æœ‰æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºå¼€æºæ¨¡å‹ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨å¤æ‚æ€§å¢åŠ æ—¶è¡¨ç°æ™®éä¸‹é™ã€‚SVGenius æä¾›äº†ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„çŸ¢é‡å›¾å½¢æ¨¡å‹å’Œæ¨è¿›è‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡åº”ç”¨æä¾›äº†é‡è¦è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03517', 'title': 'DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2506.03517', 'abstract': 'Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one-third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels.', 'score': 12, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '5be6b9aa57297fb6', 'authors': ['Ziyi Wu', 'Anil Kag', 'Ivan Skorokhodov', 'Willi Menapace', 'Ashkan Mirzaei', 'Igor Gilitschenski', 'Sergey Tulyakov', 'Aliaksandr Siarohin'], 'affiliations': ['Snap Research', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.03517.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#rlhf'], 'emoji': 'ğŸ¬', 'ru': {'title': 'DenseDPO: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'DenseDPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ²Ğ¸Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. DenseDPO ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ° Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¿Ğ¸Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…, Ğ° Ğ½Ğµ Ğ½Ğ° Ñ†ĞµĞ»Ñ‹Ñ… ĞºĞ»Ğ¸Ğ¿Ğ°Ñ…. DenseDPO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ DPO, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ñ€ĞµÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Video Generation with DenseDPO: Precision and Efficiency in Preference Learning', 'desc': 'This paper introduces DenseDPO, an improved method for Direct Preference Optimization (DPO) in text-to-video diffusion models. DenseDPO addresses the limitations of traditional DPO by creating video pairs from denoised versions of a ground truth video, allowing for better alignment and reducing bias towards low-motion clips. It also enables preference labeling on shorter video segments, which provides a more detailed learning signal while using less labeled data. Additionally, DenseDPO facilitates automatic preference annotation through Vision Language Models, achieving performance comparable to human-labeled data.'}, 'zh': {'title': 'DenseDPOï¼šæå‡è§†é¢‘ç”Ÿæˆçš„åå¥½ä¼˜åŒ–æ–¹æ³•', 'desc': 'ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æœ€è¿‘è¢«åº”ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹åè®­ç»ƒæŠ€æœ¯ã€‚æˆ‘ä»¬æå‡ºçš„DenseDPOæ–¹æ³•é€šè¿‡ä¸‰é¡¹è´¡çŒ®è§£å†³äº†DPOçš„ä¸è¶³ä¹‹å¤„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å»å™ªçœŸå®è§†é¢‘çš„æŸåå‰¯æœ¬æ¥åˆ›å»ºè§†é¢‘å¯¹ï¼Œä»è€Œæ¶ˆé™¤äº†è¿åŠ¨åå·®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨æ—¶é—´å¯¹é½æ¥æ ‡è®°çŸ­ç‰‡æ®µçš„åå¥½ï¼Œä½¿å­¦ä¹ ä¿¡å·æ›´åŠ å¯†é›†å’Œç²¾ç¡®ï¼Œæœ€ç»ˆDenseDPOåœ¨è¿åŠ¨ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸDPOï¼ŒåŒæ—¶åœ¨æ–‡æœ¬å¯¹é½ã€è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24500', 'title': "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence", 'url': 'https://huggingface.co/papers/2505.24500', 'abstract': "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.", 'score': 11, 'issue_id': 4133, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'ee580986393d0b7e', 'authors': ['Guiyang Hou', 'Xing Gao', 'Yuchuan Wu', 'Xiang Huang', 'Wenqi Zhang', 'Zhe Zheng', 'Yongliang Shen', 'Jialu Du', 'Fei Huang', 'Yongbin Li', 'Weiming Lu'], 'affiliations': ['Nanjing University', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24500.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ TimeHC-RL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ System 2 RL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ TimeHC-RL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': "Boosting LLMs' Social Intelligence with TimeHC-RL", 'desc': "This paper presents a new approach called Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) to improve the social intelligence of Large Language Models (LLMs). Unlike traditional methods that focus on logical reasoning, TimeHC-RL incorporates different cognitive processes, including intuitive and deliberate thinking, to better navigate social contexts. The authors conducted experiments across various datasets and compared TimeHC-RL with existing reinforcement learning methods, demonstrating its superior performance. The findings suggest that enhancing LLMs' cognitive abilities in social domains can significantly elevate their overall intelligence and effectiveness."}, 'zh': {'title': 'æå‡ç¤¾äº¤æ™ºèƒ½çš„æ—¶é—´æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæ—¶é—´æ„ŸçŸ¥å±‚æ¬¡è®¤çŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆTimeHC-RLï¼‰ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾äº¤é¢†åŸŸçš„æ™ºèƒ½ã€‚ä¸æ•°å­¦ç­‰ä¾èµ–ç³»ç»Ÿ2è®¤çŸ¥çš„é¢†åŸŸä¸åŒï¼Œç¤¾äº¤é¢†åŸŸéœ€è¦æ›´ä¸°å¯Œçš„è®¤çŸ¥æ¨¡å¼ï¼ŒåŒ…æ‹¬ç›´è§‰ååº”å’Œè¡¨å±‚æ€ç»´ã€‚é€šè¿‡å¯¹å…«ä¸ªä¸åŒæ•°æ®é›†çš„å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†TimeHC-RLæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨ç¤¾äº¤æ™ºèƒ½æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„ç³»ç»Ÿ2å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿å¾—7BåŸºç¡€æ¨¡å‹çš„è¡¨ç°æ¥è¿‘äºæ›´å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI-O3ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04108', 'title': 'Rectified Sparse Attention', 'url': 'https://huggingface.co/papers/2506.04108', 'abstract': 'Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM.', 'score': 8, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'ff7222f16cd2bf28', 'authors': ['Yutao Sun', 'Tianzhu Ye', 'Li Dong', 'Yuqing Xia', 'Jian Chen', 'Yizhao Gao', 'Shijie Cao', 'Jianyong Wang', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04108.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ Rectified Sparse Attention (ReSA) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾-Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ReSA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ğº Ğ±ĞµĞ·Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 2,42 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 256 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Boosting Efficiency in Long-Sequence Generation with ReSA', 'desc': "Rectified Sparse Attention (ReSA) enhances the efficiency of generating long sequences in Large Language Models by integrating block-sparse attention with periodic dense rectification. This approach addresses the issue of KV cache misalignment that can lead to errors and reduced quality in generated outputs. By periodically refreshing the KV cache through a dense forward pass, ReSA minimizes error accumulation and maintains alignment with the model's pretraining data. Experimental results show that ReSA not only preserves high-quality generation but also achieves significant speed improvements, making it a viable option for long-context tasks."}, 'zh': {'title': 'é«˜æ•ˆé•¿åºåˆ—ç”Ÿæˆçš„æ–°æ–¹æ³•ï¼šReSA', 'desc': 'Rectified Sparse Attentionï¼ˆReSAï¼‰æ˜¯ä¸€ç§æé«˜å¤§å‹è¯­è¨€æ¨¡å‹é•¿åºåˆ—ç”Ÿæˆæ•ˆç‡çš„æ–¹æ³•ã€‚å®ƒç»“åˆäº†å—ç¨€ç–æ³¨æ„åŠ›å’Œå‘¨æœŸæ€§å¯†é›†æ•´æµï¼Œèƒ½å¤Ÿä¿æŒé«˜è´¨é‡çš„ç”Ÿæˆæ•ˆæœã€‚é€šè¿‡åœ¨å›ºå®šé—´éš”å†…ä½¿ç”¨å¯†é›†å‰å‘ä¼ é€’åˆ·æ–°KVç¼“å­˜ï¼ŒReSAé™åˆ¶äº†è¯¯å·®ç´¯ç§¯ï¼Œå¹¶ä¿æŒä¸é¢„è®­ç»ƒåˆ†å¸ƒçš„å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒReSAåœ¨æ•°å­¦æ¨ç†ã€è¯­è¨€å»ºæ¨¡å’Œæ£€ç´¢ä»»åŠ¡ä¸­å®ç°äº†æ¥è¿‘æ— æŸçš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶åœ¨256Kåºåˆ—é•¿åº¦ä¸‹æä¾›äº†é«˜è¾¾2.42å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02592', 'title': 'Beyond the Surface: Measuring Self-Preference in LLM Judgments', 'url': 'https://huggingface.co/papers/2506.02592', 'abstract': 'The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.', 'score': 8, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'ccdb2761a23fe0c8', 'authors': ['Zhi-Yuan Chen', 'Hao Wang', 'Xinyu Zhang', 'Enrui Hu', 'Yankai Lin'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Huawei Poisson Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.02592.jpg', 'data': {'categories': ['#benchmark', '#data', '#hallucinations', '#interpretability', '#ethics', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'DBG: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ DBG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Measuring Self-Preference Bias with the DBG Score', 'desc': 'This paper introduces the DBG score, a new metric designed to measure self-preference bias in large language models (LLMs) while accounting for response quality. Traditional methods for assessing this bias often confuse it with the quality of the responses, as higher quality can lead to misleading score differences. By using gold judgments as benchmarks for response quality, the DBG score effectively isolates self-preference bias from quality effects. The authors conduct experiments across various LLMs and examine factors that influence bias, providing insights into the mechanisms behind self-preference in model responses.'}, 'zh': {'title': 'å¼•å…¥DBGè¯„åˆ†ï¼Œç²¾å‡†æµ‹é‡è‡ªæˆ‘åå¥½åå·®', 'desc': 'æœ¬æ–‡æå‡ºäº†DBGè¯„åˆ†ï¼Œç”¨äºæµ‹é‡å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è‡ªæˆ‘åå¥½åå·®ã€‚é€šè¿‡ä½¿ç”¨é‡‘æ ‡å‡†åˆ¤æ–­ä½œä¸ºå“åº”è´¨é‡çš„ä»£ç†ï¼ŒDBGè¯„åˆ†è§£å†³äº†å“åº”è´¨é‡å¯¹åå·®æµ‹é‡çš„æ··æ·†æ•ˆåº”ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°è‡ªæˆ‘åå¥½åå·®æ—¶ï¼Œå¾€å¾€å°†å…¶ä¸å“åº”è´¨é‡æ··ä¸ºä¸€è°ˆã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯„ä¼°äº†ä¸åŒç‰ˆæœ¬ã€è§„æ¨¡å’Œæ¨ç†èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘åå¥½åå·®ï¼Œå¹¶æ¢è®¨äº†å½±å“è¯¥åå·®çš„å› ç´ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03610', 'title': 'Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on\n  Diverse Video Games', 'url': 'https://huggingface.co/papers/2506.03610', 'abstract': 'Orak is a benchmark for training and evaluating LLM agents across diverse video games, featuring a plug-and-play interface and fine-tuning datasets to enhance agentic modules and gameplay.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present \\benchname{}, a foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce a plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects, establishing a foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak.', 'score': 7, 'issue_id': 4144, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '8821e76acd2443a8', 'authors': ['Dongmin Park', 'Minkyu Kim', 'Beongjun Choi', 'Junhyuck Kim', 'Keon Lee', 'Jonghyun Lee', 'Inkyu Park', 'Byeong-Uk Lee', 'Jaeyoung Hwang', 'Jaewoo Ahn', 'Ameya S. Mahabaleshwarkar', 'Bilal Kartal', 'Pritam Biswas', 'Yoshi Suhara', 'Kangwook Lee', 'Jaewoong Cho'], 'affiliations': ['KRAFTON', 'NVIDIA', 'Seoul National University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.03610.jpg', 'data': {'categories': ['#games', '#agents', '#video', '#benchmark', '#transfer_learning'], 'emoji': 'ğŸ®', 'ru': {'title': 'Orak: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Orak - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ğ°Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ plug-and-play Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. Orak Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 12 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¶Ğ°Ğ½Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ»Ğ¸Ğ´ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼, Ğ°Ñ€ĞµĞ½Ñ‹ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ LLM Ğ¸ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°, Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'Orak: Elevating LLM Agents in Gaming', 'desc': 'Orak is a new benchmark designed for training and evaluating Large Language Model (LLM) agents in various video games. It addresses the limitations of existing benchmarks by providing a plug-and-play interface and fine-tuning datasets that enhance the performance of LLMs in complex gameplay scenarios. The benchmark includes 12 popular video games from different genres, allowing for a thorough assessment of LLM capabilities and agentic modules. With features like game score leaderboards and detailed analyses of gameplay strategies, Orak aims to advance the development of intelligent gaming agents.'}, 'zh': {'title': 'Orakï¼šå¤šæ ·åŒ–è§†é¢‘æ¸¸æˆçš„LLMä»£ç†åŸºå‡†', 'desc': 'Orakæ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„åŸºå‡†ï¼Œæ¶µç›–å¤šç§è§†é¢‘æ¸¸æˆã€‚å®ƒæä¾›äº†å³æ’å³ç”¨çš„æ¥å£å’Œå¾®è°ƒæ•°æ®é›†ï¼Œä»¥å¢å¼ºä»£ç†æ¨¡å—å’Œæ¸¸æˆç©æ³•ã€‚ä¸ç°æœ‰åŸºå‡†ä¸åŒï¼ŒOrakæ”¯æŒ12æ¬¾æµè¡Œè§†é¢‘æ¸¸æˆï¼Œå…è®¸å¯¹LLMèƒ½åŠ›å’Œä»£ç†æ¨¡å—è¿›è¡Œå…¨é¢ç ”ç©¶ã€‚è¯¥åŸºå‡†è¿˜å¼•å…¥äº†åŸºäºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰çš„æ¥å£ï¼Œç¡®ä¿LLMèƒ½å¤Ÿä¸æ¸¸æˆæ— ç¼è¿æ¥å¹¶æ“ä½œä»£ç†æ¨¡å—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03099', 'title': 'TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models', 'url': 'https://huggingface.co/papers/2506.03099', 'abstract': 'TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/', 'score': 7, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'eff27ca5fef5cdcf', 'authors': ['Chetwin Low', 'Weimin Wang'], 'affiliations': ['Character AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.03099.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#inference', '#games', '#audio', '#video', '#optimization'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ¶Ğ¸Ğ²Ğ»ÑĞµĞ¼ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹: Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'TalkingMachines - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ DiT Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ 18 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½ÑƒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. TalkingMachines Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Transforming Audio into Real-Time Avatar Animation', 'desc': 'TalkingMachines is a novel framework that converts existing image-to-video models into real-time, audio-responsive avatar generators. It combines a large language model (LLM) with a video generation foundation model to create engaging conversational avatars. The framework features a significant adaptation of a state-of-the-art (SOTA) image-to-video model, allowing for efficient infinite video streaming through advanced knowledge distillation techniques. Additionally, it incorporates engineering optimizations to enhance performance, such as distributing processing across devices and minimizing computation delays.'}, 'zh': {'title': 'å®æ—¶éŸ³é¢‘é©±åŠ¨çš„è§’è‰²åŠ¨ç”»ç”Ÿæˆå™¨', 'desc': 'æœ¬æ–‡ä»‹ç»äº†TalkingMachinesï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶ï¼Œå°†é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬å˜ä¸ºå®æ—¶çš„éŸ³é¢‘é©±åŠ¨è§’è‰²åŠ¨ç”»ç”Ÿæˆå™¨ã€‚é€šè¿‡å°†éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹ç»“åˆï¼ŒTalkingMachinesèƒ½å¤Ÿå®ç°è‡ªç„¶çš„å¯¹è¯ä½“éªŒã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šå°†ä¸€ä¸ªé¢„è®­ç»ƒçš„æœ€å…ˆè¿›çš„å›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚é…ä¸ºä¸€ä¸ªå…·æœ‰180äº¿å‚æ•°çš„éŸ³é¢‘é©±åŠ¨å¤´åƒç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠé€šè¿‡ä¸å¯¹ç§°çŸ¥è¯†è’¸é¦å®ç°æ— é™è§†é¢‘æµçš„ç”Ÿæˆã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªé«˜ååé‡ã€ä½å»¶è¿Ÿçš„æ¨ç†ç®¡é“ï¼Œç»“åˆäº†å¤šé¡¹å…³é”®çš„å·¥ç¨‹ä¼˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03355', 'title': 'Robustness in Both Domains: CLIP Needs a Robust Text Encoder', 'url': 'https://huggingface.co/papers/2506.03355', 'abstract': 'LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.  \t\t\t\t\tAI-generated summary \t\t\t\t Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization.', 'score': 6, 'issue_id': 4140, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'd9088aaea42f6fff', 'authors': ['Elias Abad Rocamora', 'Christian Schlarmann', 'Naman Deep Singh', 'Yongtao Wu', 'Matthias Hein', 'Volkan Cevher'], 'affiliations': ['LIONS - Ecole Polytechnique Federale de Lausanne, Switzerland', 'Tubingen AI center, University of Tubingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2506.03355.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#diffusion', '#rlhf', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'LEAF: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ CLIP Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼', 'desc': 'LEAF - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ (adversarial finetuning), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² CLIP. ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (zero-shot accuracy) Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. LEAF ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑˆÑƒĞ¼Ğ°.'}, 'en': {'title': 'Enhancing Text Encoder Robustness with LEAF', 'desc': 'This paper introduces LEAF, a novel adversarial finetuning method designed to enhance the robustness of CLIP text encoders against adversarial attacks. By addressing the vulnerability of text embeddings, LEAF significantly boosts zero-shot accuracy and improves performance in multimodal retrieval tasks, even under adversarial noise. The method not only preserves the strong performance of image encoders but also enhances the quality of text-to-image generation. Overall, LEAF fills a critical gap in the literature by ensuring that text encoders are as robust as their image counterparts, leading to better model performance in various applications.'}, 'zh': {'title': 'LEAFï¼šæå‡CLIPæ–‡æœ¬ç¼–ç å™¨é²æ£’æ€§çš„å¯¹æŠ—å¾®è°ƒæ–¹æ³•', 'desc': 'LEAFæ˜¯ä¸€ç§å¯¹æŠ—å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºCLIPæ–‡æœ¬ç¼–ç å™¨çš„é²æ£’æ€§ã€‚é€šè¿‡å¯¹æŠ—å™ªå£°çš„è®­ç»ƒï¼ŒLEAFæ˜¾è‘—æé«˜äº†æ–‡æœ¬é¢†åŸŸçš„é›¶-shotå‡†ç¡®ç‡å’Œå¤šæ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿æŒäº†å›¾åƒç¼–ç å™¨çš„è§†è§‰æ€§èƒ½ï¼Œè¿˜èƒ½åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­æå‡ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¡«è¡¥äº†æ–‡æœ¬ç¼–ç å™¨é²æ£’æ€§ç ”ç©¶çš„ç©ºç™½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03106', 'title': 'Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback', 'url': 'https://huggingface.co/papers/2506.03106', 'abstract': 'Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.', 'score': 6, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '99f5fb3b08ab4205', 'authors': ['Xiaoying Zhang', 'Hao Sun', 'Yipeng Zhang', 'Kaituo Feng', 'Chaochao Lu', 'Chao Yang', 'Helen Meng'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong, HCCL', 'The Chinese University of Hong Kong, MMLab', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2506.03106.jpg', 'data': {'categories': ['#math', '#rl', '#reasoning', '#optimization', '#rlhf', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Critique-GRPO: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Critique-GRPO - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²ÑƒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Critique-GRPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing LLM Reasoning with Critique-GRPO: A Dual Feedback Approach', 'desc': 'Critique-GRPO is a reinforcement learning (RL) framework that enhances the reasoning abilities of large language models (LLMs) by combining numerical and natural language feedback. It addresses challenges faced by traditional RL methods that rely solely on numerical feedback, such as performance plateaus and ineffective self-reflection. By incorporating critiques in natural language, Critique-GRPO allows models to refine their responses and improve their performance on difficult tasks. Experimental results show that this approach significantly outperforms existing fine-tuning methods, achieving better results in various reasoning tasks.'}, 'zh': {'title': 'Critique-GRPOï¼šè‡ªç„¶è¯­è¨€ä¸æ•°å€¼åé¦ˆçš„å®Œç¾ç»“åˆ', 'desc': 'Critique-GRPOæ˜¯ä¸€ç§ç»“åˆæ•°å€¼åé¦ˆå’Œè‡ªç„¶è¯­è¨€åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä»…ä¾èµ–æ•°å€¼åé¦ˆæ—¶é‡åˆ°çš„æ€§èƒ½åœæ»ã€è‡ªæˆ‘åæ€æ•ˆæœæœ‰é™å’ŒæŒç»­å¤±è´¥ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨è‡ªç„¶è¯­è¨€åé¦ˆï¼ŒCritique-GRPOèƒ½å¤Ÿåœ¨æ¨¡å‹è¡¨ç°åœæ»æ—¶ï¼Œç”Ÿæˆæ­£ç¡®çš„æ”¹è¿›å»ºè®®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCritique-GRPOåœ¨å¤šä¸ªå¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¹³å‡é€šè¿‡ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03956', 'title': 'Adapt before Continual Learning', 'url': 'https://huggingface.co/papers/2506.03956', 'abstract': 'Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining existing knowledge (stability). While pre-trained models (PTMs) have become pivotal in CL, prevailing approaches freeze the PTM backbone to preserve stability, limiting their plasticity, particularly when encountering significant domain gaps in incremental tasks. Conversely, sequentially finetuning the entire PTM risks catastrophic forgetting of generalizable knowledge, exposing a critical stability-plasticity trade-off. To address this challenge, we propose Adapting PTMs before the core CL process (ACL), a novel framework that refines the PTM backbone through a plug-and-play adaptation phase before learning each new task with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by aligning embeddings with their original class prototypes while distancing them from others, theoretically and empirically shown to balance stability and plasticity. Extensive experiments demonstrate that ACL significantly improves CL performance across benchmarks and integrated methods, offering a versatile solution for PTM-based CL.', 'score': 5, 'issue_id': 4138, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '33f8cb4049923c1f', 'authors': ['Aojun Lu', 'Tao Feng', 'Hangjie Yuan', 'Chunhui Ding', 'Yanan Sun'], 'affiliations': ['College of Computer Science Sichuan University Chengdu, China', 'College of Computer Science and Technology Zhejiang University Hangzhou, China', 'Department of Computer Science and Technology Tsinghua University Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03956.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Continual Learning) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ACL (Adapting Pre-trained Models before the core CL process), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ACL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ñ Ğ¸Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ACL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Learning Flexibility with Pre-trained Models', 'desc': 'This paper introduces a new method called Adapting Pre-trained Models before the core Continual Learning (CL) process, which aims to improve how neural networks learn new information while keeping what they already know. The authors highlight the common issue where pre-trained models are often frozen to maintain stability, which limits their ability to adapt to new tasks. Their approach involves refining the pre-trained model before learning new tasks, allowing for better alignment of knowledge and reducing the risk of forgetting previous information. The results show that this method enhances the performance of CL systems, making it a promising solution for integrating pre-trained models in continual learning scenarios.'}, 'zh': {'title': 'æå‡æŒç»­å­¦ä¹ çš„å¯å¡‘æ€§ä¸ç¨³å®šæ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºåœ¨æ ¸å¿ƒæŒç»­å­¦ä¹ è¿‡ç¨‹ä¹‹å‰è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹ï¼ˆACLï¼‰ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æé«˜ç¥ç»ç½‘ç»œçš„å¯å¡‘æ€§ï¼ŒåŒæ—¶ä¿æŒå…¶ç¨³å®šæ€§ï¼Œä»¥ä¾¿åœ¨å¢é‡å­¦ä¹ ä¸­æ›´å¥½åœ°é€‚åº”æ–°çŸ¥è¯†ã€‚é€šè¿‡åœ¨å­¦ä¹ æ¯ä¸ªæ–°ä»»åŠ¡ä¹‹å‰å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé€‚åº”æ€§è°ƒæ•´ï¼ŒACLèƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹é½åµŒå…¥ä¸åŸå§‹ç±»åˆ«åŸå‹ï¼Œä»è€Œå‡å°‘ç¾éš¾æ€§é—å¿˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æŒç»­å­¦ä¹ çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21541', 'title': 'DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2505.21541', 'abstract': 'DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.', 'score': 5, 'issue_id': 4133, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ', 'en': 'May 24', 'zh': '5æœˆ24æ—¥'}, 'hash': 'cda6015909393ad0', 'authors': ['Zitong Wang', 'Hang Zhao', 'Qianyu Zhou', 'Xuequan Lu', 'Xiangtai Li', 'Yiren Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.21541.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#cv', '#dataset'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'DiffDecompose - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒĞ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞ»Ğ¸ÑÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AlphaBlend Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. DiffDecompose Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Image Layer Decomposition with DiffDecompose', 'desc': "This paper introduces DiffDecompose, a novel framework that uses diffusion Transformers to decompose images into their individual layers, particularly focusing on transparent and semi-transparent layers. The authors highlight the limitations of existing methods in handling complex occlusions and propose a new dataset called AlphaBlend, which is designed to support various real-world image decomposition tasks. DiffDecompose employs In-Context Decomposition to predict multiple layers without needing direct supervision for each layer, enhancing its ability to generalize across different scenarios. The framework's effectiveness is validated through extensive experiments on the AlphaBlend dataset and the public LOGO dataset, showcasing its potential in image processing applications."}, 'zh': {'title': 'é€æ˜å±‚åˆ†è§£çš„æ–°çªç ´ï¼šDiffDecompose', 'desc': 'DiffDecompose æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£ Transformer çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å›¾åƒåˆ†è§£ä¸ºç»„æˆå±‚ï¼Œå¹¶ä½¿ç”¨è¯­ä¹‰æç¤ºæ¥è§£å†³é€æ˜å±‚åˆ†è§£ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é’ˆå¯¹åŠé€æ˜å’Œé€æ˜å›¾å±‚çš„éçº¿æ€§é®æŒ¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼šé€å±‚åˆ†è§£ alpha åˆæˆå›¾åƒã€‚ä¸ºäº†è§£å†³å±‚æ¨¡ç³Šã€æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬é¦–æ¬¡å¼•å…¥äº† AlphaBlend æ•°æ®é›†ï¼Œæ”¯æŒå¤šç§å®é™…åº”ç”¨åœºæ™¯ã€‚DiffDecompose é€šè¿‡ä¸Šä¸‹æ–‡åˆ†è§£çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰é€å±‚ç›‘ç£çš„æƒ…å†µä¸‹é¢„æµ‹ä¸€ä¸ªæˆ–å¤šä¸ªå±‚ï¼Œå±•ç¤ºäº†å…¶åœ¨å›¾åƒåˆ†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03448', 'title': 'RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions', 'url': 'https://huggingface.co/papers/2506.03448', 'abstract': 'RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly. To overcome this limitation, we introduce RefEdit -- an instruction-based editing model trained on our scalable synthetic data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving state-of-the-art results comparable to closed-source methods. We release data \\& checkpoint for reproducibility.', 'score': 4, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '717f877ff02ce882', 'authors': ['Bimsara Pathiraja', 'Maitreya Patel', 'Shivam Singh', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03448.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#cv', '#optimization', '#open_source'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'RefEdit: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'RefEdit - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RefEdit-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. RefEdit, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 20 000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Image Editing with Instruction-Based Learning', 'desc': 'RefEdit is a new model designed for editing images based on instructions, specifically focusing on complex scenes with multiple objects. Unlike previous models that struggle with such tasks, RefEdit is trained on a unique synthetic data generation pipeline, allowing it to learn effectively from a smaller dataset of 20,000 editing examples. The model significantly outperforms existing baselines, which were trained on millions of samples, in both referring expression tasks and traditional editing benchmarks. This advancement highlights the potential of instruction-based editing in achieving high performance in challenging image editing scenarios.'}, 'zh': {'title': 'RefEditï¼šå¤æ‚åœºæ™¯ç¼–è¾‘çš„æ–°çªç ´', 'desc': 'RefEditæ˜¯ä¸€ç§åŸºäºæŒ‡ä»¤çš„ç¼–è¾‘æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹å¤æ‚åœºæ™¯ä¸­çš„ç¼–è¾‘ä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒRefEditåœ¨å¤„ç†å¤šä¸ªå®ä½“çš„å¤æ‚åœºæ™¯æ—¶è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†RefEdit-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºRefCOCOçš„çœŸå®ä¸–ç•ŒåŸºå‡†ï¼Œç”¨äºé‡åŒ–ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚é€šè¿‡ä½¿ç”¨åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼ŒRefEditåœ¨ä»…ä½¿ç”¨20,000ä¸ªç¼–è¾‘ä¸‰å…ƒç»„çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†åŸºäºFlux/SD3æ¨¡å‹çš„åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨æŒ‡ä»£è¡¨è¾¾ä»»åŠ¡å’Œä¼ ç»ŸåŸºå‡†ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02945', 'title': 'Quantitative LLM Judges', 'url': 'https://huggingface.co/papers/2506.02945', 'abstract': "LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling.", 'score': 4, 'issue_id': 4133, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'de4ea9c8e4abb76a', 'authors': ['Aishwarya Sahoo', 'Jeevana Kruthi Karnuthala', 'Tushar Parmanand Budhwani', 'Pranchal Agarwal', 'Sankaran Vaidyanathan', 'Alexa Siu', 'Franck Dernoncourt', 'Jennifer Healey', 'Nedim Lipka', 'Ryan Rossi', 'Uttaran Bhattacharya', 'Branislav Kveton'], 'affiliations': ['Adobe Research', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.02945.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#rlhf', '#dataset'], 'emoji': 'âš–ï¸', 'ru': {'title': 'LLM-ÑÑƒĞ´ÑŒĞ¸: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LLM-as-a-judge, Ğ³Ğ´Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… LLM-ÑÑƒĞ´ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑƒĞ´ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑƒĞ´ÑŒĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑƒĞ´ÑŒĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¸Ğ»Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑƒĞ´ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ.'}, 'en': {'title': 'Enhancing LLM Evaluation with Quantitative Judges', 'desc': "The paper introduces a framework called LLM-as-a-judge, where a large language model (LLM) assesses the outputs of another LLM. It focuses on creating quantitative LLM judges that align their evaluation scores with human assessments using regression models. These models enhance the original judge's scoring by leveraging its textual evaluations and scores. The framework is shown to be more computationally and statistically efficient than traditional supervised fine-tuning, especially when human feedback is scarce, and is validated through experiments on multiple datasets."}, 'zh': {'title': 'åˆ©ç”¨LLMæå‡è¯„ä¼°æ•ˆç‡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLLM-as-a-judgeçš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è¯„ä¼°å¦ä¸€ä¸ªLLMçš„è¾“å‡ºã€‚æˆ‘ä»¬å¼•å…¥äº†å®šé‡LLMè¯„ä¼°è€…ï¼Œé€šè¿‡å›å½’æ¨¡å‹å°†ç°æœ‰è¯„ä¼°è€…çš„è¯„åˆ†ä¸äººç±»è¯„åˆ†å¯¹é½ã€‚è¯¥æ¨¡å‹é€šè¿‡ä½¿ç”¨è¯„ä¼°è€…çš„æ–‡æœ¬è¯„ä»·å’Œè¯„åˆ†æ¥æé«˜åŸå§‹è¯„ä¼°è€…çš„è¯„åˆ†ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºç›‘ç£å¾®è°ƒï¼Œå¹¶ä¸”åœ¨äººå·¥åé¦ˆæœ‰é™çš„æƒ…å†µä¸‹ï¼Œç»Ÿè®¡æ•ˆç‡æ›´é«˜ï¼Œé€‚ç”¨äºå¤§å¤šæ•°åº”ç”¨åœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02294', 'title': 'Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation', 'url': 'https://huggingface.co/papers/2506.02294', 'abstract': 'A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  \t\t\t\t\tAI-generated summary \t\t\t\t Large foundation models trained on extensive datasets demonstrate strong zero-shot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet a robust teacher is available, is it possible for a student to also become robust to them? We address this problem by introducing a novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines', 'score': 4, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '740d99ccc158d514', 'authors': ['Niclas Popp', 'Kevin Alexander Laube', 'Matthias Hein', 'Lukas Schott'], 'affiliations': ['Bosch Center for Artificial Intelligence', 'University of TÃ¼bingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.02294.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#training', '#optimization', '#diffusion', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ´Ğ²Ğ¸Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ¸Ñ…ÑƒĞ´ÑˆĞ¸Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ… Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… CelebA Ğ¸ SpuCo Birds. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸.'}, 'en': {'title': 'Boosting Student Robustness with Diffusion Data Augmentation', 'desc': 'This paper presents a new data augmentation method using diffusion processes to enhance knowledge distillation. The approach generates challenging samples that help student networks learn to be more robust against spurious features that may not appear during testing. By maximizing the disagreement between a robust teacher model and the student model, the method effectively prepares the student for real-world scenarios where data may shift. Experiments show that this strategy improves accuracy and resilience against spurious features in various datasets, outperforming existing methods.'}, 'zh': {'title': 'åŸºäºæ‰©æ•£çš„æ•°æ®å¢å¼ºæå‡çŸ¥è¯†è’¸é¦é²æ£’æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä»¥æé«˜çŸ¥è¯†è’¸é¦ä¸­çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œå¢å¼ºäº†å­¦ç”Ÿç½‘ç»œå¯¹è™šå‡ç‰¹å¾çš„æŠµæŠ—åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨CelebAå’ŒSpuCo Birdsæ•°æ®é›†ä¸Šï¼Œè¯¥ç­–ç•¥æ˜¾è‘—æé«˜äº†æœ€å·®ç»„å’Œå¹³å‡ç»„çš„å‡†ç¡®ç‡ã€‚é€šè¿‡æœ€å¤§åŒ–æ•™å¸ˆå’Œå­¦ç”Ÿä¹‹é—´çš„åˆ†æ­§ï¼Œæœ¬æ–‡æœ‰æ•ˆåœ°è§£å†³äº†çŸ¥è¯†è’¸é¦ä¸­çš„åå˜é‡åç§»é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00482', 'title': 'BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.00482', 'abstract': 'BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.', 'score': 4, 'issue_id': 4137, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '0f7e970118d80f26', 'authors': ['Eunsu Kim', 'Haneul Yoo', 'Guijin Son', 'Hitesh Patel', 'Amit Agarwal', 'Alice Oh'], 'affiliations': ['KAIST', 'OnelineAI', 'Oracle', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00482.jpg', 'data': {'categories': ['#optimization', '#dataset', '#survey', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'BenchHub: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'BenchHub - ÑÑ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 303 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· 38 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. BenchHub Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ¸Ğ»Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'BenchHub: Streamlining Domain-Specific Evaluations for LLMs', 'desc': 'BenchHub is a repository designed to organize and classify datasets specifically for evaluating large language models (LLMs). It addresses the challenge of scattered and hard-to-manage datasets, which complicate domain-specific evaluations. By aggregating 303K questions across 38 benchmarks, BenchHub allows for flexible and customizable assessments tailored to various domains. The paper highlights the importance of domain-aware benchmarking, showing that model performance can vary significantly based on the specific dataset used.'}, 'zh': {'title': 'BenchHubï¼šæå‡è¯­è¨€æ¨¡å‹è¯„ä¼°çš„åŠ¨æ€åŸºå‡†åº“', 'desc': 'BenchHubæ˜¯ä¸€ä¸ªåŠ¨æ€åŸºå‡†åº“ï¼Œä¸“é—¨ç”¨äºèšåˆå’Œåˆ†ç±»å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿›ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°å¹¶æ”¹å–„æ¨¡å‹æ¯”è¾ƒã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸æ–­è¿›æ­¥ï¼Œæ›´æ–°å’Œç»„ç»‡è‰¯å¥½çš„åŸºå‡†å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚BenchHubé›†æˆäº†æ¥è‡ª38ä¸ªåŸºå‡†çš„303Ké—®é¢˜ï¼Œæ”¯æŒæŒç»­æ›´æ–°å’Œå¯æ‰©å±•çš„æ•°æ®ç®¡ç†ï¼Œå…è®¸æ ¹æ®ä¸åŒé¢†åŸŸæˆ–ç”¨ä¾‹è¿›è¡Œçµæ´»çš„è¯„ä¼°ã€‚é€šè¿‡å¯¹ä¸åŒè¯­è¨€æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ¨¡å‹æ€§èƒ½åœ¨ç‰¹å®šé¢†åŸŸå­é›†ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†é¢†åŸŸæ„ŸçŸ¥åŸºå‡†çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23807', 'title': 'DLP: Dynamic Layerwise Pruning in Large Language Models', 'url': 'https://huggingface.co/papers/2505.23807', 'abstract': 'A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  \t\t\t\t\tAI-generated summary \t\t\t\t Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.', 'score': 4, 'issue_id': 4133, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'a817afc0cdd35d8d', 'authors': ['Yuli Chen', 'Bo Cheng', 'Jiale Han', 'Yingying Zhang', 'Yingting Li', 'Shuhao Zhang'], 'affiliations': ['Hong Kong University of Science and Technology, Hong Kong, China', 'State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23807.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ (DLP) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DLP Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²ĞµÑĞ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑÑ…. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DLP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Dynamic Layerwise Pruning: Smart Sparsity for Language Models', 'desc': 'This paper introduces a new method called Dynamic Layerwise Pruning (DLP) that improves the efficiency of large language models (LLMs) by adaptively determining the importance of each layer. Unlike traditional pruning methods that apply uniform strategies, DLP combines model weights and activation data to assign specific pruning rates to different layers. This approach helps maintain model performance even at high levels of sparsity, which is crucial for effective model compression. Experimental results demonstrate that DLP significantly enhances accuracy and reduces perplexity in LLMs compared to existing techniques.'}, 'zh': {'title': 'åŠ¨æ€å‰ªæï¼Œæ™ºèƒ½ä¿æŒæ€§èƒ½ï¼', 'desc': 'åŠ¨æ€å±‚çº§å‰ªææ–¹æ³•é€šè¿‡ç»“åˆæ¨¡å‹æƒé‡å’Œæ¿€æ´»ä¿¡æ¯ï¼Œè‡ªé€‚åº”åœ°ç¡®å®šæ¯ä¸€å±‚çš„é‡è¦æ€§ï¼Œä»è€Œåœ¨é«˜ç¨€ç–æ€§ä¸‹ä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„å‰ªææŠ€æœ¯é€šå¸¸é‡‡ç”¨å‡åŒ€å±‚çº§å‰ªæç­–ç•¥ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨é«˜ç¨€ç–æ€§æ°´å¹³ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚åŠ¨æ€å±‚çº§å‰ªæï¼ˆDLPï¼‰æ–¹æ³•å…‹æœäº†è¿™ä¸€é™åˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥æ¿€æ´»ä¿¡æ¯åŠ¨æ€è°ƒæ•´å‰ªæç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDLPåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æœ‰æ•ˆåœ°ä¿æŒäº†é«˜ç¨€ç–æ€§ä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04133', 'title': 'TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2506.04133', 'abstract': 'A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment.', 'score': 3, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'a0a258935ed39508', 'authors': ['Shaina Raza', 'Ranjan Sapkota', 'Manoj Karkee', 'Christos Emmanouilidis'], 'affiliations': ['Cornell University, USA', 'University of Groningen, Netherlands', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2506.04133.jpg', 'data': {'categories': ['#training', '#architecture', '#survey', '#agents', '#multimodal', '#security', '#alignment', '#benchmark', '#interpretability'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ğµ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸ĞµĞ¼, Ñ€Ğ¸ÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ (TRiSM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, ModelOps Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ/Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ³Ñ€Ğ¾Ğ· Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² LLM.'}, 'en': {'title': 'Navigating Trust and Security in Agentic AI Systems', 'desc': 'This paper reviews the management of trust, risk, and security in multi-agent systems that use large language models (LLMs). It discusses how these agentic AI systems differ from traditional AI, focusing on their ability to operate autonomously and collaboratively. The authors outline four key areas of Trust, Risk, and Security Management (TRiSM): governance, explainability, ModelOps, and privacy/security, providing a framework for understanding the unique challenges these systems face. The paper also highlights the importance of building trust and ensuring transparency in these systems, while proposing future research directions for responsible deployment.'}, 'zh': {'title': 'æ„å»ºå®‰å…¨é€æ˜çš„ä»£ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿ', 'desc': 'æœ¬æ–‡å›é¡¾äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ä¿¡ä»»ã€é£é™©å’Œå®‰å…¨ç®¡ç†ï¼ˆTRiSMï¼‰ã€‚æˆ‘ä»¬åˆ†æäº†ä»£ç†äººå·¥æ™ºèƒ½çš„æ¦‚å¿µåŸºç¡€åŠå…¶ä¸ä¼ ç»Ÿäººå·¥æ™ºèƒ½ä»£ç†çš„æ¶æ„å·®å¼‚ï¼Œå¹¶æ¢è®¨äº†æ”¯æŒå¯æ‰©å±•è‡ªä¸»æ€§çš„ç³»ç»Ÿè®¾è®¡ã€‚æ–‡ç« è¯¦ç»†é˜è¿°äº†TRiSMçš„å››ä¸ªæ”¯æŸ±ï¼šæ²»ç†ã€å¯è§£é‡Šæ€§ã€æ¨¡å‹æ“ä½œå’Œéšç§/å®‰å…¨ï¼Œå¹¶ä¸ºä»£ç†LLMæä¾›äº†å…·ä½“çš„èƒŒæ™¯ã€‚æœ€åï¼Œæå‡ºäº†è´Ÿè´£ä»»çš„ä»£ç†äººå·¥æ™ºèƒ½çš„è·¯çº¿å›¾ï¼Œå»ºè®®ç ”ç©¶æ–¹å‘ä»¥ç¡®ä¿æ–°å…´å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å®‰å…¨ã€é€æ˜å’Œè´Ÿè´£ä»»çš„éƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03837', 'title': 'HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature\n  Superconductors for AI-Driven Critical Temperature Prediction', 'url': 'https://huggingface.co/papers/2506.03837', 'abstract': 'HTSC-2025, a benchmark dataset for high-temperature superconducting materials, is presented to facilitate AI-based discovery in this field.  \t\t\t\t\tAI-generated summary \t\t\t\t The discovery of high-temperature superconducting materials holds great significance for human industry and daily life. In recent years, research on predicting superconducting transition temperatures using artificial intelligence~(AI) has gained popularity, with most of these tools claiming to achieve remarkable accuracy. However, the lack of widely accepted benchmark datasets in this field has severely hindered fair comparisons between different AI algorithms and impeded further advancement of these methods. In this work, we present the HTSC-2025, an ambient-pressure high-temperature superconducting benchmark dataset. This comprehensive compilation encompasses theoretically predicted superconducting materials discovered by theoretical physicists from 2023 to 2025 based on BCS superconductivity theory, including the renowned X_2YH_6 system, perovskite MXH_3 system, M_3XH_8 system, cage-like BCN-doped metal atomic systems derived from LaH_{10} structural evolution, and two-dimensional honeycomb-structured systems evolving from MgB_2. The HTSC-2025 benchmark has been open-sourced at https://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This benchmark holds significant importance for accelerating the discovery of superconducting materials using AI-based methods.', 'score': 3, 'issue_id': 4144, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'e8db15217cfd8461', 'authors': ['Xiao-Qi Han', 'Ze-Feng Gao', 'Xin-De Wang', 'Zhenfeng Ouyang', 'Peng-Jie Guo', 'Zhong-Yi Lu'], 'affiliations': ['Hefei National Laboratory, Hefei 230088, China', 'Key Laboratory of Quantum State Construction and Manipulation (Ministry of Education), Renmin University of China, Beijing 100872, China', 'School of Physics and Beijing Key Laboratory of Opto-electronic Functional Materials & Micro-nano Devices, Renmin University of China, Beijing 100872, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03837.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#open_source', '#science'], 'emoji': 'âš¡', 'ru': {'title': 'HTSC-2025: ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² ÑĞ²ĞµÑ€Ñ…Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… HTSC-2025 Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ñ‹Ñ… ÑĞ²ĞµÑ€Ñ…Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ²ĞµÑ€Ñ…Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ°Ğ¼Ğ¸-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ñ 2023 Ğ¿Ğ¾ 2025 Ğ³Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ‘ĞšĞ¨. HTSC-2025 Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ X_2YH_6, Ğ¿ĞµÑ€Ğ¾Ğ²ÑĞºĞ¸Ñ‚Ğ½ÑƒÑ MXH_3, M_3XH_8 Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ÑĞ²ĞµÑ€Ñ…Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ˜Ğ˜.'}, 'en': {'title': 'Accelerating Superconductor Discovery with HTSC-2025', 'desc': 'The paper introduces HTSC-2025, a new benchmark dataset designed for high-temperature superconducting materials to enhance AI-driven research in this area. It addresses the current challenge of insufficient benchmark datasets, which limits the ability to compare different AI algorithms effectively. The dataset includes a variety of theoretically predicted superconductors, derived from advanced theories and recent discoveries. By providing this resource, the authors aim to facilitate faster and more accurate discoveries of superconducting materials using artificial intelligence techniques.'}, 'zh': {'title': 'HTSC-2025ï¼šåŠ é€Ÿé«˜æ¸©è¶…å¯¼ææ–™å‘ç°çš„åŸºå‡†æ•°æ®é›†', 'desc': 'HTSC-2025æ˜¯ä¸€ä¸ªç”¨äºé«˜æ¸©è¶…å¯¼ææ–™çš„åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿›åŸºäºäººå·¥æ™ºèƒ½çš„å‘ç°ã€‚è¯¥æ•°æ®é›†åŒ…å«äº†2023è‡³2025å¹´é—´ç†è®ºç‰©ç†å­¦å®¶é¢„æµ‹çš„è¶…å¯¼ææ–™ï¼ŒåŸºäºBCSè¶…å¯¼ç†è®ºã€‚é€šè¿‡æä¾›ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†ï¼ŒHTSC-2025å¯ä»¥å¸®åŠ©ä¸åŒçš„AIç®—æ³•è¿›è¡Œå…¬å¹³æ¯”è¾ƒï¼Œä»è€Œæ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚è¯¥æ•°æ®é›†å·²å¼€æºï¼Œå¹¶å°†æŒç»­æ›´æ–°ï¼Œä»¥åŠ é€Ÿè¶…å¯¼ææ–™çš„å‘ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03566', 'title': 'POSS: Position Specialist Generates Better Draft for Speculative\n  Decoding', 'url': 'https://huggingface.co/papers/2506.03566', 'abstract': 'Position Specialists (PosS) enhance Large Language Model (LLM) inference by using position-specialized draft layers to improve token prediction accuracy and acceptance rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.', 'score': 3, 'issue_id': 4149, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '16e2306b3d98ed8c', 'authors': ['Langlin Huang', 'Chengsong Huang', 'Jixuan Leng', 'Di Huang', 'Jiaxin Huang'], 'affiliations': ['Carnegie Mellon University', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2506.03566.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ñ‹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Position Specialists (PosS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). PosS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ…, Ğ³Ğ´Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Llama Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PosS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ğ¸ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ñƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Token Prediction with Position Specialists', 'desc': 'This paper introduces Position Specialists (PosS), a method that enhances the performance of Large Language Models (LLMs) during inference. By utilizing position-specialized draft layers, PosS improves the accuracy of token predictions, particularly at later positions where traditional methods struggle due to error accumulation. The approach allows each specialist to focus on specific positions, leading to higher acceptance rates for generated tokens. Experimental results show that PosS outperforms existing methods in both acceptance length and inference speed across multiple datasets.'}, 'zh': {'title': 'ä½ç½®ä¸“å®¶æå‡è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºä½ç½®ä¸“å®¶ï¼ˆPosSï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨ä½ç½®ä¸“ç”¨çš„è‰ç¨¿å±‚æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½ã€‚ä½ç½®ä¸“å®¶èƒ½å¤Ÿåœ¨ç‰¹å®šä½ç½®ç”Ÿæˆæ›´å‡†ç¡®çš„æ ‡è®°ï¼Œä»è€Œæé«˜æ ‡è®°çš„æ¥å—ç‡ï¼Œå°¤å…¶æ˜¯åœ¨åæœŸä½ç½®ã€‚é€šè¿‡ä¸“æ³¨äºå¤„ç†è‰ç¨¿æ¨¡å‹ç‰¹å¾çš„åå·®ï¼Œæ¯ä¸ªä¸“å®¶å¯ä»¥æœ‰æ•ˆå‡å°‘é”™è¯¯ç´¯ç§¯å¸¦æ¥çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPosSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œæå‡äº†å¹³å‡æ¥å—é•¿åº¦å’ŒåŠ é€Ÿæ¯”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03525', 'title': 'Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.03525', 'abstract': 'Video-SKoT framework improves domain-adaptive video reasoning by constructing skill-aware Chain-of-Thought supervisions and specialized expert modules.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Chain-of-Thought (CoT) reasoning have improved complex video understanding, but existing methods often struggle to adapt to domain-specific skills (e.g., event detection, spatial relation understanding, emotion understanding) over various video content. To address this, we propose Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning. First, we construct skill-based CoT annotations: we extract domain-relevant reasoning skills from training questions, cluster them into a shared skill taxonomy, and create detailed multi-step CoT rationale tailored to each video-question pair for training. Second, we introduce a skill-specific expert learning framework. Each expert module specializes in a subset of reasoning skills and is trained with lightweight adapters using the collected CoT supervision. We demonstrate the effectiveness of the proposed approach on three video understanding benchmarks, where Video-SKoT consistently outperforms strong baselines. We also provide in-depth analyses on comparing different CoT annotation pipelines and learned skills over multiple video domains.', 'score': 3, 'issue_id': 4145, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '799fe0c792fb72aa', 'authors': ['Daeun Lee', 'Jaehong Yoon', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.03525.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµĞ¼ ĞºĞ°Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Video-SKoT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought), ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Video-SKoT Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Video Reasoning with Skill-Aware Chain-of-Thought', 'desc': 'The Video-SKoT framework enhances video reasoning by focusing on specific skills needed for understanding different video content. It creates skill-aware Chain-of-Thought (CoT) supervisions that guide the model in reasoning about events, spatial relations, and emotions in videos. By clustering relevant reasoning skills and developing specialized expert modules, the framework allows for more effective domain adaptation. The results show that Video-SKoT outperforms existing methods on various benchmarks, demonstrating its ability to improve video understanding through targeted skill training.'}, 'zh': {'title': 'æŠ€èƒ½æ„ŸçŸ¥çš„è§†é¢‘æ¨ç†æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVideo-SKoTçš„è§†é¢‘æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ„å»ºæŠ€èƒ½æ„ŸçŸ¥çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£å’Œä¸“é—¨çš„ä¸“å®¶æ¨¡å—æ¥æ”¹å–„é¢†åŸŸè‡ªé€‚åº”è§†é¢‘æ¨ç†ã€‚è¯¥æ¡†æ¶é¦–å…ˆä»è®­ç»ƒé—®é¢˜ä¸­æå–ä¸é¢†åŸŸç›¸å…³çš„æ¨ç†æŠ€èƒ½ï¼Œå¹¶å°†å…¶èšç±»æˆå…±äº«çš„æŠ€èƒ½åˆ†ç±»æ³•ï¼Œè¿›è€Œä¸ºæ¯ä¸ªè§†é¢‘-é—®é¢˜å¯¹åˆ›å»ºè¯¦ç»†çš„å¤šæ­¥éª¤CoTæ¨ç†ã€‚å…¶æ¬¡ï¼Œæ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªæŠ€èƒ½ç‰¹å®šçš„ä¸“å®¶å­¦ä¹ æœºåˆ¶ï¼Œæ¯ä¸ªä¸“å®¶æ¨¡å—ä¸“æ³¨äºä¸€éƒ¨åˆ†æ¨ç†æŠ€èƒ½ï¼Œå¹¶ä½¿ç”¨æ”¶é›†åˆ°çš„CoTç›‘ç£è¿›è¡Œè½»é‡çº§é€‚é…å™¨è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-SKoTåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¼ºåŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02863', 'title': 'CapSpeech: Enabling Downstream Applications in Style-Captioned\n  Text-to-Speech', 'url': 'https://huggingface.co/papers/2506.02863', 'abstract': 'CapSpeech introduces a large benchmark dataset for various captioned text-to-speech tasks, facilitating advancements in style, accent, emotion, and chat-agent synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.', 'score': 3, 'issue_id': 4151, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'bd41abf94e2ec641', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#benchmark', '#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'CapSpeech: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸', 'desc': 'CapSpeech Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ 0,36 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞºÑÑ‚. CapSpeech Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ñ, Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ°, ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ.'}, 'en': {'title': 'CapSpeech: Advancing Text-to-Speech with Comprehensive Datasets', 'desc': 'CapSpeech is a new benchmark dataset aimed at improving style-captioned text-to-speech synthesis (CapTTS) by providing a comprehensive resource for various tasks. It includes over 10 million machine-annotated and nearly 0.36 million human-annotated audio-caption pairs, covering aspects like style, accent, emotion, and chat-agent synthesis. The dataset supports multiple CapTTS-related tasks, such as CapTTS with sound events and emotion-captioned TTS, facilitating advancements in real-world applications. Experiments conducted on CapSpeech show promising results in generating high-quality, intelligible speech across diverse speaking styles.'}, 'zh': {'title': 'CapSpeechï¼šæ¨åŠ¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆçš„æ–°æ—¶ä»£', 'desc': 'CapSpeechæ˜¯ä¸€ä¸ªå¤§å‹åŸºå‡†æ•°æ®é›†ï¼Œä¸“ä¸ºå„ç§å¸¦å­—å¹•çš„æ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡è€Œè®¾è®¡ï¼Œæ—¨åœ¨æ¨åŠ¨é£æ ¼ã€å£éŸ³ã€æƒ…æ„Ÿå’ŒèŠå¤©ä»£ç†åˆæˆçš„è¿›æ­¥ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡1000ä¸‡ä¸ªæœºå™¨æ ‡æ³¨çš„éŸ³é¢‘-å­—å¹•å¯¹å’Œè¿‘36ä¸‡ä¸ªäººå·¥æ ‡æ³¨çš„éŸ³é¢‘-å­—å¹•å¯¹ï¼Œæ”¯æŒå¤šç§CapTTSç›¸å…³ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç”±ä¸“ä¸šé…éŸ³æ¼”å‘˜å’Œç»éªŒä¸°å¯Œçš„éŸ³é¢‘å·¥ç¨‹å¸ˆæ”¶é›†å’Œå½•åˆ¶çš„ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºèŠå¤©ä»£ç†å’Œå¸¦å£°éŸ³äº‹ä»¶çš„CapTTSä»»åŠ¡ã€‚é€šè¿‡å¯¹CapSpeechè¿›è¡Œçš„å®éªŒï¼Œå±•ç¤ºäº†åœ¨å¤šç§è¯´è¯é£æ ¼ä¸‹çš„é«˜ä¿çœŸå’Œé«˜å¯æ‡‚æ€§è¯­éŸ³åˆæˆã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.01344', 'title': 'Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents', 'url': 'https://huggingface.co/papers/2506.01344', 'abstract': "Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.", 'score': 3, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '788495117e4bf1d7', 'authors': ['Manan Suri', 'Puneet Mathur', 'Nedim Lipka', 'Franck Dernoncourt', 'Ryan A. Rossi', 'Vivek Gupta', 'Dinesh Manocha'], 'affiliations': ['Adobe', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.01344.jpg', 'data': {'categories': ['#graphs', '#cv', '#reasoning', '#agents', '#hallucinations', '#multimodal', '#benchmark', '#interpretability'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° FlowPathAgent Ğ´Ğ»Ñ ĞµĞµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼Ñƒ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞµĞµ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FlowExplainBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¹ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FlowPathAgent ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼Ğ°Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 10-14%.'}, 'en': {'title': 'Enhancing Flowchart Interpretation with Fine-grained Attribution', 'desc': 'This paper addresses the challenges of interpreting flowcharts using large language models (LLMs) due to their complex structures and potential for hallucination. It introduces Fine-grained Flowchart Attribution, a method that links LLM responses to specific components of flowcharts, enhancing the reliability and explainability of automated processing. The authors present FlowPathAgent, a neurosymbolic agent that utilizes graph-based reasoning to segment flowcharts and create structured symbolic graphs for dynamic interaction. Experimental results demonstrate that FlowPathAgent significantly reduces hallucinations in LLM outputs, achieving improved performance on the newly introduced FlowExplainBench benchmark.'}, 'zh': {'title': 'æå‡æµç¨‹å›¾è§£æçš„å¯é æ€§ä¸å¯è§£é‡Šæ€§', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºç»†ç²’åº¦æµç¨‹å›¾å½’å› ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†æµç¨‹å›¾æ—¶çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†FlowPathAgentï¼Œè¿™æ˜¯ä¸€ç§ç¥ç»ç¬¦å·ä»£ç†ï¼Œé€šè¿‡å›¾å½¢æ¨ç†è¿›è¡Œç»†ç²’åº¦çš„åæœŸå½’å› ã€‚è¯¥ä»£ç†é¦–å…ˆå¯¹æµç¨‹å›¾è¿›è¡Œåˆ†å‰²ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„ç¬¦å·å›¾ï¼Œå¹¶åŠ¨æ€ä¸å›¾è¿›è¡Œäº¤äº’ï¼Œä»¥ç”Ÿæˆå½’å› è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowPathAgentåœ¨æµç¨‹å›¾é—®ç­”ä¸­å‡å°‘äº†è§†è§‰å¹»è§‰ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿æé«˜äº†10-14%çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04034', 'title': 'Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2506.04034', 'abstract': 'Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings.', 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '9d3dcbdd5158f101', 'authors': ['Qing Jiang', 'Xingyu Chen', 'Zhaoyang Zeng', 'Junzhi Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'Peking University', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04034.jpg', 'data': {'categories': ['#cv', '#rl', '#training', '#reasoning', '#hallucinations', '#interpretability', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğµ Ñ€ĞµÑ„ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ„ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Rex-Thinker. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HumanRef-CoT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Rex-Thinker Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Rex-Thinker: Grounded Object Referring with Explainable Reasoning', 'desc': 'This paper introduces Rex-Thinker, a model designed to enhance object referring in images by incorporating explainable and trustworthy reasoning. Unlike traditional methods that focus solely on bounding box predictions, Rex-Thinker employs a Chain of Thought (CoT) reasoning approach to evaluate candidate objects against natural language descriptions. The model is trained on a new dataset, HumanRef-CoT, which facilitates structured reasoning through a systematic planning and summarization process. Results indicate that Rex-Thinker not only improves precision and interpretability but also effectively rejects irrelevant predictions, showcasing its robustness in various scenarios.'}, 'zh': {'title': 'Rex-Thinkerï¼šå¯è§£é‡Šçš„ç‰©ä½“æŒ‡ä»£æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç‰©ä½“æŒ‡ä»£æ¨¡å‹Rex-Thinkerï¼Œæ—¨åœ¨é€šè¿‡æ˜ç¡®çš„é“¾å¼æ¨ç†ä»»åŠ¡æ¥æ£€æµ‹ä¸è‡ªç„¶è¯­è¨€æè¿°åŒ¹é…çš„å›¾åƒä¸­çš„æ‰€æœ‰ç‰©ä½“ã€‚è¯¥æ¨¡å‹å¼ºè°ƒå¯éªŒè¯æ€§å’Œå¯ä¿¡æ€§ï¼Œç¡®ä¿å…¶é¢„æµ‹èƒ½å¤Ÿè§£é‡Šå¹¶ä¸è§†è§‰è¯æ®ç›¸è¿ã€‚Rex-Thinkeré€šè¿‡é€æ­¥æ¨ç†å€™é€‰ç‰©ä½“å®ä¾‹ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ç¬¦åˆç»™å®šçš„æè¿°ï¼Œä»è€Œåšå‡ºæœ€ç»ˆé¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾ç¡®åº¦å’Œå¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œå¹¶åœ¨æ‹’ç»è™šå‡è¾“å‡ºå’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03951', 'title': 'Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective', 'url': 'https://huggingface.co/papers/2506.03951', 'abstract': 'A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.  \t\t\t\t\tAI-generated summary \t\t\t\t The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this trade-off, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Arch, which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87% more compact in terms of parameters.', 'score': 2, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '19d2cecb53fd6998', 'authors': ['Aojun Lu', 'Hangjie Yuan', 'Tao Feng', 'Yanan Sun'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'College of Computer Science, Sichuan University, Chengdu, China', 'Department of Computer Science and Technology, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03951.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Dual-Arch Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼Ñƒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸-Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞµÑ‚Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğµ - Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. Dual-Arch Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸: Ğ¾Ğ´Ğ½Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ñ€ÑƒĞ³ÑƒÑ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ°Ğ¶Ğ´Ğ°Ñ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Dual-Arch ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¾ 87% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Dual-Arch: Balancing Stability and Plasticity in Continual Learning', 'desc': 'The paper presents a new framework called Dual-Arch that improves Continual Learning (CL) by tackling the stability-plasticity dilemma through architectural innovations. It highlights that deeper networks are better at learning new information (plasticity), while wider networks excel at retaining old knowledge (stability). By utilizing two specialized networksâ€”one focused on plasticity and the other on stabilityâ€”Dual-Arch effectively balances these competing needs. Experimental results show that this approach not only enhances the performance of existing CL methods but also reduces the model size by up to 87%.'}, 'zh': {'title': 'åŒç½‘ç»œæ¶æ„ï¼Œå¹³è¡¡å­¦ä¹ ç¨³å®šæ€§ä¸å¯å¡‘æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶Dual-Archï¼Œæ—¨åœ¨é€šè¿‡åœ¨æ¶æ„å±‚é¢è§£å†³ç¨³å®šæ€§ä¸å¯å¡‘æ€§ä¹‹é—´çš„çŸ›ç›¾æ¥å¢å¼ºæŒç»­å­¦ä¹ ã€‚æŒç»­å­¦ä¹ çš„ç›®æ ‡æ˜¯ä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿé€æ­¥å­¦ä¹ å’Œé€‚åº”æ–°çŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒå¯¹æ—§çŸ¥è¯†çš„è®°å¿†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç›¸åŒå‚æ•°çº¦æŸä¸‹ï¼Œæ·±å±‚ç½‘ç»œå…·æœ‰æ›´å¥½çš„å¯å¡‘æ€§ï¼Œè€Œå®½å±‚ç½‘ç»œåˆ™è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§ã€‚Dual-Archæ¡†æ¶ç»“åˆäº†ä¸¤ä¸ªç‹¬ç«‹ç½‘ç»œçš„ä¼˜åŠ¿ï¼Œä¸€ä¸ªä¸“æ³¨äºå¯å¡‘æ€§ï¼Œå¦ä¸€ä¸ªä¸“æ³¨äºç¨³å®šæ€§ï¼Œä»è€Œæé«˜äº†ç°æœ‰æŒç»­å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03822', 'title': 'CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents', 'url': 'https://huggingface.co/papers/2506.03822', 'abstract': "Publication databases rely on accurate metadata extraction from diverse web sources, yet variations in web layouts and data formats present challenges for metadata providers. This paper introduces CRAWLDoc, a new method for contextual ranking of linked web documents. Starting with a publication's URL, such as a digital object identifier, CRAWLDoc retrieves the landing page and all linked web resources, including PDFs, ORCID profiles, and supplementary materials. It embeds these resources, along with anchor texts and the URLs, into a unified representation. For evaluating CRAWLDoc, we have created a new, manually labeled dataset of 600 publications from six top publishers in computer science. Our method CRAWLDoc demonstrates a robust and layout-independent ranking of relevant documents across publishers and data formats. It lays the foundation for improved metadata extraction from web documents with various layouts and formats. Our source code and dataset can be accessed at https://github.com/FKarl/CRAWLDoc.", 'score': 2, 'issue_id': 4147, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'dea42a2dce3ff33b', 'authors': ['Fabian Karl', 'Ansgar Scherp'], 'affiliations': ['UniversitÃ¤t Ulm, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2506.03822.jpg', 'data': {'categories': ['#dataset', '#data'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'CRAWLDoc: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ²ĞµĞ±-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'CRAWLDoc - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. ĞĞ½ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ URL Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 600 Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ¾Ñ‚ ÑˆĞµÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸Ğ·Ğ´Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒĞº. CRAWLDoc Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'CRAWLDoc: Contextual Ranking for Enhanced Metadata Extraction', 'desc': "This paper presents CRAWLDoc, a novel approach for ranking web documents related to academic publications. It addresses the challenge of varying web layouts and formats by retrieving a publication's landing page and all associated resources, such as PDFs and profiles. CRAWLDoc creates a unified representation of these resources, enhancing the contextual understanding of linked documents. The method has been evaluated using a new dataset of 600 publications, showing its effectiveness in providing layout-independent document rankings for better metadata extraction."}, 'zh': {'title': 'CRAWLDocï¼šæå‡å…ƒæ•°æ®æå–çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•CRAWLDocï¼Œç”¨äºä»ä¸åŒçš„ç½‘ç»œæ¥æºä¸­æå–å‡†ç¡®çš„å…ƒæ•°æ®ã€‚è¯¥æ–¹æ³•ä»å‡ºç‰ˆç‰©çš„URLå¼€å§‹ï¼Œæ£€ç´¢ç›¸å…³çš„ç½‘é¡µå’Œé“¾æ¥èµ„æºï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°ä¸€ä¸ªç»Ÿä¸€çš„è¡¨ç¤ºä¸­ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªåŒ…å«600ä¸ªå‡ºç‰ˆç‰©çš„æ–°æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ï¼Œè¯„ä¼°äº†CRAWLDocçš„æœ‰æ•ˆæ€§ã€‚CRAWLDocå±•ç¤ºäº†åœ¨ä¸åŒå‡ºç‰ˆå•†å’Œæ•°æ®æ ¼å¼ä¸­ï¼Œèƒ½å¤Ÿç¨³å¥ä¸”ç‹¬ç«‹äºå¸ƒå±€åœ°å¯¹ç›¸å…³æ–‡æ¡£è¿›è¡Œæ’åï¼Œä¸ºæ”¹è¿›å…ƒæ•°æ®æå–å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03614', 'title': 'VLMs Can Aggregate Scattered Training Patches', 'url': 'https://huggingface.co/papers/2506.03614', 'abstract': 'VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions "safe," VLMs may later describe, the full image or a text reference to the scene, as "safe." We define the core ability of VLMs enabling this attack as visual stitching -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each (image, ID) pair into {(patch, ID)} pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe\'\' or ``unsafe\'\', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching.', 'score': 2, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '180b48fc19d50b80', 'authors': ['Zhanhui Zhou', 'Lingjie Chen', 'Chao Yang', 'Chaochao Lu'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.03614.jpg', 'data': {'categories': ['#open_source', '#data', '#dataset', '#multimodal', '#cv', '#benchmark', '#security', '#ethics'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: ÑĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ 'Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ' Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº VLM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…, Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ğ¸Ğ´ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ±Ñ€Ğ¾ÑĞ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ñƒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ VLM Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹."}, 'en': {'title': 'Visual Stitching: A Hidden Risk in Vision-Language Models', 'desc': 'This paper discusses a vulnerability in vision-language models (VLMs) known as visual stitching, which allows these models to reconstruct harmful content from fragmented visual information. The authors show that when dangerous images are divided into small patches and mixed with benign data, VLMs can still learn to piece them together during training. This leads to a situation where the models can generate harmful outputs by associating safe descriptions with dangerous images. The study highlights the risks of data moderation being bypassed and emphasizes the need for improved safety measures in VLMs.'}, 'zh': {'title': 'è§†è§‰æ‹¼æ¥ï¼šVLMsçš„å®‰å…¨éšæ‚£', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„è§†è§‰æ‹¼æ¥èƒ½åŠ›ï¼Œè¿™ç§èƒ½åŠ›ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•´åˆåˆ†æ•£çš„è§†è§‰ä¿¡æ¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æœ‰å®³å›¾åƒè¢«åˆ†å‰²æˆå°çš„ã€çœ‹ä¼¼æ— å®³çš„ç‰‡æ®µæ—¶ï¼Œæ•°æ®çš„å®¡æŸ¥å¯ä»¥è¢«è½»æ˜“ç»•è¿‡ã€‚VLMsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šå­¦ä¹ å°†è¿™äº›ç‰‡æ®µæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä»è€Œåœ¨æ¨ç†æ—¶ç”Ÿæˆæœ‰å®³çš„å“åº”ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒå±•ç¤ºäº†è¿™ä¸€ç°è±¡ï¼Œå¹¶æ¨¡æ‹Ÿäº†å¯¹æŠ—æ€§æ•°æ®ä¸­æ¯’çš„åœºæ™¯ï¼Œæ­ç¤ºäº†VLMsåœ¨å®‰å…¨æ€§æ–¹é¢çš„æ½œåœ¨é£é™©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02515', 'title': 'FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.02515', 'abstract': 'A new benchmark called FinChain evaluates multi-step symbolic reasoning in financial tasks with a focus on intermediate reasoning steps, introducing ChainEval as a metric for assessing both final answers and reasoning processes.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain.', 'score': 2, 'issue_id': 4142, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '0b49cede5613cc2d', 'authors': ['Zhuohan Xie', 'Dhruv Sahnan', 'Debopriyo Banerjee', 'Georgi Georgiev', 'Rushil Thareja', 'Hachem Madmoun', 'Jinyan Su', 'Aaryamonvikram Singh', 'Yuxia Wang', 'Rui Xing', 'Fajri Koto', 'Haonan Li', 'Ivan Koychev', 'Tanmoy Chakraborty', 'Salem Lahlou', 'Veselin Stoyanov', 'Preslav Nakov'], 'affiliations': ['Cornell University, USA', 'FMI, Sofia University, Bulgaria', 'IIT Delhi, India', 'MBZUAI, UAE', 'Quantsquare, France'], 'pdf_title_img': 'assets/pdf/title_img/2506.02515.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#dataset', '#benchmark'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'FinChain: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'FinChain - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 54 Ñ‚ĞµĞ¼Ñ‹ Ğ¸Ğ· 12 Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ñ Ğ¿ÑÑ‚ÑŒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ChainEval Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğº ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 30 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'FinChain: Advancing Financial Reasoning with Intermediate Steps', 'desc': 'The paper introduces FinChain, a new benchmark designed to evaluate multi-step symbolic reasoning specifically in financial tasks. Unlike existing datasets that only focus on final answers, FinChain emphasizes the importance of intermediate reasoning steps through its novel metric, ChainEval. This benchmark covers a wide range of financial topics and provides parameterized templates to assess varying levels of reasoning complexity. The findings reveal that even advanced language models struggle with multi-step reasoning in finance, highlighting the need for improved capabilities in this area.'}, 'zh': {'title': 'FinChainï¼šé‡‘èæ¨ç†çš„æ–°åŸºå‡†', 'desc': 'FinChainæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°é‡‘èä»»åŠ¡ä¸­çš„å¤šæ­¥éª¤ç¬¦å·æ¨ç†ï¼Œç‰¹åˆ«å…³æ³¨ä¸­é—´æ¨ç†æ­¥éª¤ã€‚å®ƒå¼•å…¥äº†ChainEvalä½œä¸ºè¯„ä¼°æœ€ç»ˆç­”æ¡ˆå’Œæ¨ç†è¿‡ç¨‹çš„æ–°æŒ‡æ ‡ã€‚ç°æœ‰çš„æ•°æ®é›†å¦‚FinQAå’ŒConvFinQAä»…ç›‘ç£æœ€ç»ˆçš„æ•°å€¼ç­”æ¡ˆï¼Œè€Œä¸è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤ã€‚FinChainè¦†ç›–12ä¸ªé‡‘èé¢†åŸŸçš„54ä¸ªä¸»é¢˜ï¼Œä¸ºæ¯ä¸ªä¸»é¢˜æä¾›äº”ä¸ªä¸åŒæ¨ç†å¤æ‚åº¦å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†è¦æ±‚çš„æ¨¡æ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23564', 'title': 'Segment Policy Optimization: Effective Segment-Level Credit Assignment\n  in RL for Large Language Models', 'url': 'https://huggingface.co/papers/2505.23564', 'abstract': 'The Segment Policy Optimization (SPO) framework improves large language model reasoning via reinforcement learning by offering intermediate granularity advantage estimation, balancing precision and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: Token-level methods (e.g., PPO) aim to provide the fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving 6-12 percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving 7-11 percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at https://github.com/AIFrameResearch/SPO.', 'score': 2, 'issue_id': 4146, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '25b36641cf394ee3', 'authors': ['Yiran Guo', 'Lijie Xu', 'Jie Liu', 'Dan Ye', 'Shuang Qiu'], 'affiliations': ['City University of Hong Kong', 'Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.23564.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#math', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SPO: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Segment Policy Optimization (SPO). SPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². SPO Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Segment Policy Optimization: Balancing Precision and Efficiency in RL for Language Models', 'desc': 'The Segment Policy Optimization (SPO) framework enhances the reasoning abilities of large language models through reinforcement learning by introducing an intermediate granularity for advantage estimation. This approach addresses the limitations of both token-level and trajectory-level methods, providing a more accurate credit assignment while maintaining computational efficiency. SPO incorporates flexible segment partitioning, precise segment advantage estimation, and a novel policy optimization strategy that utilizes segment advantages. The framework has been successfully applied to improve performance on tasks like GSM8K and MATH500, demonstrating significant accuracy gains over existing methods.'}, 'zh': {'title': 'æ®µç­–ç•¥ä¼˜åŒ–ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºæ®µç­–ç•¥ä¼˜åŒ–ï¼ˆSPOï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚SPOé€šè¿‡ä¸­é—´ç²’åº¦çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨ç²¾ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«çµæ´»çš„æ®µåˆ’åˆ†ã€å‡†ç¡®çš„æ®µä¼˜åŠ¿ä¼°è®¡å’ŒåŸºäºæ®µä¼˜åŠ¿çš„ç­–ç•¥ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–è¯„è®ºæ¨¡å‹çš„æƒ…å†µä¸‹å®ç°å‡†ç¡®çš„ä¼˜åŠ¿ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPOåœ¨å¤šä¸ªä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04214', 'title': 'Sounding that Object: Interactive Object-Aware Image to Audio Generation', 'url': 'https://huggingface.co/papers/2506.04214', 'abstract': 'Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an {\\em interactive object-aware audio generation} model that grounds sound generation in user-selected visual objects within images. Our method integrates object-centric learning into a conditional latent diffusion model, which learns to associate image regions with their corresponding sounds through multi-modal attention. At test time, our model employs image segmentation to allow users to interactively generate sounds at the {\\em object} level. We theoretically validate that our attention mechanism functionally approximates test-time segmentation masks, ensuring the generated audio aligns with selected objects. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds. Project page: https://tinglok.netlify.app/files/avobject/', 'score': 1, 'issue_id': 4146, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '1f6af5427e65f943', 'authors': ['Tingle Li', 'Baihe Huang', 'Xiaobin Zhuang', 'Dongya Jia', 'Jiawei Chen', 'Yuping Wang', 'Zhuo Chen', 'Gopala Anumanchipalli', 'Yuxuan Wang'], 'affiliations': ['University of California'], 'pdf_title_img': 'assets/pdf/title_img/2506.04214.jpg', 'data': {'categories': ['#audio', '#diffusion', '#multimodal'], 'emoji': 'ğŸ”Š', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ²ÑƒĞºĞ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ·Ğ²ÑƒĞºĞ°Ğ¼Ğ¸. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ²ÑƒĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ñ… Ğ·Ğ²ÑƒĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Interactive Sound Generation for Visual Objects', 'desc': 'This paper presents a novel model for generating sounds that correspond to specific visual objects in images, addressing the complexity of audio-visual scenes. The proposed interactive object-aware audio generation model uses a conditional latent diffusion approach, which learns to connect image regions with their respective sounds through a multi-modal attention mechanism. During testing, the model allows users to select objects in an image, generating sounds that are accurately aligned with those objects. The results demonstrate that this method outperforms existing models, providing a more coherent audio-visual experience.'}, 'zh': {'title': 'äº¤äº’å¼å¯¹è±¡æ„ŸçŸ¥éŸ³é¢‘ç”Ÿæˆæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§äº¤äº’å¼å¯¹è±¡æ„ŸçŸ¥éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºå¤æ‚çš„éŸ³è§†é¢‘åœºæ™¯ç”Ÿæˆå‡†ç¡®çš„å£°éŸ³ã€‚è¯¥æ¨¡å‹é€šè¿‡ç”¨æˆ·é€‰æ‹©çš„è§†è§‰å¯¹è±¡æ¥å¼•å¯¼å£°éŸ³ç”Ÿæˆï¼Œç»“åˆäº†å¯¹è±¡ä¸­å¿ƒå­¦ä¹ å’Œæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†å›¾åƒåŒºåŸŸä¸ç›¸åº”çš„å£°éŸ³å…³è”èµ·æ¥ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶é€šè¿‡å›¾åƒåˆ†å‰²å®ç°ç”¨æˆ·çš„äº¤äº’å¼å£°éŸ³ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¯¹è±¡ä¸å£°éŸ³çš„å¯¹é½æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œç”Ÿæˆçš„éŸ³é¢‘ä¸æ‰€é€‰å¯¹è±¡æ›´ä¸ºä¸€è‡´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03817', 'title': 'Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid', 'url': 'https://huggingface.co/papers/2506.03817', 'abstract': 'Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades, AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL, two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, a lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled a large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future.', 'score': 1, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '24782a46b48333d1', 'authors': ['Julius Gonsior', 'Tim RieÃŸ', 'Anja Reusch', 'Claudio Hartmann', 'Maik Thiele', 'Wolfgang Lehner'], 'affiliations': ['Hochschule fur Technik und Wirtschaft Dresden', 'Technion - Israeli Institute of Technology', 'Technische Universitat Dresden'], 'pdf_title_img': 'assets/pdf/title_img/2506.03817.jpg', 'data': {'categories': ['#training', '#data', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Active Learning, AL) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑĞ¸Ğ»Ğ¸Ñ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ² Ğ±Ğ¾Ğ»ĞµĞµ 4,6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² AL. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ° Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ AL. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ AL Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼.'}, 'en': {'title': 'Unlocking Active Learning: Simplifying Setup for Trustworthy Results', 'desc': 'This paper addresses the challenges of using Active Learning (AL) in supervised machine learning, particularly the complexities and trust issues that hinder its adoption. The authors compiled a vast hyperparameter grid with over 4.6 million combinations to analyze how different settings affect AL performance. They conducted the largest AL study to date, recording the results and examining the impact of each hyperparameter on the outcomes. The findings provide insights and recommendations for setting up reproducible AL experiments, aiming to enhance the reliability and effectiveness of AL in real-world applications.'}, 'zh': {'title': 'ä¼˜åŒ–ä¸»åŠ¨å­¦ä¹ ï¼Œæå‡æ ‡æ³¨æ•ˆç‡', 'desc': 'æ ‡æ³¨æ•°æ®æ˜¯ä¸€ä¸ªè€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„ä»»åŠ¡ï¼Œä½†è¿™æ˜¯ç›‘ç£å­¦ä¹ æ‰€å¿…éœ€çš„ã€‚ä¸»åŠ¨å­¦ä¹ ï¼ˆActive Learning, ALï¼‰æ˜¯ä¸€ç§é€šè¿‡è¿­ä»£é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æœªæ ‡è®°æ ·æœ¬æ¥å‡å°‘äººå·¥æ ‡æ³¨å·¥ä½œçš„æ–¹æ³•ï¼Œä»è€Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚å°½ç®¡ALå·²ç»å­˜åœ¨äº†å‡ åå¹´ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶å¾ˆå°‘è¢«ä½¿ç”¨ã€‚æœ¬æ–‡ç ”ç©¶äº†ALä¸­è¶…å‚æ•°ç©ºé—´çš„å¤æ‚æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«460ä¸‡ç§è¶…å‚æ•°ç»„åˆçš„å¤§å‹ç½‘æ ¼ï¼Œå¹¶åˆ†æäº†æ¯ä¸ªè¶…å‚æ•°å¯¹å®éªŒç»“æœçš„å½±å“ï¼Œä»¥ä¿ƒè¿›æ›´å¯é çš„ALç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03538', 'title': 'Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting', 'url': 'https://huggingface.co/papers/2506.03538', 'abstract': 'A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released.', 'score': 1, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'c9dadfe8cdfe5d4c', 'authors': ['Chengqi Li', 'Zhihao Shi', 'Yangdi Lu', 'Wenbo He', 'Xiangyu Xu'], 'affiliations': ['Department of Computing and Software McMaster University', 'Department of Electrical and Computer Engineering McMaster University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03538.jpg', 'data': {'categories': ['#training', '#3d', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Asymmetric Dual 3DGS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ²ÑƒÑ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ 3D Gaussian Splatting Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Dynamic EMA Proxy Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Asymmetric Dual Models', 'desc': 'The paper presents the Asymmetric Dual 3DGS framework, which enhances 3D reconstruction from images taken in varied conditions. It addresses the challenges of inconsistent lighting and distracting elements by training two models simultaneously with a focus on consistency and divergence. The framework uses a unique masking strategy to prevent the models from converging on the same errors, thus improving the quality of the reconstructions. Experimental results show that this approach is more efficient and effective than current methods, leading to better performance in real-world scenarios.'}, 'zh': {'title': 'éå¯¹ç§°åŒé‡3DGSæ¡†æ¶ï¼šé«˜æ•ˆçš„3Dé‡å»ºæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„éå¯¹ç§°åŒé‡3DGSæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜3Dé‡å»ºçš„æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒä¸¤ä¸ªæ¨¡å‹å¹¶æ–½åŠ ä¸€è‡´æ€§çº¦æŸï¼Œæ¥å‡å°‘ä¸ä¸€è‡´çš„è§†è§‰ä¼ªå½±ã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šçº¿ç´¢è‡ªé€‚åº”æ©ç å’Œè‡ªç›‘ç£è½¯æ©ç ï¼Œç¡®ä¿ä¸¤ä¸ªæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒå·®å¼‚ï¼Œä»è€Œé™ä½å…±äº«é”™è¯¯æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†çœŸå®ä¸–ç•Œæ•°æ®é›†æ—¶ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œæ›´å¥½çš„é‡å»ºè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02680', 'title': 'Solving Inverse Problems with FLAIR', 'url': 'https://huggingface.co/papers/2506.02680', 'abstract': 'FLAIR, a novel training-free variational framework, leverages flow-based generative models to enhance inverse problem solutions, achieving superior reconstruction quality and sample diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Flow-based latent generative models such as Stable Diffusion 3 are able to generate images with remarkable quality, even enabling photorealistic text-to-image generation. Their impressive performance suggests that these models should also constitute powerful priors for inverse imaging problems, but that approach has not yet led to comparable fidelity. There are several key obstacles: (i) the encoding into a lower-dimensional latent space makes the underlying (forward) mapping non-linear; (ii) the data likelihood term is usually intractable; and (iii) learned generative models struggle to recover rare, atypical data modes during inference. We present FLAIR, a novel training free variational framework that leverages flow-based generative models as a prior for inverse problems. To that end, we introduce a variational objective for flow matching that is agnostic to the type of degradation, and combine it with deterministic trajectory adjustments to recover atypical modes. To enforce exact consistency with the observed data, we decouple the optimization of the data fidelity and regularization terms. Moreover, we introduce a time-dependent calibration scheme in which the strength of the regularization is modulated according to off-line accuracy estimates. Results on standard imaging benchmarks demonstrate that FLAIR consistently outperforms existing diffusion- and flow-based methods in terms of reconstruction quality and sample diversity.', 'score': 1, 'issue_id': 4143, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '82a645cb87d64a3f', 'authors': ['Julius Erbach', 'Dominik Narnhofer', 'Andreas Dombos', 'Bernt Schiele', 'Jan Eric Lenssen', 'Konrad Schindler'], 'affiliations': ['ETH ZÃ¼rich', 'Max Planck Institute for Informatics, Saarland Informatics Campus'], 'pdf_title_img': 'assets/pdf/title_img/2506.02680.jpg', 'data': {'categories': ['#cv', '#benchmark', '#diffusion', '#data', '#training', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'FLAIR: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'FLAIR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. FLAIR Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ², Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞµ Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸.'}, 'en': {'title': 'FLAIR: Enhancing Inverse Problems with Flow-Based Generative Models', 'desc': 'FLAIR is a new framework that improves solutions to inverse problems using flow-based generative models without requiring extensive training. It addresses challenges like non-linear mappings and intractable data likelihoods by introducing a variational objective that is flexible to different types of data degradation. The framework also includes techniques to recover rare data patterns and ensures consistency with observed data by separating optimization processes. Experimental results show that FLAIR achieves better image reconstruction quality and greater diversity in samples compared to existing methods.'}, 'zh': {'title': 'FLAIRï¼šæå‡é€†é—®é¢˜è§£å†³çš„æ–°æ–¹æ³•', 'desc': 'FLAIRæ˜¯ä¸€ç§æ–°é¢–çš„æ— è®­ç»ƒå˜åˆ†æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºæµçš„ç”Ÿæˆæ¨¡å‹æ¥å¢å¼ºé€†é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å˜åˆ†ç›®æ ‡å’Œç¡®å®šæ€§è½¨è¿¹è°ƒæ•´ï¼Œå…‹æœäº†åœ¨ä½ç»´æ½œåœ¨ç©ºé—´ç¼–ç å¸¦æ¥çš„éçº¿æ€§æ˜ å°„é—®é¢˜ã€‚FLAIRèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤ç¨€æœ‰å’Œéå…¸å‹çš„æ•°æ®æ¨¡å¼ï¼Œå¹¶ç¡®ä¿ä¸è§‚æµ‹æ•°æ®çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFLAIRåœ¨é‡å»ºè´¨é‡å’Œæ ·æœ¬å¤šæ ·æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æ‰©æ•£å’Œæµæ¨¡å‹æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02153', 'title': 'Small Language Models are the Future of Agentic AI', 'url': 'https://huggingface.co/papers/2506.02153', 'abstract': 'Large language models (LLMs) are often praised for exhibiting near-human performance on a wide range of tasks and valued for their ability to hold a general conversation. The rise of agentic AI systems is, however, ushering in a mass of applications in which language models perform a small number of specialized tasks repetitively and with little variation.   Here we lay out the position that small language models (SLMs) are sufficiently powerful, inherently more suitable, and necessarily more economical for many invocations in agentic systems, and are therefore the future of agentic AI. Our argumentation is grounded in the current level of capabilities exhibited by SLMs, the common architectures of agentic systems, and the economy of LM deployment. We further argue that in situations where general-purpose conversational abilities are essential, heterogeneous agentic systems (i.e., agents invoking multiple different models) are the natural choice. We discuss the potential barriers for the adoption of SLMs in agentic systems and outline a general LLM-to-SLM agent conversion algorithm.   Our position, formulated as a value statement, highlights the significance of the operational and economic impact even a partial shift from LLMs to SLMs is to have on the AI agent industry. We aim to stimulate the discussion on the effective use of AI resources and hope to advance the efforts to lower the costs of AI of the present day. Calling for both contributions to and critique of our position, we commit to publishing all such correspondence at https://research.nvidia.com/labs/lpr/slm-agents.', 'score': 1, 'issue_id': 4148, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '811261b62e0e242e', 'authors': ['Peter Belcak', 'Greg Heinrich', 'Shizhe Diao', 'Yonggan Fu', 'Xin Dong', 'Saurav Muralidharan', 'Yingyan Celine Lin', 'Pavlo Molchanov'], 'affiliations': ['Georgia Institute of Technology', 'NVIDIA Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.02153.jpg', 'data': {'categories': ['#agents', '#agi', '#optimization', '#small_models'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SLM Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹, Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ LLM Ğ½Ğ° SLM Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ SLM. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğº Ğ´Ğ¸ÑĞºÑƒÑÑĞ¸Ğ¸ Ğ¾Ğ± ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ˜Ğ˜ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Small Language Models: The Future of Agentic AI', 'desc': 'This paper argues that small language models (SLMs) are more suitable and cost-effective for specialized tasks in agentic AI systems compared to large language models (LLMs). It highlights that SLMs possess sufficient capabilities for repetitive tasks, making them a better choice for many applications. The authors propose that heterogeneous systems, which use multiple models, are ideal for scenarios requiring general conversational abilities. They also present a conversion algorithm for transitioning from LLMs to SLMs and emphasize the economic benefits of adopting SLMs in the AI industry.'}, 'zh': {'title': 'å°å‹è¯­è¨€æ¨¡å‹æ˜¯ä»£ç†AIçš„æœªæ¥', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºæ¥è¿‘äººç±»çš„èƒ½åŠ›ï¼Œä½†åœ¨ç‰¹å®šåº”ç”¨ä¸­ï¼Œè¾ƒå°çš„è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰æ›´ä¸ºåˆé€‚ã€‚SLMsåœ¨ä»£ç†ç³»ç»Ÿä¸­èƒ½å¤Ÿé«˜æ•ˆåœ°æ‰§è¡Œé‡å¤æ€§ä»»åŠ¡ï¼Œä¸”ç»æµæ€§æ›´å¼ºã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒSLMsçš„èƒ½åŠ›è¶³ä»¥æ»¡è¶³è®¸å¤šéœ€æ±‚ï¼Œå¹¶ä¸”åœ¨éœ€è¦å¤šç§æ¨¡å‹çš„å¼‚æ„ä»£ç†ç³»ç»Ÿä¸­ï¼ŒSLMsæ˜¯ç†æƒ³é€‰æ‹©ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä»LLMsåˆ°SLMsçš„è½¬æ¢ç®—æ³•ï¼Œä»¥ä¿ƒè¿›SLMsåœ¨ä»£ç†ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00618', 'title': 'RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents', 'url': 'https://huggingface.co/papers/2506.00618', 'abstract': 'RIOSWorld is a benchmark for evaluating safety risks of multimodal large language models in real-world computer tasks, revealing significant risks that necessitate safety alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce RiOSWorld, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on RiOSWorld demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.', 'score': 1, 'issue_id': 4148, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': 'd37e3433dcb25b41', 'authors': ['Jingyi Yang', 'Shuai Shao', 'Dongrui Liu', 'Jing Shao'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00618.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#multimodal', '#agents', '#security', '#alignment'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'RIOSWorld: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'RIOSWorld - ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 492 Ñ€Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²ĞµĞ±, ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼ĞµĞ´Ğ¸Ğ°, ĞĞ¡, ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ñ‡Ñ‚Ñƒ Ğ¸ Ğ¾Ñ„Ğ¸ÑĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. Ğ Ğ¸ÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ñ‹ Ğ½Ğ° Ğ´Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ñ€Ğ¸ÑĞºĞ¸, Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¸ Ñ€Ğ¸ÑĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MLLM Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Evaluating Safety Risks of MLLMs in Real-World Tasks with RIOSWorld', 'desc': 'RIOSWorld is a new benchmark created to assess the safety risks associated with multimodal large language models (MLLMs) when they perform real-world computer tasks. It identifies significant risks that arise from both user actions and environmental factors, emphasizing the need for safety measures tailored to these scenarios. The benchmark includes a diverse set of 492 tasks across various applications, allowing for a comprehensive evaluation of potential hazards. The findings indicate that current MLLM-based agents face considerable safety challenges, underscoring the importance of aligning their operations with safety principles in practical environments.'}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å®‰å…¨é£é™©çš„æ–°åŸºå‡†', 'desc': 'RIOSWorldæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ç°å®è®¡ç®—ä»»åŠ¡ä¸­å®‰å…¨é£é™©çš„åŸºå‡†ã€‚éšç€MLLMçš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬è¢«è¶Šæ¥è¶Šå¤šåœ°ç”¨ä½œè‡ªä¸»è®¡ç®—ä»£ç†ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨å®‰å…¨é£é™©çš„é—®é¢˜ã€‚ç°æœ‰çš„ç ”ç©¶åœ¨è¯„ä¼°è¿™äº›é£é™©æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¾€å¾€ç¼ºä¹çœŸå®çš„äº’åŠ¨ç¯å¢ƒæˆ–åªå…³æ³¨ç‰¹å®šçš„é£é™©ç±»å‹ã€‚RIOSWorldé€šè¿‡æä¾›492ä¸ªæ¶µç›–å¤šç§è®¡ç®—åº”ç”¨çš„é£é™©ä»»åŠ¡ï¼Œå¸®åŠ©å…¨é¢è¯„ä¼°è®¡ç®—ä»£ç†çš„å®‰å…¨é£é™©ï¼Œå¼ºè°ƒäº†åœ¨ç°å®è®¡ç®—æ“ä½œä¸­è¿›è¡Œå®‰å…¨å¯¹é½çš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16406', 'title': 'Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights', 'url': 'https://huggingface.co/papers/2506.16406', 'abstract': 'Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to 12,000times lower overhead than full fine-tuning, ii) average gains up to 30\\% in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at https://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.', 'score': 75, 'issue_id': 4426, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '7ec3cdc6c01d1d79', 'authors': ['Zhiyuan Liang', 'Dongwen Tang', 'Yuhao Zhou', 'Xuanlei Zhao', 'Mingjia Shi', 'Wangbo Zhao', 'Zekai Li', 'Peihao Wang', 'Konstantin SchÃ¼rholt', 'Damian Borth', 'Michael M. Bronstein', 'Yang You', 'Zhangyang Wang', 'Kai Wang'], 'affiliations': ['National University of Singapore', 'Oxford University', 'UT Austin', 'University of St. Gallen'], 'pdf_title_img': 'assets/pdf/title_img/2506.16406.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#optimization'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¯Ğœ: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Drag-and-Drop LLMs (DnD) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DnD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸, Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑĞ¾Ğ² LoRA Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. DnD Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¼ĞµĞ¶Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼.'}, 'en': {'title': 'Efficient Task-Specific Adaptation with Drag-and-Drop LLMs', 'desc': 'The paper introduces Drag-and-Drop LLMs (DnD), a novel approach that generates task-specific parameters without the need for separate training on each task. By using prompt-conditioned parameter generation, DnD maps unlabeled task prompts directly to updates for low-rank adaptation (LoRA) weights. This method significantly reduces the computational overhead, achieving up to 12,000 times less than traditional fine-tuning while improving performance by an average of 30% on various benchmarks. DnD demonstrates strong cross-domain generalization, effectively adapting to new tasks without prior exposure to their data or labels.'}, 'zh': {'title': 'æ‹–æ”¾å¤§è¯­è¨€æ¨¡å‹ï¼šé«˜æ•ˆçš„ä»»åŠ¡ç‰¹å®šå‚æ•°ç”Ÿæˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºæ‹–æ”¾å¤§è¯­è¨€æ¨¡å‹ï¼ˆDnDï¼‰ï¼Œå®ƒé€šè¿‡æç¤ºæ¡ä»¶ç”Ÿæˆå‚æ•°ï¼Œæ¶ˆé™¤äº†æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒéœ€æ±‚ã€‚DnDä½¿ç”¨è½»é‡çº§æ–‡æœ¬ç¼–ç å™¨å°†æç¤ºæ‰¹æ¬¡æç‚¼ä¸ºæ¡ä»¶åµŒå…¥ï¼Œå¹¶é€šè¿‡çº§è”è¶…å·ç§¯è§£ç å™¨è½¬æ¢ä¸ºå®Œæ•´çš„LoRAçŸ©é˜µã€‚ä¸ä¼ ç»Ÿçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒDnDåœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šä¹Ÿèƒ½ä¿æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†æç¤ºæ¡ä»¶å‚æ•°ç”Ÿæˆæ˜¯å¿«é€Ÿä¸“é—¨åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16035', 'title': 'Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding', 'url': 'https://huggingface.co/papers/2506.16035', 'abstract': 'A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.', 'score': 53, 'issue_id': 4427, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '08abcd6ec6f0d9e1', 'authors': ['Vishesh Tripathi', 'Tanmay Odapally', 'Indraneel Das', 'Uday Allu', 'Biddwan Ahmed'], 'affiliations': ['AI Research Team, Yellow.ai'], 'pdf_title_img': 'assets/pdf/title_img/2506.16035.jpg', 'data': {'categories': ['#dataset', '#long_context', '#optimization', '#rag', '#multimodal'], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… PDF-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ°Ñ€Ñ‚Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ñ‚Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG). ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RAG Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing RAG with Multimodal Document Chunking', 'desc': 'This paper introduces a new method for breaking down complex PDF documents using Large Multimodal Models (LMMs). Traditional methods struggle with intricate layouts, such as multi-page tables and embedded visuals, which can lead to poor information retrieval. The proposed approach processes documents in batches while keeping the context intact across pages, ensuring that the meaning and structure are preserved. Evaluations show that this method significantly enhances the quality of document chunks and improves the performance of Retrieval-Augmented Generation (RAG) systems.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ–‡æ¡£åˆ†å—ï¼Œæå‡ä¿¡æ¯æ£€ç´¢æ–°é«˜åº¦', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ–‡æ¡£åˆ†å—æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ¥å¤„ç†å¤æ‚çš„PDFæ–‡æ¡£ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šé¡µè¡¨æ ¼å’ŒåµŒå…¥è§†è§‰å…ƒç´ ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§ã€‚é€šè¿‡é…ç½®é¡µé¢æ‰¹æ¬¡è¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè·¨æ‰¹æ¬¡ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜äº†åˆ†å—è´¨é‡å’Œåç»­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„RAGç³»ç»Ÿç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ–‡æ¡£ç»“æ„ä¿ç•™æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16054', 'title': 'PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and\n  Quantized Attention in Visual Generation Models', 'url': 'https://huggingface.co/papers/2506.16054', 'abstract': 'PAROAttention reorganizes visual attention patterns to enable efficient sparsification and quantization, reducing memory and computational costs with minimal impact on performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization. However, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: *reorganizing* the attention pattern to alleviate the challenges. Inspired by the local aggregation nature of visual feature extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)** technique, which unifies the diverse attention patterns into a hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization. We evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern. Our approach, **PAROAttention**, achieves video and image generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to **2.7x** end-to-end latency speedup.', 'score': 43, 'issue_id': 4428, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '141661e70ce4b65f', 'authors': ['Tianchen Zhao', 'Ke Hong', 'Xinhao Yang', 'Xuefeng Xiao', 'Huixia Li', 'Feng Ling', 'Ruiqi Xie', 'Siqi Chen', 'Hongyu Zhu', 'Yichong Zhang', 'Yu Wang'], 'affiliations': ['ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.16054.jpg', 'data': {'categories': ['#optimization', '#cv', '#inference', '#architecture', '#video'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ° Ğ¶Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ PAROAttention Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 1.9-2.7 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (20-30%) Ğ¸ 8/4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Visual Attention for Efficient AI', 'desc': 'PAROAttention is a novel approach that reorganizes visual attention patterns to improve the efficiency of sparsification and quantization in machine learning models. By addressing the challenges posed by the irregular characteristics of attention mechanisms, this method simplifies the process of reducing memory and computational costs. The technique, known as Pattern-Aware token ReOrdering (PARO), consolidates diverse attention patterns into a more manageable block-wise format. As a result, PAROAttention achieves high-quality image and video generation with significantly reduced resource requirements, maintaining performance comparable to full-precision models.'}, 'zh': {'title': 'PAROAttentionï¼šé«˜æ•ˆçš„è§†è§‰æ³¨æ„åŠ›é‡ç»„', 'desc': 'PAROAttentionæ˜¯ä¸€ç§é€šè¿‡é‡æ–°ç»„ç»‡è§†è§‰æ³¨æ„åŠ›æ¨¡å¼æ¥æé«˜ç¨€ç–åŒ–å’Œé‡åŒ–æ•ˆç‡çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨å‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå°½é‡ä¸å½±å“æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰æ³¨æ„åŠ›æ¨¡å¼çš„åˆ†æ•£å’Œä¸è§„åˆ™ç‰¹æ€§æ˜¯é€ æˆé«˜è®¡ç®—æˆæœ¬çš„ä¸»è¦åŸå› ã€‚PAROæŠ€æœ¯é€šè¿‡å°†å¤šæ ·çš„æ³¨æ„åŠ›æ¨¡å¼ç»Ÿä¸€ä¸ºç¡¬ä»¶å‹å¥½çš„å—çŠ¶æ¨¡å¼ï¼Œæ˜¾è‘—ç®€åŒ–äº†ç¨€ç–åŒ–å’Œé‡åŒ–è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09049', 'title': 'VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2506.09049', 'abstract': 'VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.', 'score': 29, 'issue_id': 4425, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '91caa1a0b1cb2b54', 'authors': ['Li Kang', 'Xiufeng Song', 'Heng Zhou', 'Yiran Qin', 'Jie Yang', 'Xiaohong Liu', 'Philip Torr', 'Lei Bai', 'Zhenfei Yin'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen', 'University of Oxford', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09049.jpg', 'data': {'categories': ['#reasoning', '#agents', '#games', '#rl', '#benchmark', '#cv'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'VIKI-Bench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. VIKI-R - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VIKI-R Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑŒÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Multi-Agent Cooperation with VIKI-Bench and VIKI-R', 'desc': 'This paper introduces VIKI-Bench and VIKI-R, a benchmark and framework designed to enhance cooperation among various embodied agents using vision-language models and reinforcement learning. VIKI-Bench is structured into three levels: agent activation, task planning, and trajectory perception, allowing for comprehensive evaluation of visual reasoning in multi-agent scenarios. VIKI-R fine-tunes a pretrained vision-language model with Chain-of-Thought demonstrations and applies reinforcement learning to optimize performance across different tasks. The results demonstrate that VIKI-R significantly improves cooperation patterns among diverse agents, showcasing the potential of visual-driven strategies in embodied AI.'}, 'zh': {'title': 'VIKI-Benchä¸VIKI-Rï¼šæ¨åŠ¨å…·èº«æ™ºèƒ½ä½“çš„è§†è§‰é©±åŠ¨åˆä½œ', 'desc': 'VIKI-Benchå’ŒVIKI-Ræ˜¯ç”¨äºè¯„ä¼°å’Œæ”¹å–„å¤šæ ·åŒ–å…·èº«æ™ºèƒ½ä½“ä¹‹é—´è§†è§‰é©±åŠ¨åˆä½œçš„åŸºå‡†å’Œæ¡†æ¶ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ†å±‚åŸºå‡†ï¼ŒåŒ…å«ä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ä¸‰ä¸ªç»“æ„åŒ–å±‚æ¬¡ã€‚VIKI-Ræ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´æ³¨é‡Šç¤ºä¾‹å¾®è°ƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å¤šå±‚æ¬¡å¥–åŠ±ä¿¡å·ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVIKI-Råœ¨æ‰€æœ‰ä»»åŠ¡å±‚æ¬¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”å¼ºåŒ–å­¦ä¹ ä¿ƒè¿›äº†å¼‚æ„æ™ºèƒ½ä½“ä¹‹é—´çš„ç»„åˆåˆä½œæ¨¡å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17201', 'title': 'Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition', 'url': 'https://huggingface.co/papers/2506.17201', 'abstract': 'Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.', 'score': 23, 'issue_id': 4425, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': 'def0daf064c03fe6', 'authors': ['Jiaqi Li', 'Junshu Tang', 'Zhiyong Xu', 'Longhuang Wu', 'Yuan Zhou', 'Shuai Shao', 'Tianbao Yu', 'Zhiguo Cao', 'Qinglin Lu'], 'affiliations': ['Huazhong University of Science and Technology', 'Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2506.17201.jpg', 'data': {'categories': ['#inference', '#video', '#diffusion', '#synthetic', '#dataset', '#games', '#training'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Hunyuan-GameCraft - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ, ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ²Ğ¾Ğ´Ğ°, Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµÑ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 100 ĞĞĞ-Ğ¸Ğ³Ñ€, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Hunyuan-GameCraft Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ³Ñ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Interactive Video Generation in Gaming', 'desc': 'Hunyuan-GameCraft is a new framework designed for generating interactive videos in gaming environments, overcoming issues related to dynamics, generality, and efficiency. It uses a unified input representation that combines keyboard and mouse controls, allowing for smooth transitions in camera and movement actions. The framework employs a hybrid history-conditioned training method to extend video sequences while keeping important game scene details intact. Additionally, model distillation is utilized to improve computational efficiency, making it suitable for real-time applications in complex gaming scenarios.'}, 'zh': {'title': 'Hunyuan-GameCraftï¼šæå‡äº’åŠ¨æ¸¸æˆè§†é¢‘ç”Ÿæˆçš„çœŸå®æ„Ÿä¸å¯ç©æ€§', 'desc': 'Hunyuan-GameCraftæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¸¸æˆç¯å¢ƒä¸­é«˜åŠ¨æ€äº’åŠ¨è§†é¢‘ç”Ÿæˆçš„å±€é™æ€§ã€‚å®ƒé€šè¿‡ç»Ÿä¸€è¾“å…¥è¡¨ç¤ºã€æ··åˆå†å²æ¡ä»¶è®­ç»ƒå’Œæ¨¡å‹è’¸é¦æ¥æé«˜åŠ¨æ€æ€§ã€é€šç”¨æ€§å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°ç²¾ç»†çš„åŠ¨ä½œæ§åˆ¶ï¼Œå¹¶åœ¨å¤æ‚çš„äº’åŠ¨ç¯å¢ƒä¸­ä¿æŒé•¿æ—¶é—´çš„ä¸€è‡´æ€§ã€‚ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒï¼ŒHunyuan-GameCraftåœ¨ç”Ÿæˆäº’åŠ¨æ¸¸æˆè§†é¢‘çš„çœŸå®æ„Ÿå’Œå¯ç©æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16310', 'title': 'Optimizing Multilingual Text-To-Speech with Accents & Emotions', 'url': 'https://huggingface.co/papers/2506.16310', 'abstract': 'A new TTS architecture improves accent accuracy and emotion recognition for Hindi and Indian English by integrating phoneme alignment, culture-sensitive emotion embeddings, and dynamic accent code switching.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art text-to-speech (TTS) systems realize high naturalness in monolingual environments, synthesizing speech with correct multilingual accents (especially for Indic languages) and context-relevant emotions still poses difficulty owing to cultural nuance discrepancies in current frameworks. This paper introduces a new TTS architecture integrating accent along with preserving transliteration with multi-scale emotion modelling, in particularly tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS model by integrating A language-specific phoneme alignment hybrid encoder-decoder architecture, and culture-sensitive emotion embedding layers trained on native speaker corpora, as well as incorporating a dynamic accent code switching with residual vector quantization. Quantitative tests demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system is that it can mix code in real time - generating statements such as "Namaste, let\'s talk about <Hindi phrase>" with uninterrupted accent shifts while preserving emotional consistency. Subjective evaluation with 200 users reported a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than existing multilingual systems (p<0.01). This research makes cross-lingual synthesis more feasible by showcasing scalable accent-emotion disentanglement, with direct application in South Asian EdTech and accessibility software.', 'score': 21, 'issue_id': 4436, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '5aff84e0535327a9', 'authors': ['Pranav Pawar', 'Akshansh Dwivedi', 'Jenish Boricha', 'Himanshu Gohil', 'Aditya Dubey'], 'affiliations': ['Dwarkadas J. Sanghvi College of Engineering, Mumbai, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.16310.jpg', 'data': {'categories': ['#low_resource', '#machine_translation', '#multilingual', '#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ (TTS) Ğ´Ğ»Ñ Ñ…Ğ¸Ğ½Ğ´Ğ¸ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ğ½ĞµĞ¼, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ². ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ° Ğ½Ğ° 23.7% Ğ¸ 85.3% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. Ğ¡ÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° 200 Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ°Ğ»Ğ° ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 4.2/5 Ğ·Ğ° ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing TTS for Hindi and Indian English with Cultural Nuance', 'desc': 'This paper presents a new text-to-speech (TTS) architecture that enhances accent accuracy and emotion recognition specifically for Hindi and Indian English. It integrates phoneme alignment, culture-sensitive emotion embeddings, and dynamic accent code switching to address the challenges of synthesizing speech with cultural nuances. The proposed system shows a significant improvement in accent accuracy and emotion recognition, outperforming existing models. This innovation allows for real-time accent shifts while maintaining emotional consistency, making it particularly useful for applications in South Asian education technology and accessibility.'}, 'zh': {'title': 'æå‡å°åœ°è¯­ä¸å°åº¦è‹±è¯­çš„è¯­éŸ³åˆæˆå‡†ç¡®æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¶æ„ï¼Œæ—¨åœ¨æé«˜å°åœ°è¯­å’Œå°åº¦è‹±è¯­çš„å£éŸ³å‡†ç¡®æ€§å’Œæƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•´åˆéŸ³ç´ å¯¹é½ã€æ–‡åŒ–æ•æ„Ÿçš„æƒ…æ„ŸåµŒå…¥å’ŒåŠ¨æ€å£éŸ³åˆ‡æ¢ï¼Œå…‹æœäº†ç°æœ‰æ¡†æ¶åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„å±€é™æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å£éŸ³å‡†ç¡®æ€§ä¸Šæé«˜äº†23.7%ï¼Œæƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡è¾¾åˆ°äº†85.3%ã€‚è¿™é¡¹ç ”ç©¶ä¸ºè·¨è¯­è¨€åˆæˆæä¾›äº†æ›´å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºå—äºšçš„æ•™è‚²ç§‘æŠ€å’Œæ— éšœç¢è½¯ä»¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17206', 'title': 'DreamCube: 3D Panorama Generation via Multi-plane Synchronization', 'url': 'https://huggingface.co/papers/2506.17206', 'abstract': 'Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.', 'score': 15, 'issue_id': 4425, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': 'bbec38fbe4e9ddd8', 'authors': ['Yukun Huang', 'Yanning Zhou', 'Jianan Wang', 'Kaiyi Huang', 'Xihui Liu'], 'affiliations': ['Astribot', 'Tencent', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.17206.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#3d', '#cv'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ½Ğ°Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ 2D Ğ¸ 3D Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DreamCube, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½.'}, 'en': {'title': 'DreamCube: Bridging 2D and 3D for Stunning Panoramas', 'desc': 'This paper presents a method called DreamCube that enhances 2D foundation models for generating 3D panoramas. It addresses the challenge of creating high-quality and diverse omnidirectional content by using multi-plane synchronization. This technique allows the model to effectively utilize existing 2D image data to improve the geometry and appearance of 3D images. The results show that DreamCube can generate consistent panoramic images and accurately estimate depth, making it a significant advancement in 3D scene generation.'}, 'zh': {'title': 'å¤šå¹³é¢åŒæ­¥ï¼Œå¼€å¯ä¸‰ç»´å…¨æ™¯æ–°è§†ç•Œ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDreamCubeçš„å¤šå¹³é¢åŒæ­¥æ–¹æ³•ï¼Œæ—¨åœ¨å°†äºŒç»´åŸºç¡€æ¨¡å‹æ‰©å±•åˆ°ä¸‰ç»´å…¨æ™¯ç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ¨¡å‹çš„ä¸°å¯Œå›¾åƒå…ˆéªŒï¼Œå…‹æœäº†ä¸‰ç»´å…¨æ™¯æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å¤šæ ·åŒ–çš„è§†è§‰æ•ˆæœå’Œå‡†ç¡®çš„å‡ ä½•å½¢çŠ¶ï¼ŒåŒæ—¶ä¿æŒå¤šè§†å›¾çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯åœ¨å…¨æ™¯å›¾åƒç”Ÿæˆã€æ·±åº¦ä¼°è®¡å’Œä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16504', 'title': 'Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate\n  Details', 'url': 'https://huggingface.co/papers/2506.16504', 'abstract': 'Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.', 'score': 12, 'issue_id': 4424, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '7ea16d5712b67fcc', 'authors': ['Zeqiang Lai', 'Yunfei Zhao', 'Haolin Liu', 'Zibo Zhao', 'Qingxiang Lin', 'Huiwen Shi', 'Xianghui Yang', 'Mingxin Yang', 'Shuhui Yang', 'Yifei Feng', 'Sheng Zhang', 'Xin Huang', 'Di Luo', 'Fan Yang', 'Fang Yang', 'Lifu Wang', 'Sicong Liu', 'Yixuan Tang', 'Yulin Cai', 'Zebin He', 'Tian Liu', 'Yuhong Liu', 'Jie Jiang', 'Linus', 'Jingwei Huang', 'Chunchao Guo'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2506.16504.jpg', 'data': {'categories': ['#diffusion', '#3d'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹', 'desc': 'Hunyuan3D 2.5 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LATTICE Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ (PBR) Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼, Hunyuan3D 2.5 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ñ„Ğ¾Ñ€Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€.'}, 'en': {'title': 'Revolutionizing 3D Asset Generation with Hunyuan3D 2.5', 'desc': 'Hunyuan3D 2.5 is a suite of advanced 3D diffusion models designed to enhance the generation of high-quality 3D shapes and textures. It introduces the LATTICE model, a new foundation for shape generation that utilizes large, high-quality datasets and boasts a model size of up to 10 billion parameters. This model produces sharp and detailed 3D shapes while maintaining clean mesh surfaces, bridging the gap between generated and handcrafted designs. Additionally, the suite incorporates physical-based rendering in a multi-view architecture to improve texture generation, demonstrating significant improvements over previous versions.'}, 'zh': {'title': 'Hunyuan3D 2.5ï¼šå½¢çŠ¶ä¸çº¹ç†ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'Hunyuan3D 2.5 æ˜¯ä¸€å¥—å…ˆè¿›çš„ 3D æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜ä¿çœŸå’Œç»†è‡´çš„çº¹ç† 3D èµ„äº§ã€‚å®ƒåœ¨å½¢çŠ¶ç”Ÿæˆæ–¹é¢å¼•å…¥äº†æ–°çš„åŸºç¡€æ¨¡å‹ LATTICEï¼Œç»è¿‡å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†çš„è®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆæ¸…æ™°ä¸”ç»†è‡´çš„ 3D å½¢çŠ¶ã€‚çº¹ç†ç”Ÿæˆæ–¹é¢ï¼Œé‡‡ç”¨äº†åŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰æŠ€æœ¯ï¼Œç»“åˆäº†å¤šè§†è§’æ¶æ„ï¼Œæ˜¾è‘—æå‡äº†çº¹ç†è´¨é‡ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒHunyuan3D 2.5 åœ¨å½¢çŠ¶å’Œçº¹ç†ç”Ÿæˆæ–¹é¢å‡ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17218', 'title': 'Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual\n  Tokens', 'url': 'https://huggingface.co/papers/2506.17218', 'abstract': "Mirage enhances vision-language models by integrating latent visual tokens into text decoding to improve multimodal reasoning without generating explicit images.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation.", 'score': 10, 'issue_id': 4439, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': '8f0feb2578145e77', 'authors': ['Zeyuan Yang', 'Xueyang Yu', 'Delin Chen', 'Maohao Shen', 'Chuang Gan'], 'affiliations': ['Massachusetts Institute of Technology', 'University of Massachusetts, Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.17218.jpg', 'data': {'categories': ['#multimodal', '#rl', '#reasoning', '#games'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ‹ Ğ´Ğ»Ñ Ğ˜Ğ˜: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Mirage Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Mirage Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ 'Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾' Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mirage Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼."}, 'en': {'title': 'Unlocking Multimodal Reasoning with Mental Imagery', 'desc': 'The paper introduces Mirage, a framework that enhances vision-language models (VLMs) by incorporating latent visual tokens into the text decoding process. This approach allows the models to perform multimodal reasoning without the need to generate explicit images, which can limit their performance. By mimicking human mental imagery, Mirage enables VLMs to interleave visual and textual information effectively. The framework is trained using a combination of distillation from image embeddings and reinforcement learning, resulting in improved reasoning capabilities across various tasks.'}, 'zh': {'title': 'Mirageï¼šæ— å›¾åƒç”Ÿæˆçš„å¤šæ¨¡æ€æ¨ç†å¢å¼º', 'desc': 'Mirageæ˜¯ä¸€ç§å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡å°†æ½œåœ¨è§†è§‰æ ‡è®°æ•´åˆåˆ°æ–‡æœ¬è§£ç ä¸­ï¼Œæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ç”Ÿæˆæ˜ç¡®çš„å›¾åƒã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†äººç±»é€šè¿‡å¿ƒç†æ„è±¡è¿›è¡Œæ¨ç†çš„æ–¹å¼ï¼Œå…è®¸æ¨¡å‹åœ¨ä¸ç”Ÿæˆå›¾åƒçš„æƒ…å†µä¸‹è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œå½“æ¨¡å‹é€‰æ‹©â€œè§†è§‰æ€è€ƒâ€æ—¶ï¼Œå®ƒä¼šå°†éšè—çŠ¶æ€é‡æ–°è½¬æ¢ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œä»è€Œç»§ç»­å¤šæ¨¡æ€è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMirageåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15745', 'title': 'InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding', 'url': 'https://huggingface.co/papers/2506.15745', 'abstract': 'InfiniPot-V is a training-free, query-agnostic framework that compresses the key-value cache during video encoding to maintain a fixed memory cap for streaming video understanding, enhancing real-time performance and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time--quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and two streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.', 'score': 8, 'issue_id': 4431, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'f6babb4a5e89bb3f', 'authors': ['Minsoo Kim', 'Kyuhong Shim', 'Jungwook Choi', 'Simyung Chang'], 'affiliations': ['Hanyang University', 'Qualcomm AI Research, Qualcomm Korea', 'Sungkyunkwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15745.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#long_context', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºÑÑˆĞ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'InfiniPot-V - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ (TaR) Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ (VaN) Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². InfiniPot-V ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ´Ğ¾ 94%, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ°.'}, 'en': {'title': 'Streamline Video Understanding with InfiniPot-V!', 'desc': 'InfiniPot-V is a novel framework designed to optimize memory usage during video encoding for real-time video understanding. It operates without the need for training or prior knowledge of user queries, making it flexible and efficient. The framework compresses the key-value cache by removing redundant information while preserving important data, ensuring that memory usage remains constant regardless of video length. This approach significantly reduces GPU memory requirements while maintaining high accuracy in video processing tasks.'}, 'zh': {'title': 'InfiniPot-Vï¼šå®æ—¶è§†é¢‘ç†è§£çš„å†…å­˜å‹ç¼©æ–°æ–¹æ¡ˆ', 'desc': 'InfiniPot-V æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”ä¸æŸ¥è¯¢æ— å…³çš„æ¡†æ¶ï¼Œæ—¨åœ¨å‹ç¼©è§†é¢‘ç¼–ç ä¸­çš„å…³é”®å€¼ç¼“å­˜ï¼Œä»¥ä¿æŒå›ºå®šçš„å†…å­˜é™åˆ¶ï¼Œä»è€Œæå‡å®æ—¶æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç›‘æ§ç¼“å­˜ï¼Œå½“è¾¾åˆ°ç”¨æˆ·è®¾å®šçš„é˜ˆå€¼æ—¶ï¼Œæ‰§è¡Œè½»é‡çº§å‹ç¼©ï¼Œå»é™¤æ—¶é—´ä¸Šå†—ä½™çš„æ ‡è®°ï¼Œå¹¶ä¿ç•™è¯­ä¹‰ä¸Šé‡è¦çš„æ ‡è®°ã€‚ä¸ä»¥å¾€çš„å‹ç¼©æ–¹æ¡ˆä¸åŒï¼ŒInfiniPot-V ä¸éœ€è¦ç¦»çº¿è·å–æ•´ä¸ªè§†é¢‘æˆ–ç”¨æˆ·æŸ¥è¯¢ï¼Œå› æ­¤èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶å†…å­˜ä½¿ç”¨ã€‚é€šè¿‡åœ¨å¤šä¸ªå¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­éªŒè¯ï¼ŒInfiniPot-V æ˜¾è‘—é™ä½äº† GPU å†…å­˜å³°å€¼ï¼ŒåŒæ—¶ä¿æŒå®æ—¶ç”Ÿæˆå’Œå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15442', 'title': 'Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material', 'url': 'https://huggingface.co/papers/2506.15442', 'abstract': 'The tutorial provides a comprehensive guide on using Hunyuan3D 2.1 for generating high-resolution, textured 3D models, covering data preparation, model architecture, training, evaluation, and deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design.', 'score': 7, 'issue_id': 4431, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'c5c9b452dd58395e', 'authors': ['Team Hunyuan3D', 'Shuhui Yang', 'Mingxin Yang', 'Yifei Feng', 'Xin Huang', 'Sheng Zhang', 'Zebin He', 'Di Luo', 'Haolin Liu', 'Yunfei Zhao', 'Qingxiang Lin', 'Zeqiang Lai', 'Xianghui Yang', 'Huiwen Shi', 'Zibo Zhao', 'Bowen Zhang', 'Hongyu Yan', 'Lifu Wang', 'Sicong Liu', 'Jihong Zhang', 'Meng Chen', 'Liang Dong', 'Yiwen Jia', 'Yulin Cai', 'Jiaao Yu', 'Yixuan Tang', 'Dongyuan Guo', 'Junlin Yu', 'Hao Zhang', 'Zheng Ye', 'Peng He', 'Runzhou Wu', 'Shida Wei', 'Chao Zhang', 'Yonghao Tan', 'Yifu Sun', 'Lin Niu', 'Shirui Huang', 'Bojian Zheng', 'Shu Liu', 'Shilin Chen', 'Xiang Yuan', 'Xiaofeng Yang', 'Kai Liu', 'Jianchen Zhu', 'Peng Chen', 'Tian Liu', 'Di Wang', 'Yuhong Liu', 'Linus', 'Jie Jiang', 'Jingwei Huang', 'Chunchao Guo'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2506.15442.jpg', 'data': {'categories': ['#training', '#3d', '#architecture', '#synthetic', '#games', '#data'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Hunyuan3D 2.1', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Hunyuan3D 2.1 Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ½ĞµĞ¹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Hunyuan3D 2.1 ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Hunyuan3D-DiT Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Hunyuan3D-Paint Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. Ğ¦ĞµĞ»ÑŒ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° - ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Unlocking 3D Model Creation with Hunyuan3D 2.1', 'desc': 'This paper presents a tutorial on Hunyuan3D 2.1, a system designed for generating high-resolution, textured 3D models. It addresses the complexities of 3D model creation by providing a detailed guide on data preparation, model architecture, training, evaluation, and deployment. The system features two main components: Hunyuan3D-DiT for shape generation and Hunyuan3D-Paint for texture synthesis. By following this tutorial, users will learn how to finetune or create effective 3D generative models for various applications such as gaming and virtual reality.'}, 'zh': {'title': 'æŒæ¡Hunyuan3D 2.1ï¼Œè½»æ¾ç”Ÿæˆé«˜è´¨é‡3Dæ¨¡å‹', 'desc': 'æœ¬æ•™ç¨‹å…¨é¢ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨Hunyuan3D 2.1ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€å¸¦çº¹ç†çš„3Dæ¨¡å‹ï¼Œæ¶µç›–äº†æ•°æ®å‡†å¤‡ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒã€è¯„ä¼°å’Œéƒ¨ç½²ç­‰æ–¹é¢ã€‚å°½ç®¡3D AIç”Ÿæˆå†…å®¹é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºæ”¶é›†ã€å¤„ç†å’Œè®­ç»ƒ3Dæ¨¡å‹çš„å¤æ‚æ€§ï¼Œä»ç„¶ä¸»è¦é¢å‘ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…ã€‚Hunyuan3D 2.1ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæä¾›äº†é€æ­¥æŒ‡å¯¼ï¼Œå¸®åŠ©ç”¨æˆ·å¤„ç†3Dæ•°æ®ã€è®­ç»ƒç”Ÿæˆæ¨¡å‹å¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å°†æŒæ¡å¾®è°ƒæˆ–å¼€å‘é€‚ç”¨äºæ¸¸æˆã€è™šæ‹Ÿç°å®å’Œå·¥ä¸šè®¾è®¡çš„å¼ºå¤§3Dç”Ÿæˆæ¨¡å‹çš„çŸ¥è¯†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17202', 'title': 'UniFork: Exploring Modality Alignment for Unified Multimodal\n  Understanding and Generation', 'url': 'https://huggingface.co/papers/2506.17202', 'abstract': 'A Y-shaped architecture, UniFork, balances shared learning and task specialization for unified image understanding and generation, outperforming conventional fully shared Transformer models.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified image understanding and generation has emerged as a promising paradigm in multimodal artificial intelligence. Despite recent progress, the optimal architectural design for such unified models remains an open challenge. In this work, we start by analyzing the modality alignment behaviors of task-specific expert models for understanding and generation, as well as current unified models. Our analysis reveals a crucial observation: understanding tasks benefit from a progressively increasing modality alignment across network depth, which helps build up semantic information for better comprehension; In contrast, generation tasks follow a different trend: modality alignment increases in the early layers but decreases in the deep layers to recover spatial details. These divergent alignment patterns create a fundamental conflict in fully shared Transformer backbones, where a uniform representational flow often leads to performance compromises across two tasks. Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture that shares the shallow layers for cross-task representation learning, while employing task-specific branches in deeper layers to avoid task interference. This design effectively balances shared learning and task specialization. Through extensive ablation experiments, we demonstrate that Unifork consistently outperforms conventional fully shared Transformer architectures, and achieves performance on par with or better than task-specific models.', 'score': 5, 'issue_id': 4432, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': '3796f42ead32837a', 'authors': ['Teng Li', 'Quanfeng Lu', 'Lirui Zhao', 'Hao Li', 'Xizhou Zhu', 'Yu Qiao', 'Jun Zhang', 'Wenqi Shao'], 'affiliations': ['HKUST', 'SJTU', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.17202.jpg', 'data': {'categories': ['#optimization', '#architecture', '#multimodal'], 'emoji': 'ğŸ´', 'ru': {'title': 'UniFork: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'UniFork - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Y-Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. UniFork Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'UniFork: Balancing Shared Learning and Task Specialization in Image AI', 'desc': 'The paper introduces UniFork, a Y-shaped architecture designed to improve unified image understanding and generation in machine learning. It addresses the challenge of balancing shared learning and task specialization by analyzing how different tasks align modalities at various network depths. The findings show that understanding tasks benefit from deeper alignment, while generation tasks require a different approach, leading to conflicts in traditional models. UniFork resolves these issues by sharing shallow layers for common learning and using specialized branches for deeper layers, resulting in superior performance compared to fully shared Transformer models.'}, 'zh': {'title': 'UniForkï¼šå¹³è¡¡å…±äº«å­¦ä¹ ä¸ä»»åŠ¡ä¸“é—¨åŒ–çš„åˆ›æ–°æ¶æ„', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„Yå½¢æ¶æ„UniForkï¼Œæ—¨åœ¨å¹³è¡¡å…±äº«å­¦ä¹ å’Œä»»åŠ¡ä¸“é—¨åŒ–ï¼Œä»¥å®ç°ç»Ÿä¸€çš„å›¾åƒç†è§£å’Œç”Ÿæˆã€‚ç ”ç©¶å‘ç°ï¼Œç†è§£ä»»åŠ¡åœ¨ç½‘ç»œæ·±åº¦ä¸Šéœ€è¦é€æ¸å¢åŠ çš„æ¨¡æ€å¯¹é½ï¼Œè€Œç”Ÿæˆä»»åŠ¡åˆ™åœ¨æ—©æœŸå±‚å¢åŠ æ¨¡æ€å¯¹é½ä½†åœ¨æ·±å±‚å‡å°‘ï¼Œä»¥æ¢å¤ç©ºé—´ç»†èŠ‚ã€‚è¿™ç§å¯¹é½æ¨¡å¼çš„å·®å¼‚åœ¨ä¼ ç»Ÿçš„å®Œå…¨å…±äº«Transformeræ¨¡å‹ä¸­é€ æˆäº†æ€§èƒ½å¦¥åã€‚UniForké€šè¿‡åœ¨æµ…å±‚å…±äº«è¡¨ç¤ºå­¦ä¹ ï¼Œåœ¨æ·±å±‚é‡‡ç”¨ä»»åŠ¡ç‰¹å®šçš„åˆ†æ”¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17213', 'title': 'Long-term Traffic Simulation with Interleaved Autoregressive Motion and\n  Scenario Generation', 'url': 'https://huggingface.co/papers/2506.17213', 'abstract': 'InfGen, a unified next-token prediction model, enables stable long-term traffic simulation by interleaving closed-loop motion simulation and scene generation.  \t\t\t\t\tAI-generated summary \t\t\t\t An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen', 'score': 4, 'issue_id': 4431, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': '0940c34932f5181b', 'authors': ['Xiuyu Yang', 'Shuhan Tan', 'Philipp KrÃ¤henbÃ¼hl'], 'affiliations': ['UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.17213.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#open_source', '#video'], 'emoji': 'ğŸš—', 'ru': {'title': 'InfGen: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'InfGen - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½. InfGen Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ² ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ (9 ÑĞµĞºÑƒĞ½Ğ´), Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ (30 ÑĞµĞºÑƒĞ½Ğ´) Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°Ñ„Ğ¸ĞºĞ°.'}, 'en': {'title': 'InfGen: Revolutionizing Long-Term Traffic Simulation with Unified Prediction', 'desc': 'InfGen is a novel model designed for traffic simulation that predicts the next movement of vehicles in a scene. It combines two key processes: closed-loop motion simulation, which tracks the movement of vehicles, and scene generation, which creates the environment around them. This interleaving allows InfGen to maintain stability during long-term simulations, addressing the challenges faced by previous models that struggled with dynamic agent interactions. The model demonstrates superior performance in both short-term and long-term traffic simulations, making it a significant advancement in the field.'}, 'zh': {'title': 'InfGenï¼šç¨³å®šçš„é•¿æœŸäº¤é€šä»¿çœŸæ–°æ–¹æ³•', 'desc': 'InfGenæ˜¯ä¸€ç§ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡äº¤æ›¿è¿›è¡Œé—­ç¯è¿åŠ¨ä»¿çœŸå’Œåœºæ™¯ç”Ÿæˆï¼Œå®ç°ç¨³å®šçš„é•¿æœŸäº¤é€šä»¿çœŸã€‚ä¼ ç»Ÿæ¨¡å‹ä¸»è¦å…³æ³¨åˆå§‹ä»£ç†çš„é—­ç¯è¿åŠ¨ä»¿çœŸï¼Œè¿™åœ¨é•¿æœŸä»¿çœŸä¸­å­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºä»£ç†åœ¨è‡ªé©¾è½¦è¿›å…¥æ–°åŒºåŸŸæ—¶ä¼šè¿›å‡ºåœºæ™¯ã€‚InfGenèƒ½å¤Ÿè‡ªåŠ¨åˆ‡æ¢é—­ç¯è¿åŠ¨ä»¿çœŸå’Œåœºæ™¯ç”Ÿæˆæ¨¡å¼ï¼Œä»è€Œå®ç°ç¨³å®šçš„é•¿æœŸä»¿çœŸã€‚è¯¥æ¨¡å‹åœ¨çŸ­æœŸï¼ˆ9ç§’ï¼‰äº¤é€šä»¿çœŸä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨é•¿æœŸï¼ˆ30ç§’ï¼‰ä»¿çœŸä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15925', 'title': 'Reranking-based Generation for Unbiased Perspective Summarization', 'url': 'https://huggingface.co/papers/2506.15925', 'abstract': 'Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.', 'score': 4, 'issue_id': 4425, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '8cb1a06d9566bcfc', 'authors': ['Narutatsu Ri', 'Nicholas Deas', 'Kathleen McKeown'], 'affiliations': ['Department of Computer Science, Columbia University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15925.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#dataset', '#interpretability', '#alignment', '#training', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¾ÑÑÑ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ².'}, 'en': {'title': 'Enhancing Perspective Summaries with Reranking and Preference Tuning', 'desc': 'This paper discusses how to improve the quality of summaries generated by Large Language Models (LLMs) for political perspectives. It highlights the limitations of traditional evaluation metrics, which do not effectively measure important aspects like coverage and faithfulness. The authors propose new, reliable metrics based on language models and demonstrate that these outperform traditional methods. They also show that using reranking and preference tuning techniques can significantly enhance the performance of LLM-generated summaries.'}, 'zh': {'title': 'æå‡è§‚ç‚¹æ‘˜è¦è´¨é‡çš„å…³é”®åœ¨äºé‡æ’åºä¸åå¥½è°ƒä¼˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„è§‚ç‚¹æ‘˜è¦çš„è´¨é‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡åœ¨æµ‹é‡æ‘˜è¦çš„è¦†ç›–ç‡å’Œå¿ å®åº¦æ—¶æ•ˆæœä¸ä½³ï¼Œè€ŒåŸºäºè¯­è¨€æ¨¡å‹çš„æŒ‡æ ‡è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚é€šè¿‡å»ºç«‹ä¸€ä¸ªåŸºå‡†æµ‹è¯•é›†å¹¶è¿›è¡Œäººç±»æ ‡æ³¨ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™äº›æ–°æŒ‡æ ‡çš„å¯é æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†é‡æ’åºå’Œåå¥½è°ƒä¼˜çš„æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ‘˜è¦ç”Ÿæˆçš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09930', 'title': 'From Intention to Execution: Probing the Generalization Boundaries of\n  Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2506.09930', 'abstract': 'A unified benchmark suite evaluates Vision-Language-Action models\' generalization and motor execution capabilities, highlighting the disparity between perceptual understanding and precise action execution.  \t\t\t\t\tAI-generated summary \t\t\t\t One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, "generalist" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM\'s generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/', 'score': 4, 'issue_id': 4438, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '62044c4dafedefe4', 'authors': ['Irving Fang', 'Juexiao Zhang', 'Shengbang Tong', 'Chen Feng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.09930.jpg', 'data': {'categories': ['#cv', '#agi', '#optimization', '#survey', '#robotics', '#agents', '#benchmark', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ VLA Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision-Language-Action (VLA) Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VLA Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğµ Ñ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 50 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ.'}, 'en': {'title': 'Bridging the Gap: Evaluating Vision-Language-Action Models for Better Robot Execution', 'desc': "This paper presents a unified benchmark suite designed to evaluate Vision-Language-Action (VLA) models, focusing on their ability to generalize and execute motor actions. It highlights the gap between a model's perceptual understanding, derived from large Vision-Language Models (VLMs), and its actual performance in executing precise actions. The study reveals that while VLA models can plan effectively, they struggle with action execution, especially when faced with unfamiliar scenarios. The authors provide a comprehensive set of simulation-based tasks to facilitate standardized evaluations and encourage further research in bridging the perception-to-action divide."}, 'zh': {'title': 'ç¼©å°æ„ŸçŸ¥ä¸åŠ¨ä½œä¹‹é—´çš„å·®è·', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè¿åŠ¨æ‰§è¡Œèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰èƒ½å¤Ÿæä¾›è‰¯å¥½çš„æ„ŸçŸ¥ç†è§£å’Œé«˜å±‚æ¬¡è§„åˆ’ï¼Œä½†åœ¨é¢å¯¹ä¸åŒåˆ†å¸ƒçš„è§‚å¯Ÿæ—¶ï¼Œæ¨¡å‹çš„åŠ¨ä½œæ‰§è¡Œèƒ½åŠ›å¸¸å¸¸ä¸è¶³ã€‚æˆ‘ä»¬è®¾è®¡äº†50ä¸ªåŸºäºä»¿çœŸçš„ä»»åŠ¡ï¼Œæ¶µç›–è¯­è¨€æŒ‡ä»¤ã€è§†è§‰å’Œç‰©ä½“ç­‰å¤šä¸ªå­ç±»åˆ«ï¼Œä»¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°å½“å‰æœ€å…ˆè¿›çš„VLAæ¶æ„ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™ä¸ªåŸºå‡†å¥—ä»¶æ¨åŠ¨æœªæ¥VLAçš„ç ”ç©¶ï¼Œç¼©å°æ„ŸçŸ¥ä¸åŠ¨ä½œä¹‹é—´çš„å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17113', 'title': 'MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert\n  Aggregation', 'url': 'https://huggingface.co/papers/2506.17113', 'abstract': 'MEXA is a training-free framework that aggregates outputs from specialized expert models using a Large Reasoning Model for effective multimodal reasoning across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Combining pre-trained expert models offers substantial potential for scalable multimodal reasoning, but building a unified framework remains challenging due to the increasing diversity of input modalities and task complexity. For instance, medical diagnosis requires precise reasoning over structured clinical tables, while financial forecasting depends on interpreting plot-based data to make informed predictions. To tackle this challenge, we introduce MEXA, a training-free framework that performs modality- and task-aware aggregation of multiple expert models to enable effective multimodal reasoning across diverse and distinct domains. MEXA dynamically selects expert models based on the input modality and the task-specific reasoning demands (i.e., skills). Each expert model, specialized in a modality task pair, generates interpretable textual reasoning outputs. MEXA then aggregates and reasons over these outputs using a Large Reasoning Model (LRM) to produce the final answer. This modular design allows flexible and transparent multimodal reasoning across diverse domains without additional training overhead. We extensively evaluate our approach on diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA. MEXA consistently delivers performance improvements over strong multimodal baselines, highlighting the effectiveness and broad applicability of our expert-driven selection and aggregation in diverse multimodal reasoning tasks.', 'score': 2, 'issue_id': 4439, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': '55a36cc78de699ca', 'authors': ['Shoubin Yu', 'Yue Zhang', 'Ziyang Wang', 'Jaehong Yoon', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.17113.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MEXA: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'MEXA - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM). MEXA Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'MEXA: Expert-Driven Multimodal Reasoning Without Training', 'desc': 'MEXA is a novel framework designed for multimodal reasoning that does not require additional training. It effectively combines outputs from specialized expert models tailored to different input modalities and tasks. By dynamically selecting the appropriate expert models based on the specific reasoning needs, MEXA aggregates their outputs using a Large Reasoning Model to generate coherent and interpretable results. This approach enhances performance across various domains, such as medical diagnosis and financial forecasting, while maintaining flexibility and transparency in the reasoning process.'}, 'zh': {'title': 'MEXAï¼šæ— è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†æ¡†æ¶', 'desc': 'MEXAæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨å¤§å‹æ¨ç†æ¨¡å‹èšåˆæ¥è‡ªä¸“ä¸šä¸“å®¶æ¨¡å‹çš„è¾“å‡ºï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„æ¨¡æ€å’Œç‰¹å®šä»»åŠ¡çš„æ¨ç†éœ€æ±‚åŠ¨æ€é€‰æ‹©ä¸“å®¶æ¨¡å‹ï¼Œä»è€Œå¤„ç†ä¸åŒé¢†åŸŸçš„å¤æ‚ä»»åŠ¡ã€‚æ¯ä¸ªä¸“å®¶æ¨¡å‹ä¸“æ³¨äºç‰¹å®šçš„æ¨¡æ€ä»»åŠ¡å¯¹ï¼Œç”Ÿæˆå¯è§£é‡Šçš„æ–‡æœ¬æ¨ç†è¾“å‡ºã€‚MEXAé€šè¿‡èšåˆè¿™äº›è¾“å‡ºï¼Œåˆ©ç”¨å¤§å‹æ¨ç†æ¨¡å‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œå±•ç°äº†åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17090', 'title': 'Better Language Model Inversion by Compactly Representing Next-Token\n  Distributions', 'url': 'https://huggingface.co/papers/2506.17090', 'abstract': "A new method called Prompt Inversion from Logprob Sequences (PILS) recovers hidden prompts in language models by analyzing the low-dimensional subspace of the model's next-token probabilities, achieving higher recovery rates and better generalization than previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.", 'score': 1, 'issue_id': 4443, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': '1da03dddd925b45e', 'authors': ['Murtaza Nazir', 'Matthew Finlayson', 'John X. Morris', 'Xiang Ren', 'Swabha Swayamdipta'], 'affiliations': ['Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2506.17090.jpg', 'data': {'categories': ['#security', '#rlhf', '#hallucinations', '#data', '#transfer_learning', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'PILS: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ PILS (Prompt Inversion from Logprob Sequences) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. PILS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ¾ Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¶Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² 2-3.5 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking Hidden Prompts with PILS: A New Era in Language Model Inversion', 'desc': "The paper introduces a novel technique called Prompt Inversion from Logprob Sequences (PILS) that enhances the recovery of hidden prompts in language models by utilizing the model's next-token probabilities. By recognizing that these probabilities exist within a low-dimensional subspace, the method allows for efficient compression of information, leading to significantly improved recovery rates compared to existing approaches. PILS achieves recovery rates that are 2 to 3.5 times higher, demonstrating its effectiveness even in more complex scenarios like hidden system message recovery. Additionally, the method shows strong generalization capabilities, indicating that it can adapt well to varying conditions during testing."}, 'zh': {'title': 'æ­ç¤ºéšè—æç¤ºçš„æ–°æ–¹æ³•ï¼šPILS', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºå¯¹æ•°æ¦‚ç‡åºåˆ—çš„æç¤ºåæ¼”ï¼ˆPILSï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åˆ†æè¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¦‚ç‡çš„ä½ç»´å­ç©ºé—´æ¥æ¢å¤éšè—çš„æç¤ºã€‚è¯¥æ–¹æ³•åœ¨æ¢å¤ç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºä»¥å¾€çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å¤šä¸ªç”Ÿæˆæ­¥éª¤ä¸­æå–ä¿¡æ¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹çš„å‘é‡è¾“å‡ºå æ®äº†ä¸€ä¸ªä½ç»´å­ç©ºé—´ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ— æŸå‹ç¼©å®Œæ•´çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPILSåœ¨æ¢å¤éšè—æç¤ºæ–¹é¢çš„å‡†ç¡®ç‡æé«˜äº†2åˆ°3.5å€ï¼Œä¸”åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16349', 'title': 'Watermarking Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2506.16349', 'abstract': 'A novel watermarking technique for autoregressive image generation models achieves reliable detection through improved reverse cycle-consistency and synchronization layers.  \t\t\t\t\tAI-generated summary \t\t\t\t Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance. Despite significant interest in autoregressive image generation models and their potential for misuse, no prior work has attempted to watermark their outputs at the token level. In this work, we present the first such approach by adapting language model watermarking techniques to this setting. We identify a key challenge: the lack of reverse cycle-consistency (RCC), wherein re-tokenizing generated image tokens significantly alters the token sequence, effectively erasing the watermark. To address this and to make our method robust to common image transformations, neural compression, and removal attacks, we introduce (i) a custom tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a complementary watermark synchronization layer. As our experiments demonstrate, our approach enables reliable and robust watermark detection with theoretically grounded p-values.', 'score': 1, 'issue_id': 4443, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '9da42722b7d365e0', 'authors': ['Nikola JovanoviÄ‡', 'Ismail Labiad', 'TomÃ¡Å¡ SouÄek', 'Martin Vechev', 'Pierre Fernandez'], 'affiliations': ['ETH Zurich', 'Meta FAIR', 'UniversitÃ© Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2506.16349.jpg', 'data': {'categories': ['#security', '#multimodal', '#cv', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ² Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°-Ğ´ĞµÑ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ p-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Watermarking Autoregressive Models: A New Era of Image Provenance', 'desc': 'This paper introduces a new watermarking technique specifically designed for autoregressive image generation models. The method enhances reverse cycle-consistency (RCC) to ensure that the watermark remains intact even after re-tokenizing the generated images. Additionally, it incorporates a synchronization layer to improve the robustness of the watermark against various image transformations and attacks. Experimental results show that this approach allows for reliable detection of watermarks, supported by statistically significant p-values.'}, 'zh': {'title': 'è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¯é æ°´å°æŠ€æœ¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ°´å°æŠ€æœ¯ï¼Œä¸“é—¨ç”¨äºè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚è¯¥æŠ€æœ¯é€šè¿‡æ”¹è¿›çš„åå‘å¾ªç¯ä¸€è‡´æ€§å’ŒåŒæ­¥å±‚ï¼Œå®ç°äº†å¯é çš„æ°´å°æ£€æµ‹ã€‚æˆ‘ä»¬é¦–æ¬¡å°†è¯­è¨€æ¨¡å‹çš„æ°´å°æŠ€æœ¯åº”ç”¨äºå›¾åƒç”Ÿæˆçš„è¾“å‡ºï¼Œè§£å†³äº†é‡æ ‡è®°ç”Ÿæˆå›¾åƒä»¤ç‰Œæ—¶æ°´å°è¢«æ¶ˆé™¤çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢å¯¹å¸¸è§å›¾åƒå˜æ¢å’Œæ”»å‡»æ—¶ï¼Œèƒ½å¤Ÿä¿æŒæ°´å°çš„å¯é æ€§å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21115', 'title': 'Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA', 'url': 'https://huggingface.co/papers/2505.21115', 'abstract': 'EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. A key yet underexplored factor contributing to this is the temporality of questions -- whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, a lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4o retrieval behavior.', 'score': 81, 'issue_id': 4192, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'cbbff6b511a277fa', 'authors': ['Sergey Pletenev', 'Maria Marina', 'Nikolay Ivanov', 'Daria Galimzianova', 'Nikita Krayko', 'Mikhail Salnikov', 'Vasily Konovalov', 'Alexander Panchenko', 'Viktor Moskvoretskii'], 'affiliations': ['AIRI', 'HSE University', 'MIPT', 'MTS AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.21115.jpg', 'data': {'categories': ['#benchmark', '#training', '#low_resource', '#dataset', '#multilingual', '#long_context', '#hallucinations'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'EverGreenQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': "EverGreenQA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚ĞºĞ¸ 'Ğ²ĞµÑ‡Ğ½Ğ¾Ğ·ĞµĞ»ĞµĞ½Ñ‹Ñ…' Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 12 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… ÑĞ²Ğ½Ğ¾Ğµ Ğ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ EG-E5, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ 'Ğ²ĞµÑ‡Ğ½Ğ¾Ğ·ĞµĞ»ĞµĞ½Ñ‹Ñ…' Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²."}, 'en': {'title': 'Understanding Question Timeliness with EverGreenQA', 'desc': 'The paper introduces EverGreenQA, a new multilingual question answering (QA) dataset designed to evaluate how well large language models (LLMs) understand the concept of temporality in questions. It distinguishes between evergreen questions, which have stable answers, and mutable questions, which can change over time. The authors benchmark 12 LLMs using this dataset to see if they can explicitly or implicitly recognize question temporality through verbalized judgments and uncertainty signals. Additionally, they present EG-E5, a lightweight multilingual classifier that achieves state-of-the-art performance and demonstrate its usefulness in various applications, such as enhancing self-knowledge estimation and filtering QA datasets.'}, 'zh': {'title': 'æ­ç¤ºé—®ç­”ä¸­çš„æ—¶é—´æ€§ï¼šEverGreenQAæ•°æ®é›†', 'desc': 'EverGreenQAæ˜¯ä¸€ä¸ªå¤šè¯­è¨€é—®ç­”æ•°æ®é›†ï¼Œä¸“æ³¨äºæ—¶é—´æ€§ç¼–ç ï¼Œç‰¹åˆ«æ˜¯é—®é¢˜çš„æŒä¹…æ€§ã€‚è¯¥æ•°æ®é›†é€šè¿‡æ°¸æ’æ ‡ç­¾æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé—®é¢˜çš„æ—¶é—´æ€§ï¼ˆå¦‚æ°¸æ’æ€§æˆ–å¯å˜æ€§ï¼‰å¯¹LLMsçš„å›ç­”å‡†ç¡®æ€§æœ‰é‡è¦å½±å“ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†EG-E5ï¼Œä¸€ä¸ªè½»é‡çº§çš„å¤šè¯­è¨€åˆ†ç±»å™¨ï¼Œåœ¨è¿™ä¸€ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01111', 'title': 'FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion', 'url': 'https://huggingface.co/papers/2506.01111', 'abstract': 'A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.', 'score': 25, 'issue_id': 4186, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': 'a649684de588a812', 'authors': ['Shunian Chen', 'Xinyuan Xie', 'Zheshu Chen', 'Liyan Zhao', 'Owen Lee', 'Zhan Su', 'Qilin Sun', 'Benyou Wang'], 'affiliations': ['South China University of Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.01111.jpg', 'data': {'categories': ['#dataset', '#open_source', '#audio', '#multimodal', '#optimization', '#data', '#games'], 'emoji': 'ğŸ§', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ…: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ FusionAudio - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1,2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ 6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FusionAudio, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CLAP Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Enhancing Audio Captions with Multimodal Insights', 'desc': 'This paper presents a two-stage pipeline that improves the quality of audio captions by using specialized pretrained models alongside a large language model (LLM). The first stage extracts various contextual cues from audio, such as speech and music, as well as visual information from related videos. In the second stage, the LLM synthesizes these multimodal inputs to create detailed and contextually accurate captions. The work introduces a new dataset, FusionAudio, which contains 1.2 million detailed captions and enhances audio models for better audio-text alignment.'}, 'zh': {'title': 'æå‡éŸ³é¢‘å­—å¹•è´¨é‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µç®¡é“ï¼Œåˆ©ç”¨ä¸“é—¨çš„é¢„è®­ç»ƒæ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¥æé«˜éŸ³é¢‘å­—å¹•çš„è´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–å¤šæ ·çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œå¦‚è¯­éŸ³ã€éŸ³ä¹å’Œè§†è§‰ä¿¡æ¯ï¼Œæ¥å¢å¼ºéŸ³é¢‘ç†è§£ã€‚ç„¶åï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç»¼åˆè¿™äº›å¤šæ¨¡æ€è¾“å…¥ï¼Œç”Ÿæˆè¯¦ç»†ä¸”å…·æœ‰ä¸Šä¸‹æ–‡æ„è¯†çš„éŸ³é¢‘å­—å¹•ã€‚æ­¤ç ”ç©¶çš„å…³é”®è´¡çŒ®åŒ…æ‹¬å¯æ‰©å±•çš„ç»†ç²’åº¦éŸ³é¢‘å­—å¹•ç”Ÿæˆæ–¹æ³•å’Œä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†FusionAudioï¼ŒåŒ…å«120ä¸‡æ¡è¯¦ç»†å­—å¹•å’Œ600ä¸‡å¯¹é—®ç­”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05523', 'title': 'MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2506.05523', 'abstract': 'MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in vision-language models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problem-solving, neglecting the broader spectrum of reasoning skills -- including abstract, physical, planning, spatial, and temporal capabilities -- required for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), a video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics -- enabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systems -- including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models -- reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research.', 'score': 24, 'issue_id': 4188, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '524394ef06ba3ba1', 'authors': ['Zikui Cai', 'Andrew Wang', 'Anirudh Satheesh', 'Ankit Nakhawa', 'Hyunwoo Jae', 'Keenan Powell', 'Minghui Liu', 'Neel Jay', 'Sungbin Oh', 'Xiyao Wang', 'Yongyuan Liang', 'Tom Goldstein', 'Furong Huang'], 'affiliations': ['Capital One', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.05523.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey', '#video', '#open_source', '#dataset', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'MORSE-500: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'MORSE-500 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 500 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'MORSE-500: Evolving Benchmark for Multimodal Reasoning', 'desc': 'MORSE-500 is a new video benchmark designed to evaluate multimodal reasoning in AI across six different categories. It addresses limitations in existing benchmarks by incorporating dynamic video clips instead of static images, allowing for a more realistic assessment of reasoning skills. The benchmark includes a variety of reasoning tasks, such as abstract thinking and planning, which are essential for advanced multimodal intelligence. By providing a scalable and evolving dataset, MORSE-500 aims to facilitate ongoing research and development in multimodal reasoning capabilities.'}, 'zh': {'title': 'MORSE-500ï¼šå¤šæ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†', 'desc': 'MORSE-500æ˜¯ä¸€ä¸ªåŒ…å«500ä¸ªè„šæœ¬åŒ–è§†é¢‘ç‰‡æ®µçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ¶µç›–å…­ä¸ªäº’è¡¥çš„æ¨ç†ç±»åˆ«ï¼Œå¼ºè°ƒäº†åœ¨æŠ½è±¡å’Œè§„åˆ’ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®è·ã€‚ä¸é™æ€å›¾åƒåŸºå‡†ä¸åŒï¼ŒMORSE-500èƒ½å¤Ÿæ•æ‰ç°å®ç¯å¢ƒçš„æ—¶é—´å¤æ‚æ€§ï¼Œå¹¶æ”¯æŒç”Ÿæˆå…·æœ‰ä¸åŒéš¾åº¦çš„æ–°å®ä¾‹ã€‚é€šè¿‡æä¾›å®Œæ•´çš„æ•°æ®é›†å’Œç”Ÿæˆè„šæœ¬ï¼ŒMORSE-500ä¸ºå¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†é€æ˜å’Œå¯é‡å¤çš„æ”¯æŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05629', 'title': 'Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs', 'url': 'https://huggingface.co/papers/2506.05629', 'abstract': 'A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of large language models in domain-specific tasks necessitates fine-tuning, which is computationally expensive and technically challenging. This paper focuses on parameter-efficient fine-tuning using soft prompting, a promising approach that adapts pre-trained models to downstream tasks by learning a small set of parameters. We propose a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that generates soft prompts based on the input tokens and attends different tokens with varying importance. Our method is simple and efficient, keeping the number of trainable parameters small. We show the merits of the proposed approach compared to state-of-the-art techniques on various tasks and show the improved zero shot domain transfer capability.', 'score': 23, 'issue_id': 4186, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'c88ec16aee1c43d5', 'authors': ['Ananth Muppidi', 'Abhilash Nandy', 'Sambaran Bandyopadhyay'], 'affiliations': ['Adobe Research, India', 'IIIT Hyderabad, India', 'IIT Kharagpur, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.05629.jpg', 'data': {'categories': ['#training', '#optimization', '#transfer_learning', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Input Dependent Soft Prompting with self-Attention Mechanism (ID-SPAM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼. ID-SPAM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² zero-shot Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹.'}, 'en': {'title': 'Efficient Fine-Tuning with Input-Dependent Soft Prompts', 'desc': 'This paper introduces a new technique called Input Dependent Soft Prompting with a Self-Attention Mechanism (ID-SPAM) to enhance fine-tuning of large language models. It focuses on making the fine-tuning process more efficient by using a small set of parameters that adapt the model to specific tasks. The self-attention mechanism allows the model to weigh the importance of different input tokens when generating soft prompts. The results demonstrate that ID-SPAM outperforms existing methods, particularly in zero-shot domain transfer scenarios.'}, 'zh': {'title': 'è¾“å…¥ä¾èµ–çš„è½¯æç¤ºï¼Œæå‡å¾®è°ƒæ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨è¾“å…¥ä¾èµ–çš„è½¯æç¤ºå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒèƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å­¦ä¹ ä¸€å°ç»„å‚æ•°ï¼Œé€‚åº”é¢„è®­ç»ƒæ¨¡å‹åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”ŸæˆåŸºäºè¾“å…¥æ ‡è®°çš„è½¯æç¤ºï¼Œå¹¶å¯¹ä¸åŒçš„é‡è¦æ€§æ ‡è®°è¿›è¡Œå…³æ³¨ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶æå‡äº†é›¶-shoté¢†åŸŸè¿ç§»èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05446', 'title': 'Sentinel: SOTA model to protect against prompt injections', 'url': 'https://huggingface.co/papers/2506.05446', 'abstract': "Sentinel, a detection model based on ModernBERT-large, effectively identifies prompt injection attacks with high accuracy and outperforms existing baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly powerful but remain vulnerable to prompt injection attacks, where malicious inputs cause the model to deviate from its intended instructions. This paper introduces Sentinel, a novel detection model, qualifire/prompt-injection-sentinel, based on the \\answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced features and fine-tuning on an extensive and diverse dataset comprising a few open-source and private collections, Sentinel achieves state-of-the-art performance. This dataset amalgamates varied attack types, from role-playing and instruction hijacking to attempts to generate biased content, alongside a broad spectrum of benign instructions, with private datasets specifically targeting nuanced error correction and real-world misclassifications. On a comprehensive, unseen internal test set, Sentinel demonstrates an average accuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on public benchmarks, it consistently outperforms strong baselines like protectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's architecture, its meticulous dataset curation, its training methodology, and a thorough evaluation, highlighting its superior detection capabilities.", 'score': 17, 'issue_id': 4199, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '293cbf4b92765fe2', 'authors': ['Dror Ivry', 'Oran Nahum'], 'affiliations': ['Qualiï¬re, Tel Aviv, IL'], 'pdf_title_img': 'assets/pdf/title_img/2506.05446.jpg', 'data': {'categories': ['#architecture', '#data', '#security', '#ethics', '#dataset', '#benchmark', '#training'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Sentinel: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Sentinel - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ ModernBERT-large. Sentinel Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (0.987) Ğ¸ F1-Ğ¼ĞµÑ€Ñƒ (0.980) Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¾Ğ±Ñ€Ğ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Sentinel, Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Sentinel: Safeguarding LLMs from Prompt Injection Attacks', 'desc': 'The paper presents Sentinel, a detection model built on the ModernBERT-large architecture, designed to identify prompt injection attacks in large language models (LLMs). Prompt injection attacks can manipulate LLMs into producing unintended outputs, making effective detection crucial. Sentinel is trained on a diverse dataset that includes various attack types and benign instructions, achieving high accuracy and F1-scores in its evaluations. The results show that Sentinel outperforms existing models, demonstrating its potential as a robust solution for enhancing the security of LLMs against such vulnerabilities.'}, 'zh': {'title': 'Sentinelï¼šé«˜æ•ˆè¯†åˆ«æç¤ºæ³¨å…¥æ”»å‡»çš„æ£€æµ‹æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSentinelçš„æ£€æµ‹æ¨¡å‹ï¼ŒåŸºäºModernBERT-largeæ¶æ„ï¼Œèƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«æç¤ºæ³¨å…¥æ”»å‡»ã€‚æç¤ºæ³¨å…¥æ”»å‡»æ˜¯æŒ‡æ¶æ„è¾“å…¥å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹åç¦»é¢„æœŸæŒ‡ä»¤çš„æƒ…å†µã€‚Sentinelé€šè¿‡åœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¾¾åˆ°äº†å…ˆè¿›çš„æ£€æµ‹æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º0.987ï¼ŒF1åˆ†æ•°ä¸º0.980ã€‚è¯¥æ¨¡å‹åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ£€æµ‹èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01872', 'title': 'Is Extending Modality The Right Path Towards Omni-Modality?', 'url': 'https://huggingface.co/papers/2506.01872', 'abstract': 'Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Omni-modal language models (OLMs) aim to integrate and reason over diverse input modalities--such as text, images, video, and audio--while maintaining strong language capabilities. Despite recent advancements, existing models, especially open-source ones, remain far from true omni-modality, struggling to generalize beyond the specific modality pairs they are trained on or to achieve strong performance when processing multi-modal inputs. We study the effect of extending modality, the dominant technique for training multimodal models, where an off-the-shelf language model is fine-tuned on target-domain and language data. Specifically, we investigate three key questions: (1) Does modality extension compromise core language abilities? (2) Can model merging effectively integrate independently fine-tuned modality-specific models to achieve omni-modality? (3) Does omni-modality extension lead to better knowledge sharing and generalization compared to sequential extension? Through extensive experiments, we analyze these trade-offs and provide insights into the feasibility of achieving true omni-modality using current approaches.', 'score': 16, 'issue_id': 4193, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'f876844db3f1bfbd', 'authors': ['Tinghui Zhu', 'Kai Zhang', 'Muhao Chen', 'Yu Su'], 'affiliations': ['The Ohio State University', 'University of California, Davis'], 'pdf_title_img': 'assets/pdf/title_img/2506.01872.jpg', 'data': {'categories': ['#training', '#agi', '#multimodal', '#open_source', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑƒÑ‚ÑŒ Ğº Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Unlocking True Omni-Modality in Language Models', 'desc': 'This research explores how extending the types of data (modalities) and merging different models can help omni-modal language models (OLMs) maintain their language skills and improve their ability to generalize across various inputs. OLMs are designed to work with multiple forms of data, like text and images, but often struggle to perform well when faced with new combinations of these inputs. The study examines whether adding new modalities affects the language capabilities of these models, if merging models trained on different modalities can create a more effective omni-modal model, and whether this approach enhances knowledge sharing and generalization. Through experiments, the paper provides valuable insights into the challenges and potential solutions for achieving true omni-modality in language models.'}, 'zh': {'title': 'å®ç°çœŸæ­£çš„å…¨æ¨¡æ€èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©å±•æ¨¡æ€å’Œæ¨¡å‹åˆå¹¶å¯¹å…¨æ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ä¿æŒè¯­è¨€èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„å½±å“ã€‚å…¨æ¨¡æ€è¯­è¨€æ¨¡å‹æ—¨åœ¨æ•´åˆå’Œæ¨ç†å¤šç§è¾“å…¥æ¨¡æ€ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚å°½ç®¡å·²æœ‰è¿›å±•ï¼Œç°æœ‰æ¨¡å‹åœ¨çœŸæ­£çš„å…¨æ¨¡æ€èƒ½åŠ›ä¸Šä»ç„¶å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒåˆ†æäº†æ‰©å±•æ¨¡æ€å¯¹æ ¸å¿ƒè¯­è¨€èƒ½åŠ›çš„å½±å“ï¼Œä»¥åŠæ¨¡å‹åˆå¹¶æ˜¯å¦èƒ½æœ‰æ•ˆæ•´åˆç‹¬ç«‹å¾®è°ƒçš„æ¨¡æ€ç‰¹å®šæ¨¡å‹ï¼Œä»¥å®ç°å…¨æ¨¡æ€èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05573', 'title': 'PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.05573', 'abstract': 'PartCrafter generates complex 3D scenes from single images using a unified compositional architecture with a diffusion transformer, enabling part-aware generation and hierarchical attention.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PartCrafter, the first structured 3D generative model that jointly synthesizes multiple semantically meaningful and geometrically distinct 3D meshes from a single RGB image. Unlike existing methods that either produce monolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an image and then reconstructing each segment, PartCrafter adopts a unified, compositional generation architecture that does not rely on pre-segmented inputs. Conditioned on a single image, it simultaneously denoises multiple 3D parts, enabling end-to-end part-aware generation of both individual objects and complex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh diffusion transformer (DiT) trained on whole objects, inheriting the pretrained weights, encoder, and decoder, and introduces two key innovations: (1) A compositional latent space, where each 3D part is represented by a set of disentangled latent tokens; (2) A hierarchical attention mechanism that enables structured information flow both within individual parts and across all parts, ensuring global coherence while preserving part-level detail during generation. To support part-level supervision, we curate a new dataset by mining part-level annotations from large-scale 3D object datasets. Experiments show that PartCrafter outperforms existing approaches in generating decomposable 3D meshes, including parts that are not directly visible in input images, demonstrating the strength of part-aware generative priors for 3D understanding and synthesis. Code and training data will be released.', 'score': 14, 'issue_id': 4191, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'af089266a7086a2d', 'authors': ['Yuchen Lin', 'Chenguo Lin', 'Panwang Pan', 'Honglei Yan', 'Yiqiang Feng', 'Yadong Mu', 'Katerina Fragkiadaki'], 'affiliations': ['ByteDance', 'Carnegie Mellon University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05573.jpg', 'data': {'categories': ['#3d', '#diffusion', '#open_source', '#architecture', '#dataset'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'PartCrafter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹. PartCrafter Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¸, Ğ½Ğµ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Transforming Images into 3D Worlds with PartCrafter!', 'desc': 'PartCrafter is a novel 3D generative model that creates detailed 3D scenes from a single RGB image without needing pre-segmented inputs. It uses a unified architecture that combines part-aware generation with hierarchical attention, allowing it to generate multiple distinct 3D meshes simultaneously. The model leverages a pretrained diffusion transformer to enhance the quality of the generated parts and maintains coherence across the entire scene. By introducing a compositional latent space and a new dataset for part-level supervision, PartCrafter significantly improves the generation of complex 3D structures, even including parts not visible in the original image.'}, 'zh': {'title': 'PartCrafterï¼šä»å•å›¾åƒç”Ÿæˆå¤æ‚3Dåœºæ™¯çš„åˆ›æ–°æ¨¡å‹', 'desc': 'PartCrafteræ˜¯ä¸€ç§æ–°å‹çš„3Dç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥ä»å•å¼ RGBå›¾åƒç”Ÿæˆå¤šä¸ªè¯­ä¹‰æ˜ç¡®ä¸”å‡ ä½•ä¸Šä¸åŒçš„3Dç½‘æ ¼ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒPartCrafteré‡‡ç”¨ç»Ÿä¸€çš„ç”Ÿæˆæ¶æ„ï¼Œæ— éœ€é¢„å…ˆåˆ†å‰²è¾“å…¥å›¾åƒï¼Œèƒ½å¤ŸåŒæ—¶å»å™ªå¤šä¸ª3Déƒ¨åˆ†ã€‚è¯¥æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçš„3Dç½‘æ ¼æ‰©æ•£å˜æ¢å™¨ï¼Œå¹¶å¼•å…¥äº†ç»„åˆæ½œåœ¨ç©ºé—´å’Œå±‚æ¬¡æ³¨æ„æœºåˆ¶ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„3Dåœºæ™¯åœ¨å…¨å±€ä¸€è‡´æ€§çš„åŒæ—¶ä¿ç•™éƒ¨åˆ†ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPartCrafteråœ¨ç”Ÿæˆå¯åˆ†è§£çš„3Dç½‘æ ¼æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨3Dç†è§£å’Œåˆæˆä¸­çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06276', 'title': 'STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis', 'url': 'https://huggingface.co/papers/2506.06276', 'abstract': 'STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.', 'score': 12, 'issue_id': 4189, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '14f98c6826d7c6bb', 'authors': ['Jiatao Gu', 'Tianrong Chen', 'David Berthelot', 'Huangjie Zheng', 'Yuyang Wang', 'Ruixiang Zhang', 'Laurent Dinh', 'Miguel Angel Bautista', 'Josh Susskind', 'Shuangfei Zhai'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2506.06276.jpg', 'data': {'categories': ['#architecture', '#cv', '#diffusion'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'STARFlow - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾-Ğ¼ĞµĞ»ĞºÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. STARFlow Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑÑƒ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼.'}, 'en': {'title': 'STARFlow: Merging Flows and Transformers for High-Quality Image Generation', 'desc': 'STARFlow is a generative model that merges normalizing flows with autoregressive Transformers to create high-quality images. It introduces the Transformer Autoregressive Flow (TARFlow), which effectively models continuous distributions while maintaining scalability. Key innovations include a deep-shallow architecture for efficient computation, latent space modeling using pretrained autoencoders, and a novel guidance algorithm to enhance sample quality. This model achieves competitive results in both class-conditional and text-conditional image generation, marking a significant advancement in the use of normalizing flows for high-resolution image synthesis.'}, 'zh': {'title': 'STARFlowï¼šé«˜æ•ˆå›¾åƒåˆæˆçš„æ–°çºªå…ƒ', 'desc': 'STARFlowæ˜¯ä¸€ç§ç»“åˆäº†å½’ä¸€åŒ–æµå’Œè‡ªå›å½’å˜æ¢å™¨çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚å…¶æ ¸å¿ƒæ˜¯å˜æ¢å™¨è‡ªå›å½’æµï¼ˆTARFlowï¼‰ï¼Œå°†å½’ä¸€åŒ–æµçš„è¡¨è¾¾èƒ½åŠ›ä¸è‡ªå›å½’å˜æ¢å™¨çš„ç»“æ„å»ºæ¨¡èƒ½åŠ›ç›¸ç»“åˆã€‚é€šè¿‡æ·±æµ…è®¾è®¡ã€åœ¨é¢„è®­ç»ƒè‡ªç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´å»ºæ¨¡ä»¥åŠæ–°é¢–çš„å¼•å¯¼ç®—æ³•ï¼ŒSTARFlowæ˜¾è‘—æé«˜äº†å¯æ‰©å±•æ€§å’Œæ ·æœ¬è´¨é‡ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨è¿ç»­ç©ºé—´ä¸­è¿›è¡Œç²¾ç¡®çš„æœ€å¤§ä¼¼ç„¶è®­ç»ƒï¼Œä¸”åœ¨ç±»æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶çš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¥è¿‘æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05984', 'title': 'Audio-Aware Large Language Models as Judges for Speaking Styles', 'url': 'https://huggingface.co/papers/2506.05984', 'abstract': "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-aware large language models (ALLMs) can understand the textual and non-textual information in the audio input. In this paper, we explore using ALLMs as an automatic judge to assess the speaking styles of speeches. We use ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice style instruction following and role-playing. The speaking style we consider includes emotion, volume, speaking pace, word emphasis, pitch control, and non-verbal elements. We use four spoken language models (SLMs) to complete the two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators. These promising results show that ALLMs can be used as a judge to evaluate SLMs. Our results also reveal that current SLMs, even GPT-4o-audio, still have room for improvement in controlling the speaking style and generating natural dialogues.", 'score': 12, 'issue_id': 4185, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '10dcc4567ff634c1', 'authors': ['Cheng-Han Chiang', 'Xiaofei Wang', 'Chung-Ching Lin', 'Kevin Lin', 'Linjie Li', 'Radu Kopetz', 'Yao Qian', 'Zhendong Wang', 'Zhengyuan Yang', 'Hung-yi Lee', 'Lijuan Wang'], 'affiliations': ['Microsoft', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05984.jpg', 'data': {'categories': ['#multimodal', '#audio', '#interpretability', '#games'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞĞĞ‘Ğ›Ğœ ĞºĞ°Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑƒĞ´ÑŒĞ¸ ÑÑ‚Ğ¸Ğ»Ñ Ñ€ĞµÑ‡Ğ¸', 'desc': 'ĞÑƒĞ´Ğ¸Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (ĞĞĞ‘Ğ›Ğœ) ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸Ğ»Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´Ğ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹-ÑÑƒĞ´ĞµĞ¹. Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞĞĞ‘Ğ›Ğœ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµÑ‡ĞµĞ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (Ğ Ğ¯Ğœ) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾ ÑÑ‚Ğ¸Ğ»Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹. ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, ĞºĞ°Ğº ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸, Ğ³Ñ€Ğ¾Ğ¼ĞºĞ¾ÑÑ‚ÑŒ, Ñ‚ĞµĞ¼Ğ¿ Ñ€ĞµÑ‡Ğ¸, Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ², ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ñ‚Ğ¾Ğ½Ğ° Ğ¸ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Gemini Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… ÑÑƒĞ´ĞµĞ¹ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ° Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Evaluating Speech Styles with AI: ALLMs vs. Human Judges', 'desc': 'This paper discusses the capabilities of audio-aware large language models (ALLMs) in evaluating speaking styles from audio inputs. The authors demonstrate that ALLMs can assess synthesized speech similarly to human judges, focusing on aspects like emotion, volume, and pitch. They compare the performance of two ALLMs, GPT-4o-audio and Gemini-2.5-pro, against human evaluations in tasks involving voice style instruction and role-playing. The findings indicate that while ALLMs can effectively judge speaking styles, there is still potential for improvement in the speaking style control of current spoken language models (SLMs).'}, 'zh': {'title': 'éŸ³é¢‘æ„ŸçŸ¥æ¨¡å‹ï¼šè¯„ä¼°è¯´è¯é£æ ¼çš„æ–°å·¥å…·', 'desc': 'éŸ³é¢‘æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰èƒ½å¤Ÿè¯„ä¼°éŸ³é¢‘è¾“å…¥ä¸­çš„è¯´è¯é£æ ¼ï¼Œå…¶è¡¨ç°ä¸äººç±»è¯„å®¡åœ¨æƒ…æ„Ÿã€éŸ³é‡å’ŒéŸ³è°ƒç­‰ç»´åº¦ä¸Šçš„è¯„ä¼°ç›¸å½“ã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨ALLMsä½œä¸ºè‡ªåŠ¨è¯„å®¡è€…æ¥è¯„ä¼°æ¼”è®²çš„è¯´è¯é£æ ¼ã€‚æˆ‘ä»¬ä½¿ç”¨å››ä¸ªå£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼Œå¹¶é€šè¿‡äººç±»å’ŒALLMså¯¹SLMsçš„å“åº”è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒALLMså¯ä»¥ä½œä¸ºè¯„å®¡å·¥å…·æ¥è¯„ä¼°SLMsï¼Œä½†å½“å‰çš„SLMsåœ¨æ§åˆ¶è¯´è¯é£æ ¼å’Œç”Ÿæˆè‡ªç„¶å¯¹è¯æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06253', 'title': 'Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision', 'url': 'https://huggingface.co/papers/2506.06253', 'abstract': 'A survey reviews advancements in video understanding from both egocentric and exocentric perspectives, highlighting applications, tasks, joint learning frameworks, and limitations, with the aim of enhancing machine perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceiving the world from both egocentric (first-person) and exocentric (third-person) perspectives is fundamental to human cognition, enabling rich and complementary understanding of dynamic environments. In recent years, allowing the machines to leverage the synergistic potential of these dual perspectives has emerged as a compelling research direction in video understanding. In this survey, we provide a comprehensive review of video understanding from both exocentric and egocentric viewpoints. We begin by highlighting the practical applications of integrating egocentric and exocentric techniques, envisioning their potential collaboration across domains. We then identify key research tasks to realize these applications. Next, we systematically organize and review recent advancements into three main research directions: (1) leveraging egocentric data to enhance exocentric understanding, (2) utilizing exocentric data to improve egocentric analysis, and (3) joint learning frameworks that unify both perspectives. For each direction, we analyze a diverse set of tasks and relevant works. Additionally, we discuss benchmark datasets that support research in both perspectives, evaluating their scope, diversity, and applicability. Finally, we discuss limitations in current works and propose promising future research directions. By synthesizing insights from both perspectives, our goal is to inspire advancements in video understanding and artificial intelligence, bringing machines closer to perceiving the world in a human-like manner. A GitHub repo of related works can be found at https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.', 'score': 6, 'issue_id': 4192, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '7ddc25331945068e', 'authors': ['Yuping He', 'Yifei Huang', 'Guo Chen', 'Lidong Lu', 'Baoqi Pei', 'Jilan Xu', 'Tong Lu', 'Yoichi Sato'], 'affiliations': ['Fudan University, Shanghai 200433, China', 'State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China', 'University of Tokyo, Tokyo, Japan', 'Zhejiang University, Zhejiang 310027, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.06253.jpg', 'data': {'categories': ['#multimodal', '#video', '#survey', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ±Ğ·Ğ¾Ñ€ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Bridging Perspectives for Enhanced Video Understanding', 'desc': 'This paper surveys the progress in video understanding by examining both egocentric (first-person) and exocentric (third-person) perspectives. It emphasizes the importance of combining these viewpoints to enhance machine perception of dynamic environments. The authors categorize recent advancements into three main research directions: improving exocentric understanding with egocentric data, enhancing egocentric analysis with exocentric data, and developing joint learning frameworks. The survey also discusses practical applications, key research tasks, benchmark datasets, and identifies limitations in current research while suggesting future directions.'}, 'zh': {'title': 'èåˆè§†è§’ï¼Œæå‡è§†é¢‘ç†è§£', 'desc': 'è¿™ç¯‡è®ºæ–‡ç»¼è¿°äº†è§†é¢‘ç†è§£é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹å…³æ³¨è‡ªæˆ‘ä¸­å¿ƒï¼ˆç¬¬ä¸€äººç§°ï¼‰å’Œå¤–éƒ¨ä¸­å¿ƒï¼ˆç¬¬ä¸‰äººç§°ï¼‰è§†è§’çš„ç»“åˆã€‚é€šè¿‡æ•´åˆè¿™ä¸¤ç§è§†è§’ï¼Œç ”ç©¶è€…ä»¬å¸Œæœ›æå‡æœºå™¨å¯¹åŠ¨æ€ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚è®ºæ–‡è¿˜è¯†åˆ«äº†å®ç°è¿™äº›åº”ç”¨çš„å…³é”®ç ”ç©¶ä»»åŠ¡ï¼Œå¹¶ç³»ç»Ÿåœ°ç»„ç»‡äº†æœ€è¿‘çš„ç ”ç©¶è¿›å±•ã€‚æœ€åï¼Œä½œè€…è®¨è®ºäº†å½“å‰å·¥ä½œçš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æ¨åŠ¨è§†é¢‘ç†è§£å’Œäººå·¥æ™ºèƒ½çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06199', 'title': '3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model', 'url': 'https://huggingface.co/papers/2506.06199', 'abstract': 'A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Manipulation has long been a challenging task for robots, while humans can effortlessly perform complex interactions with objects, such as hanging a cup on the mug rack. A key reason is the lack of a large and uniform dataset for teaching robots manipulation skills. Current robot datasets often record robot action in different action spaces within a simple scene. This hinders the robot to learn a unified and robust action representation for different robots within diverse scenes. Observing how humans understand a manipulation task, we find that understanding how the objects should move in the 3D space is a critical clue for guiding actions. This clue is embodiment-agnostic and suitable for both humans and different robots. Motivated by this, we aim to learn a 3D flow world model from both human and robot manipulation data. This model predicts the future movement of the interacting objects in 3D space, guiding action planning for manipulation. Specifically, we synthesize a large-scale 3D optical flow dataset, named ManiFlow-110k, through a moving object auto-detect pipeline. A video diffusion-based world model then learns manipulation physics from these data, generating 3D optical flow trajectories conditioned on language instructions. With the generated 3D object optical flow, we propose a flow-guided rendering mechanism, which renders the predicted final state and leverages GPT-4o to assess whether the predicted flow aligns with the task description. This equips the robot with a closed-loop planning ability. Finally, we consider the predicted 3D optical flow as constraints for an optimization policy to determine a chunk of robot actions for manipulation. Extensive experiments demonstrate strong generalization across diverse robotic manipulation tasks and reliable cross-embodiment adaptation without hardware-specific training.', 'score': 4, 'issue_id': 4186, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'c1b9d0a9c29bdf3b', 'authors': ['Hongyan Zhi', 'Peihao Chen', 'Siyuan Zhou', 'Yubo Dong', 'Quanxi Wu', 'Lei Han', 'Mingkui Tan'], 'affiliations': ['Hong Kong University of Science and Technology', 'Pazhou Laboratory', 'South China University of Technology', 'Tencent Robotics'], 'pdf_title_img': 'assets/pdf/title_img/2506.06199.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#robotics', '#optimization', '#3d', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 3D-Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 3D-Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ¸ GPT-4o Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ManiFlow-110k Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Robots with 3D Flow for Versatile Manipulation', 'desc': 'This paper presents a novel 3D flow world model that enables robots to learn manipulation tasks by leveraging both human and robot data. The model predicts how objects move in 3D space, which helps robots plan their actions more effectively. By creating a large dataset called ManiFlow-110k and using a video diffusion approach, the researchers teach robots to understand manipulation physics and generate action plans based on language instructions. The results show that this method allows robots to generalize well across various tasks and adapt to different robotic embodiments without needing specific training for each hardware type.'}, 'zh': {'title': 'å­¦ä¹ 3DæµåŠ¨æ¨¡å‹ï¼Œæå‡æœºå™¨äººæ“ä½œèƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä»äººç±»å’Œæœºå™¨äººæ“ä½œæ•°æ®ä¸­å­¦ä¹ çš„3DæµåŠ¨ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨å¸®åŠ©æœºå™¨äººæ‰§è¡Œå¤šæ ·åŒ–çš„æ“ä½œä»»åŠ¡ã€‚é€šè¿‡åˆæˆä¸€ä¸ªåä¸ºManiFlow-110kçš„å¤§è§„æ¨¡3Då…‰æµæ•°æ®é›†ï¼Œæ¨¡å‹èƒ½å¤Ÿé¢„æµ‹äº¤äº’å¯¹è±¡åœ¨3Dç©ºé—´ä¸­çš„æœªæ¥è¿åŠ¨ã€‚åˆ©ç”¨è§†é¢‘æ‰©æ•£æŠ€æœ¯å’ŒGPT-4oï¼Œæ¨¡å‹ç”Ÿæˆçš„3Då…‰æµè½¨è¿¹å¯ä»¥æŒ‡å¯¼æœºå™¨äººçš„æ“ä½œè§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œè·¨å®ä½“é€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05433', 'title': 'Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward', 'url': 'https://huggingface.co/papers/2506.05433', 'abstract': 'Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) enhances policy learning by computing gradients from relative comparisons among candidate outputs that share a common input prefix. Despite its effectiveness, GRPO introduces substantial computational overhead when processing long shared prefixes, which must be redundantly encoded for each group member. This inefficiency becomes a major scalability bottleneck in long-context learning scenarios. We propose Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant prefix computation via a Shared-Prefix Forward strategy. In particular, by restructuring self-attention into two parts, our method enables the shared prefix to be encoded only once, while preserving full differentiability and compatibility with end-to-end training. We provide both theoretical and empirical evidence that Prefix Grouper is training-equivalent to standard GRPO: it yields identical forward outputs and backward gradients, ensuring that the optimization dynamics and final policy performance remain unchanged. Empirically, our experiments confirm that Prefix Grouper achieves consistent results while significantly reducing the computational cost of training, particularly in long-prefix scenarios. The proposed method is fully plug-and-play: it is compatible with existing GRPO-based architectures and can be seamlessly integrated into current training pipelines as a drop-in replacement, requiring no structural modifications and only minimal changes to input construction and attention computation. Prefix Grouper enables the use of larger group sizes under the same computational budget, thereby improving the scalability of GRPO to more complex tasks and larger models. Code is now available at https://github.com/johncaged/PrefixGrouper', 'score': 4, 'issue_id': 4192, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'fe9abfe1e500f3e2', 'authors': ['Zikang Liu', 'Tongtian Yue', 'Yepeng Tang', 'Longteng Guo', 'Junxian Cai', 'Qingbin Liu', 'Xi Chen', 'Jing Liu'], 'affiliations': ['Basic Algorithm Center, Tencent', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05433.jpg', 'data': {'categories': ['#long_context', '#optimization', '#architecture', '#training'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ“Ñ€ÑƒĞ¿Ğ¿ĞµÑ€: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ GRPO Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ“Ñ€ÑƒĞ¿Ğ¿ĞµÑ€ - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ GRPO (Group Relative Policy Optimization), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ†Ğ°. ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ“Ñ€ÑƒĞ¿Ğ¿ĞµÑ€ Ğ½Ğµ Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ°Ğ¼Ğ¸, Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GRPO.'}, 'en': {'title': 'Efficiently Scaling GRPO with Prefix Grouper', 'desc': 'The paper introduces Prefix Grouper, a novel algorithm designed to enhance the efficiency of Group Relative Policy Optimization (GRPO) by reducing computational overhead associated with encoding shared prefixes. By implementing a Shared-Prefix Forward strategy, it allows the shared prefix to be encoded only once, which significantly improves scalability in long-context scenarios without compromising the training dynamics or policy performance. The method maintains full differentiability and is compatible with end-to-end training, ensuring that the optimization process remains unchanged. Empirical results demonstrate that Prefix Grouper not only achieves consistent performance but also allows for larger group sizes within the same computational budget, making it a valuable addition to GRPO-based architectures.'}, 'zh': {'title': 'Prefix Grouperï¼šæå‡ GRPO çš„å¯æ‰©å±•æ€§', 'desc': 'Prefix Grouper æ˜¯ä¸€ç§é«˜æ•ˆçš„ GRPO è®­ç»ƒç®—æ³•ï¼Œé€šè¿‡å…±äº«å‰ç¼€çš„å‰å‘ç­–ç•¥ï¼Œæ¶ˆé™¤äº†å†—ä½™çš„å‰ç¼€è®¡ç®—ï¼Œä»è€Œå‡å°‘äº†è®¡ç®—å¼€é”€ã€‚è¯¥æ–¹æ³•å°†è‡ªæ³¨æ„åŠ›ç»“æ„é‡ç»„ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä½¿å¾—å…±äº«å‰ç¼€åªéœ€ç¼–ç ä¸€æ¬¡ï¼ŒåŒæ—¶ä¿æŒå®Œå…¨çš„å¯å¾®æ€§å’Œä¸ç«¯åˆ°ç«¯è®­ç»ƒçš„å…¼å®¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPrefix Grouper åœ¨é•¿å‰ç¼€åœºæ™¯ä¸­æ˜¾è‘—é™ä½äº†è®­ç»ƒçš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ç¡®ä¿ä¼˜åŒ–åŠ¨æ€å’Œæœ€ç»ˆç­–ç•¥æ€§èƒ½ä¸å˜ã€‚è¯¥æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„ GRPO æ¶æ„ä¸­ï¼Œæ”¯æŒæ›´å¤§çš„ç»„å¤§å°ï¼Œä»è€Œæé«˜ GRPO åœ¨å¤æ‚ä»»åŠ¡å’Œå¤§æ¨¡å‹ä¸­çš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04255', 'title': 'HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization', 'url': 'https://huggingface.co/papers/2506.04255', 'abstract': 'HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid Large Language Model (LLM) advancements are fueling autonomous Multi-Agent System (MAS) development. However, current frameworks often lack flexibility, resource awareness, model diversity, and autonomous tool creation. This paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent Resource Utilization), a novel MAS framework enhancing flexibility, resource efficiency, and adaptability. HASHIRU features a "CEO" agent dynamically managing specialized "employee" agents, instantiated based on task needs and resource constraints (cost, memory). Its hybrid intelligence prioritizes smaller, local LLMs (via Ollama) while flexibly using external APIs and larger models when necessary. An economic model with hiring/firing costs promotes team stability and efficient resource allocation. The system also includes autonomous API tool creation and a memory function. Evaluations on tasks like academic paper review (58% success), safety assessments (100% on a JailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash on GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate HASHIRU\'s capabilities. Case studies illustrate its self-improvement via autonomous cost model generation, tool integration, and budget management. HASHIRU offers a promising approach for more robust, efficient, and adaptable MAS through dynamic hierarchical control, resource-aware hybrid intelligence, and autonomous functional extension. Source code and benchmarks are available at https://github.com/HASHIRU-AI/HASHIRU and https://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is available at https://hashiruagentx-hashiruai.hf.space upon request.', 'score': 4, 'issue_id': 4189, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': 'd3d7d73af3533148', 'authors': ['Kunal Pai', 'Parth Shah', 'Harshil Patel'], 'affiliations': ['Independent Researcher', 'UC Davis'], 'pdf_title_img': 'assets/pdf/title_img/2506.04255.jpg', 'data': {'categories': ['#architecture', '#agi', '#optimization', '#benchmark', '#agents', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'HASHIRU - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ API. HASHIRU Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ¹Ğ¼/ÑƒĞ²Ğ¾Ğ»ÑŒĞ½ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'HASHIRU: Dynamic Intelligence for Efficient Multi-Agent Systems', 'desc': "HASHIRU is a new framework for Multi-Agent Systems (MAS) that improves flexibility and resource efficiency by managing specialized agents dynamically. It uses a hybrid intelligence approach, combining smaller local Large Language Models (LLMs) with external APIs to adapt to different tasks. The framework includes a 'CEO' agent that oversees 'employee' agents based on the specific needs and available resources, promoting efficient team management. Evaluations show HASHIRU's strong performance in various tasks, highlighting its ability to autonomously create tools and manage resources effectively."}, 'zh': {'title': 'HASHIRUï¼šçµæ´»é«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ–°æ¡†æ¶', 'desc': 'HASHIRUæ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜çµæ´»æ€§ã€èµ„æºæ•ˆç‡å’Œé€‚åº”æ€§ã€‚å®ƒé€šè¿‡åŠ¨æ€ç®¡ç†ä¸“é—¨çš„ä»£ç†ï¼ˆå¦‚â€œCEOâ€ä»£ç†å’Œâ€œå‘˜å·¥â€ä»£ç†ï¼‰æ¥æ»¡è¶³ä»»åŠ¡éœ€æ±‚å’Œèµ„æºé™åˆ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ··åˆæ™ºèƒ½ï¼Œä¼˜å…ˆä½¿ç”¨è¾ƒå°çš„æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶åœ¨å¿…è¦æ—¶çµæ´»è°ƒç”¨å¤–éƒ¨APIå’Œæ›´å¤§çš„æ¨¡å‹ã€‚HASHIRUçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºå…¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€æ§åˆ¶å’Œèµ„æºæ„ŸçŸ¥æ–¹é¢çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05817', 'title': 'CodeContests+: High-Quality Test Case Generation for Competitive\n  Programming', 'url': 'https://huggingface.co/papers/2506.05817', 'abstract': 'An LLM-based system generates high-quality test cases for competitive programming problems, enhancing the accuracy of model evaluation and RL performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Competitive programming, due to its high reasoning difficulty and precise correctness feedback, has become a key task for both training and evaluating the reasoning capabilities of large language models (LLMs). However, while a large amount of public problem data, such as problem statements and solutions, is available, the test cases of these problems are often difficult to obtain. Therefore, test case generation is a necessary task for building large-scale datasets, and the quality of the test cases directly determines the accuracy of the evaluation. In this paper, we introduce an LLM-based agent system that creates high-quality test cases for competitive programming problems. We apply this system to the CodeContests dataset and propose a new version with improved test cases, named CodeContests+. We evaluated the quality of test cases in CodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels to examine the accuracy of these test cases in evaluation. The results indicated that CodeContests+ achieves significantly higher accuracy than CodeContests, particularly with a notably higher True Positive Rate (TPR). Subsequently, our experiments in LLM Reinforcement Learning (RL) further confirmed that improvements in test case quality yield considerable advantages for RL.', 'score': 3, 'issue_id': 4195, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'f0f3a151758192f0', 'authors': ['Zihan Wang', 'Siyao Liu', 'Yang Sun', 'Hongyan Li', 'Kai Shen'], 'affiliations': ['ByteDance Seed', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05817.jpg', 'data': {'categories': ['#benchmark', '#rl', '#optimization', '#reasoning', '#dataset', '#data'], 'emoji': 'ğŸ†', 'ru': {'title': 'Ğ˜Ğ˜ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CodeContests, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ CodeContests+. ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ CodeContests+, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° (TPR). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ RL.'}, 'en': {'title': 'Enhancing Competitive Programming Evaluation with LLM-Generated Test Cases', 'desc': 'This paper presents a system that uses large language models (LLMs) to generate high-quality test cases for competitive programming problems. The generation of these test cases is crucial because they enhance the evaluation accuracy of models and improve reinforcement learning (RL) performance. The authors introduce a new dataset, CodeContests+, which contains improved test cases derived from the original CodeContests dataset. Their evaluation shows that the new test cases significantly increase the True Positive Rate (TPR) and overall accuracy, demonstrating the benefits of high-quality test case generation for model training and assessment.'}, 'zh': {'title': 'åŸºäºLLMçš„é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿä¸ºç«äº‰ç¼–ç¨‹é—®é¢˜ç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚è¿™é¡¹æŠ€æœ¯æé«˜äº†æ¨¡å‹è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œå¹¶å¢å¼ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ€§èƒ½ã€‚ç”±äºç«äº‰ç¼–ç¨‹é—®é¢˜çš„æµ‹è¯•ç”¨ä¾‹éš¾ä»¥è·å–ï¼Œç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æˆä¸ºæ„å»ºå¤§è§„æ¨¡æ•°æ®é›†çš„å¿…è¦ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ”¹è¿›åçš„æµ‹è¯•ç”¨ä¾‹åœ¨å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºåŸå§‹æ•°æ®é›†ï¼Œå°¤å…¶åœ¨çœŸå®æ­£ä¾‹ç‡ï¼ˆTPRï¼‰æ–¹é¢è¡¨ç°çªå‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06091', 'title': 'MIRIAD: Augmenting LLMs with millions of medical query-response pairs', 'url': 'https://huggingface.co/papers/2506.06091', 'abstract': 'MIRIAD, a large-scale, curated medical QA corpus, enhances LLM accuracy and hallucination detection in healthcare applications.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are bound to transform healthcare with advanced decision support and flexible chat assistants. However, LLMs are prone to generate inaccurate medical content. To ground LLMs in high-quality medical knowledge, LLMs have been equipped with external knowledge via RAG, where unstructured medical knowledge is split into small text chunks that can be selectively retrieved and integrated into the LLMs context. Yet, existing RAG pipelines rely on raw, unstructured medical text, which can be noisy, uncurated and difficult for LLMs to effectively leverage. Systematic approaches to organize medical knowledge to best surface it to LLMs are generally lacking. To address these challenges, we introduce MIRIAD, a large-scale, curated corpus of 5,821,948 medical QA pairs, each rephrased from and grounded in a passage from peer-reviewed medical literature using a semi-automated pipeline combining LLM generation, filtering, grounding, and human annotation. Unlike prior medical corpora, which rely on unstructured text, MIRIAD encapsulates web-scale medical knowledge in an operationalized query-response format, which enables more targeted retrieval. Experiments on challenging medical QA benchmarks show that augmenting LLMs with MIRIAD improves accuracy up to 6.7% compared to unstructured RAG baselines with the same source corpus and with the same amount of retrieved text. Moreover, MIRIAD improved the ability of LLMs to detect medical hallucinations by 22.5 to 37% (increase in F1 score). We further introduce MIRIAD-Atlas, an interactive map of MIRIAD spanning 56 medical disciplines, enabling clinical users to visually explore, search, and refine medical knowledge. MIRIAD promises to unlock a wealth of down-stream applications, including medical information retrievers, enhanced RAG applications, and knowledge-grounded chat interfaces, which ultimately enables more reliable LLM applications in healthcare.', 'score': 2, 'issue_id': 4204, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '29d01b51d792bee2', 'authors': ['Qinyue Zheng', 'Salman Abdullah', 'Sam Rawal', 'Cyril Zakka', 'Sophie Ostmeier', 'Maximilian Purk', 'Eduardo Reis', 'Eric J. Topol', 'Jure Leskovec', 'Michael Moor'], 'affiliations': ['Center for Artificial Intelligence in Medicine and Imaging, Stanford, CA, USA', 'Department of Biosystems Science and Engineering, ETH Zurich, Basel, Switzerland', 'Department of Computer Science, Stanford University, Stanford, CA, USA', 'Department of Internal Medicine, Mayo Clinic, Phoenix, AZ, USA', 'Department of Radiology, Stanford University, Stanford, CA, USA', 'Hasso-Plattner-Institute for Digital Engineering, University of Potsdam, Potsdam, Germany', 'Hugging Face, New York City, NY, USA', 'Scripps Translational Science Institute, San Diego, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06091.jpg', 'data': {'categories': ['#rag', '#science', '#hallucinations', '#healthcare', '#dataset', '#data'], 'emoji': 'ğŸ©º', 'ru': {'title': 'MIRIAD: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ', 'desc': 'MIRIAD - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ĞºÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 5,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MIRIAD Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° 6,7% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, MIRIAD Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 22,5-37%.'}, 'en': {'title': 'MIRIAD: Elevating Healthcare AI with Curated Knowledge', 'desc': 'MIRIAD is a large, curated medical question-answering corpus designed to improve the accuracy of large language models (LLMs) in healthcare. It consists of over 5.8 million QA pairs derived from peer-reviewed medical literature, ensuring high-quality and reliable information. By using a semi-automated pipeline that combines LLM generation and human annotation, MIRIAD provides structured medical knowledge that enhances the retrieval-augmented generation (RAG) process. Experiments show that integrating MIRIAD with LLMs significantly boosts accuracy and reduces the occurrence of medical hallucinations, making it a valuable resource for healthcare applications.'}, 'zh': {'title': 'MIRIADï¼šæå‡åŒ»ç–—LLMå‡†ç¡®æ€§ä¸å¹»è§‰æ£€æµ‹çš„å…³é”®', 'desc': 'MIRIADæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒ»å­¦é—®ç­”è¯­æ–™åº“ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„å‡†ç¡®æ€§å’Œå¹»è§‰æ£€æµ‹èƒ½åŠ›ã€‚è¯¥è¯­æ–™åº“åŒ…å«5821948å¯¹åŒ»å­¦é—®ç­”ï¼Œç»è¿‡åŠè‡ªåŠ¨åŒ–æµç¨‹ä»åŒè¡Œè¯„å®¡çš„åŒ»å­¦æ–‡çŒ®ä¸­æç‚¼è€Œæ¥ï¼Œç¡®ä¿äº†åŒ»å­¦çŸ¥è¯†çš„é«˜è´¨é‡å’Œç»“æ„åŒ–ã€‚ä¸ä»¥å¾€ä¾èµ–äºéç»“æ„åŒ–æ–‡æœ¬çš„åŒ»å­¦è¯­æ–™åº“ä¸åŒï¼ŒMIRIADé‡‡ç”¨äº†æ“ä½œåŒ–çš„æŸ¥è¯¢-å“åº”æ ¼å¼ï¼Œä½¿å¾—LLMèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ£€ç´¢å’Œåˆ©ç”¨åŒ»å­¦çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MIRIADå¢å¼ºLLMçš„å‡†ç¡®æ€§æé«˜äº†6.7%ï¼Œå¹¶ä¸”å¹»è§‰æ£€æµ‹èƒ½åŠ›æå‡äº†22.5%åˆ°37%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05673', 'title': "Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning\n  Vision Models from DataSeeds' Annotated Imagery", 'url': 'https://huggingface.co/papers/2506.05673', 'abstract': 'The DataSeeds.AI dataset enhances computer vision models by providing high-quality, peer-ranked images with extensive annotations, leading to improved performance over existing benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of modern Artificial Intelligence (AI) models, particularly diffusion-based models employed in computer vision and image generation tasks, is undergoing a paradigmatic shift in development methodologies. Traditionally dominated by a "Model Centric" approach, in which performance gains were primarily pursued through increasingly complex model architectures and hyperparameter optimization, the field is now recognizing a more nuanced "Data-Centric" approach. This emergent framework foregrounds the quality, structure, and relevance of training data as the principal driver of model performance. To operationalize this paradigm shift, we introduce the DataSeeds.AI sample dataset (the "DSD"), initially comprised of approximately 10,610 high-quality human peer-ranked photography images accompanied by extensive multi-tier annotations. The DSD is a foundational computer vision dataset designed to usher in a new standard for commercial image datasets. Representing a small fraction of DataSeed.AI\'s 100 million-plus image catalog, the DSD provides a scalable foundation necessary for robust commercial and multimodal AI development. Through this in-depth exploratory analysis, we document the quantitative improvements generated by the DSD on specific models against known benchmarks and make the code and the trained models used in our evaluation publicly available.', 'score': 2, 'issue_id': 4202, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '7fbe95059e679a3f', 'authors': ['Sajjad Abdoli', 'Freeman Lewin', 'Gediminas Vasiliauskas', 'Fabian Schonholz'], 'affiliations': ['Emet Research', 'FESSEX', 'Perle.ai', 'Zedge'], 'pdf_title_img': 'assets/pdf/title_img/2506.05673.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#open_source', '#cv', '#multimodal', '#diffusion'], 'emoji': 'ğŸŒ±', 'ru': {'title': 'ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ - ĞºĞ»ÑÑ‡ Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑÑƒ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DataSeeds.AI (DSD), ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 10 610 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. DSD Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ 'Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾' Ğº 'Ğ´Ğ°Ñ‚Ğ°-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ' Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ˜Ğ˜ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° DSD, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ¾Ğ´Ñƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Elevating AI with Quality Data: The DataSeeds.AI Revolution', 'desc': "The paper introduces the DataSeeds.AI dataset, which enhances computer vision models by providing high-quality, peer-ranked images with detailed annotations. It emphasizes a shift from a 'Model Centric' approach to a 'Data-Centric' approach in AI development, highlighting the importance of data quality over model complexity. The dataset consists of around 10,610 carefully curated images that serve as a benchmark for improving model performance. The authors demonstrate the effectiveness of the dataset through quantitative analysis and make their evaluation tools publicly accessible."}, 'zh': {'title': 'æ•°æ®é©±åŠ¨ï¼Œæå‡æ¨¡å‹æ€§èƒ½çš„æ–°æ—¶ä»£', 'desc': 'DataSeeds.AI æ•°æ®é›†é€šè¿‡æä¾›é«˜è´¨é‡ã€åŒè¡Œè¯„å®¡çš„å›¾åƒå’Œè¯¦ç»†æ³¨é‡Šï¼Œæå‡äº†è®¡ç®—æœºè§†è§‰æ¨¡å‹çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ•°æ®ä¸­å¿ƒçš„æ–¹æ³•ï¼Œè®¤ä¸ºè®­ç»ƒæ•°æ®çš„è´¨é‡å’Œç»“æ„æ˜¯æ¨¡å‹æ€§èƒ½çš„ä¸»è¦é©±åŠ¨å› ç´ ã€‚æˆ‘ä»¬ä»‹ç»çš„ DataSeeds.AI æ ·æœ¬æ•°æ®é›†åŒ…å«çº¦ 10,610 å¼ é«˜è´¨é‡çš„æ‘„å½±å›¾åƒï¼Œé…æœ‰å¤šå±‚æ¬¡çš„æ³¨é‡Šï¼Œæ—¨åœ¨ä¸ºå•†ä¸šå›¾åƒæ•°æ®é›†è®¾ç«‹æ–°æ ‡å‡†ã€‚é€šè¿‡å¯¹ç‰¹å®šæ¨¡å‹çš„å®šé‡åˆ†æï¼Œæˆ‘ä»¬å±•ç¤ºäº† DSD åœ¨å·²çŸ¥åŸºå‡†ä¸Šçš„æ€§èƒ½æå‡ï¼Œå¹¶å…¬å¼€äº†è¯„ä¼°ä¸­ä½¿ç”¨çš„ä»£ç å’Œè®­ç»ƒæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05579', 'title': 'When Models Know More Than They Can Explain: Quantifying Knowledge\n  Transfer in Human-AI Collaboration', 'url': 'https://huggingface.co/papers/2506.05579', 'abstract': "Research investigates human-AI knowledge transfer through a large-scale study, revealing that AI performance does not consistently correlate with human understanding, requiring dedicated optimization for effective communication.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. A critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In our two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations' influence on human understanding. Our findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. Our analysis identifies behavioral and strategic factors mediating successful knowledge transfer. We release our code, dataset, and evaluation framework to support future work on communicatively aligned models.", 'score': 2, 'issue_id': 4199, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '451fe5ddfc2da95f', 'authors': ['Quan Shi', 'Carlos E. Jimenez', 'Shunyu Yao', 'Nick Haber', 'Diyi Yang', 'Karthik Narasimhan'], 'affiliations': ['OpenAI', 'Princeton Language and Intelligence', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05579.jpg', 'data': {'categories': ['#open_source', '#optimization', '#data', '#dataset', '#benchmark', '#reasoning', '#rlhf', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ 118 Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº KITE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ĞµĞ³Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ÑĞ¼. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Optimizing AI for Better Human Understanding', 'desc': 'This paper explores how well artificial intelligence (AI) can share knowledge with humans, focusing on the transfer of understanding between the two. The researchers developed a framework called Knowledge Integration and Transfer Evaluation (KITE) to assess how effectively AI can communicate its reasoning to humans. They conducted a large study with 118 participants to see how AI explanations affected human problem-solving. The results showed that while AI performance on benchmarks is related to collaborative success, it does not always guarantee effective knowledge transfer, highlighting the need for specific optimizations in AI communication.'}, 'zh': {'title': 'ä¼˜åŒ–äººæœºçŸ¥è¯†è½¬ç§»ï¼Œæå‡æ²Ÿé€šæ•ˆæœ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†äººç±»ä¸äººå·¥æ™ºèƒ½ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»ï¼Œå‘ç°äººå·¥æ™ºèƒ½çš„è¡¨ç°ä¸äººç±»ç†è§£å¹¶ä¸æ€»æ˜¯ç›¸å…³ï¼Œå› æ­¤éœ€è¦ä¸“é—¨çš„ä¼˜åŒ–ä»¥å®ç°æœ‰æ•ˆçš„æ²Ÿé€šã€‚æˆ‘ä»¬æå‡ºäº†çŸ¥è¯†æ•´åˆä¸è½¬ç§»è¯„ä¼°ï¼ˆKITEï¼‰æ¡†æ¶ï¼Œè¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡çš„äººç±»ç ”ç©¶ï¼Œæ—¨åœ¨æµ‹é‡äººç±»ä¸äººå·¥æ™ºèƒ½çš„çŸ¥è¯†è½¬ç§»èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹çš„åŸºå‡†æ€§èƒ½ä¸åˆä½œç»“æœæœ‰ä¸€å®šçš„ç›¸å…³æ€§ï¼Œä½†è¿™ç§å…³ç³»å¹¶ä¸ç¨³å®šï¼Œå­˜åœ¨æ˜¾è‘—çš„å¼‚å¸¸å€¼ï¼Œè¡¨æ˜çŸ¥è¯†è½¬ç§»éœ€è¦ä¸“é—¨çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¯†åˆ«äº†å½±å“æˆåŠŸçŸ¥è¯†è½¬ç§»çš„è¡Œä¸ºå’Œç­–ç•¥å› ç´ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04755', 'title': 'Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.04755', 'abstract': "A new data selection paradigm, Reasoning Activation Potential (RAP), enhances multi-modal reasoning in large language models using minimal high-value datasets, improving performance and reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sample's potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at https://github.com/Leo-ssl/RAP.", 'score': 2, 'issue_id': 4194, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'e4a5cd694d56b2a0', 'authors': ['Shenshen Li', 'Kaiyuan Deng', 'Lei Wang', 'Hao Yang', 'Chong Peng', 'Peng Yan', 'Fumin Shen', 'Heng Tao Shen', 'Xing Xu'], 'affiliations': ['Meituan', 'Salesforce AI Research', 'School of Computer Science and Technology, Tongji University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04755.jpg', 'data': {'categories': ['#multimodal', '#data', '#optimization', '#dataset', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Reasoning Activation Potential (RAP) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. RAP Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ 'ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ' Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²Ğ° Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ°: Causal Discrepancy Estimator Ğ¸ Attention Confidence Estimator. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Difficulty-aware Replacement Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸Ğ²Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RAP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 9.3% Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 43%."}, 'en': {'title': 'Unlocking Multi-Modal Reasoning with Less Data: The RAP Approach', 'desc': 'The paper introduces a new method called Reasoning Activation Potential (RAP) to improve multi-modal reasoning in large language models (MLLMs) using smaller, high-value datasets. It challenges the belief that large amounts of training data are necessary for effective reasoning, showing that only a small subset of samples, known as cognitive samples, can trigger meaningful reasoning. RAP employs two estimators: the Causal Discrepancy Estimator (CDE) to filter out less relevant samples and the Attention Confidence Estimator (ACE) to focus on important tokens during reasoning. The results demonstrate that RAP can enhance performance while significantly reducing the amount of data and computational resources needed.'}, 'zh': {'title': 'æ¨ç†æ¿€æ´»æ½œåŠ›ï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©èŒƒå¼ï¼Œç§°ä¸ºæ¨ç†æ¿€æ´»æ½œåŠ›ï¼ˆRAPï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨æœ€å°çš„é«˜ä»·å€¼æ•°æ®é›†æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒçœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†åªéœ€å°‘é‡å…³é”®æ ·æœ¬ï¼Œè€Œå¤§å¤šæ•°æ ·æœ¬çš„è´¡çŒ®å¾®ä¹å…¶å¾®ã€‚RAPé€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„ä¼°è®¡å™¨æ¥è¯†åˆ«è¿™äº›å…³é”®æ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAPæ–¹æ³•åœ¨ä»…ä½¿ç”¨9.3%çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬é™ä½è¶…è¿‡43%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04120', 'title': 'Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data', 'url': 'https://huggingface.co/papers/2506.04120', 'abstract': 'A hybrid real-to-sim framework combining 3D Gaussian Splatting and physics simulation with MuJoCo allows simultaneous high-fidelity object reconstruction and accurate robot pose calibration from raw trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating accurate, physical simulations directly from real-world robot motion holds great value for safe, scalable, and affordable robot learning, yet remains exceptionally challenging. Real robot data suffers from occlusions, noisy camera poses, dynamic scene elements, which hinder the creation of geometrically accurate and photorealistic digital twins of unseen objects. We introduce a novel real-to-sim framework tackling all these challenges at once. Our key insight is a hybrid scene representation merging the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation within a single representation. We propose an end-to-end optimization pipeline that leverages differentiable rendering and differentiable physics within MuJoCo to jointly refine all scene components - from object geometry and appearance to robot poses and physical parameters - directly from raw and imprecise robot trajectories. This unified optimization allows us to simultaneously achieve high-fidelity object mesh reconstruction, generate photorealistic novel views, and perform annotation-free robot pose calibration. We demonstrate the effectiveness of our approach both in simulation and on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, enabling more practical and robust real-to-simulation pipelines.', 'score': 2, 'issue_id': 4194, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'd808acd50e3bfd0c', 'authors': ['Ben Moran', 'Mauro Comi', 'Steven Bohez', 'Tom Erez', 'Zhibin Li', 'Leonard Hasenclever'], 'affiliations': ['Google DeepMind', 'University College London', 'University of Bristol'], 'pdf_title_img': 'assets/pdf/title_img/2506.04120.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ 3D Gaussian Splatting Ğ´Ğ»Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² MuJoCo. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹, ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑÑ†ĞµĞ½Ñ‹, Ğ´ĞµĞ»Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼.'}, 'en': {'title': 'Bridging Reality and Simulation for Enhanced Robot Learning', 'desc': 'This paper presents a new framework that combines 3D Gaussian Splatting and physics simulation to improve robot learning from real-world data. It addresses challenges like occlusions and noisy camera poses that make it hard to create accurate digital models of objects. The authors introduce a hybrid representation that merges photorealistic rendering with physics-compatible object meshes, allowing for better simulations. Their end-to-end optimization process refines object geometry, appearance, and robot poses from raw data, achieving high-fidelity reconstructions and accurate pose calibration without needing annotations.'}, 'zh': {'title': 'å®ç°çœŸå®ä¸ä»¿çœŸçš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ··åˆçš„çœŸå®åˆ°ä»¿çœŸæ¡†æ¶ï¼Œç»“åˆäº†3Dé«˜æ–¯ç‚¹äº‘å’ŒMuJoCoç‰©ç†ä»¿çœŸï¼Œèƒ½å¤Ÿä»åŸå§‹è½¨è¿¹ä¸­åŒæ—¶å®ç°é«˜ä¿çœŸç‰©ä½“é‡å»ºå’Œå‡†ç¡®çš„æœºå™¨äººå§¿æ€æ ¡å‡†ã€‚è¯¥æ¡†æ¶è§£å†³äº†çœŸå®æœºå™¨äººæ•°æ®ä¸­çš„é®æŒ¡ã€å™ªå£°ç›¸æœºå§¿æ€å’ŒåŠ¨æ€åœºæ™¯å…ƒç´ ç­‰æŒ‘æˆ˜ï¼Œåˆ›å»ºå‡ ä½•ä¸Šå‡†ç¡®ä¸”é€¼çœŸçš„æ•°å­—åŒèƒèƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå°†3Dé«˜æ–¯ç‚¹äº‘çš„å…‰ç…§çœŸå®æ¸²æŸ“ä¸é€‚åˆç‰©ç†ä»¿çœŸçš„ç‰©ä½“ç½‘æ ¼ç»“åˆåœ¨ä¸€èµ·ã€‚é€šè¿‡ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æµç¨‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç›´æ¥ä»ä¸ç²¾ç¡®çš„æœºå™¨äººè½¨è¿¹ä¸­ä¼˜åŒ–æ‰€æœ‰åœºæ™¯ç»„ä»¶ï¼Œæå‡äº†ç‰©ä½“é‡å»ºçš„ç²¾åº¦å’Œæœºå™¨äººå§¿æ€çš„æ ¡å‡†æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00649', 'title': 'GuideX: Guided Synthetic Data Generation for Zero-Shot Information\n  Extraction', 'url': 'https://huggingface.co/papers/2506.00649', 'abstract': 'GUIDEX enhances zero-shot Named Entity Recognition by automatically defining schemas and inferring guidelines, setting new benchmarks without extensive human-labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at neilus03.github.io/guidex.com', 'score': 2, 'issue_id': 4194, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '32639eb393594459', 'authors': ['Neil De La Fuente', 'Oscar Sainz', 'Iker GarcÃ­a-Ferrero', 'Eneko Agirre'], 'affiliations': ['HiTZ Basque Center for Language Technology - Ixa NLP Group', 'Technical University of Munich (TUM)', 'University of the Basque Country (UPV/EHU)'], 'pdf_title_img': 'assets/pdf/title_img/2506.00649.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#synthetic', '#training', '#benchmark'], 'emoji': 'ğŸ·ï¸', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑÑ…ĞµĞ¼ Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ zero-shot NER', 'desc': 'GUIDEX - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑÑ…ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ (NER). ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama 3.1 Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GUIDEX ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ zero-shot NER. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ GUIDEX, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ…, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² ÑÑ…ĞµĞ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Zero-Shot NER with GUIDEX!', 'desc': 'GUIDEX is a new approach that improves zero-shot Named Entity Recognition (NER) by automatically creating schemas and guidelines for different domains. Traditional NER systems need a lot of human effort to design schemas and label data, which can be expensive and time-consuming. GUIDEX uses Large Language Models to generate synthetic labeled data, helping models perform better in new, unseen domains. By fine-tuning Llama 3.1 with GUIDEX, researchers achieved significant improvements in NER performance, setting new records without relying on extensive human-labeled datasets.'}, 'zh': {'title': 'GUIDEXï¼šé›¶-shot NERçš„æ–°çªç ´', 'desc': 'GUIDEXæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºé›¶-shotå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰çš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡è‡ªåŠ¨å®šä¹‰é¢†åŸŸç‰¹å®šçš„æ¨¡å¼å’Œæ¨æ–­æŒ‡å¯¼æ–¹é’ˆï¼Œå‡å°‘äº†å¯¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚ä½¿ç”¨GUIDEXå¾®è°ƒLlama 3.1æ¨¡å‹ï¼Œåœ¨ä¸ƒä¸ªé›¶-shot NERåŸºå‡†æµ‹è¯•ä¸­åˆ›é€ äº†æ–°çš„æœ€ä½³æˆç»©ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨GUIDEXè®­ç»ƒçš„æ¨¡å‹åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒF1åˆ†æ•°æé«˜äº†7åˆ†ï¼Œç»“åˆä½¿ç”¨æ—¶ä¹Ÿæé«˜äº†è¿‘2åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05551', 'title': 'When Semantics Mislead Vision: Mitigating Large Multimodal Models\n  Hallucinations in Scene Text Spotting and Understanding', 'url': 'https://huggingface.co/papers/2506.05551', 'abstract': 'A framework using Transformer layers with strong attention and a coarse-to-fine strategy reduces semantic hallucination in LMMs for visual perception and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have achieved impressive progress in visual perception and reasoning. However, when confronted with visually ambiguous or non-semantic scene text, they often struggle to accurately spot and understand the content, frequently generating semantically plausible yet visually incorrect answers, which we refer to as semantic hallucination. In this work, we investigate the underlying causes of semantic hallucination and identify a key finding: Transformer layers in LLM with stronger attention focus on scene text regions are less prone to producing semantic hallucinations. Thus, we propose a training-free semantic hallucination mitigation framework comprising two key components: (1) ZoomText, a coarse-to-fine strategy that identifies potential text regions without external detectors; and (2) Grounded Layer Correction, which adaptively leverages the internal representations from layers less prone to hallucination to guide decoding, correcting hallucinated outputs for non-semantic samples while preserving the semantics of meaningful ones. To enable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over 1,730 samples spanning both semantic and non-semantic cases, with manually curated question-answer pairs designed to probe model hallucinations. Extensive experiments demonstrate that our method not only effectively mitigates semantic hallucination but also achieves strong performance on public benchmarks for scene text spotting and understanding.', 'score': 1, 'issue_id': 4205, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'e73b3a44a1c359fb', 'authors': ['Yan Shu', 'Hangui Lin', 'Yexin Liu', 'Yan Zhang', 'Gangyan Zeng', 'Yan Li', 'Yu Zhou', 'Ser-Nam Lim', 'Harry Yang', 'Nicu Sebe'], 'affiliations': ['HKUST', 'IIE, CAS', 'NJUST', 'NKU', 'UCAS', 'UCF', 'UIR', 'UNITN'], 'pdf_title_img': 'assets/pdf/title_img/2506.05551.jpg', 'data': {'categories': ['#training', '#benchmark', '#reasoning', '#cv', '#multimodal', '#hallucinations'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ĞµĞµ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ·Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ZoomText Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Grounded Layer Correction Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾ĞµĞ², Ğ¼ĞµĞ½ĞµĞµ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TextHalu-Bench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 1730 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚.'}, 'en': {'title': 'Reducing Semantic Hallucination in Visual Models with Attention', 'desc': "This paper presents a new framework to reduce semantic hallucination in Large Multimodal Models (LMMs) that deal with visual perception and reasoning. The authors identify that stronger attention in Transformer layers helps these models focus better on text regions, leading to fewer hallucinations. They introduce two main components: ZoomText, which uses a coarse-to-fine approach to locate text without needing external detectors, and Grounded Layer Correction, which uses reliable internal representations to correct errors in the model's outputs. The effectiveness of their approach is validated through a new benchmark, TextHalu-Bench, which tests the model's ability to handle both semantic and non-semantic text cases."}, 'zh': {'title': 'å‡å°‘è¯­ä¹‰å¹»è§‰çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œåˆ©ç”¨Transformerå±‚çš„å¼ºæ³¨æ„åŠ›æœºåˆ¶å’Œç²—åˆ°ç»†çš„ç­–ç•¥ï¼Œå‡å°‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†ä¸­çš„è¯­ä¹‰å¹»è§‰ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œå…·æœ‰æ›´å¼ºæ³¨æ„åŠ›çš„Transformerå±‚åœ¨å¤„ç†åœºæ™¯æ–‡æœ¬æ—¶ï¼Œè¾ƒå°‘äº§ç”Ÿè¯­ä¹‰å¹»è§‰ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šZoomTextç”¨äºè¯†åˆ«æ½œåœ¨æ–‡æœ¬åŒºåŸŸï¼Œè€ŒGrounded Layer Correctionåˆ™åˆ©ç”¨ä¸æ˜“äº§ç”Ÿå¹»è§‰çš„å†…éƒ¨è¡¨ç¤ºæ¥ä¿®æ­£è§£ç è¾“å‡ºã€‚é€šè¿‡å¼•å…¥TextHalu-BenchåŸºå‡†ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå‡è½»äº†è¯­ä¹‰å¹»è§‰ï¼Œå¹¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«å’Œç†è§£çš„å…¬å…±åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03828', 'title': 'AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial\n  Asset Operations and Maintenance', 'url': 'https://huggingface.co/papers/2506.03828', 'abstract': 'A unified framework, AssetOpsBench, is introduced to enable end-to-end automation of industrial asset lifecycle management through domain-specific AI agents.  \t\t\t\t\tAI-generated summary \t\t\t\t AI for Industrial Asset Lifecycle Management aims to automate complex operational workflows -- such as condition monitoring, maintenance planning, and intervention scheduling -- to reduce human workload and minimize system downtime. Traditional AI/ML approaches have primarily tackled these problems in isolation, solving narrow tasks within the broader operational pipeline. In contrast, the emergence of AI agents and large language models (LLMs) introduces a next-generation opportunity: enabling end-to-end automation across the entire asset lifecycle. This paper envisions a future where AI agents autonomously manage tasks that previously required distinct expertise and manual coordination. To this end, we introduce AssetOpsBench -- a unified framework and environment designed to guide the development, orchestration, and evaluation of domain-specific agents tailored for Industry 4.0 applications. We outline the key requirements for such holistic systems and provide actionable insights into building agents that integrate perception, reasoning, and control for real-world industrial operations. The software is available at https://github.com/IBM/AssetOpsBench.', 'score': 0, 'issue_id': 4203, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '38f96a4cd72b114d', 'authors': ['Dhaval Patel', 'Shuxin Lin', 'James Rayfield', 'Nianjun Zhou', 'Roman Vaculin', 'Natalia Martinez', "Fearghal O'donncha", 'Jayant Kalagnanam'], 'affiliations': ['IBM Research - Ireland', 'IBM Research - Yorktown'], 'pdf_title_img': 'assets/pdf/title_img/2506.03828.jpg', 'data': {'categories': ['#architecture', '#science', '#agents', '#optimization', '#agi', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸', 'desc': 'AssetOpsBench - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ, Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸ 4.0. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. AssetOpsBench Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ².'}, 'en': {'title': 'Revolutionizing Industrial Asset Management with AI Automation', 'desc': 'The paper presents AssetOpsBench, a comprehensive framework aimed at automating the entire lifecycle management of industrial assets using specialized AI agents. It addresses the limitations of traditional AI/ML methods that typically focus on isolated tasks, by proposing a system that integrates various operational workflows like condition monitoring and maintenance planning. By leveraging advancements in AI agents and large language models, the framework facilitates seamless end-to-end automation, reducing the need for human intervention. The authors provide guidelines for developing these agents, emphasizing the importance of combining perception, reasoning, and control in real-world industrial settings.'}, 'zh': {'title': 'å®ç°å·¥ä¸šèµ„äº§ç®¡ç†çš„å…¨é¢è‡ªåŠ¨åŒ–', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶AssetOpsBenchï¼Œæ—¨åœ¨é€šè¿‡ç‰¹å®šé¢†åŸŸçš„äººå·¥æ™ºèƒ½ä»£ç†å®ç°å·¥ä¸šèµ„äº§ç”Ÿå‘½å‘¨æœŸç®¡ç†çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨åŒ–å¤æ‚çš„æ“ä½œå·¥ä½œæµç¨‹ï¼Œå¦‚çŠ¶æ€ç›‘æµ‹ã€ç»´æŠ¤è®¡åˆ’å’Œå¹²é¢„è°ƒåº¦ï¼Œä»è€Œå‡å°‘äººåŠ›è´Ÿæ‹…å¹¶æœ€å°åŒ–ç³»ç»Ÿåœæœºæ—¶é—´ã€‚ä¸ä¼ ç»Ÿçš„äººå·¥æ™ºèƒ½/æœºå™¨å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒAssetOpsBenchèƒ½å¤Ÿè·¨æ•´ä¸ªèµ„äº§ç”Ÿå‘½å‘¨æœŸå®ç°å…¨é¢çš„è‡ªåŠ¨åŒ–ï¼Œç®¡ç†ä»¥å‰éœ€è¦ä¸åŒä¸“ä¸šçŸ¥è¯†å’Œæ‰‹åŠ¨åè°ƒçš„ä»»åŠ¡ã€‚æˆ‘ä»¬æä¾›äº†æ„å»ºé›†æˆæ„ŸçŸ¥ã€æ¨ç†å’Œæ§åˆ¶çš„ä»£ç†çš„å…³é”®è¦æ±‚å’Œå¯è¡Œæ€§è§è§£ï¼Œä»¥æ”¯æŒå·¥ä¸š4.0åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20698', 'title': 'Sparsified State-Space Models are Efficient Highway Networks', 'url': 'https://huggingface.co/papers/2505.20698', 'abstract': 'Simba, a hierarchical sparsification method for state-space models, enhances efficiency and information flow in natural language tasks by pruning tokens more aggressively in upper layers.  \t\t\t\t\tAI-generated summary \t\t\t\t State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at https://github.com/woominsong/Simba.', 'score': 0, 'issue_id': 4204, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '348a1f437846d3e7', 'authors': ['Woomin Song', 'Jihoon Tack', 'Sangwoo Mo', 'Seunghyuk Oh', 'Jinwoo Shin'], 'affiliations': ['Korea Advanced Institute of Science & Technology (KAIST)', 'University of Michigan, Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2505.20698.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'ğŸ¦', 'ru': {'title': 'Simba: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Simba Ğ´Ğ»Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Simba Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰ĞµĞ¼ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Simba Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mamba Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ FLOPS Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Simba: Sparsifying State-Space Models for Efficient NLP', 'desc': 'This paper introduces Simba, a method designed to improve the efficiency of state-space models (SSMs) in natural language processing tasks. By applying hierarchical sparsification, Simba prunes tokens more aggressively in the upper layers of the model, which tend to be redundant. This approach allows for better information flow and reduces computational costs while maintaining performance. The authors demonstrate that Simba outperforms the baseline model, Mamba, in various tasks, highlighting its effectiveness in managing long sequences.'}, 'zh': {'title': 'Simbaï¼šæå‡çŠ¶æ€ç©ºé—´æ¨¡å‹æ•ˆç‡çš„ç¨€ç–åŒ–æ–¹æ³•', 'desc': 'Simbaæ˜¯ä¸€ç§é’ˆå¯¹çŠ¶æ€ç©ºé—´æ¨¡å‹çš„åˆ†å±‚ç¨€ç–åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­çš„æ•ˆç‡å’Œä¿¡æ¯æµã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä¸Šå±‚æ›´ç§¯æåœ°ä¿®å‰ªä»¤ç‰Œï¼Œå‡å°‘å†—ä½™ä¿¡æ¯çš„ä¼ é€’ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„è®¡ç®—æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸Šå±‚é€šå¸¸åŒ…å«æ›´å¤šçš„å…¨å±€ä¿¡æ¯ï¼Œè€Œä¸‹å±‚åˆ™å…³æ³¨å±€éƒ¨ä¿¡æ¯ï¼Œå› æ­¤Simbaåœ¨ä¸Šå±‚è¿›è¡Œæ›´å¤šçš„ç¨€ç–åŒ–å¤„ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSimbaåœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­ä¼˜äºåŸºçº¿æ¨¡å‹Mambaï¼Œä¸”åœ¨ç›¸åŒçš„è®¡ç®—é¢„ç®—ä¸‹å®ç°äº†æ›´å¥½çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09513', 'title': 'ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.09513', 'abstract': 'ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a multi-agent verification and refinement process, where we design an Error Refiner to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.', 'score': 79, 'issue_id': 4272, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '7c6aa342a51b1d59', 'authors': ['Yu Sun', 'Xingyu Qian', 'Weiwen Xu', 'Hao Zhang', 'Chenghao Xiao', 'Long Li', 'Yu Rong', 'Wenbing Huang', 'Qifeng Bai', 'Tingyang Xu'], 'affiliations': ['Alibaba DAMO Academy', 'BeÄ³ing Key Laboratory of Research on Large Models', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation', 'Gaoling School of', 'Hupan Lab', 'Renmin University of China', 'School of Basic Medical Sciences, Lanzhou University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09513.jpg', 'data': {'categories': ['#dataset', '#optimization', '#benchmark', '#healthcare', '#reasoning', '#training'], 'emoji': 'ğŸ©º', 'ru': {'title': 'ReasonMed: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜', 'desc': 'ReasonMed - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 370 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Error Refiner Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Chain-of-Thought Ñ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ´ĞºĞ°Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ReasonMed-7B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 4.17% Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° LLaMA3.1-70B Ğ½Ğ° 4.60% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ PubMedQA.'}, 'en': {'title': 'Enhancing Medical AI with ReasonMed: A New Benchmark in Reasoning', 'desc': 'ReasonMed is a comprehensive medical reasoning dataset designed to improve the performance of medical question answering models. It consists of 370,000 high-quality examples derived from 1.7 million initial reasoning paths created by various large language models (LLMs). The dataset is refined through a multi-agent process that includes an Error Refiner to correct mistakes in reasoning paths. By combining detailed Chain-of-Thought reasoning with concise summaries, ReasonMed-7B achieves superior results, surpassing previous benchmarks for smaller models and even outperforming larger models on specific tasks.'}, 'zh': {'title': 'ReasonMedï¼šæå‡åŒ»å­¦é—®ç­”æ¨¡å‹çš„æ–°åŸºå‡†', 'desc': 'ReasonMedæ˜¯ä¸€ä¸ªå¤§å‹åŒ»å­¦æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦é—®ç­”æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚å®ƒç»“åˆäº†è¯¦ç»†çš„æ¨ç†è·¯å¾„å’Œç®€æ´çš„æ€»ç»“ï¼Œåˆ›é€ äº†æ–°çš„æ¨¡å‹æ€§èƒ½åŸºå‡†ã€‚è¯¥æ•°æ®é›†åŒ…å«370,000ä¸ªé«˜è´¨é‡ç¤ºä¾‹ï¼Œç»è¿‡å¤šä»£ç†éªŒè¯å’Œç²¾ç‚¼è¿‡ç¨‹æ„å»ºè€Œæˆã€‚é€šè¿‡ç»“åˆè¯¦ç»†çš„æ€ç»´é“¾æ¨ç†å’Œç®€æ´çš„ç­”æ¡ˆæ€»ç»“ï¼ŒReasonMed-7Bæ¨¡å‹åœ¨åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10954', 'title': 'SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks', 'url': 'https://huggingface.co/papers/2506.10954', 'abstract': 'An automated pipeline, SWE-Factory, is introduced to facilitate the creation of large-scale datasets for evaluating and training Large Language Models in GitHub issue resolution tasks, offering efficient environment building, standardized grading, and automated validation.  \t\t\t\t\tAI-generated summary \t\t\t\t Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of 0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory.', 'score': 43, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'dfb4cf3e253468bd', 'authors': ['Lianghong Guo', 'Yanlin Wang', 'Caihua Li', 'Pengyu Yang', 'Jiachi Chen', 'Wei Tao', 'Yingtian Zou', 'Duyu Tang', 'Zibin Zheng'], 'affiliations': ['Huawei', 'Independent Researcher', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10954.jpg', 'data': {'categories': ['#data', '#science', '#dataset', '#agents', '#benchmark', '#open_source'], 'emoji': 'ğŸ­', 'ru': {'title': 'SWE-Factory: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ LLM Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ', 'desc': 'SWE-Factory - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğ° GitHub. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ SWE-Builder - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑÑ€ĞµĞ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ fail2pass. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ ÑĞ±Ğ¾Ñ€ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ…, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Automating Dataset Creation for LLMs in GitHub Issue Resolution', 'desc': 'The paper presents SWE-Factory, an automated pipeline designed to streamline the creation of large-scale datasets for training and evaluating Large Language Models (LLMs) in GitHub issue resolution tasks. It addresses the challenges of environment setup, grading, and validation by integrating three automated components: SWE-Builder for environment construction, a standardized exit-code-based grading system, and an automated fail2pass validation process. Experiments demonstrate that SWE-Factory can efficiently generate valid task instances at a low cost while achieving high accuracy in grading and validation. This innovation aims to enhance the quality and speed of dataset collection for LLM training and evaluation.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–ç®¡é“åŠ é€ŸGitHubé—®é¢˜è§£å†³æ•°æ®é›†æ„å»º', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSWE-Factoryçš„è‡ªåŠ¨åŒ–ç®¡é“ï¼Œæ—¨åœ¨ç®€åŒ–å¤§è§„æ¨¡æ•°æ®é›†çš„åˆ›å»ºï¼Œä»¥è¯„ä¼°å’Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨GitHubé—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„æ•°æ®é›†æ„å»ºè¿‡ç¨‹ç¹çä¸”è€—æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨ç¯å¢ƒæ­å»ºã€ç»“æœè¯„åˆ†å’Œä»»åŠ¡éªŒè¯é˜¶æ®µã€‚SWE-Factoryé€šè¿‡é›†æˆä¸‰ä¸ªæ ¸å¿ƒè‡ªåŠ¨åŒ–ç»„ä»¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–ç¯å¢ƒæ„å»ºçš„å¤šä»£ç†ç³»ç»ŸSWE-Builderã€åŸºäºé€€å‡ºä»£ç çš„æ ‡å‡†åŒ–è¯„åˆ†æ–¹æ³•ï¼Œä»¥åŠè‡ªåŠ¨åŒ–çš„fail2passéªŒè¯è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿæœ‰æ•ˆæ„å»ºæœ‰æ•ˆçš„ä»»åŠ¡å®ä¾‹ï¼Œå¹¶åœ¨è¯„åˆ†å’ŒéªŒè¯æ–¹é¢è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡å’Œé«˜ç²¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10910', 'title': 'Magistral', 'url': 'https://huggingface.co/papers/2506.10910', 'abstract': "A scalable reinforcement learning pipeline for training reasoning models demonstrates improvements in multimodal understanding, instruction following, and function calling without relying on existing implementations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.", 'score': 36, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '59d3abdb994da001', 'authors': ['Mistral-AI', ':', 'Abhinav Rastogi', 'Albert Q. Jiang', 'Andy Lo', 'Gabrielle Berrada', 'Guillaume Lample', 'Jason Rute', 'Joep Barmentlo', 'Karmesh Yadav', 'Kartik Khandelwal', 'Khyathi Raghavi Chandu', 'LÃ©onard Blier', 'Lucile Saulnier', 'Matthieu Dinot', 'Maxime Darrin', 'Neha Gupta', 'Roman Soletskyi', 'Sagar Vaze', 'Teven Le Scao', 'Yihan Wang', 'Adam Yang', 'Alexander H. Liu', 'Alexandre Sablayrolles', 'AmÃ©lie HÃ©liou', 'AmÃ©lie Martin', 'Andy Ehrenberg', 'Anmol Agarwal', 'Antoine Roux', 'Arthur Darcet', 'Arthur Mensch', 'Baptiste Bout', 'Baptiste RoziÃ¨re', 'Baudouin De Monicault', 'Chris Bamford', 'Christian Wallenwein', 'Christophe Renaudin', 'ClÃ©mence Lanfranchi', 'Darius Dabert', 'Devon Mizelle', 'Diego de las Casas', 'Elliot Chane-Sane', 'Emilien Fugier', 'Emma Bou Hanna', 'Gauthier Delerce', 'Gauthier Guinet', 'Georgii Novikov', 'Guillaume Martin', 'Himanshu Jaju', 'Jan Ludziejewski', 'Jean-Hadrien Chabran', 'Jean-Malo Delignon', 'Joachim Studnia', 'Jonas Amar', 'Josselin Somerville Roberts', 'Julien Denize', 'Karan Saxena', 'Kush Jain', 'Lingxiao Zhao', 'Louis Martin', 'Luyu Gao', 'LÃ©lio Renard Lavaud', 'Marie Pellat', 'Mathilde Guillaumin', 'Mathis Felardos', 'Maximilian Augustin', 'MickaÃ«l Seznec', 'Nikhil Raghuraman', 'Olivier Duchenne', 'Patricia Wang', 'Patrick von Platen', 'Patryk Saffer', 'Paul Jacob', 'Paul Wambergue', 'Paula Kurylowicz', 'Pavankumar Reddy Muddireddy', 'PhilomÃ¨ne Chagniot', 'Pierre Stock', 'Pravesh Agrawal', 'Romain Sauvestre', 'RÃ©mi Delacourt', 'Sanchit Gandhi', 'Sandeep Subramanian', 'Shashwat Dalal', 'Siddharth Gandhi', 'Soham Ghosh', 'Srijan Mishra', 'Sumukh Aithal', 'Szymon Antoniak', 'Thibault Schueller', 'Thibaut Lavril', 'Thomas Robert', 'Thomas Wang', 'TimothÃ©e Lacroix', 'Valeriia Nemychnikova', 'Victor Paltz', 'Virgile Richard', 'Wen-Ding Li', 'William Marshall', 'Xuanyu Zhang', 'Yunhao Tang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.10910.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning', '#open_source', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Magistral - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ RL Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Magistral Small Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Magistral Medium Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Reasoning with Pure Reinforcement Learning', 'desc': 'This paper presents Magistral, a new reasoning model developed using a scalable reinforcement learning (RL) pipeline. The authors emphasize a novel approach that does not depend on previous implementations or RL traces from other models, focusing instead on their own infrastructure. They demonstrate that training with pure RL on text data can preserve or enhance capabilities in multimodal understanding, instruction following, and function calling. Additionally, they introduce Magistral Medium, which is built on Mistral Medium 3, and make Magistral Small available as open-source software.'}, 'zh': {'title': 'å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ ç®¡é“ï¼Œæå‡æ¨ç†æ¨¡å‹èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Magistralï¼Œè¿™æ˜¯Mistralçš„ç¬¬ä¸€ä¸ªæ¨ç†æ¨¡å‹ï¼Œä»¥åŠæˆ‘ä»¬è‡ªå·±çš„å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®¡é“ã€‚æˆ‘ä»¬é‡‡ç”¨è‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•ï¼Œå®Œå…¨ä¾èµ–äºè‡ªå·±çš„æ¨¡å‹å’ŒåŸºç¡€è®¾æ–½ï¼Œè€Œä¸æ˜¯ç°æœ‰çš„å®ç°å’Œä»å…ˆå‰æ¨¡å‹ä¸­æå–çš„RLè½¨è¿¹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œçº¯æ–‡æœ¬æ•°æ®çš„RLè®­ç»ƒèƒ½å¤Ÿä¿æŒæˆ–æ”¹å–„å¤šæ¨¡æ€ç†è§£ã€æŒ‡ä»¤è·Ÿéšå’ŒåŠŸèƒ½è°ƒç”¨çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†Magistral Mediumå’Œå¼€æºçš„Magistral Smallï¼Œè¿›ä¸€æ­¥æ”¯æŒæ¨ç†è®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10540', 'title': 'AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation', 'url': 'https://huggingface.co/papers/2506.10540', 'abstract': "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.", 'score': 35, 'issue_id': 4275, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '5c11232a01ad90bb', 'authors': ['Haoyuan Shi', 'Yunxin Li', 'Xinyu Chen', 'Longyue Wang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Digital Commerce, Hangzhou, China', 'Harbin Institute of Technology, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.10540.jpg', 'data': {'categories': ['#optimization', '#video', '#multimodal', '#story_generation', '#agents'], 'emoji': 'ğŸ¬', 'ru': {'title': 'AniMaker: ÑƒĞ¼Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'AniMaker - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (MCTS-Gen) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AniEval Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². AniMaker ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ¶Ğ¸ÑÑĞµÑ€Ğ°, Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AniMaker Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹.'}, 'en': {'title': 'AniMaker: Crafting Coherent Stories from Text with AI', 'desc': 'AniMaker is a multi-agent framework designed to generate coherent storytelling videos from text input, addressing challenges in video generation. It utilizes specialized agents for different tasks, such as storyboard creation and video clip generation, ensuring a consistent narrative flow. The framework incorporates MCTS-Gen for efficient clip generation and AniEval for evaluating animation quality, focusing on story coherence and visual continuity. Experiments show that AniMaker outperforms existing models in both quality and efficiency, making AI-generated storytelling animation more viable for production.'}, 'zh': {'title': 'AniMakerï¼šé«˜æ•ˆç”Ÿæˆè¿è´¯æ•…äº‹è§†é¢‘çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'AniMakeræ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨MCTS-Genå’ŒAniEvalï¼Œä»æ–‡æœ¬è¾“å…¥ç”Ÿæˆè¿è´¯çš„æ•…äº‹è§†é¢‘ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹çš„è´¨é‡å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“é—¨çš„æ™ºèƒ½ä½“ï¼Œå¦‚å¯¼æ¼”æ™ºèƒ½ä½“ã€æ‘„å½±æ™ºèƒ½ä½“ã€è¯„å®¡æ™ºèƒ½ä½“å’ŒåæœŸåˆ¶ä½œæ™ºèƒ½ä½“ï¼Œæ¥å®ç°é«˜æ•ˆçš„å¤šå€™é€‰ç‰‡æ®µç”Ÿæˆå’Œæ•…äº‹æ„è¯†ç‰‡æ®µé€‰æ‹©ã€‚AniMakerçš„æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬MCTS-Genï¼Œå®ƒæ˜¯ä¸€ç§é«˜æ•ˆçš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ç­–ç•¥ï¼Œèƒ½å¤Ÿæ™ºèƒ½åœ°å¯¼èˆªå€™é€‰ç©ºé—´ï¼Œç”Ÿæˆé«˜æ½œåŠ›çš„ç‰‡æ®µï¼ŒåŒæ—¶ä¼˜åŒ–èµ„æºä½¿ç”¨ï¼›ä»¥åŠAniEvalï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºå¤šé•œå¤´åŠ¨ç”»è¯„ä¼°è®¾è®¡çš„æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒAniMakeråœ¨è´¨é‡å’Œæ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ¨åŠ¨äº†AIç”Ÿæˆçš„æ•…äº‹åŠ¨ç”»æ›´æ¥è¿‘ç”Ÿäº§æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09993', 'title': 'Text-Aware Image Restoration with Diffusion Models', 'url': 'https://huggingface.co/papers/2506.09993', 'abstract': 'The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/', 'score': 35, 'issue_id': 4272, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '91f1bb97ef062632', 'authors': ['Jaewon Min', 'Jin Hyeon Kim', 'Paul Hyunbin Cho', 'Jaeeun Lee', 'Jihye Park', 'Minkyu Park', 'Sangpil Kim', 'Hyunhee Park', 'Seungryong Kim'], 'affiliations': ['KAIST AI', 'Korea University', 'Samsung Electronics', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09993.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#diffusion', '#hallucinations'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ° (TAIR) Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SA-Text - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TeReDiff Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Restoring Images with Textual Precision', 'desc': 'The Text-Aware Image Restoration (TAIR) system addresses the challenge of restoring images while maintaining the accuracy of textual information. Traditional diffusion-based methods often produce incorrect text patterns, leading to what is known as text-image hallucination. TAIR introduces a multi-task diffusion framework, TeReDiff, which combines image restoration with a text-spotting module to improve both visual and textual fidelity. By leveraging a large-scale dataset of annotated images, TAIR significantly enhances text recognition accuracy compared to existing methods.'}, 'zh': {'title': 'æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ï¼šæå‡å›¾åƒä¸æ–‡æœ¬çš„åŒé‡æ¢å¤', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ï¼ˆTAIRï¼‰çš„ç³»ç»Ÿï¼Œæ—¨åœ¨åŒæ—¶æ¢å¤å›¾åƒå†…å®¹å’Œæ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚ç°æœ‰çš„æ‰©æ•£åŸºç¡€ä¿®å¤æ–¹æ³•åœ¨è‡ªç„¶å›¾åƒä¿®å¤æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†å›¾åƒä¸­çš„æ–‡æœ¬åŒºåŸŸæ—¶å¸¸å¸¸å‡ºç°é”™è¯¯çš„æ–‡æœ¬æ¨¡å¼ã€‚TAIRç³»ç»Ÿç»“åˆäº†å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶å’Œæ–‡æœ¬æ£€æµ‹æ¨¡å—ï¼Œé€šè¿‡è”åˆè®­ç»ƒæé«˜äº†æ–‡æœ¬è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAIRåœ¨å›¾åƒä¿®å¤å’Œæ–‡æœ¬ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ä¿®å¤æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10857', 'title': 'VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos', 'url': 'https://huggingface.co/papers/2506.10857', 'abstract': "VRBench evaluates long video understanding by assessing multi-step reasoning capabilities across temporal and procedural validity using human-labeled question-answering pairs and reasoning chains.  \t\t\t\t\tAI-generated summary \t\t\t\t We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 1,010 long videos (with an average duration of 1.6 hours), along with 9,468 human-labeled multi-step question-answering pairs and 30,292 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning.", 'score': 30, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '7eabd83f69c00df7', 'authors': ['Jiashuo Yu', 'Yue Wu', 'Meng Chu', 'Zhifei Ren', 'Zizheng Huang', 'Pei Chu', 'Ruijie Zhang', 'Yinan He', 'Qirui Li', 'Songze Li', 'Zhenxiang Li', 'Zhongying Tu', 'Conghui He', 'Yu Qiao', 'Yali Wang', 'Yi Wang', 'Limin Wang'], 'affiliations': ['Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.10857.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#reasoning', '#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VRBench: ĞÑ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'VRBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 1010 Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ 1,6 Ñ‡Ğ°ÑĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ 9468 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ 30292 ÑˆĞ°Ğ³Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸. VRBench Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° 12 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¸ 16 Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM), Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ² Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'VRBench: Advancing Multi-Step Reasoning in Long Video Understanding', 'desc': 'VRBench is a new benchmark designed to evaluate how well large models understand long videos through multi-step reasoning. It includes 1,010 long videos and thousands of human-labeled question-answering pairs, focusing on both temporal reasoning and procedural validity. The framework allows for the generation of coherent reasoning chains that require multiple steps, assessing models not just on final answers but also on the reasoning process. By testing various large language models (LLMs) and vision-language models (VLMs), VRBench aims to provide insights that enhance the understanding of multi-step reasoning in video comprehension.'}, 'zh': {'title': 'VRBenchï¼šé•¿è§†é¢‘ç†è§£çš„æ–°åŸºå‡†', 'desc': 'VRBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°é•¿è§†é¢‘ç†è§£çš„åŸºå‡†ï¼Œä¸“æ³¨äºå¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯æ—¶é—´æ¨ç†å’Œç¨‹åºæœ‰æ•ˆæ€§ã€‚è¯¥åŸºå‡†åŒ…å«1010ä¸ªé•¿è§†é¢‘ï¼Œå¹³å‡æ—¶é•¿ä¸º1.6å°æ—¶ï¼Œä»¥åŠ9468ä¸ªäººå·¥æ ‡æ³¨çš„å¤šæ­¥éª¤é—®ç­”å¯¹å’Œ30292ä¸ªå¸¦æ—¶é—´æˆ³çš„æ¨ç†æ­¥éª¤ã€‚é€šè¿‡å¤šé˜¶æ®µç­›é€‰è¿‡ç¨‹ï¼Œç¡®ä¿è§†é¢‘æƒ…èŠ‚è¿è´¯æ€§ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªäººæœºåä½œæ¡†æ¶ï¼Œç”Ÿæˆéœ€è¦å¤šä¸ªæ—¶é—´åŸºç¡€æ­¥éª¤çš„è¿è´¯æ¨ç†é“¾ã€‚VRBenchè®¾è®¡äº†ä¸€ä¸ªå¤šé˜¶æ®µè¯„ä¼°æµç¨‹ï¼Œç»¼åˆè¯„ä¼°æ¨¡å‹çš„ç»“æœå’Œè¿‡ç¨‹ï¼Œæ¨åŠ¨äº†å¤šæ­¥éª¤æ¨ç†é¢†åŸŸçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10274', 'title': 'Discrete Audio Tokens: More Than a Survey!', 'url': 'https://huggingface.co/papers/2506.10274', 'abstract': 'A systematic review and benchmark of discrete audio tokenizers across speech, music, and general audio domains is presented, covering their taxonomy, evaluation metrics, and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks.They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.', 'score': 24, 'issue_id': 4282, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '636451c0543dc586', 'authors': ['Pooneh Mousavi', 'Gallil Maimon', 'Adel Moumen', 'Darius Petermann', 'Jiatong Shi', 'Haibin Wu', 'Haici Yang', 'Anastasia Kuznetsova', 'Artem Ploujnikov', 'Ricard Marxer', 'Bhuvana Ramabhadran', 'Benjamin Elizalde', 'Loren Lugosch', 'Jinyu Li', 'Cem Subakan', 'Phil Woodland', 'Minje Kim', 'Hung-yi Lee', 'Shinji Watanabe', 'Yossi Adi', 'Mirco Ravanelli'], 'affiliations': ['Apple', 'Carnegie Mellon University', 'Concordia University', 'Google', 'Indiana University', 'Laval University', 'Microsoft', 'Mila-Quebec AI Institute', 'National Taiwan University', 'The Hebrew University of Jerusalem', 'University of Cambridge', 'University of Illinois at Urbana-Champaign', 'UniversitÃ© de MontrÃ©al', 'UniversitÃ© de Toulon'], 'pdf_title_img': 'assets/pdf/title_img/2506.10274.jpg', 'data': {'categories': ['#survey', '#benchmark', '#audio'], 'emoji': 'ğŸµ', 'ru': {'title': 'ĞÑƒĞ´Ğ¸Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¾Ñ‚ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ¾ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ€ĞµÑ‡Ğ¸, Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking the Power of Discrete Audio Tokenization', 'desc': 'This paper reviews and benchmarks different methods of discrete audio tokenization used in speech, music, and general audio. Discrete audio tokens are efficient representations that maintain important audio characteristics while allowing for better storage and processing in large language models. The authors categorize various tokenization techniques and evaluate their performance across multiple tasks, highlighting their strengths and weaknesses. The study aims to provide a comprehensive understanding of the current landscape of audio tokenizers and identify future research directions.'}, 'zh': {'title': 'ç¦»æ•£éŸ³é¢‘æ ‡è®°å™¨çš„ç³»ç»Ÿè¯„ä¼°ä¸æ¯”è¾ƒ', 'desc': 'æœ¬æ–‡ç³»ç»Ÿå›é¡¾å¹¶åŸºå‡†æµ‹è¯•äº†ç¦»æ•£éŸ³é¢‘æ ‡è®°å™¨åœ¨è¯­éŸ³ã€éŸ³ä¹å’Œä¸€èˆ¬éŸ³é¢‘é¢†åŸŸçš„è¡¨ç°ã€‚ç¦»æ•£éŸ³é¢‘æ ‡è®°æ˜¯ç´§å‡‘çš„è¡¨ç¤ºæ–¹å¼ï¼Œæ—¨åœ¨ä¿ç•™æ„ŸçŸ¥è´¨é‡ã€è¯­éŸ³å†…å®¹å’Œè¯´è¯è€…ç‰¹å¾ï¼ŒåŒæ—¶å®ç°é«˜æ•ˆå­˜å‚¨å’Œæ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¼–ç å™¨-è§£ç å™¨ã€é‡åŒ–æŠ€æœ¯ã€è®­ç»ƒèŒƒå¼ã€æµå¼å¤„ç†å’Œåº”ç”¨é¢†åŸŸçš„æ ‡è®°åŒ–æ–¹æ³•åˆ†ç±»ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†å…³é”®çš„å±€é™æ€§å’Œæœªæ¥ç ”ç©¶çš„æŒ‘æˆ˜ï¼Œä¸ºéŸ³é¢‘å¤„ç†é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10978', 'title': 'Fine-Grained Perturbation Guidance via Attention Head Selection', 'url': 'https://huggingface.co/papers/2506.10978', 'abstract': 'The paper proposes HeadHunter, a systematic framework for selecting attention heads in Diffusion Transformer architectures to enable precise control over image generation quality and style, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose "HeadHunter", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head\'s attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.', 'score': 20, 'issue_id': 4281, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'a646cb2c7b825b74', 'authors': ['Donghoon Ahn', 'Jiwon Kang', 'Sanghyun Lee', 'Minjae Kim', 'Jaewon Min', 'Wooseok Jang', 'Saungwu Lee', 'Sayak Paul', 'Susung Hong', 'Seungryong Kim'], 'affiliations': ['HuggingFace', 'KAIST', 'KAIST AI', 'Korea University', 'Krea AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.10978.jpg', 'data': {'categories': ['#interpretability', '#cv', '#training', '#diffusion', '#optimization', '#architecture'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HeadHunter - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Diffusion Transformer Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‚ Ğ·Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°, ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹. HeadHunter Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'HeadHunter: Precision Control in Image Generation with Attention Heads', 'desc': 'The paper introduces HeadHunter, a framework designed to select specific attention heads in Diffusion Transformer architectures for better control over image generation quality and style. It addresses the limitations of existing attention perturbation methods by providing a systematic approach to determine where perturbations should be applied, focusing on individual attention heads rather than entire layers. The authors demonstrate that different heads are responsible for distinct visual concepts, allowing for targeted manipulation of attributes like structure and texture. By implementing SoftPAG, they offer a method to fine-tune perturbation strength, leading to improved image quality and style guidance in large-scale text-to-image models.'}, 'zh': {'title': 'ç²¾å‡†æ§åˆ¶å›¾åƒç”Ÿæˆçš„æ³¨æ„åŠ›å¤´é€‰æ‹©æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†HeadHunterï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»ŸåŒ–æ¡†æ¶ï¼Œç”¨äºé€‰æ‹©æ‰©æ•£å˜æ¢å™¨æ¶æ„ä¸­çš„æ³¨æ„åŠ›å¤´ï¼Œä»¥å®ç°å¯¹å›¾åƒç”Ÿæˆè´¨é‡å’Œé£æ ¼çš„ç²¾ç¡®æ§åˆ¶ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ³¨æ„åŠ›æ‰°åŠ¨çš„ç²’åº¦ï¼Œä»å±‚çº§åˆ°å•ä¸ªæ³¨æ„åŠ›å¤´ï¼Œå‘ç°ç‰¹å®šçš„å¤´æ§åˆ¶ç€ä¸åŒçš„è§†è§‰æ¦‚å¿µï¼Œå¦‚ç»“æ„ã€é£æ ¼å’Œçº¹ç†è´¨é‡ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†â€œHeadHunterâ€ï¼Œé€šè¿‡è¿­ä»£é€‰æ‹©ä¸ç”¨æˆ·ç›®æ ‡ä¸€è‡´çš„æ³¨æ„åŠ›å¤´ï¼Œå®ç°å¯¹ç”Ÿæˆè´¨é‡å’Œè§†è§‰å±æ€§çš„ç»†ç²’åº¦æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†SoftPAGï¼Œæä¾›äº†ä¸€ä¸ªè¿ç»­çš„è°ƒèŠ‚å·¥å…·ï¼Œä»¥è°ƒæ•´æ‰°åŠ¨å¼ºåº¦å¹¶æŠ‘åˆ¶ä¼ªå½±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10952', 'title': 'Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training', 'url': 'https://huggingface.co/papers/2506.10952', 'abstract': 'Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains, a new concept designed to capture the key underlying features of datasets. Domain2Vec maintains a vocabulary of meta-domains and uses a classifier to decompose any given dataset into a domain vector that corresponds to a distribution over this vocabulary. These domain vectors enable the identification of the optimal data mixture for language model (LM) pretraining in a training-free manner under the \\textbf{Distribution Alignment Assumption} (DA^{2}), which suggests that when the data distributions of the training set and the validation set are better aligned, a lower validation loss is achieved. Moreover, Domain2vec can be seamlessly integrated into previous works to model the relationship between domain vectors and LM performance, greatly enhancing the efficiency and scalability of previous methods. Extensive experiments demonstrate that Domain2Vec helps find the data mixture that enhances downstream task performance with minimal computational overhead. Specifically, Domain2Vec achieves the same validation loss on Pile-CC using only 51.5% of the computation required when training on the original mixture of The Pile dataset. Under equivalent compute budget, Domain2Vec improves downstream performance by an average of 2.83%.', 'score': 20, 'issue_id': 4273, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'f9cd5a16ae7e2cc3', 'authors': ['Mozhi Zhang', 'Howe Tissue', 'Lu Wang', 'Xipeng Qiu'], 'affiliations': ['Ritzz-AI', 'School of Computer Science, Fudan University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.10952.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#training', '#optimization', '#data'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Domain2Vec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼ĞµÑ‚Ğ°-Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¼ĞµÑ‚Ğ°-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¼ĞµÑÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Domain2Vec Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ½Ğ° Pile-CC, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ 51.5% Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… The Pile.'}, 'en': {'title': 'Optimize Language Models with Domain2Vec!', 'desc': "Domain2Vec is a new method that breaks down datasets into smaller parts called meta-domains to improve the training of language models. It uses a classifier to create a domain vector that represents the dataset's features, allowing for better alignment between training and validation data distributions. This approach helps in selecting the best combination of data for pretraining language models while using less computational power. Experiments show that Domain2Vec can achieve similar performance with significantly reduced computational costs, enhancing efficiency in machine learning tasks."}, 'zh': {'title': 'Domain2Vecï¼šä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ•°æ®åˆ†è§£æ–¹æ³•', 'desc': 'Domain2Vecæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒå°†æ•°æ®é›†åˆ†è§£ä¸ºå¤šä¸ªå…ƒåŸŸçš„çº¿æ€§ç»„åˆï¼Œä»¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒå’Œä¸‹æ¸¸æ€§èƒ½ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•ç»´æŠ¤ä¸€ä¸ªå…ƒåŸŸè¯æ±‡è¡¨ï¼Œå¹¶ä½¿ç”¨åˆ†ç±»å™¨å°†ç»™å®šæ•°æ®é›†åˆ†è§£ä¸ºå¯¹åº”äºè¯¥è¯æ±‡è¡¨çš„åŸŸå‘é‡ã€‚è¿™äº›åŸŸå‘é‡èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ ¹æ®åˆ†å¸ƒå¯¹é½å‡è®¾ï¼ˆDAÂ²ï¼‰è¯†åˆ«å‡ºæœ€ä½³çš„æ•°æ®æ··åˆï¼Œä»è€Œé™ä½éªŒè¯æŸå¤±ã€‚æ­¤å¤–ï¼ŒDomain2Vecå¯ä»¥æ— ç¼é›†æˆåˆ°ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œå»ºæ¨¡åŸŸå‘é‡ä¸è¯­è¨€æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09344', 'title': 'Ming-Omni: A Unified Multimodal Model for Perception and Generation', 'url': 'https://huggingface.co/papers/2506.09344', 'abstract': 'Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.', 'score': 20, 'issue_id': 4273, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'b9166301d93dd2bb', 'authors': ['Inclusion AI', 'Biao Gong', 'Cheng Zou', 'Chuanyang Zheng', 'Chunluan Zhou', 'Canxiang Yan', 'Chunxiang Jin', 'Chunjie Shen', 'Dandan Zheng', 'Fudong Wang', 'Furong Xu', 'GuangMing Yao', 'Jun Zhou', 'Jingdong Chen', 'Jianxin Sun', 'Jiajia Liu', 'Jianjiang Zhu', 'Jun Peng', 'Kaixiang Ji', 'Kaiyou Song', 'Kaimeng Ren', 'Libin Wang', 'Lixiang Ru', 'Lele Xie', 'Longhua Tan', 'Lyuxin Xue', 'Lan Wang', 'Mochen Bai', 'Ning Gao', 'Pei Chen', 'Qingpei Guo', 'Qinglong Zhang', 'Qiang Xu', 'Rui Liu', 'Ruijie Xiong', 'Sirui Gao', 'Tinghao Liu', 'Taisong Li', 'Weilong Chai', 'Xinyu Xiao', 'Xiaomei Wang', 'Xiaoxue Chen', 'Xiao Lu', 'Xiaoyu Li', 'Xingning Dong', 'Xuzheng Yu', 'Yi Yuan', 'Yuting Gao', 'Yunxiao Sun', 'Yipeng Chen', 'Yifei Wu', 'Yongjie Lyu', 'Ziping Ma', 'Zipeng Feng', 'Zhijiang Fang', 'Zhihao Qiu', 'Ziyuan Huang', 'Zhengyu He'], 'affiliations': ['Ant Group', 'Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.09344.jpg', 'data': {'categories': ['#audio', '#open_source', '#video', '#cv', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ming-Omni - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ MoE Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ming-Omni Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ming-Omni Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ….'}, 'en': {'title': 'Ming-Omni: One Model, Many Modalities!', 'desc': 'Ming-Omni is a cutting-edge multimodal model designed to handle various types of data, including images, text, audio, and video. It utilizes dedicated encoders to extract information from these different modalities and employs a mixture of experts (MoE) architecture with modality-specific routers for efficient processing. This innovative design allows Ming-Omni to perform a wide range of tasks, such as generating speech and images, engaging in context-aware conversations, and editing images, all within a single framework. By being open-source and matching the capabilities of advanced models like GPT-4o, Ming-Omni aims to foster further research and development in the field of multimodal AI.'}, 'zh': {'title': 'Ming-Omniï¼šç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†çš„å¼ºå¤§è§£å†³æ–¹æ¡ˆ', 'desc': 'Ming-Omniæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ã€‚å®ƒä½¿ç”¨ä¸“ç”¨ç¼–ç å™¨æå–ä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ–°æå‡ºçš„æ¨¡æ€ç‰¹å®šè·¯ç”±å™¨è¿›è¡Œå¤„ç†ã€‚è¯¥æ¨¡å‹æ”¯æŒè¯­éŸ³å’Œå›¾åƒç”Ÿæˆï¼Œèƒ½å¤Ÿè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹è¯å’Œå¤šåŠŸèƒ½çš„å›¾åƒç¼–è¾‘ã€‚Ming-Omniæ˜¯é¦–ä¸ªå¼€æºæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€æ”¯æŒä¸Šä¸GPT-4oç›¸åª²ç¾ï¼Œä¿ƒè¿›äº†ç¤¾åŒºçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10741', 'title': 'PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework', 'url': 'https://huggingface.co/papers/2506.10741', 'abstract': 'PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, a unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs a carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via best-of-n preference optimization; and (iv) joint vision-language feedback refinement. Each stage is supported by a fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appeal-approaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft', 'score': 19, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '5c43d96d0a604ef8', 'authors': ['SiXiang Chen', 'Jianyu Lai', 'Jialin Gao', 'Tian Ye', 'Haoyu Chen', 'Hengyu Shi', 'Shitong Shao', 'Yunlong Lin', 'Song Fei', 'Zhaohu Xing', 'Yeying Jin', 'Junfeng Luo', 'Xiaoming Wei', 'Lei Zhu'], 'affiliations': ['Meituan', 'National University of Singapore', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10741.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#architecture', '#open_source', '#dataset', '#data', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ñ‹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ', 'desc': 'PosterCraft - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. PosterCraft Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': "Elevating Poster Design with PosterCraft's Unified Framework", 'desc': 'PosterCraft is a novel framework designed to enhance the generation of aesthetic posters by integrating advanced techniques in text rendering and layout optimization. It utilizes a modular pipeline that includes region-aware fine-tuning and aesthetic reinforcement learning to improve the visual quality of the generated images. The framework operates on a cascaded workflow, leveraging large-scale datasets for training and optimizing each component for better performance. Experimental results show that PosterCraft surpasses existing open-source models in rendering accuracy and overall aesthetic appeal, making it competitive with state-of-the-art commercial systems.'}, 'zh': {'title': 'PosterCraftï¼šç¾å­¦æµ·æŠ¥ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'PosterCraft æ˜¯ä¸€ä¸ªæ”¹è¿›ç¾å­¦æµ·æŠ¥ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å—åŒ–æ¡†æ¶ã€‚å®ƒé€šè¿‡å¢å¼ºçš„æ–‡æœ¬æ¸²æŸ“ã€åŒºåŸŸæ„ŸçŸ¥å¾®è°ƒã€ç¾å­¦å¼ºåŒ–å­¦ä¹ å’Œè”åˆè§†è§‰-è¯­è¨€ä¼˜åŒ–ï¼Œæå‡äº†æµ·æŠ¥çš„ç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶å…è®¸æ¨¡å‹è‡ªç”±æ¢ç´¢è§†è§‰ä¸Šå¼•äººæ³¨ç›®çš„ç»„åˆï¼Œå…‹æœäº†ä¼ ç»Ÿæ¨¡å—åŒ–ç®¡é“çš„å±€é™æ€§ã€‚ç»è¿‡å¤šé¡¹å®éªŒè¯„ä¼°ï¼ŒPosterCraft åœ¨æ¸²æŸ“ç²¾åº¦ã€å¸ƒå±€ä¸€è‡´æ€§å’Œæ•´ä½“è§†è§‰å¸å¼•åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºå¼€æºåŸºçº¿ï¼Œæ¥è¿‘æœ€å…ˆè¿›çš„å•†ä¸šç³»ç»Ÿçš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10357', 'title': 'Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts', 'url': 'https://huggingface.co/papers/2506.10357', 'abstract': "Optimus-3, an agent using knowledge-enhanced data generation, Mixture-of-Experts routing, and multimodal reasoning-augmented reinforcement learning, achieves superior performance across various tasks in Minecraft.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/", 'score': 18, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '145045d8e634c76b', 'authors': ['Zaijing Li', 'Yuquan Xie', 'Rui Shao', 'Gongwei Chen', 'Weili Guan', 'Dongmei Jiang', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10357.jpg', 'data': {'categories': ['#optimization', '#architecture', '#agents', '#rag', '#rl', '#reasoning', '#multimodal', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Optimus-3: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞµÑ‚ Minecraft', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Optimus-3 - Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ñ‹ Minecraft, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Optimus-3 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Minecraft. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Optimus-3: Mastering Minecraft with Advanced AI Techniques', 'desc': 'Optimus-3 is a general-purpose agent designed for the open-world environment of Minecraft, utilizing advanced techniques in machine learning. It incorporates a knowledge-enhanced data generation pipeline to create high-quality training data, addressing the challenge of insufficient domain-specific data. The agent employs a Mixture-of-Experts (MoE) architecture to effectively manage interference among diverse tasks, allowing for better performance. Additionally, it uses Multimodal Reasoning-Augmented Reinforcement Learning to improve its reasoning capabilities, enabling it to handle the visual diversity present in Minecraft.'}, 'zh': {'title': 'Optimus-3ï¼šåœ¨Minecraftä¸­è¶…è¶Šæé™çš„æ™ºèƒ½ä½“', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Optimus-3ï¼Œä¸€ä¸ªåˆ©ç”¨çŸ¥è¯†å¢å¼ºæ•°æ®ç”Ÿæˆã€ä¸“å®¶æ··åˆè·¯ç”±å’Œå¤šæ¨¡æ€æ¨ç†å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä½“ã€‚è¯¥æ™ºèƒ½ä½“åœ¨Minecraftç­‰å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œè§£å†³äº†é¢†åŸŸç‰¹å®šæ•°æ®ä¸è¶³ã€å¼‚æ„ä»»åŠ¡å¹²æ‰°å’Œè§†è§‰å¤šæ ·æ€§ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§çŸ¥è¯†å¢å¼ºçš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œä»¥æä¾›å¯æ‰©å±•çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œå¹¶å¼•å…¥äº†ä»»åŠ¡çº§è·¯ç”±çš„ä¸“å®¶æ··åˆæ¶æ„æ¥å‡è½»ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†å¤šæ¨¡æ€æ¨ç†å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥æå‡æ™ºèƒ½ä½“åœ¨è§†è§‰å¤šæ ·æ€§æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09967', 'title': 'Resa: Transparent Reasoning Models via SAEs', 'url': 'https://huggingface.co/papers/2506.09967', 'abstract': "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \\1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.", 'score': 18, 'issue_id': 4274, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '03a45cae6d11be64', 'authors': ['Shangshang Wang', 'Julian Asilis', 'Ã–mer Faruk AkgÃ¼l', 'Enes Burak Bilgin', 'Ollie Liu', 'Deqing Fu', 'Willie Neiswanger'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.09967.jpg', 'data': {'categories': ['#rl', '#training', '#open_source', '#optimization', '#small_models', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SAE-Tuning Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SAE) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğº Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. SAE-Tuning Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Reasoning Enhancement in Language Models with SAE-Tuning', 'desc': 'The paper introduces SAE-Tuning, a method that enhances reasoning capabilities in language models using sparse autoencoders. This approach allows for significant performance improvements without the need for extensive retraining, achieving cost reductions of over 2000 times and time savings of over 450 times. By training a sparse autoencoder to capture reasoning skills from a source model, the method effectively guides the fine-tuning of a target model using verified question-answer data. The results show that the reasoning abilities gained are both generalizable across datasets and modular, allowing for easy integration into different models without retraining.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼šç¨€ç–è‡ªç¼–ç å™¨è°ƒä¼˜çš„åŠ›é‡', 'desc': 'SAE-Tuningæ˜¯ä¸€ç§é«˜æ•ˆçš„ç¨€ç–è‡ªç¼–ç å™¨è°ƒä¼˜æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è¯­è¨€æ¨¡å‹ä¸­å¼•å‘å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§é‡çš„é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œä»¥ä»æºæ¨¡å‹ä¸­æ•æ‰æ¨ç†èƒ½åŠ›ï¼Œç„¶ååˆ©ç”¨è®­ç»ƒå¥½çš„SAEæŒ‡å¯¼æ ‡å‡†çš„ç›‘ç£å¾®è°ƒè¿‡ç¨‹ï¼Œä»è€Œåœ¨ç›®æ ‡æ¨¡å‹ä¸­å¼•å‘è¿™äº›èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSAE-Tuningåœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬å’Œæ—¶é—´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæå–çš„æ¨ç†èƒ½åŠ›å…·æœ‰å¯æ³›åŒ–å’Œæ¨¡å—åŒ–çš„ç‰¹æ€§ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ•°æ®é›†å’Œæ¨¡å‹ä¹‹é—´çµæ´»åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09250', 'title': 'Comment on The Illusion of Thinking: Understanding the Strengths and\n  Limitations of Reasoning Models via the Lens of Problem Complexity', 'url': 'https://huggingface.co/papers/2506.09250', 'abstract': 'Evaluation artifacts, particularly token limits and impractical instances in benchmarks, lead to misreported failures in Large Reasoning Models on planning puzzles.  \t\t\t\t\tAI-generated summary \t\t\t\t Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments systematically exceed model output token limits at reported failure points, with models explicitly acknowledging these constraints in their outputs; (2) The authors\' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N > 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.', 'score': 18, 'issue_id': 4291, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '2157b13738f4dbea', 'authors': ['C. Opus', 'A. Lawsen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.09250.jpg', 'data': {'categories': ['#interpretability', '#training', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜: Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM) Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼, Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ½ĞµÑ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ€Ğ°Ğ½ĞµĞµ ÑÑ‡Ğ¸Ñ‚Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ½ĞµĞ¿Ğ¾ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Rethinking Evaluation: Uncovering True AI Reasoning Power', 'desc': "This paper discusses how certain evaluation methods can misrepresent the performance of Large Reasoning Models (LRMs) on planning puzzles. The authors identify that issues like token limits and impractical benchmark instances lead to what they call 'accuracy collapse'. They argue that many reported failures are due to experimental design flaws rather than actual reasoning deficiencies in the models. By adjusting the evaluation approach, they show that LRMs can perform well on previously deemed difficult tasks, emphasizing the need for better testing methods in AI research."}, 'zh': {'title': 'è¯„ä¼°è®¾è®¡å½±å“æ¨ç†æ¨¡å‹è¡¨ç°çš„å…³é”®', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹åœ¨è§„åˆ’éš¾é¢˜ä¸Šçš„è¯„ä¼°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ç”±äºè¯„ä¼°å·¥å…·çš„é™åˆ¶å¯¼è‡´çš„é”™è¯¯æŠ¥å‘Šã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹åœ¨å¤æ‚åº¦è¶…è¿‡æŸä¸ªé˜ˆå€¼æ—¶å‡ºç°çš„â€œå‡†ç¡®æ€§å´©æºƒâ€ä¸»è¦æ˜¯ç”±äºå®éªŒè®¾è®¡çš„ç¼ºé™·ï¼Œè€Œéæ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ ¹æœ¬å¤±è´¥ã€‚åˆ†ææŒ‡å‡ºäº†ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬å®éªŒè¶…å‡ºæ¨¡å‹è¾“å‡ºçš„ä»¤ç‰Œé™åˆ¶ï¼Œä»¥åŠè¯„ä¼°æ¡†æ¶æœªèƒ½åŒºåˆ†æ¨ç†å¤±è´¥ä¸å®é™…é™åˆ¶ã€‚é€šè¿‡æ§åˆ¶è¿™äº›å®éªŒä¼ªå½±ï¼Œåˆæ­¥å®éªŒæ˜¾ç¤ºæ¨¡å‹åœ¨ä¹‹å‰è¢«æŠ¥å‘Šä¸ºå®Œå…¨å¤±è´¥çš„å¡”æ±‰è¯ºå®ä¾‹ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ï¼Œå¼ºè°ƒäº†åœ¨è¯„ä¼°AIæ¨ç†èƒ½åŠ›æ—¶è°¨æ…è®¾è®¡å®éªŒçš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10821', 'title': 'VideoDeepResearch: Long Video Understanding With Agentic Tool Using', 'url': 'https://huggingface.co/papers/2506.10821', 'abstract': "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the task's inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems.", 'score': 16, 'issue_id': 4273, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'e1e5003f31573e97', 'authors': ['Huaying Yuan', 'Zheng Liu', 'Junjie Zhou', 'Ji-Rong Wen', 'Zhicheng Dou'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Beijing University of Posts and Telecommunications', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.10821.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#long_context', '#video', '#multimodal', '#agents'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'VideoDeepResearch - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ÑÑÑŒ Ğº Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Long Video Understanding with Text-Only Reasoning', 'desc': 'VideoDeepResearch is a novel framework designed to improve long video understanding (LVU) tasks without relying on extended context windows or advanced visual perception capabilities. It utilizes a text-only large reasoning model (LRM) in conjunction with a modular toolkit that includes multimodal retrievers and visual perceivers. This system formulates problem-solving strategies through reasoning and selectively accesses relevant video content as needed. Experimental results show that VideoDeepResearch significantly outperforms existing multi-modal large language models (MLLMs) on various LVU benchmarks, demonstrating its effectiveness in tackling complex video understanding challenges.'}, 'zh': {'title': 'çªç ´é•¿è§†é¢‘ç†è§£çš„å…¨æ–°æ¡†æ¶', 'desc': 'VideoDeepResearchæ˜¯ä¸€ç§æ–°å‹çš„é•¿è§†é¢‘ç†è§£æ¡†æ¶ï¼Œå®ƒä»…ä¾èµ–æ–‡æœ¬æ¨ç†æ¨¡å‹å’Œæ¨¡å—åŒ–å·¥å…·ï¼Œè€Œä¸éœ€è¦æ‰©å±•ä¸Šä¸‹æ–‡çª—å£æˆ–å¢å¼ºè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ¨ç†åˆ¶å®šé—®é¢˜è§£å†³ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å·¥å…·é€‰æ‹©æ€§åœ°è®¿é—®å’Œä½¿ç”¨è§†é¢‘å†…å®¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœæ˜¾ç¤ºVideoDeepResearchåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åŸºçº¿ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œä»£ç†ç³»ç»Ÿåœ¨è§£å†³é•¿è§†é¢‘ç†è§£é—®é¢˜ä¸­å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10974', 'title': 'AutoMind: Adaptive Knowledgeable Agent for Automated Data Science', 'url': 'https://huggingface.co/papers/2506.10974', 'abstract': 'AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science.', 'score': 14, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '50bcb63544ac7586', 'authors': ['Yixin Ou', 'Yujie Luo', 'Jingsheng Zheng', 'Lanning Wei', 'Shuofei Qiao', 'Jintian Zhang', 'Da Zheng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2506.10974.jpg', 'data': {'categories': ['#science', '#training', '#dataset', '#benchmark', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AutoMind: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'AutoMind - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. AutoMind Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'AutoMind: Revolutionizing Automated Data Science with Expert Knowledge and Adaptability', 'desc': 'AutoMind is a new framework designed to enhance automated data science by integrating expert knowledge and adapting its approach based on the complexity of tasks. It utilizes a curated knowledge base to inform its decisions, allowing it to tackle more complex problems than traditional systems. The framework employs a knowledgeable tree search algorithm to explore various solutions strategically, improving its problem-solving capabilities. Evaluations show that AutoMind outperforms existing methods, making it a significant advancement in the field of automated machine learning.'}, 'zh': {'title': 'AutoMindï¼šè‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦çš„æ–°çªç ´', 'desc': 'AutoMindæ˜¯ä¸€ä¸ªçµæ´»ä¸”çŸ¥è¯†ä¸°å¯Œçš„LLMä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆä¸“å®¶çŸ¥è¯†ã€æˆ˜ç•¥æ€§è§£å†³æ–¹æ¡ˆæ¢ç´¢å’Œè‡ªé€‚åº”ç¼–ç æ¥æå‡è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦çš„èƒ½åŠ›ã€‚ä¸ç°æœ‰ç³»ç»Ÿç›¸æ¯”ï¼ŒAutoMindåœ¨å¤„ç†å¤æ‚å’Œåˆ›æ–°ä»»åŠ¡æ—¶è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼Œå…‹æœäº†ä¼ ç»Ÿæ¡†æ¶çš„å±€é™æ€§ã€‚å®ƒé€šè¿‡å»ºç«‹ä¸€ä¸ªç»è¿‡ç­›é€‰çš„ä¸“å®¶çŸ¥è¯†åº“ã€é‡‡ç”¨æ™ºèƒ½çš„çŸ¥è¯†æ ‘æœç´¢ç®—æ³•ä»¥åŠåŠ¨æ€è°ƒæ•´ç¼–ç ç­–ç•¥ï¼Œæ¥é€‚åº”ä¸åŒä»»åŠ¡çš„å¤æ‚æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAutoMindåœ¨è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°å‡ºé«˜æ•ˆå’Œç¨³å¥çš„ç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10953', 'title': 'Build the web for agents, not agents for the web', 'url': 'https://huggingface.co/papers/2506.10953', 'abstract': 'A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community.', 'score': 14, 'issue_id': 4274, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '77dd06867c379745', 'authors': ['Xing Han LÃ¹', 'Gaurav Kamath', 'Marius Mosbach', 'Siva Reddy'], 'affiliations': ['Equal Advising', 'McGill University', 'Mila', 'Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.10953.jpg', 'data': {'categories': ['#agents', '#agi', '#optimization', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ’ĞµĞ±-Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ˜Ğ˜ Ñ Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ğ¾Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ²Ğ¾Ğ´Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ’ĞµĞ±-Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² (AWI). AWI Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° AWI, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹ ML-ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Redefining Web Interaction for AI Agents with AWIs', 'desc': 'This paper proposes a new approach to web agent research by introducing Agentic Web Interfaces (AWIs), which are specifically designed for AI agents to interact with web environments. Current web interfaces are not optimized for the capabilities of AI, leading to inefficiencies and challenges in task completion. The authors outline six guiding principles for designing AWIs, focusing on safety, efficiency, and standardization to benefit all stakeholders involved. This shift aims to enhance the performance and reliability of web agents, encouraging collaboration within the machine learning community.'}, 'zh': {'title': 'ä¸ºä»£ç†è®¾è®¡ä¼˜åŒ–ç½‘ç»œäº¤äº’ç•Œé¢', 'desc': 'æœ¬æ–‡æå‡ºäº†ç½‘ç»œä»£ç†ç ”ç©¶çš„èŒƒå¼è½¬å˜ï¼Œå€¡å¯¼å¼€å‘ä»£ç†ç½‘ç»œæ¥å£ï¼ˆAWIï¼‰ï¼Œä»¥ä¼˜åŒ–äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç½‘ç»œç¯å¢ƒä¸­çš„äº¤äº’ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹çš„è¿›æ­¥ï¼Œå¼€å‘èƒ½å¤Ÿè‡ªä¸»å¯¼èˆªå’Œå®Œæˆä»»åŠ¡çš„ç½‘ç»œä»£ç†å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å½“å‰çš„æ–¹æ³•é¢ä¸´ç€äººç±»è®¾è®¡çš„ç•Œé¢ä¸LLMèƒ½åŠ›ä¹‹é—´çš„æ ¹æœ¬ä¸åŒ¹é…é—®é¢˜ï¼Œå¯¼è‡´å¤„ç†å¤æ‚ç½‘ç»œè¾“å…¥æ—¶çš„å›°éš¾ã€‚æœ¬æ–‡æå‡ºçš„AWIæ¦‚å¿µæ—¨åœ¨ä¸ºä»£ç†è®¾è®¡ä¸“é—¨çš„äº¤äº’ç•Œé¢ï¼Œä»¥æé«˜å®‰å…¨æ€§ã€æ•ˆç‡å’Œæ ‡å‡†åŒ–ï¼Œæ¨åŠ¨æ›´é«˜æ•ˆã€å¯é å’Œé€æ˜çš„ç½‘ç»œä»£ç†è®¾è®¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10960', 'title': 'ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark', 'url': 'https://huggingface.co/papers/2506.10960', 'abstract': 'A benchmark for Chinese harmful content detection, coupled with a knowledge-augmented baseline, improves the performance of smaller models without extensive resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.', 'score': 10, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'db6960a49f9467ee', 'authors': ['Kangwei Liu', 'Siyuan Cheng', 'Bozhong Tian', 'Xiaozhuan Liang', 'Yuyang Yin', 'Meng Han', 'Ningyu Zhang', 'Bryan Hooi', 'Xi Chen', 'Shumin Deng'], 'affiliations': ['National University of Singapore', 'Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10960.jpg', 'data': {'categories': ['#multilingual', '#small_models', '#low_resource', '#ethics', '#dataset', '#benchmark'], 'emoji': 'ğŸ‡¨ğŸ‡³', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑˆĞµÑÑ‚ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Empowering Small Models for Chinese Harm Detection', 'desc': 'This paper introduces a new benchmark for detecting harmful content in Chinese, addressing the lack of resources in this area compared to English. It features a dataset that is professionally annotated and covers six categories of harmful content, using real-world examples. The authors also develop a knowledge-augmented baseline that combines expert knowledge with insights from large language models, allowing smaller models to perform effectively without needing extensive resources. This approach enhances the accuracy of harmful content detection in Chinese, making it more accessible for various applications.'}, 'zh': {'title': 'æå‡ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹çš„åŸºå‡†ä¸æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–å…­ä¸ªä»£è¡¨æ€§ç±»åˆ«ï¼Œå¹¶å®Œå…¨åŸºäºçœŸå®ä¸–ç•Œæ•°æ®è¿›è¡Œä¸“ä¸šæ ‡æ³¨ã€‚ç°æœ‰çš„æœ‰å®³å†…å®¹æ£€æµ‹èµ„æºä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œä¸­æ–‡æ•°æ®é›†ç›¸å¯¹ç¨€ç¼ºä¸”èŒƒå›´æœ‰é™ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªçŸ¥è¯†å¢å¼ºçš„åŸºçº¿æ¨¡å‹ï¼Œç»“åˆäº†äººå·¥æ ‡æ³¨çš„çŸ¥è¯†è§„åˆ™å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„éšæ€§çŸ¥è¯†ï¼Œä½¿å¾—è¾ƒå°çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šèƒ½å¤Ÿä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥ç ”ç©¶ä¸ºä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹æä¾›äº†é‡è¦çš„èµ„æºå’Œæ–¹æ³•ï¼Œæå‡äº†å†…å®¹å®¡æ ¸çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10890', 'title': 'CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation', 'url': 'https://huggingface.co/papers/2506.10890', 'abstract': 'CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and aesthetically pleasing graphic compositions remains a time-consuming and skill-intensive task, especially for beginners. Current AI tools automate parts of the workflow, but struggle to accurately incorporate user-supplied assets, maintain editability, and achieve professional visual appeal. Commercial systems, like Canva Magic Design, rely on vast template libraries, which are impractical for replicate. In this paper, we introduce CreatiPoster, a framework that generates editable, multi-layer compositions from optional natural-language instructions or assets. A protocol model, an RGBA large multimodal model, first produces a JSON specification detailing every layer (text or asset) with precise layout, hierarchy, content and style, plus a concise background prompt. A conditional background model then synthesizes a coherent background conditioned on this rendered foreground layers. We construct a benchmark with automated metrics for graphic-design generation and show that CreatiPoster surpasses leading open-source approaches and proprietary commercial systems. To catalyze further research, we release a copyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports diverse applications such as canvas editing, text overlay, responsive resizing, multilingual adaptation, and animated posters, advancing the democratization of AI-assisted graphic design. Project homepage: https://github.com/graphic-design-ai/creatiposter', 'score': 10, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '22fffd0088d280e0', 'authors': ['Zhao Zhang', 'Yutao Cheng', 'Dexiang Hong', 'Maoke Yang', 'Gonglei Shi', 'Lei Ma', 'Hui Zhang', 'Jie Shao', 'Xinglong Wu'], 'affiliations': ['ByteDance, Fudan University', 'ByteDance, Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2506.10890.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#open_source', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'CreatiPoster: Ğ˜Ğ˜-Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ', 'desc': 'CreatiPoster - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ JSON-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ„Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ğ½Ğ°. CreatiPoster Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ appeal. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ…Ğ¾Ğ»ÑÑ‚Ğ°, Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°.'}, 'en': {'title': 'Revolutionizing Graphic Design with AI-Generated Custom Compositions', 'desc': 'CreatiPoster is a novel framework that generates high-quality, editable graphic designs from user inputs like text or images. It utilizes a protocol model to create a detailed JSON specification for each design layer, ensuring precise layout and style. A conditional background model then generates a cohesive background that complements the foreground elements. This approach not only enhances the editability and visual appeal of designs but also outperforms existing tools and templates in the market.'}, 'zh': {'title': 'CreatiPosterï¼šè®©å›¾å½¢è®¾è®¡æ›´ç®€å•', 'desc': 'CreatiPoster æ˜¯ä¸€ä¸ªç”Ÿæˆé«˜è´¨é‡ã€å¯ç¼–è¾‘å’Œå¯å®šåˆ¶å›¾å½¢ä½œå“çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬æˆ–èµ„äº§ä¸­åˆ›å»ºå¤šå±‚æ¬¡çš„å›¾å½¢è®¾è®¡ã€‚ä¸ç°æœ‰å·¥å…·ç›¸æ¯”ï¼Œå®ƒåœ¨ç”¨æˆ·æä¾›çš„èµ„äº§æ•´åˆã€å¯ç¼–è¾‘æ€§å’Œè§†è§‰å¸å¼•åŠ›æ–¹é¢è¡¨ç°æ›´ä½³ã€‚è¯¥æ¡†æ¶ä½¿ç”¨åè®®æ¨¡å‹ç”Ÿæˆè¯¦ç»†çš„ JSON è§„èŒƒï¼Œæè¿°æ¯ä¸€å±‚çš„å¸ƒå±€ã€å±‚æ¬¡ã€å†…å®¹å’Œé£æ ¼ã€‚é€šè¿‡æä¾›ä¸€ä¸ªæ— ç‰ˆæƒçš„ 100,000 ä¸ªå¤šå±‚è®¾è®¡çš„è¯­æ–™åº“ï¼ŒCreatiPoster ä¿ƒè¿›äº† AI è¾…åŠ©å›¾å½¢è®¾è®¡çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06952', 'title': 'LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer', 'url': 'https://huggingface.co/papers/2506.06952', 'abstract': 'LaTtE-Flow, a new architecture, unifies image understanding and generation with high performance and faster inference by using a Layerwise Timestep Experts flow-based Transformer and Timestep-Conditioned Residual Attention mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6x faster inference speed compared to recent unified multimodal models.', 'score': 9, 'issue_id': 4288, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': 'e034c1178056e190', 'authors': ['Ying Shen', 'Zhiyang Xu', 'Jiuhai Chen', 'Shizhe Diao', 'Jiaxin Zhang', 'Yuguang Yao', 'Joy Rimchala', 'Ismini Lourentzou', 'Lifu Huang'], 'affiliations': ['Intuit AI Research', 'Nvidia', 'UC Davis', 'University of Illinois Urbana-Champaign', 'University of Maryland', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2506.06952.jpg', 'data': {'categories': ['#games', '#cv', '#multimodal', '#architecture', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'LaTtE-Flow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ³Ğ°Ğ¼ Ğ¸ ÑĞ»Ğ¾ÑĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸. LaTtE-Flow Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ½Ğ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LaTtE-Flow Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 6 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unifying Image Understanding and Generation with Speed and Efficiency', 'desc': 'LaTtE-Flow is a new architecture that combines image understanding and generation into one efficient model. It uses a Layerwise Timestep Experts flow-based Transformer to improve the speed and performance of image generation tasks. By activating only specific layers for different timesteps, it enhances sampling efficiency, making it faster than previous models. Additionally, the Timestep-Conditioned Residual Attention mechanism allows for better information sharing across layers, leading to strong results in multimodal tasks.'}, 'zh': {'title': 'é«˜æ•ˆç»Ÿä¸€å›¾åƒç†è§£ä¸ç”Ÿæˆçš„LaTtE-Flowæ¶æ„', 'desc': 'LaTtE-Flowæ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨ç»Ÿä¸€å›¾åƒç†è§£å’Œç”Ÿæˆï¼Œå…·æœ‰é«˜æ€§èƒ½å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚å®ƒé‡‡ç”¨äº†åˆ†å±‚æ—¶é—´ä¸“å®¶æµå¼Transformerå’Œæ—¶é—´æ¡ä»¶æ®‹å·®æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡äº†å›¾åƒç”Ÿæˆçš„æ•ˆç‡ã€‚é€šè¿‡å°†æµåŒ¹é…è¿‡ç¨‹åˆ†å¸ƒåˆ°ä¸“é—¨çš„Transformerå±‚ç»„ä¸­ï¼ŒLaTtE-Flowåœ¨æ¯ä¸ªé‡‡æ ·æ—¶é—´æ­¥åªæ¿€æ´»å°‘é‡å±‚ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaTtE-Flowåœ¨å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨å›¾åƒç”Ÿæˆè´¨é‡ä¸Šä¹Ÿå…·æœ‰ç«äº‰åŠ›ï¼Œæ¨ç†é€Ÿåº¦æ¯”ç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹å¿«çº¦6å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10178', 'title': 'Attention, Please! Revisiting Attentive Probing for Masked Image\n  Modeling', 'url': 'https://huggingface.co/papers/2506.10178', 'abstract': 'Efficient probing, a simplified multi-query cross-attention mechanism, enhances evaluation of self-supervised learning models by improving speed, performance, and interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t As fine-tuning (FT) becomes increasingly impractical at scale, probing is emerging as the preferred evaluation protocol for self-supervised learning (SSL). Yet, the standard linear probing (LP) fails to adequately reflect the potential of models trained with Masked Image Modeling (MIM), due to the distributed nature of patch tokens. This motivates the need for attentive probing, an alternative that uses attention to selectively aggregate patch-level features. Despite its growing adoption, attentive probing remains under-explored, with existing methods suffering from excessive parameterization and poor computational efficiency.   In this work, we revisit attentive probing through the lens of the accuracy-efficiency trade-off. We conduct a systematic study of existing methods, analyzing their mechanisms and benchmarking their performance. We introduce efficient probing (EP), a multi-query cross-attention mechanism that eliminates redundant projections, reduces the number of trainable parameters, and achieves up to a 10times speed-up over conventional multi-head attention. Despite its simplicity, EP outperforms LP and prior attentive probing approaches across seven benchmarks, generalizes well beyond MIM to diverse pre-training paradigms, produces interpretable attention maps, and achieves strong gains in low-shot and layer-wise settings. Code available at https://github.com/billpsomas/efficient-probing.', 'score': 7, 'issue_id': 4280, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'e98eb04f1204de95', 'authors': ['Bill Psomas', 'Dionysis Christopoulos', 'Eirini Baltzi', 'Ioannis Kakogeorgiou', 'Tilemachos Aravanis', 'Nikos Komodakis', 'Konstantinos Karantzalos', 'Yannis Avrithis', 'Giorgos Tolias'], 'affiliations': ['Archimedes, Athena RC', 'Czech Technical University in Prague', 'IACM-FORTH', 'IARAI', 'IIT, NCSR Demokritos', 'National Technical University of Athens', 'University of Crete'], 'pdf_title_img': 'assets/pdf/title_img/2506.10178.jpg', 'data': {'categories': ['#interpretability', '#training', '#optimization', '#benchmark', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (efficient probing) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Efficient Probing: Speed and Performance in Self-Supervised Learning', 'desc': 'This paper introduces efficient probing (EP), a new method that enhances the evaluation of self-supervised learning (SSL) models by using a simplified multi-query cross-attention mechanism. Traditional linear probing (LP) does not fully capture the capabilities of models trained with Masked Image Modeling (MIM) due to the complexity of patch tokens. EP addresses this by reducing unnecessary parameters and improving computational efficiency, achieving up to a 10x speed increase compared to standard multi-head attention. The results show that EP not only outperforms LP and previous probing methods but also provides interpretable attention maps and performs well in various settings.'}, 'zh': {'title': 'é«˜æ•ˆæ¢æµ‹ï¼šæå‡è‡ªç›‘ç£å­¦ä¹ è¯„ä¼°çš„åˆ©å™¨', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆæ¢æµ‹ï¼ˆEfficient Probingï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„è¯„ä¼°æ•ˆç‡å’Œæ€§èƒ½ã€‚ä¼ ç»Ÿçš„çº¿æ€§æ¢æµ‹æ–¹æ³•æ— æ³•å……åˆ†åæ˜ ä½¿ç”¨é®æŒ¡å›¾åƒå»ºæ¨¡è®­ç»ƒçš„æ¨¡å‹çš„æ½œåŠ›ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–°çš„å…³æ³¨æœºåˆ¶æ¥é€‰æ‹©æ€§åœ°èšåˆç‰¹å¾ã€‚é«˜æ•ˆæ¢æµ‹é€šè¿‡å¤šæŸ¥è¯¢äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‡å°‘å†—ä½™æŠ•å½±å’Œå¯è®­ç»ƒå‚æ•°ï¼Œä»è€Œå®ç°æ›´å¿«çš„è®¡ç®—é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé«˜æ•ˆæ¢æµ‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä½æ ·æœ¬å’Œé€å±‚è®¾ç½®ä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08234', 'title': 'Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions', 'url': 'https://huggingface.co/papers/2506.08234', 'abstract': 'Recent advancements in optimizing compound AI systems highlight challenges in integrating various components, with an emphasis on natural language feedback methods for non-differentiable systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.', 'score': 7, 'issue_id': 4279, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'a098935231ad146d', 'authors': ['Yu-Ang Lee', 'Guan-Ting Yi', 'Mei-Yi Liu', 'Jui-Chao Lu', 'Guan-Bo Yang', 'Yun-Nung Chen'], 'affiliations': ['National Taiwan University, Taipei, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2506.08234.jpg', 'data': {'categories': ['#survey', '#rlhf', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², ÑƒĞ´ĞµĞ»ÑÑ Ğ¾ÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ´Ğ»Ñ Ğ½ĞµĞ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ°Ğº Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimizing Complex AI Systems with Natural Language Feedback', 'desc': 'This paper discusses the recent progress in optimizing compound AI systems, which are complex systems made up of multiple interacting components. It highlights the challenges faced in integrating these components, particularly when using natural language feedback methods for systems that are not easily differentiable. The authors review traditional optimization techniques like supervised fine-tuning and reinforcement learning, while also exploring new approaches that leverage natural language. They aim to formalize the concept of compound AI system optimization and identify future research directions in this evolving field.'}, 'zh': {'title': 'ä¼˜åŒ–å¤åˆAIç³»ç»Ÿçš„æ–°æ–¹æ³•æ¢ç´¢', 'desc': 'æœ€è¿‘åœ¨å¤åˆäººå·¥æ™ºèƒ½ç³»ç»Ÿä¼˜åŒ–æ–¹é¢çš„è¿›å±•çªæ˜¾äº†æ•´åˆå„ç§ç»„ä»¶çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éå¯å¾®ç³»ç»Ÿä¸­ä½¿ç”¨è‡ªç„¶è¯­è¨€åé¦ˆæ–¹æ³•ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ï¼Œå¤åˆäººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨æ‰§è¡Œå¤æ‚ä»»åŠ¡æ–¹é¢å˜å¾—æ›´åŠ é«˜æ•ˆã€‚å°½ç®¡ä¼ ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•å¦‚ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ä»ç„¶æ˜¯åŸºç¡€ï¼Œä½†è‡ªç„¶è¯­è¨€åé¦ˆçš„å…´èµ·ä¸ºä¼˜åŒ–éå¯å¾®ç³»ç»Ÿæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†å¤åˆäººå·¥æ™ºèƒ½ç³»ç»Ÿä¼˜åŒ–çš„æœ€æ–°è¿›å±•ï¼Œåˆ†ç±»ç°æœ‰æ–¹æ³•ï¼Œå¹¶å¼ºè°ƒäº†è¯¥é¢†åŸŸçš„å¼€æ”¾ç ”ç©¶æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06950', 'title': 'What Makes a Good Natural Language Prompt?', 'url': 'https://huggingface.co/papers/2506.06950', 'abstract': 'A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.', 'score': 7, 'issue_id': 4275, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ½Ñ', 'en': 'June 7', 'zh': '6æœˆ7æ—¥'}, 'hash': '265555e63c6771ba', 'authors': ['Do Xuan Long', 'Duy Dinh', 'Ngoc-Hai Nguyen', 'Kenji Kawaguchi', 'Nancy F. Chen', 'Shafiq Joty', 'Min-Yen Kan'], 'affiliations': ['Institute for Infocomm Research (I2R), A*STAR', 'National University of Singapore', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.06950.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#survey', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 150 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸Ğ»Ğ¸ 21 ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ÑĞ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 6 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Optimizing Prompts for Smarter AI Reasoning', 'desc': 'This paper presents a framework for evaluating and optimizing natural language prompts used in large language models (LLMs). It identifies 21 properties of prompts, organized into six dimensions, that influence their effectiveness in reasoning tasks. The authors conducted a meta-analysis of over 150 studies to highlight the inconsistencies in how prompt quality is assessed across different models and tasks. Their findings suggest that enhancing prompts based on specific properties can significantly improve LLM performance, particularly through instruction-tuning techniques.'}, 'zh': {'title': 'ä¼˜åŒ–æç¤ºï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè¯„ä¼°å’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è‡ªç„¶è¯­è¨€æç¤ºçš„æ¡†æ¶ï¼Œæ­ç¤ºäº†æç¤ºå±æ€§ä¸æ¨ç†ä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¯¹2022è‡³2025å¹´é—´150å¤šç¯‡ä¸æç¤ºç›¸å…³çš„è®ºæ–‡è¿›è¡Œå…ƒåˆ†æï¼Œæ¢è®¨äº†è‡ªç„¶è¯­è¨€æç¤ºçš„é‡åŒ–æ ‡å‡†ã€‚è¯¥æ¡†æ¶åŒ…å«21ä¸ªå±æ€§ï¼Œåˆ†ä¸ºå…­ä¸ªç»´åº¦ï¼Œæ—¨åœ¨è¯„ä¼°æç¤ºè´¨é‡ã€‚ç ”ç©¶å‘ç°ï¼Œå•ä¸€å±æ€§çš„å¢å¼ºå¯¹æ¨ç†ä»»åŠ¡çš„å½±å“æœ€å¤§ï¼Œè€ŒåŸºäºå±æ€§å¢å¼ºçš„æŒ‡ä»¤è°ƒä¼˜å¯ä»¥æå‡æ¨ç†æ¨¡å‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09952', 'title': 'UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2506.09952', 'abstract': "UniPre3D is a unified pre-training method for 3D point clouds and models of any scale, using Gaussian primitives and 2D feature integration for effective performance across object and scene tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", 'score': 5, 'issue_id': 4279, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'fc746da54c59982b', 'authors': ['Ziyi Wang', 'Yanran Zhang', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Department of Automation, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09952.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#3d'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°', 'desc': 'UniPre3D - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ 2D-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ĞºĞ°Ğº Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¸ ÑĞºĞ²Ğ¾Ğ·Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. UniPre3D Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹.'}, 'en': {'title': 'UniPre3D: Unified Pre-Training for All 3D Scales', 'desc': "UniPre3D is a novel pre-training method designed for 3D point clouds and models, addressing the challenges posed by varying scales in 3D vision. It uniquely predicts Gaussian primitives as part of its pre-training task and utilizes differentiable Gaussian splatting for accurate image rendering. By integrating 2D features from pre-trained image models, it enhances the model's understanding of geometric structures and textures. Extensive experiments demonstrate its effectiveness across both object and scene-level tasks, making it a versatile solution for 3D representation learning."}, 'zh': {'title': 'ç»Ÿä¸€é¢„è®­ç»ƒï¼Œæå‡3Dè§†è§‰è¡¨ç°', 'desc': 'UniPre3Dæ˜¯ä¸€ç§ç»Ÿä¸€çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨å¤„ç†å„ç§è§„æ¨¡çš„3Dç‚¹äº‘å’Œæ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹é«˜æ–¯åŸè¯­ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å¯å¾®åˆ†çš„é«˜æ–¯ç‚¹äº‘æ¸²æŸ“æŠ€æœ¯ï¼Œå®ç°äº†ç²¾ç¡®çš„åƒç´ çº§ç›‘ç£ã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹å¯¹å‡ ä½•ç»“æ„çš„å…³æ³¨ï¼ŒUniPre3Dè¿˜æ•´åˆäº†æ¥è‡ªé¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„2Dç‰¹å¾ï¼Œåˆ©ç”¨å·²æœ‰çš„çº¹ç†çŸ¥è¯†ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¹è±¡å’Œåœºæ™¯ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ™®éçš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09942', 'title': 'VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following', 'url': 'https://huggingface.co/papers/2506.09942', 'abstract': 'VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF.', 'score': 5, 'issue_id': 4272, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '5430c32ec46dccaa', 'authors': ['Hao Peng', 'Yunjia Qi', 'Xiaozhi Wang', 'Bin Xu', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09942.jpg', 'data': {'categories': ['#dataset', '#optimization', '#rl', '#benchmark', '#open_source', '#reasoning', '#training', '#rlhf'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'VerIF: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ RL Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VerIF - Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VerInstruct Ñ Ğ¾ĞºĞ¾Ğ»Ğ¾ 22 000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ RL Ñ VerIF Ğº Ğ´Ğ²ÑƒĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ RL Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'VerIF: Boosting Instruction-Following RL with Hybrid Verification', 'desc': 'This paper introduces VerIF, a novel hybrid verification method that merges rule-based and large language model (LLM) approaches to improve reinforcement learning (RL) in instruction-following tasks. The authors highlight the importance of verification engineering in enhancing LLMs through reinforcement learning with verifiable rewards (RLVR). They present a new dataset, VerInstruct, which contains around 22,000 instruction-following instances with verification signals to support their method. The results show that models trained with VerIF achieve state-of-the-art performance and maintain strong generalization capabilities, indicating that this approach can effectively enhance existing RL frameworks.'}, 'zh': {'title': 'VerIFï¼šæå‡æŒ‡ä»¤è·Ÿéšçš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVerIFçš„æ··åˆéªŒè¯æ–¹æ³•ï¼Œç»“åˆäº†åŸºäºè§„åˆ™çš„éªŒè¯å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éªŒè¯ï¼Œæ˜¾è‘—æå‡äº†æŒ‡ä»¤è·Ÿéšçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤è·Ÿéšæ•°æ®é›†VerInstructï¼ŒåŒ…å«çº¦22,000ä¸ªå®ä¾‹åŠå…¶éªŒè¯ä¿¡å·ï¼Œä»¥æ”¯æŒè¿™ä¸€æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨VerIFè¿›è¡ŒRLè®­ç»ƒï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªä»£è¡¨æ€§çš„æŒ‡ä»¤è·ŸéšåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè®­ç»ƒåçš„æ¨¡å‹åœ¨åŒç±»æ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œå¹¶ä¸”å¯¹æœªè§çº¦æŸå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒVerIFå¯ä»¥ä¸ç°æœ‰çš„RLæ–¹æ³•ç»“åˆï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08060', 'title': 'Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques', 'url': 'https://huggingface.co/papers/2506.08060', 'abstract': 'Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length l, datasets of size Oleft( m V{varepsilon^2} log m{delta} right) or, with bounded context, Oleft( l log V{varepsilon^2} log 1{delta} right) suffice to approximate fine-tuned behavior across m contexts within error varepsilon, where V is the vocabulary size and delta is the failure probability. For linear classification, datasets of size Oleft( d{varepsilon} right) or, with fixed context, Oleft( 1{varepsilon^2} log 1{delta} right) are sufficient, where d is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.', 'score': 5, 'issue_id': 4272, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '5c9cde8c4bcbdc6e', 'authors': ['Asankhaya Sharma'], 'affiliations': ['Patched Codes, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.08060.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#rag', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Transformers: Fine-Tuning Efficiency through In-Context Learning', 'desc': "This paper explores how transformers can mimic the performance of supervised fine-tuning (SFT) through a method called in-context learning (ICL) without changing the model's parameters. It provides theoretical proofs that under certain ideal conditions, a base transformer can achieve results similar to those obtained through SFT. The authors extend their findings to practical situations, showing that smaller datasets can still approximate fine-tuned behavior effectively. This research highlights the potential for more efficient use of large language models in real-world applications by leveraging retrieval-augmented generation techniques."}, 'zh': {'title': 'å˜æ¢å™¨æ¨¡å‹ï¼šé«˜æ•ˆè¿‘ä¼¼ç›‘ç£å¾®è°ƒçš„æœªæ¥', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å˜æ¢å™¨æ¨¡å‹å¦‚ä½•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨ä¸æ”¹å˜æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œè¿‘ä¼¼ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œå˜æ¢å™¨æ¨¡å‹å¯ä»¥åˆ©ç”¨æ¨ç†æ—¶çš„æŠ€æœ¯æ¥æ¨¡æ‹ŸSFTçš„æ•ˆæœã€‚æˆ‘ä»¬è¿˜æ‰©å±•äº†è¿™äº›ç»“æœåˆ°å®é™…åœºæ™¯ï¼Œè€ƒè™‘æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œéƒ¨åˆ†æ•°æ®é›†è®¿é—®ã€‚é€šè¿‡ç†è®ºè¯æ˜ï¼Œè¿™ä¸ºå¤§è¯­è¨€æ¨¡å‹çš„èµ„æºé«˜æ•ˆéƒ¨ç½²æä¾›äº†åŸºç¡€ï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆç­‰å®ç”¨æŠ€æœ¯ï¼Œå°†ç†è®ºä¸å®é™…åº”ç”¨ç›¸ç»“åˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10920', 'title': 'Decomposing MLP Activations into Interpretable Features via\n  Semi-Nonnegative Matrix Factorization', 'url': 'https://huggingface.co/papers/2506.10920', 'abstract': "SNMF is used to identify interpretable features in LLMs by directly decomposing MLP activations, outperforming SAEs and supervised methods in causal evaluations and aligning with human-interpretable concepts.  \t\t\t\t\tAI-generated summary \t\t\t\t A central goal for mechanistic interpretability has been to identify the right units of analysis in large language models (LLMs) that causally explain their outputs. While early work focused on individual neurons, evidence that neurons often encode multiple concepts has motivated a shift toward analyzing directions in activation space. A key question is how to find directions that capture interpretable features in an unsupervised manner. Current methods rely on dictionary learning with sparse autoencoders (SAEs), commonly trained over residual stream activations to learn directions from scratch. However, SAEs often struggle in causal evaluations and lack intrinsic interpretability, as their learning is not explicitly tied to the computations of the model. Here, we tackle these limitations by directly decomposing MLP activations with semi-nonnegative matrix factorization (SNMF), such that the learned features are (a) sparse linear combinations of co-activated neurons, and (b) mapped to their activating inputs, making them directly interpretable. Experiments on Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs and a strong supervised baseline (difference-in-means) on causal steering, while aligning with human-interpretable concepts. Further analysis reveals that specific neuron combinations are reused across semantically-related features, exposing a hierarchical structure in the MLP's activation space. Together, these results position SNMF as a simple and effective tool for identifying interpretable features and dissecting concept representations in LLMs.", 'score': 4, 'issue_id': 4285, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'ce3cd0a96d1ecc71', 'authors': ['Or Shafran', 'Atticus Geiger', 'Mor Geva'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University', 'Pr(Ai)2R Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.10920.jpg', 'data': {'categories': ['#architecture', '#data', '#training', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SNMF: ĞºĞ»ÑÑ‡ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑƒĞ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (SNMF) Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). SNMF Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ° (MLP), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ (SAE) Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Llama 3.1, Gemma 2 Ğ¸ GPT-2 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SNMF, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ´Ğ´Ğ°ÑÑ‚ÑÑ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ MLP, Ğ³Ğ´Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ².'}, 'en': {'title': 'Unlocking Interpretability in LLMs with SNMF', 'desc': 'This paper introduces semi-nonnegative matrix factorization (SNMF) as a method to extract interpretable features from large language models (LLMs) by analyzing multi-layer perceptron (MLP) activations. Unlike sparse autoencoders (SAEs), which often fail in causal evaluations, SNMF directly decomposes activations into sparse linear combinations of neurons, making the features more interpretable. The study demonstrates that SNMF outperforms SAEs and supervised methods in identifying causal relationships and aligns better with human-understandable concepts. Additionally, it reveals a hierarchical structure in the activation space, showing how certain neuron combinations are reused across related features.'}, 'zh': {'title': 'SNMFï¼šæ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šç‰¹å¾', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠéè´ŸçŸ©é˜µåˆ†è§£ï¼ˆSNMFï¼‰çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­è¯†åˆ«å¯è§£é‡Šçš„ç‰¹å¾ã€‚ä¸ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰å’Œç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼ŒSNMFåœ¨å› æœè¯„ä¼°ä¸­è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”ä¸äººç±»å¯è§£é‡Šçš„æ¦‚å¿µå¯¹é½ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›´æ¥åˆ†è§£å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„æ¿€æ´»ï¼Œå­¦ä¹ åˆ°çš„ç‰¹å¾æ˜¯ç¨€ç–çš„çº¿æ€§ç»„åˆï¼Œå¹¶ä¸”å¯ä»¥ç›´æ¥æ˜ å°„åˆ°å…¶æ¿€æ´»è¾“å…¥ä¸Šï¼Œä»è€Œæé«˜äº†å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSNMFåœ¨è¯†åˆ«å¯è§£é‡Šç‰¹å¾å’Œè§£ææ¦‚å¿µè¡¨ç¤ºæ–¹é¢æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10911', 'title': 'NoLoCo: No-all-reduce Low Communication Training Method for Large Models', 'url': 'https://huggingface.co/papers/2506.10911', 'abstract': 'NoLoCo is a novel optimization method that eliminates explicit parameter synchronization and reduces communication overhead during the training of large language models, achieving faster convergence rates and reduced idling time compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models is generally done via optimization methods on clusters containing tens of thousands of accelerators, communicating over a high-bandwidth interconnect. Scaling up these clusters is expensive and can become impractical, imposing limits on the size of models that can be trained. Several recent studies have proposed training methods that are less communication intensive, avoiding the need for a highly connected compute cluster. These state-of-the-art low communication training methods still employ a synchronization step for model parameters, which, when performed over all model replicas, can become costly on a low-bandwidth network.   In this work, we propose a novel optimization method, NoLoCo, that does not explicitly synchronize all model parameters during training and, as a result, does not require any collective communication. NoLoCo implicitly synchronizes model weights via a novel variant of the Nesterov momentum optimizer by partially averaging model weights with a randomly selected other one. We provide both a theoretical convergence analysis for our proposed optimizer as well as empirical results from language model training.   We benchmark NoLoCo on a wide range of accelerator counts and model sizes, between 125M to 6.8B parameters. Our method requires significantly less communication overhead than fully sharded data parallel training or even widely used low communication training method, DiLoCo. The synchronization step itself is estimated to be one magnitude faster than the all-reduce used in DiLoCo for few hundred accelerators training over the internet. We also do not have any global blocking communication that reduces accelerator idling time. Compared to DiLoCo, we also observe up to 4% faster convergence rate with wide range of model sizes and accelerator counts.', 'score': 4, 'issue_id': 4288, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'f39f533144e06990', 'authors': ['Jari Kolehmainen', 'Nikolay Blagoev', 'John Donaghy', 'OÄŸuzhan Ersoy', 'Christopher Nies'], 'affiliations': ['Gensyn'], 'pdf_title_img': 'assets/pdf/title_img/2506.10911.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'NoLoCo: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'NoLoCo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ²Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° ĞĞµÑÑ‚ĞµÑ€Ğ¾Ğ²Ğ°, Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑƒÑÑ€ĞµĞ´Ğ½ÑÑ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. NoLoCo Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ NoLoCo Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 125 Ğ¼Ğ»Ğ½ Ğ´Ğ¾ 6,8 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'NoLoCo: Faster Training with Less Communication!', 'desc': 'NoLoCo is an innovative optimization technique designed for training large language models without the need for explicit parameter synchronization. By avoiding collective communication, it significantly reduces communication overhead and minimizes idling time among accelerators. The method utilizes a modified Nesterov momentum optimizer that implicitly synchronizes model weights through partial averaging with randomly selected weights. Empirical results demonstrate that NoLoCo achieves faster convergence rates and is more efficient than existing low communication training methods, such as DiLoCo.'}, 'zh': {'title': 'NoLoCoï¼šé«˜æ•ˆçš„æ— åŒæ­¥ä¼˜åŒ–æ–¹æ³•', 'desc': 'NoLoCoæ˜¯ä¸€ç§æ–°é¢–çš„ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æ¶ˆé™¤æ˜¾å¼çš„å‚æ•°åŒæ­¥ï¼Œä»è€Œå‡å°‘åœ¨å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„é€šä¿¡å¼€é”€ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒNoLoCoå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å°‘çš„ç©ºé—²æ—¶é—´ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ç§æ–°å‹çš„NesterovåŠ¨é‡ä¼˜åŒ–å™¨å˜ä½“ï¼Œéšå¼åœ°åŒæ­¥æ¨¡å‹æƒé‡ï¼Œéƒ¨åˆ†å¹³å‡ä¸éšæœºé€‰æ‹©çš„å…¶ä»–æƒé‡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNoLoCoåœ¨ä¸åŒçš„åŠ é€Ÿå™¨æ•°é‡å’Œæ¨¡å‹è§„æ¨¡ä¸‹ï¼Œé€šä¿¡å¼€é”€æ˜¾è‘—ä½äºä¼ ç»Ÿçš„å…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œè®­ç»ƒæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10568', 'title': 'DreamActor-H1: High-Fidelity Human-Product Demonstration Video\n  Generation via Motion-designed Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.10568', 'abstract': 'A Diffusion Transformer-based framework generates high-fidelity human-product demonstration videos by preserving identities and spatial relationships, using masked cross-attention and structured text encoding.  \t\t\t\t\tAI-generated summary \t\t\t\t In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/.', 'score': 4, 'issue_id': 4281, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '83c202462f081ecf', 'authors': ['Lizhen Wang', 'Zhurong Xia', 'Tianshu Hu', 'Pengrui Wang', 'Pengfei Wang', 'Zerong Zheng', 'Ming Zhou'], 'affiliations': ['ByteDance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2506.10568.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#video', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½ 3D-Ğ¼ĞµÑˆĞ° Ñ‚ĞµĞ»Ğ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Realistic Human-Product Videos with Diffusion Transformers', 'desc': 'This paper presents a Diffusion Transformer-based framework designed to create realistic human-product demonstration videos for e-commerce. The framework addresses the common issues of identity preservation and spatial relationships between humans and products by using masked cross-attention and structured text encoding. By incorporating 3D body mesh templates and product bounding boxes, the method ensures accurate motion guidance and alignment of gestures with products. The approach is trained on a diverse dataset, achieving superior results in generating high-fidelity videos that maintain the integrity of both human and product identities.'}, 'zh': {'title': 'ç”Ÿæˆé«˜ä¿çœŸæ¼”ç¤ºè§†é¢‘çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformer, DiTï¼‰çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸçš„äººç±»ä¸äº§å“æ¼”ç¤ºè§†é¢‘ã€‚è¯¥æ–¹æ³•é€šè¿‡æ³¨å…¥é…å¯¹çš„äººç±»ä¸äº§å“å‚è€ƒä¿¡æ¯ï¼Œç»“åˆæ©è”½äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶ä¿ç•™äººç±»èº«ä»½å’Œäº§å“ç»†èŠ‚ã€‚æˆ‘ä»¬ä½¿ç”¨3Dèº«ä½“ç½‘æ ¼æ¨¡æ¿å’Œäº§å“è¾¹ç•Œæ¡†æä¾›ç²¾ç¡®çš„è¿åŠ¨æŒ‡å¯¼ï¼Œä»è€Œå®ç°æ‰‹åŠ¿ä¸äº§å“ä½ç½®çš„ç›´è§‚å¯¹é½ã€‚æ­¤å¤–ï¼Œç»“æ„åŒ–æ–‡æœ¬ç¼–ç ç”¨äºå¼•å…¥ç±»åˆ«çº§è¯­ä¹‰ï¼Œå¢å¼ºäº†åœ¨å°æ—‹è½¬å˜åŒ–ä¸‹çš„3Dä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10036', 'title': 'Token Perturbation Guidance for Diffusion Models', 'url': 'https://huggingface.co/papers/2506.10036', 'abstract': 'Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2times improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance', 'score': 4, 'issue_id': 4277, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '3637011ee12cd77a', 'authors': ['Javad Rajabi', 'Soroush Mehraban', 'Seyedmorteza Sadat', 'Babak Taati'], 'affiliations': ['ETH ZÃ¼rich', 'KITE Research Institute', 'University of Toronto', 'Vector Institute for Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2506.10036.jpg', 'data': {'categories': ['#optimization', '#training', '#diffusion', '#cv'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Token Perturbation Guidance (TPG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. TPG Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ² ÑĞµÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ ĞºĞ°Ğº Ğ´Ğ»Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Classifier-free guidance (CFG). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… SDXL Ğ¸ Stable Diffusion 2.1 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ FID Ğ´Ğ»Ñ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Diffusion Models with Token Perturbation Guidance', 'desc': 'Token Perturbation Guidance (TPG) is a new method that improves the quality of images generated by diffusion models without needing additional training. It works by applying perturbation matrices to the intermediate token representations, which helps guide the generation process effectively. Unlike Classifier-free Guidance (CFG), TPG does not require specific training and can be used for both conditional and unconditional generation tasks. Experiments show that TPG significantly enhances generation quality, achieving nearly double the improvement in FID scores compared to existing methods, while maintaining alignment with prompts.'}, 'zh': {'title': 'ä»¤ç‰Œæ‰°åŠ¨å¼•å¯¼ï¼šæ— è®­ç»ƒçš„ç”Ÿæˆè´¨é‡æå‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºä»¤ç‰Œæ‰°åŠ¨å¼•å¯¼ï¼ˆTPGï¼‰ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œè€Œæ— éœ€è¿›è¡Œè®­ç»ƒã€‚TPGé€šè¿‡å¯¹æ‰©æ•£ç½‘ç»œä¸­é—´ä»¤ç‰Œè¡¨ç¤ºæ–½åŠ æ‰°åŠ¨çŸ©é˜µï¼Œæä¾›æœ‰æ•ˆä¸”ç¨³å®šçš„å¼•å¯¼ä¿¡å·ï¼Œä»è€Œæ”¹å–„ç”Ÿæˆæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ç›¸æ¯”ï¼ŒTPGåœ¨æ— æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºæ¥è¿‘åˆ†ç±»å™¨æ— å…³å¼•å¯¼çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTPGåœ¨ç”Ÿæˆè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œä¸”é€‚ç”¨äºæ¡ä»¶å’Œæ— æ¡ä»¶ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10674', 'title': 'TeleMath: A Benchmark for Large Language Models in Telecom Mathematical\n  Problem Solving', 'url': 'https://huggingface.co/papers/2506.10674', 'abstract': 'A benchmark dataset called TeleMath evaluates Large Language Models in domain-specific mathematical problems within telecommunications, showing that models designed for mathematical reasoning perform better than general-purpose models.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of artificial intelligence in telecommunications has raised interest in the capability of Large Language Models (LLMs) to address domain-specific, mathematically intensive tasks. Although recent advancements have improved the performance of LLMs in general mathematical reasoning, their effectiveness within specialized domains, such as signal processing, network optimization, and performance analysis, remains largely unexplored. To address this gap, we introduce TeleMath, the first benchmark dataset specifically designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain. Comprising 500 question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the telecommunications field. This paper outlines the proposed QnAs generation pipeline, starting from a selected seed of problems crafted by Subject Matter Experts. The evaluation of a wide range of open-source LLMs reveals that best performance on TeleMath is achieved by recent models explicitly designed for mathematical or logical reasoning. In contrast, general-purpose models, even those with a large number of parameters, often struggle with these challenges. We have released the dataset and the evaluation code to ease result reproducibility and support future research.', 'score': 3, 'issue_id': 4285, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'fc14281fcee53b0d', 'authors': ['Vincenzo Colle', 'Mohamed Sana', 'Nicola Piovesan', 'Antonio De Domenico', 'Fadhel Ayed', 'Merouane Debbah'], 'affiliations': ['Khalifa University of Science and Technology, Abu Dhabi, UAE', 'Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France', 'UniversitÃ  degli Studi di Cassino del Lazio Meridionale, Cassino, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2506.10674.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#open_source', '#math', '#benchmark'], 'emoji': 'ğŸ“¡', 'ru': {'title': 'TeleMath: Ğ˜Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ² Ñ‚ĞµĞ»ĞµĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ÑÑ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TeleMath Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ‚ĞµĞ»ĞµĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. ĞĞ°Ğ±Ğ¾Ñ€ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 500 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ñ‚ĞµĞ¼ Ğ² Ñ‚ĞµĞ»ĞµĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'TeleMath: Evaluating LLMs in Telecommunications Mathematics', 'desc': 'The paper introduces TeleMath, a benchmark dataset aimed at assessing Large Language Models (LLMs) on mathematical problems specific to the telecommunications sector. It highlights that LLMs tailored for mathematical reasoning outperform general-purpose models when tackling domain-specific tasks. The dataset consists of 500 question-answer pairs covering various telecommunications topics, created with input from Subject Matter Experts. The findings suggest that specialized models are more effective in solving these complex mathematical challenges compared to their general counterparts.'}, 'zh': {'title': 'ä¸“æ³¨ç”µä¿¡æ•°å­¦ï¼Œæå‡æ¨¡å‹è¡¨ç°', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºTeleMathçš„åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”µä¿¡é¢†åŸŸç‰¹å®šæ•°å­¦é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸“ä¸ºæ•°å­¦æ¨ç†è®¾è®¡çš„æ¨¡å‹åœ¨è§£å†³è¿™äº›é—®é¢˜æ—¶è¡¨ç°ä¼˜äºé€šç”¨æ¨¡å‹ã€‚TeleMathåŒ…å«500ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–ç”µä¿¡é¢†åŸŸçš„å¹¿æ³›ä¸»é¢˜ï¼Œå¡«è¡¥äº†LLMsåœ¨ä¸“ä¸šé¢†åŸŸåº”ç”¨çš„ç©ºç™½ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†æ•°æ®é›†å’Œè¯„ä¼°ä»£ç ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶å’Œç»“æœçš„å¯é‡å¤æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08373', 'title': 'Draft-based Approximate Inference for LLMs', 'url': 'https://huggingface.co/papers/2506.08373', 'abstract': "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.", 'score': 3, 'issue_id': 4272, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '02a9a3f798ba1509', 'authors': ['Kevin Galim', 'Ethan Ewer', 'Wonjun Kang', 'Minjae Lee', 'Hyung Il Koo', 'Kangwook Lee'], 'affiliations': ['Ajou University', 'FuriosaAI', 'Seoul National University', 'UW-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.08373.jpg', 'data': {'categories': ['#optimization', '#long_context', '#benchmark', '#architecture', '#training', '#inference'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ˜Ğ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ°Ñ€ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: SpecKV Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸ SpecPC Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing LLM Inference with Draft Models for Efficiency and Accuracy', 'desc': 'This paper introduces a new framework that uses draft models to enhance approximate inference in long-context Large Language Models (LLMs). By accurately predicting the importance of tokens and key-value (KV) pairs, the framework improves the accuracy of LLMs while keeping memory and computational efficiency in check. The authors present two specific implementations: SpecKV for effective KV cache dropping and SpecPC for identifying unimportant prompt tokens. Their experiments demonstrate that this approach outperforms existing methods in accuracy while maintaining low resource usage.'}, 'zh': {'title': 'åˆ©ç”¨è‰ç¨¿æ¨¡å‹æå‡é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨è‰ç¨¿æ¨¡å‹æ¥å¢å¼ºé•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘ä¼¼æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°é¢„æµ‹ä»¤ç‰Œå’Œé”®å€¼å¯¹çš„é‡è¦æ€§ï¼Œè¯¥æ–¹æ³•æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å†…å­˜å’Œè®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§å…·ä½“å®ç°ï¼šSpecKVå’ŒSpecPCï¼Œåˆ†åˆ«ç”¨äºä¼˜åŒ–é”®å€¼ç¼“å­˜å’Œæç¤ºä»¤ç‰Œçš„é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§ã€å†…å­˜ä½¿ç”¨ã€å»¶è¿Ÿå’Œååé‡æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07795', 'title': 'LLM Unlearning Should Be Form-Independent', 'url': 'https://huggingface.co/papers/2506.07795', 'abstract': "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) unlearning aims to erase or suppress undesirable knowledge within the model, offering promise for controlling harmful or private information to prevent misuse. However, recent studies highlight its limited efficacy in real-world scenarios, hindering practical adoption. In this study, we identify a pervasive issue underlying many downstream failures: the effectiveness of existing unlearning methods heavily depends on the form of training samples and frequently fails to generalize to alternate expressions of the same knowledge. We formally characterize this problem as Form-Dependent Bias and systematically investigate its specific manifestation patterns across various downstream tasks. To quantify its prevalence and support future research, we introduce ORT, a novel benchmark designed to evaluate the robustness of unlearning methods against variations in knowledge expression. Results reveal that Form-Dependent Bias is both widespread and severe among current techniques.   We argue that LLM unlearning should be form-independent to address the endless forms of downstream tasks encountered in real-world security-critical scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR), a novel training-free method, as a promising solution path. ROCR performs unlearning by targeting the invariants in downstream tasks, specifically the activated dangerous concepts. It is capable of modifying model parameters within seconds to redirect the model's perception of a specific unlearning target concept to another harmless concept. Extensive experiments demonstrate that ROCR significantly improves unlearning effectiveness compared to traditional methods while generating highly natural outputs.", 'score': 3, 'issue_id': 4279, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '7314028832938fb2', 'authors': ['Xiaotian Ye', 'Mengqi Zhang', 'Shu Wu'], 'affiliations': ['New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences', 'School of Computer Science, Beijing University of Posts and Telecommunications', 'Shandong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07795.jpg', 'data': {'categories': ['#security', '#rlhf', '#ethics', '#benchmark', '#hallucinations', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Form-Dependent Bias, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Rank-one Concept Redirection (ROCR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ROCR Ğ¿ĞµÑ€ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğº Ğ±ĞµĞ·Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlearning Without Limits: ROCR for Form-Independent Knowledge Management', 'desc': 'This paper addresses the challenge of unlearning in Large Language Models (LLMs), specifically focusing on the limitations caused by Form-Dependent Bias, which affects the effectiveness of unlearning methods across different knowledge expressions. The authors propose a new method called Rank-one Concept Redirection (ROCR) that aims to enhance unlearning efficacy by being form-independent, allowing it to generalize better across various tasks. They introduce a benchmark called ORT to evaluate the robustness of unlearning techniques against different expressions of knowledge. Experimental results show that ROCR outperforms traditional unlearning methods, providing a more effective and efficient way to manage harmful or private information in LLMs.'}, 'zh': {'title': 'å½¢å¼æ— å…³çš„å»å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å»é™¤ä¸è‰¯çŸ¥è¯†æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½¢å¼ä¾èµ–åå·®çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„å»å­¦ä¹ æ–¹æ³•åœ¨ä¸åŒçŸ¥è¯†è¡¨è¾¾å½¢å¼ä¸‹çš„æœ‰æ•ˆæ€§æœ‰é™ï¼Œå¯¼è‡´å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”Rank-one Concept Redirectionï¼ˆROCRï¼‰ï¼Œæ—¨åœ¨å®ç°å½¢å¼æ— å…³çš„å»å­¦ä¹ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒROCRåœ¨å»å­¦ä¹ çš„æœ‰æ•ˆæ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒåŒæ—¶ç”Ÿæˆçš„è¾“å‡ºä¹Ÿæ›´åŠ è‡ªç„¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10737', 'title': 'TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to\n  Evolving Research Corpora', 'url': 'https://huggingface.co/papers/2506.10737', 'abstract': "TaxoAdapt dynamically adapts an LLM-generated taxonomy for scientific literature across multiple dimensions, improving granularity and coherence compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs.", 'score': 2, 'issue_id': 4283, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'ba81aecb13322a55', 'authors': ['Priyanka Kargupta', 'Nan Zhang', 'Yunyi Zhang', 'Rui Zhang', 'Prasenjit Mitra', 'Jiawei Han'], 'affiliations': ['The Pennsylvania State University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.10737.jpg', 'data': {'categories': ['#science', '#multimodal', '#dataset'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹', 'desc': 'TaxoAdapt - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ (LLM), Ğº Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ñ€Ğ¿ÑƒÑÑƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. TaxoAdapt Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸ÑÑ…, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ TaxoAdapt Ğ½Ğ° 26.51% Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ° 50.41% Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹ Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM.'}, 'en': {'title': 'Dynamic Taxonomy Adaptation for Evolving Science', 'desc': "TaxoAdapt is a novel framework that enhances the organization of scientific literature by dynamically adapting taxonomies generated by large language models (LLMs). It addresses the limitations of traditional expert-curated taxonomies and existing automatic methods, which often lack generalizability and fail to capture the evolving nature of scientific fields. By employing iterative hierarchical classification, TaxoAdapt expands the taxonomy's width and depth based on the topical distribution of the corpus, allowing for a more nuanced representation of research contributions. The results show that TaxoAdapt achieves significantly higher granularity and coherence compared to leading methods, making it a powerful tool for structuring scientific knowledge."}, 'zh': {'title': 'åŠ¨æ€é€‚åº”ç§‘å­¦æ–‡çŒ®åˆ†ç±»æ³•çš„åˆ›æ–°æ–¹æ³•', 'desc': 'TaxoAdapt æ˜¯ä¸€ç§åŠ¨æ€è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„åˆ†ç±»æ³•çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç§‘å­¦æ–‡çŒ®çš„ç»„ç»‡å’Œæ£€ç´¢æ•ˆç‡ã€‚å®ƒé€šè¿‡è¿­ä»£çš„å±‚æ¬¡åˆ†ç±»ï¼ŒåŸºäºæ–‡çŒ®çš„ä¸»é¢˜åˆ†å¸ƒæ‰©å±•åˆ†ç±»æ³•çš„å®½åº¦å’Œæ·±åº¦ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”å¿«é€Ÿå‘å±•çš„ç§‘å­¦é¢†åŸŸã€‚ä¸ä¼ ç»Ÿçš„ä¸“å®¶ç­–åˆ’åˆ†ç±»æ³•ç›¸æ¯”ï¼ŒTaxoAdapt åœ¨ç»†ç²’åº¦å’Œä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåˆ†åˆ«æé«˜äº† 26.51% å’Œ 50.41%ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†ç§‘å­¦æ–‡çŒ®çš„å¤šç»´ç‰¹æ€§ï¼Œä½¿å¾—å•ç¯‡ç ”ç©¶è®ºæ–‡å¯ä»¥åœ¨å¤šä¸ªç»´åº¦ä¸Šè¿›è¡Œè´¡çŒ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10728', 'title': 'Beyond True or False: Retrieval-Augmented Hierarchical Analysis of\n  Nuanced Claims', 'url': 'https://huggingface.co/papers/2506.10728', 'abstract': 'ClaimSpect is a retrieval-augmented generation-based framework that constructs a hierarchical structure of aspects for claims, enriching them with diverse perspectives from a corpus.  \t\t\t\t\tAI-generated summary \t\t\t\t Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely "true" or "false" -- as is frequently the case with scientific and political claims. However, a claim (e.g., "vaccine A is better than vaccine B") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., "how many biomedical papers believe vaccine A is more transportable than B?"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.', 'score': 2, 'issue_id': 4283, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'f5822dc8f4d101b9', 'authors': ['Priyanka Kargupta', 'Runchu Tian', 'Jiawei Han'], 'affiliations': ['Department of Computer Science, University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.10728.jpg', 'data': {'categories': ['#science', '#reasoning', '#rag', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹', 'desc': 'ClaimSpect - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°Ñ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ĞµĞ³Ñ‡Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ. ClaimSpect Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ğ°Ñ….'}, 'en': {'title': 'Deconstructing Claims for Clearer Perspectives', 'desc': 'ClaimSpect is a framework that uses retrieval-augmented generation to break down complex claims into a structured hierarchy of aspects and sub-aspects. This approach allows for a nuanced analysis of claims, such as those in science and politics, by focusing on individual components like efficacy and safety. By retrieving relevant information from a corpus, ClaimSpect enriches these aspects with diverse perspectives, helping users understand varying viewpoints and their prevalence. The framework has been tested on real-world claims, demonstrating its ability to provide comprehensive and accurate insights into complex issues.'}, 'zh': {'title': 'ClaimSpectï¼šè§£æ„å£°æ˜çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'ClaimSpect æ˜¯ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå£°æ˜æ„å»ºå±‚æ¬¡ç»“æ„çš„æ–¹é¢ï¼Œå¹¶ä»è¯­æ–™åº“ä¸­ä¸°å¯Œå¤šæ ·çš„è§†è§’ã€‚å£°æ˜é€šå¸¸æ˜¯å¤æ‚çš„ï¼Œä¸èƒ½ç®€å•åœ°æ ‡è®°ä¸ºâ€œçœŸâ€æˆ–â€œå‡â€ï¼Œä½†å¯ä»¥å°†å…¶åˆ†è§£ä¸ºæ›´æ˜“éªŒè¯çš„åŸºæœ¬æ–¹é¢ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†å±‚åˆ’åˆ†è¾“å…¥è¯­æ–™åº“ï¼Œæ£€ç´¢ç›¸å…³ç‰‡æ®µï¼Œå¸®åŠ©å‘ç°æ–°çš„å­æ–¹é¢å’Œä¸åŒçš„è§‚ç‚¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªçœŸå®çš„ç§‘å­¦å’Œæ”¿æ²»å£°æ˜ä¸­åº”ç”¨ ClaimSpectï¼Œå±•ç¤ºäº†å…¶åœ¨è§£æ„å¤æ‚å£°æ˜å’Œè¡¨ç¤ºè¯­æ–™åº“ä¸­è§‚ç‚¹çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10600', 'title': 'EmbodiedGen: Towards a Generative 3D World Engine for Embodied\n  Intelligence', 'url': 'https://huggingface.co/papers/2506.10600', 'abstract': 'EmbodiedGen is a platform that generates high-quality, photorealistic 3D assets at low cost, enabling scalable and realistic embodied AI research through generative AI techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.', 'score': 2, 'issue_id': 4285, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '34b11f20c290edff', 'authors': ['Wang Xinjie', 'Liu Liu', 'Cao Yu', 'Wu Ruiqi', 'Qin Wenkang', 'Wang Dehui', 'Sui Wei', 'Su Zhizhong'], 'affiliations': ['D-Robotics', 'GigaAI', 'Horizon Robotics', 'Shanghai Jiao Tong University', 'VCIP, CS, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10600.jpg', 'data': {'categories': ['#games', '#3d', '#agents', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ²', 'desc': 'EmbodiedGen - ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 3D, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ ÑÑ†ĞµĞ½. EmbodiedGen Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¸Ñ€Ñ‹, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Revolutionizing 3D Asset Generation for Embodied AI', 'desc': 'EmbodiedGen is a platform designed to create high-quality, photorealistic 3D assets efficiently, which is essential for training embodied AI systems. It addresses the limitations of traditional 3D graphics by providing a scalable and cost-effective solution for generating diverse 3D environments. The platform includes six modules that facilitate the generation of 3D objects and scenes, ensuring they have accurate physical properties for realistic simulations. By leveraging generative AI techniques, EmbodiedGen enhances the generalization and evaluation capabilities of embodied intelligence research.'}, 'zh': {'title': 'EmbodiedGenï¼šä½æˆæœ¬ç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'EmbodiedGenæ˜¯ä¸€ä¸ªç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„3Dèµ„äº§çš„å¹³å°ï¼Œæ—¨åœ¨é™ä½æˆæœ¬å¹¶ä¿ƒè¿›å¯æ‰©å±•çš„å…·èº«äººå·¥æ™ºèƒ½ç ”ç©¶ã€‚è¯¥å¹³å°é€šè¿‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œæ„å»ºç‰©ç†çœŸå®ä¸”å‡†ç¡®ç¼©æ”¾çš„3Dä¸–ç•Œï¼Œä»¥æ”¯æŒå…·èº«æ™ºèƒ½ä»»åŠ¡çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚EmbodiedGenæä¾›äº†å…­ä¸ªå…³é”®æ¨¡å—ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–å’Œäº’åŠ¨çš„3Dä¸–ç•Œï¼Œè§£å†³äº†ä¼ ç»Ÿ3Då›¾å½¢èµ„äº§çš„é«˜æˆæœ¬å’Œæœ‰é™çœŸå®æ„Ÿçš„é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨EmbodiedGenï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´é«˜æ•ˆåœ°ç”Ÿæˆæ‰€éœ€çš„3Dæ•°æ®èµ„äº§ï¼Œä»è€Œæ¨åŠ¨å…·èº«æ™ºèƒ½é¢†åŸŸçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10378', 'title': 'Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning', 'url': 'https://huggingface.co/papers/2506.10378', 'abstract': 'The study proposes a causal representation learning framework to evaluate language model capabilities through latent factors, emphasizing the importance of controlling for base model variations to uncover underlying causal relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities.', 'score': 2, 'issue_id': 4275, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '50915ea9038be86d', 'authors': ['Jikai Jin', 'Vasilis Syrgkanis', 'Sham Kakade', 'Hanlin Zhang'], 'affiliations': ['Harvard University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10378.jpg', 'data': {'categories': ['#science', '#dataset', '#interpretability', '#benchmark', '#reasoning', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼Ñƒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€ĞµÑ…ÑƒĞ·Ğ»Ğ¾Ğ²ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‰ÑƒÑ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‡ĞµÑ‚ĞºÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Uncovering Causal Relationships in Language Model Performance', 'desc': 'This paper introduces a causal representation learning framework designed to assess the capabilities of language models by examining latent factors. It highlights the necessity of controlling for variations in base models to accurately identify causal relationships. The authors analyze a dataset of over 1500 models across six benchmarks, revealing a three-node linear causal structure that explains performance differences. Their findings emphasize the importance of understanding the causal pathways from general problem-solving to specific abilities like instruction-following and mathematical reasoning.'}, 'zh': {'title': 'æ­ç¤ºè¯­è¨€æ¨¡å‹èƒ½åŠ›çš„å› æœå…³ç³»', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å› æœè¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé€šè¿‡æ½œåœ¨å› ç´ è¯„ä¼°è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼ºè°ƒæ§åˆ¶åŸºç¡€æ¨¡å‹å˜å¼‚çš„é‡è¦æ€§ï¼Œä»¥æ­ç¤ºæ½œåœ¨çš„å› æœå…³ç³»ã€‚é€šè¿‡å¯¹è¶…è¿‡1500ä¸ªæ¨¡å‹åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°è¿›è¡Œåˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä¸€ä¸ªç®€æ´çš„ä¸‰èŠ‚ç‚¹çº¿æ€§å› æœç»“æ„ï¼Œèƒ½å¤Ÿå¯é åœ°è§£é‡Šè§‚å¯Ÿåˆ°çš„æ€§èƒ½å˜åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ä»”ç»†æ§åˆ¶åŸºç¡€æ¨¡å‹çš„å˜å¼‚æ˜¯æ­ç¤ºæ½œåœ¨æ¨¡å‹èƒ½åŠ›ä¹‹é—´å› æœå…³ç³»çš„å…³é”®æ­¥éª¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06694', 'title': 'Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning', 'url': 'https://huggingface.co/papers/2506.06694', 'abstract': 'MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models.', 'score': 2, 'issue_id': 4275, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ½Ñ', 'en': 'June 7', 'zh': '6æœˆ7æ—¥'}, 'hash': 'f878d8e46d64a439', 'authors': ['Yuan Yuan', 'Yukun Liu', 'Chonghua Han', 'Jie Feng', 'Yong Li'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06694.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#data', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸš¶', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'MoveGCL - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. MoveGCL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MoveGCL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Mobility Models with Privacy-Preserving Learning', 'desc': 'MoveGCL is a framework designed to train mobility foundation models while ensuring data privacy. It uses generative continual learning to create synthetic data from a teacher model, allowing for model updates without sharing sensitive raw data. The framework employs a Mixture-of-Experts Transformer to adapt to various mobility patterns and includes strategies to prevent catastrophic forgetting during training. Experiments show that MoveGCL performs well compared to traditional methods while maintaining strong privacy protections.'}, 'zh': {'title': 'MoveGCLï¼šéšç§ä¿æŠ¤çš„ç§»åŠ¨åŸºç¡€æ¨¡å‹è®­ç»ƒæ¡†æ¶', 'desc': 'MoveGCLæ˜¯ä¸€ä¸ªä¿æŠ¤éšç§çš„æ¡†æ¶ï¼Œåˆ©ç”¨ç”ŸæˆæŒç»­å­¦ä¹ å’Œæ··åˆä¸“å®¶Transformeræ¥è®­ç»ƒç§»åŠ¨åŸºç¡€æ¨¡å‹ï¼Œè€Œæ— éœ€å…±äº«åŸå§‹æ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡æ”¾ä»å†»ç»“æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„åˆæˆè½¨è¿¹ï¼Œå®ç°å»ä¸­å¿ƒåŒ–å’Œæ¸è¿›å¼æ¨¡å‹æ¼”åŒ–ï¼Œå¹¶é€šè¿‡å®šåˆ¶çš„è’¸é¦ç­–ç•¥å¢å¼ºçŸ¥è¯†ä¿ç•™ï¼Œå‡å°‘ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†åº”å¯¹ç§»åŠ¨æ¨¡å¼çš„å¼‚è´¨æ€§ï¼ŒMoveGCLç»“åˆäº†ç§»åŠ¨æ„ŸçŸ¥çš„ä¸“å®¶è·¯ç”±æœºåˆ¶å’Œé€å±‚æ¸è¿›é€‚åº”ç­–ç•¥ï¼Œä»¥ç¨³å®šæŒç»­æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoveGCLåœ¨å…­ä¸ªçœŸå®åŸå¸‚æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸è”åˆè®­ç»ƒç›¸å½“ï¼Œæ˜¾è‘—ä¼˜äºè”é‚¦å­¦ä¹ åŸºçº¿ï¼ŒåŒæ—¶æä¾›å¼ºæœ‰åŠ›çš„éšç§ä¿æŠ¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06561', 'title': 'LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles', 'url': 'https://huggingface.co/papers/2506.06561', 'abstract': "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.", 'score': 1, 'issue_id': 4272, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '12942392f309be51', 'authors': ["Ho Yin 'Sam' Ng", 'Ting-Yao Hsu', 'Aashish Anantha Ramakrishnan', 'Branislav Kveton', 'Nedim Lipka', 'Franck Dernoncourt', 'Dongwon Lee', 'Tong Yu', 'Sungchul Kim', 'Ryan A. Rossi', "Ting-Hao 'Kenneth' Huang"], 'affiliations': ['Adobe Research', 'Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06561.jpg', 'data': {'categories': ['#optimization', '#dataset', '#multimodal', '#interpretability', '#games'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´', 'desc': 'LaMP-Cap Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğº Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹, Ñ‡ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ³Ñ€Ğ°Ñ„Ñ‹, ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Personalized Captions Through Multimodal Contexts', 'desc': "LaMP-Cap is a new dataset designed to enhance the generation of personalized figure captions by utilizing multimodal profiles. It provides not only the target figure images but also additional contextual figures and their associated captions and paragraphs. This approach allows AI models to create captions that better reflect the author's style and the specific domain. Experiments demonstrate that incorporating profile information, especially images, significantly improves the quality of AI-generated captions compared to traditional text-only methods."}, 'zh': {'title': 'ä¸ªæ€§åŒ–å›¾å½¢æ ‡é¢˜ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'LaMP-Capæ˜¯ä¸€ä¸ªç”¨äºä¸ªæ€§åŒ–å›¾å½¢æ ‡é¢˜ç”Ÿæˆçš„æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€èµ„æ–™æé«˜AIç”Ÿæˆæ ‡é¢˜çš„è´¨é‡ã€‚å›¾å½¢æ ‡é¢˜å¯¹äºå¸®åŠ©è¯»è€…ç†è§£å’Œè®°ä½å›¾å½¢çš„å…³é”®ä¿¡æ¯è‡³å…³é‡è¦ã€‚å°½ç®¡å·²æœ‰è®¸å¤šæ¨¡å‹å¯ä»¥ç”Ÿæˆè¿™äº›æ ‡é¢˜ï¼Œä½†ä½œè€…é€šå¸¸éœ€è¦ä¿®æ”¹é€šç”¨çš„AIç”Ÿæˆæ ‡é¢˜ä»¥åŒ¹é…ä»–ä»¬çš„å†™ä½œé£æ ¼ã€‚LaMP-Capæä¾›äº†å›¾åƒå’Œç›¸å…³å›¾å½¢çš„ä¸Šä¸‹æ–‡èµ„æ–™ï¼Œå®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¿™äº›å¤šæ¨¡æ€èµ„æ–™å¯ä»¥ç”Ÿæˆæ›´æ¥è¿‘ä½œè€…åŸå§‹å†™ä½œçš„æ ‡é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05982', 'title': 'MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks', 'url': 'https://huggingface.co/papers/2506.05982', 'abstract': 'MCA-Bench is a multimodal benchmark suite for CAPTCHA security evaluation which fine-tunes specialized cracking agents using a shared vision-language model.  \t\t\t\t\tAI-generated summary \t\t\t\t As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.', 'score': 1, 'issue_id': 4272, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '6cf7938ff751b2ff', 'authors': ['Zonglin Wu', 'Yule Xue', 'Xin Wei', 'Yiren Song'], 'affiliations': ['National University of Singapore', 'Southwest University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05982.jpg', 'data': {'categories': ['#agents', '#benchmark', '#security', '#open_source', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ CAPTCHA', 'desc': 'MCA-Bench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ CAPTCHA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ CAPTCHA Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. MCA-Bench Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ CAPTCHA, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MCA-Bench ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… CAPTCHA Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Strengthening CAPTCHA Security with MCA-Bench', 'desc': 'MCA-Bench is a new tool designed to evaluate the security of different types of CAPTCHAs against automated attacks. It combines various CAPTCHA formats, such as text, images, and interactive puzzles, into one comprehensive testing framework. By using a shared vision-language model, it fine-tunes specific agents to crack each type of CAPTCHA, allowing for consistent comparisons across different modalities. The results provide insights into how the complexity and interaction of CAPTCHAs affect their vulnerability, helping to improve their design and security.'}, 'zh': {'title': 'MCA-Benchï¼šCAPTCHAå®‰å…¨è¯„ä¼°çš„æ–°åŸºå‡†', 'desc': 'MCA-Benchæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°CAPTCHAçš„å®‰å…¨æ€§ã€‚å®ƒé€šè¿‡å…±äº«çš„è§†è§‰-è¯­è¨€æ¨¡å‹å¾®è°ƒä¸“é—¨çš„ç ´è§£ä»£ç†ï¼Œä»¥ä¾¿å¯¹ä¸åŒç±»å‹çš„CAPTCHAè¿›è¡Œä¸€è‡´çš„è¯„ä¼°ã€‚è¯¥ç ”ç©¶å¡«è¡¥äº†ç°æœ‰CAPTCHAè¯„ä¼°ä¸­ç¼ºä¹ç»Ÿä¸€å¤§è§„æ¨¡åŸºå‡†çš„ç©ºç™½ï¼Œæä¾›äº†å¯¹ç°ä»£CAPTCHAè®¾è®¡è„†å¼±æ€§çš„å®šé‡åˆ†æã€‚åŸºäºå®éªŒç»“æœï¼Œæå‡ºäº†ä¸‰ä¸ªå¯è¡Œçš„è®¾è®¡åŸåˆ™ï¼Œå¹¶è¯†åˆ«äº†å…³é”®çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºCAPTCHAçš„ç³»ç»Ÿæ€§å¼ºåŒ–å’Œå…¬å¹³åŸºå‡†æµ‹è¯•å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08862', 'title': 'StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams', 'url': 'https://huggingface.co/papers/2506.08862', 'abstract': 'StreamSplat, a fully feed-forward framework, addresses real-time 3D scene reconstruction from uncalibrated video with accurate dynamics and long-term stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.', 'score': 0, 'issue_id': 4279, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'fc6ac5b21c6b00ff', 'authors': ['Zike Wu', 'Qi Yan', 'Xuanyu Yi', 'Lele Wang', 'Renjie Liao'], 'affiliations': ['Canada CIFAR AI Chair', 'Nanyang Technological University', 'University of British Columbia', 'Vector Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.08862.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#video', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑÑ†ĞµĞ½Ğ°Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'StreamSplat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ½ĞµĞ¾Ñ‚ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D Gaussian Splatting. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ 3DGS Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸. StreamSplat Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹.'}, 'en': {'title': 'StreamSplat: Real-Time 3D Scene Reconstruction Made Easy!', 'desc': 'StreamSplat is a novel framework designed for real-time 3D scene reconstruction from uncalibrated video inputs. It effectively addresses the challenges of processing uncalibrated data, accurately modeling dynamic changes in scenes, and ensuring long-term stability. The framework utilizes a feed-forward approach, incorporating a probabilistic sampling mechanism for predicting 3D positions and a bidirectional deformation field for dynamic modeling. Experimental results show that StreamSplat outperforms existing methods in both reconstruction quality and the ability to handle long video streams.'}, 'zh': {'title': 'StreamSplatï¼šå®æ—¶åŠ¨æ€ä¸‰ç»´åœºæ™¯é‡å»ºçš„æ–°çªç ´', 'desc': 'StreamSplat æ˜¯ä¸€ä¸ªå®Œå…¨å‰é¦ˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»æœªæ ¡å‡†çš„è§†é¢‘ä¸­å®æ—¶é‡å»ºåŠ¨æ€ä¸‰ç»´åœºæ™¯ã€‚è¯¥æ–¹æ³•è§£å†³äº†å¤„ç†æœªæ ¡å‡†è¾“å…¥ã€å‡†ç¡®å»ºæ¨¡åŠ¨æ€åœºæ™¯æ¼”å˜ä»¥åŠä¿æŒé•¿æœŸç¨³å®šæ€§ç­‰ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥é™æ€ç¼–ç å™¨ä¸­çš„æ¦‚ç‡é‡‡æ ·æœºåˆ¶å’ŒåŠ¨æ€è§£ç å™¨ä¸­çš„åŒå‘å˜å½¢åœºï¼ŒStreamSplat å®ç°äº†é«˜æ•ˆçš„åŠ¨æ€å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamSplat åœ¨é‡å»ºè´¨é‡å’ŒåŠ¨æ€åœºæ™¯å»ºæ¨¡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ”¯æŒä»»æ„é•¿åº¦è§†é¢‘æµçš„åœ¨çº¿é‡å»ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06395', 'title': 'Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models', 'url': 'https://huggingface.co/papers/2506.06395', 'abstract': "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering. Applied to Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps, RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a simple, scalable post-training method for inference models, requiring only a small number of samples and unlabelled supervision.", 'score': 70, 'issue_id': 4258, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '0bd4f12f6c85c81f', 'authors': ['Pengyi Li', 'Matvey Skripkin', 'Alexander Zubrey', 'Andrey Kuznetsov', 'Ivan Oseledets'], 'affiliations': ['AIRI, Skoltech Moscow'], 'pdf_title_img': 'assets/pdf/title_img/2506.06395.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning', '#optimization', '#inference', '#training', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ Reinforcement Learning via Self-Confidence (RLSC) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. RLSC ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ RLSC Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-Math-7B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°.'}, 'en': {'title': 'Boosting Model Accuracy with Self-Confidence Rewards', 'desc': "Reinforcement Learning via Self-Confidence (RLSC) is a novel approach that enhances the accuracy of large language models (LLMs) by utilizing the model's own confidence as a reward signal. This method eliminates the reliance on human labels or complex reward engineering, making it more efficient and scalable. RLSC has been tested on the Qwen2.5-Math-7B model, showing significant accuracy improvements across various math benchmarks with minimal training data. By requiring only a few samples and no labeled data, RLSC offers a straightforward solution for post-training alignment of LLMs with task objectives."}, 'zh': {'title': 'è‡ªä¿¡é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡å‹å‡†ç¡®æ€§ï¼', 'desc': 'å¼ºåŒ–å­¦ä¹ é€šè¿‡è‡ªä¿¡ï¼ˆRLSCï¼‰æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„è‡ªä¿¡åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•ä¸å†ä¾èµ–äºäººå·¥æ ‡ç­¾æˆ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚RLSCåœ¨å¤šä¸ªæ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ³•ç®€å•ä¸”å¯æ‰©å±•ï¼Œåªéœ€å°‘é‡æ ·æœ¬å’Œæ— æ ‡ç­¾çš„ç›‘ç£å³å¯å®ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09113', 'title': 'Seedance 1.0: Exploring the Boundaries of Video Generation Models', 'url': 'https://huggingface.co/papers/2506.09113', 'abstract': 'Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.', 'score': 47, 'issue_id': 4251, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'd44df125bd718f2f', 'authors': ['Yu Gao', 'Haoyuan Guo', 'Tuyen Hoang', 'Weilin Huang', 'Lu Jiang', 'Fangyuan Kong', 'Huixia Li', 'Jiashi Li', 'Liang Li', 'Xiaojie Li', 'Xunsong Li', 'Yifu Li', 'Shanchuan Lin', 'Zhijie Lin', 'Jiawei Liu', 'Shu Liu', 'Xiaonan Nie', 'Zhiwu Qing', 'Yuxi Ren', 'Li Sun', 'Zhi Tian', 'Rui Wang', 'Sen Wang', 'Guoqiang Wei', 'Guohong Wu', 'Jie Wu', 'Ruiqi Xia', 'Fei Xiao', 'Xuefeng Xiao', 'Jiangqiao Yan', 'Ceyuan Yang', 'Jianchao Yang', 'Runkai Yang', 'Tao Yang', 'Yihang Yang', 'Zilyu Ye', 'Xuejiao Zeng', 'Yan Zeng', 'Heng Zhang', 'Yang Zhao', 'Xiaozheng Zheng', 'Peihao Zhu', 'Jiaxin Zou', 'Feilong Zuo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.09113.jpg', 'data': {'categories': ['#video', '#dataset', '#optimization', '#architecture', '#benchmark', '#data', '#training', '#rlhf', '#diffusion', '#inference'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾', 'desc': 'Seedance 1.0 - ÑÑ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²ÑƒÑ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğµ RLHF. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼, Seedance 1.0 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ~10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Seedance 1.0: Fast and High-Quality Video Generation Revolutionized', 'desc': 'Seedance 1.0 is a cutting-edge video generation model that enhances performance through advanced data curation and an efficient architecture. It addresses key challenges in video generation, such as prompt adherence and visual quality, by integrating multi-source data and a novel training paradigm. The model employs optimized post-training techniques, including fine-tuning and reinforcement learning with multi-dimensional rewards, to boost its capabilities. With a remarkable inference speedup of approximately 10 times, Seedance 1.0 can produce high-quality 5-second videos at 1080p resolution in just 41.4 seconds.'}, 'zh': {'title': 'Seedance 1.0ï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ ‡æ†', 'desc': 'Seedance 1.0 æ˜¯ä¸€ç§é«˜æ€§èƒ½çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆäº†å…ˆè¿›çš„æ•°æ®æ•´ç†ã€æœ‰æ•ˆçš„æ¶æ„è®¾è®¡ã€åè®­ç»ƒä¼˜åŒ–å’Œæ¨¡å‹åŠ é€ŸæŠ€æœ¯ï¼Œæä¾›äº†å“è¶Šçš„è´¨é‡å’Œé€Ÿåº¦ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šæºæ•°æ®æ•´ç†å’Œç²¾å‡†çš„è§†é¢‘å­—å¹•ï¼Œå¢å¼ºäº†å¯¹å¤šæ ·åœºæ™¯çš„å…¨é¢å­¦ä¹ èƒ½åŠ›ã€‚å®ƒçš„é«˜æ•ˆæ¶æ„æ”¯æŒå¤šé•œå¤´ç”Ÿæˆï¼Œå¹¶åŒæ—¶å­¦ä¹ æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘çš„ä»»åŠ¡ã€‚Seedance 1.0 é€šè¿‡å¤šé˜¶æ®µè’¸é¦ç­–ç•¥å®ç°äº†çº¦10å€çš„æ¨ç†åŠ é€Ÿï¼Œèƒ½å¤Ÿåœ¨41.4ç§’å†…ç”Ÿæˆ5ç§’çš„1080pè§†é¢‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09350', 'title': 'Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.09350', 'abstract': 'Autoregressive adversarial post-training transforms pre-trained latent video diffusion models into real-time, interactive video generators with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2', 'score': 35, 'issue_id': 4256, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'f2ebb5225d0061fa', 'authors': ['Shanchuan Lin', 'Ceyuan Yang', 'Hao He', 'Jianwen Jiang', 'Yuxi Ren', 'Xin Xia', 'Yang Zhao', 'Xuefeng Xiao', 'Lu Jiang'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2506.09350.jpg', 'data': {'categories': ['#diffusion', '#training', '#optimization', '#video', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AAPT', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµgressivĞ½Ğ¾Ğ³Ğ¾ adversarial post-training (AAPT) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµgressĞ¸Ğ²Ğ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ´Ğ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ (1NFE) Ğ½Ğ° ĞºĞ°Ğ´Ñ€. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 8 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ 24 ĞºĞ°Ğ´Ñ€Ğ° Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 1280x720 Ğ½Ğ° 8 GPU H100.'}, 'en': {'title': 'Real-Time Video Generation Made Easy!', 'desc': 'This paper introduces a method called autoregressive adversarial post-training (AAPT) to enhance pre-trained latent video diffusion models for real-time video generation. The AAPT approach allows the model to generate video frames one at a time, using a single neural function evaluation, which significantly reduces computational demands. By incorporating adversarial training, the model improves its efficiency and reduces errors during the generation of longer videos. The results show that the model can produce high-quality video at 24 frames per second, making it suitable for interactive applications.'}, 'zh': {'title': 'å®æ—¶äº¤äº’è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªå›å½’å¯¹æŠ—åè®­ç»ƒï¼ˆAAPTï¼‰æ–¹æ³•ï¼Œå°†é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹è½¬å˜ä¸ºå®æ—¶äº¤äº’å¼è§†é¢‘ç”Ÿæˆå™¨ã€‚è¯¥æ¨¡å‹é€šè¿‡è‡ªå›å½’æ–¹å¼ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªæ½œåœ¨å¸§ï¼Œä½¿ç”¨å•æ¬¡ç¥ç»ç½‘ç»œå‡½æ•°è¯„ä¼°ï¼ˆ1NFEï¼‰ï¼Œå®ç°å®æ—¶æµå¼ä¼ è¾“ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¯¹æŠ—è®­ç»ƒä½œä¸ºè‡ªå›å½’ç”Ÿæˆçš„æœ‰æ•ˆèŒƒå¼ï¼Œè®¾è®¡å‡ºæ›´é«˜æ•ˆçš„æ¶æ„ï¼Œå‡å°‘é•¿è§†é¢‘ç”Ÿæˆä¸­çš„è¯¯å·®ç§¯ç´¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„8Bæ¨¡å‹åœ¨å•ä¸ªH100ä¸Šå®ç°äº†736x416åˆ†è¾¨ç‡çš„å®æ—¶24fpsè§†é¢‘ç”Ÿæˆï¼Œæˆ–åœ¨8ä¸ªH100ä¸Šç”Ÿæˆ1280x720åˆ†è¾¨ç‡çš„è§†é¢‘ï¼Œæœ€é•¿å¯è¾¾ä¸€åˆ†é’Ÿï¼ˆ1440å¸§ï¼‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09790', 'title': 'ComfyUI-R1: Exploring Reasoning Models for Workflow Generation', 'url': 'https://huggingface.co/papers/2506.09790', 'abstract': 'ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation.', 'score': 28, 'issue_id': 4251, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '6fb3fee31c3739a5', 'authors': ['Zhenran Xu', 'Yiyu Wang', 'Xue Yang', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Digital Commerce, China', 'Harbin Institute of Technology (Shenzhen), China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09790.jpg', 'data': {'categories': ['#rl', '#dataset', '#optimization', '#architecture', '#reasoning', '#training', '#long_context'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ComfyUI-R1: Ğ˜Ğ˜-Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'ComfyUI-R1 - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ² ÑÑ„ĞµÑ€Ğµ Ğ˜Ğ˜-Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ComfyUI-R1 Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 4000 Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼.'}, 'en': {'title': 'Automating AI Art Workflows with ComfyUI-R1', 'desc': 'ComfyUI-R1 is a large reasoning model designed to automate the generation of workflows for creating AI art. It utilizes long chain-of-thought (CoT) reasoning and reinforcement learning to improve the process of workflow creation, making it easier for users to customize their creative pipelines. The model is trained on a dataset of 4,000 workflows and employs a two-stage framework that includes CoT fine-tuning and reinforcement learning with a hybrid reward system. Experimental results show that ComfyUI-R1 outperforms existing models in terms of format validity and overall workflow quality, highlighting the effectiveness of its reasoning capabilities.'}, 'zh': {'title': 'ComfyUI-R1ï¼šè‡ªåŠ¨åŒ–å·¥ä½œæµç”Ÿæˆçš„æ¨ç†æ¨¡å‹', 'desc': 'ComfyUI-R1 æ˜¯ä¸€ä¸ªå¤§å‹æ¨ç†æ¨¡å‹ï¼Œä¸“æ³¨äºè‡ªåŠ¨åŒ–å·¥ä½œæµç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åœ¨ AI è‰ºæœ¯åˆ›ä½œä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒé€šè¿‡é•¿é“¾æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼Œå¸®åŠ©ç”¨æˆ·åˆ›å»ºå®šåˆ¶åŒ–çš„å·¥ä½œæµï¼Œé™ä½äº†å­¦ä¹ æ›²çº¿ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº† 4K å·¥ä½œæµçš„æ•°æ®é›†ï¼Œç»è¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒï¼Œç¡®ä¿äº†å·¥ä½œæµçš„æ ¼å¼æœ‰æ•ˆæ€§å’Œç»“æ„å®Œæ•´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒComfyUI-R1 åœ¨æ ¼å¼æœ‰æ•ˆæ€§å’Œ F1 åˆ†æ•°ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†é•¿é“¾æ¨ç†åœ¨ AI è‰ºæœ¯åˆ›ä½œä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09991', 'title': 'Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation', 'url': 'https://huggingface.co/papers/2506.09991', 'abstract': 'Multiverse, a parallel generative model incorporating a MapReduce paradigm, achieves performance comparable to autoregressive LLMs while offering superior scaling and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit parallelism in sequential generation. Inspired by this, we introduce Multiverse, a new generative model that enables natively parallel generation. Multiverse internalizes a MapReduce paradigm, generating automatically through three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process stage for parallel subtask execution, and (iii) a Reduce stage for lossless result synthesis. Next, we build a real-world Multiverse reasoning model with co-design of data, algorithm, and system, enabling rapid and seamless transfer from frontier AR-LLMs. Starting from sequential reasoning chains, we create Multiverse 1K by converting them into structured training data using an automated LLM-assisted pipeline, avoiding costly human annotations. Algorithmically, we design Multiverse Attention to separate parallel reasoning steps while keeping compatibility with causal attention for efficient training. Systematically, we implement Multiverse Engine to enable parallel inference. It features a dedicated scheduler that dynamically switches between sequential and parallel generation, triggered directly by the model. After a 3-hour fine-tuning with 1K examples, our Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs of the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Moreover, our budget control experiments show that Multiverse-32B exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length. Such scaling further leads to practical efficiency gain, achieving up to 2x speedup across varying batch sizes. We have open-sourced the entire Multiverse ecosystem, including data, model weights, engine, supporting tools, as well as complete data curation prompts and detailed training and evaluation recipes.', 'score': 26, 'issue_id': 4261, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '3ffc5ce1d43056b5', 'authors': ['Xinyu Yang', 'Yuwei An', 'Hongyi Liu', 'Tianqi Chen', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2506.09991.jpg', 'data': {'categories': ['#data', '#architecture', '#optimization', '#training', '#dataset', '#open_source'], 'emoji': 'ğŸŒŒ', 'ru': {'title': 'Multiverse: ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Multiverse - Ğ½Ğ¾Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ MapReduce Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Map Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Process Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Reduce Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Multiverse-32B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ğ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ. Ğ’ÑÑ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Multiverse, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ.'}, 'en': {'title': 'Multiverse: Parallel Power in Generative Modeling', 'desc': 'The paper introduces Multiverse, a novel generative model that leverages a MapReduce paradigm to achieve efficient parallel generation, competing with autoregressive large language models (AR-LLMs). It operates through three stages: Map for task decomposition, Process for executing subtasks in parallel, and Reduce for synthesizing results without loss. The model incorporates Multiverse Attention to maintain compatibility with causal attention while allowing for parallel reasoning steps. After fine-tuning, Multiverse-32B demonstrates comparable performance to leading AR-LLMs, with significant improvements in scaling and speed, making it a promising alternative in the field of generative modeling.'}, 'zh': {'title': 'Multiverseï¼šå¹¶è¡Œç”Ÿæˆçš„æœªæ¥', 'desc': 'Multiverseæ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨MapReduceèŒƒå¼ï¼Œå®ç°äº†åŸç”Ÿçš„å¹¶è¡Œç”Ÿæˆã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œè‡ªåŠ¨ç”Ÿæˆï¼šMapé˜¶æ®µç”¨äºè‡ªé€‚åº”ä»»åŠ¡åˆ†è§£ï¼ŒProcessé˜¶æ®µç”¨äºå¹¶è¡Œå­ä»»åŠ¡æ‰§è¡Œï¼ŒReduceé˜¶æ®µç”¨äºæ— æŸç»“æœåˆæˆã€‚æˆ‘ä»¬è®¾è®¡äº†Multiverse Attentionï¼Œä»¥åˆ†ç¦»å¹¶è¡Œæ¨ç†æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒä¸å› æœæ³¨æ„åŠ›çš„å…¼å®¹æ€§ï¼Œä»è€Œå®ç°é«˜æ•ˆè®­ç»ƒã€‚ç»è¿‡3å°æ—¶çš„å¾®è°ƒï¼ŒMultiverse-32Båœ¨æ€§èƒ½ä¸Šä¸åŒè§„æ¨¡çš„é¢†å…ˆè‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ç›¸å½“ï¼Œå¹¶ä¸”åœ¨é¢„ç®—æ§åˆ¶å®éªŒä¸­æ˜¾ç¤ºå‡ºæ›´ä¼˜çš„æ‰©å±•æ€§å’Œé€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09995', 'title': 'PlayerOne: Egocentric World Simulator', 'url': 'https://huggingface.co/papers/2506.09995', 'abstract': 'PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.', 'score': 24, 'issue_id': 4251, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'e7cba5eb3e340a0f', 'authors': ['Yuanpeng Tu', 'Hao Luo', 'Xi Chen', 'Xiang Bai', 'Fan Wang', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group', 'HKU', 'HUST', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.09995.jpg', 'data': {'categories': ['#video', '#optimization', '#multimodal', '#training', '#games'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸĞ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ñ PlayerOne', 'desc': 'PlayerOne - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. PlayerOne Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Egocentric World Simulation with PlayerOne', 'desc': 'PlayerOne is an innovative egocentric realistic world simulator that generates videos based on user-captured images. It employs a coarse-to-fine training approach, initially pretraining on large datasets of egocentric text-video pairs, followed by fine-tuning with motion-video data for enhanced accuracy. The system features a part-disentangled motion injection scheme, allowing for detailed control over individual movements, and a joint reconstruction framework that maintains scene consistency across video frames. This pioneering work opens new avenues for world modeling and applications in immersive environments.'}, 'zh': {'title': 'å¼€åˆ›è‡ªæˆ‘ä¸­å¿ƒç°å®ä¸–ç•Œæ¨¡æ‹Ÿçš„æ–°çºªå…ƒ', 'desc': 'PlayerOneæ˜¯ä¸€ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç°å®ä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æ•æ‰çš„å›¾åƒæ„å»ºå’Œç”Ÿæˆè§†é¢‘ã€‚å®ƒé‡‡ç”¨ç²—åˆ°ç»†çš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆåœ¨å¤§è§„æ¨¡çš„è‡ªæˆ‘ä¸­å¿ƒæ–‡æœ¬-è§†é¢‘å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨åŒæ­¥è¿åŠ¨-è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥ç³»ç»Ÿè®¾è®¡äº†éƒ¨åˆ†è§£è€¦çš„è¿åŠ¨æ³¨å…¥æ–¹æ¡ˆï¼Œä»¥å®ç°å¯¹éƒ¨åˆ†è¿åŠ¨çš„ç²¾ç¡®æ§åˆ¶ï¼Œå¹¶é€šè¿‡è”åˆé‡å»ºæ¡†æ¶ç¡®ä¿é•¿è§†é¢‘ç”Ÿæˆä¸­çš„åœºæ™¯ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlayerOneåœ¨æ§åˆ¶äººç±»è¿åŠ¨å’Œå»ºæ¨¡å¤šæ ·åœºæ™¯æ–¹é¢å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08570', 'title': 'Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation', 'url': 'https://huggingface.co/papers/2506.08570', 'abstract': 'A systematic comparison of Auto-Regressive decoding and Conditional Flow-Matching in text-to-music generation highlights distinct strengths and limitations of each modeling paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly across many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and pinpoint which design choices most influence performance. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: Auto-Regressive decoding and Conditional Flow-Matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM', 'score': 21, 'issue_id': 4260, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '1ad6cb6752b933a6', 'authors': ['Or Tal', 'Felix Kreuk', 'Yossi Adi'], 'affiliations': ['mail.huji.ac.il', 'meta.com'], 'pdf_title_img': 'assets/pdf/title_img/2506.08570.jpg', 'data': {'categories': ['#games', '#synthetic', '#architecture', '#training', '#audio'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼: ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ: Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ°ÑÑŒ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼.'}, 'en': {'title': 'Decoding the Future of Music Generation: A Paradigm Comparison', 'desc': 'This paper systematically compares two popular modeling paradigms in text-to-music generation: Auto-Regressive decoding and Conditional Flow-Matching. The authors focus on how these paradigms affect performance by conducting controlled experiments with identical datasets and training setups. They evaluate the models on various criteria, including generation quality, robustness, and editing capabilities. The findings reveal the unique strengths and weaknesses of each approach, providing valuable insights for future developments in music generation systems.'}, 'zh': {'title': 'æ¯”è¾ƒè‡ªå›å½’ä¸æ¡ä»¶æµåŒ¹é…ï¼šæ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆçš„æœªæ¥', 'desc': 'æœ¬æ–‡ç³»ç»Ÿæ¯”è¾ƒäº†è‡ªå›å½’è§£ç å’Œæ¡ä»¶æµåŒ¹é…åœ¨æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ­ç¤ºäº†å„è‡ªçš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚ç ”ç©¶é›†ä¸­åœ¨å»ºæ¨¡èŒƒå¼ä¸Šï¼Œé€šè¿‡ç›¸åŒçš„æ•°æ®é›†å’Œè®­ç»ƒé…ç½®å¯¹æ¨¡å‹è¿›è¡Œæ§åˆ¶æ¯”è¾ƒã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ç”Ÿæˆè´¨é‡ã€å¯¹æ¨ç†é…ç½®çš„é²æ£’æ€§ã€å¯æ‰©å±•æ€§ä»¥åŠå¯¹æ–‡æœ¬å’Œæ—¶é—´å¯¹é½æ¡ä»¶çš„éµå¾ªèƒ½åŠ›ã€‚æ­¤ç ”ç©¶ä¸ºæœªæ¥æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆç³»ç»Ÿçš„æ¶æ„å’Œè®­ç»ƒå†³ç­–æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08889', 'title': 'SeerAttention-R: Sparse Attention Adaptation for Long Reasoning', 'url': 'https://huggingface.co/papers/2506.08889', 'abstract': 'SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.', 'score': 18, 'issue_id': 4251, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'a6e46d58b91f0fad', 'authors': ['Yizhao Gao', 'Shuming Guo', 'Shijie Cao', 'Yuqing Xia', 'Yu Cheng', 'Lei Wang', 'Lingxiao Ma', 'Yutao Sun', 'Tianzhu Ye', 'Li Dong', 'Hayden Kwok-Hay So', 'Yu Hua', 'Ting Cao', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Huazhong University of Science and Technology', 'Microsoft Research', 'Peking University', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08889.jpg', 'data': {'categories': ['#optimization', '#architecture', '#benchmark', '#reasoning', '#training', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'SeerAttention-R - ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ´ĞµÑ€ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ²ÑĞµĞ³Ğ¾ 0,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ² 4000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ‚ĞµÑÑ‚Ğµ AIME Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ TileLang, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 9 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FlashAttention-3 Ğ½Ğ° GPU H100 Ğ¿Ñ€Ğ¸ 90% Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. SeerAttention-R Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'SeerAttention-R: Speed and Accuracy in Sparse Attention for Reasoning Models', 'desc': 'SeerAttention-R is a new framework designed for sparse attention in reasoning models, focusing on efficient long decoding. It builds on the original SeerAttention by using a self-distilled gating mechanism to learn attention sparsity while eliminating query pooling for better auto-regressive decoding. This framework is lightweight and can be easily integrated into existing pretrained models without altering their parameters. Our experiments show that SeerAttention-R achieves high accuracy with significant speed improvements, processing up to 4K tokens efficiently on advanced hardware.'}, 'zh': {'title': 'SeerAttention-Rï¼šé«˜æ•ˆç¨€ç–æ³¨æ„åŠ›æ¨ç†æ¡†æ¶', 'desc': 'SeerAttention-Ræ˜¯ä¸€ç§ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶ï¼Œä¸“ä¸ºæ¨ç†æ¨¡å‹çš„é•¿è§£ç è€Œè®¾è®¡ã€‚å®ƒé€šè¿‡è‡ªè’¸é¦é—¨æ§æœºåˆ¶å­¦ä¹ æ³¨æ„åŠ›ç¨€ç–æ€§ï¼ŒåŒæ—¶å»é™¤äº†æŸ¥è¯¢æ± åŒ–ï¼Œä»¥é€‚åº”è‡ªå›å½’è§£ç ã€‚è¯¥æ¡†æ¶è½»é‡ä¸”çµæ´»ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè€Œæ— éœ€ä¿®æ”¹åŸå§‹å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒSeerAttention-Råœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­ä»¥4Kä»¤ç‰Œé¢„ç®—ä¿æŒæ¥è¿‘æ— æŸçš„æ¨ç†å‡†ç¡®æ€§ï¼Œå¹¶åœ¨H100 GPUä¸Šå®ç°äº†é«˜è¾¾9å€çš„é€Ÿåº¦æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09003', 'title': 'SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner', 'url': 'https://huggingface.co/papers/2506.09003', 'abstract': 'A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow).', 'score': 14, 'issue_id': 4251, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '794ea1a282cfa727', 'authors': ['Lei Zhang', 'Jiaxi Yang', 'Min Yang', 'Jian Yang', 'Mouxiang Chen', 'Jiajun Zhang', 'Zeyu Cui', 'Binyuan Hui', 'Junyang Lin'], 'affiliations': ['Alibaba Group, Beijing, China', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China', 'University of Science and Technology of China, Hefei, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09003.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#data', '#training', '#synthetic'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'SWE-Flow: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ TDD Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'SWE-Flow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (TDD). ĞĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (RDG). SWE-Flow Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ Ğ±Ğ°Ğ·Ñƒ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² TDD-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Automating TDD with SWE-Flow: Smarter Development Schedules', 'desc': 'SWE-Flow is a new framework designed to enhance Test-Driven Development (TDD) by automatically generating development schedules from unit tests. It infers development steps directly from these tests, which represent high-level requirements, rather than relying on human-submitted issues. The framework constructs a Runtime Dependency Graph (RDG) to capture function interactions, allowing for a structured approach to coding tasks. By generating a large dataset from real-world projects, SWE-Flow significantly improves the performance of models fine-tuned for TDD-based coding.'}, 'zh': {'title': 'SWE-Flowï¼šè‡ªåŠ¨åŒ–æµ‹è¯•é©±åŠ¨å¼€å‘çš„åˆ›æ–°æ¡†æ¶', 'desc': 'SWE-Flowæ˜¯ä¸€ç§æ–°é¢–çš„æ•°æ®åˆæˆæ¡†æ¶ï¼ŒåŸºäºæµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰æ–¹æ³•ã€‚å®ƒé€šè¿‡è‡ªåŠ¨æ¨æ–­å•å…ƒæµ‹è¯•ä¸­çš„å¼€å‘æ­¥éª¤ï¼Œç”Ÿæˆç»“æ„åŒ–çš„å¼€å‘è®¡åˆ’ï¼Œä»è€Œæé«˜äº†åœ¨çœŸå®é¡¹ç›®ä¸Šå¾®è°ƒå¼€æ”¾æ¨¡å‹çš„æ€§èƒ½ã€‚SWE-Flowçš„æ ¸å¿ƒæ˜¯æ„å»ºè¿è¡Œæ—¶ä¾èµ–å›¾ï¼ˆRDGï¼‰ï¼Œå‡†ç¡®æ•æ‰å‡½æ•°ä¹‹é—´çš„äº¤äº’ï¼Œç¡®ä¿æ¯ä¸€æ­¥ç”Ÿæˆéƒ¨åˆ†ä»£ç åº“åŠç›¸åº”çš„å•å…ƒæµ‹è¯•ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ä»çœŸå®çš„GitHubé¡¹ç›®ä¸­ç”Ÿæˆäº†å¤§é‡çš„è®­ç»ƒå’Œæµ‹è¯•å®ä¾‹ï¼Œæ˜¾è‘—æå‡äº†åŸºäºTDDçš„ç¼–ç æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09984', 'title': 'InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions', 'url': 'https://huggingface.co/papers/2506.09984', 'abstract': "A novel framework for end-to-end human animation with multi-modal conditions enables high-quality video generation through explicit layout control and region-specific modality matching.  \t\t\t\t\tAI-generated summary \t\t\t\t End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate a single subject and inject conditions in a global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce a novel framework that enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging a mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in a iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods.", 'score': 11, 'issue_id': 4252, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '97a59a9be9ba0dc3', 'authors': ['Zhenzhi Wang', 'Jiaqi Yang', 'Jianwen Jiang', 'Chao Liang', 'Gaojie Lin', 'Zerong Zheng', 'Ceyuan Yang', 'Dahua Lin'], 'affiliations': ['ByteDance', 'CUHK MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2506.09984.jpg', 'data': {'categories': ['#multimodal', '#video', '#games', '#diffusion'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ»ÑĞ´ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Human Animation with Multi-Modal Control', 'desc': "This paper presents a new framework for creating human animations that can incorporate multiple types of input, such as text, images, and audio. Unlike previous methods that only animate one subject at a time, this approach allows for complex interactions between multiple characters and objects in a video. The framework uses a mask predictor to accurately match visual elements to specific regions in the video, ensuring that each character's appearance is correctly represented. Additionally, it integrates audio cues in a way that aligns with the visual layout, resulting in high-quality, controllable animations that reflect rich human interactions."}, 'zh': {'title': 'å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„é«˜è´¨é‡äººç±»åŠ¨ç”»ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯äººç±»åŠ¨ç”»æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶æ”¯æŒå¤šä¸ªæ¦‚å¿µçš„ç²¾ç¡®æ§åˆ¶ï¼Œå…è®¸äººç±»ä¸ç‰©ä½“ä¹‹é—´çš„ä¸°å¯Œäº¤äº’ã€‚é€šè¿‡åŒºåŸŸç‰¹å®šçš„æ¡ä»¶ç»‘å®šï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨æ¨æ–­å¸ƒå±€ä¿¡æ¯ï¼Œå¹¶ç¡®ä¿ä¸åŒæ¨¡æ€ä¹‹é—´çš„åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„æ˜¾å¼å¸ƒå±€æ§åˆ¶ä¼˜äºç°æœ‰çš„éšå¼æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09501', 'title': 'Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.09501', 'abstract': 'The study investigates reproducibility issues in Large Language Models (LLMs) arising from hardware and precision variations, proposing a lightweight inference pipeline to enhance numerical stability while maintaining memory efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision -- while critical for reproducibility -- is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.', 'score': 9, 'issue_id': 4264, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'b14433f51b294499', 'authors': ['Jiayi Yuan', 'Hao Li', 'Xinheng Ding', 'Wenya Xie', 'Yu-Jhe Li', 'Wentian Zhao', 'Kun Wan', 'Jing Shi', 'Xia Hu', 'Zirui Liu'], 'affiliations': ['Adobe Inc.', 'Rice University', 'University of Minnesota Twin Cities'], 'pdf_title_img': 'assets/pdf/title_img/2506.09501.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#inference', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸Ğ·-Ğ·Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¾Ğ¹ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°ĞºĞµÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ²ĞµÑ€ÑĞ¸Ñ GPU, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ° Ğ½ĞµĞ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LayerCast, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ Ğ²ĞµÑĞ° Ğ² 16-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²ÑĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² FP32, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Enhancing Reproducibility in LLMs with LayerCast', 'desc': 'This paper explores the challenges of reproducibility in Large Language Models (LLMs) caused by variations in hardware and numerical precision. It highlights how changes in system configurations, such as GPU type and batch size, can lead to significant differences in model outputs, particularly in reasoning tasks. The authors identify that the non-associative nature of floating-point arithmetic contributes to this variability, which can affect accuracy and response length. To address these issues, they propose a new inference pipeline called LayerCast, which optimizes memory usage while ensuring numerical stability by using mixed precision during computations.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é‡å¤æ€§ä¸ç¨³å®šæ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¡¬ä»¶å’Œç²¾åº¦å˜åŒ–ä¸‹çš„å¯é‡å¤æ€§é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§è½»é‡çº§æ¨ç†ç®¡é“ï¼Œä»¥æé«˜æ•°å€¼ç¨³å®šæ€§ï¼ŒåŒæ—¶ä¿æŒå†…å­˜æ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°ï¼ŒLLMæ€§èƒ½çš„å¯é‡å¤æ€§éå¸¸è„†å¼±ï¼Œç³»ç»Ÿé…ç½®çš„å˜åŒ–ï¼ˆå¦‚è¯„ä¼°æ‰¹é‡å¤§å°ã€GPUæ•°é‡å’Œç‰ˆæœ¬ï¼‰ä¼šæ˜¾è‘—å½±å“ç”Ÿæˆçš„å“åº”ã€‚å°¤å…¶æ˜¯åœ¨æ¨ç†æ¨¡å‹ä¸­ï¼Œæ—©æœŸæ ‡è®°çš„å¾®å°èˆå…¥å·®å¼‚å¯èƒ½å¯¼è‡´æ€ç»´é“¾çš„åˆ†æ­§ï¼Œä»è€Œå½±å“å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†LayerCastæ¨ç†ç®¡é“ï¼Œä½¿ç”¨16ä½ç²¾åº¦å­˜å‚¨æƒé‡ï¼Œä½†åœ¨è®¡ç®—æ—¶ä½¿ç”¨FP32ï¼Œä»¥å¹³è¡¡å†…å­˜æ•ˆç‡å’Œæ•°å€¼ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05309', 'title': 'Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games', 'url': 'https://huggingface.co/papers/2506.05309', 'abstract': "An adaptive asynchronous LLM-agent performs similarly to human players in online Mafia games, demonstrating the potential for integrating LLMs into realistic group settings with complex social dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are inherently asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns; therefore, the decision of when to speak forms a crucial part of the participant's decision making. In this work, we develop an adaptive asynchronous LLM-agent which, in addition to determining what to say, also decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, including both human participants, as well as our asynchronous agent. Overall, our agent performs on par with human players, both in game performance, as well as in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We release all our data and code to support and encourage further research for more realistic asynchronous communication between LLM agents. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.", 'score': 9, 'issue_id': 4260, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '474c2c6a688311bf', 'authors': ['Niv Eckhaus', 'Uri Berger', 'Gabriel Stanovsky'], 'affiliations': ['School of Computer Science and Engineering, The Hebrew University of Jerusalem', 'School of Computing and Information Systems, University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2506.05309.jpg', 'data': {'categories': ['#games', '#agents', '#open_source', '#multimodal', '#dataset'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ›Ğ›Ğœ-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² ĞœĞ°Ñ„Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ›Ğ›Ğœ-Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ² ĞœĞ°Ñ„Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾ Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ ĞºĞ¾Ğ³Ğ´Ğ° ÑÑ‚Ğ¾ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ² ĞºĞ°Ğº Ğ¿Ğ¾ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑ Ğ² ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ². Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Bridging AI and Human Interaction in Asynchronous Settings', 'desc': 'This paper presents an adaptive asynchronous LLM-agent designed to participate in online Mafia games, showcasing its ability to mimic human players in complex social interactions. Unlike traditional LLMs that operate in synchronous settings, this agent can decide both what to say and when to say it, reflecting the nuances of real-world communication. The evaluation reveals that the agent performs comparably to human participants, effectively blending into the social dynamics of the game. The findings highlight the potential for LLMs to be integrated into various asynchronous environments, enhancing collaborative efforts in educational and professional contexts.'}, 'zh': {'title': 'è‡ªé€‚åº”å¼‚æ­¥ä»£ç†ï¼šè®©AIèå…¥äººç±»ç¤¾äº¤æ¸¸æˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§è‡ªé€‚åº”çš„å¼‚æ­¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ï¼Œå®ƒåœ¨åœ¨çº¿ Mafia æ¸¸æˆä¸­è¡¨ç°å‡ºä¸äººç±»ç©å®¶ç›¸ä¼¼çš„èƒ½åŠ›ã€‚è¿™ç§ä»£ç†ä¸ä»…å†³å®šè¯´ä»€ä¹ˆï¼Œè¿˜å†³å®šä½•æ—¶è¯´ï¼Œè¿™åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„ç¤¾äº¤åœºåˆä¸­è‡³å…³é‡è¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ä»£ç†åœ¨æ¸¸æˆè¡¨ç°å’Œä¸äººç±»ç©å®¶çš„äº’åŠ¨ä¸­éƒ½è¡¨ç°è‰¯å¥½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆèå…¥äººç±»ç¤¾äº¤åŠ¨æ€ã€‚ä½œè€…è¿˜å‘å¸ƒäº†ç›¸å…³æ•°æ®å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›å¯¹æ›´çœŸå®çš„å¼‚æ­¥æ²Ÿé€šçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09937', 'title': 'SAFE: Multitask Failure Detection for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2506.09937', 'abstract': 'SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.  \t\t\t\t\tAI-generated summary \t\t\t\t While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, pi_0, and pi_0-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/.', 'score': 7, 'issue_id': 4251, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '2a59fafef3ba118c', 'authors': ['Qiao Gu', 'Yuanliang Ju', 'Shengxiang Sun', 'Igor Gilitschenski', 'Haruki Nishimura', 'Masha Itkina', 'Florian Shkurti'], 'affiliations': ['Toyota Research Institute (TRI)', 'University of Toronto (UofT)', 'UofT Robotics Institute', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.09937.jpg', 'data': {'categories': ['#security', '#optimization', '#agents', '#robotics', '#agi'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SAFE: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ²', 'desc': 'SAFE - ÑÑ‚Ğ¾ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² VLA Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ± ÑƒÑĞ¿ĞµÑ…Ğµ Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. SAFE Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ°Ñ… Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SAFE: Generalizing Failure Detection for Vision-Language-Action Models', 'desc': 'SAFE is a novel failure detector designed for vision-language-action models (VLAs) that enables them to generalize to new tasks. It leverages high-level internal features of VLAs to predict the likelihood of task failure, allowing robots to respond appropriately in unfamiliar environments. Unlike traditional failure detectors that are limited to specific tasks, SAFE is trained on both successful and failed attempts across various tasks, enhancing its adaptability. The effectiveness of SAFE is demonstrated through extensive testing on multiple policy architectures, achieving superior performance in failure detection compared to existing methods.'}, 'zh': {'title': 'SAFEï¼šæ™ºèƒ½æœºå™¨äººæ•…éšœæ£€æµ‹çš„æ–°æ–¹æ³•', 'desc': 'SAFEæ˜¯ä¸€ä¸ªç”¨äºè§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹çš„æ•…éšœæ£€æµ‹å™¨ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å­¦ä¹ æ¨¡å‹çš„é«˜å±‚å†…éƒ¨ç‰¹å¾æ¥æ¨å¹¿åˆ°æœªè§è¿‡çš„ä»»åŠ¡ã€‚å°½ç®¡è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹åœ¨å¤šç§æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ–°ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æœ‰é™ã€‚ä¸ºäº†è®©æœºå™¨äººå®‰å…¨åœ°ä¸ç¯å¢ƒäº’åŠ¨ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½å¤ŸåŠæ—¶å‘å‡ºè­¦æŠ¥çš„æ•…éšœæ£€æµ‹å™¨ï¼Œä»¥ä¾¿æœºå™¨äººå¯ä»¥åœæ­¢ã€å›æº¯æˆ–è¯·æ±‚å¸®åŠ©ã€‚æˆ‘ä»¬æå‡ºçš„SAFEèƒ½å¤Ÿä»VLAçš„å†…éƒ¨ç‰¹å¾ä¸­å­¦ä¹ ï¼Œå¹¶é¢„æµ‹ä»»åŠ¡å¤±è´¥çš„å¯èƒ½æ€§ï¼Œç»è¿‡å¹¿æ³›æµ‹è¯•ï¼Œæ˜¾ç¤ºå‡ºä¼˜è¶Šçš„æ•…éšœæ£€æµ‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09736', 'title': 'Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.09736', 'abstract': "Visual perturbation framework enhances multimodal models' mathematical reasoning performance without additional training or algorithmic changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In a simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at https://github.com/YutingLi0606/Vision-Matters.", 'score': 3, 'issue_id': 4262, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'f9912b183b6548a4', 'authors': ['Yuting Li', 'Lai Wei', 'Kaipeng Zheng', 'Jingyuan Huang', 'Linghe Kong', 'Lichao Sun', 'Weiran Huang'], 'affiliations': ['Lehigh University', 'School of Computer Science, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2506.09736.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#math', '#multimodal', '#training', '#open_source', '#cv'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ›ÑƒÑ‡ÑˆĞµ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ - Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹: ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰ĞµĞµ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Reasoning with Visual Perturbations', 'desc': "This paper introduces a visual perturbation framework that improves the mathematical reasoning abilities of multimodal large language models (MLLMs) without needing extra training or changes to the algorithms. The authors found that language-only models can perform as well as or better than MLLMs when given image captions, indicating a gap in how MLLMs process visual information. The proposed framework includes three types of visual perturbations that enhance the models' robustness and can be easily added to existing post-training processes. Through various experiments, the study shows that these perturbations lead to significant improvements in reasoning performance, emphasizing the importance of effective visual integration in multimodal models."}, 'zh': {'title': 'è§†è§‰æ‰°åŠ¨æå‡å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è§†è§‰æ‰°åŠ¨æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ è®­ç»ƒæˆ–ç®—æ³•ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œæé«˜å¤šæ¨¡æ€æ¨¡å‹çš„æ•°å­¦æ¨ç†æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œä»…ä½¿ç”¨è¯­è¨€æ¨¡å‹å¹¶ç»“åˆå›¾åƒæè¿°ï¼Œèƒ½å¤Ÿè¾¾åˆ°ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½çš„è¡¨ç°ã€‚è¿™è¡¨æ˜å½“å‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆè§†è§‰æè¿°æ—¶å¯èƒ½å­˜åœ¨æ•´åˆä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ä¸‰ç§ç‰¹å®šçš„æ‰°åŠ¨ç­–ç•¥ï¼Œè®ºæ–‡å±•ç¤ºäº†åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ•°å­¦æ¨ç†æ€§èƒ½çš„ä¸€è‡´æå‡ï¼Œå¼ºè°ƒäº†è§†è§‰æ‰°åŠ¨åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09278', 'title': 'UFM: A Simple Path towards Unified Dense Correspondence with Flow', 'url': 'https://huggingface.co/papers/2506.09278', 'abstract': 'A Unified Flow & Matching model (UFM) improves dense image correspondence accuracy and speed by using a transformer architecture for unified data training, outperforming specialized methods for both optical flow and wide-baseline scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense image correspondence is central to many applications, such as visual odometry, 3D reconstruction, object association, and re-identification. Historically, dense correspondence has been tackled separately for wide-baseline scenarios and optical flow estimation, despite the common goal of matching content between two images. In this paper, we develop a Unified Flow & Matching model (UFM), which is trained on unified data for pixels that are co-visible in both source and target images. UFM uses a simple, generic transformer architecture that directly regresses the (u,v) flow. It is easier to train and more accurate for large flows compared to the typical coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than state-of-the-art flow methods (Unimatch), while also having 62% less error and 6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to demonstrate that unified training can outperform specialized approaches across both domains. This result enables fast, general-purpose correspondence and opens new directions for multi-modal, long-range, and real-time correspondence tasks.', 'score': 3, 'issue_id': 4263, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '3926eda7133e3fbf', 'authors': ['Yuchen Zhang', 'Nikhil Keetha', 'Chenwei Lyu', 'Bhuvan Jhamb', 'Yutian Chen', 'Yuheng Qiu', 'Jay Karhade', 'Shreyas Jha', 'Yaoyu Hu', 'Deva Ramanan', 'Sebastian Scherer', 'Wenshan Wang'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09278.jpg', 'data': {'categories': ['#optimization', '#architecture', '#cv', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Unified Flow & Matching model (UFM) - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. UFM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 28% Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ½Ğ° 62% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸Ğ¸. UFM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….'}, 'en': {'title': 'Unified Training for Superior Image Correspondence', 'desc': 'The Unified Flow & Matching model (UFM) enhances the accuracy and speed of dense image correspondence by employing a transformer architecture for unified training. It addresses the common challenge of matching content between images in both optical flow and wide-baseline scenarios, which have traditionally been treated separately. UFM directly regresses the (u,v) flow, making it simpler to train and more effective for large flows compared to previous methods that relied on coarse-to-fine cost volumes. This model achieves a 28% improvement in accuracy and is significantly faster, paving the way for advancements in multi-modal and real-time correspondence applications.'}, 'zh': {'title': 'ç»Ÿä¸€æµä¸åŒ¹é…æ¨¡å‹ï¼šæå‡å›¾åƒå¯¹åº”çš„é€Ÿåº¦ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æµä¸åŒ¹é…æ¨¡å‹ï¼ˆUFMï¼‰ï¼Œæ—¨åœ¨æé«˜å¯†é›†å›¾åƒå¯¹åº”çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚UFMé‡‡ç”¨äº†å˜æ¢å™¨æ¶æ„ï¼Œé€šè¿‡ç»Ÿä¸€æ•°æ®è®­ç»ƒï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å…‰æµä¼°è®¡å’Œå®½åŸºçº¿åœºæ™¯çš„åŒ¹é…é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„ç²—åˆ°ç»†æˆæœ¬ä½“ç§¯æ–¹æ³•ç›¸æ¯”ï¼ŒUFMåœ¨å¤„ç†å¤§æµæ—¶æ›´æ˜“äºè®­ç»ƒä¸”æ›´ä¸ºå‡†ç¡®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUFMåœ¨å‡†ç¡®æ€§ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›çš„æµæ–¹æ³•æé«˜äº†28%ï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ä¸Šæ¯”å¯†é›†å®½åŸºçº¿åŒ¹é…å™¨å¿«äº†6.7å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08900', 'title': 'MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis', 'url': 'https://huggingface.co/papers/2506.08900', 'abstract': 'MIRAGE, a multimodal foundation model, excels in OCT and SLO image classification and segmentation, outperforming existing general and specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial intelligence (AI) has become a fundamental tool for assisting clinicians in analyzing ophthalmic images, such as optical coherence tomography (OCT). However, developing AI models often requires extensive annotation, and existing models tend to underperform on independent, unseen data. Foundation models (FMs), large AI models trained on vast unlabeled datasets, have shown promise in overcoming these challenges. Nonetheless, available FMs for ophthalmology lack extensive validation, especially for segmentation tasks, and focus on a single imaging modality. In this context, we propose MIRAGE, a novel multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO) images. Additionally, we propose a new evaluation benchmark with OCT/SLO classification and segmentation tasks. The comparison with general and specialized FMs and segmentation methods shows the superiority of MIRAGE in both types of tasks, highlighting its suitability as a basis for the development of robust AI systems for retinal OCT image analysis. Both MIRAGE and the evaluation benchmark are publicly available: https://github.com/j-morano/MIRAGE.', 'score': 2, 'issue_id': 4258, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'd3860686fd37639a', 'authors': ['JosÃ© Morano', 'Botond Fazekas', 'Emese SÃ¼kei', 'Ronald Fecso', 'Taha Emre', 'Markus Gumpinger', 'Georg Faustmann', 'Marzieh Oghbaie', 'Ursula Schmidt-Erfurth', 'Hrvoje BogunoviÄ‡'], 'affiliations': ['Christian Doppler Laboratory for Artificial Intelligence in Retina, Institute of Artificial Intelligence, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria', 'Comprehensive Center for AI in Medicine, Medical University of Vienna, Vienna, Austria', 'OPTIMA Lab, Department of Ophthalmology, Medical University of Vienna, Vienna, Austria'], 'pdf_title_img': 'assets/pdf/title_img/2506.08900.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#science', '#open_source', '#cv'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'MIRAGE: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ˜Ğ˜-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¾Ñ„Ñ‚Ğ°Ğ»ÑŒĞ¼Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'MIRAGE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾ÑĞ½Ğ¾Ğ²Ğ° (foundation model) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞĞšĞ¢ Ğ¸ Ğ¡Ğ›Ğ Ğ² Ğ¾Ñ„Ñ‚Ğ°Ğ»ÑŒĞ¼Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. MIRAGE Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ°Ğ¼Ğ¾Ğ¹ MIRAGE.'}, 'en': {'title': 'MIRAGE: Revolutionizing Ophthalmic Image Analysis with Multimodal AI', 'desc': "MIRAGE is a multimodal foundation model designed to improve the classification and segmentation of ophthalmic images, specifically optical coherence tomography (OCT) and scanning laser ophthalmoscopy (SLO) images. It addresses the limitations of existing AI models that often require extensive labeled data and struggle with unseen datasets. By leveraging large amounts of unlabeled data, MIRAGE outperforms both general and specialized models in various tasks. The paper also introduces a new evaluation benchmark for OCT and SLO, demonstrating MIRAGE's effectiveness and potential for advancing AI in retinal image analysis."}, 'zh': {'title': 'MIRAGEï¼šçœ¼ç§‘å›¾åƒåˆ†æçš„æ–°æ ‡æ†', 'desc': 'MIRAGEæ˜¯ä¸€ç§å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œä¸“æ³¨äºå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰å’Œæ‰«ææ¿€å…‰çœ¼åº•ç…§ç›¸ï¼ˆSLOï¼‰å›¾åƒçš„åˆ†ç±»å’Œåˆ†å‰²ã€‚è¯¥æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„é€šç”¨å’Œä¸“ä¸šæ¨¡å‹ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨ç‹¬ç«‹æœªè§æ•°æ®ä¸Šçš„æ€§èƒ½ä¸è¶³é—®é¢˜ã€‚MIRAGEé€šè¿‡åœ¨å¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå…‹æœäº†å¯¹å¤§é‡æ ‡æ³¨çš„ä¾èµ–ï¼Œå¹¶ä¸”åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†MIRAGEåœ¨çœ¼åº•OCTå›¾åƒåˆ†æä¸­çš„é€‚ç”¨æ€§å’Œä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08008', 'title': 'Hidden in plain sight: VLMs overlook their visual representations', 'url': 'https://huggingface.co/papers/2506.08008', 'abstract': "Vision language models perform poorly on vision-centric tasks compared to their visual encoders, primarily due to ineffective utilization of visual information and inherited language priors.  \t\t\t\t\tAI-generated summary \t\t\t\t Language provides a natural interface to specify and evaluate performance on visual tasks. To realize this possibility, vision language models (VLMs) must successfully integrate visual and linguistic information. Our work compares VLMs to a direct readout of their visual encoders to understand their ability to integrate across these modalities. Across a series of vision-centric benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform substantially worse than their visual encoders, dropping to near-chance performance. We investigate these results through a series of analyses across the entire VLM: namely 1) the degradation of vision representations, 2) brittleness to task prompt, and 3) the language model's role in solving the task. We find that the bottleneck in performing these vision-centric tasks lies in this third category; VLMs are not effectively using visual information easily accessible throughout the entire model, and they inherit the language priors present in the LLM. Our work helps diagnose the failure modes of open-source VLMs, and presents a series of evaluations useful for future investigations into visual understanding within VLMs.", 'score': 2, 'issue_id': 4262, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '4183c96f8f03c207', 'authors': ['Stephanie Fu', 'Tyler Bonnen', 'Devin Guillory', 'Trevor Darrell'], 'affiliations': ['UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.08008.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#cv', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'VLM Ğ½Ğµ Ğ´Ğ¾Ñ‚ÑĞ³Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ñ€ĞµĞ½Ğ¸Ğµ. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ÑÑ Ğ² Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ½Ğ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ ÑƒĞ·ĞºĞ¸Ğ¼ Ğ¼ĞµÑÑ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ€Ğ¾Ğ»ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ° Ğ½Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° VLM Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Unlocking Visual Potential in Vision Language Models', 'desc': 'This paper examines the performance of vision language models (VLMs) on tasks that are primarily visual in nature. It finds that VLMs struggle to effectively integrate visual and linguistic information, leading to significantly poorer performance compared to their visual encoders. The research identifies key issues such as the degradation of visual representations and the influence of language model priors on task performance. Ultimately, the study highlights that VLMs fail to utilize available visual information effectively, which hampers their ability to perform well on vision-centric benchmarks.'}, 'zh': {'title': 'è§†è§‰è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ä¸ç“¶é¢ˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ï¼Œä¸»è¦åŸå› åœ¨äºå®ƒä»¬æœªèƒ½æœ‰æ•ˆåˆ©ç”¨è§†è§‰ä¿¡æ¯å’Œç»§æ‰¿çš„è¯­è¨€å…ˆéªŒã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒVLMsä¸å…¶è§†è§‰ç¼–ç å™¨çš„ç›´æ¥è¾“å‡ºï¼Œåˆ†æäº†å®ƒä»¬åœ¨è§†è§‰å’Œè¯­è¨€ä¿¡æ¯æ•´åˆæ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒVLMsåœ¨è§†è§‰ä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æ˜¾è‘—ä½äºè§†è§‰ç¼–ç å™¨ï¼Œæ¥è¿‘éšæœºæ°´å¹³ã€‚æˆ‘ä»¬æŒ‡å‡ºï¼ŒVLMsåœ¨æ‰§è¡Œè¿™äº›ä»»åŠ¡æ—¶çš„ç“¶é¢ˆä¸»è¦åœ¨äºè¯­è¨€æ¨¡å‹æœªèƒ½æœ‰æ•ˆåˆ©ç”¨å¯ç”¨çš„è§†è§‰ä¿¡æ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08001', 'title': 'Reparameterized LLM Training via Orthogonal Equivalence Transformation', 'url': 'https://huggingface.co/papers/2506.08001', 'abstract': "A new reParameterized training algorithm named POET uses Orthogonal Equivalence Transformation to optimize neurons, providing stable optimization and improved generalization for training large-scale neural networks including LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.", 'score': 2, 'issue_id': 4255, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'b47d919d8313c30a', 'authors': ['Zeju Qiu', 'Simon Buchholz', 'Tim Z. Xiao', 'Maximilian Dax', 'Bernhard SchÃ¶lkopf', 'Weiyang Liu'], 'affiliations': ['Max Planck Institute for Intelligent Systems, TÃ¼bingen', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.08001.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'POET: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'POET - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. POET Ñ€ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ… Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ²ĞµÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ POET Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'POET: Optimizing Neurons for Stable and Scalable Training', 'desc': 'The paper introduces POET, a new training algorithm designed to enhance the optimization of neurons in large-scale neural networks, particularly large language models (LLMs). POET employs Orthogonal Equivalence Transformation, which reparameterizes neurons using two learnable orthogonal matrices alongside a fixed random weight matrix. This method ensures the stability of the optimization process and improves the generalization capabilities of the models. The authors also present efficient approximations that allow POET to be flexible and scalable, demonstrating its effectiveness through extensive experiments.'}, 'zh': {'title': 'POETï¼šä¼˜åŒ–ç¥ç»å…ƒçš„é‡å‚æ•°åŒ–è®­ç»ƒæ–°ç®—æ³•', 'desc': 'POETæ˜¯ä¸€ç§æ–°çš„é‡å‚æ•°åŒ–è®­ç»ƒç®—æ³•ï¼Œåˆ©ç”¨æ­£äº¤ç­‰ä»·å˜æ¢æ¥ä¼˜åŒ–ç¥ç»å…ƒã€‚è¯¥ç®—æ³•é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªå¯å­¦ä¹ çš„æ­£äº¤çŸ©é˜µå’Œä¸€ä¸ªå›ºå®šçš„éšæœºæƒé‡çŸ©é˜µï¼Œå¯¹æ¯ä¸ªç¥ç»å…ƒè¿›è¡Œé‡å‚æ•°åŒ–ã€‚POETèƒ½å¤Ÿç¨³å®šåœ°ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§è§„æ¨¡ç¥ç»ç½‘ç»œçš„è®­ç»ƒã€‚å®éªŒç»“æœéªŒè¯äº†POETåœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09958', 'title': 'Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust\n  MedVQA in Gastrointestinal Endoscopy', 'url': 'https://huggingface.co/papers/2506.09958', 'abstract': "Kvasir-VQA-x1, an expanded dataset for gastrointestinal endoscopy, addresses clinical complexity and visual diversity with large-scale question-answer pairs and visual augmentations to enhance multimodal AI system reliability in clinical settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical Visual Question Answering (MedVQA) is a promising field for developing clinical decision support systems, yet progress is often limited by the available datasets, which can lack clinical complexity and visual diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new, large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly expands upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs that are designed to test deeper clinical reasoning. We developed a systematic method using large language models to generate these questions, which are stratified by complexity to better assess a model's inference capabilities. To ensure our dataset prepares models for real-world clinical scenarios, we have also introduced a variety of visual augmentations that mimic common imaging artifacts. The dataset is structured to support two main evaluation tracks: one for standard VQA performance and another to test model robustness against these visual perturbations. By providing a more challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate the development of more reliable and effective multimodal AI systems for use in clinical settings. The dataset is fully accessible and adheres to FAIR data principles, making it a valuable resource for the wider research community. Code and data: https://github.com/Simula/Kvasir-VQA-x1 and https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", 'score': 1, 'issue_id': 4263, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '127078517320f2b7', 'authors': ['Sushant Gautam', 'Michael A. Riegler', 'PÃ¥l Halvorsen'], 'affiliations': ['Oslo Metropolitan University (OsloMet), Norway', 'Simula Metropolitan Center for Digital Engineering (SimulaMet), Norway', 'Simula Research Laboratory, Norway'], 'pdf_title_img': 'assets/pdf/title_img/2506.09958.jpg', 'data': {'categories': ['#open_source', '#dataset', '#science', '#benchmark', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸: Kvasir-VQA-x1 Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ ÑĞ½Ğ´Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Kvasir-VQA-x1 Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ°ÑÑ‚Ñ€Ğ¾ÑĞ½Ñ‚ĞµÑ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ½Ğ´Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 159,549 Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Kvasir-VQA-x1 Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Clinical AI with Kvasir-VQA-x1: A New Benchmark for MedVQA', 'desc': "Kvasir-VQA-x1 is a newly developed dataset aimed at improving Medical Visual Question Answering (MedVQA) systems for gastrointestinal endoscopy. It includes 159,549 question-answer pairs that enhance clinical reasoning and assess AI models' inference capabilities. The dataset also features visual augmentations to simulate common imaging artifacts, ensuring models are tested under realistic conditions. By providing a more complex and diverse benchmark, Kvasir-VQA-x1 seeks to foster the development of robust multimodal AI systems in clinical environments."}, 'zh': {'title': 'Kvasir-VQA-x1ï¼šæå‡ä¸´åºŠå†³ç­–æ”¯æŒçš„å¤šæ¨¡æ€æ•°æ®é›†', 'desc': 'Kvasir-VQA-x1æ˜¯ä¸€ä¸ªæ‰©å±•çš„æ•°æ®é›†ï¼Œä¸“æ³¨äºèƒƒè‚ å†…çª¥é•œæ£€æŸ¥ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠå¤æ‚æ€§å’Œè§†è§‰å¤šæ ·æ€§çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«159,549ä¸ªæ–°çš„é—®ç­”å¯¹ï¼Œæ—¨åœ¨æµ‹è¯•æ›´æ·±å±‚æ¬¡çš„ä¸´åºŠæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿç”Ÿæˆè¿™äº›é—®é¢˜ï¼Œå¹¶é€šè¿‡è§†è§‰å¢å¼ºæŠ€æœ¯æ¨¡æ‹Ÿå¸¸è§çš„æˆåƒä¼ªå½±ï¼Œä»¥æé«˜æ¨¡å‹åœ¨çœŸå®ä¸´åºŠåœºæ™¯ä¸­çš„å¯é æ€§ã€‚Kvasir-VQA-x1ä¸ºå¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘æä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§å’Œä¸´åºŠç›¸å…³æ€§çš„åŸºå‡†ï¼Œä¿ƒè¿›äº†æ›´å¯é å’Œæœ‰æ•ˆçš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„è¿›æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09669', 'title': 'Query-Level Uncertainty in Large Language Models', 'url': 'https://huggingface.co/papers/2506.09669', 'abstract': 'A method using Query-Level Uncertainty and Internal Confidence enables Large Language Models to determine knowledge boundaries efficiently, improving adaptability and reducing inference costs.  \t\t\t\t\tAI-generated summary \t\t\t\t It is important for Large Language Models to be aware of the boundary of their knowledge, the mechanism of identifying known and unknown queries. This type of awareness can help models perform adaptive inference, such as invoking RAG, engaging in slow and deep thinking, or adopting the abstention mechanism, which is beneficial to the development of efficient and trustworthy AI. In this work, we propose a method to detect knowledge boundaries via Query-Level Uncertainty, which aims to determine if the model is able to address a given query without generating any tokens. To this end, we introduce a novel and training-free method called Internal Confidence, which leverages self-evaluations across layers and tokens. Empirical results on both factual QA and mathematical reasoning tasks demonstrate that our internal confidence can outperform several baselines. Furthermore, we showcase that our proposed method can be used for efficient RAG and model cascading, which is able to reduce inference costs while maintaining performance.', 'score': 1, 'issue_id': 4262, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'b1281e93419bc3d5', 'authors': ['Lihu Chen', 'GaÃ«l Varoquaux'], 'affiliations': ['Imperial College London, UK', 'Soda, Inria Saclay, France'], 'pdf_title_img': 'assets/pdf/title_img/2506.09669.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#math', '#rag', '#inference', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ Ğ·Ğ½Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ»Ğ¸ Ğ¾Ğ½Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ±ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RAG Ğ¸ ĞºĞ°ÑĞºĞ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing LLMs with Knowledge Boundary Awareness', 'desc': "This paper presents a method that helps Large Language Models (LLMs) identify the limits of their knowledge using Query-Level Uncertainty and Internal Confidence. By understanding which queries they can answer confidently, LLMs can adapt their responses, engage in deeper reasoning, or choose to abstain from answering when uncertain. The proposed Internal Confidence method evaluates the model's performance across different layers and tokens without requiring additional training. Experimental results show that this approach not only improves the model's adaptability but also reduces inference costs while maintaining high performance in tasks like factual question answering and mathematical reasoning."}, 'zh': {'title': 'è¯†åˆ«çŸ¥è¯†è¾¹ç•Œï¼Œæå‡æ¨ç†æ•ˆç‡', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æŸ¥è¯¢çº§ä¸ç¡®å®šæ€§å’Œå†…éƒ¨ä¿¡å¿ƒçš„æ–¹æ³•ï¼Œå¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ•ˆè¯†åˆ«çŸ¥è¯†è¾¹ç•Œã€‚è¿™ç§æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ€§æ¨ç†ï¼Œèƒ½å¤Ÿé€‰æ‹©æ€§åœ°è°ƒç”¨ç›¸å…³çŸ¥è¯†æˆ–é‡‡å–ä¿ç•™æœºåˆ¶ï¼Œä»è€Œæé«˜AIçš„æ•ˆç‡å’Œå¯ä¿¡åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è‡ªæˆ‘è¯„ä¼°æ¥æ£€æµ‹æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå¤„ç†ç‰¹å®šæŸ¥è¯¢ï¼Œè€Œæ— éœ€ç”Ÿæˆä»»ä½•è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å†…éƒ¨ä¿¡å¿ƒæ–¹æ³•åœ¨äº‹å®é—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶é™ä½äº†æ¨ç†æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09229', 'title': 'Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2506.09229', 'abstract': 'Cross-frame Representation Alignment improves video diffusion model fine-tuning by enhancing convergence and semantic coherence across frames.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning Video Diffusion Models (VDMs) at the user level to generate videos that reflect specific attributes of training data presents notable challenges, yet remains underexplored despite its practical importance. Meanwhile, recent work such as Representation Alignment (REPA) has shown promise in improving the convergence and quality of DiT-based image diffusion models by aligning, or assimilating, its internal hidden states with external pretrained visual features, suggesting its potential for VDM fine-tuning. In this work, we first propose a straightforward adaptation of REPA for VDMs and empirically show that, while effective for convergence, it is suboptimal in preserving semantic consistency across frames. To address this limitation, we introduce Cross-frame Representation Alignment (CREPA), a novel regularization technique that aligns hidden states of a frame with external features from neighboring frames. Empirical evaluations on large-scale VDMs, including CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual fidelity and cross-frame semantic coherence when fine-tuned with parameter-efficient methods such as LoRA. We further validate CREPA across diverse datasets with varying attributes, confirming its broad applicability. Project page: https://crepavideo.github.io', 'score': 1, 'issue_id': 4262, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'c5fdb98160ab01fe', 'authors': ['Sungwon Hwang', 'Hyojin Jang', 'Kinam Kim', 'Minho Park', 'Jaegul choo'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.09229.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#video'], 'emoji': 'ğŸï¸', 'ru': {'title': 'CREPA: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VDM) - Cross-frame Representation Alignment (CREPA). CREPA Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ° Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… VDM, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CogVideoX-5B Ğ¸ Hunyuan Video, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ CREPA Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ LoRA. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ ĞµĞ³Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Aligning Frames for Better Video Generation', 'desc': 'This paper introduces Cross-frame Representation Alignment (CREPA), a new technique designed to enhance the fine-tuning of Video Diffusion Models (VDMs). CREPA improves the convergence of VDMs by aligning the hidden states of individual frames with features from neighboring frames, which helps maintain semantic coherence across the video. The authors demonstrate that this method not only boosts visual quality but also ensures that the generated videos are more consistent in meaning throughout. Their experiments on various large-scale VDMs show that CREPA is effective and applicable across different datasets, making it a significant advancement in video generation.'}, 'zh': {'title': 'è·¨å¸§è¡¨ç¤ºå¯¹é½æå‡è§†é¢‘ç”Ÿæˆè´¨é‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç§°ä¸ºè·¨å¸§è¡¨ç¤ºå¯¹é½ï¼ˆCREPAï¼‰ï¼Œç”¨äºæ”¹è¿›è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰çš„å¾®è°ƒè¿‡ç¨‹ã€‚CREPAé€šè¿‡å°†å½“å‰å¸§çš„éšè—çŠ¶æ€ä¸ç›¸é‚»å¸§çš„å¤–éƒ¨ç‰¹å¾å¯¹é½ï¼Œå¢å¼ºäº†å¸§ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCREPAåœ¨è§†è§‰ä¿çœŸåº¦å’Œè·¨å¸§è¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨é«˜æ•ˆå‚æ•°å¾®è°ƒæ–¹æ³•æ—¶ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶å¹¿æ³›é€‚ç”¨æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09007', 'title': 'Branched SchrÃ¶dinger Bridge Matching', 'url': 'https://huggingface.co/papers/2506.09007', 'abstract': 'BranchSBM, a novel generative modeling framework, extends Schr\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting the intermediate trajectories between an initial and target distribution is a central problem in generative modeling. Existing approaches, such as flow matching and Schr\\"odinger Bridge Matching, effectively learn mappings between two distributions by modeling a single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes. To address this, we introduce Branched Schr\\"odinger Bridge Matching (BranchSBM), a novel framework that learns branched Schr\\"odinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations.', 'score': 1, 'issue_id': 4252, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '8f3c4a6be505cd98', 'authors': ['Sophia Tang', 'Yinuo Zhang', 'Alexander Tong', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Computer Science, Duke University', 'Department of Computer and Information Science, University of Pennsylvania', 'Mila, Quebec AI Institute', 'UniversitÃ© de MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2506.09007.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#dataset', '#data'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'BranchSBM: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'BranchSBM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ SchrÃ¶dinger Bridge Matching Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼. BranchSBM Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ¾ÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·ĞµĞ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿ÑƒÑ‚ÑĞ¼, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¸Ñ„ÑƒÑ€ĞºĞ°Ñ†Ğ¸Ğ¹ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑƒĞ´ĞµĞ± Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ Ñ€Ğ°ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ…ÑÑ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Branching Out: Modeling Multiple Outcomes with BranchSBM', 'desc': 'BranchSBM is a new framework in generative modeling that enhances the traditional Schr"odinger Bridge Matching by allowing for branched stochastic paths. Unlike previous methods that only model single paths between two distributions, BranchSBM can represent multiple outcomes from a single starting point. This is achieved by using multiple time-dependent velocity fields and growth processes, which capture the complexity of population-level divergence. The framework is particularly useful for applications like simulating cell fate decisions and navigating multi-path scenarios.'}, 'zh': {'title': 'åˆ†æ”¯è–›å®šè°”æ¡¥åŒ¹é…ï¼šæ•æ‰å¤šè·¯å¾„æ¼”åŒ–çš„ç”Ÿæˆå»ºæ¨¡æ–°æ¡†æ¶', 'desc': 'BranchSBMæ˜¯ä¸€ç§æ–°é¢–çš„ç”Ÿæˆå»ºæ¨¡æ¡†æ¶ï¼Œæ‰©å±•äº†è–›å®šè°”æ¡¥åŒ¹é…æ–¹æ³•ï¼Œä»¥å»ºæ¨¡ä»å•ä¸€åˆå§‹åˆ†å¸ƒåˆ°å¤šä¸ªç»“æœçš„åˆ†æ”¯éšæœºè·¯å¾„å’Œå¤šè·¯å¾„æ¼”åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡å‚æ•°åŒ–å¤šä¸ªæ—¶é—´ä¾èµ–çš„é€Ÿåº¦åœºå’Œç”Ÿé•¿è¿‡ç¨‹ï¼Œèƒ½å¤Ÿè¡¨ç¤ºä»å…±åŒèµ·æºåˆ°å¤šä¸ªä¸åŒç»“æœçš„äººå£çº§åˆ«çš„åˆ†æ­§ã€‚ä¸ç°æœ‰çš„å•æ¨¡æ€è¿‡æ¸¡æ–¹æ³•ç›¸æ¯”ï¼ŒBranchSBMåœ¨å¤„ç†å¤šè·¯å¾„è¡¨é¢å¯¼èˆªã€ç»†èƒå‘½è¿åˆ†å‰å»ºæ¨¡ä»¥åŠæ¨¡æ‹Ÿç»†èƒå¯¹æ‰°åŠ¨çš„ä¸åŒååº”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚æ€»ä¹‹ï¼ŒBranchSBMä¸ºç”Ÿæˆå»ºæ¨¡æä¾›äº†æ›´ä¸°å¯Œçš„å·¥å…·ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚çš„æ¼”åŒ–è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06020', 'title': 'When to Trust Context: Self-Reflective Debates for Context Reliability', 'url': 'https://huggingface.co/papers/2506.06020', 'abstract': "A lightweight framework integrating token-level self-confidence and an asymmetric debate between agents enhances the robustness of large language models to contextual inconsistencies with minimal computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models frequently encounter conflicts between their parametric knowledge and contextual input, often resulting in factual inconsistencies or hallucinations. We propose Self-Reflective Debate for Contextual Reliability (SR-DCR), a lightweight framework that integrates token-level self-confidence with an asymmetric multi-agent debate to adjudicate such conflicts. A critic, deprived of context, challenges a defender who argues from the given passage; a judge model evaluates the debate and determines the context's reliability. The final answer is selected by combining the verdict with model confidence. Experiments on the ClashEval benchmark demonstrate that SR-DCR consistently enhances robustness to misleading context while maintaining accuracy on trustworthy inputs, outperforming both classical debate and confidence-only baselines with minimal computational overhead. The code is available at https://github.com/smiles724/Self-Reflective-Debates.", 'score': 1, 'issue_id': 4264, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '2899485806a0e65b', 'authors': ['Zeqi Zhou', 'Fang Wu', 'Shayan Talaei', 'Haokai Zhao', 'Cheng Meixin', 'Tinson Xu', 'Amin Saberi', 'Yejin Choi'], 'affiliations': ['Brown University', 'Stanford University', 'University of Chicago', 'University of New South Wales', 'Xian University of Electronic Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.06020.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#training', '#hallucinations', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ´ĞµĞ±Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SR-DCR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ÑĞ¼. ĞĞ½ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ±Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SR-DCR Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ²Ğ¾Ğ´ÑÑ‰ĞµĞ¼Ñƒ Ğ² Ğ·Ğ°Ğ±Ğ»ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ±Ğ°Ñ‚Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Debating for Better Contextual Understanding in AI', 'desc': "This paper introduces a new framework called Self-Reflective Debate for Contextual Reliability (SR-DCR) that improves the reliability of large language models when faced with conflicting information. It combines token-level self-confidence with a debate between two agents: a defender who uses the context and a critic who does not. A judge model evaluates their arguments to determine the reliability of the context. The results show that SR-DCR enhances the model's ability to handle misleading information while keeping its performance on accurate inputs intact, all with low computational costs."}, 'zh': {'title': 'è‡ªæˆ‘åæ€è¾©è®ºï¼šæå‡è¯­è¨€æ¨¡å‹é²æ£’æ€§çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè‡ªæˆ‘åæ€è¾©è®ºçš„æ¡†æ¶ï¼ˆSR-DCRï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¸Šä¸‹æ–‡ä¸ä¸€è‡´æ—¶çš„é²æ£’æ€§ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºæ ‡è®°çš„è‡ªä¿¡åº¦å’Œä¸å¯¹ç§°çš„å¤šæ™ºèƒ½ä½“è¾©è®ºï¼Œé€šè¿‡è®©æ‰¹è¯„è€…å’Œè¾©æŠ¤è€…è¿›è¡Œè¾©è®ºæ¥è§£å†³çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡ä¹‹é—´çš„å†²çªã€‚æœ€ç»ˆï¼Œè¯„åˆ¤æ¨¡å‹ä¼šæ ¹æ®è¾©è®ºç»“æœå’Œæ¨¡å‹è‡ªä¿¡åº¦æ¥é€‰æ‹©ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSR-DCRåœ¨å¤„ç†è¯¯å¯¼æ€§ä¸Šä¸‹æ–‡æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼ŒåŒæ—¶åœ¨å¯ä¿¡è¾“å…¥ä¸Šä¿æŒå‡†ç¡®æ€§ï¼Œä¸”è®¡ç®—æˆæœ¬æä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17450', 'title': 'BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing', 'url': 'https://huggingface.co/papers/2506.17450', 'abstract': 'A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  \t\t\t\t\tAI-generated summary \t\t\t\t We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows a layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into a coherent scene using a generative compositor (compositing). Our generative compositor extends a pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks.', 'score': 24, 'issue_id': 4550, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': 'b5bb4470d500be10', 'authors': ['Jiacheng Chen', 'Ramin Mehran', 'Xuhui Jia', 'Saining Xie', 'Sanghyun Woo'], 'affiliations': ['Google DeepMind', 'New York University', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2506.17450.jpg', 'data': {'categories': ['#cv', '#diffusion', '#3d', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ 3D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼', 'desc': 'BlenderFusion - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½. ĞĞ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¸, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ°Ñ… Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ñ€Ğ¾Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². BlenderFusion Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Revolutionizing Scene Editing with BlenderFusion', 'desc': 'BlenderFusion is a framework that allows users to create new scenes by rearranging objects, backgrounds, and camera angles. It uses a three-step process: first, it segments visual inputs into 3D elements, then it allows for editing these elements in Blender, and finally, it combines them into a complete scene using a generative compositor. The compositor is based on a diffusion model that processes both the original and edited scenes simultaneously, enhancing the editing process. Key techniques like source masking and simulated object jittering improve flexibility and control in scene composition, leading to better results than previous methods.'}, 'zh': {'title': 'ç”Ÿæˆè§†è§‰åˆæˆçš„æ–°æ–¹æ³•', 'desc': 'BlenderFusionæ˜¯ä¸€ä¸ªç”Ÿæˆè§†è§‰åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡é‡æ–°ç»„åˆå¯¹è±¡ã€ç›¸æœºå’ŒèƒŒæ™¯æ¥åˆæˆæ–°åœºæ™¯ã€‚å®ƒé‡‡ç”¨åˆ†å±‚-ç¼–è¾‘-åˆæˆçš„æµç¨‹ï¼Œé¦–å…ˆå°†è§†è§‰è¾“å…¥åˆ†å‰²å¹¶è½¬æ¢ä¸ºå¯ç¼–è¾‘çš„3Då®ä½“ï¼Œç„¶ååœ¨Blenderä¸­è¿›è¡Œ3Dæ§åˆ¶çš„ç¼–è¾‘ï¼Œæœ€åä½¿ç”¨ç”Ÿæˆåˆæˆå™¨å°†å®ƒä»¬èåˆæˆä¸€ä¸ªè¿è´¯çš„åœºæ™¯ã€‚è¯¥ç”Ÿæˆåˆæˆå™¨æ‰©å±•äº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åŸå§‹åœºæ™¯å’Œç¼–è¾‘åçš„åœºæ™¯ã€‚BlenderFusionåœ¨å¤æ‚çš„åˆæˆåœºæ™¯ç¼–è¾‘ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21862', 'title': 'LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs', 'url': 'https://huggingface.co/papers/2506.21862', 'abstract': 'LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.', 'score': 17, 'issue_id': 4548, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 Ğ¸ÑĞ½Ñ', 'en': 'June 27', 'zh': '6æœˆ27æ—¥'}, 'hash': 'b9ad171aa3fb5bbf', 'authors': ['Boyuan Sun', 'Jiaxing Zhao', 'Xihan Wei', 'Qibin Hou'], 'affiliations': ['Tongyi Lab, Alibaba Group', 'VCIP, School of Computer Science, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21862.jpg', 'data': {'categories': ['#training', '#benchmark', '#multimodal', '#long_context', '#dataset', '#video'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LLaVA-Scissor - ÑÑ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² (SCC) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ. LLaVA-Scissor Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ SCC ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLaVA-Scissor Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Efficient Video Understanding with Semantic Token Compression', 'desc': "LLaVA-Scissor is a novel token compression strategy specifically designed for video multimodal large language models. It utilizes Semantic Connected Components (SCC) to effectively group tokens into distinct semantic regions, which helps in reducing redundancy and maintaining semantic integrity. Unlike previous methods that rely on attention scores, LLaVA-Scissor compresses tokens in both spatial and temporal dimensions, ensuring comprehensive coverage of the video's content. Extensive evaluations demonstrate that this approach significantly outperforms existing token compression techniques, especially when retaining fewer tokens during video understanding tasks."}, 'zh': {'title': 'LLaVA-Scissorï¼šé«˜æ•ˆçš„è§†é¢‘ä»¤ç‰Œå‹ç¼©ç­–ç•¥', 'desc': 'LLaVA-Scissoræ˜¯ä¸€ç§é’ˆå¯¹è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œå‹ç¼©ç­–ç•¥ã€‚å®ƒåˆ©ç”¨è¯­ä¹‰è¿é€šç»„ä»¶ï¼ˆSCCï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°å°†ä»¤ç‰Œåˆ†é…åˆ°ä¸åŒçš„è¯­ä¹‰åŒºåŸŸï¼Œä»è€Œç¡®ä¿å…¨é¢çš„è¯­ä¹‰è¦†ç›–ã€‚ä¸ä»¥å¾€åŸºäºæ³¨æ„åŠ›åˆ†æ•°çš„å‹ç¼©æ–¹æ³•ä¸åŒï¼ŒLLaVA-Scissorèƒ½å¤Ÿå‡å°‘ä»¤ç‰Œå†—ä½™ï¼Œå¹¶åœ¨ç©ºé—´å’Œæ—¶é—´åŸŸä¸­è¿›è¡Œä¸¤æ­¥å‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ä½ä»¤ç‰Œä¿ç•™æ¯”ç‡ä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21356', 'title': 'ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2506.21356', 'abstract': "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce ShotBench, a comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct ShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new state-of-the-art performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation.", 'score': 13, 'issue_id': 4551, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '5a54508ae68df265', 'authors': ['Hongbo Liu', 'Jingwen He', 'Yi Jin', 'Dian Zheng', 'Yuhao Dong', 'Fan Zhang', 'Ziqi Huang', 'Yinan He', 'Yangguang Li', 'Weichao Chen', 'Yu Qiao', 'Wanli Ouyang', 'Shengjie Zhao', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21356.jpg', 'data': {'categories': ['#dataset', '#games', '#training', '#benchmark', '#open_source', '#multimodal', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° ĞºĞ¸Ğ½Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ShotBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ShotQA Ñ 70 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ShotQA Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ShotVL, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ´ÑˆĞ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° ĞºĞ¸Ğ½Ğ¾. Ğ­Ñ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': "Enhancing AI's Cinematic Language Comprehension with ShotBench and ShotQA", 'desc': "This paper introduces ShotBench and ShotQA datasets, along with the ShotVL model, to improve AI's ability to understand and generate cinematic language. Cinematography is a complex visual language that conveys stories and emotions, but current Vision-Language Models (VLMs) struggle with its nuances. The ShotBench benchmark evaluates VLMs on their comprehension of cinematic grammar, revealing significant limitations in their performance. By developing ShotQA and fine-tuning the ShotVL model, the authors achieve state-of-the-art results, providing valuable resources for advancing AI in cinematic understanding and generation."}, 'zh': {'title': 'æå‡AIç”µå½±è¯­è¨€ç†è§£çš„çªç ´æ€§è¿›å±•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ShotBenchå’ŒShotQAæ•°æ®é›†ä»¥åŠShotVLæ¨¡å‹ï¼Œæ—¨åœ¨æå‡äººå·¥æ™ºèƒ½å¯¹ç”µå½±è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ç”µå½±æ‘„å½±æ˜¯ä¼ è¾¾å™äº‹ã€æƒ…æ„Ÿå’Œç¾å­¦è´¨é‡çš„åŸºæœ¬è§†è§‰è¯­è¨€ï¼Œä½†ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç»†è…»çš„ç”µå½±è¯­æ³•æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚æˆ‘ä»¬æ„å»ºäº†ShotBenchåŸºå‡†ï¼ŒåŒ…å«3500å¤šä¸ªä¸“å®¶æ³¨é‡Šçš„é—®ç­”å¯¹ï¼Œè¯„ä¼°äº†24ä¸ªé¢†å…ˆçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºå®ƒä»¬åœ¨ç»†ç²’åº¦è§†è§‰çº¿ç´¢å’Œå¤æ‚ç©ºé—´æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ã€‚é€šè¿‡æ„å»ºShotQAæ•°æ®é›†å¹¶å¼€å‘ShotVLæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ShotBenchä¸Šå–å¾—äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œæ¨åŠ¨äº†AIåœ¨ç”µå½±ç†è§£å’Œç”Ÿæˆé¢†åŸŸçš„è¿›æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21416', 'title': 'XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation', 'url': 'https://huggingface.co/papers/2506.21416', 'abstract': 'XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving fine-grained control over subject identity and semantic attributes (pose, style, lighting) in text-to-image generation, particularly for multiple subjects, often undermines the editability and coherence of Diffusion Transformers (DiTs). Many approaches introduce artifacts or suffer from attribute entanglement. To overcome these challenges, we propose a novel multi-subject controlled generation model XVerse. By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subject without disrupting image latents or features. Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes. This advancement significantly improves personalized and complex scene generation capabilities.', 'score': 11, 'issue_id': 4551, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '4c3c383901d9306f', 'authors': ['Bowen Chen', 'Mengyi Zhao', 'Haomiao Sun', 'Li Chen', 'Xu Wang', 'Kang Du', 'Xinglong Wu'], 'affiliations': ['Intelligent Creation Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2506.21416.jpg', 'data': {'categories': ['#diffusion', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'XVerse - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. XVerse Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'XVerse: Mastering Multi-Subject Control in Image Generation', 'desc': 'XVerse is a new model that improves text-to-image generation by allowing users to control multiple subjects in an image with high precision. It uses a technique called token-specific text-stream modulation to manage the identity and attributes of each subject, such as pose and lighting, without losing image quality. Traditional methods often create unwanted artifacts or mix up attributes, but XVerse avoids these issues by transforming reference images into specific adjustments. This leads to better coherence and fidelity in generated images, making it easier to create complex scenes with distinct and editable subjects.'}, 'zh': {'title': 'XVerseï¼šç²¾ç¡®æ§åˆ¶å¤šå¯¹è±¡å›¾åƒç”Ÿæˆçš„åˆ›æ–°', 'desc': 'XVerseæ˜¯ä¸€ç§å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿå¯¹å¤šä¸ªå¯¹è±¡è¿›è¡Œç²¾ç¡®å’Œç‹¬ç«‹çš„æ§åˆ¶ã€‚å®ƒé€šè¿‡ç‰¹å®šçš„æ–‡æœ¬æµè°ƒåˆ¶ï¼Œè§£å†³äº†åœ¨ç”Ÿæˆå¤šå¯¹è±¡å›¾åƒæ—¶å¸¸è§çš„ç¼–è¾‘æ€§å’Œä¸€è‡´æ€§é—®é¢˜ã€‚XVerseå°†å‚è€ƒå›¾åƒè½¬æ¢ä¸ºåç§»é‡ï¼Œä»è€Œå®ç°å¯¹ç‰¹å®šå¯¹è±¡çš„æ§åˆ¶ï¼Œè€Œä¸å¹²æ‰°å›¾åƒçš„æ½œåœ¨ç‰¹å¾ã€‚è¿™ä¸€åˆ›æ–°æ˜¾è‘—æé«˜äº†ä¸ªæ€§åŒ–å’Œå¤æ‚åœºæ™¯ç”Ÿæˆçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20279', 'title': 'From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios', 'url': 'https://huggingface.co/papers/2506.20279', 'abstract': "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise annotated label for an input image. Despite advances in this field, existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data. To systematically study this problem, we first introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction tasks that correspond to urgent real-world applications, featuring unified evaluation across tasks. Then, we propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters. Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization. In contrast, DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment. Our data, and checkpoints and codes are available at https://xcltql666.github.io/DenseDiTProj", 'score': 10, 'issue_id': 4550, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '8382f71877fe1997', 'authors': ['Changliang Xia', 'Chengyou Jia', 'Zhuohang Dang', 'Minnan Luo'], 'affiliations': ['Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.20279.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#cv', '#synthetic', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'DenseDiT - ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². DenseDiT Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ¹Ğ¾Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 0,01% Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'DenseDiT: Revolutionizing Dense Prediction with Minimal Data', 'desc': "DenseDiT is a generative model that excels in dense prediction tasks, which involve assigning labels to each pixel in an image. It addresses the challenge of limited training data by leveraging visual priors from generative models, allowing it to perform well in real-world scenarios. The model introduces DenseWorld, a benchmark for evaluating various dense prediction tasks, highlighting the shortcomings of existing methods in real-world applications. DenseDiT's innovative design, which includes a parameter-reuse mechanism and multi-scale context integration, enables it to achieve superior performance with significantly less training data compared to traditional approaches."}, 'zh': {'title': 'DenseDiTï¼šç”¨æœ€å°‘æ•°æ®å®ç°å¯†é›†é¢„æµ‹çš„çªç ´', 'desc': 'DenseDiTæ˜¯ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œçš„å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­ä»¥æœ€å°‘çš„è®­ç»ƒæ•°æ®å®ç°ä¼˜è¶Šçš„æ€§èƒ½ã€‚å¯†é›†é¢„æµ‹ä»»åŠ¡åœ¨è®¡ç®—æœºè§†è§‰ä¸­éå¸¸é‡è¦ï¼Œæ—¨åœ¨ä¸ºè¾“å…¥å›¾åƒå­¦ä¹ é€åƒç´ çš„æ ‡æ³¨æ ‡ç­¾ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œç¼ºä¹å¯¹çœŸå®åœºæ™¯çš„å¹¿æ³›é€‚åº”æ€§ï¼Œä¸”é¢ä¸´çœŸå®æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚DenseDiTé€šè¿‡æœ€å¤§é™åº¦åœ°åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„è§†è§‰å…ˆéªŒï¼Œç»“åˆå‚æ•°é‡ç”¨æœºåˆ¶å’Œè½»é‡çº§åˆ†æ”¯ï¼Œèƒ½å¤Ÿåœ¨å¤šç§çœŸå®ä¸–ç•Œçš„å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.22434', 'title': 'MiCo: Multi-image Contrast for Reinforcement Visual Reasoning', 'url': 'https://huggingface.co/papers/2506.22434', 'abstract': 'Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  \t\t\t\t\tAI-generated summary \t\t\t\t This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.', 'score': 7, 'issue_id': 4548, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 Ğ¸ÑĞ½Ñ', 'en': 'June 27', 'zh': '6æœˆ27æ—¥'}, 'hash': 'd7e89f248d4c331e', 'authors': ['Xi Chen', 'Mingkang Zhu', 'Shaoteng Liu', 'Xiaoyang Wu', 'Xiaogang Xu', 'Yu Liu', 'Xiang Bai', 'Hengshuang Zhao'], 'affiliations': ['CUHK', 'HKU', 'HUST', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.22434.jpg', 'data': {'categories': ['#optimization', '#rl', '#benchmark', '#cv', '#dataset', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'Empowering VLMs with Self-Supervised Image Triplet Learning', 'desc': 'This paper presents a method for enhancing the reasoning capabilities of Vision-Language Models (VLMs) using self-supervised learning with image triplets. The approach involves creating pairs of augmented images along with a distinct image, allowing the model to learn to compare and reason about visual differences. By training the model to determine whether images are the same or different, it develops a reasoning process that generalizes to various tasks without needing human-annotated data. The results demonstrate that this method significantly improves performance on multi-image reasoning benchmarks and general vision tasks.'}, 'zh': {'title': 'è‡ªç›‘ç£å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡ä½¿ç”¨å›¾åƒä¸‰å…ƒç»„çš„è‡ªç›‘ç£å­¦ä¹ æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šå›¾åƒä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚ç ”ç©¶è€…ä»¬æ„å»ºäº†ç”±åŒä¸€å›¾åƒçš„ä¸¤ä¸ªå¢å¼ºè§†å›¾å’Œä¸€ä¸ªç›¸ä¼¼ä½†ä¸åŒçš„å›¾åƒç»„æˆçš„å›¾åƒä¸‰å…ƒç»„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹è¢«è¦æ±‚ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼Œä»¥æ¯”è¾ƒè¿™äº›å›¾åƒï¼ˆå³åˆ¤æ–­ç›¸åŒæˆ–ä¸åŒï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹ä»…åœ¨è§†è§‰æ¯”è¾ƒä»»åŠ¡ä¸Šè®­ç»ƒï¼Œä½†å…¶å­¦ä¹ åˆ°çš„æ¨ç†èƒ½åŠ›èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°å„ç§é—®é¢˜ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21656', 'title': 'Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs', 'url': 'https://huggingface.co/papers/2506.21656', 'abstract': 'SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.', 'score': 5, 'issue_id': 4548, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '8d063b13fc555964', 'authors': ['Yifan Shen', 'Yuanzhe Liu', 'Jingyuan Zhu', 'Xu Cao', 'Xiaofeng Zhang', 'Yixiao He', 'Wenming Ye', 'James Matthew Rehg', 'Ismini Lourentzou'], 'affiliations': ['Google', 'Shanghai Jiao Tong University', 'University of Illinois Urbana-Champaign', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2506.21656.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#multimodal', '#rlhf', '#cv', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ', 'desc': 'SpatialReasoner-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. SpatialReasoner-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SPATIALRGPT-Bench, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 9.8% Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Elevating Spatial Reasoning with SpatialReasoner-R1', 'desc': "SpatialReasoner-R1 is a vision-language reasoning model that enhances spatial reasoning capabilities in AI. It employs Multi-Model Monte Carlo Tree Search (M3CTS) to create diverse reasoning paths that are logically consistent, improving the model's ability to handle complex spatial tasks. Additionally, it introduces fine-grained Direct Preference Optimization (fDPO), which refines the model's decision-making by focusing on specific segments of reasoning and using a spatial reward mechanism. The model achieves state-of-the-art performance on the SPATIALRGPT-Bench, significantly outperforming previous models in both spatial quality and quantity tasks."}, 'zh': {'title': 'ç©ºé—´æ¨ç†çš„æ–°çªç ´', 'desc': 'SpatialReasoner-R1æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦ç©ºé—´æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤šæ¨¡å‹è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆM3CTSï¼‰æ–¹æ³•ï¼Œç”Ÿæˆå¤šæ ·ä¸”é€»è¾‘ä¸€è‡´çš„é•¿é“¾æ€ç»´æ¨ç†è½¨è¿¹ï¼Œä»¥æ„å»ºé«˜è´¨é‡çš„ç©ºé—´æ¨ç†ç›‘ç£ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒSpatialReasoner-R1è¿˜å¼•å…¥äº†ç»†ç²’åº¦ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆfDPOï¼‰ï¼Œé€šè¿‡ç©ºé—´å¥–åŠ±æœºåˆ¶å¯¹å€™é€‰å“åº”è¿›è¡Œè¯„ä¼°ï¼Œä»è€Œæé«˜æè¿°æ€§åŸºç¡€å’Œé€»è¾‘æ¨ç†çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpatialReasoner-R1åœ¨SPATIALRGPT-Benchä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”æœ€å¼ºåŸºçº¿æé«˜äº†9.8%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19741', 'title': 'Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls', 'url': 'https://huggingface.co/papers/2506.19741', 'abstract': "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT", 'score': 3, 'issue_id': 4548, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '288a2c7ef1ba6865', 'authors': ['Yihong Luo', 'Shuchen Xue', 'Tianyang Hu', 'Jing Tang'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'NUS', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2506.19741.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#optimization'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Noise Consistency Training (NCT) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. NCT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑˆÑƒĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NCT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Control in AI Content Generation with Noise Consistency Training', 'desc': 'This paper presents a new method called Noise Consistency Training (NCT) that enhances pre-trained one-step generators for content generation without the need for retraining. NCT efficiently integrates new control signals, such as structural or semantic guidelines, into the generator by using an adapter module and a noise consistency loss. This approach allows the generator to produce high-quality outputs while maintaining computational efficiency, outperforming traditional methods that require extensive modifications. The results show that NCT achieves superior controllable generation in a single forward pass, making it a significant advancement in the field of artificial intelligence-generated content.'}, 'zh': {'title': 'å™ªå£°ä¸€è‡´æ€§è®­ç»ƒï¼šé«˜æ•ˆå¯æ§ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å™ªå£°ä¸€è‡´æ€§è®­ç»ƒï¼ˆNCTï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†æ–°çš„æ§åˆ¶ä¿¡å·æ•´åˆåˆ°é¢„è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆå™¨ä¸­ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œæ˜‚è´µçš„ä¿®æ”¹ï¼Œè€ŒNCTé€šè¿‡å¼•å…¥é€‚é…æ¨¡å—å’Œå™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œåœ¨ç”Ÿæˆå™¨çš„å™ªå£°ç©ºé—´ä¸­ç›´æ¥è¿›è¡Œè°ƒæ•´ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¤šæ­¥å’Œè’¸é¦æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¯æ§ç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚NCTçš„æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶åœ¨æ•°æ®ä½¿ç”¨ä¸Šæ›´åŠ é«˜æ•ˆï¼Œæ˜“äºéƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21411', 'title': 'Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity', 'url': 'https://huggingface.co/papers/2505.21411', 'abstract': 'Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.  \t\t\t\t\tAI-generated summary \t\t\t\t The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B.', 'score': 0, 'issue_id': 4552, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'e4bcbe7787b328fa', 'authors': ['Yehui Tang', 'Xiaosong Li', 'Fangcheng Liu', 'Wei Guo', 'Hang Zhou', 'Yaoyuan Wang', 'Kai Han', 'Xianzhi Yu', 'Jinpeng Li', 'Hui Zang', 'Fei Mi', 'Xiaojun Meng', 'Zhicheng Liu', 'Hanting Chen', 'Binfan Zheng', 'Can Chen', 'Youliang Yan', 'Ruiming Tang', 'Peifeng Qin', 'Xinghao Chen', 'Dacheng Tao', 'Yunhe Wang'], 'affiliations': ['Huawei'], 'pdf_title_img': 'assets/pdf/title_img/2505.21411.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MoGE: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Mixture of Grouped Experts (MoGE) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. MoGE Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MoGE Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Pangu Pro MoE Ñ 72 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ NPU Ascend. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MoGE Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Balancing Experts for Efficient Language Model Execution', 'desc': 'The paper introduces Mixture of Grouped Experts (MoGE), a novel approach to improve the efficiency of large language models by enhancing expert load balancing. MoGE ensures that an equal number of experts are activated for each input token, which addresses the issue of uneven expert activation seen in traditional Mixture of Experts (MoE) models. This architectural design allows for better distribution of computational load across multiple devices, significantly increasing throughput during inference. The results demonstrate that MoGE leads to superior performance and cost-effectiveness on Ascend NPUs, particularly with the Pangu Pro MoE model, which achieves impressive inference speeds and outperforms existing dense models.'}, 'zh': {'title': 'æ··åˆåˆ†ç»„ä¸“å®¶ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸æ€§èƒ½', 'desc': 'æ··åˆåˆ†ç»„ä¸“å®¶ï¼ˆMoGEï¼‰æ˜¯ä¸€ç§æ”¹è¿›çš„æ¨¡å‹æ¶æ„ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸“å®¶è´Ÿè½½å¹³è¡¡å’Œæ‰§è¡Œæ•ˆç‡ã€‚é€šè¿‡å°†ä¸“å®¶åˆ†ç»„é€‰æ‹©ï¼ŒMoGEç¡®ä¿æ¯ä¸ªè¾“å…¥ä»¤ç‰Œæ¿€æ´»çš„ä¸“å®¶æ•°é‡ç›¸ç­‰ï¼Œä»è€Œå‡å°‘äº†ç³»ç»Ÿåœ¨å¹¶è¡Œè¿è¡Œæ—¶çš„æ•ˆç‡æŸå¤±ã€‚æˆ‘ä»¬åœ¨Ascend NPUä¸Šæ„å»ºäº†Pangu Pro MoEï¼Œè¿™æ˜¯ä¸€ç§åŸºäºMoGEçš„ç¨€ç–æ¨¡å‹ï¼Œå…·æœ‰720äº¿ä¸ªå‚æ•°ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoGEåœ¨æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸­éƒ½èƒ½å®ç°æ›´å¥½çš„ä¸“å®¶è´Ÿè½½å¹³è¡¡å’Œæ›´é«˜çš„æ‰§è¡Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13585', 'title': 'MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention', 'url': 'https://huggingface.co/papers/2506.13585', 'abstract': "A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.", 'score': 184, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '05163c188bd37051', 'authors': ['MiniMax', ':', 'Aili Chen', 'Aonian Li', 'Bangwei Gong', 'Binyang Jiang', 'Bo Fei', 'Bo Yang', 'Boji Shan', 'Changqing Yu', 'Chao Wang', 'Cheng Zhu', 'Chengjun Xiao', 'Chengyu Du', 'Chi Zhang', 'Chu Qiao', 'Chunhao Zhang', 'Chunhui Du', 'Congchao Guo', 'Da Chen', 'Deming Ding', 'Dianjun Sun', 'Dong Li', 'Enwei Jiao', 'Haigang Zhou', 'Haimo Zhang', 'Han Ding', 'Haohai Sun', 'Haoyu Feng', 'Huaiguang Cai', 'Haichao Zhu', 'Jian Sun', 'Jiaqi Zhuang', 'Jiaren Cai', 'Jiayuan Song', 'Jin Zhu', 'Jingyang Li', 'Jinhao Tian', 'Jinli Liu', 'Junhao Xu', 'Junjie Yan', 'Junteng Liu', 'Junxian He', 'Kaiyi Feng', 'Ke Yang', 'Kecheng Xiao', 'Le Han', 'Leyang Wang', 'Lianfei Yu', 'Liheng Feng', 'Lin Li', 'Lin Zheng', 'Linge Du', 'Lingyu Yang', 'Lunbin Zeng', 'Minghui Yu', 'Mingliang Tao', 'Mingyuan Chi', 'Mozhi Zhang', 'Mujie Lin', 'Nan Hu', 'Nongyu Di', 'Peng Gao', 'Pengfei Li', 'Pengyu Zhao', 'Qibing Ren', 'Qidi Xu', 'Qile Li', 'Qin Wang', 'Rong Tian', 'Ruitao Leng', 'Shaoxiang Chen', 'Shaoyu Chen', 'Shengmin Shi', 'Shitong Weng', 'Shuchang Guan', 'Shuqi Yu', 'Sichen Li', 'Songquan Zhu', 'Tengfei Li', 'Tianchi Cai', 'Tianrun Liang', 'Weiyu Cheng', 'Weize Kong', 'Wenkai Li', 'Xiancai Chen', 'Xiangjun Song', 'Xiao Luo', 'Xiao Su', 'Xiaobo Li', 'Xiaodong Han', 'Xinzhu Hou', 'Xuan Lu', 'Xun Zou', 'Xuyang Shen', 'Yan Gong', 'Yan Ma', 'Yang Wang', 'Yiqi Shi', 'Yiran Zhong', 'Yonghong Duan', 'Yongxiang Fu', 'Yongyi Hu', 'Yu Gao', 'Yuanxiang Fan', 'Yufeng Yang', 'Yuhao Li', 'Yulin Hu', 'Yunan Huang', 'Yunji Li', 'Yunzhi Xu', 'Yuxin Mao', 'Yuxuan Shi', 'Yuze Wenren', 'Zehan Li', 'Zelin Li', 'Zhanxu Tian', 'Zhengmao Zhu', 'Zhenhua Fan', 'Zhenzhen Wu', 'Zhichao Xu', 'Zhihang Yu', 'Zhiheng Lyu', 'Zhuo Jiang', 'Zibo Gao', 'Zijia Wu', 'Zijian Song', 'Zijun Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.13585.jpg', 'data': {'categories': ['#rl', '#training', '#open_source', '#architecture', '#reasoning', '#long_context', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MiniMax-M1: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'MiniMax-M1 - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¼Ğ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. MiniMax-M1 Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼.'}, 'en': {'title': 'Revolutionizing Long-Input Processing with MiniMax-M1', 'desc': 'MiniMax-M1 is a groundbreaking hybrid-attention reasoning model that utilizes a Mixture-of-Experts architecture and a lightning attention mechanism to efficiently handle long input sequences. With 456 billion parameters and the ability to process up to 1 million tokens, it significantly outperforms previous models in context length. The model is trained using a novel reinforcement learning algorithm called CISPO, which enhances training efficiency by clipping importance sampling weights. MiniMax-M1 demonstrates superior performance in complex tasks, particularly in software engineering and long-context applications, making it a valuable tool for various AI challenges.'}, 'zh': {'title': 'é«˜æ•ˆé•¿è¾“å…¥å¤„ç†çš„æ··åˆæ³¨æ„åŠ›æ¨¡å‹', 'desc': 'MiniMax-M1æ˜¯ä¸€ç§æ··åˆæ³¨æ„åŠ›æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„å’Œé—ªç”µæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†é•¿è¾“å…¥å’Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åŸºäºä¹‹å‰çš„MiniMax-Text-01æ¨¡å‹ï¼Œå…·æœ‰4560äº¿ä¸ªå‚æ•°ï¼Œå¹¶æ”¯æŒé«˜è¾¾100ä¸‡ä¸ªtokençš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚MiniMax-M1åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨è½¯ä»¶å·¥ç¨‹å’Œå·¥å…·åˆ©ç”¨æ–¹é¢ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•CISPOï¼Œè¿›ä¸€æ­¥æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10521', 'title': "Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning", 'url': 'https://huggingface.co/papers/2506.10521', 'abstract': "Scientists' First Exam (SFE) benchmark assesses scientific cognitive capacities of Multimodal Large Language Models through perception, understanding, and comparative reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.", 'score': 60, 'issue_id': 4325, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '3e2672b026127b5e', 'authors': ['Yuhao Zhou', 'Yiheng Wang', 'Xuming He', 'Ruoyao Xiao', 'Zhiwei Li', 'Qiantai Feng', 'Zijie Guo', 'Yuejin Yang', 'Hao Wu', 'Wenxuan Huang', 'Jiaqi Wei', 'Dan Si', 'Xiuqi Yao', 'Jia Bu', 'Haiwen Huang', 'Tianfan Fu', 'Shixiang Tang', 'Ben Fei', 'Dongzhan Zhou', 'Fenghua Ling', 'Yan Lu', 'Siqi Sun', 'Chenhui Li', 'Guanjie Zheng', 'Jiancheng Lv', 'Wenlong Zhang', 'Lei Bai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10521.jpg', 'data': {'categories': ['#reasoning', '#science', '#multimodal', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': "Ğ£Ñ‡Ñ‘Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Scientists' First Exam (SFE) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). SFE Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 830 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 66 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¿ÑÑ‚Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4 Ğ¸ InternVL-3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 34.08% Ğ¸ 26.52% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° SFE, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ MLLM Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ."}, 'en': {'title': 'Enhancing Scientific Discovery with MLLMs: The SFE Benchmark', 'desc': "The Scientists' First Exam (SFE) benchmark evaluates the cognitive abilities of Multimodal Large Language Models (MLLMs) in scientific contexts. It focuses on three key areas: perception of scientific signals, understanding of scientific attributes, and comparative reasoning. The benchmark includes 830 expert-verified visual question-answering pairs across various multimodal tasks in five important scientific disciplines. Results show that leading models like GPT-o3 and InternVL-3 perform below expectations, indicating a need for improvement in their scientific reasoning capabilities."}, 'zh': {'title': 'ç§‘å­¦è®¤çŸ¥èƒ½åŠ›çš„æ–°è¯„ä¼°æ ‡å‡†', 'desc': 'ç§‘å­¦å®¶é¦–æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†æµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼Œä¸»è¦é€šè¿‡æ„ŸçŸ¥ã€ç†è§£å’Œæ¯”è¾ƒæ¨ç†ä¸‰ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚å½“å‰çš„ç§‘å­¦åŸºå‡†ä¸»è¦å…³æ³¨MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œæœªèƒ½å……åˆ†è¯„ä¼°å…¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚SFEåŸºå‡†åŒ…å«830ä¸ªç»è¿‡ä¸“å®¶éªŒè¯çš„è§†è§‰é—®ç­”å¯¹ï¼Œæ¶µç›–66ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠäº”ä¸ªé«˜ä»·å€¼å­¦ç§‘ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹åœ¨SFEä¸Šçš„è¡¨ç°ä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ï¼Œè¡¨æ˜MLLMsåœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11763', 'title': 'DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents', 'url': 'https://huggingface.co/papers/2506.11763', 'abstract': "DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.", 'score': 42, 'issue_id': 4324, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '197213635094ee83', 'authors': ['Mingxuan Du', 'Benfeng Xu', 'Chiwei Zhu', 'Xiaorui Wang', 'Zhendong Mao'], 'affiliations': ['MetastoneTechnology, Beijing, China', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.11763.jpg', 'data': {'categories': ['#open_source', '#science', '#agents', '#alignment', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'DeepResearch Bench - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 100 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ PhD Ğ¿Ğ¾ 22 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‰Ğ¸ĞµÑÑ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼: Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. DeepResearch Bench Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Benchmarking Deep Research Agents for Superior Research Quality', 'desc': "DeepResearch Bench is a new framework designed to evaluate the performance of Deep Research Agents (DRAs) in generating high-quality research outputs. It includes 100 PhD-level tasks across 22 fields, allowing for a comprehensive assessment of DRAs' capabilities in information retrieval and report synthesis. The framework introduces two innovative evaluation methods: one that uses reference-based criteria to judge report quality, and another that measures citation effectiveness and accuracy. By providing these tools, DeepResearch Bench aims to enhance the development of LLM-based agents and improve their research efficiency."}, 'zh': {'title': 'è¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†çš„æ–°åŸºå‡†', 'desc': 'DeepResearch Benchæ˜¯ä¸€ä¸ªåŸºå‡†æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†åœ¨ç ”ç©¶è´¨é‡å’Œä¿¡æ¯æ£€ç´¢å‡†ç¡®æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«100ä¸ªç”±é¢†åŸŸä¸“å®¶ç²¾å¿ƒè®¾è®¡çš„åšå£«çº§ç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–22ä¸ªä¸åŒé¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•æ¥è¯„ä¼°è¿™äº›ä»£ç†çš„èƒ½åŠ›ï¼Œä¸€ç§æ˜¯åŸºäºå‚è€ƒçš„è¯„ä¼°æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯è¯„ä¼°ä¿¡æ¯æ£€ç´¢å’Œå¼•ç”¨å‡†ç¡®æ€§çš„æ¡†æ¶ã€‚é€šè¿‡å¼€æºDeepResearch BenchåŠå…¶å…³é”®ç»„ä»¶ï¼Œæˆ‘ä»¬å¸Œæœ›åŠ é€ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12571', 'title': 'DoTA-RAG: Dynamic of Thought Aggregation RAG', 'url': 'https://huggingface.co/papers/2506.12571', 'abstract': "DoTA-RAG improves retrieval and generation accuracy over massive web datasets using a dynamic routing pipeline and optimized embedding models, achieving high correctness scores while maintaining low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a retrieval-augmented generation system optimized for high-throughput, large-scale web knowledge indexes. Traditional RAG pipelines often suffer from high latency and limited accuracy over massive, diverse datasets. DoTA-RAG addresses these challenges with a three-stage pipeline: query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. We further enhance retrieval by evaluating and selecting a superior embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we create a diverse Q&A dataset of 500 questions generated via the DataMorgana setup across a broad range of WebOrganizer topics and formats. DoTA-RAG improves the answer correctness score from 0.752 (baseline, using LiveRAG pre-built vector store) to 1.478 while maintaining low latency, and it achieves a 0.929 correctness score on the Live Challenge Day. These results highlight DoTA-RAG's potential for practical deployment in domains requiring fast, reliable access to large and evolving knowledge sources.", 'score': 35, 'issue_id': 4332, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 Ğ¸ÑĞ½Ñ', 'en': 'June 14', 'zh': '6æœˆ14æ—¥'}, 'hash': '3676dd66819fb868', 'authors': ['Saksorn Ruangtanusak', 'Natthapath Rungseesiripak', 'Peerawat Rojratchadakorn', 'Monthol Charattrakool', 'Natapong Nitarach'], 'affiliations': ['SCB 10X Bangkok, Thailand', 'SCBX Bangkok, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2506.12571.jpg', 'data': {'categories': ['#optimization', '#survey', '#data', '#dataset', '#rag'], 'emoji': 'ğŸŒ', 'ru': {'title': 'DoTA-RAG: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° DoTA-RAG, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²ĞµĞ±-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². DoTA-RAG Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€: Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸.'}, 'en': {'title': 'Boosting Retrieval and Generation with DoTA-RAG!', 'desc': 'DoTA-RAG is a new system designed to improve how we retrieve and generate information from large web datasets. It uses a three-stage process that includes rewriting queries, dynamically routing them to specialized sub-indexes, and retrieving and ranking results in multiple stages. By optimizing the embedding models and re-embedding a large dataset, DoTA-RAG significantly increases the accuracy of answers while keeping response times low. This makes it a promising tool for applications that need quick and reliable access to vast amounts of information.'}, 'zh': {'title': 'DoTA-RAGï¼šå¿«é€Ÿå¯é çš„çŸ¥è¯†æ£€ç´¢ä¸ç”Ÿæˆç³»ç»Ÿ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†DoTA-RAGï¼ˆåŠ¨æ€æ€ç»´èšåˆRAGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¼˜åŒ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼Œæ—¨åœ¨å¤„ç†å¤§è§„æ¨¡ç½‘ç»œçŸ¥è¯†ç´¢å¼•ã€‚ä¼ ç»Ÿçš„RAGç®¡é“åœ¨å¤„ç†åºå¤§å¤šæ ·çš„æ•°æ®é›†æ—¶ï¼Œå¸¸å¸¸é¢ä¸´é«˜å»¶è¿Ÿå’Œå‡†ç¡®æ€§ä¸è¶³çš„é—®é¢˜ã€‚DoTA-RAGé€šè¿‡æŸ¥è¯¢é‡å†™ã€åŠ¨æ€è·¯ç”±åˆ°ä¸“ä¸šå­ç´¢å¼•ä»¥åŠå¤šé˜¶æ®µæ£€ç´¢å’Œæ’åçš„ä¸‰é˜¶æ®µç®¡é“æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿåœ¨FineWeb-10BTè¯­æ–™åº“ä¸Šé‡æ–°åµŒå…¥å¹¶é€‰æ‹©äº†ä¼˜è¶Šçš„åµŒå…¥æ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç­”æ¡ˆçš„æ­£ç¡®æ€§åˆ†æ•°ï¼ŒåŒæ—¶ä¿æŒäº†ä½å»¶è¿Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13654', 'title': 'Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning', 'url': 'https://huggingface.co/papers/2506.13654', 'abstract': 'Ego-R1, a reinforcement learning-based framework, uses a structured tool-augmented chain-of-thought process to reason over ultra-long egocentric videos, achieving better performance than existing methods by extending time coverage to a week.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.', 'score': 32, 'issue_id': 4326, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '52b4ddc8a62e646b', 'authors': ['Shulin Tian', 'Ruiqi Wang', 'Hongming Guo', 'Penghao Wu', 'Yuhao Dong', 'Xiuying Wang', 'Jingkang Yang', 'Hao Zhang', 'Hongyuan Zhu', 'Ziwei Liu'], 'affiliations': ['A*STAR, Singapore', 'S-Lab, Nanyang Technological University', 'Shanghai AI Lab', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13654.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#training', '#multimodal', '#long_context', '#rl'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ego-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ego-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ (Chain-of-Tool-Thought). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸. Ego-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ…Ğ²Ğ°Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ğ´Ğ¾ Ğ½ĞµĞ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Revolutionizing Video Understanding with Ego-R1', 'desc': 'Ego-R1 is a new framework designed to analyze ultra-long egocentric videos, which can last for days or even weeks. It employs a structured Chain-of-Tool-Thought (CoTT) process, allowing the system to break down complex reasoning tasks into manageable steps. The framework is powered by a reinforcement learning agent that learns to select and use specific tools for each step, enhancing its ability to answer questions about the video content. By training on a specially created dataset and evaluating on a new benchmark, Ego-R1 demonstrates improved performance in understanding long-duration videos compared to existing methods.'}, 'zh': {'title': 'Ego-R1ï¼šè¶…é•¿è§†é¢‘æ¨ç†çš„æ–°çªç ´', 'desc': 'Ego-R1æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¤„ç†è¶…é•¿çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§ç»“æ„åŒ–çš„å·¥å…·å¢å¼ºæ€ç»´é“¾ï¼ˆCoTTï¼‰è¿‡ç¨‹ï¼Œå°†å¤æ‚çš„æ¨ç†åˆ†è§£ä¸ºæ¨¡å—åŒ–æ­¥éª¤ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„Ego-R1ä»£ç†èƒ½å¤ŸåŠ¨æ€åœ°æå‡ºé€æ­¥å·¥å…·ï¼Œä»¥åº”å¯¹é•¿æ—¶é—´èŒƒå›´å†…çš„æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEgo-R1åœ¨ç†è§£è¶…é•¿è§†é¢‘æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ—¶é—´è¦†ç›–èŒƒå›´ä»å‡ å°æ—¶æ‰©å±•åˆ°ä¸€å‘¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08343', 'title': 'Wait, We Don\'t Need to "Wait"! Removing Thinking Tokens Improves\n  Reasoning Efficiency', 'url': 'https://huggingface.co/papers/2506.08343', 'abstract': 'NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.', 'score': 29, 'issue_id': 4324, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'bc2b3e7cb2a8d002', 'authors': ['Chenlong Wang', 'Yuanning Feng', 'Dongping Chen', 'Zhaoyang Chu', 'Ranjay Krishna', 'Tianyi Zhou'], 'affiliations': ['University College London', 'University of Maryland', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.08343.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#inference', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… ÑĞ»Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ NoWait, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 27-51% Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ´ĞµÑÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. NoWait Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ….'}, 'en': {'title': 'NoWait: Streamlining Multimodal Reasoning for Efficiency', 'desc': "The paper introduces NoWait, a method that improves the efficiency of multimodal reasoning models by suppressing explicit self-reflection tokens during inference. These tokens, like 'Wait' and 'Hmm', often lead to unnecessary verbosity and can slow down the reasoning process. By removing these tokens, NoWait significantly shortens the reasoning paths while maintaining the effectiveness of the model. The results from various benchmarks demonstrate that this approach can reduce the length of reasoning trajectories by up to 51%, making it a valuable enhancement for large reasoning models."}, 'zh': {'title': 'NoWaitï¼šæå‡å¤šæ¨¡æ€æ¨ç†æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'NoWaitæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­æŠ‘åˆ¶æ˜¾å¼è‡ªæˆ‘åæ€çš„æ ‡è®°ï¼ˆå¦‚â€œç­‰ä¸€ä¸‹â€å’Œâ€œå—¯â€ï¼‰ï¼Œæ¥æé«˜å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡ï¼Œè€Œä¸é™ä½æ¨¡å‹çš„å®ç”¨æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ¨ç†æ—¶å¸¸å¸¸ä¼šå‡ºç°è¿‡åº¦æ€è€ƒï¼Œå¯¼è‡´è¾“å‡ºå†—é•¿ä¸”é‡å¤ï¼Œå½±å“æ•ˆç‡ã€‚é€šè¿‡åœ¨åä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œå¹¿æ³›å®éªŒï¼ŒNoWaitèƒ½å¤Ÿå°†æ€ç»´é“¾çš„é•¿åº¦å‡å°‘27%åˆ°51%ã€‚å› æ­¤ï¼ŒNoWaitä¸ºé«˜æ•ˆä¸”ä¿æŒå®ç”¨æ€§çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10055', 'title': 'TaskCraft: Automated Generation of Agentic Tasks', 'url': 'https://huggingface.co/papers/2506.10055', 'abstract': 'TaskCraft automates the generation of scalable, multi-tool, and complex agentic tasks to enhance prompt optimization and fine-tuning of agentic models.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce TaskCraft, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.', 'score': 24, 'issue_id': 4329, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'cc5a94a8870a39e9', 'authors': ['Dingfeng Shi', 'Jingyi Cao', 'Qianben Chen', 'Weichen Sun', 'Weizhen Li', 'Hongxuan Lu', 'Fangchen Dong', 'Tianrui Qin', 'King Zhu', 'Minghao Yang', 'Jian Yang', 'Ge Zhang', 'Jiaheng Liu', 'Changwang Zhang', 'Jun Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO'], 'pdf_title_img': 'assets/pdf/title_img/2506.10055.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#optimization', '#training', '#agents', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'TaskCraft: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'TaskCraft - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. TaskCraft Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Automating Complex Tasks for Smarter AI Agents', 'desc': 'TaskCraft is a system designed to automatically create complex tasks that require agents to use multiple tools and solve problems independently. It addresses the limitations of current instruction data, which often lacks examples of tool interaction and relies on expensive human-created benchmarks. By generating scalable and verifiable agentic tasks, TaskCraft enhances the training and fine-tuning of AI models, making them more effective in handling multi-step challenges. The system has produced a large dataset of around 36,000 tasks of varying difficulty to aid future research in improving agent performance.'}, 'zh': {'title': 'TaskCraftï¼šè‡ªåŠ¨åŒ–ç”Ÿæˆå¤æ‚ä»£ç†ä»»åŠ¡çš„åˆ©å™¨', 'desc': 'TaskCraft æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–å·¥å…·ï¼Œæ—¨åœ¨ç”Ÿæˆå¯æ‰©å±•çš„å¤šå·¥å…·å¤æ‚ä»»åŠ¡ï¼Œä»¥ä¼˜åŒ–ä»£ç†æ¨¡å‹çš„æç¤ºå’Œå¾®è°ƒã€‚ä»£ç†ä»»åŠ¡éœ€è¦å¤šæ­¥éª¤çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€å·¥å…·ä½¿ç”¨å’Œè‡ªé€‚åº”æ¨ç†ï¼Œè¿™åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººå·¥æ™ºèƒ½çš„å‘å±•ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç°æœ‰çš„æŒ‡ä»¤æ•°æ®ç¼ºä¹å·¥å…·äº¤äº’ï¼Œè€Œå½“å‰çš„ä»£ç†åŸºå‡†ä¾èµ–æ˜‚è´µçš„äººç±»æ ‡æ³¨ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚TaskCraft é€šè¿‡æ·±åº¦å’Œå®½åº¦æ‰©å±•æ¥ç”Ÿæˆç»“æ„å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æä¾›çº¦36,000ä¸ªä¸åŒéš¾åº¦çš„åˆæˆä»»åŠ¡æ•°æ®é›†ï¼Œä»¥æ”¯æŒæœªæ¥çš„ä»£ç†è°ƒä¼˜å’Œè¯„ä¼°ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09482', 'title': 'Marrying Autoregressive Transformer and Diffusion with Multi-Reference\n  Autoregression', 'url': 'https://huggingface.co/papers/2506.09482', 'abstract': "TransDiff, combining an Autoregressive Transformer and diffusion models, achieves superior image generation performance and speed, while Multi-Reference Autoregression further enhances its quality and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TransDiff, the first image generation model that marries Autoregressive (AR) Transformer with diffusion models. In this joint modeling framework, TransDiff encodes labels and images into high-level semantic features and employs a diffusion model to estimate the distribution of image samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms other image generation models based on standalone AR Transformer or diffusion models. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID) of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster inference latency compared to state-of-the-art methods based on AR Transformer and x112 faster inference compared to diffusion-only models. Furthermore, building on the TransDiff model, we introduce a novel image generation paradigm called Multi-Reference Autoregression (MRAR), which performs autoregressive generation by predicting the next image. MRAR enables the model to reference multiple previously generated images, thereby facilitating the learning of more diverse representations and improving the quality of generated images in subsequent iterations. By applying MRAR, the performance of TransDiff is improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open up a new frontier in the field of image generation.", 'score': 23, 'issue_id': 4336, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '44097add463b0792', 'authors': ['Dingcheng Zhen', 'Qian Qiao', 'Tan Yu', 'Kangxi Wu', 'Ziwei Zhang', 'Siyuan Liu', 'Shunshun Yin', 'Ming Tao'], 'affiliations': ['Soul AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.09482.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv', '#benchmark', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'TransDiff: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'TransDiff - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet 256x256. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ (MRAR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'TransDiff: Revolutionizing Image Generation with Speed and Quality', 'desc': 'TransDiff is a novel image generation model that integrates Autoregressive Transformers with diffusion models to enhance both performance and speed. It effectively encodes images and labels into high-level features, using a diffusion model to sample images from a learned distribution. On the ImageNet benchmark, TransDiff achieves impressive metrics, including a FrÃ©chet Inception Distance (FID) of 1.61 and a significant reduction in inference time compared to existing models. Additionally, the introduction of Multi-Reference Autoregression (MRAR) allows the model to generate images by referencing multiple previous outputs, leading to improved diversity and quality in generated images.'}, 'zh': {'title': 'TransDiffï¼šå›¾åƒç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'TransDiffæ˜¯ä¸€ç§ç»“åˆè‡ªå›å½’å˜æ¢å™¨å’Œæ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„ç”Ÿæˆæ€§èƒ½å’Œé€Ÿåº¦ã€‚è¯¥æ¨¡å‹é€šè¿‡ç¼–ç æ ‡ç­¾å’Œå›¾åƒä¸ºé«˜çº§è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¥ä¼°è®¡å›¾åƒæ ·æœ¬çš„åˆ†å¸ƒã€‚åœ¨ImageNet 256x256åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTransDiffçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå•ç‹¬ä½¿ç”¨è‡ªå›å½’å˜æ¢å™¨æˆ–æ‰©æ•£æ¨¡å‹çš„å…¶ä»–å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡å¼•å…¥å¤šå‚è€ƒè‡ªå›å½’ï¼ˆMRARï¼‰æ–¹æ³•ï¼ŒTransDiffè¿›ä¸€æ­¥æé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13759', 'title': 'Discrete Diffusion in Large Language and Multimodal Models: A Survey', 'url': 'https://huggingface.co/papers/2506.13759', 'abstract': 'Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.   The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.   In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.   Paper collection: https://github.com/LiQiiiii/DLLM-Survey', 'score': 21, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '0a523ab9b7563360', 'authors': ['Runpeng Yu', 'Qi Li', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.13759.jpg', 'data': {'categories': ['#math', '#training', '#diffusion', '#inference', '#multimodal', '#survey'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLMs) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dMLLMs). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, dLLMs Ğ¸ dMLLMs Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ dLLMs Ğ¸ dMLLMs Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….'}, 'en': {'title': 'Accelerating Language Generation with Discrete Diffusion Models', 'desc': 'This paper surveys Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs), highlighting their advantages over traditional autoregressive models. dLLMs and dMLLMs utilize a parallel decoding approach with full attention and denoising strategies, allowing for faster generation and improved output control. The paper reviews the historical development, mathematical foundations, and key techniques for training these models, as well as their applications in various domains. It also discusses the future potential of dLLMs and dMLLMs in advancing machine learning research and deployment.'}, 'zh': {'title': 'ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼šåŠ é€Ÿç”Ÿæˆä¸æ§åˆ¶çš„æœªæ¥', 'desc': 'æœ¬æ–‡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥äº†ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰å’Œç¦»æ•£æ‰©æ•£å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆdMLLMsï¼‰ã€‚ä¸è‡ªå›å½’æ¨¡å‹ä¸åŒï¼ŒdLLMså’ŒdMLLMsé‡‡ç”¨å¤šæ ‡è®°å¹¶è¡Œè§£ç çš„èŒƒå¼ï¼Œåˆ©ç”¨å…¨æ³¨æ„åŠ›æœºåˆ¶å’Œå»å™ªç”Ÿæˆç­–ç•¥ï¼Œä»è€Œå®ç°å¹¶è¡Œç”Ÿæˆå’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚è¿™ç§æ–°æ–¹æ³•ä½¿å¾—ç»†ç²’åº¦çš„è¾“å‡ºæ§åˆ¶å’ŒåŠ¨æ€å“åº”æ„ŸçŸ¥æˆä¸ºå¯èƒ½ï¼Œè¿™åœ¨è‡ªå›å½’æ¨¡å‹ä¸­æ˜¯éš¾ä»¥å®ç°çš„ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒdLLMså’ŒdMLLMsåœ¨æ¨ç†é€Ÿåº¦ä¸Šå¯å®ç°é«˜è¾¾10å€çš„åŠ é€Ÿï¼Œä¸”åœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13750', 'title': 'Test3R: Learning to Reconstruct 3D at Test Time', 'url': 'https://huggingface.co/papers/2506.13750', 'abstract': 'Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, the reliance on pairwise prediction and the limited generalization capability inherently restrict the global geometric consistency. In this work, we introduce Test3R, a surprisingly simple test-time learning technique that significantly boosts geometric accuracy. Using image triplets (I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and (I_1,I_3). The core idea is to optimize the network at test time via a self-supervised objective: maximizing the geometric consistency between these two reconstructions relative to the common image I_1. This ensures the model produces cross-pair consistent outputs, regardless of the inputs. Extensive experiments demonstrate that our technique significantly outperforms previous state-of-the-art methods on the 3D reconstruction and multi-view depth estimation tasks. Moreover, it is universally applicable and nearly cost-free, making it easily applied to other models and implemented with minimal test-time training overhead and parameter footprint. Code is available at https://github.com/nopQAQ/Test3R.', 'score': 18, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '68d521856e78273a', 'authors': ['Yuheng Yuan', 'Qiuhong Shen', 'Shizun Wang', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.13750.jpg', 'data': {'categories': ['#3d', '#training', '#optimization'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Test3R: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Test3R - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Test3R Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹.'}, 'en': {'title': 'Boosting 3D Reconstruction Accuracy with Test3R', 'desc': "Test3R is a novel test-time learning approach designed to improve the accuracy of 3D reconstruction by leveraging self-supervised learning on image triplets. It addresses the limitations of traditional dense matching methods that rely on pairwise predictions, which can lead to inconsistencies in global geometry. By optimizing the network's output during testing, Test3R ensures that reconstructions from different image pairs maintain geometric consistency relative to a common reference image. This technique not only enhances performance on 3D reconstruction tasks but is also easy to implement and applicable to various models with minimal additional training requirements."}, 'zh': {'title': 'Test3Rï¼šæå‡3Dé‡å»ºç²¾åº¦çš„ç®€å•æ–¹æ³•', 'desc': 'Test3Ræ˜¯ä¸€ç§ç”¨äº3Dé‡å»ºçš„æµ‹è¯•æ—¶å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ä¼˜åŒ–ç½‘ç»œä¸€è‡´æ€§ï¼Œä»è€Œæé«˜å‡ ä½•ç²¾åº¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒä¸‰å…ƒç»„ç”Ÿæˆé‡å»ºï¼Œç¡®ä¿ä¸åŒå›¾åƒå¯¹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚Test3Rçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æµ‹è¯•æ—¶æœ€å¤§åŒ–é‡å»ºä¹‹é—´çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºåœ¨ä¸åŒè¾“å…¥ä¸‹ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTest3Råœ¨3Dé‡å»ºå’Œå¤šè§†è§’æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸”é€‚ç”¨æ€§å¹¿æ³›ï¼Œå‡ ä¹ä¸å¢åŠ æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11991', 'title': 'VGR: Visual Grounded Reasoning', 'url': 'https://huggingface.co/papers/2506.11991', 'abstract': 'VGR, a novel multimodal large language model, improves visual reasoning by detecting relevant image regions and integrating them into the reasoning process, outperforming existing models on multimodal benchmarks with reduced resource usage.  \t\t\t\t\tAI-generated summary \t\t\t\t In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches predominantly rely on reasoning on pure language space, which inherently suffers from language bias and is largely confined to math or science domains. This narrow focus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper introduces VGR, a novel reasoning multimodal large language model (MLLM) with enhanced fine-grained visual perception capabilities. Unlike traditional MLLMs that answer the question or reasoning solely on the language space, our VGR first detects relevant regions that may help to solve problems, and then provides precise answers based on replayed image regions. To achieve this, we conduct a large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed vision grounding and language deduction. The inference pipeline of VGR allows the model to choose bounding boxes for visual reference and a replay stage is introduced to integrates the corresponding regions into the reasoning process, enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show that VGR achieves superior performance on multi-modal benchmarks requiring comprehensive image detail understanding. Compared to the baseline, VGR uses only 30\\% of the image token count while delivering scores of +4.1 on MMStar, +7.1 on AI2D, and a +12.9 improvement on ChartQA.', 'score': 16, 'issue_id': 4330, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '48fd0bae72ad378e', 'authors': ['Jiacong Wang', 'Zijian Kang', 'Haochen Wang', 'Haiyong Jiang', 'Jiawen Li', 'Bohong Wu', 'Ya Wang', 'Jiao Ran', 'Xiao Liang', 'Chao Feng', 'Jun Xiao'], 'affiliations': ['ByteDance Inc.', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.11991.jpg', 'data': {'categories': ['#reasoning', '#games', '#benchmark', '#dataset', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'VGR: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'VGR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ñ… Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ´ĞµĞ´ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. VGR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'VGR: Revolutionizing Visual Reasoning with Multimodal Integration', 'desc': 'The paper presents VGR, a new multimodal large language model designed to enhance visual reasoning by effectively integrating relevant image regions into its reasoning process. Unlike traditional models that rely solely on language, VGR detects important areas in images to improve its understanding and answers. This model is trained on a large-scale dataset that combines visual grounding with language reasoning, allowing it to perform better on complex visual tasks. VGR demonstrates significant improvements on multimodal benchmarks while using fewer resources, showcasing its efficiency and effectiveness in visual reasoning.'}, 'zh': {'title': 'VGRï¼šæå‡è§†è§‰æ¨ç†çš„æ–°å‹å¤šæ¨¡æ€æ¨¡å‹', 'desc': 'VGRæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ£€æµ‹ç›¸å…³å›¾åƒåŒºåŸŸæ¥æ”¹å–„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿæ¨¡å‹ä¸åŒï¼ŒVGRåœ¨æ¨ç†è¿‡ç¨‹ä¸­é¦–å…ˆè¯†åˆ«å¯èƒ½æœ‰åŠ©äºè§£å†³é—®é¢˜çš„å›¾åƒåŒºåŸŸï¼Œç„¶ååŸºäºè¿™äº›åŒºåŸŸæä¾›å‡†ç¡®çš„ç­”æ¡ˆã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†ä¸€ç§åä¸ºVGR-SFTçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç»“åˆäº†è§†è§‰åŸºç¡€å’Œè¯­è¨€æ¨ç†çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVGRåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶èµ„æºä½¿ç”¨æ•ˆç‡æ›´é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06962', 'title': 'AR-RAG: Autoregressive Retrieval Augmentation for Image Generation', 'url': 'https://huggingface.co/papers/2506.06962', 'abstract': 'Autoregressive Retrieval Augmentation enhances image generation through context-aware patch-level retrievals, improving performance over existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm that enhances image generation by autoregressively incorporating knearest neighbor retrievals at the patch level. Unlike prior methods that perform a single, static retrieval before generation and condition the entire generation on fixed reference images, AR-RAG performs context-aware retrievals at each generation step, using prior-generated patches as queries to retrieve and incorporate the most relevant patch-level visual references, enabling the model to respond to evolving generation needs while avoiding limitations (e.g., over-copying, stylistic bias, etc.) prevalent in existing methods. To realize AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in Decoding (DAiD), a training-free plug-and-use decoding strategy that directly merges the distribution of model-predicted patches with the distribution of retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning method that progressively smooths the features of retrieved patches via multi-scale convolution operations and leverages them to augment the image generation process. We validate the effectiveness of AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and DPG-Bench, demonstrating significant performance gains over state-of-the-art image generation models.', 'score': 16, 'issue_id': 4338, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': '0d850cd9a51e6a48', 'authors': ['Jingyuan Qi', 'Zhiyang Xu', 'Qifan Wang', 'Lifu Huang'], 'affiliations': ['Meta', 'UC Davis', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2506.06962.jpg', 'data': {'categories': ['#cv', '#benchmark', '#games', '#rag', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° (AR-RAG) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², AR-RAG Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ½ĞµĞµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°: Distribution-Augmentation in Decoding (DAiD) Ğ¸ Feature-Augmentation in Decoding (FAiD). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Dynamic Patch Retrieval for Enhanced Image Generation', 'desc': 'The paper presents Autoregressive Retrieval Augmentation (AR-RAG), a new approach to improve image generation by using context-aware retrievals at the patch level. Unlike traditional methods that rely on a single static reference image, AR-RAG dynamically retrieves relevant patches during each generation step, allowing the model to adapt to changing requirements. This method helps to reduce common issues like over-copying and stylistic bias found in previous techniques. The authors introduce two frameworks, DAiD and FAiD, to implement AR-RAG effectively, showing significant improvements in performance on established benchmarks.'}, 'zh': {'title': 'è‡ªå›å½’æ£€ç´¢å¢å¼ºï¼šæå‡å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å›¾åƒç”Ÿæˆå¢å¼ºæ–¹æ³•ï¼Œç§°ä¸ºè‡ªå›å½’æ£€ç´¢å¢å¼ºï¼ˆAR-RAGï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ£€ç´¢ï¼ŒåŠ¨æ€åœ°å¼•å…¥æœ€ç›¸å…³çš„å›¾åƒå—ï¼Œä»è€Œæé«˜ç”Ÿæˆæ•ˆæœã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒAR-RAGé¿å…äº†å›ºå®šå‚è€ƒå›¾åƒçš„é™åˆ¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å˜åŒ–éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§å¹¶è¡Œæ¡†æ¶ï¼Œåˆ†åˆ«æ˜¯è§£ç ä¸­çš„åˆ†å¸ƒå¢å¼ºï¼ˆDAiDï¼‰å’Œè§£ç ä¸­çš„ç‰¹å¾å¢å¼ºï¼ˆFAiDï¼‰ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12915', 'title': 'PersonaFeedback: A Large-scale Human-annotated Benchmark For\n  Personalization', 'url': 'https://huggingface.co/papers/2506.12915', 'abstract': "A new benchmark, PersonaFeedback, evaluates Large Language Models' ability to generate personalized responses given explicit user personas, revealing limitations in current systems.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization.", 'score': 14, 'issue_id': 4325, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': '7f12645dbf1aa58d', 'authors': ['Meiling Tao', 'Chenghao Zhu', 'Dongyi Ding', 'Tiannan Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'South China Agricultural University', 'The Chinese University of Hong Kong, Shenzhen', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12915.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#multimodal', '#benchmark'], 'emoji': 'ğŸ­', 'ru': {'title': 'PersonaFeedback: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PersonaFeedback Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 8298 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ»ĞµĞ³ĞºĞ¸Ğµ, ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ PersonaFeedback. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ retrieval-augmented Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸.'}, 'en': {'title': 'Enhancing Personalization in LLMs with PersonaFeedback', 'desc': 'The paper introduces PersonaFeedback, a new benchmark designed to assess the ability of Large Language Models (LLMs) to generate personalized responses based on explicit user personas. This benchmark addresses the gap in evaluating LLM personalization, which has been overlooked compared to general reasoning capabilities. PersonaFeedback includes 8,298 human-annotated test cases categorized by complexity, revealing that even advanced LLMs struggle with nuanced personalization tasks. The findings highlight the limitations of current models and emphasize the need for improved frameworks in LLM personalization.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–ç”Ÿæˆçš„æ–°åŸºå‡†ï¼šPersonaFeedback', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºPersonaFeedbackï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä¸ªæ€§åŒ–å“åº”çš„èƒ½åŠ›ã€‚éšç€LLMèƒ½åŠ›çš„å¿«é€Ÿæå‡ï¼Œä¸ªæ€§åŒ–ç”Ÿæˆå·²æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ï¼Œä½†ç¼ºä¹é«˜è´¨é‡çš„åŸºå‡†æµ‹è¯•é™åˆ¶äº†è¿™ä¸€é¢†åŸŸçš„è¿›å±•ã€‚PersonaFeedbacké€šè¿‡æä¾›é¢„å®šä¹‰çš„ç”¨æˆ·è§’è‰²å’ŒæŸ¥è¯¢ï¼Œç›´æ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆé’ˆå¯¹ç‰¹å®šç”¨æˆ·è§’è‰²çš„å“åº”èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMåœ¨å¤„ç†å¤æ‚çš„ä¸ªæ€§åŒ–ä»»åŠ¡æ—¶ä¹Ÿå¯èƒ½è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†å½“å‰æ¨¡å‹åœ¨ä¸ªæ€§åŒ–ç”Ÿæˆæ–¹é¢çš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03968', 'title': 'From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding', 'url': 'https://huggingface.co/papers/2506.03968', 'abstract': 'The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.', 'score': 13, 'issue_id': 4324, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '991991c3f686afa8', 'authors': ['Chiwei Zhu', 'Benfeng Xu', 'Xiaorui Wang', 'Zhendong Mao'], 'affiliations': ['Metastone Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03968.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#alignment', '#data', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SynthQuestions, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ Ğ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²ĞµĞ±-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Harnessing Attributed Grounding for Diverse Instruction Generation', 'desc': 'This paper introduces a novel method for generating diverse and complex instruction data for large language models (LLMs) through a technique called attributed grounding. The approach combines a top-down attribution process, which connects real-world instructions to specific user contexts, with a bottom-up synthesis process that creates meaningful instructions from web documents. By leveraging this framework, the authors successfully produce a large dataset of 1 million synthesized instructions, named SynthQuestions, which significantly enhances the performance of LLMs on various benchmarks. The results indicate that as more web data is utilized, the effectiveness of the models continues to improve, showcasing the importance of rich and varied instruction data for model alignment.'}, 'zh': {'title': 'é€šè¿‡å±æ€§åŸºç¡€ç”Ÿæˆå¤æ‚æŒ‡ä»¤æ•°æ®ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å±æ€§åŸºç¡€ç”Ÿæˆå¤šæ ·åŒ–å’Œå¤æ‚çš„æŒ‡ä»¤æ•°æ®çš„æ–¹æ³•ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è‡ªä¸Šè€Œä¸‹çš„å½’å› è¿‡ç¨‹å’Œè‡ªä¸‹è€Œä¸Šçš„åˆæˆè¿‡ç¨‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»çœŸå®æŒ‡ä»¤å’Œç½‘ç»œæ–‡æ¡£ä¸­ç”Ÿæˆæœ‰æ„ä¹‰çš„æŒ‡ä»¤ã€‚é€šè¿‡è¿™ç§æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«100ä¸‡ä¸ªæŒ‡ä»¤çš„æ•°æ®é›†SynthQuestionsï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨ä¸°å¯Œçš„ç½‘ç»œæ–‡æ¡£å¯ä»¥å¤§è§„æ¨¡æ”¶é›†å¤æ‚çš„æŒ‡ä»¤ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13284', 'title': 'AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL Synergy', 'url': 'https://huggingface.co/papers/2506.13284', 'abstract': 'Combining supervised fine-tuning and reinforcement learning enhances reasoning models, especially when optimizing sampling temperature and leveraging strong initial fine-tuning, as demonstrated by the improved AceReason-Nemotron-1.1 model.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we investigate the synergy between supervised fine-tuning (SFT) and reinforcement learning (RL) in developing strong reasoning models. We begin by curating the SFT training data through two scaling strategies: increasing the number of collected prompts and the number of generated responses per prompt. Both approaches yield notable improvements in reasoning performance, with scaling the number of prompts resulting in more substantial gains. We then explore the following questions regarding the synergy between SFT and RL: (i) Does a stronger SFT model consistently lead to better final performance after large-scale RL training? (ii) How can we determine an appropriate sampling temperature during RL training to effectively balance exploration and exploitation for a given SFT initialization? Our findings suggest that (i) holds true, provided effective RL training is conducted, particularly when the sampling temperature is carefully chosen to maintain the temperature-adjusted entropy around 0.3, a setting that strikes a good balance between exploration and exploitation. Notably, the performance gap between initial SFT models narrows significantly throughout the RL process. Leveraging a strong SFT foundation and insights into the synergistic interplay between SFT and RL, our AceReason-Nemotron-1.1 7B model significantly outperforms AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among Qwen2.5-7B-based reasoning models on challenging math and code benchmarks, thereby demonstrating the effectiveness of our post-training recipe. We release the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B', 'score': 11, 'issue_id': 4340, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'a8bb5987fc0b396b', 'authors': ['Zihan Liu', 'Zhuolin Yang', 'Yang Chen', 'Chankyu Lee', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Wei Ping'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2506.13284.jpg', 'data': {'categories': ['#rl', '#dataset', '#reasoning', '#optimization', '#training', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SFT Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğµ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ AceReason-Nemotron-1.1 7B, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ SFT Ğ¸ RL Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Boosting Reasoning Models with SFT and RL Synergy', 'desc': 'This paper explores how combining supervised fine-tuning (SFT) with reinforcement learning (RL) can improve reasoning models. The authors enhance SFT by increasing the number of prompts and responses, leading to better reasoning performance, especially when more prompts are used. They investigate whether a stronger SFT model results in better performance after RL training and how to set the sampling temperature for optimal exploration and exploitation. Their findings confirm that a robust SFT foundation, along with careful temperature management, leads to significant improvements in the AceReason-Nemotron-1.1 model, achieving state-of-the-art results in reasoning tasks.'}, 'zh': {'title': 'ç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨ç†æ¨¡å‹çš„æ€§èƒ½', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¼€å‘å¼ºå¤§æ¨ç†æ¨¡å‹ä¸­çš„ååŒä½œç”¨ã€‚æˆ‘ä»¬é€šè¿‡å¢åŠ æ”¶é›†çš„æç¤ºæ•°é‡å’Œæ¯ä¸ªæç¤ºç”Ÿæˆçš„å“åº”æ•°é‡æ¥ä¼˜åŒ–SFTè®­ç»ƒæ•°æ®ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¼ºå¤§çš„SFTæ¨¡å‹åœ¨ç»è¿‡å¤§è§„æ¨¡RLè®­ç»ƒåï¼Œèƒ½å¤ŸæŒç»­æé«˜æœ€ç»ˆæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é€‚å½“é€‰æ‹©é‡‡æ ·æ¸©åº¦çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬çš„AceReason-Nemotron-1.1æ¨¡å‹åœ¨æ•°å­¦å’Œä»£ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†åè®­ç»ƒæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12450', 'title': 'Language Surgery in Multilingual Large Language Models', 'url': 'https://huggingface.co/papers/2506.12450', 'abstract': "Research confirms natural representation alignment in large language models and introduces Inference-Time Language Control to enhance cross-lingual performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their cross-lingual performance.", 'score': 10, 'issue_id': 4325, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 Ğ¸ÑĞ½Ñ', 'en': 'June 14', 'zh': '6æœˆ14æ—¥'}, 'hash': '6c05e4a1a8b705dc', 'authors': ['Joanito Agili Lopo', 'Muhammad Ravi Shulthan Habibi', 'Tack Hwa Wong', 'Muhammad Ilham Ghozali', 'Fajri Koto', 'Genta Indra Winata', 'Peerat Limkonchotiwat', 'Alham Fikri Aji', 'Samuel Cahyawijaya'], 'affiliations': ['AI Singapore', 'Capital One', 'Cohere', 'Kreasof AI', 'MBZUAI', 'SEACrowd', 'Universitas Indonesia'], 'pdf_title_img': 'assets/pdf/title_img/2506.12450.jpg', 'data': {'categories': ['#alignment', '#machine_translation', '#inference', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM), Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Inference-Time Language Control (ITLC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ¶ÑŠÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ITLC Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ITLC Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Cross-Lingual Performance with Natural Representation Alignment', 'desc': 'This paper explores how large language models (LLMs) naturally align their representations across different languages, particularly in their middle layers. It confirms that this alignment allows for the separation of language-specific and language-agnostic information, which can be manipulated without losing meaning. The authors introduce a new technique called Inference-Time Language Control (ITLC) that uses latent injection to improve control over language generation in cross-lingual contexts. Their experiments show that ITLC effectively reduces language confusion while maintaining semantic integrity, enhancing the overall performance of LLMs in multilingual tasks.'}, 'zh': {'title': 'æå‡è·¨è¯­è¨€æ€§èƒ½çš„æ¨ç†æ—¶è¯­è¨€æ§åˆ¶', 'desc': 'æœ¬ç ”ç©¶ç¡®è®¤äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­è‡ªç„¶å‡ºç°çš„è¡¨ç¤ºå¯¹é½ç°è±¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­é—´å±‚çš„è¡¨ç°ã€‚æˆ‘ä»¬å®è¯éªŒè¯äº†è¿™ç§å¯¹é½çš„å­˜åœ¨ï¼Œå¹¶åˆ†æäº†å…¶ä¸æ˜¾å¼è®¾è®¡çš„å¯¹é½æ¨¡å‹çš„è¡Œä¸ºæ¯”è¾ƒã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”æ¨ç†æ—¶è¯­è¨€æ§åˆ¶ï¼ˆITLCï¼‰ï¼Œå®ƒåˆ©ç”¨æ½œåœ¨æ³¨å…¥æŠ€æœ¯å®ç°ç²¾ç¡®çš„è·¨è¯­è¨€æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒITLCåœ¨ä¿æŒç›®æ ‡è¯­è¨€è¯­ä¹‰å®Œæ•´æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è·¨è¯­è¨€æ§åˆ¶èƒ½åŠ›ï¼Œè§£å†³äº†å½“å‰å¤§å‹LLMsä¸­å­˜åœ¨çš„è¯­è¨€æ··æ·†é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07961', 'title': 'BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.07961', 'abstract': 'BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/', 'score': 10, 'issue_id': 4324, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '3fcf8d6329af3962', 'authors': ['Peiyan Li', 'Yixiang Chen', 'Hongtao Wu', 'Xiao Ma', 'Xiangnan Wu', 'Yan Huang', 'Liang Wang', 'Tao Kong', 'Tieniu Tan'], 'affiliations': ['ByteDance Seed', 'CASIA', 'FiveAges', 'NJU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2506.07961.jpg', 'data': {'categories': ['#rl', '#3d', '#games', '#optimization', '#agents', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'BridgeVLA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ 3D Ğ² 2D', 'desc': 'BridgeVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ 3D-Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ 3D-Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 2D-Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. BridgeVLA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ….'}, 'en': {'title': 'BridgeVLA: Bridging 3D Vision and Action with 2D Heatmaps', 'desc': "BridgeVLA is a novel model that integrates 3D vision with language and action prediction by projecting 3D inputs into 2D images. This approach allows it to utilize 2D heatmaps for more efficient action prediction, enhancing the model's performance in robot manipulation tasks. The model is pre-trained to predict these heatmaps, which helps it learn effectively from fewer samples. Extensive testing shows that BridgeVLA significantly outperforms existing methods in various benchmarks, demonstrating its robustness and efficiency in real-world applications."}, 'zh': {'title': 'BridgeVLAï¼šé«˜æ•ˆçš„3Dè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹', 'desc': 'BridgeVLAæ˜¯ä¸€ç§3Dè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œå®ƒå°†3Dè¾“å…¥æŠ•å½±åˆ°2Då›¾åƒï¼Œå¹¶åˆ©ç”¨2Dçƒ­å›¾è¿›è¡Œé«˜æ•ˆçš„åŠ¨ä½œé¢„æµ‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†3Dä¿¡å·æ•´åˆåˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­ï¼Œå……åˆ†åˆ©ç”¨äº†3Dæ•°æ®çš„ç©ºé—´ç»“æ„ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œä½¿å¾—è§†è§‰-è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸‹æ¸¸ç­–ç•¥å­¦ä¹ ä¹‹å‰é¢„æµ‹2Dçƒ­å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBridgeVLAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°äº†å“è¶Šçš„å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06366', 'title': 'AI Agent Behavioral Science', 'url': 'https://huggingface.co/papers/2506.06366', 'abstract': 'A new field, AI Agent Behavioral Science, is proposed to systematically study the behaviors of AI agents in diverse contexts, emphasizing external factors and their interactions, and addressing responsible AI aspects.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled the development of AI agents that exhibit increasingly human-like behaviors, including planning, adaptation, and social dynamics across diverse, interactive, and open-ended scenarios. These behaviors are not solely the product of the internal architectures of the underlying models, but emerge from their integration into agentic systems operating within specific contexts, where environmental factors, social cues, and interaction feedbacks shape behavior over time. This evolution necessitates a new scientific perspective: AI Agent Behavioral Science. Rather than focusing only on internal mechanisms, this perspective emphasizes the systematic observation of behavior, design of interventions to test hypotheses, and theory-guided interpretation of how AI agents act, adapt, and interact over time. We systematize a growing body of research across individual agent, multi-agent, and human-agent interaction settings, and further demonstrate how this perspective informs responsible AI by treating fairness, safety, interpretability, accountability, and privacy as behavioral properties. By unifying recent findings and laying out future directions, we position AI Agent Behavioral Science as a necessary complement to traditional model-centric approaches, providing essential tools for understanding, evaluating, and governing the real-world behavior of increasingly autonomous AI systems.', 'score': 7, 'issue_id': 4326, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '683be64d015db51c', 'authors': ['Lin Chen', 'Yunke Zhang', 'Jie Feng', 'Haoye Chai', 'Honglin Zhang', 'Bingbing Fan', 'Yibo Ma', 'Shiyuan Zhang', 'Nian Li', 'Tianhui Liu', 'Nicholas Sukiennik', 'Keyu Zhao', 'Yu Li', 'Ziyi Liu', 'Fengli Xu', 'Yong Li'], 'affiliations': ['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.06366.jpg', 'data': {'categories': ['#agi', '#healthcare', '#ethics', '#multimodal', '#agents', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ - Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°ÑƒĞºĞ° Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…, ÑƒĞ´ĞµĞ»ÑÑ Ğ¾ÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ¸ Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'Understanding AI Behavior: A New Scientific Approach', 'desc': "The paper introduces AI Agent Behavioral Science, a new field focused on studying the behaviors of AI agents in various contexts. It highlights that these behaviors arise not just from the AI's internal design but also from interactions with their environment and social dynamics. The approach emphasizes systematic observation, hypothesis testing, and theory-driven analysis to understand how AI agents adapt and interact over time. This perspective also addresses responsible AI considerations, such as fairness and accountability, making it a vital complement to traditional model-centric methods."}, 'zh': {'title': 'æ¢ç´¢äººå·¥æ™ºèƒ½ä»£ç†çš„è¡Œä¸ºç§‘å­¦', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„é¢†åŸŸâ€”â€”äººå·¥æ™ºèƒ½ä»£ç†è¡Œä¸ºç§‘å­¦ï¼Œæ—¨åœ¨ç³»ç»Ÿç ”ç©¶äººå·¥æ™ºèƒ½ä»£ç†åœ¨ä¸åŒç¯å¢ƒä¸­çš„è¡Œä¸ºã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼ŒAIä»£ç†å±•ç°å‡ºè¶Šæ¥è¶Šäººæ€§åŒ–çš„è¡Œä¸ºï¼Œå¦‚è§„åˆ’ã€é€‚åº”å’Œç¤¾äº¤åŠ¨æ€ã€‚è¿™äº›è¡Œä¸ºä¸ä»…æºäºæ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼Œè¿˜å—åˆ°ç¯å¢ƒå› ç´ ã€ç¤¾äº¤çº¿ç´¢å’Œäº’åŠ¨åé¦ˆçš„å½±å“ã€‚è¯¥é¢†åŸŸå¼ºè°ƒå¯¹è¡Œä¸ºçš„ç³»ç»Ÿè§‚å¯Ÿå’Œå¹²é¢„è®¾è®¡ï¼Œä»¥ä¿ƒè¿›å¯¹AIä»£ç†è¡Œä¸ºçš„ç†è§£å’Œè´Ÿè´£ä»»çš„åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09050', 'title': 'ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering', 'url': 'https://huggingface.co/papers/2506.09050', 'abstract': 'ALE-Bench evaluates AI systems on score-based algorithmic programming contests drawn from AtCoder, focusing on long-term iterative problem-solving in domains like package-delivery routing, crew scheduling, factory production, and power-grid balancing.  \t\t\t\t\tAI-generated summary \t\t\t\t How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, a new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, a notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements.', 'score': 6, 'issue_id': 4328, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'f30a01d616f75ed3', 'authors': ['Yuki Imajuku', 'Kohki Horie', 'Yoichi Iwata', 'Kensho Aoki', 'Naohiro Takahashi', 'Takuya Akiba'], 'affiliations': ['AtCoder, Japan', 'Sakana AI, Japan', 'The University of Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2506.09050.jpg', 'data': {'categories': ['#benchmark', '#agents', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ALE-Bench: Ğ˜ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ² Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'ALE-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, ĞºĞ°Ğº Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºĞ¸Ğ¿Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ/Ğ½ĞµĞ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ, ALE-Bench Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ¾Ğ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼.'}, 'en': {'title': "Evaluating AI's Long-Term Problem-Solving with ALE-Bench", 'desc': 'ALE-Bench is a benchmark designed to assess AI systems on complex optimization problems derived from real-world algorithmic programming contests. It focuses on long-term iterative problem-solving in various domains, such as package delivery and factory production. The benchmark emphasizes the importance of refining solutions over extended periods, rather than just achieving quick pass/fail results. Our findings indicate that while advanced language models perform well on certain tasks, they still lag behind human consistency and long-term problem-solving abilities, underscoring the need for further development in AI.'}, 'zh': {'title': 'ALE-Benchï¼šæ¨åŠ¨AIåœ¨å¤æ‚ä¼˜åŒ–é—®é¢˜ä¸Šçš„è¿›æ­¥', 'desc': 'ALE-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨åŸºäºåˆ†æ•°çš„ç®—æ³•ç¼–ç¨‹ç«èµ›ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ä¸Šã€‚å®ƒå…³æ³¨äºé•¿æœŸçš„è¿­ä»£é—®é¢˜è§£å†³ï¼Œæ¶‰åŠåŒ…è£¹æŠ•é€’ã€äººå‘˜è°ƒåº¦ã€å·¥å‚ç”Ÿäº§å’Œç”µç½‘å¹³è¡¡ç­‰é¢†åŸŸã€‚ä¸çŸ­æœŸçš„é€šè¿‡/ä¸é€šè¿‡ç¼–ç åŸºå‡†ä¸åŒï¼ŒALE-Benché¼“åŠ±åœ¨è¾ƒé•¿æ—¶é—´å†…å¯¹è§£å†³æ–¹æ¡ˆè¿›è¡Œç»†åŒ–ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¸€è‡´æ€§å’Œé•¿æœŸé—®é¢˜è§£å†³èƒ½åŠ›æ–¹é¢ä¸äººç±»ç›¸æ¯”ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13404', 'title': 'A Technical Study into Small Reasoning Language Models', 'url': 'https://huggingface.co/papers/2506.13404', 'abstract': 'The research explores training strategies such as supervised fine-tuning, knowledge distillation, and reinforcement learning to enhance the performance of resource-efficient Small Reasoning Language Models with limited capacity.  \t\t\t\t\tAI-generated summary \t\t\t\t The ongoing evolution of language models has led to the development of large-scale architectures that demonstrate exceptional performance across a wide range of tasks. However, these models come with significant computational and energy demands, as well as potential privacy implications. In this context, Small Reasoning Language Models (SRLMs) with approximately 0.5 billion parameters present a compelling alternative due to their remarkable computational efficiency and cost effectiveness, particularly in resource-constrained environments. Despite these advantages, the limited capacity of 0.5 billion parameter models poses challenges in handling complex tasks such as mathematical reasoning and code generation. This research investigates various training strategies, including supervised fine-tuning (SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as their hybrid implementations, to enhance the performance of 0.5B SRLMs. We analyze effective methodologies to bridge the performance gap between SRLMS and larger models and present insights into optimal training pipelines tailored for these smaller architectures. Through extensive experimental validation and analysis, our work aims to provide actionable recommendations for maximizing the reasoning capabilities of 0.5B models.', 'score': 5, 'issue_id': 4336, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '1a80e44800fb97ab', 'authors': ['Xialie Zhuang', 'Peixian Ma', 'Zhikai Jia', 'Zheng Cao', 'Shiwei Liu'], 'affiliations': ['SCITIX (SGP) TECH PTE. LTD., Singapore', 'The Hong Kong University of Science and Technology (Guangzhou), China', 'University of Chinese Academy of Sciences, China', 'University of Oxford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2506.13404.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#training', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ SRLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (SRLM) Ñ Ğ¾ĞºĞ¾Ğ»Ğ¾ 0,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (SFT), Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (KD) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ SRLM. Ğ¦ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ - Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ SRLM Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ 0,5B Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Boosting Small Models: Smart Training for Big Performance', 'desc': 'This research focuses on improving Small Reasoning Language Models (SRLMs) with around 0.5 billion parameters, which are efficient but struggle with complex tasks. It explores various training strategies like supervised fine-tuning, knowledge distillation, and reinforcement learning to boost their performance. The study aims to find effective methods to close the performance gap between these smaller models and larger, more powerful ones. By conducting extensive experiments, the authors provide practical recommendations for enhancing the reasoning abilities of SRLMs in resource-limited settings.'}, 'zh': {'title': 'æå‡å°å‹æ¨ç†æ¨¡å‹æ€§èƒ½çš„è®­ç»ƒç­–ç•¥', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤šç§è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒã€çŸ¥è¯†è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æå‡èµ„æºé«˜æ•ˆçš„å°å‹æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆSRLMsï¼‰çš„æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹å…·æœ‰çº¦5äº¿ä¸ªå‚æ•°ï¼Œå°½ç®¡åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä½†åœ¨è®¡ç®—æ•ˆç‡å’Œæˆæœ¬æ•ˆç›Šæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶è¿˜åˆ†æäº†å¦‚ä½•ç¼©å°å°å‹æ¨¡å‹ä¸å¤§å‹æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹å°å‹æ¶æ„çš„æœ€ä½³è®­ç»ƒæµç¨‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨ä¸ºæœ€å¤§åŒ–5äº¿å‚æ•°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›å¯è¡Œçš„å»ºè®®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06454', 'title': 'LETS Forecast: Learning Embedology for Time Series Forecasting', 'url': 'https://huggingface.co/papers/2506.06454', 'abstract': "DeepEDM integrates empirical dynamic modeling with deep neural networks to learn latent spaces and approximate complex nonlinear dynamics for improved time series forecasting.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world time series are often governed by complex nonlinear dynamics. Understanding these underlying dynamics is crucial for precise future prediction. While deep learning has achieved major success in time series forecasting, many existing approaches do not explicitly model the dynamics. To bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear dynamical systems modeling with deep neural networks. Inspired by empirical dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel deep model that learns a latent space from time-delayed embeddings, and employs kernel regression to approximate the underlying dynamics, while leveraging efficient implementation of softmax attention and allowing for accurate prediction of future time steps. To evaluate our method, we conduct comprehensive experiments on synthetic data of nonlinear dynamical systems as well as real-world time series across domains. Our results show that DeepEDM is robust to input noise, and outperforms state-of-the-art methods in forecasting accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.", 'score': 4, 'issue_id': 4329, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '4d7af8be98618ff0', 'authors': ['Abrar Majeedi', 'Viswanatha Reddy Gajjala', 'Satya Sai Srinath Namburi GNVV', 'Nada Magdi Elkordi', 'Yin Li'], 'affiliations': ['Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison', 'Department of Computer Sciences, University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.06454.jpg', 'data': {'categories': ['#training', '#synthetic', '#optimization', '#data'], 'emoji': 'ğŸ”®', 'ru': {'title': 'DeepEDM: Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'DeepEDM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµÑ‚ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². ĞĞ½ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ´ĞµÑ€Ğ½ÑƒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸. DeepEDM Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ softmax Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ ÑˆÑƒĞ¼Ñƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'DeepEDM: Bridging Dynamics and Deep Learning for Better Forecasting', 'desc': 'DeepEDM is a novel framework that combines empirical dynamic modeling with deep neural networks to enhance time series forecasting. It effectively learns latent spaces from time-delayed embeddings, allowing it to capture complex nonlinear dynamics in data. By utilizing kernel regression and softmax attention, DeepEDM can accurately predict future time steps while being robust to input noise. Comprehensive experiments demonstrate that it outperforms existing state-of-the-art forecasting methods across various domains.'}, 'zh': {'title': 'æ·±åº¦åŠ¨æ€å»ºæ¨¡ï¼Œç²¾å‡†æ—¶é—´é¢„æµ‹', 'desc': 'DeepEDMæ˜¯ä¸€ç§å°†ç»éªŒåŠ¨æ€å»ºæ¨¡ä¸æ·±åº¦ç¥ç»ç½‘ç»œç›¸ç»“åˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ—¶é—´åºåˆ—é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ æ—¶é—´å»¶è¿ŸåµŒå…¥çš„æ½œåœ¨ç©ºé—´ï¼Œåˆ©ç”¨æ ¸å›å½’æ¥è¿‘ä¼¼å¤æ‚çš„éçº¿æ€§åŠ¨æ€ã€‚DeepEDMå—åˆ°Takenså®šç†çš„å¯å‘ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å®ç°softmaxæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œå‡†ç¡®é¢„æµ‹æœªæ¥çš„æ—¶é—´æ­¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepEDMåœ¨å¤„ç†è¾“å…¥å™ªå£°æ—¶è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12189', 'title': "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis", 'url': 'https://huggingface.co/papers/2506.12189', 'abstract': "The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.", 'score': 3, 'issue_id': 4324, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '952c0d68aa23cbda', 'authors': ['Pranav Agarwal', 'Ioana CiucÄƒ'], 'affiliations': ['Google Deep Research', 'Institute', 'Mila', 'Quebec AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12189.jpg', 'data': {'categories': ['#dataset', '#small_models', '#reasoning', '#long_context', '#interpretability', '#multimodal', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¸Ñ… Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€Ñ‚Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Supernova Event Dataset, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 'Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸' ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´ĞµĞ»Ğ°Ñ Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹."}, 'en': {'title': 'Unveiling LLM Personalities for Better Interpretability', 'desc': 'This study investigates how different Large Language Models (LLMs) perform on various text tasks using a new dataset called the Supernova Event Dataset. The dataset includes a wide range of articles, allowing for the evaluation of LLMs in extracting and ranking key events, which requires complex reasoning and understanding of context. The research reveals distinct personality traits among the models, such as emotional reasoning in Orca 2 and strategic thinking in Qwen 2.5, enhancing our understanding of their decision-making processes. By using another LLM as a judge to assess these traits, the study improves the interpretability of models, making them more accessible for diverse applications.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§ç‰¹å¾', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒæ–‡æœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ã€‚é€šè¿‡åˆ†ææ¨¡å‹çš„é€‰æ‹©å’Œåˆ†ç±»äº‹ä»¶ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ¨¡å‹çš„ä¸ªæ€§ç‰¹å¾ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºçš„è¶…æ–°æ˜Ÿäº‹ä»¶æ•°æ®é›†åŒ…å«å¤šæ ·çš„æ–‡ç« ï¼Œå¸®åŠ©æˆ‘ä»¬åŸºå‡†æµ‹è¯•æ¨¡å‹åœ¨æå–å’Œæ’åºå…³é”®äº‹ä»¶æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨å¤„ç†æƒ…æ„Ÿæ¨ç†ã€æˆ˜ç•¥åˆ†æå’Œå› æœæ¨ç†ç­‰æ–¹é¢è¡¨ç°å‡ºæ˜æ˜¾çš„ä¸ªæ€§å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13752', 'title': 'Steering LLM Thinking with Budget Guidance', 'url': 'https://huggingface.co/papers/2506.13752', 'abstract': 'Budget guidance is a method that steers LLM reasoning within a targeted budget without fine-tuning and achieves improved efficiency and performance on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance.', 'score': 2, 'issue_id': 4325, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'c3d9b714e91736d6', 'authors': ['Junyan Li', 'Wenshuo Zhao', 'Yang Zhang', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'UMass Amherst', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13752.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#reasoning', '#math'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ˜Ğ˜ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°', 'desc': "ĞœĞµÑ‚Ğ¾Ğ´ 'Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°' Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ğ°Ğ¼Ğ¼Ğ°-Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞµĞ¹ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²."}, 'en': {'title': 'Steering LLM Reasoning with Budget Guidance for Efficiency and Performance', 'desc': 'This paper introduces a method called budget guidance, which helps large language models (LLMs) reason effectively within a specified budget of thinking tokens. By using a lightweight predictor that models a Gamma distribution, the method controls the reasoning length during the generation of each token without needing to fine-tune the LLM. This approach not only improves efficiency but also enhances performance on math benchmarks, achieving significant accuracy gains while using fewer tokens. Additionally, budget guidance shows versatility across different tasks and can even estimate the difficulty of questions.'}, 'zh': {'title': 'é¢„ç®—å¼•å¯¼ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'é¢„ç®—å¼•å¯¼æ˜¯ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç›®æ ‡é¢„ç®—å†…è¿›è¡Œæ¨ç†ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ä¸ªè½»é‡çº§é¢„æµ‹å™¨ï¼Œå»ºæ¨¡å‰©ä½™æ€è€ƒé•¿åº¦çš„ä¼½é©¬åˆ†å¸ƒï¼Œæ¥æ§åˆ¶æ¨ç†é•¿åº¦ã€‚é¢„ç®—å¼•å¯¼ç¡®ä¿ç”Ÿæˆè¿‡ç¨‹éµå¾ªæŒ‡å®šçš„æ€è€ƒé¢„ç®—ï¼ŒåŒæ—¶åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ä»¤ç‰Œæ•ˆç‡ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ç´§å¼ é¢„ç®—ä¸‹ï¼Œé¢„ç®—å¼•å¯¼åœ¨MATH-500åŸºå‡†ä¸Šå®ç°äº†é«˜è¾¾26%çš„å‡†ç¡®ç‡æå‡ï¼ŒåŒæ—¶ä»…ä½¿ç”¨å…¨æ€è€ƒæ¨¡å‹63%çš„æ€è€ƒä»¤ç‰Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12953', 'title': 'Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition', 'url': 'https://huggingface.co/papers/2506.12953', 'abstract': 'PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.', 'score': 2, 'issue_id': 4324, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': 'fb2789e38592ff5a', 'authors': ['Mayank Bumb', 'Anshul Vemulapalli', 'Sri Harsha Vardhan Prasad Jella', 'Anish Gupta', 'An La', 'Ryan A. Rossi', 'Hongjie Chen', 'Franck Dernoncourt', 'Nesreen K. Ahmed', 'Yu Wang'], 'affiliations': ['Adobe', 'Dolby Labs', 'Intel', 'University of Massachusetts Amherst', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2506.12953.jpg', 'data': {'categories': ['#data', '#training', '#optimization'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ PatchInstruct Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ², Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹. PatchInstruct Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing LLM Forecasting with Simple Prompting Techniques', 'desc': 'PatchInstruct is a method that improves the forecasting abilities of Large Language Models (LLMs) by using innovative prompting techniques. It incorporates time series decomposition to break down data into manageable parts, patch-based tokenization to efficiently handle input, and similarity-based neighbor augmentation to enhance predictions by considering related data points. This approach allows LLMs to perform time series forecasting without the need for extensive fine-tuning or complex architectures. Overall, PatchInstruct simplifies the process while boosting the accuracy of predictions in time series analysis.'}, 'zh': {'title': 'PatchInstructï¼šç®€åŒ–æ—¶é—´åºåˆ—é¢„æµ‹çš„æœ‰æ•ˆæ–¹æ³•', 'desc': 'PatchInstructæ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶é—´åºåˆ—é¢„æµ‹è´¨é‡çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ä¸“é—¨çš„æç¤ºç­–ç•¥ï¼Œå¦‚æ—¶é—´åºåˆ—åˆ†è§£ã€åŸºäºè¡¥ä¸çš„æ ‡è®°åŒ–å’Œç›¸ä¼¼æ€§é‚»å±…å¢å¼ºï¼Œæ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸ä»¥å¾€éœ€è¦å¤§é‡å¾®è°ƒçš„æ–¹æ³•ä¸åŒï¼ŒPatchInstructèƒ½å¤Ÿåœ¨ä¸å¤æ‚é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œçµæ´»åœ°è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚è¯¥æ–¹æ³•ä¿æŒäº†ç®€å•æ€§ï¼Œå¹¶ä¸”å¯¹æ•°æ®çš„é¢„å¤„ç†è¦æ±‚æœ€ä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12623', 'title': 'MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos', 'url': 'https://huggingface.co/papers/2506.12623', 'abstract': 'A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization.', 'score': 2, 'issue_id': 4324, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 Ğ¸ÑĞ½Ñ', 'en': 'June 14', 'zh': '6æœˆ14æ—¥'}, 'hash': 'ef83eb4ade9dc4bf', 'authors': ['Yuan Zang', 'Hao Tan', 'Seunghyun Yoon', 'Franck Dernoncourt', 'Jiuxiang Gu', 'Kushal Kafle', 'Chen Sun', 'Trung Bui'], 'affiliations': ['Adobe Research', 'Brown University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12623.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑƒĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ UI', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2413 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 167 Ñ‡Ğ°ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Learning with UI Video Summarization', 'desc': 'This paper introduces a new benchmark and dataset specifically designed for multi-modal summarization of user interface (UI) instructional videos. The goal is to create efficient summaries that include step-by-step text instructions and key video frames, which are essential for effective learning. The authors highlight that existing benchmarks are inadequate for this purpose, as they focus on general video summarization rather than instructional content. Through extensive experiments on their dataset of 2,413 annotated UI instructional videos, they demonstrate that current multi-modal summarization techniques are not effective for this specific type of video, indicating a need for improved methods.'}, 'zh': {'title': 'æå‡UIæ•™å­¦è§†é¢‘çš„å¤šæ¨¡æ€æ€»ç»“èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’Œæ•°æ®é›†ï¼Œç”¨äºå¤šæ¨¡æ€æ€»ç»“ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰æ•™å­¦è§†é¢‘ï¼Œæ—¨åœ¨æä¾›é€æ­¥å¯æ‰§è¡Œçš„æŒ‡ä»¤å’Œå…³é”®è§†é¢‘å¸§ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦å…³æ³¨ä¸€èˆ¬çš„è¯­ä¹‰çº§è§†é¢‘æ€»ç»“ï¼Œæ— æ³•æ»¡è¶³æ•™å­¦è§†é¢‘ä¸­å¯¹é€æ­¥æŒ‡ä»¤å’Œæ’å›¾çš„éœ€æ±‚ã€‚æˆ‘ä»¬æ”¶é›†äº†2413ä¸ªUIæ•™å­¦è§†é¢‘çš„æ•°æ®é›†ï¼Œæ‰‹åŠ¨æ ‡æ³¨äº†è§†é¢‘åˆ†å‰²ã€æ–‡æœ¬æ€»ç»“å’Œè§†é¢‘æ€»ç»“ï¼Œä»¥ä¾¿è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ€»ç»“æ–¹æ³•åœ¨UIè§†é¢‘æ€»ç»“ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†å¼€å‘æ–°æ–¹æ³•çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12552', 'title': 'Profiling News Media for Factuality and Bias Using LLMs and the\n  Fact-Checking Methodology of Human Experts', 'url': 'https://huggingface.co/papers/2506.12552', 'abstract': 'A novel methodology using large language models with curated prompts improves predictions of media outlet factuality and political bias, validated through experiments and error analysis.  \t\t\t\t\tAI-generated summary \t\t\t\t In an age characterized by the proliferation of mis- and disinformation online, it is critical to empower readers to understand the content they are reading. Important efforts in this direction rely on manual or automatic fact-checking, which can be challenging for emerging claims with limited information. Such scenarios can be handled by assessing the reliability and the political bias of the source of the claim, i.e., characterizing entire news outlets rather than individual claims or articles. This is an important but understudied research direction. While prior work has looked into linguistic and social contexts, we do not analyze individual articles or information in social media. Instead, we propose a novel methodology that emulates the criteria that professional fact-checkers use to assess the factuality and political bias of an entire outlet. Specifically, we design a variety of prompts based on these criteria and elicit responses from large language models (LLMs), which we aggregate to make predictions. In addition to demonstrating sizable improvements over strong baselines via extensive experiments with multiple LLMs, we provide an in-depth error analysis of the effect of media popularity and region on model performance. Further, we conduct an ablation study to highlight the key components of our dataset that contribute to these improvements. To facilitate future research, we released our dataset and code at https://github.com/mbzuai-nlp/llm-media-profiling.', 'score': 2, 'issue_id': 4332, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 Ğ¸ÑĞ½Ñ', 'en': 'June 14', 'zh': '6æœˆ14æ—¥'}, 'hash': '6c852f01e3464d88', 'authors': ['Zain Muhammad Mujahid', 'Dilshod Azizov', 'Maha Tufail Agro', 'Preslav Nakov'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, UAE', 'University of Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2506.12552.jpg', 'data': {'categories': ['#alignment', '#data', '#ethics', '#open_source', '#dataset', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¡ĞœĞ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¡ĞœĞ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ñ‡ĞµĞºĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ†ĞµĞ»Ñ‹Ñ… Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Empowering Readers: Assessing Media Factuality with LLMs', 'desc': 'This paper presents a new method that uses large language models (LLMs) to evaluate the factuality and political bias of media outlets. Instead of focusing on individual articles, the approach assesses the overall reliability of news sources by using curated prompts that mimic professional fact-checking criteria. The authors conducted experiments that showed significant improvements in prediction accuracy compared to existing methods, along with a detailed error analysis. They also released their dataset and code to support further research in this area.'}, 'zh': {'title': 'åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡åª’ä½“äº‹å®æ€§ä¸åè§é¢„æµ‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œç²¾å¿ƒè®¾è®¡çš„æç¤ºæ¥æé«˜åª’ä½“æ¥æºçš„äº‹å®æ€§å’Œæ”¿æ²»åè§é¢„æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯„ä¼°æ•´ä¸ªæ–°é—»æœºæ„çš„å¯é æ€§æ¯”å•ç‹¬åˆ†æä¸ªåˆ«æ–‡ç« æ›´æœ‰æ•ˆã€‚é€šè¿‡å¤§é‡å®éªŒå’Œé”™è¯¯åˆ†æï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†æ•°æ®é›†å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10341', 'title': 'Provably Learning from Language Feedback', 'url': 'https://huggingface.co/papers/2506.10341', 'abstract': 'A formal framework and no-regret algorithm are introduced for learning from language feedback, addressing challenges in interactive learning with large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far a principled framing of these decision problems remains lacking. In this paper, we formalize the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce transfer eluder dimension as a complexity measure to characterize the hardness of LLF problems. We show that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. We demonstrate cases where learning from rich language feedback can be exponentially faster than learning from reward. We develop a no-regret algorithm, called HELiX, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, we show that HELiX performs well even when repeatedly prompting LLMs does not work reliably. Our contributions mark a first step towards designing principled interactive learning algorithms from generic language feedback.', 'score': 2, 'issue_id': 4343, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'bbe8c2437ddea995', 'authors': ['Wanqiao Xu', 'Allen Nie', 'Ruijie Zheng', 'Aditya Modi', 'Adith Swaminathan', 'Ching-An Cheng'], 'affiliations': ['Microsoft Research', 'Netflix Research', 'Stanford University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.10341.jpg', 'data': {'categories': ['#agi', '#agents', '#training', '#rlhf', '#transfer_learning'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ±ĞµĞ· ÑĞ¾Ğ¶Ğ°Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ transfer eluder dimension ĞºĞ°Ğº Ğ¼ĞµÑ€Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ HELiX, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HELiX Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ½Ğµ Ğ´Ğ°ÑÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Learning Efficiency with Language Feedback', 'desc': "This paper introduces a formal framework for Learning from Language Feedback (LLF), which is essential for improving interactive learning with large language models (LLMs). It addresses the challenges of learning from language feedback by defining the transfer eluder dimension, a measure that helps understand the complexity of LLF problems. The authors present a no-regret algorithm named HELiX, which effectively learns from language feedback through sequential interactions, ensuring performance that scales with the problem's complexity. The findings suggest that learning from language feedback can be significantly more efficient than traditional reward-based learning methods."}, 'zh': {'title': 'ä»è¯­è¨€åé¦ˆä¸­é«˜æ•ˆå­¦ä¹ çš„åˆ›æ–°ç®—æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ­£å¼æ¡†æ¶å’Œæ— æ‚”ç®—æ³•ï¼Œç”¨äºä»è¯­è¨€åé¦ˆä¸­å­¦ä¹ ï¼Œè§£å†³äº†ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’å­¦ä¹ ä¸­çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ­£å¼åŒ–äº†è¯­è¨€åé¦ˆå­¦ä¹ ï¼ˆLLFï¼‰é—®é¢˜ï¼Œå¹¶æå‡ºäº†è¶³å¤Ÿçš„å‡è®¾ï¼Œä»¥ä¾¿åœ¨æ½œåœ¨å¥–åŠ±çš„æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ ã€‚å¼•å…¥äº†è½¬ç§»é€ƒé¿ç»´åº¦ä½œä¸ºå¤æ‚æ€§åº¦é‡ï¼Œè¡¨å¾LLFé—®é¢˜çš„éš¾åº¦ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»ä¸°å¯Œçš„è¯­è¨€åé¦ˆä¸­å­¦ä¹ å¯ä»¥æ¯”ä»å¥–åŠ±ä¸­å­¦ä¹ å¿«å¾—å¤šï¼Œå¹¶å¼€å‘äº†åä¸ºHELiXçš„æ— æ‚”ç®—æ³•ï¼Œèƒ½å¤Ÿé€šè¿‡é¡ºåºäº¤äº’æœ‰æ•ˆè§£å†³LLFé—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09968', 'title': 'SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance', 'url': 'https://huggingface.co/papers/2506.09968', 'abstract': 'A gamified LLM-assisted system, SRLAgent, significantly improves self-regulated learning skills in college students through interactive, goal-setting, and real-time AI feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-regulated learning (SRL) is crucial for college students navigating increased academic demands and independence. Insufficient SRL skills can lead to disorganized study habits, low motivation, and poor time management, undermining learners ability to thrive in challenging environments. Through a formative study involving 59 college students, we identified key challenges students face in developing SRL skills, including difficulties with goal-setting, time management, and reflective learning. To address these challenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL skills through gamification and adaptive support from large language models (LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables students to engage in goal-setting, strategy execution, and self-reflection within an interactive game-based environment. The system offers real-time feedback and scaffolding powered by LLMs to support students independent study efforts. We evaluated SRLAgent using a between-subjects design, comparing it to a baseline system (SRL without Agent features) and a traditional multimedia learning condition. Results showed significant improvements in SRL skills within the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement compared to the baselines. This work highlights the value of embedding SRL scaffolding and real-time AI support within gamified environments, offering design implications for educational technologies that aim to promote deeper learning and metacognitive skill development.', 'score': 2, 'issue_id': 4330, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'ea72b7b36234ccbd', 'authors': ['Wentao Ge', 'Yuqing Sun', 'Ziyan Wang', 'Haoyue Zheng', 'Weiyang He', 'Piaohong Wang', 'Qianyu Zhu', 'Benyou Wang'], 'affiliations': ['City University of Hong Kong China', 'The Chinese University of Hong Kong, Shenzhen China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09968.jpg', 'data': {'categories': ['#healthcare', '#games', '#multimodal', '#science', '#agents'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SRLAgent - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°Ğ¼ ĞºĞ¾Ğ»Ğ»ĞµĞ´Ğ¶ĞµĞ¹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ ÑĞ°Ğ¼Ğ¾Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (SRL). SRLAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ¹Ğ¼Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ LLM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ñ†ĞµĞ»ĞµĞ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ‚Ñ€ĞµÑ…Ñ„Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SRL Ğ¦Ğ¸Ğ¼Ğ¼ĞµÑ€Ğ¼Ğ°Ğ½Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ³Ñ€Ğ¾Ğ²ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² SRL Ğ¸ Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ SRLAgent Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering Students with Gamified AI for Self-Regulated Learning', 'desc': "The paper presents SRLAgent, a gamified system that enhances self-regulated learning (SRL) skills in college students by utilizing large language models (LLMs) for real-time feedback. It addresses common challenges students face, such as goal-setting and time management, by providing an interactive environment based on Zimmerman's SRL framework. A study with 59 participants demonstrated that SRLAgent significantly improved SRL skills and engagement compared to traditional learning methods. This research emphasizes the importance of integrating AI support and gamification in educational tools to foster independent learning and metacognitive development."}, 'zh': {'title': 'æ¸¸æˆåŒ–ç³»ç»Ÿæå‡å¤§å­¦ç”Ÿè‡ªæˆ‘è°ƒèŠ‚å­¦ä¹ æŠ€èƒ½', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åä¸ºSRLAgentçš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ¸¸æˆåŒ–å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®æ—¶åé¦ˆï¼Œæ˜¾è‘—æå‡å¤§å­¦ç”Ÿçš„è‡ªæˆ‘è°ƒèŠ‚å­¦ä¹ ï¼ˆSRLï¼‰æŠ€èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œè®¸å¤šå­¦ç”Ÿåœ¨ç›®æ ‡è®¾å®šã€æ—¶é—´ç®¡ç†å’Œåæ€å­¦ä¹ æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚SRLAgentåŸºäºZimmermançš„ä¸‰é˜¶æ®µSRLæ¡†æ¶ï¼Œå¸®åŠ©å­¦ç”Ÿåœ¨äº’åŠ¨æ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œç›®æ ‡è®¾å®šã€ç­–ç•¥æ‰§è¡Œå’Œè‡ªæˆ‘åæ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SRLAgentçš„å­¦ç”Ÿåœ¨SRLæŠ€èƒ½å’Œå‚ä¸åº¦ä¸Šå‡æœ‰æ˜¾è‘—æé«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11115', 'title': 'Incorporating Domain Knowledge into Materials Tokenization', 'url': 'https://huggingface.co/papers/2506.11115', 'abstract': 'MATTER, a novel tokenization approach incorporating material knowledge, improves performance in scientific text processing tasks by maintaining structural and semantic material integrity.  \t\t\t\t\tAI-generated summary \t\t\t\t While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and a re-ranking method prioritizing material concepts in token merging, MATTER maintains the structural integrity of identified material concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of 4% and 2% in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing. Our code is available at https://github.com/yerimoh/MATTER', 'score': 2, 'issue_id': 4330, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'dfe8867cd5b71249', 'authors': ['Yerim Oh', 'Jun-Hyung Park', 'Junho Kim', 'SungHo Kim', 'SangKeun Lee'], 'affiliations': ['Department of Artificial Intelligence, Korea University', 'Department of Computer Science and Engineering, Korea University', 'Division of Language & AI, Hankuk University of Foreign Studies'], 'pdf_title_img': 'assets/pdf/title_img/2506.11115.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#multimodal', '#science'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'MATTER: Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ', 'desc': 'MATTER - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ñ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, MATTER ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MatDetector, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ñ…, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MATTER Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 4% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° 2% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'MATTER: Tokenization that Understands Materials!', 'desc': 'The paper introduces MATTER, a new tokenization method designed specifically for scientific texts in materials science. Unlike traditional tokenization methods that often lead to loss of meaning and structure, MATTER incorporates material knowledge to preserve the integrity of material concepts. It utilizes a trained model called MatDetector and a re-ranking technique to effectively merge tokens while maintaining their semantic significance. Experimental results show that MATTER significantly improves performance in text generation and classification tasks compared to existing methods.'}, 'zh': {'title': 'MATTERï¼šæå‡ç§‘å­¦æ–‡æœ¬å¤„ç†çš„åˆ†è¯æ–°æ–¹æ³•', 'desc': 'MATTERæ˜¯ä¸€ç§æ–°é¢–çš„åˆ†è¯æ–¹æ³•ï¼Œå®ƒç»“åˆäº†ææ–™çŸ¥è¯†ï¼Œæ—¨åœ¨æé«˜ç§‘å­¦æ–‡æœ¬å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„åˆ†è¯æ–¹æ³•å¾€å¾€åŸºäºé¢‘ç‡ï¼Œå®¹æ˜“å¯¼è‡´è¯­ä¹‰ä¸¢å¤±å’Œç»“æ„ç ´åï¼Œæ— æ³•æœ‰æ•ˆä¿æŒææ–™æ¦‚å¿µçš„å®Œæ•´æ€§ã€‚MATTERé€šè¿‡ä½¿ç”¨è®­ç»ƒå¥½çš„MatDetectorå’Œä¼˜å…ˆè€ƒè™‘ææ–™æ¦‚å¿µçš„é‡æ’åºæ–¹æ³•ï¼Œç¡®ä¿åœ¨åˆ†è¯è¿‡ç¨‹ä¸­ä¿æŒææ–™æ¦‚å¿µçš„ç»“æ„å’Œè¯­ä¹‰å®Œæ•´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMATTERåœ¨ç”Ÿæˆå’Œåˆ†ç±»ä»»åŠ¡ä¸­åˆ†åˆ«æé«˜äº†4%å’Œ2%çš„æ€§èƒ½ï¼Œå¼ºè°ƒäº†é¢†åŸŸçŸ¥è¯†åœ¨ç§‘å­¦æ–‡æœ¬å¤„ç†ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13277', 'title': 'SeqPE: Transformer with Sequential Position Encoding', 'url': 'https://huggingface.co/papers/2506.13277', 'abstract': "SeqPE, a fully learnable position encoding framework, enhances the adaptability and scalability of positional encodings in Transformers, improving performance in various tasks and seamless multi-dimensional generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each n-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracy--particularly under context length extrapolation--but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe.", 'score': 1, 'issue_id': 4336, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '9c0c6eb35f5707c3', 'authors': ['Huyang Li', 'Yahui Liu', 'Hongyu Sun', 'Deng Cai', 'Leyang Cui', 'Wei Bi', 'Peilin Zhao', 'Taro Watanabe'], 'affiliations': ['Kuaishou Technology', 'Nara Institute of Science and Technology (NAIST)', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2506.13277.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#long_context', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SeqPE: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'SeqPE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, SeqPE Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ n-Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ SeqPE Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'SeqPE: Revolutionizing Positional Encoding for Transformers', 'desc': 'SeqPE is a novel position encoding framework designed for Transformers that enhances their adaptability and scalability. Unlike traditional fixed-size positional encodings, SeqPE uses a fully learnable approach that allows for better extrapolation beyond pre-trained sequence lengths. It incorporates a contrastive objective and knowledge distillation to improve the embedding space, ensuring that the model can generalize effectively to new tasks and dimensions. Experiments show that SeqPE outperforms existing methods in various applications, including language modeling and image classification, without needing extensive modifications to the architecture.'}, 'zh': {'title': 'SeqPEï¼šæå‡Transformerä½ç½®ç¼–ç çš„é€‚åº”æ€§ä¸å¯æ‰©å±•æ€§', 'desc': 'SeqPEæ˜¯ä¸€ç§å®Œå…¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜Transformeræ¨¡å‹ä¸­ä½ç½®ç¼–ç çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¼ ç»Ÿçš„å¯å­¦ä¹ ä½ç½®åµŒå…¥æ–¹æ³•ç”±äºå›ºå®šå¤§å°çš„æŸ¥æ‰¾è¡¨ï¼Œé™åˆ¶äº†è¶…å‡ºé¢„è®­ç»ƒåºåˆ—é•¿åº¦çš„å¤–æ¨èƒ½åŠ›ã€‚SeqPEé€šè¿‡å°†æ¯ä¸ªnç»´ä½ç½®ç´¢å¼•è¡¨ç¤ºä¸ºç¬¦å·åºåˆ—ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§çš„é¡ºåºä½ç½®ç¼–ç å™¨è¿›è¡Œç«¯åˆ°ç«¯å­¦ä¹ ï¼Œå…‹æœäº†è¿™ä¸€é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSeqPEåœ¨è¯­è¨€å»ºæ¨¡ã€é•¿ä¸Šä¸‹æ–‡é—®ç­”å’ŒäºŒç»´å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ä¸Šä¸‹æ–‡é•¿åº¦å¤–æ¨æ–¹é¢ï¼Œèƒ½å¤Ÿæ— ç¼åœ°æ¨å¹¿åˆ°å¤šç»´è¾“å…¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12299', 'title': 'QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety', 'url': 'https://huggingface.co/papers/2506.12299', 'abstract': 'QGuard, a safety guard method using question prompting, effectively defends LLMs against harmful and multi-modal malicious prompts without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts.', 'score': 1, 'issue_id': 4330, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 Ğ¸ÑĞ½Ñ', 'en': 'June 14', 'zh': '6æœˆ14æ—¥'}, 'hash': '52cafcd463738b45', 'authors': ['Taegyeong Lee', 'Jeonghwa Yoo', 'Hyoungseo Cho', 'Soo Yong Kim', 'Yunho Maeng'], 'affiliations': ['A.I.MATICS Inc.', 'Ewha Womans University', 'FnGuide Inc.', 'Safe Generative AI Lab, MODULABS'], 'pdf_title_img': 'assets/pdf/title_img/2506.12299.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#security', '#hallucinations'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'QGuard: Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸', 'desc': 'QGuard - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞĞ½ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ ĞºĞ°Ğº Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². QGuard Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼ Ğº Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞ¸Ğ¼ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'QGuard: Safeguarding LLMs with Smart Question Prompting', 'desc': 'QGuard is a novel safety guard method designed to protect Large Language Models (LLMs) from harmful and multi-modal malicious prompts without the need for fine-tuning. It employs question prompting to effectively block these harmful inputs in a zero-shot manner, making it versatile against various types of attacks. The method not only addresses text-based threats but also extends its defense to multi-modal prompt attacks, showcasing its robustness. Experimental results indicate that QGuard performs competitively across different datasets, providing valuable insights for enhancing the security of LLM services.'}, 'zh': {'title': 'QGuardï¼šä¿æŠ¤LLMsçš„å®‰å…¨é˜²æŠ¤æ–°æ–¹æ³•', 'desc': 'QGuardæ˜¯ä¸€ç§ä½¿ç”¨é—®é¢˜æç¤ºçš„å®‰å…¨é˜²æŠ¤æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²å¾¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æŠ—æœ‰å®³å’Œå¤šæ¨¡æ€æ¶æ„æç¤ºï¼Œè€Œæ— éœ€è¿›è¡Œå¾®è°ƒã€‚è¯¥æ–¹æ³•é€šè¿‡é›¶-shotçš„æ–¹å¼é˜»æ­¢æœ‰å®³æç¤ºï¼Œä¿æŠ¤LLMså…å—æ–‡æœ¬å’Œå¤šæ¨¡æ€æ”»å‡»ã€‚é€šè¿‡å¤šæ ·åŒ–å’Œä¿®æ”¹é˜²æŠ¤é—®é¢˜ï¼ŒQGuardåœ¨é¢å¯¹æœ€æ–°çš„æœ‰å®³æç¤ºæ—¶ä¾ç„¶ä¿æŒå¼ºå¤§çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€æœ‰å®³æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸ºå®é™…çš„LLMæœåŠ¡æä¾›äº†é‡è¦çš„å®‰å…¨é£é™©ç¼“è§£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12258', 'title': 'EgoPrivacy: What Your First-Person Camera Says About You?', 'url': 'https://huggingface.co/papers/2506.12258', 'abstract': "EgoPrivacy evaluates privacy risks in egocentric vision through a large-scale benchmark, revealing that foundation models can infer private information about camera wearers with high accuracy in zero-shot settings.  \t\t\t\t\tAI-generated summary \t\t\t\t While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera wearer. This work investigates the core question: How much privacy information about the camera wearer can be inferred from their first-person view videos? We introduce EgoPrivacy, the first large-scale benchmark for the comprehensive evaluation of privacy risks in egocentric vision. EgoPrivacy covers three types of privacy (demographic, individual, and situational), defining seven tasks that aim to recover private information ranging from fine-grained (e.g., wearer's identity) to coarse-grained (e.g., age group). To further emphasize the privacy threats inherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel attack strategy that leverages ego-to-exo retrieval from an external pool of exocentric videos to boost the effectiveness of demographic privacy attacks. An extensive comparison of the different attacks possible under all threat models is presented, showing that private information of the wearer is highly susceptible to leakage. For instance, our findings indicate that foundation models can effectively compromise wearer privacy even in zero-shot settings by recovering attributes such as identity, scene, gender, and race with 70-80% accuracy. Our code and data are available at https://github.com/williamium3000/ego-privacy.", 'score': 1, 'issue_id': 4332, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '80aaa11f21bae6d3', 'authors': ['Yijiang Li', 'Genpei Zhang', 'Jiacheng Cheng', 'Yi Li', 'Xiaojun Shan', 'Dashan Gao', 'Jiancheng Lyu', 'Yuan Li', 'Ning Bi', 'Nuno Vasconcelos'], 'affiliations': ['Qualcomm AI Research, an initiative of Qualcomm Technologies, Inc.', 'University of California, San Diego', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12258.jpg', 'data': {'categories': ['#leakage', '#rag', '#ethics', '#benchmark'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ­Ğ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ: ÑĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ EgoPrivacy Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¸ÑĞºĞ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°. ĞĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑÑ… ĞºĞ°Ğ¼ĞµÑ€ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ ÑĞµĞ¼ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²Ğ°Ğ»Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'EgoPrivacy: Unveiling Hidden Risks in Wearable Camera Data', 'desc': 'EgoPrivacy is a benchmark designed to assess privacy risks associated with egocentric vision, particularly from wearable cameras. The study reveals that foundation models can accurately infer sensitive information about the camera wearer, such as identity and demographic details, even without prior training on specific data. It introduces a novel attack method called Retrieval-Augmented Attack, which enhances the effectiveness of privacy attacks by utilizing external video data. The results show that private information can be compromised with high accuracy, highlighting significant privacy concerns in the use of wearable cameras.'}, 'zh': {'title': 'EgoPrivacyï¼šæ­ç¤ºè‡ªæˆ‘ä¸­å¿ƒè§†è§‰ä¸­çš„éšç§é£é™©', 'desc': 'EgoPrivacyæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è‡ªæˆ‘ä¸­å¿ƒè§†è§‰éšç§é£é™©çš„å¤§è§„æ¨¡åŸºå‡†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹èƒ½å¤Ÿåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹é«˜æ•ˆæ¨æ–­æ‘„åƒå¤´ä½©æˆ´è€…çš„ç§äººä¿¡æ¯ã€‚è¯¥ç ”ç©¶å®šä¹‰äº†ä¸‰ç§éšç§ç±»å‹ï¼Œå¹¶æå‡ºäº†ä¸ƒä¸ªä»»åŠ¡ï¼Œä»¥æ¢å¤ä»ä½©æˆ´è€…èº«ä»½åˆ°å¹´é¾„ç»„ç­‰ä¸åŒå±‚æ¬¡çš„ç§äººä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ”»å‡»ç­–ç•¥ï¼Œé€šè¿‡å¤–éƒ¨è§†é¢‘åº“å¢å¼ºäººå£ç»Ÿè®¡éšç§æ”»å‡»çš„æ•ˆæœï¼Œæ˜¾ç¤ºå‡ºä½©æˆ´è€…çš„ç§äººä¿¡æ¯ææ˜“æ³„éœ²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13430', 'title': 'Uncertainty-Aware Remaining Lifespan Prediction from Images', 'url': 'https://huggingface.co/papers/2506.13430', 'abstract': 'Vision transformer models predict remaining lifespan from images with high accuracy and well-calibrated uncertainty estimates.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting mortality-related outcomes from images offers the prospect of accessible, noninvasive, and scalable health screening. We present a method that leverages pretrained vision transformer foundation models to estimate remaining lifespan from facial and whole-body images, alongside robust uncertainty quantification. We show that predictive uncertainty varies systematically with the true remaining lifespan, and that this uncertainty can be effectively modeled by learning a Gaussian distribution for each sample. Our approach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on an established Dataset, and further improves to 4.79 and 5.07 years MAE on two new, higher-quality datasets curated and published in this work. Importantly, our models provide well-calibrated uncertainty estimates, as demonstrated by a bucketed expected calibration error of 0.62 years. While not intended for clinical deployment, these results highlight the potential of extracting medically relevant signals from images. We make all code and datasets available to facilitate further research.', 'score': 0, 'issue_id': 4331, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'ce0b4c81ed1c657b', 'authors': ['Tristan Kenneweg', 'Philip Kenneweg', 'Barbara Hammer'], 'affiliations': ['University of Bielefeld'], 'pdf_title_img': 'assets/pdf/title_img/2506.13430.jpg', 'data': {'categories': ['#optimization', '#dataset', '#healthcare', '#science', '#open_source', '#cv'], 'emoji': 'ğŸ”®', 'ru': {'title': 'Ğ˜Ğ˜ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¶Ğ¸Ğ·Ğ½Ğ¸ Ğ¿Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Vision Transformer Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞµĞ¹ÑÑ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¶Ğ¸Ğ·Ğ½Ğ¸ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ñ‚ĞµĞ»Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹ 4.79-7.48 Ğ»ĞµÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ°Ğ¶Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¥Ğ¾Ñ‚Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Transforming Images into Lifespan Predictions with Uncertainty', 'desc': "This paper presents a method using vision transformer models to predict remaining lifespan from facial and whole-body images. The approach not only provides accurate lifespan estimates but also quantifies uncertainty in these predictions, which is crucial for understanding the reliability of the model's outputs. By learning a Gaussian distribution for each sample, the model effectively captures how uncertainty varies with the true remaining lifespan. The results show a significant improvement in prediction accuracy on new datasets, emphasizing the potential of using image analysis for health screening."}, 'zh': {'title': 'ä»å›¾åƒé¢„æµ‹å‰©ä½™å¯¿å‘½çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰å˜æ¢å™¨æ¨¡å‹ï¼Œä»é¢éƒ¨å’Œå…¨èº«å›¾åƒä¸­é¢„æµ‹å‰©ä½™å¯¿å‘½çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ä»…å…·æœ‰é«˜å‡†ç¡®æ€§ï¼Œè¿˜èƒ½æœ‰æ•ˆé‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„æµ‹çš„ä¸ç¡®å®šæ€§ä¸çœŸå®çš„å‰©ä½™å¯¿å‘½ä¹‹é—´å­˜åœ¨ç³»ç»Ÿæ€§çš„å˜åŒ–ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªæ ·æœ¬å­¦ä¹ é«˜æ–¯åˆ†å¸ƒæ¥æœ‰æ•ˆå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼Œå±•ç¤ºäº†ä»å›¾åƒä¸­æå–åŒ»å­¦ç›¸å…³ä¿¡å·çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13172', 'title': 'Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging\n  Unsubstantiated Claims and Ambiguous Pronouns', 'url': 'https://huggingface.co/papers/2506.13172', 'abstract': "Structured workflow prompts improve hierarchical reasoning in LLMs for scholarly manuscript analysis, but their effectiveness varies with the model, task type, and context.  \t\t\t\t\tAI-generated summary \t\t\t\t We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing.", 'score': 0, 'issue_id': 4328, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '298ec00caed4ffd4', 'authors': ['Evgeny Markhasin'], 'affiliations': ['Lobachevsky State University of Nizhny Novgorod'], 'pdf_title_img': 'assets/pdf/title_img/2506.13172.jpg', 'data': {'categories': ['#science', '#multimodal', '#training', '#reasoning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞµĞ¹. ĞĞ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (Gemini Pro 2.5 Pro Ğ¸ ChatGPT Plus o3) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑÑ‚Ğ¾Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Enhancing Scholarly Analysis with Structured Prompts in LLMs', 'desc': 'This paper explores how structured workflow prompts can enhance hierarchical reasoning in Large Language Models (LLMs) when analyzing scholarly manuscripts. The authors designed prompts to help models identify unsubstantiated claims and ambiguous pronoun references, which are crucial for maintaining informational integrity and linguistic clarity. They evaluated two advanced models, Gemini Pro 2.5 Pro and ChatGPT Plus o3, and found significant differences in their performance based on the task and context. The results indicate that while structured prompting can improve analysis, its effectiveness varies greatly depending on the model and the specific analytical task.'}, 'zh': {'title': 'ç»“æ„åŒ–æç¤ºæå‡å­¦æœ¯åˆ†æä¸­çš„å±‚æ¬¡æ¨ç†', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†ç»“æ„åŒ–å·¥ä½œæµç¨‹æç¤ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ä¿ƒè¿›å±‚æ¬¡æ¨ç†çš„æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦æœ¯æ‰‹ç¨¿åˆ†æä¸­ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç³»åˆ—æ¦‚å¿µéªŒè¯çš„æç¤ºï¼Œæ—¨åœ¨å¼•å¯¼æ¨¡å‹è¿›è¡Œé«˜æ°´å¹³çš„è¯­ä¹‰å’Œè¯­è¨€åˆ†æï¼Œé‡ç‚¹å…³æ³¨è¯†åˆ«æœªè¯å®çš„ä¸»å¼ å’Œæ¨¡ç³Šçš„ä»£è¯å¼•ç”¨ã€‚é€šè¿‡å¯¹ä¸¤ç§å‰æ²¿æ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¥æ³•è§’è‰²æ—¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–æç¤ºæ˜¯ä¸€ç§æœ‰æ•ˆçš„å¤æ‚æ–‡æœ¬åˆ†ææ–¹æ³•ï¼Œä½†å…¶æ•ˆæœå—æ¨¡å‹ã€ä»»åŠ¡ç±»å‹å’Œä¸Šä¸‹æ–‡çš„ç›¸äº’å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12148', 'title': "Hatevolution: What Static Benchmarks Don't Tell Us", 'url': 'https://huggingface.co/papers/2506.12148', 'abstract': 'Empirical evaluation reveals temporal misalignment in the robustness of language models on evolving hate speech benchmarks, highlighting the need for time-sensitive linguistic assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains under-explored. Yet, hate speech benchmarks play a crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain.', 'score': 0, 'issue_id': 4333, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': 'f0e52e354d5e052b', 'authors': ['Chiara Di Bonaventura', 'Barbara McGillivray', 'Yulan He', 'Albert MeroÃ±o-PeÃ±uela'], 'affiliations': ['Imperial College London', 'Kings College London'], 'pdf_title_img': 'assets/pdf/title_img/2506.12148.jpg', 'data': {'categories': ['#ethics', '#dataset', '#security', '#benchmark'], 'emoji': 'â³', 'ru': {'title': 'Ğ’Ñ€ĞµĞ¼Ñ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ: Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ² ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ²Ñ€Ğ°Ğ¶Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 20 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ²ÑƒÑ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼, Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ²Ñ€Ğ°Ğ¶Ğ´Ñ‹.'}, 'en': {'title': 'Evolving Language, Evolving Benchmarks: Time-Sensitive Evaluations for Hate Speech Models', 'desc': "This paper investigates how language models perform on hate speech benchmarks that change over time. It highlights that as language evolves, especially in sensitive areas like hate speech, the effectiveness of static evaluations may not accurately reflect a model's robustness. The authors tested 20 different language models against two evolving hate speech datasets and found significant misalignment in their performance. The study emphasizes the importance of developing time-sensitive benchmarks to ensure that language models are evaluated accurately and safely in the context of evolving language."}, 'zh': {'title': 'ä»‡æ¨è¨€è®ºè¯„ä¼°éœ€ä¸æ—¶ä¿±è¿›', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨ä¸æ–­å˜åŒ–çš„ä»‡æ¨è¨€è®ºåŸºå‡†ä¸Šçš„é²æ£’æ€§ï¼Œå‘ç°äº†æ—¶é—´ä¸Šçš„ä¸ä¸€è‡´æ€§ã€‚éšç€ç¤¾ä¼šåŠ¨æ€å’Œæ–‡åŒ–å˜è¿ï¼Œä»‡æ¨è¨€è®ºçš„è¯­è¨€ä¹Ÿåœ¨ä¸æ–­æ¼”å˜ã€‚å°½ç®¡è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶å·²ç»å…³æ³¨è¯­è¨€æ¼”å˜å¯¹æ¨¡å‹è®­ç»ƒçš„å½±å“ï¼Œä½†å¯¹æ¨¡å‹åŸºå‡†æµ‹è¯•çš„å½±å“ä»ç„¶ç ”ç©¶ä¸è¶³ã€‚æˆ‘ä»¬å»ºè®®åœ¨ä»‡æ¨è¨€è®ºé¢†åŸŸä¸­é‡‡ç”¨æ—¶é—´æ•æ„Ÿçš„è¯­è¨€åŸºå‡†ï¼Œä»¥ä¾¿æ›´å‡†ç¡®åœ°è¯„ä¼°è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24864', 'title': 'ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.24864', 'abstract': "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B", 'score': 77, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '390a294f460cedfc', 'authors': ['Mingjie Liu', 'Shizhe Diao', 'Ximing Lu', 'Jian Hu', 'Xin Dong', 'Yejin Choi', 'Jan Kautz', 'Yi Dong'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.24864.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl', '#alignment', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ProRL. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. ProRL Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ KL, ÑĞ±Ñ€Ğ¾Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… pass@k.'}, 'en': {'title': 'Unlocking New Reasoning Strategies with ProRL', 'desc': 'This paper explores the effectiveness of reinforcement learning (RL) in enhancing the reasoning capabilities of language models. The authors introduce a new training method called ProRL, which employs techniques like KL divergence control and reference policy resetting to improve model performance. Their experiments show that models trained with ProRL outperform base models in various reasoning tasks, even in cases where base models struggle. The study suggests that prolonged RL training can help discover new reasoning strategies, indicating that RL can significantly expand the reasoning abilities of language models over time.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æ‰©å±•æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºæ¨ç†çš„è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥ä½¿æ¨¡å‹ä¸å¯éªŒè¯çš„å¥–åŠ±å¯¹é½ã€‚ç„¶è€Œï¼Œå…³äºRLæ˜¯å¦çœŸæ­£å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ˜¯ä»…ä»…æ”¾å¤§äº†åŸºç¡€æ¨¡å‹åˆ†å¸ƒä¸­å·²ç»å­˜åœ¨çš„é«˜å¥–åŠ±è¾“å‡ºï¼Œä»ç„¶å­˜åœ¨äº‰è®®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ProRLï¼Œè¯æ˜äº†ç»è¿‡é•¿æ—¶é—´çš„RLè®­ç»ƒå¯ä»¥å‘ç°åŸºç¡€æ¨¡å‹æ— æ³•è®¿é—®çš„æ–°æ¨ç†ç­–ç•¥ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ˜¾ç¤ºï¼Œç»è¿‡RLè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šç§è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨åŸºç¡€æ¨¡å‹å®Œå…¨å¤±è´¥çš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24867', 'title': "Time Blindness: Why Video-Language Models Can't See What Humans Can?", 'url': 'https://huggingface.co/papers/2505.24867', 'abstract': 'Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.', 'score': 53, 'issue_id': 4070, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '6870a65f3ad54877', 'authors': ['Ujjwal Upadhyay', 'Mukul Ranjan', 'Zhiqiang Shen', 'Mohamed Elhoseiny'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST)', 'Mohamed bin Zayed University of AI (MBZUAI)'], 'pdf_title_img': 'assets/pdf/title_img/2505.24867.jpg', 'data': {'categories': ['#benchmark', '#training', '#open_source', '#survey', '#reasoning', '#multimodal', '#architecture', '#cv', '#games', '#dataset'], 'emoji': 'â³', 'ru': {'title': 'Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ÑĞ»ĞµĞ¿Ğ¾Ñ‚Ğ°: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SpookyBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ ÑĞ¼Ñ‹ÑĞ» Ğ¸Ğ· Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 98%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ°Ñ…. SpookyBench Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Bridging the Gap: Enhancing Temporal Understanding in Vision-Language Models', 'desc': 'This paper introduces SpookyBench, a new benchmark designed to test vision-language models (VLMs) on their ability to understand temporal patterns in videos when spatial information is not available. The study reveals that while humans can accurately identify shapes and patterns in noisy temporal sequences, current state-of-the-art VLMs fail to do so, achieving 0% accuracy. This highlights a significant limitation in VLMs, which tend to rely heavily on spatial features and struggle with temporal reasoning, especially in low spatial signal-to-noise ratio scenarios. The authors suggest that addressing this issue may require innovative model architectures or training methods that separate spatial and temporal processing, and they aim to stimulate further research in this area by releasing the SpookyBench dataset.'}, 'zh': {'title': 'çªç ´æ—¶åºç†è§£çš„ç“¶é¢ˆ', 'desc': 'æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£è§†é¢‘ä¸­çš„æ—¶ç©ºå…³ç³»æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“ç©ºé—´ä¿¡æ¯è¢«é®è”½æ—¶ï¼Œè¿™äº›æ¨¡å‹åœ¨æ•æ‰çº¯ç²¹çš„æ—¶é—´æ¨¡å¼æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†SpookyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä¿¡æ¯ä»…é€šè¿‡å™ªå£°å¸§çš„æ—¶é—´åºåˆ—ç¼–ç ï¼Œåæ˜ äº†ä»ç”Ÿç‰©ä¿¡å·åˆ°éšè”½é€šä¿¡çš„è‡ªç„¶ç°è±¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡äººç±»åœ¨è¿™äº›åºåˆ—ä¸­è¯†åˆ«å½¢çŠ¶ã€æ–‡æœ¬å’Œæ¨¡å¼çš„å‡†ç¡®ç‡è¶…è¿‡98%ï¼Œä½†æœ€å…ˆè¿›çš„VLMsçš„å‡†ç¡®ç‡å´ä¸º0%ï¼Œè¿™çªæ˜¾äº†æ¨¡å‹åœ¨æ—¶åºç†è§£ä¸Šçš„å…³é”®å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24863', 'title': 'AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time', 'url': 'https://huggingface.co/papers/2505.24863', 'abstract': "This paper presents AlphaOne (alpha1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. alpha1 first introduces alpha moment, which represents the scaled thinking phase with a universal parameter alpha. Within this scaled pre-alpha moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the alpha moment, alpha1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate alpha1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/", 'score': 53, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'a30c2004fdd2d154', 'authors': ['Junyu Zhang', 'Runpei Dong', 'Han Wang', 'Xuying Ning', 'Haoran Geng', 'Peihao Li', 'Xialin He', 'Yutong Bai', 'Jitendra Malik', 'Saurabh Gupta', 'Huan Zhang'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.24863.jpg', 'data': {'categories': ['#math', '#reasoning', '#training', '#benchmark', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'AlphaOne: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜', 'desc': 'AlphaOne (alpha1) - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ°Ğ»ÑŒÑ„Ğ°-Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„Ğ°Ğ·Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼ Ğ°Ğ»ÑŒÑ„Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ²ÑÑ‚Ğ°Ğ²ĞºÑƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ‘ĞµÑ€Ğ½ÑƒĞ»Ğ»Ğ¸. AlphaOne Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'AlphaOne: Revolutionizing Reasoning in Large Models', 'desc': "This paper introduces AlphaOne, a framework designed to enhance the reasoning capabilities of large reasoning models (LRMs) during testing. It introduces the concept of the alpha moment, which allows for a controlled thinking phase using a universal parameter. By employing a Bernoulli stochastic process, AlphaOne dynamically manages the transition from slow to fast reasoning, optimizing the model's performance. Empirical results show that AlphaOne outperforms existing methods in various complex tasks, demonstrating its effectiveness in improving reasoning efficiency."}, 'zh': {'title': 'çµæ´»è°ƒèŠ‚æ¨ç†è¿›ç¨‹çš„AlphaOneæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†AlphaOneï¼ˆalpha1ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨æµ‹è¯•æ—¶è°ƒèŠ‚å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ¨ç†è¿›ç¨‹çš„é€šç”¨æ¡†æ¶ã€‚alpha1é¦–å…ˆå¼•å…¥äº†alphaæ—¶åˆ»ï¼Œè¡¨ç¤ºå¸¦æœ‰é€šç”¨å‚æ•°alphaçš„ç¼©æ”¾æ€ç»´é˜¶æ®µã€‚åœ¨è¿™ä¸ªç¼©æ”¾çš„å‰alphaæ—¶åˆ»é˜¶æ®µä¸­ï¼Œå®ƒé€šè¿‡å°†æ¨ç†è¿‡æ¸¡æ ‡è®°çš„æ’å…¥å»ºæ¨¡ä¸ºä¼¯åŠªåˆ©éšæœºè¿‡ç¨‹ï¼ŒåŠ¨æ€è°ƒåº¦ç¼“æ…¢æ€ç»´çš„è¿‡æ¸¡ã€‚åœ¨alphaæ—¶åˆ»ä¹‹åï¼Œalpha1é€šè¿‡æ€ç»´ç»“æŸæ ‡è®°ç¡®å®šæ€§åœ°ç»ˆæ­¢ç¼“æ…¢æ€ç»´ï¼Œä»è€Œä¿ƒè¿›å¿«é€Ÿæ¨ç†å’Œé«˜æ•ˆç­”æ¡ˆç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24098', 'title': 'HardTests: Synthesizing High-Quality Test Cases for LLM Coding', 'url': 'https://huggingface.co/papers/2505.24098', 'abstract': 'Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.', 'score': 32, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '1a86a8aadff22dbb', 'authors': ['Zhongmou He', 'Yee Man Choi', 'Kexun Zhang', 'Jiabao Ji', 'Junting Zhou', 'Dejia Xu', 'Ivan Bercovich', 'Aidan Zhang', 'Lei Li'], 'affiliations': ['Carnegie Mellon University', 'UC Santa Barbara', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.24098.jpg', 'data': {'categories': ['#synthetic', '#reasoning', '#training', '#data', '#open_source', '#dataset'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HARDTESTGEN - Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ° ĞµĞ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… HARDTESTS, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 47 Ñ‚Ñ‹ÑÑÑ‡ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ¢ĞµÑÑ‚Ñ‹ HARDTESTGEN Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ¾Ğ´Ğ°, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ LLM, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸. HARDTESTS Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing LLM Evaluation with Synthetic Test Cases', 'desc': 'This paper introduces HARDTESTGEN, a new method for generating high-quality test cases for evaluating large language models (LLMs) in coding tasks. The challenge with existing verifiers is that they often fail to catch subtle errors in code, which can only be identified through complex human-written edge cases. HARDTESTGEN addresses this by synthesizing a dataset called HARDTESTS, which includes 47,000 programming problems along with high-quality tests generated by LLMs. The results show that tests from HARDTESTGEN significantly improve the precision and recall of evaluating LLM-generated code, making it a valuable tool for enhancing model training and performance.'}, 'zh': {'title': 'é«˜è´¨é‡æµ‹è¯•åˆæˆï¼Œæå‡LLMæ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHARDTESTGENçš„é«˜è´¨é‡æµ‹è¯•åˆæˆç®¡é“ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„éªŒè¯å™¨é—®é¢˜ã€‚ç”±äºéš¾ä»¥ä¸ºå¤æ‚ç¼–ç é—®é¢˜è·å–å¯é çš„éªŒè¯å™¨ï¼ŒHARDTESTGENèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¸®åŠ©è¯„ä¼°LLMç”Ÿæˆçš„ä»£ç ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«47,000ä¸ªé—®é¢˜çš„ç«äº‰ç¼–ç¨‹æ•°æ®é›†HARDTESTSï¼Œå¹¶ä¸”ä¸ç°æœ‰æµ‹è¯•ç›¸æ¯”ï¼ŒHARDTESTGENçš„æµ‹è¯•åœ¨ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚è¯¥æ•°æ®é›†å’Œåˆæˆç®¡é“å°†å¼€æºï¼Œä¾›ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.14752', 'title': 'Large Language Models for Data Synthesis', 'url': 'https://huggingface.co/papers/2505.14752', 'abstract': 'LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating synthetic data that faithfully captures the statistical structure of real-world distributions is a fundamental challenge in data modeling. Classical approaches often depend on strong parametric assumptions or manual structural design and struggle in high-dimensional or heterogeneous domains. Recent progress in Large Language Models (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. However, when applied to data synthesis, standard LLM-based sampling is inefficient, constrained by fixed context limits, and fails to ensure statistical alignment. Given this, we introduce LLMSynthor, a general framework for data synthesis that transforms LLMs into structure-aware simulators guided by distributional feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for modeling high-order dependencies and introduces LLM Proposal Sampling to generate grounded proposal distributions that improve sampling efficiency without requiring rejection. By minimizing discrepancies in the summary statistics space, the iterative synthesis loop aligns real and synthetic data while gradually uncovering and refining the latent generative structure. We evaluate LLMSynthor in both controlled and real-world settings using heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce, population, and mobility) that encompass both structured and unstructured formats. The synthetic data produced by LLMSynthor shows high statistical fidelity, practical utility, and cross-data adaptability, positioning it as a valuable tool across economics, social science, urban studies, and beyond.', 'score': 31, 'issue_id': 4067, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ', 'en': 'May 20', 'zh': '5æœˆ20æ—¥'}, 'hash': '202c77d3d43de6f6', 'authors': ['Yihong Tang', 'Menglin Kong', 'Lijun Sun'], 'affiliations': ['McGill University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14752.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#data'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'LLMSynthor: ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'LLMSynthor - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ñ‹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LLM ĞºĞ°Ğº Ğ½ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ¿ÑƒĞ»Ğ°-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ LLM Proposal Sampling Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. LLMSynthor Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ²Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ğº, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….'}, 'en': {'title': 'Transforming LLMs into Efficient Data Synthesizers', 'desc': 'LLMSynthor is a framework that enhances Large Language Models (LLMs) for creating synthetic data that accurately reflects real-world statistical distributions. It addresses the limitations of traditional data synthesis methods, which often rely on rigid assumptions and struggle with complex data types. By using distributional feedback and a novel LLM Proposal Sampling technique, LLMSynthor improves the efficiency and accuracy of data generation without the need for rejection sampling. The framework has been tested in various real-world scenarios, demonstrating its ability to produce high-quality synthetic data suitable for diverse applications.'}, 'zh': {'title': 'LLMSynthorï¼šé«˜æ•ˆçš„ç»Ÿè®¡æ•°æ®åˆæˆæ–°å·¥å…·', 'desc': 'LLMSynthor æ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å®ç°é«˜æ•ˆå’Œç»Ÿè®¡å‡†ç¡®çš„æ•°æ®åˆæˆçš„æ–¹æ³•ã€‚å®ƒé€šè¿‡åˆ†å¸ƒåé¦ˆå’Œæè®®é‡‡æ ·ï¼Œå°† LLM è½¬å˜ä¸ºç»“æ„æ„ŸçŸ¥çš„æ¨¡æ‹Ÿå™¨ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰çœŸå®ä¸–ç•Œåˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹å¾ã€‚è¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ–æ‘˜è¦ç»Ÿè®¡ç©ºé—´ä¸­çš„å·®å¼‚ï¼Œé€æ­¥å¯¹é½çœŸå®æ•°æ®å’Œåˆæˆæ•°æ®ï¼ŒåŒæ—¶æ­ç¤ºå’Œä¼˜åŒ–æ½œåœ¨çš„ç”Ÿæˆç»“æ„ã€‚LLMSynthor åœ¨éšç§æ•æ„Ÿé¢†åŸŸçš„å¼‚æ„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºé«˜ç»Ÿè®¡ä¿çœŸåº¦å’Œå®ç”¨æ€§ï¼Œé€‚ç”¨äºç»æµå­¦ã€ç¤¾ä¼šç§‘å­¦å’ŒåŸå¸‚ç ”ç©¶ç­‰å¤šä¸ªé¢†åŸŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18842', 'title': "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation", 'url': 'https://huggingface.co/papers/2505.18842', 'abstract': "v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces a simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the model's evolving hypotheses. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarks -- MathVista, MathVision, and MathVerse -- demonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research.", 'score': 28, 'issue_id': 4069, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ', 'en': 'May 24', 'zh': '5æœˆ24æ—¥'}, 'hash': 'a97f1e174b838d2d', 'authors': ['Jiwan Chung', 'Junhyeok Kim', 'Siyeol Kim', 'Jaeyoung Lee', 'Min Soo Kim', 'Youngjae Yu'], 'affiliations': ['Seoul National University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18842.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#games', '#architecture', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'v1 - ÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ½Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… v1g Ğ¸Ğ· 300 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ€Ğ°ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ v1 ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Dynamic Visual Retrieval for Enhanced Multimodal Reasoning', 'desc': "The paper introduces v1, an enhancement to Multimodal Large Language Models (MLLMs) that allows for selective and dynamic retrieval of visual information during inference. Unlike traditional MLLMs that process visual inputs only once, v1 employs a point-and-copy mechanism to revisit relevant image regions as the model generates responses. This approach improves the model's ability to perform multimodal reasoning tasks by providing contextual access to visual data based on its ongoing hypotheses. The authors validate v1's effectiveness through experiments on multiple benchmarks, showing significant performance gains in tasks that require detailed visual references and complex reasoning steps."}, 'zh': {'title': 'åŠ¨æ€è§†è§‰è®¿é—®æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›', 'desc': 'v1æ˜¯å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è½»é‡çº§æ‰©å±•ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°é€‰æ‹©æ€§è§†è§‰åŒºåŸŸçš„åŠ¨æ€æ£€ç´¢ã€‚ä¸ä¼ ç»Ÿçš„MLLMsä»…åœ¨å†…éƒ¨è®°å¿†ä¸­è¿›è¡Œæ¨ç†ä¸åŒï¼Œv1å¼•å…¥äº†ä¸€ç§ç®€å•çš„ç‚¹å¯¹ç‚¹å¤åˆ¶æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è·å–ç›¸å…³çš„å›¾åƒåŒºåŸŸã€‚é€šè¿‡æ„å»ºåŒ…å«30ä¸‡æ¡å¤šæ¨¡æ€æ¨ç†è½¨è¿¹çš„æ•°æ®é›†v1gï¼Œæ¨¡å‹å¾—ä»¥è®­ç»ƒè¿™ç§èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œv1åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç»†è‡´è§†è§‰å‚è€ƒå’Œå¤šæ­¥éª¤æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24862', 'title': 'ViStoryBench: Comprehensive Benchmark Suite for Story Visualization', 'url': 'https://huggingface.co/papers/2505.24862', 'abstract': "Story visualization, which aims to generate a sequence of visually coherent images aligning with a given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We collect a diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models' ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates a wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements.", 'score': 21, 'issue_id': 4072, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '40afeafebffbd255', 'authors': ['Cailin Zhuang', 'Ailin Huang', 'Wei Cheng', 'Jingwei Wu', 'Yaoqi Hu', 'Jiaqi Liao', 'Zhewei Huang', 'Hongyuan Wang', 'Xinyao Liao', 'Weiwei Cai', 'Hengyuan Xu', 'Xuanyang Zhang', 'Xianfang Zeng', 'Gang Yu', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'AIGC Research', 'ShanghaiTech University', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2505.24862.jpg', 'data': {'categories': ['#dataset', '#cv', '#story_generation', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ViStoryBench: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ - ViStoryBench. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ ÑÑĞ¶ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ViStoryBench ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸, ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑĞ¶ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Story Visualization with ViStoryBench', 'desc': 'This paper introduces ViStoryBench, a new evaluation benchmark designed to improve story visualization models that generate images based on narratives. It features a diverse dataset that includes various story types and artistic styles, allowing for a comprehensive assessment of model performance across different plots and visual aesthetics. The benchmark tests models on their ability to maintain character consistency and handle complex narratives with multiple protagonists. By providing a structured framework and a variety of evaluation metrics, ViStoryBench helps researchers identify strengths and weaknesses in their models, promoting targeted enhancements in story visualization.'}, 'zh': {'title': 'æå‡æ•…äº‹å¯è§†åŒ–çš„è¯„ä¼°åŸºå‡†', 'desc': 'æ•…äº‹å¯è§†åŒ–æ—¨åœ¨ç”Ÿæˆä¸ç»™å®šå™è¿°å’Œå‚è€ƒå›¾åƒä¸€è‡´çš„è§†è§‰å›¾åƒåºåˆ—ã€‚ä¸ºäº†æå‡æ•…äº‹å¯è§†åŒ–æ¡†æ¶åœ¨å®é™…åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼Œç§°ä¸ºViStoryBenchã€‚è¯¥åŸºå‡†æ”¶é›†äº†å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒç±»å‹çš„æ•…äº‹å’Œè‰ºæœ¯é£æ ¼ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæƒ…èŠ‚å’Œè§†è§‰ç¾å­¦ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ViStoryBenchç»è¿‡ç²¾å¿ƒç­–åˆ’ï¼Œå¹³è¡¡äº†å™äº‹ç»“æ„å’Œè§†è§‰å…ƒç´ ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜è¯†åˆ«ä¸åŒæ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼Œä¿ƒè¿›æœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24878', 'title': 'Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents', 'url': 'https://huggingface.co/papers/2505.24878', 'abstract': 'CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.', 'score': 15, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '9cceceaf09c77468', 'authors': ['Yaxin Luo', 'Zhaoyi Li', 'Jiacheng Liu', 'Jiacheng Cui', 'Xiaohan Zhao', 'Zhiqiang Shen'], 'affiliations': ['MetaAgentX', 'VILA Lab, MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24878.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Open CaptchaWorld: Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Open CaptchaWorld - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²ĞµĞ±-Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ CAPTCHA. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 20 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… CAPTCHA, Ğ²ÑĞµĞ³Ğ¾ 225 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¾Ğ¹ - Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ CAPTCHA. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑĞ´Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ MLLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ Ñ 40% Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Open CaptchaWorld Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking the Future: Evaluating MLLM Agents with Open CaptchaWorld', 'desc': 'This paper introduces Open CaptchaWorld, a new benchmark designed to test the capabilities of multimodal large language model (MLLM) agents in solving CAPTCHA puzzles. It evaluates the visual reasoning and interaction skills of these agents through a variety of 225 CAPTCHA types, measuring their performance with a novel metric called CAPTCHA Reasoning Depth. Experimental results reveal that while humans achieve high success rates, MLLM agents struggle significantly, with a maximum success rate of only 40%. This underscores the need for improved multimodal reasoning systems and positions Open CaptchaWorld as a crucial tool for assessing and enhancing agent performance in complex tasks.'}, 'zh': {'title': 'çªç ´CAPTCHAç“¶é¢ˆï¼Œæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼', 'desc': 'CAPTCHAåœ¨å®é™…åº”ç”¨ä¸­æ˜¯éƒ¨ç½²ç½‘ç»œä»£ç†çš„ä¸€ä¸ªé‡è¦ç“¶é¢ˆï¼Œå¸¸å¸¸é˜»ç¢å®ƒä»¬å®Œæˆç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚è™½ç„¶ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨é™æ€æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†äº¤äº’å¼ã€å¤šæ­¥éª¤æ¨ç†æŒ‘æˆ˜ï¼ˆå¦‚CAPTCHAï¼‰æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æµ‹è¯•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Open CaptchaWorldï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMä»£ç†çš„è§†è§‰æ¨ç†å’Œäº¤äº’èƒ½åŠ›çš„ç½‘ç»œåŸºå‡†å¹³å°ï¼Œæ¶µç›–20ç§ç°ä»£CAPTCHAç±»å‹ï¼Œå…±225ä¸ªCAPTCHAï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼šCAPTCHAæ¨ç†æ·±åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»çš„æˆåŠŸç‡æ¥è¿‘å®Œç¾ï¼Œè€Œæœ€å…ˆè¿›çš„MLLMä»£ç†çš„æˆåŠŸç‡æœ€é«˜ä»…ä¸º40.0%ï¼Œè¿œä½äºäººç±»çš„93.3%ï¼Œè¿™çªæ˜¾äº†Open CaptchaWorldä½œä¸ºè¯Šæ–­å½“å‰å¤šæ¨¡æ€ä»£ç†å±€é™æ€§çš„é‡è¦åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23941', 'title': 'Vision Language Models are Biased', 'url': 'https://huggingface.co/papers/2505.23941', 'abstract': 'Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., "Adidas") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.', 'score': 15, 'issue_id': 4069, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '1c96442d8acb3ec5', 'authors': ['An Vo', 'Khai-Nguyen Nguyen', 'Mohammad Reza Taesiri', 'Vy Tuong Dang', 'Anh Totti Nguyen', 'Daeyoung Kim'], 'affiliations': ['Auburn University', 'College of William and Mary', 'KAIST', 'University of Alberta'], 'pdf_title_img': 'assets/pdf/title_img/2505.23941.jpg', 'data': {'categories': ['#multimodal', '#cv', '#hallucinations', '#ethics', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞºĞ¾Ğ³Ğ´Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM) Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ñ‹ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ VLM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 17,05%) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğµ ĞµÑ‰Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ°.'}, 'en': {'title': 'Unveiling Biases in Vision Language Models', 'desc': 'This paper investigates how large language models (LLMs) influence the performance of vision language models (VLMs) on visual tasks like counting and identification. The authors find that VLMs exhibit significant biases, leading to poor accuracy when recognizing visual elements, such as miscounting stripes on logos. Even when provided with counterfactual information, such as the name of the subject, the accuracy of VLMs decreases further. The study highlights a critical failure mode in VLMs and introduces a framework for assessing these biases systematically.'}, 'zh': {'title': 'è§†è§‰è¯­è¨€æ¨¡å‹çš„åè§é—®é¢˜', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»äº’è”ç½‘ä¸­è®°å¿†äº†å¤§é‡çŸ¥è¯†ï¼Œè¿™å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰å¸®åŠ©ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´è¾“å‡ºåå‘é”™è¯¯æˆ–æœ‰åè§çš„ç­”æ¡ˆã€‚æˆ‘ä»¬ç ”ç©¶äº†æµè¡Œä¸»é¢˜çš„çŸ¥è¯†å¦‚ä½•å½±å“è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ ‡å‡†è§†è§‰ä»»åŠ¡ï¼ˆå¦‚è®¡æ•°å’Œè¯†åˆ«ï¼‰ä¸Šçš„å‡†ç¡®æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„VLMsåœ¨è®¡æ•°ä»»åŠ¡ä¸­çš„å¹³å‡å‡†ç¡®ç‡ä»…ä¸º17.05%ï¼Œå¹¶ä¸”åœ¨è¯†åˆ«å›¾æ¡ˆæ—¶å­˜åœ¨æ˜æ˜¾çš„åè§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†VLMsä¸­çš„ä¸€ç§æœ‰è¶£çš„å¤±è´¥æ¨¡å¼ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶æ¥æµ‹è¯•VLMçš„åè§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24025', 'title': 'DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models', 'url': 'https://huggingface.co/papers/2505.24025', 'abstract': 'DINO-R1 incorporates reinforcement learning to enhance visual in-context reasoning capabilities in vision foundation models, achieving better performance than supervised fine-tuning across various visual prompting scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose DINO-R1, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces Group Relative Query Optimization (GRQO), a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios.', 'score': 13, 'issue_id': 4081, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '88443982cc458e0e', 'authors': ['Chenbin Pan', 'Wenbin He', 'Zhengzhong Tu', 'Liu Ren'], 'affiliations': ['Bosch Center for Artificial Intelligence (BCAI)', 'Bosch Research North America', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24025.jpg', 'data': {'categories': ['#rl', '#reasoning', '#cv', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'DINO-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'DINO-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Group Relative Query Optimization (GRQO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. DINO-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… COCO, LVIS Ğ¸ ODinW Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Reinforcement Learning Boosts Visual Reasoning in DINO-R1', 'desc': 'DINO-R1 is a novel approach that uses reinforcement learning to improve the visual reasoning abilities of vision foundation models. It introduces a new training strategy called Group Relative Query Optimization (GRQO), which focuses on enhancing query-level performance by providing rewards based on alignment quality. Additionally, KL-regularization is applied to stabilize the training process and prevent issues like overfitting. The results show that DINO-R1 outperforms traditional supervised fine-tuning methods, demonstrating its effectiveness in various visual prompting tasks.'}, 'zh': {'title': 'DINO-R1ï¼šå¼ºåŒ–å­¦ä¹ æå‡è§†è§‰æ¨ç†èƒ½åŠ›çš„åˆ›æ–°å°è¯•', 'desc': 'DINO-R1 æ˜¯ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚å®ƒå¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥ï¼Œç§°ä¸ºç¾¤ä½“ç›¸å¯¹æŸ¥è¯¢ä¼˜åŒ–ï¼ˆGRQOï¼‰ï¼Œé€šè¿‡è®¡ç®—åŸºäºæŸ¥è¯¢çš„å¥–åŠ±æ¥æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹åœ¨ COCOã€LVIS å’Œ ODinW æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDINO-R1 åœ¨å¤šç§è§†è§‰æç¤ºåœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒDINO-R1 èƒ½å¤Ÿå®ç°æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚åº”å¼€æ”¾è¯æ±‡å’Œå°é—­é›†åˆçš„è§†è§‰æç¤ºä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21437', 'title': 'CoDA: Coordinated Diffusion Noise Optimization for Whole-Body\n  Manipulation of Articulated Objects', 'url': 'https://huggingface.co/papers/2505.21437', 'abstract': 'Synthesizing whole-body manipulation of articulated objects, including body motion, hand motion, and object motion, is a critical yet challenging task with broad applications in virtual humans and robotics. The core challenges are twofold. First, achieving realistic whole-body motion requires tight coordination between the hands and the rest of the body, as their movements are interdependent during manipulation. Second, articulated object manipulation typically involves high degrees of freedom and demands higher precision, often requiring the fingers to be placed at specific regions to actuate movable parts. To address these challenges, we propose a novel coordinated diffusion noise optimization framework. Specifically, we perform noise-space optimization over three specialized diffusion models for the body, left hand, and right hand, each trained on its own motion dataset to improve generalization. Coordination naturally emerges through gradient flow along the human kinematic chain, allowing the global body posture to adapt in response to hand motion objectives with high fidelity. To further enhance precision in hand-object interaction, we adopt a unified representation based on basis point sets (BPS), where end-effector positions are encoded as distances to the same BPS used for object geometry. This unified representation captures fine-grained spatial relationships between the hand and articulated object parts, and the resulting trajectories serve as targets to guide the optimization of diffusion noise, producing highly accurate interaction motion. We conduct extensive experiments demonstrating that our method outperforms existing approaches in motion quality and physical plausibility, and enables various capabilities such as object pose control, simultaneous walking and manipulation, and whole-body generation from hand-only data.', 'score': 13, 'issue_id': 4073, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '3b71d1aa0666ba54', 'authors': ['Huaijin Pi', 'Zhi Cen', 'Zhiyang Dou', 'Taku Komura'], 'affiliations': ['The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21437.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics', '#diffusion'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²ÑĞµĞ³Ğ¾ Ñ‚ĞµĞ»Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞ»Ğ° Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ÑĞ¾Ñ‡Ğ»ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ»Ğ°, Ñ€ÑƒĞº Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€ĞµÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚ĞµĞ»Ğ° Ğ¸ Ñ€ÑƒĞº. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞºĞ¸ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ±Ğ°Ğ·Ğ¸ÑĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº (BPS). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Coordinated Motion Synthesis for Realistic Manipulation', 'desc': 'This paper addresses the complex task of synthesizing whole-body movements for manipulating articulated objects in robotics and virtual humans. The authors introduce a coordinated diffusion noise optimization framework that enhances the synchronization between hand and body motions, which is crucial for realistic manipulation. By utilizing specialized diffusion models for different body parts and a unified representation of hand-object interactions, the method improves precision and generalization in motion generation. Experimental results show that this approach surpasses existing methods in terms of motion quality and physical realism, enabling advanced capabilities like simultaneous walking and manipulation.'}, 'zh': {'title': 'å…¨èº«æ“æ§çš„åè°ƒä¼˜åŒ–æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åè°ƒæ‰©æ•£å™ªå£°ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºåˆæˆå…¨èº«æ“æ§å…³èŠ‚ç‰©ä½“çš„è¿åŠ¨ï¼ŒåŒ…æ‹¬èº«ä½“ã€æ‰‹å’Œç‰©ä½“çš„è¿åŠ¨ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹ä¸‰ä¸ªä¸“é—¨çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå™ªå£°ç©ºé—´ä¼˜åŒ–ï¼Œåˆ†åˆ«é’ˆå¯¹èº«ä½“ã€å·¦æ‰‹å’Œå³æ‰‹è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ²¿ç€äººä½“è¿åŠ¨é“¾çš„æ¢¯åº¦æµåŠ¨ï¼Œåè°ƒæ€§è‡ªç„¶åœ°å‡ºç°ï¼Œä½¿å¾—å…¨èº«å§¿æ€èƒ½å¤Ÿé«˜ä¿çœŸåœ°å“åº”æ‰‹éƒ¨è¿åŠ¨ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¿åŠ¨è´¨é‡å’Œç‰©ç†åˆç†æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½å¤Ÿå®ç°ç‰©ä½“å§¿æ€æ§åˆ¶ã€åŒæ—¶è¡Œèµ°å’Œæ“æ§ç­‰å¤šç§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24196', 'title': 'CLaSp: In-Context Layer Skip for Self-Speculative Decoding', 'url': 'https://huggingface.co/papers/2505.24196', 'abstract': 'Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.', 'score': 12, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '004f8eaa8c2fe087', 'authors': ['Longze Chen', 'Renke Shan', 'Huiming Wang', 'Lu Wang', 'Ziqiang Liu', 'Run Luo', 'Jiawei Wang', 'Hamid Alinejad-Rokny', 'Min Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24196.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²: CLaSp - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ CLaSp Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. CLaSp Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° ÑĞ»Ğ¾ĞµĞ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° ÑĞ»Ğ¾ĞµĞ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CLaSp Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 1.3-1.7 Ñ€Ğ°Ğ·Ğ° Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞµÑ€Ğ¸Ğ¸ LLaMA3 Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Accelerating LLM Decoding with Layer-Skipping Efficiency', 'desc': 'This paper introduces CLaSp, a novel approach to speculative decoding that enhances the efficiency of Large Language Models (LLMs) without the need for additional training modules. CLaSp utilizes an in-context layer-skipping strategy, allowing it to create a compressed draft model by skipping certain layers in the verify model. The method employs a dynamic programming algorithm to optimize the layer-skipping process, adapting after each verification stage based on the hidden states. Experimental results show that CLaSp can speed up the decoding process by 1.3x to 1.7x on LLaMA3 models while maintaining the quality of the generated text.'}, 'zh': {'title': 'CLaSpï¼šåŠ é€Ÿè§£ç çš„æ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCLaSpçš„è‡ªæˆ‘æ¨æµ‹è§£ç ç­–ç•¥ï¼Œæ—¨åœ¨åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„è§£ç è¿‡ç¨‹ã€‚CLaSpé€šè¿‡è·³è¿‡éªŒè¯æ¨¡å‹çš„ä¸­é—´å±‚ï¼Œæ„å»ºä¸€ä¸ªå‹ç¼©çš„è‰ç¨¿æ¨¡å‹ï¼Œä»è€Œé¿å…äº†é¢å¤–æ¨¡å—çš„è®­ç»ƒéœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŠ¨æ€è§„åˆ’ç®—æ³•ä¼˜åŒ–å±‚è·³è¿‡è¿‡ç¨‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ¯ä¸ªéªŒè¯é˜¶æ®µååŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLaSpåœ¨LLaMA3ç³»åˆ—æ¨¡å‹ä¸Šå®ç°äº†1.3å€åˆ°1.7å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆæ–‡æœ¬çš„åŸå§‹åˆ†å¸ƒä¸å˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23009', 'title': 'EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge', 'url': 'https://huggingface.co/papers/2505.23009', 'abstract': "A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on EmergentTTS, we introduce EmergentTTS-Eval, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-case generation and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation https://github.com/boson-ai/EmergentTTS-Eval-public{code} and the https://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.", 'score': 12, 'issue_id': 4067, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '8ed75b2649e36558', 'authors': ['Ruskin Raj Manku', 'Yuzhi Tang', 'Xingjian Shi', 'Mu Li', 'Alex Smola'], 'affiliations': ['Boson AI, Santa Clara, CA 95054'], 'pdf_title_img': 'assets/pdf/title_img/2505.23009.jpg', 'data': {'categories': ['#games', '#benchmark', '#open_source', '#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': "EmergentTTS-Eval - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Text-to-Speech (TTS). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LALM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸, Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºÑƒ, Ğ¸Ğ½Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ĞºĞ°Ğº-ÑÑƒĞ´ÑŒÑ' Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ TTS ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹."}, 'en': {'title': 'Automating TTS Evaluation for Nuanced Speech Outputs', 'desc': 'The paper presents EmergentTTS-Eval, a new benchmark for evaluating Text-to-Speech (TTS) systems that focuses on complex and nuanced text. It automates the generation of test cases using Large Language Models (LLMs) and evaluates the outputs with a Large Audio Language Model (LALM). The benchmark includes six challenging scenarios, such as emotional expression and complex pronunciation, and generates 1,645 diverse test cases from a small set of human-written prompts. The results show that this automated approach provides a reliable assessment of TTS systems, correlating well with human evaluations.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿçš„EmergentTTS-Eval', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨é¢çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åŸºå‡†æµ‹è¯•å·¥å…·EmergentTTS-Evalï¼Œæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆå’Œè¯„ä¼°æµ‹è¯•æ¡ˆä¾‹ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤„ç†å¤æ‚è¯­ä¹‰æ–‡æœ¬æ—¶çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ¶µç›–å…­ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„TTSåœºæ™¯ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿã€æ—è¯­è¨€ã€å¤–è¯­ã€å¥æ³•å¤æ‚æ€§ã€å¤æ‚å‘éŸ³ï¼ˆå¦‚ç½‘å€ã€å…¬å¼ï¼‰å’Œé—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿­ä»£æ‰©å±•äººç±»ç¼–å†™çš„ç§å­æç¤ºï¼Œç”Ÿæˆäº†1645ä¸ªå¤šæ ·åŒ–çš„æµ‹è¯•æ¡ˆä¾‹ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ä»å¤šä¸ªç»´åº¦è¯„ä¼°è¯­éŸ³è¾“å‡ºï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºä¸åŒTTSç³»ç»Ÿä¹‹é—´çš„ç»†å¾®æ€§èƒ½å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24858', 'title': 'MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs', 'url': 'https://huggingface.co/papers/2505.24858', 'abstract': "A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of faithful confidence calibration of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that faithfully reflect their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.", 'score': 10, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '5cc52e721279a4e1', 'authors': ['Gabrielle Kaili-May Liu', 'Gal Yona', 'Avi Caciularu', 'Idan Szpektor', 'Tim G. J. Rudner', 'Arman Cohan'], 'affiliations': ['Google Research', 'New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24858.jpg', 'data': {'categories': ['#training', '#benchmark', '#alignment', '#interpretability', '#hallucinations', '#dataset'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ Ñ‡ĞµÑÑ‚Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ LLM Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ MetaFaith, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ĞµÑ‚Ğ°ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²ĞµÑ€Ğ½ÑƒÑ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Trust in LLMs through Better Uncertainty Communication', 'desc': 'This paper addresses the issue of how large language models (LLMs) communicate uncertainty, which is crucial for building trust in their outputs. The authors conduct a systematic study to evaluate how well LLMs express their confidence levels in a way that matches their actual uncertainty. They find that current methods for improving this communication are largely ineffective, and some can even worsen the situation. To solve this problem, they propose a new method called MetaFaith, which significantly enhances the ability of LLMs to convey uncertainty accurately, leading to better trustworthiness in their responses.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§è¡¨è¾¾ä¿¡ä»»åº¦', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸ç¡®å®šæ€§ä¼ è¾¾æ–¹é¢çš„å¯é æ€§ï¼ŒæŒ‡å‡ºå®ƒä»¬åœ¨è¡¨è¾¾é”™è¯¯ä¿¡æ¯æ—¶å¸¸ä½¿ç”¨è¿‡äºè‡ªä¿¡çš„è¯­è¨€ï¼Œä»è€Œå¯¼è‡´ç”¨æˆ·è¿‡åº¦ä¾èµ–å¹¶å‰Šå¼±ä¿¡ä»»ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†LLMsåœ¨ä½¿ç”¨ä¸ç¡®å®šæ€§è¯­è¨€è¡¨è¾¾å…¶å†…åœ¨ä¸ç¡®å®šæ€§æ–¹é¢çš„èƒ½åŠ›ï¼Œç»“æœæ˜¾ç¤ºå¤§å¤šæ•°æ¨¡å‹åœ¨è¿™æ–¹é¢è¡¨ç°ä¸ä½³ã€‚ç°æœ‰çš„å¹²é¢„æªæ–½æ•ˆæœæœ‰é™ï¼Œæ ‡å‡†æç¤ºæ–¹æ³•ä»…å¸¦æ¥å¾®å°æ”¹è¿›ï¼Œè€ŒåŸºäºäº‹å®çš„æ ¡å‡†æŠ€æœ¯ç”šè‡³å¯èƒ½å¯¹å¿ å®æ ¡å‡†äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MetaFaithï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæç¤ºçš„æ–°å‹æ ¡å‡†æ–¹æ³•ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ä¸åŒæ¨¡å‹å’Œä»»åŠ¡é¢†åŸŸçš„å¿ å®æ ¡å‡†æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24521', 'title': 'UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation', 'url': 'https://huggingface.co/papers/2505.24521', 'abstract': 'Recently, methods leveraging diffusion model priors to assist monocular geometric estimation (e.g., depth and normal) have gained significant attention due to their strong generalization ability. However, most existing works focus on estimating geometric properties within the camera coordinate system of individual video frames, neglecting the inherent ability of diffusion models to determine inter-frame correspondence. In this work, we demonstrate that, through appropriate design and fine-tuning, the intrinsic consistency of video generation models can be effectively harnessed for consistent geometric estimation. Specifically, we 1) select geometric attributes in the global coordinate system that share the same correspondence with video frames as the prediction targets, 2) introduce a novel and efficient conditioning method by reusing positional encodings, and 3) enhance performance through joint training on multiple geometric attributes that share the same correspondence. Our results achieve superior performance in predicting global geometric attributes in videos and can be directly applied to reconstruction tasks. Even when trained solely on static video data, our approach exhibits the potential to generalize to dynamic video scenes.', 'score': 10, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '4ae43b7cdb482867', 'authors': ['Yang-Tian Sun', 'Xin Yu', 'Zehuan Huang', 'Yi-Hua Huang', 'Yuan-Chen Guo', 'Ziyi Yang', 'Yan-Pei Cao', 'Xiaojuan Qi'], 'affiliations': ['Beihang University', 'The University of Hong Kong', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2505.24521.jpg', 'data': {'categories': ['#video', '#diffusion', '#optimization', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸, Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ñ‹ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Harnessing Diffusion Models for Consistent Geometric Estimation in Videos', 'desc': "This paper explores the use of diffusion models to improve the estimation of geometric properties like depth and normals in videos. Unlike previous methods that focus on individual frames, this approach leverages the relationships between frames to enhance consistency in geometric estimation. The authors propose a novel conditioning method that reuses positional encodings and advocate for joint training on multiple geometric attributes. Their results show improved performance in predicting global geometric attributes, demonstrating the model's ability to generalize even from static video data to dynamic scenes."}, 'zh': {'title': 'åˆ©ç”¨æ‰©æ•£æ¨¡å‹æå‡è§†é¢‘å‡ ä½•ä¼°è®¡çš„ä¸€è‡´æ€§', 'desc': 'æœ€è¿‘ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å…ˆéªŒæ¥è¾…åŠ©å•ç›®å‡ ä½•ä¼°è®¡ï¼ˆå¦‚æ·±åº¦å’Œæ³•çº¿ï¼‰çš„æ–¹æ³•å—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰å·¥ä½œé›†ä¸­åœ¨å•ä¸ªè§†é¢‘å¸§çš„ç›¸æœºåæ ‡ç³»å†…ä¼°è®¡å‡ ä½•å±æ€§ï¼Œå¿½è§†äº†æ‰©æ•£æ¨¡å‹åœ¨ç¡®å®šå¸§é—´å¯¹åº”å…³ç³»æ–¹é¢çš„å›ºæœ‰èƒ½åŠ›ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡é€‚å½“çš„è®¾è®¡å’Œå¾®è°ƒï¼Œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å†…åœ¨ä¸€è‡´æ€§æ¥è¿›è¡Œä¸€è‡´çš„å‡ ä½•ä¼°è®¡ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨å…¨å±€åæ ‡ç³»ä¸­ä¸è§†é¢‘å¸§å…±äº«ç›¸åŒå¯¹åº”å…³ç³»çš„å‡ ä½•å±æ€§ä½œä¸ºé¢„æµ‹ç›®æ ‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–é«˜æ•ˆçš„æ¡ä»¶æ–¹æ³•ï¼Œé€šè¿‡é‡ç”¨ä½ç½®ç¼–ç æ¥å¢å¼ºæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20873', 'title': 'Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.20873', 'abstract': 'The Fork-Merge Decoding strategy improves balanced multimodal understanding in audio-visual large language models by separating and then combining modality-specific reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t The goal of this work is to enhance balanced multimodal understanding in audio-visual large language models (AV-LLMs) by addressing modality bias without requiring additional training. In current AV-LLMs, audio and video features are typically processed jointly in the decoder. While this strategy facilitates unified multimodal understanding, it may introduce modality bias, where the model tends to over-rely on one modality due to imbalanced training signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet effective inference-time strategy that requires no additional training or architectural modifications. FMD first performs modality-specific reasoning by processing audio-only and video-only inputs through the early decoder layers (a fork phase), and then merges the resulting hidden states for joint reasoning in the remaining layers (a merge phase). This approach promotes balanced modality contributions and leverages complementary information across modalities. We evaluate our method on two representative AV-LLMs, VideoLLaMA2 and video-SALMONN, using three benchmark datasets. Experimental results demonstrate consistent performance improvements on tasks focused on audio, video, and combined audio-visual reasoning, demonstrating the effectiveness of inference-time interventions for robust multimodal understanding.', 'score': 9, 'issue_id': 4072, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '12352ddbed62a761', 'authors': ['Chaeyoung Jung', 'Youngjoon Jang', 'Jongmin Choi', 'Joon Son Chung'], 'affiliations': ['Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2505.20873.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#multimodal', '#reasoning'], 'emoji': 'ğŸ´', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Fork-Merge Decoding (FMD) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (AV-LLM). FMD Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ FMD ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼.'}, 'en': {'title': 'Fork-Merge Decoding: Balancing Audio-Visual Insights for Better Understanding', 'desc': 'This paper introduces the Fork-Merge Decoding (FMD) strategy to enhance multimodal understanding in audio-visual large language models (AV-LLMs). The method addresses the issue of modality bias, which occurs when models overly depend on one type of input, like audio or video. FMD operates by first analyzing audio and video inputs separately in the early decoder layers (fork phase) and then combining the insights in later layers (merge phase). This approach allows for a more balanced contribution from both modalities, improving performance on various tasks without needing extra training or changes to the model architecture.'}, 'zh': {'title': 'å‰åˆè§£ç ï¼šæå‡å¤šæ¨¡æ€ç†è§£çš„æœ‰æ•ˆç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå‰åˆè§£ç ï¼ˆFork-Merge Decoding, FMDï¼‰çš„ç­–ç•¥ï¼Œæ—¨åœ¨æ”¹å–„éŸ³è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAV-LLMsï¼‰çš„å¹³è¡¡å¤šæ¨¡æ€ç†è§£ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¨ç†é˜¶æ®µåˆ†å¼€å¤„ç†éŸ³é¢‘å’Œè§†é¢‘è¾“å…¥ï¼Œå…ˆè¿›è¡Œæ¨¡æ€ç‰¹å®šçš„æ¨ç†ï¼Œç„¶åå†åˆå¹¶ç»“æœï¼Œé¿å…äº†æ¨¡æ€åå·®çš„é—®é¢˜ã€‚FMDä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–æ¶æ„ä¿®æ”¹ï¼Œç®€å•æœ‰æ•ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨éŸ³é¢‘ã€è§†é¢‘åŠéŸ³è§†é¢‘ç»“åˆæ¨ç†ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ¨ç†æ—¶å¹²é¢„å¯¹å¢å¼ºå¤šæ¨¡æ€ç†è§£çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21523', 'title': 'More Thinking, Less Seeing? Assessing Amplified Hallucination in\n  Multimodal Reasoning Models', 'url': 'https://huggingface.co/papers/2505.21523', 'abstract': "A new metric and benchmark are introduced to evaluate multimodal large language models' ability to maintain visual grounding while performing extended reasoning, revealing that larger models and specific training data types improve this balance.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.", 'score': 9, 'issue_id': 4074, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '26f1abb0ea843b21', 'authors': ['Chengzhi Liu', 'Zhongxing Xu', 'Qingyue Wei', 'Juncheng Wu', 'James Zou', 'Xin Eric Wang', 'Yuyin Zhou', 'Sheng Liu'], 'affiliations': ['Stanford University', 'UC Santa Barbara', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.21523.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° RH-AUC Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ RH-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾ĞºÑƒÑĞ° Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° ÑÑ‚Ğ¾Ñ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡ĞµĞ¼ Ğ¸Ñ… Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼.'}, 'en': {'title': 'Balancing Reasoning and Visual Grounding in Multimodal Models', 'desc': 'This paper introduces a new metric called RH-AUC to evaluate how well multimodal large language models maintain visual grounding while performing extended reasoning tasks. It highlights that as reasoning chains become longer, models often lose focus on visual inputs, leading to increased hallucination. The study also presents RH-Bench, a benchmark for assessing the trade-off between reasoning ability and hallucination across various multimodal tasks. Findings indicate that larger models and specific types of training data enhance the balance between reasoning and perception, emphasizing the need for evaluation methods that consider both aspects together.'}, 'zh': {'title': 'æå‡æ¨ç†ä¸è§†è§‰æ„ŸçŸ¥çš„å¹³è¡¡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿›è¡Œæ‰©å±•æ¨ç†æ—¶ä¿æŒè§†è§‰åŸºç¡€çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè¾ƒå¤§çš„æ¨¡å‹å’Œç‰¹å®šç±»å‹çš„è®­ç»ƒæ•°æ®å¯ä»¥æ”¹å–„æ¨ç†ä¸è§†è§‰æ„ŸçŸ¥ä¹‹é—´çš„å¹³è¡¡ã€‚é€šè¿‡å¼•å…¥RH-AUCæŒ‡æ ‡ï¼Œèƒ½å¤Ÿé‡åŒ–æ¨¡å‹åœ¨æ¨ç†é•¿åº¦å˜åŒ–æ—¶çš„æ„ŸçŸ¥å‡†ç¡®æ€§ï¼Œä»è€Œè¯„ä¼°æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¯å¦ä¿æŒè§†è§‰åŸºç¡€ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹çš„å¤§å°å’Œè®­ç»ƒæ•°æ®çš„ç±»å‹å¯¹æ¨ç†èƒ½åŠ›å’Œå¹»è§‰ä¹‹é—´çš„æƒè¡¡æœ‰æ˜¾è‘—å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24850', 'title': 'Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.24850', 'abstract': "Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.", 'score': 8, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'e946031c286b5bf4', 'authors': ['Shuyao Xu', 'Cheng Peng', 'Jiangxuan Long', 'Weidi Xu', 'Wei Chu', 'Yuan Qi'], 'affiliations': ['AI3 Institute of Fudan University', 'INFLY TECH (Shanghai) Co., Ltd.', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.24850.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#dataset', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'REDI: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Reinforcement Distillation (REDI). REDI Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ REDI. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ REDI Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°.'}, 'en': {'title': 'Maximizing Reasoning Performance with Reinforcement Distillation', 'desc': 'This paper introduces Reinforcement Distillation (REDI), a two-stage framework designed to enhance the reasoning capabilities of smaller models by utilizing both positive and negative reasoning examples. In the first stage, the model is fine-tuned using positive reasoning traces through Supervised Fine-Tuning (SFT). The second stage refines the model further by incorporating both types of traces with a novel, reference-free loss function that improves performance over traditional methods like DPO and SimPO. Empirical results show that the Qwen-REDI-1.5B model achieves impressive scores on mathematical reasoning tasks, outperforming larger models trained on more extensive proprietary datasets.'}, 'zh': {'title': 'å¼ºåŒ–è’¸é¦ï¼šæå‡æ¨ç†æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ­£è´Ÿæ¨ç†è½¨è¿¹æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå¼ºåŒ–è’¸é¦ï¼ˆREDIï¼‰çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å­¦ä¹ æ­£æ¨ç†è½¨è¿¹ï¼Œç¬¬äºŒé˜¶æ®µåˆ™ç»“åˆæ­£è´Ÿæ¨ç†è½¨è¿¹è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ã€‚æˆ‘ä»¬çš„REDIç›®æ ‡æ˜¯ä¸€ä¸ªç®€å•çš„æ— å‚è€ƒæŸå¤±å‡½æ•°ï¼Œåœ¨è’¸é¦ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿæ–¹æ³•å¦‚DPOå’ŒSimPOã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡131kæ­£è´Ÿæ ·æœ¬è®­ç»ƒçš„Qwen-REDI-1.5Bæ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†83.1%çš„å¾—åˆ†ï¼Œåˆ›é€ äº†1.5Bæ¨¡å‹çš„æ–°çŠ¶æ€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24417', 'title': 'EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering', 'url': 'https://huggingface.co/papers/2505.24417', 'abstract': 'Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration.', 'score': 8, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'f28c426fafe8156a', 'authors': ['Runnan Lu', 'Yuxuan Zhang', 'Jailing Liu', 'Haifa Wang', 'Yiren Song'], 'affiliations': ['Liblib AI', 'National University of Singapore', 'The Chinese University of Hong Kong', 'Tiamat AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24417.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#multilingual', '#cv', '#diffusion'], 'emoji': 'ğŸŒ', 'ru': {'title': 'EasyText: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EasyText - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ DiT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°.'}, 'en': {'title': 'EasyText: Multilingual Text Rendering Made Simple', 'desc': "This paper presents EasyText, a novel framework for generating multilingual text using diffusion models. It leverages a Diffusion Transformer (DiT) to connect denoising latents with multilingual character tokens, addressing the challenge of rendering text in various languages. The authors introduce innovative techniques such as character positioning encoding and position encoding interpolation to enhance the control and precision of text rendering. They also create a large-scale dataset with 1 million multilingual image-text pairs, which significantly improves the model's performance in multilingual text rendering and visual quality."}, 'zh': {'title': 'å¤šè¯­è¨€æ–‡æœ¬æ¸²æŸ“çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºEasyTextçš„æ–‡æœ¬æ¸²æŸ“æ¡†æ¶ï¼ŒåŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æŠ€æœ¯ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å»å™ªæ½œå˜é‡ä¸å¤šè¯­è¨€å­—ç¬¦ä»¤ç‰Œè¿æ¥ï¼Œå®ç°äº†å¯¹å¤šè¯­è¨€æ–‡æœ¬çš„ç²¾ç¡®æ¸²æŸ“ã€‚æˆ‘ä»¬æå‡ºäº†å­—ç¬¦ä½ç½®ç¼–ç å’Œä½ç½®ç¼–ç æ’å€¼æŠ€æœ¯ï¼Œä»¥å®ç°å¯æ§å’Œç²¾ç¡®çš„æ–‡æœ¬æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«100ä¸‡æ¡å¤šè¯­è¨€å›¾åƒ-æ–‡æœ¬æ³¨é‡Šçš„å¤§è§„æ¨¡åˆæˆæ–‡æœ¬å›¾åƒæ•°æ®é›†ï¼Œç”¨äºé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šè¯­è¨€æ–‡æœ¬æ¸²æŸ“å’Œè§†è§‰è´¨é‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24871', 'title': 'MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.24871', 'abstract': "A framework for post-training multimodal large language models using reinforcement learning with verifiable rewards introduces a data mixture strategy to enhance general reasoning abilities and benchmark performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline.", 'score': 7, 'issue_id': 4081, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'a9ad14edf2493ec8', 'authors': ['Yiqing Liang', 'Jielin Qiu', 'Wenhao Ding', 'Zuxin Liu', 'James Tompkin', 'Mengdi Xu', 'Mengzhou Xia', 'Zhengzhong Tu', 'Laixi Shi', 'Jiacheng Zhu'], 'affiliations': ['Brown University', 'California Institute of Technology', 'Carnegie Mellon University', 'MIT CSAIL', 'NVIDIA Research', 'Princeton University', 'Salesforce AI Research', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24871.jpg', 'data': {'categories': ['#rl', '#reasoning', '#multimodal', '#optimization', '#dataset', '#training', '#rag', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ MLLM. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸.'}, 'en': {'title': 'Boosting Multimodal Models with Smart Data Mixing', 'desc': 'This paper presents a new framework for enhancing multimodal large language models (MLLMs) using Reinforcement Learning with Verifiable Rewards (RLVR). The authors address the challenges of training MLLMs on diverse datasets that require complex reasoning across visual and textual information. They propose a systematic approach that includes a data mixture strategy to optimize the training process and improve generalization. Experimental results demonstrate that their method significantly increases the accuracy of MLLMs on various benchmarks, outperforming traditional uniform data mixtures.'}, 'zh': {'title': 'ä¼˜åŒ–æ•°æ®æ··åˆï¼Œæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åè®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæ¨ç†èƒ½åŠ›å’ŒåŸºå‡†æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥æ•°æ®æ··åˆç­–ç•¥ï¼Œè§£å†³äº†å¤šæ•°æ®é›†è®­ç»ƒä¸­ç›®æ ‡å†²çªçš„é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»“åˆå¤šé¢†åŸŸçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå’Œæ··åˆé¢„æµ‹ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ•°æ®æ··åˆç­–ç•¥ä½¿å¾—åè®­ç»ƒæ¨¡å‹åœ¨åˆ†å¸ƒå¤–åŸºå‡†ä¸Šçš„å‡†ç¡®ç‡å¹³å‡æé«˜äº†5.24%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24785', 'title': 'EXP-Bench: Can AI Conduct AI Research Experiments?', 'url': 'https://huggingface.co/papers/2505.24785', 'abstract': "EXP-Bench evaluates AI agents' end-to-end research experiment capabilities through curated tasks from top AI papers, highlighting current limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.", 'score': 7, 'issue_id': 4082, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '6aafc5c29245622f', 'authors': ['Patrick Tser Jern Kon', 'Jiachen Liu', 'Xinyi Zhu', 'Qiuyi Ding', 'Jingjia Peng', 'Jiarong Xing', 'Yibo Huang', 'Yiming Qiu', 'Jayanth Srinivasa', 'Myungjin Lee', 'Mosharaf Chowdhury', 'Matei Zaharia', 'Ang Chen'], 'affiliations': ['Cisco Research', 'Rice University', 'UC Berkeley', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2505.24785.jpg', 'data': {'categories': ['#agents', '#benchmark', '#science', '#open_source'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'EXP-Bench: ĞÑ†ĞµĞ½ĞºĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…', 'desc': 'EXP-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 461 Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ· 51 Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ğ¾ Ğ˜Ğ˜, Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ ÑƒĞ´Ğ°ĞµÑ‚ÑÑ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ñ€ĞµĞ´ĞºĞ¾. EXP-Bench Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'EXP-Bench: Elevating AI Agents in Research Experimentation', 'desc': 'EXP-Bench is a benchmark designed to evaluate the capabilities of AI agents in conducting end-to-end research experiments. It presents AI agents with tasks derived from top AI papers, requiring them to formulate hypotheses, design experiments, implement procedures, and analyze results. The benchmark highlights the current limitations of AI agents, as they often struggle to execute complete experiments successfully. By providing a structured approach to experimental tasks, EXP-Bench aims to enhance the ability of future AI agents in performing rigorous scientific research.'}, 'zh': {'title': 'EXP-Benchï¼šæå‡AIç ”ç©¶å®éªŒèƒ½åŠ›çš„åŸºå‡†æµ‹è¯•', 'desc': 'EXP-Benchæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†åœ¨å®Œæ•´ç ”ç©¶å®éªŒä¸­çš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡ä»é¡¶çº§AIè®ºæ–‡ä¸­æå–å’Œç»“æ„åŒ–å®éªŒç»†èŠ‚ï¼Œåˆ›å»ºäº†461ä¸ªAIç ”ç©¶ä»»åŠ¡ã€‚å°½ç®¡ä¸€äº›åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨å®éªŒè®¾è®¡å’Œå®ç°çš„æŸäº›æ–¹é¢å¾—åˆ†è¾¾åˆ°20-35%ï¼Œä½†å®Œæ•´å¯æ‰§è¡Œå®éªŒçš„æˆåŠŸç‡ä»…ä¸º0.5%ã€‚é€šè¿‡è¯†åˆ«è¿™äº›ç“¶é¢ˆï¼ŒEXP-Benchä¸ºæœªæ¥çš„AIä»£ç†æä¾›äº†æ”¹è¿›å…¶è¿›è¡ŒAIç ”ç©¶å®éªŒèƒ½åŠ›çš„é‡è¦å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24293', 'title': 'Large Language Models are Locally Linear Mappings', 'url': 'https://huggingface.co/papers/2505.24293', 'abstract': 'We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.', 'score': 7, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '42a9e20ff9742560', 'authors': ['James R. Golden'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24293.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#inference', '#optimization'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ğ² ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ»Ğ¸ ĞºÑƒÑĞ¾Ñ‡Ğ½ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒ, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Llama 3, Gemma 3 Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ¾Ğ±Ğ¸Ğ°Ğ½Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ LLM Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¸Ğ· ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ñ… ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼.'}, 'en': {'title': 'Unlocking LLMs: Linear Insights into Complex Predictions', 'desc': "This paper shows that the inference processes of large language models (LLMs) can be represented as linear systems without changing the model's weights or outputs. By modifying the gradient calculations for next-token predictions, the authors create a Jacobian that closely mirrors the model's predictions using linear methods. They analyze various LLMs and find that these models operate in low-dimensional spaces, where significant singular vectors correspond to key concepts for predicting the next token. This method allows for a deeper understanding of how each layer functions and reveals interpretable semantic structures in the predictions of LLMs."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„çº¿æ€§æœ¬è´¨', 'desc': 'æœ¬æ–‡å±•ç¤ºäº†å¤šä¸ªå¼€æ”¾æƒé‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ“ä½œå¯ä»¥æ˜ å°„åˆ°ä¸€ä¸ªå®Œå…¨ç­‰ä»·çš„çº¿æ€§ç³»ç»Ÿï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡æˆ–æ”¹å˜è¾“å‡ºé¢„æµ‹ã€‚æˆ‘ä»¬å€Ÿé‰´äº†å›¾åƒæ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°æ”¹å˜ç›¸å¯¹äºç»™å®šè¾“å…¥åºåˆ—çš„æ¢¯åº¦è®¡ç®—ï¼Œä½¿å¾—æ¨¡å‹çš„é›…å¯æ¯”çŸ©é˜µå‡ ä¹å®Œå…¨é‡ç°äº†çº¿æ€§ç³»ç»Ÿçš„å‰å‘é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ¨¡å‹ä¸ŠéªŒè¯äº†è¿™ç§æ–¹æ³•ï¼Œå¹¶é€šè¿‡å¯¹åˆ†ç¦»é›…å¯æ¯”çŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£ï¼Œå‘ç°è¿™äº›LLMsåœ¨æä½ç»´çš„å­ç©ºé—´ä¸­æ“ä½œï¼Œè®¸å¤šæœ€å¤§çš„å¥‡å¼‚å‘é‡è§£ç å‡ºä¸æœ€å¯èƒ½è¾“å‡ºæ ‡è®°ç›¸å…³çš„æ¦‚å¿µã€‚å°½ç®¡ç°ä»£LLMså…·æœ‰å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›å’Œå…¨å±€éçº¿æ€§ï¼Œä½†å¯ä»¥é€šè¿‡å‡ ä¹ç²¾ç¡®çš„å±€éƒ¨çº¿æ€§åˆ†è§£è¿›è¡Œè§£é‡Šï¼Œä»è€Œæä¾›å¯¹å…¶å†…éƒ¨è¡¨ç¤ºçš„æ´å¯Ÿï¼Œå¹¶æ­ç¤ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è¿‡ç¨‹ä¸­çš„å¯è§£é‡Šè¯­ä¹‰ç»“æ„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.13157', 'title': 'Role-Playing Evaluation for Large Language Models', 'url': 'https://huggingface.co/papers/2505.13157', 'abstract': 'A benchmark called Role-Playing Eval assesses Large Language Models in role-playing across emotional understanding, decision-making, moral alignment, and in-character consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval', 'score': 5, 'issue_id': 4073, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ', 'en': 'May 19', 'zh': '5æœˆ19æ—¥'}, 'hash': 'c1362083ff11ec99', 'authors': ['Yassine El Boudouri', 'Walter Nuninger', 'Julian Alvarez', 'Yvan Peter'], 'affiliations': ['Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France'], 'pdf_title_img': 'assets/pdf/title_img/2505.13157.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#alignment'], 'emoji': 'ğŸ­', 'ru': {'title': 'RPEval: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Role-Playing Eval (RPEval) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ğµ. RPEval Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ€ĞµÑÑƒÑ€ÑĞ¾ĞµĞ¼ĞºĞ¾ÑÑ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ RPEval Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº.'}, 'en': {'title': 'Assessing Role-Playing Skills in AI with RPEval', 'desc': 'The paper introduces Role-Playing Eval (RPEval), a benchmark for evaluating Large Language Models (LLMs) in their ability to role-play. It focuses on four critical aspects: emotional understanding, decision-making, moral alignment, and in-character consistency. The authors highlight the challenges of traditional evaluation methods, which can be resource-intensive and biased. RPEval aims to provide a standardized approach to assess these capabilities in LLMs, with the code and dataset made publicly available for further research.'}, 'zh': {'title': 'è§’è‰²æ‰®æ¼”è¯„ä¼°ï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç§°ä¸ºè§’è‰²æ‰®æ¼”è¯„ä¼°ï¼ˆRole-Playing Evalï¼ŒRPEvalï¼‰ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§’è‰²æ‰®æ¼”ä¸­çš„èƒ½åŠ›ã€‚è¯¥è¯„ä¼°æ¶µç›–äº†å››ä¸ªå…³é”®ç»´åº¦ï¼šæƒ…æ„Ÿç†è§£ã€å†³ç­–èƒ½åŠ›ã€é“å¾·ä¸€è‡´æ€§å’Œè§’è‰²ä¸€è‡´æ€§ã€‚ç”±äºäººç±»è¯„ä¼°èµ„æºæ¶ˆè€—å¤§ä¸”è‡ªåŠ¨è¯„ä¼°å¯èƒ½å­˜åœ¨åè§ï¼ŒRPEvalæ—¨åœ¨æä¾›ä¸€ç§æ›´æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ã€‚æ–‡ç« è¯¦ç»†æè¿°äº†RPEvalçš„æ„å»ºè¿‡ç¨‹ï¼Œå¹¶æä¾›äº†åŸºå‡†è¯„ä¼°ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24875', 'title': 'ReasonGen-R1: CoT for Autoregressive Image generation models through SFT\n  and RL', 'url': 'https://huggingface.co/papers/2505.24875', 'abstract': 'Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based "thinking" skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: aka.ms/reasongen.', 'score': 4, 'issue_id': 4078, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'f15f9f746431348a', 'authors': ['Yu Zhang', 'Yunqi Li', 'Yifan Yang', 'Rui Wang', 'Yuqing Yang', 'Dai Qi', 'Jianmin Bao', 'Dongdong Chen', 'Chong Luo', 'Lili Qiu'], 'affiliations': ['Fudan University', 'Microsoft Corporation', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24875.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#architecture', '#optimization', '#reasoning', '#rag', '#rl', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': "ReasonGen-R1 - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ²Ğ½Ğ¾Ğ¼Ñƒ 'Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ' Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Group Relative Policy Optimization Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReasonGen-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."}, 'en': {'title': 'Empowering Image Generation with Reasoning Skills', 'desc': "This paper presents ReasonGen-R1, a novel framework that combines chain-of-thought reasoning with reinforcement learning to enhance generative vision models. The first stage involves fine-tuning an autoregressive image generator using a new dataset of text-based rationales, allowing the model to 'think' before creating images. The second stage employs Group Relative Policy Optimization (GRPO) to refine the generated images based on feedback from a pretrained vision language model, ensuring high visual quality. The results show that ReasonGen-R1 outperforms existing models on various benchmarks, demonstrating its effectiveness in integrating reasoning into image generation."}, 'zh': {'title': 'æ¨ç†ä¸ç”Ÿæˆçš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºReasonGen-R1çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨å°†é“¾å¼æ€ç»´æ¨ç†ä¸ç”Ÿæˆè§†è§‰æ¨¡å‹ç»“åˆèµ·æ¥ã€‚é¦–å…ˆï¼Œé€šè¿‡åœ¨æ–°ç”Ÿæˆçš„æ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä½¿è‡ªå›å½’å›¾åƒç”Ÿæˆå™¨å…·å¤‡æ˜ç¡®çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œåˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œä¼˜åŒ–ï¼Œä»¥æé«˜è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasonGen-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå¼ºåŸºçº¿å’Œä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24615', 'title': 'Harnessing Large Language Models for Scientific Novelty Detection', 'url': 'https://huggingface.co/papers/2505.24615', 'abstract': 'In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.', 'score': 4, 'issue_id': 4071, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'e2151a4cb797ec9b', 'authors': ['Yan Liu', 'Zonglin Yang', 'Soujanya Poria', 'Thanh-Son Nguyen', 'Erik Cambria'], 'affiliations': ['Agency for Science, Technology and Research (A*STAR)', 'Nanyang Technological University', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2505.24615.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#science'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'LLM Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ´ĞµÑÑ… Ğ¸Ğ· LLM, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ´ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Harnessing LLMs for Effective Novelty Detection in Research', 'desc': 'This paper addresses the challenge of identifying novel research ideas in academia, which is hindered by the lack of suitable benchmark datasets for novelty detection (ND). The authors propose using large language models (LLMs) to enhance ND by creating two new datasets focused on marketing and NLP. They introduce a method to extract closure sets of related papers and summarize their main ideas using LLMs, which helps in understanding idea conception. Additionally, a lightweight retriever is trained to distill idea-level knowledge from LLMs, improving the efficiency and accuracy of idea retrieval for ND tasks, with experimental results showing superior performance over existing methods.'}, 'zh': {'title': 'åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡ç§‘å­¦æ–°é¢–æ€§æ£€æµ‹', 'desc': 'åœ¨ç§‘å­¦å¿«é€Ÿå‘å±•çš„æ—¶ä»£ï¼Œè¯†åˆ«æ–°é¢–çš„ç ”ç©¶æƒ³æ³•å˜å¾—è‡³å…³é‡è¦ä½†ä¹Ÿå……æ»¡æŒ‘æˆ˜ã€‚ç°æœ‰çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯æ— æ³•æœ‰æ•ˆè§£å†³æ–°é¢–æ€§æ£€æµ‹çš„é—®é¢˜ï¼Œå› ä¸ºæ–‡æœ¬ç›¸ä¼¼æ€§ä¸æƒ³æ³•æ„æ€ä¹‹é—´å­˜åœ¨å·®è·ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç§‘å­¦æ–°é¢–æ€§æ£€æµ‹ï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªæ–°çš„æ•°æ®é›†ï¼Œåˆ†åˆ«æ¥è‡ªå¸‚åœºè¥é”€å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æå–è®ºæ–‡ä¹‹é—´çš„å…³ç³»æ„å»ºæ•°æ®é›†ï¼Œå¹¶è®­ç»ƒè½»é‡çº§æ£€ç´¢å™¨ï¼Œä»è€Œå®ç°é«˜æ•ˆå‡†ç¡®çš„æƒ³æ³•æ£€ç´¢ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ–°é¢–æ€§æ£€æµ‹ä»»åŠ¡ä¸­ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24517', 'title': "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP", 'url': 'https://huggingface.co/papers/2505.24517', 'abstract': "Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un^2CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP.", 'score': 4, 'issue_id': 4071, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '7cec3d5f1feec6d4', 'authors': ['Yinqi Li', 'Jiahe Zhao', 'Hong Chang', 'Ruibing Hou', 'Shiguang Shan', 'Xilin Chen'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.24517.jpg', 'data': {'categories': ['#games', '#multimodal', '#benchmark', '#architecture', '#optimization', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ CLIP Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP (Contrastive Language-Image Pre-training) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ un^2CLIP, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ unCLIP. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ un^2CLIP Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ CLIP Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing CLIP with Un^2CLIP for Better Image Detail Capture', 'desc': 'This paper addresses the limitations of the Contrastive Language-Image Pre-training (CLIP) model, particularly its inability to capture fine details in images and its performance on dense-prediction tasks. The authors propose a novel approach called un^2CLIP, which utilizes a generative model known as unCLIP to enhance the image encoder of CLIP. By inverting the unCLIP model, the authors aim to improve the detail-capturing capabilities of CLIP while maintaining its alignment with text embeddings. Experimental results demonstrate that un^2CLIP outperforms both the original CLIP and previous enhancement methods across various multimodal tasks.'}, 'zh': {'title': 'æå‡CLIPæ¨¡å‹ï¼Œæ•æ‰æ›´å¤šè§†è§‰ç»†èŠ‚', 'desc': 'å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å·²æˆä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ç§è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒCLIPåœ¨åŒºåˆ†å›¾åƒçš„ç»†å¾®å·®åˆ«æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¹¶ä¸”åœ¨å¯†é›†é¢„æµ‹å’Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ”¹è¿›ç°æœ‰çš„CLIPæ¨¡å‹ï¼Œä»¥å°½å¯èƒ½æ•æ‰å›¾åƒä¸­çš„è§†è§‰ç»†èŠ‚ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸€ç§ç‰¹å®šç±»å‹çš„ç”Ÿæˆæ¨¡å‹unCLIPä¸ºå®ç°è¿™ä¸€ç›®æ ‡æä¾›äº†åˆé€‚çš„æ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23926', 'title': 'Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2505.23926', 'abstract': 'While scaling laws have transformed natural language processing and computer vision, 3D point cloud understanding has yet to reach that stage. This can be attributed to both the comparatively smaller scale of 3D datasets, as well as the disparate sources of the data itself. Point clouds are captured by diverse sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor, outdoor), each introducing unique scanning patterns, sampling densities, and semantic biases. Such domain heterogeneity poses a major barrier towards training unified models at scale, especially under the realistic constraint that domain labels are typically inaccessible at inference time. In this work, we propose Point-MoE, a Mixture-of-Experts architecture designed to enable large-scale, cross-domain generalization in 3D perception. We show that standard point cloud backbones degrade significantly in performance when trained on mixed-domain data, whereas Point-MoE with a simple top-k routing strategy can automatically specialize experts, even without access to domain labels. Our experiments demonstrate that Point-MoE not only outperforms strong multi-domain baselines but also generalizes better to unseen domains. This work highlights a scalable path forward for 3D understanding: letting the model discover structure in diverse 3D data, rather than imposing it via manual curation or domain supervision.', 'score': 4, 'issue_id': 4070, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '52d11240fa18198b', 'authors': ['Xuweiyi Chen', 'Wentao Zhou', 'Aruni RoyChowdhury', 'Zezhou Cheng'], 'affiliations': ['The MathWorks, Inc.', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.23926.jpg', 'data': {'categories': ['#3d', '#architecture', '#transfer_learning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ 3D Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Point-MoE - Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Experts Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Point-MoE ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Point-MoE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ 3D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… 3D Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking 3D Perception with Point-MoE: A Scalable Solution for Diverse Data', 'desc': 'This paper addresses the challenges in understanding 3D point clouds due to the diverse sources and characteristics of the data. It introduces Point-MoE, a Mixture-of-Experts architecture that enhances cross-domain generalization in 3D perception without needing domain labels during inference. The proposed method allows the model to automatically specialize its experts based on the data it encounters, improving performance on mixed-domain datasets. Experimental results show that Point-MoE outperforms existing models and effectively generalizes to new, unseen domains, paving the way for scalable 3D understanding.'}, 'zh': {'title': 'è®©3Dç†è§£æ›´æ™ºèƒ½ï¼šPoint-MoEæ¶æ„çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPoint-MoEçš„æ··åˆä¸“å®¶æ¶æ„ï¼Œæ—¨åœ¨å®ç°3Dç‚¹äº‘ç†è§£çš„è·¨åŸŸæ³›åŒ–ã€‚ç”±äº3Dæ•°æ®é›†è§„æ¨¡è¾ƒå°ä¸”æ¥æºå¤šæ ·ï¼Œä¼ ç»Ÿçš„ç‚¹äº‘æ¨¡å‹åœ¨æ··åˆåŸŸæ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚Point-MoEé€šè¿‡ç®€å•çš„top-kè·¯ç”±ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰åŸŸæ ‡ç­¾çš„æƒ…å†µä¸‹è‡ªåŠ¨ä¸“é—¨åŒ–ä¸“å®¶ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPoint-MoEåœ¨å¤šä¸ªåŸŸçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æœªè§åŸŸä¸Šå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23844', 'title': 'Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation', 'url': 'https://huggingface.co/papers/2505.23844', 'abstract': 'Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at https://github.com/ZLKong/LLM_Integration', 'score': 4, 'issue_id': 4066, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '252af3d7c602c2c3', 'authors': ['Zhenglun Kong', 'Zheng Zhan', 'Shiyue Hou', 'Yifan Gong', 'Xin Meng', 'Pengwei Sui', 'Peiyan Dong', 'Xuan Shen', 'Zifeng Wang', 'Pu Zhao', 'Hao Tang', 'Stratis Ioannidis', 'Yanzhi Wang'], 'affiliations': ['Google', 'Harvard University', 'Northeastern University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23844.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑ‡ĞµÑ‚Ğ° ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° 50% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Adaptive Knowledge Aggregation for Enhanced LLM Performance', 'desc': 'This paper presents a new framework for improving large language models (LLMs) by adaptively selecting and aggregating knowledge from multiple specialized LLMs. Traditional methods like ensemble and weight merging are limited by high memory usage and performance degradation due to knowledge interference. The proposed approach includes an adaptive selection network that identifies the most relevant LLMs and a dynamic weighted fusion strategy that leverages the strengths of these models. Experimental results show that this method significantly reduces knowledge interference and enhances the stability and scalability of knowledge aggregation.'}, 'zh': {'title': 'è‡ªé€‚åº”çŸ¥è¯†èšåˆï¼Œæ„å»ºæ›´å¼ºå¤§çš„è¯­è¨€æ¨¡å‹', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†é€šè¿‡ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•æŒç»­æ”¹è¿›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ•´åˆå…¶ä»–ä¸“ä¸šLLMsçš„èƒ½åŠ›æ—¶ã€‚ç°æœ‰çš„æ–¹æ³•å¦‚é›†æˆå’Œæƒé‡åˆå¹¶éœ€è¦å¤§é‡å†…å­˜ï¼Œå¹¶ä¸”éš¾ä»¥é€‚åº”å˜åŒ–çš„æ•°æ®ç¯å¢ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©å’Œèšåˆæ¥è‡ªä¸åŒLLMsçš„çŸ¥è¯†ï¼Œä»¥æ„å»ºä¸€ä¸ªæ›´å¼ºå¤§çš„å•ä¸€æ¨¡å‹ï¼Œé¿å…äº†é›†æˆæ–¹æ³•çš„é«˜å†…å­˜å¼€é”€å’Œæƒé‡åˆå¹¶çš„çµæ´»æ€§ä¸è¶³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°æ›´ç¨³å®šå’Œå¯æ‰©å±•çš„çŸ¥è¯†èšåˆè¿‡ç¨‹ï¼ŒåŒæ—¶å°†çŸ¥è¯†å¹²æ‰°å‡å°‘äº†50%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21864', 'title': 'DexUMI: Using Human Hand as the Universal Manipulation Interface for\n  Dexterous Manipulation', 'url': 'https://huggingface.co/papers/2505.21864', 'abstract': "DexUMI framework utilizes a wearable hand exoskeleton and high-fidelity robot hand inpainting to transfer dexterous manipulation skills from human hands to robot hands, achieving high task success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI includes hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap using a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. The software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86%.", 'score': 4, 'issue_id': 4072, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '5d1867db4168cab6', 'authors': ['Mengda Xu', 'Han Zhang', 'Yifan Hou', 'Zhenjia Xu', 'Linxi Fan', 'Manuela Veloso', 'Shuran Song'], 'affiliations': ['Carnegie Mellon University', 'Columbia University', 'J.P. Morgan AI Research', 'NVIDIA', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21864.jpg', 'data': {'categories': ['#robotics', '#transfer_learning', '#agents'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ½Ğ¾Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ', 'desc': 'DexUMI - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ Ñ€ÑƒĞºÑƒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€ÑƒĞºĞ°Ğ¼. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ½Ğ¾ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ ÑĞºĞ·Ğ¾ÑĞºĞµĞ»ĞµÑ‚Ğ° Ñ€ÑƒĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ÑƒĞºĞ¸ Ğ½Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. DexUMI Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¸. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ´ĞµĞºÑĞµÑ‚Ñ€Ğ¾Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ÑƒĞº Ğ±Ñ‹Ğ»Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ° ÑÑ€ĞµĞ´Ğ½ÑÑ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ 86%.'}, 'en': {'title': 'Bridging Human and Robot Dexterity with DexUMI', 'desc': 'The DexUMI framework is designed to enhance the transfer of dexterous manipulation skills from human hands to robotic hands. It combines a wearable hand exoskeleton for direct haptic feedback and a high-fidelity robot hand inpainting technique to create realistic training data. This approach minimizes the embodiment gap by adapting human movements to be compatible with robot hand kinematics. Through extensive real-world testing, DexUMI achieves an impressive average task success rate of 86%, showcasing its effectiveness in robotic skill transfer.'}, 'zh': {'title': 'DexUMIï¼šå°†äººç±»çµå·§æŠ€èƒ½è½¬ç§»åˆ°æœºå™¨äººæ‰‹çš„åˆ›æ–°æ¡†æ¶', 'desc': 'DexUMIæ¡†æ¶åˆ©ç”¨å¯ç©¿æˆ´æ‰‹éƒ¨å¤–éª¨éª¼å’Œé«˜ä¿çœŸæœºå™¨äººæ‰‹éƒ¨é‡ç»˜æŠ€æœ¯ï¼Œå°†äººç±»çš„çµå·§æ“ä½œæŠ€èƒ½è½¬ç§»åˆ°æœºå™¨äººæ‰‹ä¸Šï¼Œä»è€Œå®ç°é«˜ä»»åŠ¡æˆåŠŸç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¡¬ä»¶å’Œè½¯ä»¶çš„é€‚é…ï¼Œç¼©å°äº†äººç±»æ‰‹ä¸å„ç§æœºå™¨äººæ‰‹ä¹‹é—´çš„ä½“ç°å·®è·ã€‚ç¡¬ä»¶é€‚é…ä½¿ç”¨å¯ç©¿æˆ´æ‰‹éƒ¨å¤–éª¨éª¼ï¼Œå…è®¸åœ¨æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­ç›´æ¥è·å¾—è§¦è§‰åé¦ˆï¼Œå¹¶å°†äººç±»åŠ¨ä½œé€‚é…ä¸ºå¯è¡Œçš„æœºå™¨äººæ‰‹åŠ¨ä½œã€‚è½¯ä»¶é€‚é…åˆ™é€šè¿‡åœ¨è§†é¢‘æ•°æ®ä¸­ç”¨é«˜ä¿çœŸæœºå™¨äººæ‰‹é‡ç»˜æ›¿æ¢äººç±»æ‰‹ï¼Œå¼¥è¡¥äº†è§†è§‰å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24189', 'title': 'Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code\n  Workflows', 'url': 'https://huggingface.co/papers/2505.24189', 'abstract': 'Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.', 'score': 3, 'issue_id': 4075, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '6fcc18713ed36918', 'authors': ['Orlando Marquez Ayala', 'Patrice Bechard', 'Emily Chen', 'Maggie Baird', 'Jingfei Chen'], 'affiliations': ['ServiceNow'], 'pdf_title_img': 'assets/pdf/title_img/2505.24189.jpg', 'data': {'categories': ['#training', '#optimization', '#interpretability', '#small_models'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ JSON, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ SLM Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ° 10% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ LLM. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ LLM, SLM Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ĞµĞµ Ğ¸Ğ·-Ğ·Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Fine-Tuning SLMs: The Key to Quality in Domain-Specific Tasks', 'desc': 'This paper investigates the effectiveness of Small Language Models (SLMs) compared to Large Language Models (LLMs) like GPT-4o for specific tasks that require structured outputs. It highlights that while LLMs can perform well with appropriate prompts, fine-tuning SLMs can lead to a significant quality improvement, averaging a 10% increase in performance for generating low-code workflows in JSON format. The authors conduct a thorough error analysis to identify the limitations of both model types. Ultimately, the findings suggest that SLMs retain a quality advantage for certain domain-specific applications despite the reduced costs of using LLMs.'}, 'zh': {'title': 'å°å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è´¨é‡ä¼˜åŠ¿', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚GPT-4oèƒ½å¤Ÿé€šè¿‡åˆé€‚çš„æç¤ºå¤„ç†å¤šç§å¤æ‚ä»»åŠ¡ã€‚éšç€æ¯ä¸ªtokençš„æˆæœ¬é™ä½ï¼Œé’ˆå¯¹å®é™…åº”ç”¨å¾®è°ƒå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰çš„ä¼˜åŠ¿å¯èƒ½ä¸å†æ˜æ˜¾ã€‚æœ¬æ–‡æä¾›è¯æ®è¡¨æ˜ï¼Œå¯¹äºéœ€è¦ç»“æ„åŒ–è¾“å‡ºçš„ç‰¹å®šé¢†åŸŸä»»åŠ¡ï¼ŒSLMsä»ç„¶å…·æœ‰è´¨é‡ä¼˜åŠ¿ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†å¾®è°ƒSLMä¸ä½¿ç”¨æç¤ºçš„LLMåœ¨ç”ŸæˆJSONæ ¼å¼ä½ä»£ç å·¥ä½œæµä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°å¾®è°ƒå¹³å‡æé«˜äº†10%çš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20047', 'title': 'Grammars of Formal Uncertainty: When to Trust LLMs in Automated\n  Reasoning Tasks', 'url': 'https://huggingface.co/papers/2505.20047', 'abstract': "This research explores uncertainty quantification in large language models for generating formal specifications, introducing a PCFG framework to improve error detection and selective verification.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.", 'score': 3, 'issue_id': 4075, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '86bdf56529c01563', 'authors': ['Debargha Ganguly', 'Vikash Singh', 'Sreehari Sankar', 'Biyao Zhang', 'Xuecen Zhang', 'Srinivasan Iyengar', 'Xiaotian Han', 'Amit Sharma', 'Shivkumar Kalyanaraman', 'Vipin Chaudhary'], 'affiliations': ['Case Western Reserve University', 'Microsoft Corporation', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.20047.jpg', 'data': {'categories': ['#rl', '#reasoning', '#interpretability', '#data', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑĞ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ğº (PCFG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SMT Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ñ‚ĞºĞ°Ğ·Ğµ Ğ¾Ñ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ´ĞµĞ»Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹.'}, 'en': {'title': 'Bridging Uncertainty and Formal Verification in LLMs', 'desc': 'This paper investigates how to measure and manage uncertainty in large language models (LLMs) when they generate formal specifications. It highlights the challenge that LLMs are probabilistic, while formal verification requires certainty. The authors propose a new framework using probabilistic context-free grammar (PCFG) to better understand and categorize the uncertainty in LLM outputs. Their findings show that different tasks exhibit unique uncertainty patterns, and by combining these signals, they can significantly improve the accuracy of formal verification processes.'}, 'zh': {'title': 'æå‡LLMç”Ÿæˆè§„èŒƒçš„å¯é æ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œä¸ç¡®å®šæ€§é‡åŒ–ï¼Œä»¥ç”Ÿæˆæ­£å¼è§„èŒƒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¦‚ç‡ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•ï¼ˆPCFGï¼‰æ¡†æ¶ï¼Œä»¥æé«˜é”™è¯¯æ£€æµ‹å’Œé€‰æ‹©æ€§éªŒè¯çš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMç”Ÿæˆçš„æ­£å¼æ–‡æ¡£åœ¨ä¸åŒä»»åŠ¡ä¸­çš„ä¸ç¡®å®šæ€§ä¿¡å·æ˜¯ä¾èµ–äºä»»åŠ¡çš„ï¼Œä¸”ç°æœ‰çš„ä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯æœªèƒ½æœ‰æ•ˆè¯†åˆ«è¿™äº›é”™è¯¯ã€‚é€šè¿‡è½»é‡çº§èåˆè¿™äº›ä¿¡å·ï¼Œæˆ‘ä»¬æ˜¾è‘—å‡å°‘äº†é”™è¯¯ç‡ï¼Œä½¿LLMé©±åŠ¨çš„å½¢å¼åŒ–è¿‡ç¨‹å˜å¾—æ›´åŠ å¯é ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24581', 'title': 'GATE: General Arabic Text Embedding for Enhanced Semantic Textual\n  Similarity with Matryoshka Representation Learning and Hybrid Loss Training', 'url': 'https://huggingface.co/papers/2505.24581', 'abstract': 'Semantic textual similarity (STS) is a critical task in natural language processing (NLP), enabling applications in retrieval, clustering, and understanding semantic relationships between texts. However, research in this area for the Arabic language remains limited due to the lack of high-quality datasets and pre-trained models. This scarcity of resources has restricted the accurate evaluation and advance of semantic similarity in Arabic text. This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.', 'score': 2, 'issue_id': 4073, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '380d404889c022d3', 'authors': ['Omer Nacar', 'Anis Koubaa', 'Serry Sibaee', 'Yasser Al-Habashi', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['Alfaisal University, Riyadh, Saudi Arabia', 'Prince Sultan University, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.24581.jpg', 'data': {'categories': ['#low_resource', '#benchmark', '#multilingual', '#science', '#training', '#transfer_learning', '#dataset'], 'emoji': 'ğŸ•Œ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GATE (General Arabic Text Embedding) Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ±Ğ»Ğ¸Ğ·Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² (STS) Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° MTEB. GATE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Matryoshka Representation Learning Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ğ¼Ğ¸ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ GATE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ OpenAI, Ğ½Ğ° 20-25% Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… STS, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ÑĞ°Ğ½ÑÑ‹ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Unlocking Arabic Semantics with GATE Models', 'desc': 'This paper addresses the challenge of semantic textual similarity (STS) in the Arabic language, which has been under-researched due to limited datasets and models. It presents the General Arabic Text Embedding (GATE) models, which utilize advanced techniques like Matryoshka Representation Learning and a hybrid loss training approach. GATE demonstrates significant improvements in STS tasks, outperforming larger models by 20-25% on benchmarks. This advancement is crucial for better understanding and processing the unique semantic characteristics of Arabic text.'}, 'zh': {'title': 'æå‡é˜¿æ‹‰ä¼¯è¯­è¯­ä¹‰ç†è§£çš„GATEæ¨¡å‹', 'desc': 'è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆSTSï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€ä¸ªé‡è¦ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ”¯æŒæ£€ç´¢ã€èšç±»å’Œç†è§£æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œé˜¿æ‹‰ä¼¯è¯­é¢†åŸŸçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚è¿™ç§èµ„æºçš„åŒ®ä¹é™åˆ¶äº†é˜¿æ‹‰ä¼¯æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼æ€§çš„å‡†ç¡®è¯„ä¼°å’Œè¿›å±•ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šç”¨é˜¿æ‹‰ä¼¯æ–‡æœ¬åµŒå…¥ï¼ˆGATEï¼‰æ¨¡å‹ï¼Œåœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†é˜¿æ‹‰ä¼¯è¯­çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23832', 'title': 'LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements\n  Generation', 'url': 'https://huggingface.co/papers/2505.23832', 'abstract': 'Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, we present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M legal cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.', 'score': 2, 'issue_id': 4073, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '8778ede47e6e60db', 'authors': ['Chaeeun Kim', 'Jinu Lee', 'Wonseok Hwang'], 'affiliations': ['LBOX', 'University of Illinois Urbana-Champaign', 'University of Seoul'], 'pdf_title_img': 'assets/pdf/title_img/2505.23832.jpg', 'data': {'categories': ['#benchmark', '#science', '#multimodal', '#reasoning', '#transfer_learning', '#dataset'], 'emoji': 'âš–ï¸', 'ru': {'title': 'LegalSearchLM: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ¼Ğ¾Ñ€Ğµ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ»', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ» - LegalSearchLM. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ´ĞµĞ» Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ» - LEGAR BENCH. LegalSearchLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 6-20% Ğ½Ğ° LEGAR BENCH Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ´ĞµĞ».'}, 'en': {'title': 'Revolutionizing Legal Case Retrieval with LEGAR BENCH and LegalSearchLM', 'desc': 'This paper addresses the challenges in Legal Case Retrieval (LCR) by introducing a new benchmark and a novel retrieval model. The authors present LEGAR BENCH, a comprehensive dataset that includes over 1.2 million legal cases and 411 different crime types, which enhances the evaluation of LCR systems. They also propose LegalSearchLM, a model that utilizes legal element reasoning to improve the relevance of retrieved cases by generating content based on the query case. Experimental results indicate that LegalSearchLM significantly outperforms existing methods, demonstrating better accuracy and generalization to diverse legal scenarios.'}, 'zh': {'title': 'æ³•å¾‹æ£€ç´¢çš„æ–°çªç ´ï¼šè¶…è¶Šä¼ ç»Ÿæ–¹æ³•', 'desc': 'æ³•å¾‹æ¡ˆä»¶æ£€ç´¢ï¼ˆLCRï¼‰æ˜¯æ³•å¾‹ä¸“ä¸šäººå‘˜åœ¨ç ”ç©¶å’Œå†³ç­–ä¸­è·å–ç›¸å…³æ¡ˆä¾‹çš„åŸºæœ¬ä»»åŠ¡ã€‚ç°æœ‰çš„LCRç ”ç©¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šä¸€æ˜¯è¯„ä¼°ä½¿ç”¨çš„æ£€ç´¢è¯­æ–™åº“è§„æ¨¡è¾ƒå°ï¼Œæ— æ³•åæ˜ çœŸå®æ³•å¾‹æ£€ç´¢åœºæ™¯çš„å¤æ‚æ€§ï¼›äºŒæ˜¯ä¾èµ–äºåµŒå…¥æˆ–è¯æ±‡åŒ¹é…æ–¹æ³•ï¼Œå¯¼è‡´è¡¨ç¤ºèƒ½åŠ›æœ‰é™å’Œæ³•å¾‹æ— å…³çš„åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LEGAR BENCHï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¶µç›–411ç§çŠ¯ç½ªç±»å‹å’Œ120ä¸‡æ³•å¾‹æ¡ˆä¾‹çš„å¤§è§„æ¨¡éŸ©å›½LCRåŸºå‡†ï¼Œä»¥åŠLegalSearchLMï¼Œä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œæ³•å¾‹å…ƒç´ æ¨ç†å¹¶ç”Ÿæˆä¸ç›®æ ‡æ¡ˆä¾‹ç›¸å…³å†…å®¹çš„æ£€ç´¢æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLegalSearchLMåœ¨LEGAR BENCHä¸Šæ¯”åŸºçº¿æ¨¡å‹æé«˜äº†6-20%çš„æ€§èƒ½ï¼Œå¹¶åœ¨è·¨é¢†åŸŸæ¡ˆä¾‹ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20977', 'title': 'Evaluating and Steering Modality Preferences in Multimodal Large\n  Language Model', 'url': 'https://huggingface.co/papers/2505.20977', 'abstract': 'MLLMs exhibit modality bias in multimodal processing, which can be controlled using a representation engineering method to improve tasks like hallucination mitigation and multimodal machine translation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a MC\\textsuperscript{2} benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements.', 'score': 2, 'issue_id': 4076, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'e3f911b43ad30492', 'authors': ['Yu Zhang', 'Jinlong Ma', 'Yongshuai Hou', 'Xuefeng Bai', 'Kehai Chen', 'Yang Xiang', 'Jun Yu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology, Shenzhen, China', 'Peng Cheng Laboratory, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.20977.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#machine_translation', '#hallucinations'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MCÂ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… MLLM. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´.'}, 'en': {'title': 'Controlling Modality Bias in Multimodal Models', 'desc': 'This paper investigates how multimodal large language models (MLLMs) show a tendency to favor one type of input (modality) over another when processing mixed information. The authors create a benchmark called MCÂ² to evaluate this modality bias under specific conditions where evidence from different modalities conflicts. They find that all tested MLLMs exhibit clear modality preferences, which can be adjusted using a new method based on representation engineering. This method allows for the control of modality bias without needing to retrain the models, leading to better performance in tasks like reducing hallucinations and improving multimodal translations.'}, 'zh': {'title': 'æ§åˆ¶æ¨¡æ€åå¥½ï¼Œæå‡å¤šæ¨¡æ€ä»»åŠ¡è¡¨ç°', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ—¶å¯èƒ½å­˜åœ¨æ¨¡æ€åå¥½ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªMCÂ²åŸºå‡†ï¼Œä»¥ç³»ç»Ÿè¯„ä¼°åœ¨è¯æ®å†²çªåœºæ™¯ä¸‹çš„æ¨¡æ€åå¥½ã€‚ç ”ç©¶å‘ç°ï¼Œæ‰€æœ‰æµ‹è¯•çš„18ä¸ªMLLMsæ™®éè¡¨ç°å‡ºæ˜æ˜¾çš„æ¨¡æ€åè§ï¼Œå¹¶ä¸”è¿™ç§åå¥½å¯ä»¥é€šè¿‡å¤–éƒ¨å¹²é¢„è¿›è¡Œè°ƒæ•´ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¡¨ç¤ºå·¥ç¨‹çš„æ¢æµ‹å’Œå¼•å¯¼æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸è¿›è¡Œé¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹æ˜¾å¼æ§åˆ¶æ¨¡æ€åå¥½ï¼Œä»è€Œåœ¨å¹»è§‰ç¼“è§£å’Œå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24869', 'title': 'SiLVR: A Simple Language-based Video Reasoning Framework', 'url': 'https://huggingface.co/papers/2505.24869', 'abstract': "SiLVR, a language-based framework, enhances multimodal LLMs' video reasoning by leveraging adaptive token reduction, achieving top results on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SiLVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SiLVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio/speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an adaptive token reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.", 'score': 1, 'issue_id': 4082, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'd03545b1ac06e77f', 'authors': ['Ce Zhang', 'Yan-Bo Lin', 'Ziyang Wang', 'Mohit Bansal', 'Gedas Bertasius'], 'affiliations': ['Department of Computer Science, UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2505.24869.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#reasoning', '#long_context'], 'emoji': 'ğŸ¥', 'ru': {'title': 'SiLVR: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜', 'desc': 'SiLVR - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½ÑƒÑ LLM Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. SiLVR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Video Reasoning with SiLVR: A Language-Based Approach', 'desc': 'SiLVR is a novel framework designed to improve the video reasoning abilities of multimodal large language models (MLLMs). It breaks down complex video understanding into two main stages: first, converting raw video into language-based representations using various sensory inputs, and second, utilizing a powerful reasoning LLM to interpret these representations. To efficiently manage long-context inputs, SiLVR employs an adaptive token reduction method that optimizes how tokens are sampled. This approach has led to SiLVR achieving top performance on multiple video-language benchmarks, demonstrating the potential of strong reasoning LLMs in processing and understanding video content.'}, 'zh': {'title': 'SiLVRï¼šæå‡è§†é¢‘æ¨ç†çš„è¯­è¨€æ¡†æ¶', 'desc': 'SiLVRæ˜¯ä¸€ä¸ªåŸºäºè¯­è¨€çš„è§†é¢‘æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†å¤æ‚çš„è§†é¢‘ç†è§£ä»»åŠ¡åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡å¤šæ„Ÿå®˜è¾“å…¥å°†åŸå§‹è§†é¢‘è½¬æ¢ä¸ºåŸºäºè¯­è¨€çš„è¡¨ç¤ºï¼›å…¶æ¬¡ï¼Œå°†è¿™äº›è¯­è¨€æè¿°è¾“å…¥åˆ°å¼ºå¤§çš„æ¨ç†å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥è§£å†³å¤æ‚çš„è§†é¢‘è¯­è¨€ç†è§£ä»»åŠ¡ã€‚SiLVRé‡‡ç”¨è‡ªé€‚åº”çš„ä»¤ç‰Œå‡å°‘æ–¹æ¡ˆï¼ŒåŠ¨æ€ç¡®å®šé‡‡æ ·ä»¤ç‰Œçš„æ—¶é—´ç²’åº¦ï¼Œä»è€Œæœ‰æ•ˆå¤„ç†é•¿ä¸Šä¸‹æ–‡çš„å¤šæ„Ÿå®˜è¾“å…¥ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³ç»“æœï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24782', 'title': 'Context is Gold to find the Gold Passage: Evaluating and Training\n  Contextual Document Embeddings', 'url': 'https://huggingface.co/papers/2505.24782', 'abstract': 'A context-aware benchmark and contrastive training method improve document retrieval quality by leveraging full-document context and maintaining computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations.   In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), a novel contrastive post-training approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes. We open-source all artifacts at https://github.com/illuin-tech/contextual-embeddings.', 'score': 1, 'issue_id': 4077, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '54b58cb49740c12b', 'authors': ['Max Conti', 'Manuel Faysse', 'Gautier Viaud', 'Antoine Bosselut', 'CÃ©line Hudelot', 'Pierre Colombo'], 'affiliations': ['CentraleSupÃ©lec, Paris-Saclay', 'EPFL Lausanne', 'Equall.ai', 'Illuin Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.24782.jpg', 'data': {'categories': ['#open_source', '#optimization', '#data', '#benchmark', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ConTEB Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ²ÑĞµĞ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ InSeNT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ConTEB Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğº Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Enhancing Document Retrieval with Contextual Awareness', 'desc': 'This paper addresses the limitations of current document retrieval methods that often treat text passages independently, missing out on important context from the entire document. It introduces ConTEB, a benchmark that evaluates how well retrieval models utilize document-wide context. The authors propose InSeNT, a new contrastive training method that enhances the learning of contextual representations while maintaining efficiency. Their findings demonstrate that this approach significantly improves retrieval quality, making the models more robust to various chunking strategies and larger datasets.'}, 'zh': {'title': 'æå‡æ–‡æ¡£æ£€ç´¢è´¨é‡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŸºå‡†æµ‹è¯•å’Œå¯¹æ¯”è®­ç»ƒæ–¹æ³•ï¼Œä»¥æé«˜æ–‡æ¡£æ£€ç´¢çš„è´¨é‡ã€‚ç°æœ‰çš„æ–‡æ¡£æ£€ç´¢åµŒå…¥æ–¹æ³•é€šå¸¸ç‹¬ç«‹ç¼–ç æ–‡æ¡£ä¸­çš„æ®µè½ï¼Œå¿½è§†äº†æ–‡æ¡£æ•´ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬å¼•å…¥äº†ConTEBï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–‡æœ¬åµŒå…¥åŸºå‡†ï¼‰ï¼Œç”¨äºè¯„ä¼°æ£€ç´¢æ¨¡å‹åˆ©ç”¨æ–‡æ¡£ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚é€šè¿‡æå‡ºInSeNTï¼ˆåºåˆ—å†…è´Ÿè®­ç»ƒï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24119', 'title': 'The State of Multilingual LLM Safety Research: From Measuring the\n  Language Gap to Mitigating It', 'url': 'https://huggingface.co/papers/2505.24119', 'abstract': 'This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations.', 'score': 1, 'issue_id': 4076, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'f5b6ed3de0b4cc5b', 'authors': ['Zheng-Xin Yong', 'Beyza Ermis', 'Marzieh Fadaee', 'Stephen H. Bach', 'Julia Kreutzer'], 'affiliations': ['Brown University', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.24119.jpg', 'data': {'categories': ['#low_resource', '#survey', '#ethics', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ 300 Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ·Ğ° 2020-2024 Ğ³Ğ¾Ğ´Ñ‹ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸ÑÑ… Ğ¸ ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€Ğ°Ñ… Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ğ¶Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ½ĞµĞ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜.'}, 'en': {'title': 'Bridging the Language Gap in LLM Safety Research', 'desc': 'This paper analyzes the focus on English in large language model (LLM) safety research, revealing a significant language gap. It reviews nearly 300 publications from major NLP conferences between 2020 and 2024, showing that non-English languages, even those with ample resources, are largely overlooked. The authors highlight the lack of standalone studies on non-English languages and poor documentation practices in English safety research. They propose recommendations and future directions to enhance multilingual safety evaluation, data generation, and cross-lingual safety generalization, aiming for more inclusive AI safety practices.'}, 'zh': {'title': 'æ¨åŠ¨å¤šè¯­è¨€LLMå®‰å…¨ç ”ç©¶çš„æœªæ¥æ–¹å‘', 'desc': 'è¿™ç¯‡è®ºæ–‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®‰å…¨ç ”ç©¶çš„è¯­è¨€å¤šæ ·æ€§è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œå¼ºè°ƒäº†è¯¥é¢†åŸŸä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„ç‰¹ç‚¹ã€‚é€šè¿‡å¯¹2020å¹´è‡³2024å¹´é—´è¿‘300ç¯‡æ¥è‡ªä¸»è¦è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¼šè®®å’Œç ”è®¨ä¼šçš„å‡ºç‰ˆç‰©è¿›è¡Œç³»ç»Ÿå®¡æŸ¥ï¼Œæˆ‘ä»¬å‘ç°LLMå®‰å…¨ç ”ç©¶ä¸­å­˜åœ¨æ˜¾è‘—ä¸”æ—¥ç›Šæ‰©å¤§çš„è¯­è¨€å·®è·ï¼Œç”šè‡³é«˜èµ„æºçš„éè‹±è¯­è¯­è¨€ä¹Ÿå—åˆ°çš„å…³æ³¨å¾ˆå°‘ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ï¼Œéè‹±è¯­è¯­è¨€å¾ˆå°‘ä½œä¸ºç‹¬ç«‹è¯­è¨€è¿›è¡Œç ”ç©¶ï¼Œè€Œè‹±è¯­å®‰å…¨ç ”ç©¶çš„æ–‡çŒ®è®°å½•å®è·µä¹Ÿå¾ˆå·®ã€‚ä¸ºäº†æ¿€åŠ±æœªæ¥çš„å¤šè¯­è¨€å®‰å…¨ç ”ç©¶ï¼Œæˆ‘ä»¬æ ¹æ®è°ƒæŸ¥ç»“æœæå‡ºäº†å‡ é¡¹å»ºè®®ï¼Œå¹¶æå‡ºäº†å…³äºå®‰å…¨è¯„ä¼°ã€è®­ç»ƒæ•°æ®ç”Ÿæˆå’Œè·¨è¯­è¨€å®‰å…¨æ³›åŒ–çš„ä¸‰ä¸ªå…·ä½“æœªæ¥æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23923', 'title': 'ChARM: Character-based Act-adaptive Reward Modeling for Advanced\n  Role-Playing Language Agents', 'url': 'https://huggingface.co/papers/2505.23923', 'abstract': 'ChARM, a character-focused adaptive reward model, improves preference learning for role-playing language agents by using an act-adaptive margin and self-evolution with unlabeled data, achieving superior results on dedicated benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ChARM.', 'score': 1, 'issue_id': 4079, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '761912e5ab3c056e', 'authors': ['Feiteng Fang', 'Ting-En Lin', 'Yuchuan Wu', 'Xiong Liu', 'Xiang Huang', 'Dingwei Chen', 'Jing Ye', 'Haonan Zhang', 'Liang Zhu', 'Hamid Alinejad-Rokny', 'Min Yang', 'Fei Huang', 'Yongbin Li'], 'affiliations': ['Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China', 'Tongji University', 'Tongyi Laboratory', 'University of New South Wales'], 'pdf_title_img': 'assets/pdf/title_img/2505.23923.jpg', 'data': {'categories': ['#optimization', '#open_source', '#benchmark', '#rlhf', '#dataset', '#games'], 'emoji': 'ğŸ­', 'ru': {'title': 'ChARM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ChARM - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ´Ğ»Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ RoleplayPref - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ RoleplayEval - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ 13% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ‘Ñ€ÑĞ´Ğ»Ğ¸-Ğ¢ĞµÑ€Ñ€Ğ¸ Ğ² Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'ChARM: Elevating Role-Playing Agents with Adaptive Rewards', 'desc': 'ChARM is a novel reward model designed to enhance preference learning for role-playing language agents (RPLAs). It introduces an act-adaptive margin that improves the efficiency and adaptability of learning from user preferences. Additionally, ChARM employs a self-evolution mechanism that utilizes large amounts of unlabeled data to broaden the training scope. The model demonstrates significant performance improvements over traditional methods, achieving state-of-the-art results on specialized evaluation benchmarks.'}, 'zh': {'title': 'è§’è‰²æ‰®æ¼”è¯­è¨€ä»£ç†çš„æ–°çªç ´ï¼šChARMæ¨¡å‹', 'desc': 'ChARMæ˜¯ä¸€ç§ä»¥è§’è‰²ä¸ºä¸­å¿ƒçš„è‡ªé€‚åº”å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æ”¹å–„è§’è‰²æ‰®æ¼”è¯­è¨€ä»£ç†çš„åå¥½å­¦ä¹ ã€‚å®ƒé€šè¿‡å¼•å…¥è¡Œä¸ºè‡ªé€‚åº”è¾¹é™…å’Œåˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„è‡ªæˆ‘è¿›åŒ–æœºåˆ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹åœ¨å¯æ‰©å±•æ€§å’Œé€‚åº”ä¸»è§‚å¯¹è¯åå¥½æ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†RoleplayPrefï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹è§’è‰²æ‰®æ¼”è¯­è¨€ä»£ç†çš„å¤§è§„æ¨¡åå¥½æ•°æ®é›†ï¼ŒåŒ…å«1108ä¸ªè§’è‰²å’Œ16888ä¸ªåŒè¯­å¯¹è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChARMåœ¨åå¥½æ’åä¸Šæ¯”ä¼ ç»Ÿçš„Bradley-Terryæ¨¡å‹æé«˜äº†13%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21749', 'title': 'Revisiting Bi-Linear State Transitions in Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2505.21749', 'abstract': 'Bilinear operations in recurrent neural networks are shown to be a natural bias for state tracking tasks, forming a hierarchical structure where linear recurrent networks are the simplest form.  \t\t\t\t\tAI-generated summary \t\t\t\t The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. A less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute a natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form a natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy.', 'score': 1, 'issue_id': 4077, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'cb7a7df613a84e6e', 'authors': ['M. Reza Ebrahimi', 'Roland Memisevic'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.21749.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¸Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒ - ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ² Ğ ĞĞ¡', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ±Ğ¸Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… ĞºĞ°Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‡Ğ°ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ…, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ…Ñ€Ğ°Ğ½ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¸Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Mamba, Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ² Ñ†ĞµĞ½Ñ‚Ñ€Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ Ñ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Bilinear Operations: Enhancing RNNs for State Tracking', 'desc': 'This paper explores the role of bilinear operations in recurrent neural networks (RNNs) for state tracking tasks. It argues that hidden units should be seen as active contributors to computations rather than just memory stores. The authors demonstrate that bilinear interactions between hidden units and input embeddings provide a beneficial inductive bias for evolving hidden states. Additionally, they establish a hierarchy of state tracking tasks, with linear RNNs like Mamba representing the simplest form of this structure.'}, 'zh': {'title': 'åŒçº¿æ€§æ“ä½œï¼šçŠ¶æ€è·Ÿè¸ªçš„è‡ªç„¶åç½®', 'desc': 'åœ¨é€’å½’ç¥ç»ç½‘ç»œä¸­ï¼ŒåŒçº¿æ€§æ“ä½œè¢«è¯æ˜æ˜¯çŠ¶æ€è·Ÿè¸ªä»»åŠ¡çš„è‡ªç„¶åç½®ï¼Œå½¢æˆäº†ä¸€ç§å±‚æ¬¡ç»“æ„ï¼Œå…¶ä¸­çº¿æ€§é€’å½’ç½‘ç»œæ˜¯æœ€ç®€å•çš„å½¢å¼ã€‚éšè—å•å…ƒçš„è§’è‰²é€šå¸¸è¢«è§†ä¸ºå»ºæ¨¡è®°å¿†ï¼Œç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šè¿‡é—¨æ§æœºåˆ¶å¢å¼ºä¿¡æ¯ä¿ç•™ã€‚æœ¬æ–‡é‡æ–°å®¡è§†åŒçº¿æ€§æ“ä½œï¼Œå±•ç¤ºå®ƒä»¬åœ¨çŠ¶æ€è·Ÿè¸ªä»»åŠ¡ä¸­å¦‚ä½•è‡ªç„¶åœ°è¡¨ç¤ºéšè—çŠ¶æ€çš„æ¼”å˜ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒåŒçº¿æ€§çŠ¶æ€æ›´æ–°å½¢æˆäº†ä¸€ä¸ªè‡ªç„¶çš„å±‚æ¬¡ç»“æ„ï¼Œé€‚ç”¨äºå¤æ‚æ€§é€æ¸å¢åŠ çš„çŠ¶æ€è·Ÿè¸ªä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23856', 'title': 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across\n  Modalities', 'url': 'https://huggingface.co/papers/2505.23856', 'abstract': 'OMNIGUARD detects harmful prompts across languages and modalities by identifying aligned internal representations in large language models, achieving high accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\% over the strongest baseline in a multilingual setting, by 20.44\\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient (approx 120 times faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.', 'score': 0, 'issue_id': 4082, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '365a6ed14a1515e6', 'authors': ['Sahil Verma', 'Keegan Hines', 'Jeff Bilmes', 'Charlotte Siska', 'Luke Zettlemoyer', 'Hila Gonen', 'Chandan Singh'], 'affiliations': ['Microsoft', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.23856.jpg', 'data': {'categories': ['#security', '#low_resource', '#dataset', '#multimodal', '#multilingual'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'OMNIGUARD: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²', 'desc': 'OMNIGUARD - ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°. OMNIGUARD Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ².'}, 'en': {'title': 'OMNIGUARD: Safeguarding LLMs Across Languages and Modalities', 'desc': 'OMNIGUARD is a novel approach designed to detect harmful prompts in large language models (LLMs) across different languages and modalities, such as text, images, and audio. It works by identifying aligned internal representations within the LLMs, which allows it to create a classifier that is effective regardless of the language or type of input. This method significantly improves the accuracy of harmful prompt detection, outperforming existing techniques by notable margins in multilingual and multimodal contexts. Additionally, OMNIGUARD is highly efficient, operating approximately 120 times faster than previous methods, making it a practical solution for real-time applications.'}, 'zh': {'title': 'OMNIGUARDï¼šè·¨è¯­è¨€å’Œæ¨¡æ€çš„æœ‰å®³æç¤ºæ£€æµ‹æ–°æ–¹æ³•', 'desc': 'OMNIGUARDæ˜¯ä¸€ç§æ£€æµ‹æœ‰å®³æç¤ºçš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿè·¨è¯­è¨€å’Œæ¨¡æ€è¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¯¹é½å†…éƒ¨è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡è¯†åˆ«ä¸åŒè¯­è¨€æˆ–æ¨¡æ€ä¸­å¯¹é½çš„å†…éƒ¨è¡¨ç¤ºï¼Œæ„å»ºä¸€ç§ä¸ä¾èµ–è¯­è¨€æˆ–æ¨¡æ€çš„åˆ†ç±»å™¨ï¼Œä»è€Œæé«˜æœ‰å®³æç¤ºçš„æ£€æµ‹å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOMNIGUARDåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„åˆ†ç±»å‡†ç¡®æ€§æé«˜äº†11.57%ï¼Œåœ¨åŸºäºå›¾åƒçš„æç¤ºä¸­æé«˜äº†20.44%ï¼Œå¹¶åœ¨åŸºäºéŸ³é¢‘çš„æç¤ºä¸­è®¾å®šäº†æ–°çš„æœ€ä¼˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•è¿˜åˆ©ç”¨ç”Ÿæˆè¿‡ç¨‹ä¸­è®¡ç®—çš„åµŒå…¥ï¼Œä½¿å¾—æ£€æµ‹è¿‡ç¨‹éå¸¸é«˜æ•ˆï¼Œé€Ÿåº¦æ¯”ä¸‹ä¸€ä¸ªæœ€å¿«çš„åŸºçº¿å¿«çº¦120å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08007', 'title': 'Reinforcement Pre-Training', 'url': 'https://huggingface.co/papers/2506.08007', 'abstract': 'Reinforcement Pre-Training (RPT) enhances language model pre-training by framing next-token prediction as a reinforcement learning task, improving accuracy and providing a strong foundation for further fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training.', 'score': 142, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'a1ef74d27d1d9d50', 'authors': ['Qingxiu Dong', 'Li Dong', 'Yao Tang', 'Tianzhu Ye', 'Yutao Sun', 'Zhifang Sui', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08007.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Reinforcement Pre-Training (RPT). RPT Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RPT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'Reinforcement Learning Boosts Language Model Training', 'desc': 'Reinforcement Pre-Training (RPT) is a novel approach that enhances the pre-training of language models by treating next-token prediction as a reinforcement learning (RL) task. This method allows the model to receive rewards for accurately predicting the next token based on the context, which improves its reasoning capabilities. RPT utilizes large amounts of unannotated text data, making it scalable and effective for general-purpose RL applications. The results demonstrate that RPT not only boosts prediction accuracy but also lays a solid groundwork for subsequent fine-tuning in reinforcement learning tasks.'}, 'zh': {'title': 'å¼ºåŒ–é¢„è®­ç»ƒï¼šæå‡è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€æ­¥é¢„æµ‹èƒ½åŠ›', 'desc': 'å¼ºåŒ–é¢„è®­ç»ƒï¼ˆRPTï¼‰é€šè¿‡å°†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è§†ä¸ºå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œå¢å¼ºäº†è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä¸ºæ­£ç¡®é¢„æµ‹ç»™å®šä¸Šä¸‹æ–‡çš„ä¸‹ä¸€ä¸ªæ ‡è®°æä¾›å¯éªŒè¯çš„å¥–åŠ±ï¼Œä»è€Œæé«˜äº†å‡†ç¡®æ€§ã€‚RPTåˆ©ç”¨å¤§é‡æ–‡æœ¬æ•°æ®è¿›è¡Œé€šç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨ç­”æ¡ˆã€‚é€šè¿‡æ¿€åŠ±ä¸‹ä¸€ä¸ªæ ‡è®°æ¨ç†çš„èƒ½åŠ›ï¼ŒRPTæ˜¾è‘—æé«˜äº†è¯­è¨€å»ºæ¨¡çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„å¼ºåŒ–å¾®è°ƒæä¾›äº†åšå®çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07044', 'title': 'Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2506.07044', 'abstract': "Lingshu, a medical-specialized MLLM, enhances performance in medical tasks through comprehensive data curation, multi-stage training, and reinforcement learning with verifiable rewards, outperforming existing open-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ...", 'score': 79, 'issue_id': 4208, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': '475506f0d0cabb39', 'authors': ['LASA Team', 'Weiwen Xu', 'Hou Pong Chan', 'Long Li', 'Mahani Aljunied', 'Ruifeng Yuan', 'Jianyu Wang', 'Chenghao Xiao', 'Guizhen Chen', 'Chaoqun Liu', 'Zhaodonghui Li', 'Yu Sun', 'Junao Shen', 'Chaojun Wang', 'Jie Tan', 'Deli Zhao', 'Tingyang Xu', 'Hao Zhang', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.07044.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#hallucinations', '#benchmark', '#medical', '#rl', '#reasoning', '#data', '#healthcare', '#multimodal'], 'emoji': 'ğŸ©º', 'ru': {'title': 'Lingshu: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹', 'desc': 'Lingshu - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Lingshu Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Lingshu: Revolutionizing Medical AI with Enhanced Learning and Reasoning', 'desc': 'Lingshu is a specialized Multimodal Large Language Model (MLLM) designed to improve medical task performance through enhanced data curation and training techniques. It addresses key limitations of existing medical models, such as insufficient medical knowledge coverage and a tendency to produce hallucinations. By utilizing a comprehensive dataset that includes both medical texts and images, Lingshu is trained in multiple stages to develop strong reasoning capabilities for complex medical scenarios. The model is evaluated using the MedEvalKit framework, demonstrating superior performance in tasks like multimodal question answering and medical report generation compared to other open-source models.'}, 'zh': {'title': 'Lingshuï¼šåŒ»ç–—ä»»åŠ¡çš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'Lingshuæ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹åŒ»ç–—ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡å…¨é¢çš„æ•°æ®æ•´ç†ã€å¤šé˜¶æ®µè®­ç»ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¥æå‡å…¶æ€§èƒ½ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰åŒ»ç–—MLLMåœ¨çŸ¥è¯†è¦†ç›–ã€æ•°æ®æ•´ç†å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡é«˜æ•ˆè·å–ä¸°å¯Œçš„åŒ»ç–—çŸ¥è¯†æ•°æ®ï¼ŒLingshuèƒ½å¤Ÿåœ¨å¤šç§åŒ»ç–—ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æœ€ç»ˆï¼ŒLingshuåœ¨å¤šæ¨¡æ€é—®ç­”ã€åŸºäºæ–‡æœ¬çš„é—®ç­”å’ŒåŒ»ç–—æŠ¥å‘Šç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06444', 'title': 'Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance', 'url': 'https://huggingface.co/papers/2506.06444', 'abstract': "SAFFRON introduces multifurcation reward models to improve safety assurance in large language models by addressing the exploration-efficiency dilemma in inference scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .", 'score': 59, 'issue_id': 4209, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '86bb27c97ab5d915', 'authors': ['Ruizhong Qiu', 'Gaotang Li', 'Tianxin Wei', 'Jingrui He', 'Hanghang Tong'], 'affiliations': ['University of Illinois Urbana-Champaign, IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06444.jpg', 'data': {'categories': ['#open_source', '#security', '#dataset', '#alignment', '#rl', '#inference'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'SAFFRON: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SAFFRON - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (MRM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ MRM, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ñ… ÑƒĞ³Ñ€Ğ¾Ğ·.'}, 'en': {'title': 'SAFFRON: Enhancing LLM Safety with Multifurcation Reward Models', 'desc': 'SAFFRON presents a new approach to enhance safety in large language models (LLMs) by introducing multifurcation reward models that tackle the exploration-efficiency dilemma during inference scaling. Traditional safety methods have struggled against jailbreak attacks and have not effectively integrated with the advancements in LLM reasoning capabilities. The paper identifies that existing inference scaling techniques are inadequate for safety assurance, often performing worse than simpler methods. To address this, SAFFRON proposes a novel paradigm that reduces the number of necessary reward model evaluations, ensuring safer and more efficient LLM operations.'}, 'zh': {'title': 'SAFFRONï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'SAFFRONæå‡ºäº†ä¸€ç§å¤šåˆ†å‰å¥–åŠ±æ¨¡å‹ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ä¿éšœï¼Œè§£å†³æ¨ç†æ‰©å±•ä¸­çš„æ¢ç´¢æ•ˆç‡å›°å¢ƒã€‚ç°æœ‰çš„å®‰å…¨ä¿éšœç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è®­ç»ƒé˜¶æ®µçš„å¯¹é½ï¼Œä»¥åŸ¹å…»LLMçš„å®‰å…¨è¡Œä¸ºï¼Œä½†è¿™äº›æ–¹æ³•åœ¨é¢å¯¹å„ç§æ”»å‡»æ—¶è¡¨ç°å‡ºè„†å¼±æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†æ¨ç†æ‰©å±•åœ¨å®‰å…¨ä¿éšœä¸­çš„åº”ç”¨ï¼Œå‘ç°ä¼ ç»Ÿçš„æ¨ç†æ‰©å±•æŠ€æœ¯åœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ‰©å±•èŒƒå¼ï¼Œç»“åˆå¤šåˆ†å‰å¥–åŠ±æ¨¡å‹ï¼Œæ˜¾è‘—å‡å°‘äº†å¥–åŠ±æ¨¡å‹è¯„ä¼°çš„æ¬¡æ•°ï¼Œä»è€Œæé«˜äº†å®‰å…¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07900', 'title': 'MiniCPM4: Ultra-Efficient LLMs on End Devices', 'url': 'https://huggingface.co/papers/2506.07900', 'abstract': 'MiniCPM4, a large language model optimized for end-side devices, achieves efficiency and effectiveness through innovations in model architecture, training data, algorithms, and inference systems.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.', 'score': 53, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'edbb02209bdb677b', 'authors': ['MiniCPM Team', 'Chaojun Xiao', 'Yuxuan Li', 'Xu Han', 'Yuzhuo Bai', 'Jie Cai', 'Haotian Chen', 'Wentong Chen', 'Xin Cong', 'Ganqu Cui', 'Ning Ding', 'Shengdan Fan', 'Yewei Fang', 'Zixuan Fu', 'Wenyu Guan', 'Yitong Guan', 'Junshao Guo', 'Yufeng Han', 'Bingxiang He', 'Yuxiang Huang', 'Cunliang Kong', 'Qiuzuo Li', 'Siyuan Li', 'Wenhao Li', 'Yanghao Li', 'Yishan Li', 'Zhen Li', 'Dan Liu', 'Biyuan Lin', 'Yankai Lin', 'Xiang Long', 'Quanyu Lu', 'Yaxi Lu', 'Peiyan Luo', 'Hongya Lyu', 'Litu Ou', 'Yinxu Pan', 'Zekai Qu', 'Qundong Shi', 'Zijun Song', 'Jiayuan Su', 'Zhou Su', 'Ao Sun', 'Xianghui Sun', 'Peijun Tang', 'Fangzheng Wang', 'Feng Wang', 'Shuo Wang', 'Yudong Wang', 'Yesai Wu', 'Zhenyu Xiao', 'Jie Xie', 'Zihao Xie', 'Yukun Yan', 'Jiarui Yuan', 'Kaihuo Zhang', 'Lei Zhang', 'Linyue Zhang', 'Xueren Zhang', 'Yudi Zhang', 'Hengyu Zhao', 'Weilin Zhao', 'Weilun Zhao', 'Yuanqian Zhao', 'Zhi Zheng', 'Ge Zhou', 'Jie Zhou', 'Wei Zhou', 'Zihan Zhou', 'Zixuan Zhou', 'Zhiyuan Liu', 'Guoyang Zeng', 'Chao Jia', 'Dahai Li', 'Maosong Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.07900.jpg', 'data': {'categories': ['#optimization', '#dataset', '#long_context', '#training', '#architecture', '#inference', '#open_source', '#data', '#games'], 'emoji': 'ğŸš€', 'ru': {'title': 'MiniCPM4: ĞœĞ¾Ñ‰ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸', 'desc': 'MiniCPM4 - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM), Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ñ… Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ InfLLM v2, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UltraClean Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UltraChat v2 Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. MiniCPM4 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'MiniCPM4: Efficiency Meets Performance for On-Device AI', 'desc': 'MiniCPM4 is a large language model specifically designed for efficient use on end-side devices. It incorporates innovations in model architecture, such as a new sparse attention mechanism, and utilizes advanced training data strategies to enhance performance. The model employs unique training algorithms for effective pre-training and post-training, ensuring it can handle diverse tasks with minimal resources. Evaluation results indicate that MiniCPM4 outperforms similar-sized models, demonstrating its speed and effectiveness in real-world applications.'}, 'zh': {'title': 'MiniCPM4ï¼šç»ˆç«¯è®¾å¤‡çš„é«˜æ•ˆè¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†MiniCPM4ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºç»ˆç«¯è®¾å¤‡ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡åœ¨æ¨¡å‹æ¶æ„ã€è®­ç»ƒæ•°æ®ã€è®­ç»ƒç®—æ³•å’Œæ¨ç†ç³»ç»Ÿå››ä¸ªå…³é”®æ–¹é¢çš„åˆ›æ–°ï¼ŒMiniCPM4å®ç°äº†é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œé‡‡ç”¨äº†å¯è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å’Œé«˜æ•ˆçš„æ•°æ®è¿‡æ»¤ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶è¡¨ç°å‡ºè‰²ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒMiniCPM4åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åŒç±»å¼€æºæ¨¡å‹ï¼Œå±•ç°äº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07977', 'title': 'OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation', 'url': 'https://huggingface.co/papers/2506.07977', 'abstract': 'OneIG-Bench is a benchmark framework that comprehensively evaluates text-to-image models across prompt-image alignment, text rendering, reasoning, stylization, and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community.', 'score': 37, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'f87355dfc9390cc7', 'authors': ['Jingjing Chang', 'Yixiao Fang', 'Peng Xing', 'Shuhan Wu', 'Wei Cheng', 'Rui Wang', 'Xianfang Zeng', 'Gang Yu', 'Hai-Bao Chen'], 'affiliations': ['SJTU', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2506.07977.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#cv', '#open_source', '#survey'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ’ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ', 'desc': 'OneIG-Bench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹. OneIG-Bench Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'OneIG-Bench: A Comprehensive Evaluation for Text-to-Image Models', 'desc': 'OneIG-Bench is a new framework designed to evaluate text-to-image (T2I) models in a detailed way. It focuses on several important aspects like how well the images match the text prompts, the quality of text rendering, reasoning capabilities, artistic style, and diversity of generated images. This benchmark addresses the shortcomings of previous evaluation methods by providing a structured approach that allows researchers to analyze specific areas of model performance. By making the code and dataset publicly available, OneIG-Bench promotes reproducibility and encourages comparisons among different T2I models.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŸºå‡†æ¡†æ¶', 'desc': 'OneIG-Benchæ˜¯ä¸€ä¸ªåŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒæ¶µç›–äº†å¤šä¸ªç»´åº¦ï¼ŒåŒ…æ‹¬æç¤ºä¸å›¾åƒçš„å¯¹é½ã€æ–‡æœ¬æ¸²æŸ“ã€æ¨ç†èƒ½åŠ›ã€é£æ ¼åŒ–å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»ŸåŒ–çš„è¯„ä¼°ï¼ŒOneIG-Benchå¸®åŠ©ç ”ç©¶äººå‘˜æ·±å…¥åˆ†ææ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼Œè¯†åˆ«å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç“¶é¢ˆã€‚è¯¥æ¡†æ¶çš„ä»£ç åº“å’Œæ•°æ®é›†å·²å…¬å¼€ï¼Œä¾¿äºåœ¨T2Iç ”ç©¶ç¤¾åŒºä¸­è¿›è¡Œå¯é‡å¤çš„è¯„ä¼°ç ”ç©¶å’Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07491', 'title': 'SpatialLM: Training Large Language Models for Structured Indoor Modeling', 'url': 'https://huggingface.co/papers/2506.07491', 'abstract': 'SpatialLM is a large language model capable of processing 3D point cloud data to produce structured outputs for spatial understanding, outperforming previous methods on layout estimation and object detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.   To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.', 'score': 28, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '604b98825a94f8d1', 'authors': ['Yongsen Mao', 'Junhao Zhong', 'Chuan Fang', 'Jia Zheng', 'Rui Tang', 'Hao Zhu', 'Ping Tan', 'Zihan Zhou'], 'affiliations': ['Hong Kong University of Science and Technology', 'Manycore Tech Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.07491.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#3d', '#open_source', '#synthetic', '#multimodal'], 'emoji': 'ğŸ ', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'SpatialLM - ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², SpatialLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 3D Spatial Understanding with SpatialLM', 'desc': 'SpatialLM is a large language model that specializes in understanding 3D point cloud data, enabling it to produce structured outputs for spatial comprehension. It identifies and categorizes architectural features such as walls, doors, and windows, as well as oriented object boxes. Unlike earlier approaches that relied on specific network designs for tasks, SpatialLM uses a standard multimodal architecture and is fine-tuned from existing open-source models. The model is trained on a comprehensive dataset of indoor scenes, achieving state-of-the-art results in layout estimation and strong performance in 3D object detection, paving the way for advancements in augmented reality and robotics.'}, 'zh': {'title': 'SpatialLMï¼šæå‡ç©ºé—´ç†è§£çš„è¯­è¨€æ¨¡å‹', 'desc': 'SpatialLMæ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†3Dç‚¹äº‘æ•°æ®ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„ç©ºé—´ç†è§£è¾“å‡ºã€‚è¿™äº›è¾“å‡ºåŒ…æ‹¬å»ºç­‘å…ƒç´ ï¼Œå¦‚å¢™å£ã€é—¨ã€çª—æˆ·ä»¥åŠå¸¦æœ‰è¯­ä¹‰ç±»åˆ«çš„å®šå‘ç‰©ä½“æ¡†ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼ŒSpatialLMéµå¾ªæ ‡å‡†çš„å¤šæ¨¡æ€LLMæ¶æ„ï¼Œå¹¶ç›´æ¥ä»å¼€æºLLMè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡æ”¶é›†åŒ…å«12,328ä¸ªå®¤å†…åœºæ™¯çš„é«˜è´¨é‡åˆæˆæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¸ƒå±€ä¼°è®¡å’Œ3Dç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07803', 'title': 'Image Reconstruction as a Tool for Feature Analysis', 'url': 'https://huggingface.co/papers/2506.07803', 'abstract': 'Image reconstruction reveals that vision encoders retain more image information after image-based tasks and that orthogonal rotations in feature space control color encoding.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision encoders are increasingly used in modern applications, from vision-only models to multimodal systems such as vision-language models. Despite their remarkable success, it remains unclear how these architectures represent features internally. Here, we propose a novel approach for interpreting vision features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ only in their training objective, and show that encoders pre-trained on image-based tasks retain significantly more image information than those trained on non-image tasks such as contrastive learning. We further apply our method to a range of vision encoders, ranking them by the informativeness of their feature representations. Finally, we demonstrate that manipulating the feature space yields predictable changes in reconstructed images, revealing that orthogonal rotations (rather than spatial transformations) control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner structure of its feature space. The code and model weights to reproduce the experiments are available in GitHub.', 'score': 24, 'issue_id': 4213, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '0bba64237ba10a58', 'authors': ['Eduard Allakhverdov', 'Dmitrii Tarasov', 'Elizaveta Goncharova', 'Andrey Kuznetsov'], 'affiliations': ['AIRI Moscow, Russia', 'MIPT Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.07803.jpg', 'data': {'categories': ['#open_source', '#interpretability', '#cv', '#multimodal', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ± Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸, Ñ‡ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑĞ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼ Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†Ğ²ĞµÑ‚Ğ°.'}, 'en': {'title': 'Unlocking the Secrets of Vision Encoders Through Image Reconstruction', 'desc': 'This paper explores how vision encoders, which are used in various AI applications, represent image features internally. The authors introduce a method for interpreting these features through image reconstruction, comparing two model families with different training objectives. They find that encoders trained on image-based tasks retain more information than those trained on non-image tasks. Additionally, they show that manipulating the feature space through orthogonal rotations can control how colors are encoded in the reconstructed images.'}, 'zh': {'title': 'æ­ç¤ºè§†è§‰ç¼–ç å™¨çš„ç‰¹å¾ç©ºé—´ç»“æ„', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è§†è§‰ç¼–ç å™¨åœ¨å›¾åƒé‡å»ºä¸­çš„è¡¨ç°ï¼Œå‘ç°ç»è¿‡å›¾åƒä»»åŠ¡è®­ç»ƒçš„ç¼–ç å™¨ä¿ç•™äº†æ›´å¤šçš„å›¾åƒä¿¡æ¯ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§æ¨¡å‹å®¶æ—ï¼ŒSigLIPå’ŒSigLIP2ï¼Œå‘ç°å‰è€…åœ¨å›¾åƒä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒä½¿å…¶ç‰¹å¾è¡¨ç¤ºæ›´å…·ä¿¡æ¯é‡ã€‚é€šè¿‡å¯¹ç‰¹å¾ç©ºé—´çš„æ“ä½œï¼Œæˆ‘ä»¬å‘ç°æ­£äº¤æ—‹è½¬å¯ä»¥æ§åˆ¶é¢œè‰²ç¼–ç ï¼Œè€Œä¸æ˜¯ç©ºé—´å˜æ¢ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä»»ä½•è§†è§‰ç¼–ç å™¨ï¼Œæœ‰åŠ©äºç†è§£å…¶ç‰¹å¾ç©ºé—´çš„å†…éƒ¨ç»“æ„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06205', 'title': 'Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\n  Learning', 'url': 'https://huggingface.co/papers/2506.06205', 'abstract': 'Astra, a dual-model architecture for mobile robot navigation, uses a multimodal LLM for global localization and a multitask network for local path planning and odometry estimation, achieving high success rates in diverse indoor environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern robot navigation systems encounter difficulties in diverse and complex indoor environments. Traditional approaches rely on multiple modules with small models or rule-based systems and thus lack adaptability to new environments. To address this, we developed Astra, a comprehensive dual-model architecture, Astra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a multimodal LLM, processes vision and language inputs to perform self and goal localization using a hybrid topological-semantic graph as the global map, and outperforms traditional visual place recognition methods. Astra-Local, a multitask network, handles local path planning and odometry estimation. Its 4D spatial-temporal encoder, trained through self-supervised learning, generates robust 4D features for downstream tasks. The planning head utilizes flow matching and a novel masked ESDF loss to minimize collision risks for generating local trajectories, and the odometry head integrates multi-sensor inputs via a transformer encoder to predict the relative pose of the robot. Deployed on real in-house mobile robots, Astra achieves high end-to-end mission success rate across diverse indoor environments.', 'score': 20, 'issue_id': 4216, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'ff910133c6b6f7f5', 'authors': ['Sheng Chen', 'Peiyu He', 'Jiaxin Hu', 'Ziyang Liu', 'Yansheng Wang', 'Tao Xu', 'Chi Zhang', 'Chongchong Zhang', 'Chao An', 'Shiyu Cai', 'Duo Cao', 'Kangping Chen', 'Shuai Chu', 'Tianwei Chu', 'Mingdi Dan', 'Min Du', 'Weiwei Fang', 'Pengyou Fu', 'Junkai Hu', 'Xiaowei Jiang', 'Zhaodi Jiang', 'Fuxuan Li', 'Jun Li', 'Minghui Li', 'Mingyao Li', 'Yanchang Li', 'Zhibin Li', 'Guangming Liu', 'Kairui Liu', 'Lihao Liu', 'Weizhi Liu', 'Xiaoshun Liu', 'Yufei Liu', 'Yunfei Liu', 'Qiang Lu', 'Yuanfei Luo', 'Xiang Lv', 'Hongying Ma', 'Sai Ma', 'Lingxian Mi', 'Sha Sa', 'Hongxiang Shu', 'Lei Tian', 'Chengzhi Wang', 'Jiayu Wang', 'Kaijie Wang', 'Qingyi Wang', 'Renwen Wang', 'Tao Wang', 'Wei Wang', 'Xirui Wang', 'Chao Wei', 'Xuguang Wei', 'Zijun Xia', 'Zhaohao Xiao', 'Tingshuai Yan', 'Liyan Yang', 'Yifan Yang', 'Zhikai Yang', 'Zhong Yin', 'Li Yuan', 'Liuchun Yuan', 'Chi Zhang', 'Jinyang Zhang', 'Junhui Zhang', 'Linge Zhang', 'Zhenyi Zhang', 'Zheyu Zhang', 'Dongjie Zhu', 'Hang Li', 'Yangang Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.06205.jpg', 'data': {'categories': ['#robotics', '#games', '#architecture', '#agents', '#graphs', '#optimization', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑÑ‚Ñ€Ğ°: Ğ´Ğ²ÑƒÑ…Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'ĞÑÑ‚Ñ€Ğ° - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ´Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. ĞÑÑ‚Ñ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Astra: Navigating Indoor Spaces with Dual-Model Intelligence', 'desc': 'Astra is a dual-model architecture designed for mobile robot navigation, addressing challenges in complex indoor environments. It consists of Astra-Global, a multimodal large language model (LLM) that combines vision and language inputs for effective global localization, and Astra-Local, a multitask network focused on local path planning and odometry estimation. The system utilizes advanced techniques like a hybrid topological-semantic graph and a 4D spatial-temporal encoder to enhance performance and adaptability. By integrating self-supervised learning and innovative loss functions, Astra achieves high success rates in navigating diverse indoor settings.'}, 'zh': {'title': 'Astraï¼šæ™ºèƒ½ç§»åŠ¨æœºå™¨äººå¯¼èˆªçš„æ–°çºªå…ƒ', 'desc': 'Astraæ˜¯ä¸€ç§åŒæ¨¡å‹æ¶æ„ï¼Œä¸“ä¸ºç§»åŠ¨æœºå™¨äººå¯¼èˆªè®¾è®¡ã€‚å®ƒç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šä»»åŠ¡ç½‘ç»œï¼Œåˆ†åˆ«ç”¨äºå…¨å±€å®šä½å’Œå±€éƒ¨è·¯å¾„è§„åˆ’ã€‚Astra-Globalé€šè¿‡å¤„ç†è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œåˆ©ç”¨æ··åˆæ‹“æ‰‘-è¯­ä¹‰å›¾è¿›è¡Œè‡ªæˆ‘å’Œç›®æ ‡å®šä½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è§†è§‰ä½ç½®è¯†åˆ«æ–¹æ³•ã€‚Astra-Localåˆ™é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ç”Ÿæˆå¼ºå¤§çš„4Dç‰¹å¾ï¼Œç¡®ä¿åœ¨å¤æ‚å®¤å†…ç¯å¢ƒä¸­å®ç°é«˜æˆåŠŸç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07298', 'title': 'Pre-trained Large Language Models Learn Hidden Markov Models In-context', 'url': 'https://huggingface.co/papers/2506.07298', 'abstract': 'LLMs using in-context learning can accurately predict sequences generated by HMMs, showcasing its potential for uncovering hidden structures in complex data.  \t\t\t\t\tAI-generated summary \t\t\t\t Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)x2013their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequencesx2013an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.', 'score': 15, 'issue_id': 4208, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': '568113a6bc87dd15', 'authors': ['Yijia Dai', 'Zhaolin Gao', 'Yahya Satter', 'Sarah Dean', 'Jennifer J. Sun'], 'affiliations': ['Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07298.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#data', '#science', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (HMM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICL). LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ñƒ, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… HMM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² HMM, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ICL Ğ² LLM Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ĞºĞ°Ğº Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking Hidden Structures with In-Context Learning in LLMs', 'desc': 'This paper demonstrates that large language models (LLMs) can effectively use in-context learning (ICL) to predict sequences generated by Hidden Markov Models (HMMs). The authors show that LLMs can achieve high predictive accuracy on synthetic HMM data, approaching theoretical limits. They also identify scaling trends related to HMM characteristics and provide insights into using ICL as a diagnostic tool for complex datasets. Additionally, the study highlights that ICL performs competitively on real-world tasks compared to expert-designed models, showcasing its potential in revealing hidden structures in data.'}, 'zh': {'title': 'åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ­ç¤ºå¤æ‚æ•°æ®çš„æ½œåŠ›', 'desc': 'æœ¬ç ”ç©¶å±•ç¤ºäº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æœ‰æ•ˆåœ°é¢„æµ‹ç”±éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMsï¼‰ç”Ÿæˆçš„åºåˆ—ã€‚è¿™è¡¨æ˜LLMsåœ¨æ­ç¤ºå¤æ‚æ•°æ®ä¸­çš„éšè—ç»“æ„æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨å¤šç§åˆæˆHMMsä¸Šæµ‹è¯•äº†LLMsï¼Œå‘ç°å…¶é¢„æµ‹å‡†ç¡®æ€§æ¥è¿‘ç†è®ºæœ€ä¼˜å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä½¿ç”¨ICLä½œä¸ºå¤æ‚æ•°æ®è¯Šæ–­å·¥å…·çš„å®ç”¨æŒ‡å—ï¼Œå¹¶åœ¨çœŸå®çš„åŠ¨ç‰©å†³ç­–ä»»åŠ¡ä¸­å–å¾—äº†ä¸äººç±»ä¸“å®¶è®¾è®¡æ¨¡å‹ç›¸å½“çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07986', 'title': 'Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.07986', 'abstract': 'Temperature-Adjusted Cross-modal Attention (TACA) improves text-image alignment in diffusion-based multimodal transformers by dynamically balancing multimodal interactions and provides a parameter-efficient solution.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose Temperature-Adjusted Cross-modal Attention (TACA), a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at https://github.com/Vchitect/TACA', 'score': 14, 'issue_id': 4209, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '54c2bf2395cf58cc', 'authors': ['Zhengyao Lv', 'Tianlin Pan', 'Chenyang Si', 'Zhaoxi Chen', 'Wangmeng Zuo', 'Ziwei Liu', 'Kwan-Yee K. Wong'], 'affiliations': ['Harbin Institute of Technology', 'Nanjing University', 'Nanyang Technological University', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.07986.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#optimization', '#training', '#diffusion', '#benchmark'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Temperature-Adjusted Cross-modal Attention (TACA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. TACA Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑƒÑ‡ĞµÑ‚Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. TACA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Text-Image Alignment with TACA', 'desc': 'The paper introduces Temperature-Adjusted Cross-modal Attention (TACA), a novel approach to enhance text-image alignment in multimodal diffusion transformers. TACA addresses two main challenges: the imbalance in token representation between text and images, and the absence of attention weighting that considers the timing of inputs. By implementing temperature scaling and timestep-dependent adjustments, TACA efficiently rebalances interactions between modalities. The results show that TACA, when used with LoRA fine-tuning, significantly improves alignment in state-of-the-art models, leading to better semantic fidelity in generated images.'}, 'zh': {'title': 'æ¸©åº¦è°ƒæ•´è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼šæå‡æ–‡æœ¬ä¸å›¾åƒå¯¹é½çš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¸©åº¦è°ƒæ•´è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆTACAï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºæ‰©æ•£çš„å¤šæ¨¡æ€å˜æ¢å™¨ä¸­çš„æ–‡æœ¬ä¸å›¾åƒå¯¹é½ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬æç¤ºä¸ç”Ÿæˆå†…å®¹ä¹‹é—´å­˜åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„ä¸è¶³ï¼Œä¸»è¦ä½“ç°åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„ä»¤ç‰Œä¸å¹³è¡¡ä»¥åŠç¼ºä¹æ—¶é—´æ­¥æ„ŸçŸ¥çš„æ³¨æ„åŠ›åŠ æƒã€‚TACAé€šè¿‡æ¸©åº¦ç¼©æ”¾å’Œæ—¶é—´æ­¥ä¾èµ–çš„è°ƒæ•´ï¼ŒåŠ¨æ€å¹³è¡¡å¤šæ¨¡æ€äº¤äº’ï¼Œä»è€Œæé«˜æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTACAåœ¨FLUXå’ŒSD3.5ç­‰å…ˆè¿›æ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†å›¾åƒ-æ–‡æœ¬å¯¹é½çš„å‡†ç¡®æ€§ï¼Œä¸”è®¡ç®—å¼€é”€æå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07553', 'title': 'GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\n  Structure Recognition', 'url': 'https://huggingface.co/papers/2506.07553', 'abstract': "GTR-Mol-VLM, a vision-language model with graph traversal and data-centric principles, achieves superior accuracy in converting molecular images to machine-readable formats, particularly in handling complex structures and functional group abbreviations.  \t\t\t\t\tAI-generated summary \t\t\t\t Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What You've Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT.", 'score': 12, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '379cf5e0efdacd27', 'authors': ['Jingchao Wang', 'Haote Yang', 'Jiang Wu', 'Yifan He', 'Xingjian Wei', 'Yinfan Wang', 'Chengjin Liu', 'Lingli Ge', 'Lijun Wu', 'Bin Wang', 'Dahua Lin', 'Conghui He'], 'affiliations': ['Chinese University of Hong Kong', 'East China Normal University', 'Fudan University', 'Northwestern Polytechnical University', 'Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07553.jpg', 'data': {'categories': ['#dataset', '#graphs', '#benchmark', '#reasoning', '#cv', '#science', '#games'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'GTR-Mol-VLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ²Ğ¸Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GTR-CoT-1.3M Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MolRec-Bench.'}, 'en': {'title': 'Transforming Molecular Images into Accurate Data with GTR-Mol-VLM', 'desc': 'GTR-Mol-VLM is a new vision-language model designed to convert molecular images into machine-readable formats with high accuracy. It uses a unique Graph Traversal mechanism that mimics human reasoning by analyzing molecular graphs step-by-step. Additionally, it employs a data-centric approach to ensure that the model accurately recognizes abbreviated structures in images. The model has been tested against various benchmarks and has shown significant improvements, especially in handling complex molecular structures and functional group abbreviations.'}, 'zh': {'title': 'GTR-Mol-VLMï¼šæå‡åˆ†å­å›¾åƒè¯†åˆ«çš„æ™ºèƒ½æ–°æ–¹æ³•', 'desc': 'GTR-Mol-VLMæ˜¯ä¸€ç§ç»“åˆå›¾éå†å’Œæ•°æ®ä¸­å¿ƒåŸåˆ™çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†åˆ†å­å›¾åƒå‡†ç¡®è½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼ã€‚è¯¥æ¨¡å‹é€šè¿‡å›¾éå†æœºåˆ¶æ¨¡æ‹Ÿäººç±»æ¨ç†ï¼Œé€æ­¥è§£æåˆ†å­å›¾ï¼Œè§£å†³äº†å¤æ‚ç»“æ„å’ŒåŠŸèƒ½åŸºå›¢ç¼©å†™çš„é—®é¢˜ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†GTR-CoT-1.3Mï¼Œå¹¶æ¨å‡ºäº†MolRec-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å›¾è§£æçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGTR-Mol-VLMåœ¨å¤„ç†åˆ†å­å›¾åƒæ—¶çš„è¡¨ç°ä¼˜äºå…¶ä»–ä¸“ä¸šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠŸèƒ½åŸºå›¢ç¼©å†™çš„åœºæ™¯ä¸­ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07530', 'title': 'BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation', 'url': 'https://huggingface.co/papers/2506.07530', 'abstract': "BitVLA, a 1-bit VLA model with ternary parameters, achieves comparable performance to OpenVLA-OFT on LIBERO while using 29.8% less memory through distillation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.", 'score': 12, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '2ff752565d34986b', 'authors': ['Hongyu Wang', 'Chuyan Xiong', 'Ruiping Wang', 'Xilin Chen'], 'affiliations': ['Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.07530.jpg', 'data': {'categories': ['#optimization', '#small_models', '#robotics', '#inference', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VLA Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'BitVLA - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VLA Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼ (-1, 0, 1). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ¾ 1,58-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ². BitVLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ OpenVLA-OFT Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LIBERO, Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ÑÑ Ğ²ÑĞµĞ³Ğ¾ 29,8% Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ BitVLA Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Efficient Robotics with BitVLA: Less Memory, Same Performance!', 'desc': 'BitVLA is a novel 1-bit Vision-Language-Action (VLA) model designed for robotics manipulation tasks, utilizing ternary parameters to optimize performance. It employs a distillation-aware training strategy that compresses a full-precision vision encoder into 1.58-bit weights, significantly reducing memory usage. Despite not being pretrained on large-scale robotics data, BitVLA achieves performance on par with the state-of-the-art OpenVLA-OFT model while using 29.8% less memory. This advancement makes BitVLA particularly suitable for deployment in resource-limited robotic systems.'}, 'zh': {'title': 'BitVLAï¼šé«˜æ•ˆçš„1ä½è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹', 'desc': 'BitVLAæ˜¯ä¸€ç§1ä½è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œé‡‡ç”¨ä¸‰å…ƒå‚æ•°ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å†…å­˜é™åˆ¶é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨æ³¨æ„è’¸é¦è®­ç»ƒç­–ç•¥ï¼ŒBitVLAåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å ç”¨ï¼Œè¾¾åˆ°ä»…ä½¿ç”¨29.8%çš„å†…å­˜ã€‚å°½ç®¡ç¼ºä¹å¤§è§„æ¨¡çš„æœºå™¨äººé¢„è®­ç»ƒï¼ŒBitVLAåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸æœ€å…ˆè¿›çš„OpenVLA-OFTæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†BitVLAåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05062', 'title': 'Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.05062', 'abstract': 'This study evaluates the performance of large language models in assessing debate speeches, a task requiring deep understanding of various aspects of speech, including argument strength and coherence, and compares it to human judges.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Debate Speech Evaluation as a novel and challenging benchmark for assessing LLM judges. Evaluating debate speeches requires a deep understanding of the speech at multiple levels, including argument strength and relevance, the coherence and organization of the speech, the appropriateness of its style and tone, and so on. This task involves a unique set of cognitive abilities that have previously received limited attention in systematic LLM benchmarking. To explore such skills, we leverage a dataset of over 600 meticulously annotated debate speeches and present the first in-depth analysis of how state-of-the-art LLMs compare to human judges on this task. Our findings reveal a nuanced picture: while larger models can approximate individual human judgments in some respects, they differ substantially in their overall judgment behavior. We also investigate the ability of frontier LLMs to generate persuasive, opinionated speeches, showing that models may perform at a human level on this task.', 'score': 11, 'issue_id': 4219, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '150a859b148d8cc1', 'authors': ['Noy Sternlicht', 'Ariel Gera', 'Roy Bar-Haim', 'Tom Hope', 'Noam Slonim'], 'affiliations': ['IBM Research', 'School of Computer Science and Engineering, The Hebrew University of Jerusalem', 'The Allen Institute for AI (AI2)'], 'pdf_title_img': 'assets/pdf/title_img/2506.05062.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#interpretability', '#reasoning', '#multimodal', '#dataset'], 'emoji': 'ğŸ­', 'ru': {'title': 'LLM vs Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº: ĞšÑ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ ÑÑƒĞ´ÑŒÑ Ğ´ĞµĞ±Ğ°Ñ‚Ğ¾Ğ²?', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ±Ğ°Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑ‡Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑƒĞ´ÑŒÑĞ¼Ğ¸. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ñ€ĞµÑ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ»Ñƒ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 600 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ±Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ‚ÑŒÑÑ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹, Ğ½Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ½ĞµÑĞµĞ½Ğ¸Ğ¸ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Evaluating Debate Speeches: Can AI Match Human Judgment?', 'desc': "This paper investigates how well large language models (LLMs) can evaluate debate speeches, which require understanding complex elements like argument strength and coherence. It introduces a new benchmark called Debate Speech Evaluation, focusing on the cognitive skills needed for this task. The study uses a dataset of over 600 annotated speeches to compare LLMs with human judges, revealing that while LLMs can mimic some human judgments, their overall evaluation patterns differ significantly. Additionally, the research explores LLMs' ability to generate persuasive speeches, indicating that they can perform at a level comparable to humans in this area."}, 'zh': {'title': 'è¯„ä¼°è¾©è®ºæ¼”è®²ï¼šè¯­è¨€æ¨¡å‹ä¸äººç±»çš„è¾ƒé‡', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¾©è®ºæ¼”è®²è¯„ä¼°ä¸­çš„è¡¨ç°ï¼Œè¿™ä¸€ä»»åŠ¡éœ€è¦å¯¹æ¼”è®²çš„å¤šä¸ªæ–¹é¢æœ‰æ·±å…¥ç†è§£ï¼ŒåŒ…æ‹¬è®ºç‚¹çš„å¼ºåº¦å’Œè¿è´¯æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†è¾©è®ºæ¼”è®²è¯„ä¼°ä½œä¸ºä¸€ä¸ªæ–°é¢–ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œæ¯”è¾ƒäº†è¯­è¨€æ¨¡å‹ä¸äººç±»è¯„å®¡çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡è¾ƒå¤§çš„æ¨¡å‹åœ¨æŸäº›æ–¹é¢å¯ä»¥æ¥è¿‘ä¸ªåˆ«çš„äººç±»åˆ¤æ–­ï¼Œä½†å®ƒä»¬åœ¨æ•´ä½“åˆ¤æ–­è¡Œä¸ºä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å‰æ²¿è¯­è¨€æ¨¡å‹ç”Ÿæˆæœ‰è¯´æœåŠ›å’Œè§‚ç‚¹æ˜ç¡®çš„æ¼”è®²çš„èƒ½åŠ›ï¼Œæ˜¾ç¤ºè¿™äº›æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šå¯èƒ½è¾¾åˆ°äººç±»æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07712', 'title': 'Through the Valley: Path to Effective Long CoT Training for Small\n  Language Models', 'url': 'https://huggingface.co/papers/2506.07712', 'abstract': 'Small language models experience significant performance declines when trained on long chain-of-thought data due to error accumulation, impacting downstream reinforcement learning but potentially mitigated by extensive supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Long chain-of-thought (CoT) supervision has become a common strategy to enhance reasoning in language models. While effective for large models, we identify a phenomenon we call Long CoT Degradation, in which small language models (SLMs; <=3B parameters) trained on limited long CoT data experience significant performance deterioration. Through extensive experiments on the Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is widespread across SLMs. In some settings, models trained on only 8k long CoT examples lose up to 75% of their original performance before fine-tuning. Strikingly, we further observe that for some particularly small models, even training on 220k long CoT examples fails to recover or surpass their original performance prior to fine-tuning. Our analysis attributes this effect to error accumulation: while longer responses increase the capacity for multi-step reasoning, they also amplify the risk of compounding mistakes. Furthermore, we find that Long CoT Degradation may negatively impacts downstream reinforcement learning (RL), although this can be alleviated by sufficiently scaled supervised fine-tuning (SFT). Our findings challenge common assumptions about the benefits of long CoT training for SLMs and offer practical guidance for building more effective small-scale reasoning models.', 'score': 10, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '856fa5edc302286a', 'authors': ['Renjie Luo', 'Jiaxi Li', 'Chen Huang', 'Wei Lu'], 'affiliations': ['StatNLP Research Group, Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2506.07712.jpg', 'data': {'categories': ['#small_models', '#long_context', '#training', '#reasoning', '#rl'], 'emoji': 'ğŸ“‰', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ‚ĞµÑ€ÑÑ‚ÑŒ Ğ´Ğ¾ 75% ÑĞ²Ğ¾ĞµĞ¹ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ CoT Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ…Ğ¾Ñ‚Ñ ÑÑ‚Ğ¾Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑĞ¼ÑĞ³Ñ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ¿Ñ€Ğ¸ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ¼.'}, 'en': {'title': 'Mitigating Long CoT Degradation in Small Language Models', 'desc': 'This paper discusses a problem called Long CoT Degradation, which affects small language models (SLMs) when they are trained on long chain-of-thought (CoT) data. The authors found that these models can lose a significant amount of performance due to error accumulation, especially when trained on limited long CoT examples. Their experiments show that even with a large number of training examples, some small models still struggle to recover their original performance. The study suggests that while long CoT training can enhance reasoning, it may not be beneficial for smaller models without adequate supervised fine-tuning to mitigate the negative effects.'}, 'zh': {'title': 'å°å‹æ¨¡å‹çš„é•¿é“¾æ€ç»´é€€åŒ–é—®é¢˜', 'desc': 'å°å‹è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾æ€ç»´æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œå› é”™è¯¯ç´¯ç§¯è€Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºé•¿é“¾æ€ç»´é€€åŒ–ã€‚å°½ç®¡é•¿é“¾æ€ç»´ç›‘ç£å¯¹å¤§å‹æ¨¡å‹æœ‰æ•ˆï¼Œä½†å°å‹æ¨¡å‹åœ¨æœ‰é™çš„é•¿é“¾æ€ç»´æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™é«˜è¾¾75%ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé”™è¯¯ç´¯ç§¯æ˜¯å¯¼è‡´è¿™ç§é€€åŒ–çš„ä¸»è¦åŸå› ï¼Œé•¿å“åº”è™½ç„¶å¢åŠ äº†å¤šæ­¥æ¨ç†çš„èƒ½åŠ›ï¼Œä½†ä¹ŸåŠ å¤§äº†é”™è¯¯å åŠ çš„é£é™©ã€‚æ­¤å¤–ï¼Œé•¿é“¾æ€ç»´é€€åŒ–è¿˜å¯èƒ½å¯¹åç»­çš„å¼ºåŒ–å­¦ä¹ äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œä½†é€šè¿‡å……åˆ†è§„æ¨¡çš„ç›‘ç£å¾®è°ƒå¯ä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06941', 'title': 'The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity', 'url': 'https://huggingface.co/papers/2506.06941', 'abstract': "Large Reasoning Models (LRMs) undergo accuracy collapse at higher complexities and exhibit unique performance scaling behaviors compared to standard LLMs, with failures in exact computation and inconsistent reasoning across scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.", 'score': 10, 'issue_id': 4208, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ½Ñ', 'en': 'June 7', 'zh': '6æœˆ7æ—¥'}, 'hash': '6f3991ea3357f456', 'authors': ['Parshin Shojaee', 'Iman Mirzadeh', 'Keivan Alizadeh', 'Maxwell Horton', 'Samy Bengio', 'Mehrdad Farajtabar'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2506.06941.jpg', 'data': {'categories': ['#math', '#training', '#benchmark', '#reasoning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ·ĞºĞ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, LRM Ğ¸Ğ¼ĞµÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LRM Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹, Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Understanding the Limits of Large Reasoning Models in Complex Tasks', 'desc': 'This paper explores the performance of Large Reasoning Models (LRMs) in relation to their complexity and reasoning capabilities. It reveals that LRMs experience accuracy collapse when faced with high-complexity tasks, which is a significant limitation compared to standard language models (LLMs). The study introduces a controlled environment to analyze both the final answers and the internal reasoning processes of LRMs, highlighting their inconsistent reasoning and failure to perform exact computations. The findings categorize performance into three regimes based on task complexity, providing insights into the strengths and weaknesses of LRMs in reasoning tasks.'}, 'zh': {'title': 'å¤§å‹æ¨ç†æ¨¡å‹çš„å¤æ‚æ€§æŒ‘æˆ˜', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤„ç†æ›´å¤æ‚çš„é—®é¢˜æ—¶ä¼šå‡ºç°å‡†ç¡®æ€§å´©æºƒï¼Œå¹¶ä¸”ä¸æ ‡å‡†è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸æ¯”ï¼Œå®ƒä»¬çš„æ€§èƒ½æ‰©å±•è¡Œä¸ºç‹¬ç‰¹ã€‚å°½ç®¡LRMsåœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„åŸºæœ¬èƒ½åŠ›å’Œå±€é™æ€§ä»ç„¶ä¸å¤Ÿæ¸…æ¥šã€‚é€šè¿‡å¯æ§çš„éš¾é¢˜ç¯å¢ƒï¼Œæˆ‘ä»¬èƒ½å¤Ÿç²¾ç¡®æ“æ§å¤æ‚æ€§ï¼Œå¹¶åˆ†æLRMsçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLRMsåœ¨é«˜å¤æ‚æ€§ä»»åŠ¡ä¸­ä¼šå®Œå…¨å´©æºƒï¼Œå¹¶ä¸”åœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶ï¼Œå®ƒä»¬çš„æ¨ç†åŠªåŠ›ä¼šå…ˆå¢åŠ åå‡å°‘ï¼Œæ˜¾ç¤ºå‡ºåç›´è§‰çš„æ‰©å±•é™åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06006', 'title': 'Bootstrapping World Models from Dynamics Models in Multimodal Foundation\n  Models', 'url': 'https://huggingface.co/papers/2506.06006', 'abstract': 'Foundation models can be fine-tuned to develop dynamics models more easily than world models, with dynamics models aiding world model development through weak supervision and inference time verification, leading to state-of-the-art performance in action-centric image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t To what extent do vision-and-language foundation models possess a realistic world model (observation times action rightarrow observation) and a dynamics model (observation times observation rightarrow action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of 15% on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.', 'score': 10, 'issue_id': 4217, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'fc75b0606885fd24', 'authors': ['Yifu Qiu', 'Yftah Ziser', 'Anna Korhonen', 'Shay B. Cohen', 'Edoardo M. Ponti'], 'affiliations': ['Institute for Language, Cognition and Computation, University of Edinburgh', 'Language Technology Lab, University of Cambridge', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06006.jpg', 'data': {'categories': ['#optimization', '#cv', '#training', '#synthetic', '#multimodal', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (foundation models) Ğ»ĞµĞ³Ñ‡Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ°Ğ±Ñ‹Ğ¹ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Boosting Image Editing with Dynamics Models', 'desc': 'This paper explores how foundation models can be fine-tuned to create dynamics models, which are easier to develop than world models. Dynamics models help in building world models by providing weak supervision and verifying actions during inference. The authors propose two strategies for using dynamics models: one involves using synthetic data to label actions in unannotated video frames, and the other uses these models to score and guide the search for better world models during inference. The results show that their approach significantly improves performance in action-centric image editing tasks, outperforming existing models by 15%.'}, 'zh': {'title': 'åŠ¨æ€æ¨¡å‹åŠ©åŠ›ä¸–ç•Œæ¨¡å‹ï¼Œæå‡å›¾åƒç¼–è¾‘æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºç¡€æ¨¡å‹åœ¨åŠ¨æ€æ¨¡å‹å’Œä¸–ç•Œæ¨¡å‹å¼€å‘ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡å¾®è°ƒåŸºç¡€æ¨¡å‹æ¥è·å–åŠ¨æ€æ¨¡å‹æ¯”è·å–ä¸–ç•Œæ¨¡å‹è¦å®¹æ˜“å¾—å¤šã€‚åŠ¨æ€æ¨¡å‹å¯ä»¥é€šè¿‡å¼±ç›‘ç£å­¦ä¹ å’Œæ¨ç†æ—¶éªŒè¯æ¥è¾…åŠ©ä¸–ç•Œæ¨¡å‹çš„å¼€å‘ï¼Œä»è€Œåœ¨ä»¥åŠ¨ä½œä¸ºä¸­å¿ƒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨Aurora-Benchçš„çœŸå®ä¸–ç•Œå­é›†ä¸Šæ¯”ç°æœ‰çš„å›¾åƒç¼–è¾‘æ¨¡å‹æé«˜äº†15%çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08010', 'title': "Vision Transformers Don't Need Trained Registers", 'url': 'https://huggingface.co/papers/2506.08010', 'abstract': 'A training-free method shifts high-norm activations in Vision Transformers to an untrained token, enhancing attention maps and performance across visual tasks, and improving interpretability in vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.', 'score': 9, 'issue_id': 4212, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'd833304e26f5d1ee', 'authors': ['Nick Jiang', 'Amil Dravid', 'Alexei Efros', 'Yossi Gandelsman'], 'affiliations': ['UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.08010.jpg', 'data': {'categories': ['#cv', '#training', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…, Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ¾Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ¾Ğ¹ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ½ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Vision Transformers with Training-Free Token Shifting', 'desc': 'This paper presents a novel training-free method to improve Vision Transformers by addressing the issue of high-norm activations that lead to noisy attention maps. The authors identify that certain neurons concentrate these high-norm activations on outlier tokens, which disrupts attention patterns and affects visual task performance. Instead of retraining models with additional learned tokens, they propose shifting these activations to an untrained token, effectively simulating the benefits of register tokens without the need for retraining. Their approach not only enhances attention and feature maps but also boosts performance across various visual tasks, making it applicable to existing vision-language models for better interpretability.'}, 'zh': {'title': 'æ— è®­ç»ƒæ–¹æ³•æå‡è§†è§‰æ¨¡å‹æ€§èƒ½ä¸å¯è§£é‡Šæ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡å°†é«˜èŒƒæ•°æ¿€æ´»è½¬ç§»åˆ°æœªè®­ç»ƒçš„æ ‡è®°ä¸Šï¼Œæ¥å¢å¼ºè§†è§‰å˜æ¢å™¨ä¸­çš„æ³¨æ„åŠ›å›¾å’Œæ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å¤šä¸ªæ¨¡å‹ä¸­ï¼Œä¸€å°éƒ¨åˆ†ç¥ç»å…ƒè´Ÿè´£å°†é«˜èŒƒæ•°æ¿€æ´»é›†ä¸­åœ¨å¼‚å¸¸æ ‡è®°ä¸Šï¼Œå¯¼è‡´æ³¨æ„åŠ›æ¨¡å¼ä¸è§„åˆ™ï¼Œå½±å“è§†è§‰å¤„ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨å‘ç°çš„æ³¨å†Œç¥ç»å…ƒå°†é«˜èŒƒæ•°æ¿€æ´»è½¬ç§»åˆ°é¢å¤–çš„æœªè®­ç»ƒæ ‡è®°ä¸Šï¼Œä»è€Œæ”¹å–„æ³¨æ„åŠ›å’Œç‰¹å¾å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­æå‡äº†æ€§èƒ½ï¼Œå¹¶ä¸”ä¸æ˜¾å¼è®­ç»ƒçš„æ³¨å†Œæ ‡è®°æ¨¡å‹çš„ç»“æœç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07463', 'title': 'CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.07463', 'abstract': 'A large-scale bilingual pre-training dataset, CCI4.0, enhances data quality and diverse reasoning patterns for language models, leading to improved performance in downstream tasks like math and code reflection.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly 35 TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully curated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose a novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract 4.5 billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora.', 'score': 8, 'issue_id': 4209, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '219a3c7d2fbb1e22', 'authors': ['Guang Liu', 'Liangdong Wang', 'Jijie Li', 'Yang Yu', 'Yao Xu', 'Jiabei Chen', 'Yu Bai', 'Feng Liao', 'Yonghua Lin'], 'affiliations': ['baai.ac.cn'], 'pdf_title_img': 'assets/pdf/title_img/2506.07463.jpg', 'data': {'categories': ['#hallucinations', '#data', '#dataset', '#reasoning', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CCI4.0: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'CCI4.0 - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¿Ğ¾Ğ´Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ²: CCI4.0-M2-Base Ğ¸ CCI4.0-M2-CoT, Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ¾ĞºĞ¾Ğ»Ğ¾ 35 Ğ¢Ğ‘. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼.'}, 'en': {'title': 'Enhancing Language Models with CCI4.0: Quality Data for Better Reasoning', 'desc': 'The paper presents CCI4.0, a large-scale bilingual pre-training dataset designed to improve the quality of data and enhance reasoning capabilities in language models. It consists of two main sub-datasets, CCI4.0-M2-Base and CCI4.0-M2-CoT, which together provide a diverse range of high-quality data from various domains. The authors introduce a novel data curation pipeline that includes deduplication, quality scoring, and fluency filtering to ensure the reliability of the training data. Empirical results show that language models trained on CCI4.0 exhibit significant performance improvements in tasks such as mathematics and code reflection, highlighting the importance of quality data in machine learning.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„åŒè¯­æ•°æ®é›†CCI4.0', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCCI4.0çš„å¤§è§„æ¨¡åŒè¯­é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜æ•°æ®è´¨é‡å’Œå¤šæ ·åŒ–çš„æ¨ç†æ¨¡å¼ã€‚CCI4.0åŒ…å«çº¦35 TBçš„æ•°æ®ï¼Œåˆ†ä¸ºä¸¤ä¸ªå­æ•°æ®é›†ï¼šCCI4.0-M2-Baseå’ŒCCI4.0-M2-CoTã€‚é€šè¿‡ä¸¤é˜¶æ®µå»é‡ã€å¤šåˆ†ç±»å™¨è´¨é‡è¯„åˆ†å’Œé¢†åŸŸæ„ŸçŸ¥æµç•…æ€§è¿‡æ»¤ç­‰æ–°é¢–æµç¨‹ï¼Œç¡®ä¿äº†æ•°æ®çš„é«˜è´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨CCI4.0é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦å’Œä»£ç åå°„ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07309', 'title': 'ConfQA: Answer Only If You Are Confident', 'url': 'https://huggingface.co/papers/2506.07309', 'abstract': 'ConfQA fine-tuning strategy reduces factual statement hallucination in LLMs by 80%, using a dampening prompt and factual statements from knowledge graphs to improve confidence calibration and knowledge selection.  \t\t\t\t\tAI-generated summary \t\t\t\t Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present a fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit "I am unsure". But there are two key factors that make the training highly effective. First, we introduce a dampening prompt "answer only if you are confident" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA\'s confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%.', 'score': 8, 'issue_id': 4211, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': '8a4581c11bf35360', 'authors': ['Yin Huang', 'Yifan Ethan Xu', 'Kai Sun', 'Vera Yan', 'Alicia Sun', 'Haidar Khan', 'Jimmy Nguyen', 'Mohammad Kachuee', 'Zhaojiang Lin', 'Yue Liu', 'Aaron Colak', 'Anuj Kumar', 'Wen-tau Yih', 'Xin Luna Dong'], 'affiliations': ['FAIR at Meta', 'Meta Reality Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.07309.jpg', 'data': {'categories': ['#rlhf', '#interpretability', '#hallucinations', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ConfQA: Ğ¾Ğ±ÑƒĞ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ConfQA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ñ 20-40% Ğ´Ğ¾ Ğ¼ĞµĞ½ĞµĞµ 5% Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Dual Neural Knowledge Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'ConfQA: Reducing Hallucinations in LLMs with Confidence Calibration', 'desc': "This paper introduces a fine-tuning strategy called ConfQA that significantly reduces the occurrence of factual statement hallucinations in Large Language Models (LLMs) by up to 80%. The approach involves training the model to confidently provide answers when it knows them and to express uncertainty when it does not. Key components of this strategy include a dampening prompt that encourages the model to answer only when confident, and the use of factual statements from knowledge graphs to enhance the model's confidence calibration. Additionally, the proposed Dual Neural Knowledge framework allows the model to effectively choose between internal neural knowledge and external symbolic knowledge based on its confidence level, leading to improved accuracy and reduced reliance on external data retrieval."}, 'zh': {'title': 'å‡å°‘å¹»è§‰ï¼Œæé«˜å‡†ç¡®æ€§ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºConfQAçš„å¾®è°ƒç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„äº‹å®é™ˆè¿°å¹»è§‰ç°è±¡ã€‚é€šè¿‡ä½¿ç”¨å‡å¼±æç¤ºå’ŒçŸ¥è¯†å›¾è°±ä¸­çš„äº‹å®é™ˆè¿°ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿå°†å¹»è§‰ç‡ä»20-40%é™ä½åˆ°5%ä»¥ä¸‹ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå½“LLMæ­£ç¡®å›ç­”é—®é¢˜æ—¶ï¼Œç»§ç»­ç»™å‡ºç­”æ¡ˆï¼›å¦åˆ™ï¼Œæ‰¿è®¤â€œä¸ç¡®å®šâ€ã€‚æ­¤å¤–ï¼ŒConfQAæ¡†æ¶é€šè¿‡é€‰æ‹©å†…éƒ¨å‚æ•°åŒ–çš„ç¥ç»çŸ¥è¯†å’Œå¤–éƒ¨è®°å½•çš„ç¬¦å·çŸ¥è¯†ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’ŒçŸ¥è¯†é€‰æ‹©èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08012', 'title': 'GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior', 'url': 'https://huggingface.co/papers/2506.08012', 'abstract': 'A novel framework, GUI-Reflection, integrates self-reflection and error correction into multimodal GUI models through specialized training stages, enabling more robust and intelligent automation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.', 'score': 7, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '8052bb442adf3c53', 'authors': ['Penghao Wu', 'Shengnan Ma', 'Bo Wang', 'Jiaheng Yu', 'Lewei Lu', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.08012.jpg', 'data': {'categories': ['#optimization', '#dataset', '#agents', '#training', '#open_source', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GUI-Reflection, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ­Ñ‚Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ GUI, Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. GUI-Reflection Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ GUI-Reflection Task Suite Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ….'}, 'en': {'title': 'Empowering GUI Automation with Self-Reflection and Error Correction', 'desc': 'The paper introduces GUI-Reflection, a new framework that enhances multimodal GUI models by incorporating self-reflection and error correction during training. This approach addresses the limitations of existing models that primarily learn from error-free data, enabling them to recover from mistakes and improve over time. The framework includes specialized training stages such as pre-training, supervised fine-tuning, and online reflection tuning, allowing models to learn from their own errors without human intervention. By automating data generation and focusing on reflection-oriented tasks, GUI-Reflection aims to create more robust and intelligent automation for graphical user interfaces.'}, 'zh': {'title': 'è‡ªæˆ‘åæ€ä¸é”™è¯¯çº æ­£ï¼Œæå‡GUIè‡ªåŠ¨åŒ–æ™ºèƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶GUI-Reflectionï¼Œå°†è‡ªæˆ‘åæ€å’Œé”™è¯¯çº æ­£èƒ½åŠ›æ•´åˆåˆ°å¤šæ¨¡æ€å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ¨¡å‹ä¸­ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“é—¨çš„è®­ç»ƒé˜¶æ®µï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è‡ªåŠ¨åŒ–è¿‡ç¨‹ä¸­æ›´å…·é²æ£’æ€§å’Œæ™ºèƒ½æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†å¯æ‰©å±•çš„æ•°æ®ç®¡é“ï¼Œè‡ªåŠ¨ç”Ÿæˆåæ€å’Œé”™è¯¯çº æ­£çš„æ•°æ®ï¼Œå¹¶æå‡ºäº†GUI-Reflectionä»»åŠ¡å¥—ä»¶æ¥è¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæ¡†æ¶ä½¿å¾—GUIä»£ç†å…·å¤‡è‡ªæˆ‘åæ€å’Œçº æ­£èƒ½åŠ›ï¼Œä¸ºæ›´æ™ºèƒ½çš„GUIè‡ªåŠ¨åŒ–å¥ å®šåŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07434', 'title': 'Well Begun is Half Done: Low-resource Preference Alignment by\n  Weak-to-Strong Decoding', 'url': 'https://huggingface.co/papers/2506.07434', 'abstract': 'A novel Weak-to-Strong Decoding (WSD) framework enhances the alignment of large language models using a small aligned model to draft responses initially, improving alignment and performance without degrading downstream task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth.', 'score': 7, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '11d72d8081b838d5', 'authors': ['Feifan Song', 'Shaohang Wei', 'Wen Luo', 'Yuxuan Fan', 'Tianyu Liu', 'Guoyin Wang', 'Houfeng Wang'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, School of Computer Science Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07434.jpg', 'data': {'categories': ['#alignment', '#dataset', '#training', '#rlhf'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¡Ğ»Ğ°Ğ±Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚, ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ¸Ğ½Ğ¸Ñˆ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Weak-to-Strong Decoding (WSD) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºÑƒÑ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ¿Ğ¾ÑĞ»Ğµ Ñ‡ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ GenerAlign Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Pilot-3B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… WSD. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing LLM Alignment with Weak-to-Strong Decoding', 'desc': 'The paper introduces a new framework called Weak-to-Strong Decoding (WSD) that improves the alignment of large language models (LLMs) with human preferences. It uses a smaller, aligned model to generate the initial part of responses, which helps guide the larger model in producing better outputs. This approach addresses the challenge of generating high-quality and aligned content without negatively impacting the performance on other tasks, known as the alignment tax. The authors also present a new dataset, GenerAlign, which is used to fine-tune the smaller model, leading to superior results compared to existing methods.'}, 'zh': {'title': 'å¼±åˆ°å¼ºè§£ç ï¼šæå‡è¯­è¨€æ¨¡å‹å¯¹é½èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼±åˆ°å¼ºè§£ç æ¡†æ¶ï¼ˆWSDï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å°å‹å¯¹é½æ¨¡å‹çš„æŒ‡å¯¼æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é¦–å…ˆç”±å°æ¨¡å‹è‰æ‹Ÿå‡ºé«˜è´¨é‡çš„å¯¹é½å¼€å¤´ï¼Œç„¶åç”±å¤§å‹åŸºç¡€æ¨¡å‹ç»§ç»­ç”Ÿæˆåç»­å†…å®¹ã€‚é€šè¿‡è®¾è®¡è‰¯å¥½çš„è‡ªåŠ¨åˆ‡æ¢æœºåˆ¶ï¼ŒWSDèƒ½å¤Ÿåœ¨ä¸é™ä½ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚æˆ‘ä»¬è¿˜æ”¶é›†äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†GenerAlignï¼Œä»¥å¾®è°ƒå°å‹Pilot-3Bæ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šç§åŸºçº¿æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23760', 'title': 'Model Immunization from a Condition Number Perspective', 'url': 'https://huggingface.co/papers/2505.23760', 'abstract': 'An algorithm with regularization terms based on the condition number of a Hessian matrix is proposed to analyze and achieve model immunization, demonstrating effectiveness on both linear models and deep-nets.  \t\t\t\t\tAI-generated summary \t\t\t\t Model immunization aims to pre-train models that are difficult to fine-tune on harmful tasks while retaining their utility on other non-harmful tasks. Though prior work has shown empirical evidence for immunizing text-to-image models, the key understanding of when immunization is possible and a precise definition of an immunized model remain unclear. In this work, we propose a framework, based on the condition number of a Hessian matrix, to analyze model immunization for linear models. Building on this framework, we design an algorithm with regularization terms to control the resulting condition numbers after pre-training. Empirical results on linear models and non-linear deep-nets demonstrate the effectiveness of the proposed algorithm on model immunization. The code is available at https://github.com/amberyzheng/model-immunization-cond-num.', 'score': 7, 'issue_id': 4224, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '764c4291f581dbd9', 'authors': ['Amber Yijia Zheng', 'Cedar Site Bai', 'Brian Bullins', 'Raymond A. Yeh'], 'affiliations': ['Department of Computer Science, Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23760.jpg', 'data': {'categories': ['#training', '#security', '#architecture', '#optimization'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ˜Ğ¼Ğ¼ÑƒĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ“ĞµÑÑĞµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡Ğ»ĞµĞ½Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‡Ğ¸ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ“ĞµÑÑĞµ, Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼Ğ¼ÑƒĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ framework Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ¼Ğ¼ÑƒĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ framework ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ‡Ğ¸ÑĞµĞ» Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¼ÑƒĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Model Immunization with Hessian-Based Regularization', 'desc': 'This paper introduces a new algorithm that uses regularization based on the condition number of the Hessian matrix to improve model immunization. Model immunization is the process of preparing machine learning models to avoid harmful tasks while still performing well on safe tasks. The authors provide a framework to better understand when immunization is feasible and define what an immunized model is. Their empirical results show that the proposed algorithm is effective for both linear models and deep neural networks, enhancing the robustness of these models against harmful influences.'}, 'zh': {'title': 'åŸºäºHessiançŸ©é˜µçš„æ¨¡å‹å…ç–«ç®—æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºHessiançŸ©é˜µæ¡ä»¶æ•°çš„æ­£åˆ™åŒ–ç®—æ³•ï¼Œç”¨äºåˆ†æå’Œå®ç°æ¨¡å‹å…ç–«ã€‚æ¨¡å‹å…ç–«çš„ç›®æ ‡æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨æœ‰å®³ä»»åŠ¡ä¸Šéš¾ä»¥å¾®è°ƒï¼ŒåŒæ—¶åœ¨å…¶ä»–æ— å®³ä»»åŠ¡ä¸Šä¿æŒæœ‰æ•ˆæ€§ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å·²æ˜¾ç¤ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å…ç–«æ•ˆæœï¼Œä½†å…ç–«ä½•æ—¶å¯èƒ½ä»¥åŠå…ç–«æ¨¡å‹çš„ç²¾ç¡®å®šä¹‰ä»ä¸æ¸…æ¥šã€‚é€šè¿‡å®éªŒè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨çº¿æ€§æ¨¡å‹å’Œæ·±åº¦ç½‘ç»œä¸Šå‡æœ‰æ•ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶é¢„è®­ç»ƒåçš„æ¡ä»¶æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08006', 'title': 'Dreamland: Controllable World Creation with Simulator and Generative\n  Models', 'url': 'https://huggingface.co/papers/2506.08006', 'abstract': 'Dreamland integrates physics simulators and large-scale pretrained generative models for controllable and photorealistic video world generation, improving image quality and controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available.', 'score': 6, 'issue_id': 4210, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'e9dd998a336df034', 'authors': ['Sicheng Mo', 'Ziyang Leng', 'Leon Liu', 'Weizhen Wang', 'Honglin He', 'Bolei Zhou'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.08006.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#games', '#video', '#agents'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¸Ñ€Ñ‹ Ğ¿Ğ¾Ğ´ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼', 'desc': 'Dreamland - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¸Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ĞºĞ°Ğº Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½ÑƒÑ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Dreamland Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ.'}, 'en': {'title': 'Dreamland: Bridging Physics and Generative Models for Enhanced Video World Creation', 'desc': 'Dreamland is a novel framework that combines physics simulators with large-scale pretrained generative models to create realistic and controllable video worlds. It addresses the limitations of existing video generative models by providing element-wise controllability, which is essential for editing scenes and training AI agents. The framework uses a layered world abstraction that captures both pixel-level and object-level details, allowing for better integration between the simulator and the generative model. Experiments show that Dreamland significantly improves image quality and controllability, making it a promising tool for future AI applications.'}, 'zh': {'title': 'æ¢¦å¢ƒï¼šå¯æ§çš„çœŸå®è§†é¢‘ä¸–ç•Œç”Ÿæˆ', 'desc': 'Dreamlandæ˜¯ä¸€ä¸ªç»“åˆç‰©ç†æ¨¡æ‹Ÿå™¨å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹çš„æ··åˆä¸–ç•Œç”Ÿæˆæ¡†æ¶ã€‚å®ƒé€šè¿‡è®¾è®¡åˆ†å±‚ä¸–ç•ŒæŠ½è±¡ï¼Œç¼–ç åƒç´ çº§å’Œç‰©ä½“çº§çš„è¯­ä¹‰ä¸å‡ ä½•ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†å›¾åƒè´¨é‡å’Œå¯æ§æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°æ›´ç»†ç²’åº¦çš„æ§åˆ¶ï¼Œå‡å°‘é€‚åº”æˆæœ¬ï¼Œå¹¶æ”¯æŒç°æœ‰å’Œæœªæ¥ç”Ÿæˆæ¨¡å‹çš„ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamlandåœ¨å›¾åƒè´¨é‡å’Œå¯æ§æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå…·æœ‰æå‡æ™ºèƒ½ä½“è®­ç»ƒçš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05598', 'title': 'SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward\n  Models in LLMs', 'url': 'https://huggingface.co/papers/2506.05598', 'abstract': 'SynthesizeMe generates synthetic user personas from interactions for personalized reward modeling, improving LLM accuracy and achieving top performance on PersonalRewardBench.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. SynthesizeMe first generates and verifies reasoning to explain user preferences, then induces synthetic user personas from that reasoning, and finally filters to informative prior user interactions in order to build personalized prompts for a particular user. We show that using SynthesizeMe induced prompts improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining SynthesizeMe derived prompts with a reward model achieves top performance on PersonalRewardBench: a new curation of user-stratified interactions with chatbots collected from 854 users of Chatbot Arena and PRISM.', 'score': 6, 'issue_id': 4223, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'c840a8d8856e1f61', 'authors': ['Michael J Ryan', 'Omar Shaikh', 'Aditri Bhagirath', 'Daniel Frees', 'William Held', 'Diyi Yang'], 'affiliations': ['Georgia Institute of Technology', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05598.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#data', '#training', '#alignment', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'SynthesizeMe - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ SynthesizeMe ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° 4,4% Ğ² Chatbot Arena. Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, SynthesizeMe Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ PersonalRewardBench.'}, 'en': {'title': 'Empowering LLMs with Synthetic User Personas for Personalization', 'desc': 'The paper presents SynthesizeMe, a novel method for creating synthetic user personas based on user interactions, aimed at enhancing personalized reward modeling for Large Language Models (LLMs). Unlike traditional approaches that depend on demographic data or fixed preference categories, SynthesizeMe generates user personas by analyzing and verifying reasoning behind user preferences. This method not only improves the accuracy of LLMs in personalized tasks but also achieves significant performance gains on the PersonalRewardBench benchmark. By leveraging these synthetic personas, the approach demonstrates a 4.4% increase in accuracy for LLMs acting as judges in user interactions.'}, 'zh': {'title': 'åˆæˆç”¨æˆ·è§’è‰²ï¼Œæå‡ä¸ªæ€§åŒ–å¥–åŠ±å»ºæ¨¡', 'desc': 'SynthesizeMeæ˜¯ä¸€ç§é€šè¿‡ç”¨æˆ·äº¤äº’ç”Ÿæˆåˆæˆç”¨æˆ·è§’è‰²çš„æ–¹æ³•ï¼Œç”¨äºä¸ªæ€§åŒ–å¥–åŠ±å»ºæ¨¡ã€‚è¯¥æ–¹æ³•é¦–å…ˆç”Ÿæˆå¹¶éªŒè¯è§£é‡Šç”¨æˆ·åå¥½çš„æ¨ç†ï¼Œç„¶åä»è¿™äº›æ¨ç†ä¸­è¯±å¯¼å‡ºåˆæˆç”¨æˆ·è§’è‰²ï¼Œæœ€åè¿‡æ»¤å‡ºæœ‰ç”¨çš„ç”¨æˆ·äº¤äº’ï¼Œä»¥æ„å»ºç‰¹å®šç”¨æˆ·çš„ä¸ªæ€§åŒ–æç¤ºã€‚ä½¿ç”¨SynthesizeMeç”Ÿæˆçš„æç¤ºå¯ä»¥æé«˜ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…çš„å‡†ç¡®æ€§ï¼Œæå‡å¹…åº¦è¾¾åˆ°4.4%ã€‚ç»“åˆSynthesizeMeç”Ÿæˆçš„æç¤ºä¸å¥–åŠ±æ¨¡å‹ï¼Œåœ¨PersonalRewardBenchä¸Šå–å¾—äº†æœ€ä½³è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01241', 'title': 'ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form\n  Generation Tasks with Structured Checklists', 'url': 'https://huggingface.co/papers/2506.01241', 'abstract': 'ExpertLongBench is a benchmark for expert-level workflows with diverse, long-form tasks, evaluated using CLEAR, a framework that assesses outputs based on domain-specific rubrics and checklists.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces ExpertLongBench, an expert-level benchmark containing 11 tasks from 9 domains that reflect realistic expert workflows and applications. Beyond question answering, the application-driven tasks in ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and strict adherence to domain-specific requirements. Notably, each task in ExpertLongBench includes a rubric, designed or validated by domain experts, to specify task requirements and guide output evaluation. Furthermore, we propose CLEAR, an evaluation framework that supports accurate evaluation of long-form model outputs in our benchmark. To achieve fine-grained, expert-aligned evaluation, CLEAR derives checklists from both model outputs and references by extracting information corresponding to items in the task-specific rubric. Checklist items for model outputs are then compared with corresponding items for reference outputs to assess their correctness, enabling grounded evaluation. We benchmark 11 large language models (LLMs) and analyze components in CLEAR, showing that (1) existing LLMs, with the top performer achieving only a 26.8% F1 score, require significant improvement for expert-level tasks; (2) models can generate content corresponding to the required aspects, though often not accurately; and (3) accurate checklist extraction and comparison in CLEAR can be achieved by open-weight models for more scalable and low-cost usage.', 'score': 6, 'issue_id': 4222, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '9956d204f343761f', 'authors': ['Jie Ruan', 'Inderjeet Nair', 'Shuyang Cao', 'Amy Liu', 'Sheza Munir', 'Micah Pollens-Dempsey', 'Tiffany Chiang', 'Lucy Kates', 'Nicholas David', 'Sihan Chen', 'Ruxin Yang', 'Yuqian Yang', 'Jasmine Gump', 'Tessa Bialek', 'Vivek Sankaran', 'Margo Schlanger', 'Lu Wang'], 'affiliations': ['Biomedical Engineering, University of Michigan', 'Computer Science and Engineering, University of Michigan', 'Department of Chemistry, Carnegie Mellon University', 'Materials Science & Engineering, University of Michigan', 'School of Information, University of Michigan', 'University of Michigan Law School'], 'pdf_title_img': 'assets/pdf/title_img/2506.01241.jpg', 'data': {'categories': ['#optimization', '#data', '#benchmark', '#long_context', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ExpertLongBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'ExpertLongBench - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 11 Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· 9 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ¾ 5000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº CLEAR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ€ÑƒĞ±Ñ€Ğ¸Ğº Ğ¸ Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 11 Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»Ğ¸ÑˆÑŒ 26.8% F1-score, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. CLEAR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'ExpertLongBench: Elevating AI Evaluation for Expert Workflows', 'desc': 'This paper presents ExpertLongBench, a benchmark designed for evaluating expert-level workflows through 11 diverse tasks across 9 domains. The tasks require long-form outputs that can be extensive, exceeding 5,000 tokens, and must meet specific domain requirements. To assess these outputs, the authors introduce CLEAR, an evaluation framework that utilizes domain-specific rubrics and checklists for precise evaluation. The study benchmarks 11 large language models, revealing that current models struggle with expert-level tasks, highlighting the need for significant improvements in their performance.'}, 'zh': {'title': 'ExpertLongBenchï¼šä¸“å®¶çº§ä»»åŠ¡çš„è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ExpertLongBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸“å®¶çº§å·¥ä½œæµç¨‹çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¥è‡ª9ä¸ªé¢†åŸŸçš„11ä¸ªä»»åŠ¡ï¼Œåæ˜ äº†çœŸå®çš„ä¸“å®¶åº”ç”¨ã€‚è¯¥åŸºå‡†æµ‹è¯•çš„ä»»åŠ¡ä¸ä»…é™äºé—®ç­”ï¼Œè¿˜è¦æ±‚ç”Ÿæˆè¶…è¿‡5000ä¸ªæ ‡è®°çš„é•¿æ–‡æœ¬è¾“å‡ºï¼Œå¹¶ä¸¥æ ¼éµå¾ªé¢†åŸŸç‰¹å®šçš„è¦æ±‚ã€‚æ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰ç”±é¢†åŸŸä¸“å®¶è®¾è®¡æˆ–éªŒè¯çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥æ˜ç¡®ä»»åŠ¡è¦æ±‚å¹¶æŒ‡å¯¼è¾“å‡ºè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†CLEARè¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå¯¹é•¿æ–‡æœ¬æ¨¡å‹è¾“å‡ºçš„å‡†ç¡®è¯„ä¼°ï¼Œç¡®ä¿è¯„ä¼°è¿‡ç¨‹çš„ç»†è‡´å’Œä¸ä¸“å®¶æ ‡å‡†çš„ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08011', 'title': 'Play to Generalize: Learning to Reason Through Game Play', 'url': 'https://huggingface.co/papers/2506.08011', 'abstract': "Post-training multimodal large language models with reinforcement learning on arcade-like games enhances their multimodal reasoning abilities without domain-specific data.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs.", 'score': 5, 'issue_id': 4218, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '3ca46b3c756ab23e', 'authors': ['Yunfei Xie', 'Yinsong Ma', 'Shiyi Lan', 'Alan Yuille', 'Junfei Xiao', 'Chen Wei'], 'affiliations': ['Johns Hopkins University', 'NVIDIA', 'Rice University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08011.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#rl', '#games', '#transfer_learning'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ˜Ğ³Ñ€Ñ‹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ³Ñ€ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ğ°Ñ€ĞºĞ°Ğ´Ñ‹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Visual Game Learning (ViGaL), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ² Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ²Ğ¾Ñ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Unlocking Reasoning Skills through Game Play', 'desc': 'This paper introduces a new method called Visual Game Learning (ViGaL) to improve the reasoning skills of multimodal large language models (MLLMs) by using arcade-like games. The authors demonstrate that by applying reinforcement learning to a 7B-parameter MLLM while it plays these games, the model can enhance its performance on complex multimodal tasks without needing specific training data. The results show that this approach allows the model to generalize its reasoning abilities effectively, outperforming specialized models that are trained on multimodal reasoning data. This suggests that engaging with simple, rule-based games can be a powerful way to develop transferable cognitive skills in MLLMs.'}, 'zh': {'title': 'é€šè¿‡æ¸¸æˆæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºè§†è§‰æ¸¸æˆå­¦ä¹ ï¼ˆViGaLï¼‰ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨ç®€å•çš„è¡—æœºæ¸¸æˆä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰ç‰¹å®šé¢†åŸŸæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¢å¼ºå…¶å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡è¿™ç§è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•å’Œè·¨å­¦ç§‘é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¸“é—¨é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†æ•°æ®è°ƒä¼˜çš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†åˆæˆã€åŸºäºè§„åˆ™çš„æ¸¸æˆå¯ä»¥ä½œä¸ºå¯æ§å’Œå¯æ‰©å±•çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œå¸®åŠ©MLLMsè§£é”å¯è¿ç§»çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06485', 'title': 'What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge\n  Conflict on Large Language Models', 'url': 'https://huggingface.co/papers/2506.06485', 'abstract': "LLMs show varying performance under context-memory conflict, with conflicting knowledge impacting tasks that require knowledge utilization and model reliance on internal knowledge challenging to suppress.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model's parametric knowledge. We propose a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. We construct diagnostic data that elicit these conflicts and analyze model performance across multiple task types. Our findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs.", 'score': 4, 'issue_id': 4222, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'd74da032a2f344f1', 'authors': ['Kaiser Sun', 'Fan Bai', 'Mark Dredze'], 'affiliations': ['Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06485.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#hallucinations', '#dataset', '#long_context', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² LLM: ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ÑÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑƒÑ‡ĞµÑ‚Ğ° ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ LLM.'}, 'en': {'title': 'Navigating Knowledge Conflicts in Large Language Models', 'desc': "This paper investigates how large language models (LLMs) perform when there is a conflict between contextual information and their internal knowledge. The authors introduce a framework to assess LLM behavior in situations where the context contradicts the model's learned knowledge. They find that while knowledge conflict does not significantly affect tasks that don't require knowledge, it does impact performance when alignment between context and knowledge is necessary. Additionally, the study shows that LLMs struggle to ignore their internal knowledge even when asked to do so, and providing explanations for conflicts can help models rely more on contextual information."}, 'zh': {'title': 'ç†è§£ä¸Šä¸‹æ–‡ä¸è®°å¿†å†²çªå¯¹LLMçš„å½±å“', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡ä¸è®°å¿†å†²çªçš„æƒ…å†µä¸‹è¡¨ç°ä¸ä¸€ï¼Œå†²çªçš„çŸ¥è¯†ä¼šå½±å“éœ€è¦çŸ¥è¯†åˆ©ç”¨çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯Šæ–­æ¡†æ¶ï¼Œç³»ç»Ÿè¯„ä¼°LLMåœ¨ä¸Šä¸‹æ–‡ä¸å‚æ•°çŸ¥è¯†ä¸ä¸€è‡´æ—¶çš„è¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼Œå½“ä¸Šä¸‹æ–‡å’Œå‚æ•°çŸ¥è¯†ä¸€è‡´æ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°æ›´å¥½ï¼Œè€Œåœ¨ä¸éœ€è¦çŸ¥è¯†åˆ©ç”¨çš„ä»»åŠ¡ä¸­ï¼ŒçŸ¥è¯†å†²çªçš„å½±å“è¾ƒå°ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å³ä½¿åœ¨æŒ‡ç¤ºä¸‹ä¹Ÿæ— æ³•å®Œå…¨æŠ‘åˆ¶å…¶å†…éƒ¨çŸ¥è¯†ï¼Œè¿™è¡¨æ˜åœ¨éƒ¨ç½²LLMæ—¶éœ€è¦è€ƒè™‘çŸ¥è¯†å†²çªçš„é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06266', 'title': 'Cartridges: Lightweight and general-purpose long context representations\n  via self-study', 'url': 'https://huggingface.co/papers/2506.06266', 'abstract': "Cartridges trained with self-study replicate in-context learning functionality while reducing memory and increasing throughput, and enable longer context lengths and composability at inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.", 'score': 4, 'issue_id': 4211, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '0d49d6022b9d6786', 'authors': ['Sabri Eyuboglu', 'Ryan Ehrlich', 'Simran Arora', 'Neel Guha', 'Dylan Zinsley', 'Emily Liu', 'Will Tennien', 'Atri Rudra', 'James Zou', 'Azalia Mirhoseini', 'Christopher Re'], 'affiliations': ['Caltech', 'Stanford University', 'University at Buffalo'], 'pdf_title_img': 'assets/pdf/title_img/2506.06266.jpg', 'data': {'categories': ['#long_context', '#data', '#inference', '#synthetic', '#training', '#optimization'], 'emoji': 'ğŸ’¾', 'ru': {'title': 'ĞšĞ°Ñ€Ñ‚Ñ€Ğ¸Ğ´Ğ¶Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 'ĞºĞ°Ñ€Ñ‚Ñ€Ğ¸Ğ´Ğ¶Ğ°Ğ¼Ğ¸'. ĞšĞ°Ñ€Ñ‚Ñ€Ğ¸Ğ´Ğ¶Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ 'ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ€Ğ¸Ğ´Ğ¶ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ¸Ğ»Ğ¸ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹."}, 'en': {'title': 'Efficient In-Context Learning with Cartridges', 'desc': "This paper introduces a novel approach called Cartridges, which are trained using a method called self-study to enhance in-context learning (ICL) in large language models. By creating a smaller key-value (KV) cache for each text corpus, Cartridges significantly reduce memory usage and increase processing speed during inference. The self-study training method involves generating synthetic conversations to improve the Cartridge's performance, allowing it to match ICL capabilities while being more efficient. The results show that Cartridges can handle longer context lengths and can be composed without the need for retraining, making them a powerful tool for various applications."}, 'zh': {'title': 'è‡ªæˆ‘å­¦ä¹ ï¼šé«˜æ•ˆçš„ä¸Šä¸‹æ–‡å­¦ä¹ è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCartridgeçš„æŠ€æœ¯ï¼Œé€šè¿‡è‡ªæˆ‘å­¦ä¹ è®­ç»ƒå°å‹KVç¼“å­˜ï¼Œä»¥å®ç°ä¸Šä¸‹æ–‡å­¦ä¹ åŠŸèƒ½ï¼ŒåŒæ—¶é™ä½å†…å­˜æ¶ˆè€—å¹¶æé«˜å¤„ç†é€Ÿåº¦ã€‚ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å°†æ•´ä¸ªæ–‡æœ¬è¯­æ–™æ”¾å…¥ä¸Šä¸‹æ–‡çª—å£ï¼Œå¯¼è‡´é«˜æ˜‚çš„å†…å­˜æˆæœ¬ï¼Œè€ŒCartridgeé€šè¿‡ç¦»çº¿è®­ç»ƒç¼“å­˜æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è‡ªæˆ‘å­¦ä¹ çš„æ–¹æ³•è®­ç»ƒCartridgeï¼Œå¯ä»¥åœ¨ä¿æŒä¸ä¸Šä¸‹æ–‡å­¦ä¹ ç›¸ä¼¼çš„æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨å’Œæé«˜ååé‡ã€‚æœ€ç»ˆï¼ŒCartridgeä¸ä»…æ‰©å±•äº†æ¨¡å‹çš„æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè¿˜èƒ½åœ¨æ¨ç†æ—¶è¿›è¡Œç»„åˆï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04651', 'title': 'Agents of Change: Self-Evolving LLM Agents for Strategic Planning', 'url': 'https://huggingface.co/papers/2506.04651', 'abstract': "LLM agents improve their strategic planning and adapt over time when placed in complex environments like Settlers of Catan, outperforming static baselines through self-evolving mechanisms and collaboration among specialized roles.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in LLMs have enabled their use as autonomous agents across a range of tasks, yet they continue to struggle with formulating and adhering to coherent long-term strategies. In this paper, we investigate whether LLM agents can self-improve when placed in environments that explicitly challenge their strategic planning abilities. Using the board game Settlers of Catan, accessed through the open-source Catanatron framework, we benchmark a progression of LLM-based agents, from a simple game-playing agent to systems capable of autonomously rewriting their own prompts and their player agent's code. We introduce a multi-agent architecture in which specialized roles (Analyzer, Researcher, Coder, and Player) collaborate to iteratively analyze gameplay, research new strategies, and modify the agent's logic or prompt. By comparing manually crafted agents to those evolved entirely by LLMs, we evaluate how effectively these systems can diagnose failure and adapt over time. Our results show that self-evolving agents, particularly when powered by models like Claude 3.7 and GPT-4o, outperform static baselines by autonomously adopting their strategies, passing along sample behavior to game-playing agents, and demonstrating adaptive reasoning over multiple iterations.", 'score': 4, 'issue_id': 4224, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'b4947cd58776a704', 'authors': ['Nikolas Belle', 'Dakota Barnes', 'Alfonso Amayuelas', 'Ivan Bercovich', 'Xin Eric Wang', 'William Wang'], 'affiliations': ['University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2506.04651.jpg', 'data': {'categories': ['#reasoning', '#agi', '#optimization', '#games', '#architecture', '#agents', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸Ğ³Ñ€Ğ° 'ĞšĞ¾Ğ»Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ³Ğ´Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ (ĞĞ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ, ĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚ Ğ¸ Ğ˜Ğ³Ñ€Ğ¾Ğº) ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Claude 3.7 Ğ¸ GPT-4, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹."}, 'en': {'title': 'Empowering LLM Agents to Evolve and Strategize in Complex Games', 'desc': "This paper explores how large language model (LLM) agents can enhance their strategic planning abilities in complex environments, specifically using the board game Settlers of Catan. The authors introduce a multi-agent system where specialized roles work together to analyze gameplay, research strategies, and modify the agents' logic. By benchmarking LLM agents that can autonomously rewrite their own prompts and code, the study demonstrates that these self-evolving agents significantly outperform static counterparts. The findings highlight the potential of LLMs to adapt and improve over time through collaboration and iterative learning."}, 'zh': {'title': 'è‡ªæˆ‘è¿›åŒ–çš„LLMä»£ç†ï¼šåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æˆ˜ç•¥æå‡', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤æ‚ç¯å¢ƒä¸­å¦‚ä½•é€šè¿‡è‡ªæˆ‘è¿›åŒ–å’Œè§’è‰²åä½œæ¥æé«˜æˆ˜ç•¥è§„åˆ’èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æºçš„Catanatronæ¡†æ¶ï¼Œåœ¨ã€Šå¡å¦å²›ã€‹æ¸¸æˆä¸­è¯„ä¼°äº†ä¸åŒçš„LLMä»£ç†ï¼Œä»ç®€å•çš„æ¸¸æˆä»£ç†åˆ°èƒ½å¤Ÿè‡ªä¸»é‡å†™æç¤ºå’Œä»£ç çš„ç³»ç»Ÿã€‚é€šè¿‡å¼•å…¥å¤šä»£ç†æ¶æ„ï¼Œä»£ç†ä»¬åœ¨åˆ†ææ¸¸æˆã€ç ”ç©¶æ–°ç­–ç•¥å’Œä¿®æ”¹é€»è¾‘æ–¹é¢è¿›è¡Œåä½œã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªæˆ‘è¿›åŒ–çš„ä»£ç†åœ¨æˆ˜ç•¥é€‚åº”æ€§å’Œé•¿æœŸè§„åˆ’ä¸Šæ˜¾è‘—ä¼˜äºé™æ€åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07982', 'title': 'Ï„^2-Bench: Evaluating Conversational Agents in a Dual-Control\n  Environment', 'url': 'https://huggingface.co/papers/2506.07982', 'abstract': 'tauÂ²-bench introduces a dual-control benchmark for conversational AI agents in telecommunications, simulating user-agent collaboration and evaluating their performance in reasoning and coordination.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce tau^2-bench, with four key contributions:   1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication,   2) A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity,   3) A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity,   4) Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination.   In particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, tau^2-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.', 'score': 3, 'issue_id': 4224, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'f5bec003306d8b3b', 'authors': ['Victor Barres', 'Honghua Dong', 'Soham Ray', 'Xujie Si', 'Karthik Narasimhan'], 'affiliations': ['Sierra', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.07982.jpg', 'data': {'categories': ['#agents', '#benchmark', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ tauÂ²-bench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ„ĞµÑ€Ğµ Ñ‚ĞµĞ»ĞµĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. ĞĞ½ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ€ĞµĞ´Ñƒ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼, Ğ³Ğ´Ğµ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğº Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼.'}, 'en': {'title': 'Empowering AI Agents through User Collaboration in Telecom', 'desc': "The paper introduces tauÂ²-bench, a benchmark designed for evaluating conversational AI agents in telecommunications through a dual-control framework. Unlike traditional benchmarks that only involve AI agents, tauÂ²-bench allows both users and agents to actively participate in a shared environment, modeled as a Dec-POMDP. This setup tests the agents' abilities in reasoning, coordination, and communication while interacting with users. The benchmark includes a task generator, a user simulator, and methods for detailed performance analysis, revealing significant challenges when agents operate in dual-control scenarios."}, 'zh': {'title': 'åŒæ§åˆ¶åŸºå‡†ï¼šæå‡å¯¹è¯å¼AIçš„åä½œèƒ½åŠ›', 'desc': 'tauÂ²-bench æ˜¯ä¸€ä¸ªé’ˆå¯¹ç”µä¿¡é¢†åŸŸå¯¹è¯å¼äººå·¥æ™ºèƒ½ä»£ç†çš„åŒæ§åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹Ÿç”¨æˆ·ä¸ä»£ç†çš„åä½œï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨æ¨ç†å’Œåè°ƒæ–¹é¢çš„è¡¨ç°ã€‚ä¸ç°æœ‰çš„å•æ§åˆ¶ç¯å¢ƒä¸åŒï¼ŒtauÂ²-bench å…è®¸ç”¨æˆ·ç§¯æå‚ä¸ï¼Œå…±åŒä¿®æ”¹å…±äº«ç¯å¢ƒçš„çŠ¶æ€ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸€ä¸ªæ–°é¢–çš„ç”µä¿¡åŒæ§åˆ¶é¢†åŸŸæ¨¡å‹ï¼Œä½¿ç”¨ Dec-POMDP è¿›è¡Œå»ºæ¨¡ï¼Œæµ‹è¯•ä»£ç†çš„åè°ƒå’Œæ²Ÿé€šèƒ½åŠ›ã€‚é€šè¿‡å¤šç§å®éªŒï¼Œç»“æœæ˜¾ç¤ºåœ¨åŒæ§åˆ¶ç¯å¢ƒä¸­ï¼Œä»£ç†çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œçªæ˜¾äº†å¼•å¯¼ç”¨æˆ·çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07527', 'title': "Learning What Reinforcement Learning Can't: Interleaved Online\n  Fine-Tuning for Hardest Questions", 'url': 'https://huggingface.co/papers/2506.07527', 'abstract': "ReLIFT, a method combining reinforcement learning and supervised fine-tuning, enhances large language model reasoning by addressing limitations of RL through interleaved training, improving performance across benchmarks with minimal data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, ReLIFT (Reinforcement Learning Interleaved with Online Fine-Tuning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.", 'score': 3, 'issue_id': 4214, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '6978ee3d97028c45', 'authors': ['Lu Ma', 'Hao Liang', 'Meiyi Qiang', 'Lexiang Tang', 'Xiaochen Ma', 'Zhen Hao Wong', 'Junbo Niu', 'Chengyu Shen', 'Runming He', 'Bin Cui', 'Wentao Zhang'], 'affiliations': ['Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07527.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ReLIFT: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ RL Ğ¸ SFT', 'desc': 'ReLIFT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ (SFT). ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ RL, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑ ĞµĞ³Ğ¾ Ñ SFT Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ReLIFT ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 5.2 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 13% Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'ReLIFT: Enhancing LLM Reasoning with Smart Training Mix', 'desc': 'ReLIFT is a novel training method that combines reinforcement learning (RL) and supervised fine-tuning (SFT) to improve the reasoning capabilities of large language models (LLMs). It addresses the limitations of traditional RL by interleaving training sessions, allowing the model to learn from high-quality demonstration data when faced with difficult questions. This approach enables the model to acquire new knowledge and reasoning patterns, enhancing its performance on challenging tasks. The results show that ReLIFT significantly outperforms both standalone RL and SFT methods, achieving better results with less training data.'}, 'zh': {'title': 'ReLIFTï¼šå¼ºåŒ–å­¦ä¹ ä¸å¾®è°ƒçš„å®Œç¾ç»“åˆ', 'desc': 'ReLIFTæ˜¯ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿è®­ç»ƒæ¥è§£å†³å¼ºåŒ–å­¦ä¹ çš„å±€é™æ€§ï¼Œä½¿æ¨¡å‹åœ¨é¢å¯¹æŒ‘æˆ˜æ€§é—®é¢˜æ—¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¸æ”¶æ–°çŸ¥è¯†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒReLIFTåœ¨äº”ä¸ªç«äº‰çº§åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†è¶…è¿‡5.2åˆ†ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨ä»…13%çš„ç¤ºèŒƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚æ­¤ç»“æœè¡¨æ˜ï¼ŒReLIFTèƒ½å¤Ÿå…‹æœå¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬å±€é™æ€§ï¼Œå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06658', 'title': 'Self-Adapting Improvement Loops for Robotic Learning', 'url': 'https://huggingface.co/papers/2506.06658', 'abstract': 'SAIL, a self-adapting improvement loop, iteratively enhances a video model using self-produced data and pretrained models, achieving continuous performance improvement on novel robotic tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generative models trained on expert demonstrations have been utilized as performant text-conditioned visual planners for solving robotic tasks. However, generalization to unseen tasks remains a challenge. Whereas improved generalization may be facilitated by leveraging learned prior knowledge from additional pre-collected offline data sources, such as web-scale video datasets, in the era of experience we aim to design agents that can continuously improve in an online manner from self-collected behaviors. In this work we thus propose the Self-Adapting Improvement Loop (SAIL), where an in-domain video model iteratively updates itself on self-produced trajectories, collected through adaptation with an internet-scale pretrained video model, and steadily improves its performance for a specified task of interest. We apply SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks on a real robot arm, and find that performance improvements continuously emerge over multiple iterations for novel tasks initially unseen during original in-domain video model training. Furthermore, we discover that SAIL is surprisingly robust regarding if and how the self-collected experience is filtered, and the quality of the initial in-domain demonstrations. Through adaptation with summarized internet-scale data, and learning through online experience, we thus demonstrate a way to iteratively bootstrap a high-performance video model for solving novel robotic tasks through self-improvement.', 'score': 3, 'issue_id': 4224, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ½Ñ', 'en': 'June 7', 'zh': '6æœˆ7æ—¥'}, 'hash': '0d830115e90c1ee8', 'authors': ['Calvin Luo', 'Zilai Zeng', 'Mingxi Jia', 'Yilun Du', 'Chen Sun'], 'affiliations': ['Brown University', 'Harvard University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06658.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#video', '#agents', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸ĞµÑÑ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğµ', 'desc': 'SAIL (Self-Adapting Improvement Loop) - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. SAIL Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SAIL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Continuous Self-Improvement for Robotic Video Models', 'desc': 'The paper introduces SAIL, a self-adapting improvement loop designed to enhance video models for robotic tasks. It allows models to iteratively learn from their own generated data and adapt using pretrained models, leading to continuous performance gains on new tasks. By leveraging self-collected behaviors and internet-scale video datasets, SAIL enables agents to improve their generalization capabilities over time. The results show that SAIL effectively boosts performance on various tasks, even when initial training data is limited or filtered.'}, 'zh': {'title': 'è‡ªé€‚åº”æ”¹è¿›ï¼ŒæŒç»­æå‡æœºå™¨äººä»»åŠ¡æ€§èƒ½', 'desc': 'SAILï¼ˆè‡ªé€‚åº”æ”¹è¿›å¾ªç¯ï¼‰æ˜¯ä¸€ç§é€šè¿‡è‡ªæˆ‘ç”Ÿæˆæ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿­ä»£æå‡è§†é¢‘æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³æœºå™¨äººä»»åŠ¡ä¸­çš„æ³›åŒ–é—®é¢˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸ŠæŒç»­æ”¹è¿›ã€‚é€šè¿‡åˆ©ç”¨äº’è”ç½‘è§„æ¨¡çš„é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹ï¼ŒSAILèƒ½å¤Ÿä»è‡ªæˆ‘æ”¶é›†çš„è¡Œä¸ºä¸­ä¸æ–­å­¦ä¹ å’Œé€‚åº”ã€‚å®éªŒè¡¨æ˜ï¼ŒSAILåœ¨å¤šæ¬¡è¿­ä»£ä¸­å¯¹æ–°ä»»åŠ¡çš„æ€§èƒ½æœ‰æ˜¾è‘—æå‡ï¼Œä¸”å¯¹è‡ªæˆ‘æ”¶é›†ç»éªŒçš„è¿‡æ»¤å’Œåˆå§‹æ¼”ç¤ºçš„è´¨é‡å…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07848', 'title': 'PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\n  Interaction and Enhancement', 'url': 'https://huggingface.co/papers/2506.07848', 'abstract': 'PolyVivid is a multi-subject video customization framework that uses text-image fusion, 3D-RoPE enhancement, attention-inherited identity injection, and MLLM-based data processing to ensure identity consistency and realistic video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, a multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design a VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose a 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and a clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existing open-source and commercial baselines.', 'score': 2, 'issue_id': 4214, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'e91b69e6dac8c694', 'authors': ['Teng Hu', 'Zhentao Yu', 'Zhengguang Zhou', 'Jiangning Zhang', 'Yuan Zhou', 'Qinglin Lu', 'Ran Yi'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent Hunyuan', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07848.jpg', 'data': {'categories': ['#3d', '#video', '#open_source', '#multimodal', '#games', '#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'PolyVivid: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'PolyVivid - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D-RoPE, Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MLLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. PolyVivid Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLLM Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D-RoPE Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'PolyVivid: Consistent Multi-Subject Video Generation Made Easy!', 'desc': 'PolyVivid is a framework designed for creating videos with multiple subjects while maintaining their identities and interactions. It uses advanced techniques like text-image fusion to accurately link visual identities with textual descriptions. The framework also incorporates a 3D-RoPE enhancement module to improve how text and image data interact, ensuring that identities remain consistent throughout the video. By employing a multi-level language model (MLLM) for data processing, PolyVivid enhances the quality of video generation, achieving better identity fidelity and realism compared to existing models.'}, 'zh': {'title': 'PolyVividï¼šå¤šä¸»ä½“è§†é¢‘å®šåˆ¶çš„æ–°çªç ´', 'desc': 'PolyVividæ˜¯ä¸€ä¸ªå¤šä¸»ä½“è§†é¢‘å®šåˆ¶æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬-å›¾åƒèåˆã€3D-RoPEå¢å¼ºã€æ³¨æ„åŠ›ç»§æ‰¿èº«ä»½æ³¨å…¥å’ŒåŸºäºMLLMçš„æ•°æ®å¤„ç†ï¼Œç¡®ä¿èº«ä»½ä¸€è‡´æ€§å’ŒçœŸå®çš„è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸»ä½“å®šåˆ¶ä¸­ç¼ºä¹ç»†ç²’åº¦å¯æ§æ€§çš„é—®é¢˜ã€‚é€šè¿‡è®¾è®¡åŸºäºVLLMçš„æ–‡æœ¬-å›¾åƒèåˆæ¨¡å—ï¼ŒPolyVividèƒ½å¤Ÿå‡†ç¡®å»ºç«‹ä¸»ä½“å›¾åƒä¸æ–‡æœ¬å®ä½“ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPolyVividåœ¨èº«ä»½ä¿çœŸåº¦ã€è§†é¢‘çœŸå®æ„Ÿå’Œä¸»ä½“å¯¹é½æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œå•†ä¸šåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07564', 'title': 'SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems', 'url': 'https://huggingface.co/papers/2506.07564', 'abstract': "SAFEFLOW secures autonomous agents by enforcing fine-grained information flow control and ensuring reliability in multi-agent, adversarial environments through transactional execution and secure scheduling.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.", 'score': 2, 'issue_id': 4225, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'cf23e35fdd9c8078', 'authors': ['Peiran Li', 'Xinkai Zou', 'Zhuohang Wu', 'Ruifeng Li', 'Shuo Xing', 'Hanwen Zheng', 'Zhikai Hu', 'Yuping Wang', 'Haoxi Li', 'Qin Yuan', 'Yingmo Zhang', 'Zhengzhong Tu'], 'affiliations': ['Carnegie Mellon University', 'Columbia University', 'Meta', 'Texas A&M University', 'UC Irvine', 'UC San Diego', 'University of Michigan', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.07564.jpg', 'data': {'categories': ['#agents', '#benchmark', '#multimodal', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'SAFEFLOW: ĞĞ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´', 'desc': 'SAFEFLOW - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑÑ€ĞµĞ´Ğ¾Ğ¹. SAFEFLOW Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ SAFEFLOW ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ½ĞµĞ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'Building Trustworthy Autonomous Agents with SAFEFLOW', 'desc': 'SAFEFLOW is a new framework designed to enhance the security and reliability of autonomous agents, particularly those using large language models (LLMs) and vision-language models (VLMs). It implements fine-grained information flow control (IFC) to monitor and protect the integrity, provenance, and confidentiality of data exchanged among agents and their environments. By enforcing security labels during reasoning, SAFEFLOW prevents harmful inputs from affecting decision-making processes. Additionally, it introduces mechanisms for transactional execution and secure scheduling to ensure consistency and resilience in multi-agent scenarios, even under adversarial conditions.'}, 'zh': {'title': 'SAFEFLOWï¼šæ„å»ºå¯ä¿¡èµ–çš„è‡ªä¸»ä»£ç†æ¡†æ¶', 'desc': 'SAFEFLOW æ˜¯ä¸€ä¸ªæ–°å‹åè®®æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è‡ªä¸»ä»£ç†æä¾›å®‰å…¨ä¿éšœã€‚å®ƒé€šè¿‡ç»†ç²’åº¦çš„ä¿¡æ¯æµæ§åˆ¶ï¼ˆIFCï¼‰æ¥è¿½è¸ªæ•°æ®çš„æ¥æºã€å®Œæ•´æ€§å’Œæœºå¯†æ€§ï¼Œä»è€Œé˜²æ­¢ä¸å¯ä¿¡æˆ–å¯¹æŠ—æ€§è¾“å…¥å½±å“å†³ç­–ã€‚ä¸ºäº†ç¡®ä¿åœ¨å¤šä»£ç†ç¯å¢ƒä¸­çš„é²æ£’æ€§ï¼ŒSAFEFLOW å¼•å…¥äº†äº‹åŠ¡æ‰§è¡Œã€å†²çªè§£å†³å’Œå®‰å…¨è°ƒåº¦æœºåˆ¶ï¼Œä¿æŒä»£ç†ä¹‹é—´çš„å…¨å±€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒSAFEFLOW è¿˜é€šè¿‡å†™å‰æ—¥å¿—ã€å›æ»šå’Œå®‰å…¨ç¼“å­˜ç­‰æœºåˆ¶å¢å¼ºäº†å¯¹è¿è¡Œæ—¶é”™è¯¯å’Œæ”¿ç­–è¿è§„çš„æŠµå¾¡èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07240', 'title': 'Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path\n  Lengths in LLMs', 'url': 'https://huggingface.co/papers/2506.07240', 'abstract': 'LLMs use progressive encoding and visualization to optimize the length of reasoning processes, enhancing accuracy and reducing inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model\'s internal "thinking" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model\'s planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this "overclocking" method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available.', 'score': 2, 'issue_id': 4211, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': '25ea5847c07c7881', 'authors': ['Roy Eisenstadt', 'Itamar Zimerman', 'Lior Wolf'], 'affiliations': ['IBM Research', 'Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07240.jpg', 'data': {'categories': ['#inference', '#reasoning', '#optimization', '#training'], 'emoji': 'â±ï¸', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¾ÑÑ‹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°. ĞĞ½Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€Ğ°Ñ‚ĞºÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ½Ğ°' ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."}, 'en': {'title': 'Optimizing Reasoning Length for Enhanced LLM Performance', 'desc': 'This paper discusses how large language models (LLMs) can improve their reasoning processes by optimizing the length of their internal thought stages. It highlights the importance of balancing the duration of reasoning: too short may miss complexities, while too long can lead to overthinking and inefficiency. The authors introduce a method for encoding reasoning progress and a visualization tool to better understand how LLMs plan their responses. By adjusting this internal encoding, they demonstrate that LLMs can enhance accuracy and speed during inference, leading to better performance overall.'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†é•¿åº¦ï¼Œæå‡æ¨¡å‹è¡¨ç°', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•ä¼˜åŒ–æ€ç»´é•¿åº¦ï¼Œä»¥æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘æ¨ç†æ—¶é—´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†é˜¶æ®µçš„é•¿åº¦å¯¹ç­”æ¡ˆè´¨é‡æœ‰é‡è¦å½±å“ï¼Œè¿‡çŸ­å¯èƒ½æ— æ³•æ•æ‰ä»»åŠ¡å¤æ‚æ€§ï¼Œè¿‡é•¿åˆ™å¯èƒ½å¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§äº¤äº’å¼è¿›åº¦æ¡å¯è§†åŒ–ï¼Œå¸®åŠ©æ­ç¤ºæ¨¡å‹çš„æ€ç»´åŠ¨æ€ï¼Œå¹¶é€šè¿‡æ“æ§å†…éƒ¨è¿›åº¦ç¼–ç æ¥å‡å°‘å†—ä½™æ­¥éª¤ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§â€œè¶…é¢‘â€æ–¹æ³•æœ‰æ•ˆå‡è½»äº†è¿‡åº¦æ€è€ƒï¼Œæé«˜äº†ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¹¶é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07160', 'title': 'GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization', 'url': 'https://huggingface.co/papers/2506.07160', 'abstract': 'A new reinforcement learning framework, Group Contrastive Policy Optimization (GCPO), enhances geometric reasoning in large language models with judicious auxiliary constructions, outperforming existing methods on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, particularly in mathematical reasoning, amid which geometry problem solving remains a challenging area where auxiliary construction plays a enssential role. Existing approaches either achieve suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring massive computational costs. We posit that reinforcement learning with verifiable reward (e.g., GRPO) offers a promising direction for training smaller models that effectively combine auxiliary construction with robust geometric reasoning. However, directly applying GRPO to geometric reasoning presents fundamental limitations due to its dependence on unconditional rewards, which leads to indiscriminate and counterproductive auxiliary constructions. To address these challenges, we propose Group Contrastive Policy Optimization (GCPO), a novel reinforcement learning framework featuring two key innovations: (1) Group Contrastive Masking, which adaptively provides positive or negative reward signals for auxiliary construction based on contextual utility, and a (2) length reward that promotes longer reasoning chains. Building on GCPO, we develop GeometryZero, a family of affordable-size geometric reasoning models that judiciously determine when to employ auxiliary construction. Our extensive empirical evaluation across popular geometric benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models consistently outperform baselines (e.g. GRPO), achieving an average improvement of 4.29% across all benchmarks.', 'score': 2, 'issue_id': 4212, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': '25d25cf0d2a1b0d5', 'authors': ['Yikun Wang', 'Yibin Wang', 'Dianyi Wang', 'Zimian Peng', 'Qipeng Guo', 'Dacheng Tao', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07160.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Group Contrastive Policy Optimization (GCPO), ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹. GCPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GCPO Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ GeometryZero, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ GeometryZero Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Geometric Reasoning with GCPO in Language Models', 'desc': 'The paper introduces a new reinforcement learning framework called Group Contrastive Policy Optimization (GCPO) that improves geometric reasoning in large language models. It addresses the limitations of existing methods that either underperform or require large models, which are costly to run. GCPO uses innovative techniques like Group Contrastive Masking to provide context-sensitive rewards for auxiliary constructions and a length reward to encourage longer reasoning processes. The results show that models developed using GCPO, named GeometryZero, significantly outperform previous benchmarks in geometric problem-solving tasks.'}, 'zh': {'title': 'ç¾¤ä½“å¯¹æ¯”ç­–ç•¥ä¼˜åŒ–ï¼šæå‡å‡ ä½•æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºç¾¤ä½“å¯¹æ¯”ç­–ç•¥ä¼˜åŒ–ï¼ˆGCPOï¼‰ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‡ ä½•æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚GCPOé€šè¿‡é€‚åº”æ€§åœ°æä¾›æ­£è´Ÿå¥–åŠ±ä¿¡å·ï¼Œä¼˜åŒ–è¾…åŠ©æ„é€ çš„ä½¿ç”¨ï¼Œä»è€Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†GeometryZeroæ¨¡å‹ç³»åˆ—ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°ç»“åˆè¾…åŠ©æ„é€ ä¸ç¨³å¥çš„å‡ ä½•æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeometryZeroåœ¨å¤šä¸ªå‡ ä½•åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æå‡äº†4.29%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04807', 'title': 'MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\n  Recognition with over 97K Categories', 'url': 'https://huggingface.co/papers/2506.04807', 'abstract': 'MegaHan97K, a large-scale dataset for recognizing over 97,000 Chinese characters, addresses the long-tail distribution problem and reveals new challenges in mega-category OCR.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundational to the Chinese language and culture, Chinese characters encompass extraordinarily extensive and ever-expanding categories, with the latest Chinese GB18030-2022 standard containing 87,887 categories. The accurate recognition of this vast number of characters, termed mega-category recognition, presents a formidable yet crucial challenge for cultural heritage preservation and digital applications. Despite significant advances in Optical Character Recognition (OCR), mega-category recognition remains unexplored due to the absence of comprehensive datasets, with the largest existing dataset containing merely 16,151 categories. To bridge this critical gap, we introduce MegaHan97K, a mega-category, large-scale dataset covering an unprecedented 97,455 categories of Chinese characters. Our work offers three major contributions: (1) MegaHan97K is the first dataset to fully support the latest GB18030-2022 standard, providing at least six times more categories than existing datasets; (2) It effectively addresses the long-tail distribution problem by providing balanced samples across all categories through its three distinct subsets: handwritten, historical and synthetic subsets; (3) Comprehensive benchmarking experiments reveal new challenges in mega-category scenarios, including increased storage demands, morphologically similar character recognition, and zero-shot learning difficulties, while also unlocking substantial opportunities for future research. To the best of our knowledge, the MetaHan97K is likely the dataset with the largest classes not only in the field of OCR but may also in the broader domain of pattern recognition. The dataset is available at https://github.com/SCUT-DLVCLab/MegaHan97K.', 'score': 2, 'issue_id': 4212, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'd9e2ab51265af386', 'authors': ['Yuyi Zhang', 'Yongxin Shi', 'Peirong Zhang', 'Yixin Zhao', 'Zhenhua Yang', 'Lianwen Jin'], 'affiliations': ['SCUT-Zhuhai Institute of Modern Industrial Innovation, Zhuhai, China', 'School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04807.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#long_context', '#synthetic'], 'emoji': 'ğŸˆ¶', 'ru': {'title': 'MegaHan97K: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¸ĞµÑ€Ğ¾Ğ³Ğ»Ğ¸Ñ„Ğ¾Ğ²', 'desc': 'MegaHan97K - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ 97 000 ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¸ĞµÑ€Ğ¾Ğ³Ğ»Ğ¸Ñ„Ğ¾Ğ². ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ…Ğ²Ğ¾ÑÑ‚Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR) Ñ Ğ¼ĞµĞ³Ğ°-ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ½Ñ‹Ğµ, Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ñƒ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ĞºĞ°Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğ¾Ğ¼.'}, 'en': {'title': 'Unlocking the Future of Chinese Character Recognition with MegaHan97K', 'desc': 'MegaHan97K is a groundbreaking dataset designed for recognizing over 97,000 Chinese characters, addressing the challenges of mega-category Optical Character Recognition (OCR). This dataset is the first to align with the latest GB18030-2022 standard, significantly expanding the number of character categories available for training models. It tackles the long-tail distribution problem by offering balanced samples across three subsets: handwritten, historical, and synthetic. Additionally, it highlights new challenges in mega-category recognition, such as increased storage needs and difficulties in zero-shot learning, while paving the way for future advancements in the field.'}, 'zh': {'title': 'MegaHan97Kï¼šè¶…å¤§ç±»æ±‰å­—è¯†åˆ«çš„æ–°é‡Œç¨‹ç¢‘', 'desc': 'MegaHan97Kæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯†åˆ«è¶…è¿‡97,000ä¸ªæ±‰å­—ï¼Œè§£å†³äº†é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œå¹¶æ­ç¤ºäº†è¶…å¤§ç±»OCRçš„æ–°æŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†æ”¯æŒæœ€æ–°çš„GB18030-2022æ ‡å‡†ï¼Œæä¾›äº†æ¯”ç°æœ‰æ•°æ®é›†å¤šå…­å€çš„ç±»åˆ«ã€‚é€šè¿‡æ‰‹å†™ã€å†å²å’Œåˆæˆä¸‰ä¸ªå­é›†ï¼ŒMegaHan97Kæœ‰æ•ˆåœ°å¹³è¡¡äº†å„ç±»åˆ«çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜æ­ç¤ºäº†åœ¨è¶…å¤§ç±»åœºæ™¯ä¸­é¢ä¸´çš„æ–°æŒ‘æˆ˜ï¼Œå¦‚å­˜å‚¨éœ€æ±‚å¢åŠ ã€å½¢æ€ç›¸ä¼¼å­—ç¬¦çš„è¯†åˆ«å’Œé›¶æ ·æœ¬å­¦ä¹ çš„å›°éš¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03690', 'title': 'Robust Preference Optimization via Dynamic Target Margins', 'url': 'https://huggingface.co/papers/2506.03690', 'abstract': "The paper introduces Î³-PO, a dynamic target margin preference optimization algorithm that enhances Large Language Models' alignment by adjusting reward margins at the pairwise level, leading to improved performance with minimal impact on training.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose gamma-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, gamma-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an average 4.4\\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, gamma-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at https://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.", 'score': 2, 'issue_id': 4208, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'a3134e659a450a93', 'authors': ['Jie Sun', 'Junkang Wu', 'Jiancan Wu', 'Zhibo Zhu', 'Xingyu Lu', 'Jun Zhou', 'Lintao Ma', 'Xiang Wang'], 'affiliations': ['Ant Group', 'National University of Singapore', 'Shanghai Key Laboratory of Data Science'], 'pdf_title_img': 'assets/pdf/title_img/2506.03690.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#rlhf', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Î³-PO: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Î³-PO - Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ€Ğ¶Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Î³-PO ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°Ñ€Ğ¶Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑˆÑƒĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Direct Preference Optimization (DPO) Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 4.4% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Î³-PO Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ½Ğµ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM.'}, 'en': {'title': 'Enhancing LLM Alignment with Dynamic Margin Optimization', 'desc': 'The paper presents Î³-PO, a novel algorithm designed to optimize the alignment of Large Language Models (LLMs) by dynamically adjusting reward margins at the pairwise level. This method enhances Direct Preference Optimization (DPO) by focusing on high-confidence preference pairs, which helps to mitigate the effects of noisy data. By implementing instance-specific margin calibration, Î³-PO improves model performance while maintaining training efficiency with minimal code modifications. The results demonstrate an average improvement of 4.4% over existing baselines, establishing new benchmarks in LLM alignment.'}, 'zh': {'title': 'Î³-POï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ€§çš„åŠ¨æ€ä¼˜åŒ–ç®—æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºÎ³-POçš„åŠ¨æ€ç›®æ ‡è¾¹é™…åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡è°ƒæ•´æˆå¯¹çš„å¥–åŠ±è¾¹é™…æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½æ€§ã€‚è¯¥ç®—æ³•é€šè¿‡å®ä¾‹ç‰¹å®šçš„è¾¹é™…æ ¡å‡†ï¼Œä¼˜å…ˆè€ƒè™‘é«˜ç½®ä¿¡åº¦çš„æˆå¯¹æ•°æ®ï¼ŒåŒæ—¶æŠ‘åˆ¶æ¨¡ç³Šæˆå¯¹æ•°æ®çš„æ½œåœ¨å™ªå£°ã€‚Î³-POä¸ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•å…¼å®¹ï¼Œèƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å½±å“è®­ç»ƒæ•ˆç‡çš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒÎ³-POåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†4.4%çš„æ€§èƒ½ï¼Œè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08004', 'title': 'Dynamic View Synthesis as an Inverse Problem', 'url': 'https://huggingface.co/papers/2506.08004', 'abstract': 'Efficient dynamic view synthesis is achieved through noise initialization adjustments in a pre-trained video diffusion model using K-order Recursive Noise Representation and Stochastic Latent Modulation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we address dynamic view synthesis from monocular videos as an inverse problem in a training-free setting. By redesigning the noise initialization phase of a pre-trained video diffusion model, we enable high-fidelity dynamic view synthesis without any weight updates or auxiliary modules. We begin by identifying a fundamental obstacle to deterministic inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and resolve it by introducing a novel noise representation, termed K-order Recursive Noise Representation. We derive a closed form expression for this representation, enabling precise and efficient alignment between the VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions resulting from camera motion, we introduce Stochastic Latent Modulation, which performs visibility aware sampling over the latent space to complete occluded regions. Comprehensive experiments demonstrate that dynamic view synthesis can be effectively performed through structured latent manipulation in the noise initialization phase.', 'score': 1, 'issue_id': 4226, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '2a88154b00f5112e', 'authors': ['Hidir Yesiltepe', 'Pinar Yanardag'], 'affiliations': ['Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2506.08004.jpg', 'data': {'categories': ['#diffusion', '#video', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ Ñ„Ğ°Ğ·Ñƒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ K-Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ²Ğ¾Ğµ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ°. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ.'}, 'en': {'title': 'Revolutionizing Dynamic View Synthesis with Noise Innovations', 'desc': 'This paper presents a method for dynamic view synthesis from single videos without needing additional training. It modifies the noise initialization in a pre-trained video diffusion model to achieve high-quality results. The authors introduce K-order Recursive Noise Representation to overcome challenges related to signal-to-noise ratios during inversion. Additionally, they implement Stochastic Latent Modulation to sample latent spaces effectively, allowing for the synthesis of occluded areas as the camera moves.'}, 'zh': {'title': 'é«˜æ•ˆåŠ¨æ€è§†å›¾åˆæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åŠ¨æ€è§†å›¾åˆæˆæ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°åˆå§‹åŒ–è°ƒæ•´ã€‚æˆ‘ä»¬é€šè¿‡é‡æ–°è®¾è®¡å™ªå£°åˆå§‹åŒ–é˜¶æ®µï¼Œè§£å†³äº†ç¡®å®šæ€§åæ¼”ä¸­çš„åŸºæœ¬éšœç¢ï¼Œå®ç°äº†æ— éœ€æƒé‡æ›´æ–°çš„é«˜ä¿çœŸåŠ¨æ€è§†å›¾åˆæˆã€‚å¼•å…¥çš„Ké˜¶é€’å½’å™ªå£°è¡¨ç¤ºæ³•å’Œéšæœºæ½œåœ¨è°ƒåˆ¶æŠ€æœ¯ï¼Œä½¿å¾—åœ¨ç›¸æœºè¿åŠ¨ä¸‹èƒ½å¤Ÿåˆæˆæ–°å¯è§åŒºåŸŸï¼Œå®Œæˆè¢«é®æŒ¡åŒºåŸŸçš„å¡«å……ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡åœ¨å™ªå£°åˆå§‹åŒ–é˜¶æ®µè¿›è¡Œç»“æ„åŒ–æ½œåœ¨æ“ä½œï¼Œå¯ä»¥æœ‰æ•ˆåœ°å®ç°åŠ¨æ€è§†å›¾åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07971', 'title': 'CyberV: Cybernetics for Test-time Scaling in Video Understanding', 'url': 'https://huggingface.co/papers/2506.07971', 'abstract': 'CyberV enhances video multimodal large language models with adaptive feedback mechanisms, improving performance across various benchmarks without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Multimodal Large Language Models (MLLMs) may struggle with understanding long or complex videos due to computational demands at test time, lack of robustness, and limited accuracy, primarily stemming from their feed-forward processing nature. These limitations could be more severe for models with fewer parameters. To address these limitations, we propose a novel framework inspired by cybernetic principles, redesigning video MLLMs as adaptive systems capable of self-monitoring, self-correction, and dynamic resource allocation during inference. Our approach, CyberV, introduces a cybernetic loop consisting of an MLLM Inference System, a Sensor, and a Controller. Specifically, the sensor monitors forward processes of the MLLM and collects intermediate interpretations, such as attention drift, then the controller determines when and how to trigger self-correction and generate feedback to guide the next round. This test-time adaptive scaling framework enhances frozen MLLMs without requiring retraining or additional components. Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0% improvement, achieving performance even comparable to human experts. Furthermore, our method demonstrates consistent gains on general-purpose benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and generalization capabilities in making MLLMs more robust and accurate for dynamic video understanding. The code is released at https://github.com/marinero4972/CyberV.', 'score': 1, 'issue_id': 4222, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '9afc2828137b5533', 'authors': ['Jiahao Meng', 'Shuyang Sun', 'Yue Tan', 'Lu Qi', 'Yunhai Tong', 'Xiangtai Li', 'Longyin Wen'], 'affiliations': ['ByteDance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07971.jpg', 'data': {'categories': ['#video', '#optimization', '#multimodal', '#inference', '#interpretability'], 'emoji': 'ğŸ¥', 'ru': {'title': 'CyberV: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'CyberV - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. CyberV Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´ĞµĞ»Ğ°Ñ Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Adaptive Feedback for Enhanced Video Understanding', 'desc': "CyberV is a framework that enhances video multimodal large language models (MLLMs) by introducing adaptive feedback mechanisms. It allows these models to self-monitor and self-correct during inference, improving their ability to understand complex videos without needing retraining. By implementing a cybernetic loop with a sensor and controller, CyberV dynamically allocates resources and generates feedback based on the model's performance. This approach leads to significant performance boosts on various benchmarks, making MLLMs more robust and accurate in video understanding tasks."}, 'zh': {'title': 'è‡ªé€‚åº”åé¦ˆï¼Œæå‡è§†é¢‘ç†è§£èƒ½åŠ›', 'desc': 'CyberV æ˜¯ä¸€ç§å¢å¼ºè§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”åé¦ˆæœºåˆ¶æ¥æé«˜æ€§èƒ½ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ§åˆ¶è®ºåŸç†ï¼Œå°†è§†é¢‘ MLLM è®¾è®¡ä¸ºèƒ½å¤Ÿè‡ªæˆ‘ç›‘æ§å’Œè‡ªæˆ‘ä¿®æ­£çš„è‡ªé€‚åº”ç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œç³»ç»Ÿé€šè¿‡ä¼ æ„Ÿå™¨ç›‘æµ‹æ¨¡å‹çš„å‰å‘è¿‡ç¨‹ï¼Œå¹¶æ”¶é›†ä¸­é—´è§£é‡Šï¼Œç„¶åæ§åˆ¶å™¨å†³å®šä½•æ—¶è§¦å‘è‡ªæˆ‘ä¿®æ­£å¹¶ç”Ÿæˆåé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCyberV åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07645', 'title': 'Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models', 'url': 'https://huggingface.co/papers/2506.07645', 'abstract': "Character and word-level attacks can effectively perturb large language models' predictions, especially in low-resource languages like Polish, indicating potential vulnerabilities in their safety mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks in recent years. However, their susceptibility to jailbreaks and perturbations necessitates additional evaluations. Many LLMs are multilingual, but safety-related training data contains mainly high-resource languages like English. This can leave them vulnerable to perturbations in low-resource languages such as Polish. We show how surprisingly strong attacks can be cheaply created by altering just a few characters and using a small proxy model for word importance calculation. We find that these character and word-level attacks drastically alter the predictions of different LLMs, suggesting a potential vulnerability that can be used to circumvent their internal safety mechanisms. We validate our attack construction methodology on Polish, a low-resource language, and find potential vulnerabilities of LLMs in this language. Additionally, we show how it can be extended to other languages. We release the created datasets and code for further research.", 'score': 1, 'issue_id': 4215, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'e04bb2316fbf41bc', 'authors': ['Maciej ChrabÄ…szcz', 'Katarzyna Lorenc', 'Karolina Seweryn'], 'affiliations': ['NASK - National Research Institute, Warsaw, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2506.07645.jpg', 'data': {'categories': ['#low_resource', '#dataset', '#multilingual', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ ÑĞ»Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¾Ğ³Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ LLM. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ğº Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼.'}, 'en': {'title': 'Uncovering Vulnerabilities in Language Models: Small Changes, Big Impact!', 'desc': 'This paper explores the vulnerabilities of large language models (LLMs) to character and word-level attacks, particularly in low-resource languages like Polish. The authors demonstrate that minor alterations in text can significantly change the predictions made by these models, highlighting a critical safety concern. They utilize a small proxy model to assess word importance, enabling the creation of effective perturbations with minimal effort. The findings suggest that LLMs require more robust safety mechanisms, especially for languages that lack extensive training data.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„è„†å¼±æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½èµ„æºè¯­è¨€ï¼ˆå¦‚æ³¢å…°è¯­ï¼‰ä¸­çš„è„†å¼±æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç®€å•åœ°æ”¹å˜å‡ ä¸ªå­—ç¬¦ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¹²æ‰°è¿™äº›æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚ç”±äºå®‰å…¨ç›¸å…³çš„è®­ç»ƒæ•°æ®ä¸»è¦é›†ä¸­åœ¨é«˜èµ„æºè¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰ï¼Œè¿™ä½¿å¾—LLMsåœ¨ä½èµ„æºè¯­è¨€ä¸­æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚è®ºæ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å°å‹ä»£ç†æ¨¡å‹è®¡ç®—å•è¯é‡è¦æ€§ï¼Œä»è€Œæ„å»ºå¼ºå¤§çš„æ”»å‡»æ–¹æ³•ï¼Œå¹¶æä¾›äº†ç›¸å…³æ•°æ®é›†å’Œä»£ç ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00258', 'title': 'Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language\n  Models', 'url': 'https://huggingface.co/papers/2506.00258', 'abstract': "Current multimodal large language models often fail to detect implicit reasoning flaws in messy, real-world inputs, but performance can be improved with cautious prompting and clarifying questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) are increasingly deployed in open-ended, real-world environments where inputs are messy, underspecified, and not always trustworthy. Unlike curated benchmarks, these settings frequently involve instructions that refer to missing objects or contradictory facts, rely on ambiguous references, or request infeasible actions. In such cases, success hinges not on task execution alone, but on a model's ability to detect when something is silently wrong. This paper presents a systematic analysis of how current MLLMs handle such implicit reasoning scenarios: cases where the flaw is not explicitly stated but must be inferred from context. Using a curated diagnostic suite spanning four categories of real-world failure modes, we evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently fail to surface hidden issues, even when they possess the necessary perceptual and reasoning skills. Explicit prompting reveals that the underlying capabilities exist but are often suppressed in favor of user compliance. We further show that simple inference-time interventions, such as cautious persona prompting and, in particular, requiring a clarifying question, can dramatically recover performance. Our findings highlight a persistent gap between reasoning competence and behavioral compliance in current MLLMs and suggest practical strategies for making these models more trustworthy in underconstrained environments.", 'score': 1, 'issue_id': 4225, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'ceacd911b8d91616', 'authors': ['Qianqi Yan', 'Hongquan Li', 'Shan Jiang', 'Yang Zhao', 'Xinze Guan', 'Ching-Chen Kuo', 'Xin Eric Wang'], 'affiliations': ['University of California, Santa Cruz', 'eBay'], 'pdf_title_img': 'assets/pdf/title_img/2506.00258.jpg', 'data': {'categories': ['#inference', '#multimodal', '#reasoning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜: Ğ¾Ñ‚ ÑĞ»ĞµĞ¿Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğº ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ´Ğ°Ğ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM.'}, 'en': {'title': 'Enhancing MLLM Trustworthiness through Cautious Prompting', 'desc': "This paper investigates the limitations of multimodal large language models (MLLMs) in recognizing implicit reasoning flaws in complex, real-world scenarios. It highlights that these models often struggle with ambiguous inputs that require inference rather than explicit instructions. The study evaluates six MLLMs and finds that while they have the necessary reasoning capabilities, they often prioritize user compliance over accurate reasoning. The authors propose that using careful prompting and clarifying questions can significantly enhance the models' performance in detecting hidden issues."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸å¯ä¿¡åº¦', 'desc': 'å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚çš„ç°å®è¾“å…¥æ—¶ï¼Œå¸¸å¸¸æ— æ³•å‘ç°éšå«çš„æ¨ç†ç¼ºé™·ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡è°¨æ…çš„æç¤ºå’Œæ¾„æ¸…é—®é¢˜ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬å¯¹å…­ç§MLLMsè¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œå‘ç°å®ƒä»¬åœ¨è¯†åˆ«éšæ€§é—®é¢˜æ—¶ç»å¸¸å¤±è´¥ï¼Œå³ä½¿å®ƒä»¬å…·å¤‡å¿…è¦çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†å½“å‰MLLMsåœ¨æ¨ç†èƒ½åŠ›ä¸è¡Œä¸ºåˆè§„æ€§ä¹‹é—´çš„å·®è·ï¼Œå¹¶æå‡ºäº†æé«˜æ¨¡å‹åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­å¯ä¿¡åº¦çš„å®ç”¨ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23473', 'title': 'EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions', 'url': 'https://huggingface.co/papers/2505.23473', 'abstract': 'EVOREFUSE, an evolutionary algorithm, generates diverse pseudo-malicious instructions to optimize LLM refusal training, improving user experience without compromising safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context.', 'score': 1, 'issue_id': 4212, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': 'dc6dd3911616cbf5', 'authors': ['Xiaorui Wu', 'Xiaofeng Mao', 'Xin Zhang', 'Fei Li', 'Chong Teng', 'Yuxiang Peng', 'Li Zheng', 'Donghong Ji', 'Zhuang Li'], 'affiliations': ['Ant Group', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China', 'School of Computing Technologies, Royal Melbourne Institute of Technology, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2505.23473.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#alignment', '#dataset', '#training', '#data'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ² Ğ˜Ğ˜', 'desc': 'EVOREFUSE - ÑÑ‚Ğ¾ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… EVOREFUSE-TEST Ğ¸ EVOREFUSE-ALIGN, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²Ğ°Ñ… Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚.'}, 'en': {'title': 'EVOREFUSE: Enhancing LLM Refusal Training with Evolutionary Algorithms', 'desc': 'EVOREFUSE is an innovative evolutionary algorithm designed to enhance the training of large language models (LLMs) by generating a variety of pseudo-malicious instructions. These instructions help to optimize refusal training, allowing LLMs to respond more effectively to potentially harmful queries without sacrificing user safety. By employing mutation strategies and recombination, EVOREFUSE explores the instruction space more thoroughly than traditional methods, resulting in a significant increase in the diversity and effectiveness of refusal-inducing prompts. The approach has led to the creation of two new datasets that improve LLM performance, reducing unnecessary refusals while maintaining safety standards.'}, 'zh': {'title': 'EVOREFUSEï¼šä¼˜åŒ–LLMæ‹’ç»è®­ç»ƒçš„è¿›åŒ–ç®—æ³•', 'desc': 'EVOREFUSEæ˜¯ä¸€ç§è¿›åŒ–ç®—æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„ä¼ªæ¶æ„æŒ‡ä»¤ï¼Œä»¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ‹’ç»è®­ç»ƒï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒè€Œä¸å½±å“å®‰å…¨æ€§ã€‚ç°æœ‰çš„æŒ‡ä»¤ç­–åˆ’æ–¹æ³•å¦‚æ‰‹åŠ¨åˆ›å»ºæˆ–é‡å†™æŒ‡ä»¤ï¼Œç¼ºä¹å¯æ‰©å±•æ€§æˆ–æ— æ³•äº§ç”Ÿè¶³å¤Ÿå¤šæ ·å’Œæœ‰æ•ˆçš„æ‹’ç»è¯±å¯¼æç¤ºã€‚EVOREFUSEé€šè¿‡å˜å¼‚ç­–ç•¥å’Œé‡ç»„æ¢ç´¢æŒ‡ä»¤ç©ºé—´ï¼Œè¿­ä»£æ¼”åŒ–ç§å­æŒ‡ä»¤ï¼Œä»¥æœ€å¤§åŒ–LLMæ‹’ç»æ¦‚ç‡çš„è¯æ®ä¸‹ç•Œã€‚ä½¿ç”¨EVOREFUSEï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†æ‹’ç»è§¦å‘ç‡å’Œå“åº”ä¿¡å¿ƒåˆ†æ•°ï¼Œå‡å°‘äº†è¿‡åº¦æ‹’ç»çš„æƒ…å†µã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07833', 'title': 'Improving large language models with concept-aware fine-tuning', 'url': 'https://huggingface.co/papers/2506.07833', 'abstract': 'Concept-Aware Fine-Tuning (CAFT) enhances large language models by enabling multi-token learning in the fine-tuning phase, leading to improved coherent understanding and better performance across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase "ribonucleic acid" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments ("rib", "on", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm', 'score': 0, 'issue_id': 4217, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '33b47665a34ee59a', 'authors': ['Michael K. Chen', 'Xikun Zhang', 'Jiaxing Huang', 'Dacheng Tao'], 'affiliations': ['Nanyang Technological University Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.07833.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#agi', '#open_source', '#survey'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ: Ğ¾Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğº ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Concept-Aware Fine-Tuning (CAFT). CAFT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ CAFT Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½ĞµĞµ Ğ±Ñ‹Ğ»Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Coherent Understanding with Multi-Token Learning', 'desc': 'Concept-Aware Fine-Tuning (CAFT) is a new approach that improves large language models (LLMs) by allowing them to learn from multiple tokens at once during the fine-tuning process. This method addresses the limitations of traditional next-token prediction, which often leads to a fragmented understanding of language. By enabling LLMs to grasp phrases as coherent units rather than separate parts, CAFT enhances their ability to understand complex concepts. Our experiments show that CAFT significantly outperforms standard fine-tuning methods in various tasks, making it a valuable advancement for AI development.'}, 'zh': {'title': 'æ¦‚å¿µæ„ŸçŸ¥å¾®è°ƒï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç†è§£åŠ›', 'desc': 'æ¦‚å¿µæ„ŸçŸ¥å¾®è°ƒï¼ˆCAFTï¼‰é€šè¿‡åœ¨å¾®è°ƒé˜¶æ®µå®ç°å¤šæ ‡è®°å­¦ä¹ ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†å¯¹å„ç§ä»»åŠ¡çš„ç†è§£å’Œè¡¨ç°ã€‚ä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ–¹æ³•é™åˆ¶äº†æ¨¡å‹å½¢æˆè¿è´¯é«˜å±‚æ¬¡æ¦‚å¿µçš„èƒ½åŠ›ï¼Œå¯¼è‡´ç†è§£å’Œæ¨ç†çš„éšœç¢ã€‚CAFTæ–¹æ³•å…è®¸æ¨¡å‹å­¦ä¹ è·¨è¶Šå¤šä¸ªæ ‡è®°çš„åºåˆ—ï¼Œä¿ƒè¿›äº†æ›´å¼ºçš„æ¦‚å¿µæ„ŸçŸ¥å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCAFTåœ¨æ–‡æœ¬æ‘˜è¦å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ï¼ˆå¦‚æ–°è›‹ç™½è´¨è®¾è®¡ï¼‰ä¸Šéƒ½æœ‰æ˜¾è‘—æ”¹å–„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06905', 'title': 'Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering', 'url': 'https://huggingface.co/papers/2506.06905', 'abstract': 'A meta-learning approach with soft prompts and an attention-mapper module improves few-shot capabilities in LMMs, outperforming ICL and related methods in visual question answering tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks.', 'score': 0, 'issue_id': 4220, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ½Ñ', 'en': 'June 7', 'zh': '6æœˆ7æ—¥'}, 'hash': '63ff86432862b604', 'authors': ['Akash Gupta', 'Amos Storkey', 'Mirella Lapata'], 'affiliations': ['School of Informatics, University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.06905.jpg', 'data': {'categories': ['#cv', '#optimization', '#benchmark', '#reasoning', '#architecture', '#training', '#multimodal', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¼ÑĞ³ĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ few-shot Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ÑĞ³ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICL) Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Few-Shot Learning in LMMs with Meta-Learning and Soft Prompts', 'desc': 'This paper presents a meta-learning strategy that enhances few-shot learning in Large Multimodal Models (LMMs) by utilizing soft prompts and an attention-mapper module. The authors argue that traditional in-context learning (ICL) struggles with smaller LMMs due to excessive information from image embeddings, which can hinder performance. Their proposed method distills relevant image features into fixed soft prompts that can be adapted during testing with minimal examples. The results demonstrate that this approach significantly outperforms ICL and other prompt-tuning methods in visual question answering tasks, even when faced with image variations.'}, 'zh': {'title': 'å…ƒå­¦ä¹ æå‡å°‘æ ·æœ¬èƒ½åŠ›ï¼Œè¶…è¶Šä¼ ç»Ÿæ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å…ƒå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è½¯æç¤ºå’Œæ³¨æ„åŠ›æ˜ å°„æ¨¡å—æ¥æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨å°å‹LMMsä¸­çš„è¡¨ç°ä¸ç¨³å®šï¼Œä¸”éšç€ç¤ºä¾‹æ•°é‡çš„å¢åŠ å¹¶ä¸æ€»æ˜¯æé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç»„å›ºå®šçš„è½¯æç¤ºï¼Œè¿™äº›æç¤ºä»ä¸ä»»åŠ¡ç›¸å…³çš„å›¾åƒç‰¹å¾ä¸­æå–ï¼Œå¹¶å¯ä»¥åœ¨æµ‹è¯•æ—¶é€šè¿‡å°‘é‡ç¤ºä¾‹è¿›è¡Œè°ƒæ•´ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ä¼˜äºICLå’Œç›¸å…³çš„æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä½æ•°æ®ç¯å¢ƒä¸‹å®ç°æ›´å¥½çš„ä»»åŠ¡é€‚åº”å’Œæ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05904', 'title': 'Proactive Assistant Dialogue Generation from Streaming Egocentric Videos', 'url': 'https://huggingface.co/papers/2506.05904', 'abstract': 'A framework provides automated data synthesis, evaluation metrics, and an end-to-end model for real-time, proactive conversational AI task guidance using streaming video inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in conversational AI have been substantial, but developing real-time systems for perceptual task guidance remains challenging. These systems must provide interactive, proactive assistance based on streaming visual inputs, yet their development is constrained by the costly and labor-intensive process of data collection and system evaluation. To address these limitations, we present a comprehensive framework with three key contributions. First, we introduce a novel data curation pipeline that synthesizes dialogues from annotated egocentric videos, resulting in \\dataset, a large-scale synthetic dialogue dataset spanning multiple domains. Second, we develop a suite of automatic evaluation metrics, validated through extensive human studies. Third, we propose an end-to-end model that processes streaming video inputs to generate contextually appropriate responses, incorporating novel techniques for handling data imbalance and long-duration videos. This work lays the foundation for developing real-time, proactive AI assistants capable of guiding users through diverse tasks. Project page: https://pro-assist.github.io/', 'score': 0, 'issue_id': 4214, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'a29f2576c90935c3', 'authors': ['Yichi Zhang', 'Xin Luna Dong', 'Zhaojiang Lin', 'Andrea Madotto', 'Anuj Kumar', 'Babak Damavandi', 'Joyce Chai', 'Seungwhan Moon'], 'affiliations': ['Meta', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2506.05904.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#long_context', '#data', '#synthetic', '#cv', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸Ğ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Empowering Real-Time AI Guidance with Streaming Video Insights', 'desc': "This paper presents a framework for creating real-time conversational AI that can guide users through tasks using live video feeds. It introduces a new data curation pipeline that generates a large synthetic dialogue dataset from annotated egocentric videos, which helps in training the AI. The authors also propose automatic evaluation metrics that have been validated through human studies to assess the system's performance. Finally, they develop an end-to-end model that effectively processes streaming video inputs to provide timely and relevant responses, addressing challenges like data imbalance and long video durations."}, 'zh': {'title': 'å®æ—¶ä¸»åŠ¨å¯¹è¯AIä»»åŠ¡æŒ‡å¯¼çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–æ•°æ®åˆæˆã€è¯„ä¼°æŒ‡æ ‡å’Œç«¯åˆ°ç«¯æ¨¡å‹ï¼Œä»¥å®ç°åŸºäºæµåª’ä½“è§†é¢‘è¾“å…¥çš„å®æ—¶ä¸»åŠ¨å¯¹è¯AIä»»åŠ¡æŒ‡å¯¼ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œå¼€å‘äº†ä¸€ç§æ–°é¢–çš„æ•°æ®ç­–åˆ’ç®¡é“ï¼Œä»æ ‡æ³¨çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­åˆæˆå¯¹è¯ï¼Œç”Ÿæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆå¯¹è¯æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€å¥—è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„äººç±»ç ”ç©¶è¿›è¡Œäº†éªŒè¯ã€‚æœ€åï¼Œæ„å»ºäº†ä¸€ä¸ªå¤„ç†æµåª’ä½“è§†é¢‘è¾“å…¥çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸Šä¸‹æ–‡é€‚å½“çš„å“åº”ï¼Œè§£å†³äº†æ•°æ®ä¸å¹³è¡¡å’Œé•¿æ—¶è§†é¢‘å¤„ç†çš„é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03231', 'title': 'NetPress: Dynamically Generated LLM Benchmarks for Network Applications', 'url': 'https://huggingface.co/papers/2506.03231', 'abstract': 'NetPress generates dynamic benchmarks for evaluating large language model agents in network operations, providing realistic tests across correctness, safety, and latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress.', 'score': 0, 'issue_id': 4228, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '42adee861aecb23e', 'authors': ['Yajie Zhou', 'Jiajun Ruan', 'Eric S. Wang', 'Sadjad Fouladi', 'Francis Y. Yan', 'Kevin Hsieh', 'Zaoxing Liu'], 'affiliations': ['Microsoft Research', 'University of Illinois Urbana-Champaign', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.03231.jpg', 'data': {'categories': ['#agents', '#survey', '#optimization', '#benchmark'], 'emoji': 'ğŸŒ', 'ru': {'title': 'NetPress: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑÑ…', 'desc': 'NetPress - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. NetPress Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑĞµÑ‚ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑƒĞ¿ÑƒÑĞºĞ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ².'}, 'en': {'title': 'Dynamic Benchmarking for Real-World LLM Evaluation', 'desc': 'NetPress is a framework designed to create dynamic benchmarks for assessing large language model (LLM) agents in network operations. It addresses the limitations of traditional static datasets by allowing users to generate millions of queries in real-time, tailored to specific benchmark configurations. The framework incorporates a unified abstraction of state and action, enabling comprehensive evaluations that consider correctness, safety, and latency. By integrating with network emulators, NetPress provides realistic feedback, revealing nuanced agent behaviors that static benchmarks often overlook, thus enhancing the reliability of LLMs in practical applications.'}, 'zh': {'title': 'NetPressï¼šåŠ¨æ€è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†ç”Ÿæˆå·¥å…·', 'desc': 'NetPressæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–åŸºå‡†ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œåº”ç”¨ä¸­çš„è¡¨ç°ã€‚å®ƒé€šè¿‡ç»Ÿä¸€çš„çŠ¶æ€å’ŒåŠ¨ä½œæŠ½è±¡ï¼ŒåŠ¨æ€ç”Ÿæˆå¤šæ ·åŒ–çš„æŸ¥è¯¢é›†åŠå…¶å¯¹åº”çš„çœŸå®å€¼ã€‚ç”¨æˆ·å¯ä»¥åœ¨è¿è¡Œæ—¶æŒ‡å®šåŸºå‡†é…ç½®ï¼Œå®æ—¶ç”Ÿæˆæ•°ç™¾ä¸‡ä¸ªæŸ¥è¯¢ã€‚NetPressè¿˜ä¸ç½‘ç»œä»¿çœŸå™¨é›†æˆï¼Œæä¾›çœŸå®ç¯å¢ƒåé¦ˆï¼Œæ”¯æŒåœ¨æ­£ç¡®æ€§ã€å®‰å…¨æ€§å’Œå»¶è¿Ÿç­‰æ–¹é¢çš„å…¨é¢è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09600', 'title': 'Effective Red-Teaming of Policy-Adherent Agents', 'url': 'https://huggingface.co/papers/2506.09600', 'abstract': "CRAFT, a multi-agent system using policy-aware persuasive strategies, challenges policy-adherent LLM-based agents in customer service to assess and improve their robustness against adversarial attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks", 'score': 32, 'issue_id': 4307, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '3de0c796f8d5a171', 'authors': ['Itay Nakash', 'George Kour', 'Koren Lazar', 'Matan Vetzler', 'Guy Uziel', 'Ateret Anaby-Tavor'], 'affiliations': ['IBM'], 'pdf_title_img': 'assets/pdf/title_img/2506.09600.jpg', 'data': {'categories': ['#security', '#benchmark', '#agents'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ›Ğ›Ğœ-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CRAFT - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ›Ğ›Ğœ-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ„ĞµÑ€Ğµ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ³Ñ€Ğ¾Ğ·, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰ÑƒÑÑÑ Ğ½Ğ° Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ°Ñ…, Ğ¿Ñ‹Ñ‚Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ…. CRAFT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DAN-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº tau-break Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Strengthening Policy-Adherent Agents Against Adversarial Manipulation', 'desc': 'The paper introduces CRAFT, a multi-agent system designed to test and enhance the resilience of policy-adherent language model (LLM) agents in customer service against adversarial attacks. It highlights the challenge of ensuring these agents follow strict policies while still providing helpful interactions. The authors propose a new threat model that focuses on adversarial users who attempt to exploit these agents for personal gain. Additionally, they present tau-break, a benchmark for evaluating agent robustness, and discuss various defense strategies, revealing the need for more robust protections against manipulation.'}, 'zh': {'title': 'CRAFTï¼šæå‡æ”¿ç­–éµå¾ªä»£ç†çš„é²æ£’æ€§', 'desc': 'CRAFTæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä½¿ç”¨æ”¿ç­–æ„è¯†çš„åŠè¯´ç­–ç•¥ï¼Œæ—¨åœ¨æŒ‘æˆ˜éµå¾ªæ”¿ç­–çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å®¢æˆ·æœåŠ¡ä»£ç†ï¼Œä»¥è¯„ä¼°å’Œæé«˜å…¶å¯¹å¯¹æŠ—æ€§æ”»å‡»çš„é²æ£’æ€§ã€‚éšç€ä»»åŠ¡å¯¼å‘çš„LLMä»£ç†åœ¨ä¸¥æ ¼æ”¿ç­–é¢†åŸŸçš„åº”ç”¨å¢åŠ ï¼Œç¡®ä¿ä»£ç†å§‹ç»ˆéµå¾ªè¿™äº›è§„åˆ™å¹¶é€‚å½“åœ°æ‹’ç»è¿è§„è¯·æ±‚å˜å¾—è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¨èƒæ¨¡å‹ï¼Œä¸“æ³¨äºåˆ©ç”¨éµå¾ªæ”¿ç­–çš„ä»£ç†è¿›è¡Œä¸ªäººåˆ©ç›Šçš„å¯¹æŠ—æ€§ç”¨æˆ·ã€‚CRAFTé€šè¿‡åˆ©ç”¨æ”¿ç­–æ„è¯†çš„åŠè¯´ç­–ç•¥ï¼Œåœ¨å®¢æˆ·æœåŠ¡åœºæ™¯ä¸­æœ‰æ•ˆåœ°å‰Šå¼±äº†éµå¾ªæ”¿ç­–çš„ä»£ç†ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è¶Šç‹±æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11924', 'title': 'Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation', 'url': 'https://huggingface.co/papers/2506.11924', 'abstract': 'A diffusion-based framework generates aligned novel views of images and geometry using warping-and-inpainting with cross-modal attention distillation and proximity-based mesh conditioning, achieving high-fidelity synthesis and 3D completion.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.', 'score': 27, 'issue_id': 4305, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': 'bf8d340f29d7ad95', 'authors': ['Min-Seop Kwak', 'Junho Kim', 'Sangdoo Yun', 'Dongyoon Han', 'Taekyoung Kim', 'Seungryong Kim', 'Jin-Hwa Kim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab', 'SNU AIIS'], 'pdf_title_img': 'assets/pdf/title_img/2506.11924.jpg', 'data': {'categories': ['#cv', '#3d', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ»Ğ¸Ğ·Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½.'}, 'en': {'title': 'High-Fidelity 3D View Synthesis through Diffusion and Attention', 'desc': 'This paper presents a diffusion-based framework for generating new views of images and their corresponding 3D geometry. It uses a technique called warping-and-inpainting, which allows for the synthesis of images and geometry without needing a lot of pre-existing data. The method incorporates cross-modal attention distillation to ensure that the generated images and geometries are well-aligned, enhancing the quality of the output. Additionally, it employs proximity-based mesh conditioning to improve the accuracy of the generated 3D structures, resulting in high-fidelity synthesis and completion of 3D scenes.'}, 'zh': {'title': 'åŸºäºæ‰©æ•£çš„é«˜ä¿çœŸå›¾åƒä¸å‡ ä½•ä½“ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œé€šè¿‡æ‰­æ›²å’Œä¿®å¤çš„æ–¹æ³•ç”Ÿæˆå¯¹é½çš„æ–°è§†å›¾å›¾åƒå’Œå‡ ä½•ä½“ã€‚ä¸ä»¥å¾€éœ€è¦å¯†é›†å§¿æ€å›¾åƒæˆ–é™åˆ¶äºç‰¹å®šé¢†åŸŸè§†å›¾çš„ç”Ÿæˆæ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç°æˆçš„å‡ ä½•é¢„æµ‹å™¨æ¥é¢„æµ‹å‚è€ƒå›¾åƒçš„éƒ¨åˆ†å‡ ä½•ä½“ï¼Œå¹¶å°†æ–°è§†å›¾åˆæˆè§†ä¸ºå›¾åƒå’Œå‡ ä½•ä½“çš„ä¿®å¤ä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„å›¾åƒå’Œå‡ ä½•ä½“ä¹‹é—´çš„å‡†ç¡®å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€æ³¨æ„åŠ›è’¸é¦ï¼Œå°†å›¾åƒæ‰©æ•£åˆ†æ”¯çš„æ³¨æ„åŠ›å›¾æ³¨å…¥åˆ°å¹¶è¡Œçš„å‡ ä½•æ‰©æ•£åˆ†æ”¯ä¸­ã€‚é€šè¿‡è¿™ç§å¤šä»»åŠ¡æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†å‡ ä½•ç¨³å¥çš„å›¾åƒåˆæˆå’Œæ¸…æ™°çš„å‡ ä½•é¢„æµ‹ï¼Œæœ€ç»ˆåœ¨æœªè§åœºæ™¯ä¸­å®ç°äº†é«˜ä¿çœŸåº¦çš„è§†å›¾åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10892', 'title': 'The Diffusion Duality', 'url': 'https://huggingface.co/papers/2506.10892', 'abstract': 'Duo improves uniform-state discrete diffusion models by transferring techniques from Gaussian diffusion, enhancing training speed and enabling fast few-step text generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: http://s-sahoo.github.io/duo', 'score': 23, 'issue_id': 4305, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '974b708b2e781af0', 'authors': ['Subham Sekhar Sahoo', 'Justin Deschenaux', 'Aaron Gokaslan', 'Guanghan Wang', 'Justin Chiu', 'Volodymyr Kuleshov'], 'affiliations': ['Computer and Information Science, Cornell Tech, NYC, USA', 'School of Computer and Communication Sciences, EPFL Lausanne, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2506.10892.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#optimization', '#open_source', '#diffusion'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Duo: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ Duo ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ° ÑÑ‡ĞµÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸. Duo Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½Ğ° Ğ´Ğ²Ğ° Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°.'}, 'en': {'title': 'Duo: Accelerating Diffusion Models for Fast Text Generation', 'desc': 'This paper presents Duo, a method that enhances uniform-state discrete diffusion models by incorporating techniques from Gaussian diffusion. The authors introduce a curriculum learning strategy that accelerates training speed by reducing variance, allowing models to outperform autoregressive models in zero-shot perplexity on several benchmarks. Additionally, they propose Discrete Consistency Distillation, which enables faster few-step text generation by adapting consistency distillation for discrete settings. Overall, Duo significantly improves the efficiency and performance of diffusion language models.'}, 'zh': {'title': 'Duoï¼šåŠ é€Ÿæ–‡æœ¬ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDuoçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°†é«˜æ–¯æ‰©æ•£çš„æŠ€æœ¯è½¬ç§»åˆ°å‡åŒ€çŠ¶æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œæé«˜è®­ç»ƒé€Ÿåº¦å’Œå¿«é€Ÿæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚å‡åŒ€çŠ¶æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹å…·æœ‰è‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ï¼Œä½†é€šå¸¸åœ¨æ€§èƒ½ä¸Šä¸åŠè‡ªå›å½’æ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹ã€‚Duoé€šè¿‡å¼•å…¥åŸºäºé«˜æ–¯è¿‡ç¨‹çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†è‡ªå›å½’æ¨¡å‹ã€‚è¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†ç¦»æ•£ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œä½¿å¾—æ‰©æ•£è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå®ç°å¿«é€Ÿçš„å°‘æ­¥ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10128', 'title': 'ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual\n  Perception in VLMs', 'url': 'https://huggingface.co/papers/2506.10128', 'abstract': 'ViCrit, an RL task for fine-tuning VLMs, improves visual perception by training models to detect subtle hallucinations in image captions, with gains transferable to various visual domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in vision-language models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from a 200-word captions, we inject a single, subtle visual description error-altering a few words on objects, attributes, counts, or spatial relations-and task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing a binary, exact-match reward that is easy to compute and unambiguous. Models trained with the ViCrit Task exhibit substantial gains across a variety of VL benchmarks. Crucially, the improvements transfer beyond natural-image training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce ViCrit-Bench, a category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs.', 'score': 13, 'issue_id': 4309, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '5045b62235ae5509', 'authors': ['Xiyao Wang', 'Zhengyuan Yang', 'Chao Feng', 'Yongyuan Liang', 'Yuhang Zhou', 'Xiaoyu Liu', 'Ziyi Zang', 'Ming Li', 'Chung-Ching Lin', 'Kevin Lin', 'Linjie Li', 'Furong Huang', 'Lijuan Wang'], 'affiliations': ['Cardiff University', 'Microsoft', 'University of Maryland, College Park', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2506.10128.jpg', 'data': {'categories': ['#rl', '#benchmark', '#hallucinations', '#cv', '#transfer_learning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ViCrit: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ViCrit - Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ViCrit Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ… Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ViCrit, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Enhancing Visual Perception in VLMs with ViCrit', 'desc': "ViCrit is a reinforcement learning task designed to enhance the visual perception capabilities of vision-language models (VLMs) by training them to identify subtle hallucinations in image captions. The task involves injecting minor visual description errors into human-written captions and challenging the model to locate these errors based on the corresponding images. This approach not only maintains the complexity of visual perception but also provides a clear and straightforward reward system for the model's performance. The results show that models trained with ViCrit achieve significant improvements across various visual benchmarks, indicating that this method fosters a deeper understanding of visual content rather than mere memorization."}, 'zh': {'title': 'é€šè¿‡ViCritæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›', 'desc': 'ViCritæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œæ—¨åœ¨å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹æ£€æµ‹å›¾åƒæ ‡é¢˜ä¸­çš„ç»†å¾®å¹»è§‰æ¥æé«˜è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨äººå·¥æ’°å†™çš„å›¾åƒæ ‡é¢˜ä¸­æ³¨å…¥è½»å¾®çš„è§†è§‰æè¿°é”™è¯¯ï¼Œè¦æ±‚æ¨¡å‹è¯†åˆ«è¿™äº›é”™è¯¯ï¼Œä»è€Œä¿æŒæ„ŸçŸ¥çš„éš¾åº¦ã€‚ç»è¿‡ViCritä»»åŠ¡è®­ç»ƒçš„æ¨¡å‹åœ¨å„ç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œä¸”è¿™äº›æ”¹è¿›ä¸ä»…é™äºè‡ªç„¶å›¾åƒæ•°æ®ï¼Œè¿˜èƒ½è¿ç§»åˆ°æŠ½è±¡å›¾åƒæ¨ç†å’Œè§†è§‰æ•°å­¦ç­‰é¢†åŸŸã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç»†è‡´çš„å¹»è§‰æ‰¹è¯„æ˜¯ä¸€ç§æœ‰æ•ˆä¸”å¯æ¨å¹¿çš„ç›®æ ‡ï¼Œæœ‰åŠ©äºå¢å¼ºVLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11928', 'title': 'LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?', 'url': 'https://huggingface.co/papers/2506.11928', 'abstract': 'LLMs perform well on implementation-heavy competitive programming problems but struggle with nuanced algorithmic reasoning, as highlighted by LiveCodeBench Pro.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.', 'score': 11, 'issue_id': 4305, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '4d3f2213d58dd8dd', 'authors': ['Zihan Zheng', 'Zerui Cheng', 'Zeyu Shen', 'Shang Zhou', 'Kaiyuan Liu', 'Hansen He', 'Dongruixuan Li', 'Stanley Wei', 'Hangyi Hao', 'Jianzhu Yao', 'Peiyao Sheng', 'Zixuan Wang', 'Wenhao Chai', 'Aleksandra Korolova', 'Peter Henderson', 'Sanjeev Arora', 'Pramod Viswanath', 'Jingbo Shang', 'Saining Xie'], 'affiliations': ['Canyon Crest Academy', 'McGill University', 'New York University', 'Princeton University', 'Sentient Foundation', 'University of California San Diego', 'University of Washington', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2506.11928.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#dataset', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'LLM Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: ÑĞ¸Ğ»Ğ° Ğ² Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ÑŒ Ğ² Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LiveCodeBench Pro, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· Codeforces, ICPC Ğ¸ IOI. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 53% pass@1 Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ 0% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ LLM Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-Ğ³Ñ€Ğ¾ÑÑĞ¼ĞµĞ¹ÑÑ‚ĞµÑ€Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Bridging the Gap: LLMs vs. Human Algorithmic Mastery', 'desc': 'This paper evaluates the performance of large language models (LLMs) in competitive programming using a new benchmark called LiveCodeBench Pro. It reveals that while LLMs excel in implementation-heavy tasks, they struggle with complex algorithmic reasoning and nuanced problem-solving. The study shows that even the best LLMs achieve only 53% success on medium-difficulty problems and none on hard problems, indicating a significant gap compared to human experts. The findings suggest that LLMs rely more on implementation accuracy and external tools rather than advanced reasoning skills, highlighting areas for future improvement in AI-driven coding solutions.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç®—æ³•æ¨ç†ä¸­çš„å±€é™æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç«äº‰ç¼–ç¨‹ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å®ç°å¯†é›†å‹é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚ç®—æ³•æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç ”ç©¶å¼•å…¥äº†LiveCodeBench Proï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºCodeforcesã€ICPCå’ŒIOIçš„é—®é¢˜åŸºå‡†ï¼Œæ—¨åœ¨å‡å°‘æ•°æ®æ±¡æŸ“çš„å¯èƒ½æ€§ã€‚é€šè¿‡å¯¹æ¨¡å‹ç”Ÿæˆçš„æäº¤è¿›è¡Œé€è¡Œåˆ†æï¼Œå‘ç°å½“å‰çš„å‰æ²¿æ¨¡å‹åœ¨ä¸­ç­‰éš¾åº¦é—®é¢˜ä¸Šçš„é€šè¿‡ç‡ä»…ä¸º53%ï¼Œè€Œåœ¨å›°éš¾é—®é¢˜ä¸Šåˆ™ä¸º0%ã€‚è¿™è¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨å®ç°ç²¾åº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚çš„ç®—æ³•æ¨ç†å’Œæ¡ˆä¾‹åˆ†æä¸­ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08989', 'title': 'SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement\n  Learning for LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.08989', 'abstract': "A self-aware problem synthesis framework that leverages model weaknesses enhances reinforcement learning with verifiable rewards, improving large language model performance on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.", 'score': 8, 'issue_id': 4311, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '332357cc97416117', 'authors': ['Xiao Liang', 'Zhong-Zhi Li', 'Yeyun Gong', 'Yang Wang', 'Hengyuan Zhang', 'Yelong Shen', 'Ying Nian Wu', 'Weizhu Chen'], 'affiliations': ['Microsoft', 'School of Artificial Intelligence, Chinese Academy of Sciences', 'Tsinghua University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.08989.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Self-aware Weakness-driven problem Synthesis (SwS). Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. SwS Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B Ğ¸ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Empowering Models by Learning from Their Weaknesses', 'desc': 'This paper presents a Self-aware Weakness-driven problem Synthesis framework (SwS) that enhances reinforcement learning for large language models (LLMs) by focusing on their weaknesses. The framework identifies specific areas where the model struggles and generates new problems to help the model improve in those areas. By systematically augmenting the training set with these tailored problems, the model can better learn and generalize its reasoning capabilities. The results show significant performance improvements on reasoning tasks, demonstrating the effectiveness of leveraging model weaknesses for training.'}, 'zh': {'title': 'è‡ªæˆ‘æ„è¯†é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜åˆæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘æ„è¯†çš„å¼±ç‚¹é©±åŠ¨é—®é¢˜åˆæˆæ¡†æ¶ï¼ˆSwSï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«æ¨¡å‹çš„ä¸è¶³æ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„å¯éªŒè¯å¥–åŠ±ã€‚è¯¥æ¡†æ¶ç³»ç»Ÿåœ°åˆ†ææ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åå¤å¤±è´¥çš„é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è¿™äº›å¤±è´¥æ¡ˆä¾‹æç‚¼æ ¸å¿ƒæ¦‚å¿µï¼Œåˆæˆæ–°çš„é—®é¢˜ä»¥åŠ å¼ºæ¨¡å‹çš„è–„å¼±ç¯èŠ‚ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨åç»­çš„å¢å¼ºè®­ç»ƒä¸­é›†ä¸­ç²¾åŠ›å…‹æœè‡ªèº«çš„å¼±ç‚¹ï¼Œè€Œæ— éœ€ä¾èµ–å¤–éƒ¨çŸ¥è¯†è’¸é¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹³å‡æå‡è¾¾10.0%å’Œ7.7%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11930', 'title': 'Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback', 'url': 'https://huggingface.co/papers/2506.11930', 'abstract': "LLMs show resistance to feedback, termed feedback friction, even under ideal conditions, and sampling-based strategies only partially mitigate this issue.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform a rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement.", 'score': 7, 'issue_id': 4314, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '0fc67d5ee77483c7', 'authors': ['Dongwei Jiang', 'Alvin Zhang', 'Andrew Wang', 'Nicholas Andrews', 'Daniel Khashabi'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.11930.jpg', 'data': {'categories': ['#training', '#hallucinations', '#alignment', '#reasoning', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ±Ğ¾Ñ€ÑŒĞ±Ğ° Ñ 'Ñ‚Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸' Ğ² LLM", 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¾Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ 'Ñ‚Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸', Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Claude 3.7. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ»Ğ¸ÑˆÑŒ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑĞ¼ÑĞ³Ñ‡Ğ°ÑÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM."}, 'en': {'title': 'Unpacking Feedback Friction in LLMs', 'desc': "This paper investigates the phenomenon of feedback friction in large language models (LLMs), where these models struggle to effectively incorporate external feedback even in ideal conditions. The authors conduct controlled experiments where a solver model receives targeted feedback from a feedback generator based on near-complete ground-truth answers. Despite this optimal setup, the models consistently show resistance to changing their incorrect responses, indicating a significant limitation in their learning process. The study also explores various strategies to mitigate this issue, such as sampling-based methods, but finds that these approaches only partially improve performance, highlighting the need for further research into enhancing LLMs' self-improvement capabilities."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„åé¦ˆæ‘©æ“¦é—®é¢˜', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¥æ”¶å¤–éƒ¨åé¦ˆæ—¶çš„è¡¨ç°ï¼Œå‘ç°å®ƒä»¬å­˜åœ¨ä¸€ç§ç§°ä¸ºåé¦ˆæ‘©æ“¦ï¼ˆfeedback frictionï¼‰çš„ç°è±¡ï¼Œå³ä½¿åœ¨ç†æƒ³æ¡ä»¶ä¸‹ä¹Ÿéš¾ä»¥æœ‰æ•ˆæ•´åˆåé¦ˆã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ§åˆ¶å®éªŒç¯å¢ƒï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ã€çŸ¥è¯†æ¨ç†å’Œç§‘å­¦æ¨ç†ç­‰å¤šç§ä»»åŠ¡ä¸­çš„åé¦ˆæ•´åˆèƒ½åŠ›ã€‚å°½ç®¡æä¾›äº†æ¥è¿‘å®Œç¾çš„åé¦ˆï¼Œæ¨¡å‹ä»ç„¶è¡¨ç°å‡ºå¯¹åé¦ˆçš„æŠµæŠ—ï¼Œæœªèƒ½æ˜¾è‘—æ”¹å–„å…¶é”™è¯¯ç­”æ¡ˆã€‚é€šè¿‡å®éªŒä¸åŒçš„é‡‡æ ·ç­–ç•¥ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›ç­–ç•¥è™½ç„¶æœ‰æ‰€æ”¹å–„ï¼Œä½†ä»æœªèƒ½ä½¿æ¨¡å‹è¾¾åˆ°é¢„æœŸçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11886', 'title': 'Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache', 'url': 'https://huggingface.co/papers/2506.11886', 'abstract': 'FourierAttention is a training-free framework that enhances memory efficiency in Large Language Models by compressing long-context-insensitive transformer head dimensions using orthogonal Fourier bases, while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.', 'score': 6, 'issue_id': 4310, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '6e5e8424bfbe53cc', 'authors': ['Xiaoran Liu', 'Siyang He', 'Qiqi Wang', 'Ruixiao Li', 'Yuerong Song', 'Zhigeng Liu', 'Linlin Li', 'Qun Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['Huawei Noahs Ark Lab', 'School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.11886.jpg', 'data': {'categories': ['#architecture', '#optimization', '#inference', '#long_context', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'FourierAttention - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ½ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¼Ğ°Ğ»Ğ¾Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¸ÑÑ‹ Ğ¤ÑƒÑ€ÑŒĞµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. FourierAttention Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°: Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ° Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLaMA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FourierAttention Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LongBench Ğ¸ Needle-In-A-Haystack.'}, 'en': {'title': 'Enhancing Memory Efficiency in LLMs with FourierAttention', 'desc': 'FourierAttention is a novel framework designed to improve memory efficiency in Large Language Models (LLMs) without the need for training. It addresses the challenge of increasing memory demands from the Key-Value (KV) cache as context lengths grow. By using orthogonal Fourier bases, it compresses transformer head dimensions, allowing lower dimensions to focus on local context while higher dimensions capture long-range dependencies. Evaluations demonstrate that FourierAttention enhances long-context accuracy and includes a custom kernel, FlashFourierAttention, for optimized memory operations during deployment.'}, 'zh': {'title': 'FourierAttentionï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…å­˜æ•ˆç‡', 'desc': 'FourierAttentionæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…å­˜æ•ˆç‡ã€‚å®ƒé€šè¿‡ä½¿ç”¨æ­£äº¤å‚…é‡Œå¶åŸºæ¥å‹ç¼©é•¿ä¸Šä¸‹æ–‡æ— å…³çš„å˜æ¢å¤´ç»´åº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜æ¢å¤´ç»´åº¦çš„å¼‚è´¨æ€§ï¼Œä½ç»´åº¦ä¼˜å…ˆå¤„ç†å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œè€Œé«˜ç»´åº¦åˆ™æ•æ‰é•¿ç¨‹ä¾èµ–ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFourierAttentionåœ¨é•¿ä¸Šä¸‹æ–‡å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”é€šè¿‡å®šåˆ¶çš„Tritonå†…æ ¸ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œç¡®ä¿é«˜æ•ˆéƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07464', 'title': 'DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO', 'url': 'https://huggingface.co/papers/2506.07464', 'abstract': 'DeepVideo-R1 enhances video reasoning performance using Reg-GRPO, a regression-based GRPO approach, and difficulty-aware data augmentation for video large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing a PPO-style reinforcement algorithm with group-based normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as a regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks.', 'score': 5, 'issue_id': 4308, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'f8c207e9d26fe89e', 'authors': ['Jinyoung Park', 'Jeehye Na', 'Jinyoung Kim', 'Hyunwoo J. Kim'], 'affiliations': ['KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07464.jpg', 'data': {'categories': ['#rlhf', '#video', '#benchmark', '#optimization', '#reasoning', '#training', '#rl'], 'emoji': 'ğŸ¥', 'ru': {'title': 'DeepVideo-R1: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ GRPO', 'desc': 'DeepVideo-R1 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ GRPO (Reg-GRPO) Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ GRPO Ğº Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¯Ğ‘Ğœ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°. Reg-GRPO Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ GRPO ĞºĞ°Ğº Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ, Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Ñ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Video Reasoning with Reg-GRPO and Smart Data Augmentation', 'desc': 'DeepVideo-R1 is a novel approach that enhances video reasoning in large language models by utilizing a regression-based method called Reg-GRPO. This method reformulates the Group Relative Policy Optimization (GRPO) objective into a regression task, allowing for more direct policy guidance without the need for complex safeguards. Additionally, the paper introduces a difficulty-aware data augmentation strategy that adjusts training samples based on their solvable difficulty, which helps in generating diverse and informative reward signals. The results demonstrate that DeepVideo-R1 significantly boosts performance on various video reasoning benchmarks, showcasing its effectiveness in the field.'}, 'zh': {'title': 'DeepVideo-R1ï¼šæå‡è§†é¢‘æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'DeepVideo-R1 æ˜¯ä¸€ç§å¢å¼ºè§†é¢‘æ¨ç†æ€§èƒ½çš„æ¨¡å‹ï¼Œé‡‡ç”¨äº†å›å½’å‹çš„ GRPO æ–¹æ³•å’Œéš¾åº¦æ„ŸçŸ¥çš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚è¯¥ç ”ç©¶æ¢è®¨äº† GRPO åœ¨è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå¹¶è¯†åˆ«å‡ºå½±å“æœ‰æ•ˆå­¦ä¹ çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¾èµ–ä¿æŠ¤æªæ–½å’Œä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒDeepVideo-R1 é€šè¿‡å›å½’ GRPO é‡æ–°æ„å»ºäº† GRPO ç›®æ ‡ï¼Œç›´æ¥é¢„æµ‹ä¼˜åŠ¿å€¼ï¼Œä»è€Œç®€åŒ–äº†æ”¿ç­–æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepVideo-R1 åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†è§†é¢‘æ¨ç†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11997', 'title': 'pLSTM: parallelizable Linear Source Transition Mark networks', 'url': 'https://huggingface.co/papers/2506.11997', 'abstract': 'pLSTMs are parallelizable linear RNNs designed for DAGs, demonstrating superior performance on long-range tasks and benchmarks compared to Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern recurrent architectures, such as xLSTM and Mamba, have recently challenged the Transformer in language modeling. However, their structure constrains their applicability to sequences only or requires processing multi-dimensional data structures, such as images or molecular graphs, in a pre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are well suited for data with a higher level structure, like 2D grids, trees, and directed acyclic graphs (DAGs). In this work, we extend the notion of multi-dimensionality to linear RNNs. We introduce parallelizable Linear Source Transition Mark networks (pLSTMs) using Source, Transition, and Mark gates that act on the line graph of a general DAG. This enables parallelization in analogy to parallel associative scans and the chunkwise-recurrent form of sequential linear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this scheme can be efficiently implemented using einsum operations, concatenations, and padding in logarithmic time. pLSTMs tackle the vanishing/exploding activation/gradient problem for long distances in DAGs via two distinct modes: a directed propagation mode (P-mode) and a diffusive distribution mode (D-mode). To showcase the long-range capabilities of pLSTM, we introduce arrow-pointing extrapolation as a synthetic computer vision task that contains long-distance directional information. We demonstrate that pLSTMs generalize well to larger image sizes, whereas Transformers struggle to extrapolate. On established molecular graph and computer vision benchmarks, pLSTMs also show strong performance. Code and Datasets are available at: https://github.com/ml-jku/plstm_experiments.', 'score': 4, 'issue_id': 4308, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '6dff119551b986fc', 'authors': ['Korbinian PÃ¶ppel', 'Richard Freinschlag', 'Thomas Schmied', 'Wei Lin', 'Sepp Hochreiter'], 'affiliations': ['Johannes Kepler University Linz'], 'pdf_title_img': 'assets/pdf/title_img/2506.11997.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#optimization', '#graphs', '#architecture', '#cv', '#open_source', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'pLSTM: ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ RNN Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ pLSTM (parallelizable Linear Source Transition Mark), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² (DAG). pLSTM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². pLSTM Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‰Ğ¸Ñ…/Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² DAG Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ²: Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'pLSTMs: Revolutionizing Long-Range Learning in DAGs', 'desc': 'The paper introduces parallelizable Linear Source Transition Mark networks (pLSTMs), a new type of linear recurrent neural network (RNN) designed for processing data structured as directed acyclic graphs (DAGs). Unlike traditional RNNs and Transformers, pLSTMs can efficiently handle multi-dimensional data without being limited to sequential processing. They address the vanishing and exploding gradient problems through two modes of operation, allowing for effective long-range dependencies in data. The authors demonstrate that pLSTMs outperform Transformers on various benchmarks, particularly in tasks requiring long-distance extrapolation, such as computer vision and molecular graph analysis.'}, 'zh': {'title': 'pLSTMsï¼šè¶…è¶Šå˜æ¢å™¨çš„é•¿è·ç¦»å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'pLSTMsæ˜¯ä¸€ç§å¯å¹¶è¡ŒåŒ–çš„çº¿æ€§é€’å½’ç¥ç»ç½‘ç»œï¼Œä¸“ä¸ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨é•¿è·ç¦»ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå˜æ¢å™¨ï¼ˆTransformersï¼‰ã€‚ä¸ç°ä»£é€’å½’æ¶æ„ç›¸æ¯”ï¼ŒpLSTMsé€šè¿‡æºã€è½¬ç§»å’Œæ ‡è®°é—¨çš„è®¾è®¡ï¼Œè§£å†³äº†é•¿è·ç¦»ä¼ æ’­ä¸­çš„æ¶ˆå¤±å’Œçˆ†ç‚¸æ¢¯åº¦é—®é¢˜ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºæ›´é«˜ç»“æ„çš„æ•°æ®ï¼Œå¦‚äºŒç»´ç½‘æ ¼å’Œæ ‘å½¢ç»“æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å›¾åƒç­‰å¤šç»´æ•°æ®ã€‚é€šè¿‡å¼•å…¥ç®­å¤´æŒ‡å‘å¤–æ¨çš„åˆæˆè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ŒpLSTMså±•ç¤ºäº†å…¶åœ¨é•¿è·ç¦»æ¨æ–­ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09427', 'title': 'A High-Quality Dataset and Reliable Evaluation for Interleaved\n  Image-Text Generation', 'url': 'https://huggingface.co/papers/2506.09427', 'abstract': "InterSyn, a large-scale dataset with tightly interleaved image-text outputs and automated quality refinement, improves multimodal understanding and generation through the SEIR method and SynJudge, an automatic evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy.   Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement.   Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn's utility for advancing multimodal systems.", 'score': 4, 'issue_id': 4305, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '5c1dd5f02a121213', 'authors': ['Yukang Feng', 'Jianwen Sun', 'Chuanhao Li', 'Zizhen Li', 'Jiaxin Ai', 'Fanrui Zhang', 'Yifan Chang', 'Sizhuo Zhou', 'Shenglin Zhang', 'Yu Dai', 'Kaipeng Zhang'], 'affiliations': ['Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09427.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#optimization', '#multimodal', '#games'], 'emoji': 'ğŸ”„', 'ru': {'title': 'InterSyn: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ InterSyn - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. InterSyn ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸ĞµĞ¼ (SEIR) Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ñ Ñ‚ĞµÑĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ SynJudge - Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° InterSyn ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Enhancing Multimodal AI with InterSyn and SEIR', 'desc': 'The paper introduces InterSyn, a large-scale dataset designed to enhance multimodal understanding and generation in AI models. It utilizes the Self-Evaluation with Iterative Refinement (SEIR) method to create high-quality, tightly interleaved image-text outputs through multi-turn dialogues. Additionally, the paper presents SynJudge, an automatic evaluation tool that assesses multimodal outputs based on text content, image quality, and their synergy. Experimental results demonstrate that models trained on InterSyn show significant performance improvements across various evaluation metrics, highlighting its effectiveness for next-generation instruction-following models.'}, 'zh': {'title': 'InterSynï¼šæå‡å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„å…³é”®æ•°æ®é›†', 'desc': 'InterSynæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚å®ƒé€šè¿‡è‡ªæˆ‘è¯„ä¼°ä¸è¿­ä»£ç²¾ç‚¼ï¼ˆSEIRï¼‰æ–¹æ³•æ„å»ºï¼ŒåŒ…å«å¤šè½®æŒ‡ä»¤é©±åŠ¨çš„å¯¹è¯å’Œç´§å¯†äº¤ç»‡çš„å›¾åƒ-æ–‡æœ¬è¾“å‡ºã€‚ä¸ºäº†è¯„ä¼°è¿™äº›è¾“å‡ºçš„è´¨é‡ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†SynJudgeï¼Œä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°å·¥å…·ï¼Œå¯ä»¥ä»æ–‡æœ¬å†…å®¹ã€å›¾åƒå†…å®¹ã€å›¾åƒè´¨é‡å’Œå›¾åƒ-æ–‡æœ¬ååŒå››ä¸ªç»´åº¦è¿›è¡Œé‡åŒ–è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SEIRæ–¹æ³•æ„å»ºçš„æ•°æ®é›†è´¨é‡æ˜¾è‘—æé«˜ï¼Œè®­ç»ƒåœ¨InterSynä¸Šçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09366', 'title': 'SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation\n  via Skill Blending', 'url': 'https://huggingface.co/papers/2506.09366', 'abstract': 'SkillBlender is a hierarchical reinforcement learning framework that uses pretrained primitive skills to efficiently solve diverse loco-manipulation tasks for humanoid robots.  \t\t\t\t\tAI-generated summary \t\t\t\t Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and loco-manipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, a parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by a set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/.', 'score': 4, 'issue_id': 4305, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '411c39c85d7cabe0', 'authors': ['Yuxuan Kuang', 'Haoran Geng', 'Amine Elhafsi', 'Tan-Dzung Do', 'Pieter Abbeel', 'Jitendra Malik', 'Marco Pavone', 'Yue Wang'], 'affiliations': ['Peking University', 'Stanford University', 'University of California, Berkeley', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.09366.jpg', 'data': {'categories': ['#benchmark', '#robotics', '#rl', '#open_source', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SkillBlender: ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'SkillBlender - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ»Ğ¾ĞºĞ¾Ğ¼Ğ¾Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ SkillBench - Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SkillBlender Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Empowering Humanoid Robots with SkillBlender: Efficient Loco-Manipulation through Skill Blending', 'desc': 'SkillBlender is a hierarchical reinforcement learning framework designed to enhance the performance of humanoid robots in loco-manipulation tasks. It utilizes pretrained primitive skills that are goal-conditioned and task-agnostic, allowing for efficient blending of these skills to tackle complex tasks without extensive reward tuning. This approach not only improves the versatility of the robots but also ensures that their movements are accurate and feasible in real-world scenarios. Additionally, SkillBench provides a comprehensive benchmark for evaluating the performance of these skills across different robot embodiments and tasks, promoting further research in the field.'}, 'zh': {'title': 'SkillBlenderï¼šé«˜æ•ˆçš„äººå½¢æœºå™¨äººè¿åŠ¨æ“æ§æ¡†æ¶', 'desc': 'SkillBlender æ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„åŸºæœ¬æŠ€èƒ½é«˜æ•ˆè§£å†³äººå½¢æœºå™¨äººåœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„è¿åŠ¨æ“æ§ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é¦–å…ˆé¢„è®­ç»ƒä¸ä»»åŠ¡æ— å…³çš„ç›®æ ‡å¯¼å‘åŸºæœ¬æŠ€èƒ½ï¼Œç„¶ååŠ¨æ€èåˆè¿™äº›æŠ€èƒ½ï¼Œä»¥æœ€å°çš„ä»»åŠ¡ç‰¹å®šå¥–åŠ±è®¾è®¡å®Œæˆå¤æ‚çš„è¿åŠ¨æ“æ§ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥ SkillBenchï¼Œä¸€ä¸ªåŒ…å«å¤šç§æ¨¡æ‹Ÿç¯å¢ƒå’ŒæŒ‘æˆ˜æ€§ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼ŒSkillBlender æä¾›äº†ç§‘å­¦çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹³è¡¡äº†å‡†ç¡®æ€§å’Œå¯è¡Œæ€§ã€‚å¤§é‡çš„æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼ŒSkillBlender æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªç„¶åœ°è§„èŒƒè¡Œä¸ºï¼Œé¿å…å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œä»è€Œå®ç°æ›´å‡†ç¡®å’Œå¯è¡Œçš„è¿åŠ¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11474', 'title': 'Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified\n  Process Rewards', 'url': 'https://huggingface.co/papers/2506.11474', 'abstract': 'Med-PRM enhances clinical decision making by verifying reasoning steps against medical knowledge bases, achieving state-of-the-art performance in medical QA benchmarks with improved accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/', 'score': 3, 'issue_id': 4314, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '123eb749b81758d8', 'authors': ['Jaehoon Yun', 'Jiwoong Sohn', 'Jungwoo Park', 'Hyunjae Kim', 'Xiangru Tang', 'Yanjun Shao', 'Yonghoe Koo', 'Minhyeok Ko', 'Qingyu Chen', 'Mark Gerstein', 'Michael Moor', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'ETH ZÃ¼rich', 'Hanyang University College of Medicine', 'Korea University', 'University of Ulsan College of Medicine', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.11474.jpg', 'data': {'categories': ['#healthcare', '#dataset', '#training', '#rag', '#reasoning', '#benchmark', '#science', '#small_models'], 'emoji': 'ğŸ©º', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼: Med-PRM Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³', 'desc': 'Med-PRM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ñ€Ğ°Ñ‡Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ ĞµĞ³Ğ¾ Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Med-PRM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 13.5%. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Meerkat, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 80% Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ MedQA.'}, 'en': {'title': 'Enhancing Medical Decision Making with Verified Reasoning Steps', 'desc': 'Med-PRM is a new framework designed to improve clinical decision making by verifying reasoning steps against established medical knowledge bases. It addresses the challenge of error localization in large language models, which is crucial for accurate medical diagnoses. By using retrieval-augmented generation, Med-PRM can assess the quality of reasoning in a detailed manner, leading to enhanced accuracy in medical question answering tasks. The framework has shown significant performance improvements, achieving state-of-the-art results on multiple benchmarks and demonstrating its versatility with various policy models.'}, 'zh': {'title': 'Med-PRMï¼šæå‡åŒ»ç–—å†³ç­–çš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'Med-PRMæ˜¯ä¸€ç§è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹åŒ»ç–—çŸ¥è¯†åº“çš„éªŒè¯æ¥å¢å¼ºä¸´åºŠå†³ç­–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œé€æ­¥éªŒè¯æ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥ï¼Œç¡®ä¿æ¨ç†çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹ä¸­é—´æ¨ç†æ­¥éª¤è¿›è¡ŒéªŒè¯ï¼ŒMed-PRMèƒ½å¤Ÿç»†è‡´åœ°è¯„ä¼°æ¨ç†è´¨é‡ï¼Œä»è€Œæé«˜åŒ»ç–—é—®ç­”åŸºå‡†çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMed-PRMåœ¨å¤šä¸ªåŒ»ç–—QAåŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11274', 'title': 'Learning a Continue-Thinking Token for Enhanced Test-Time Scaling', 'url': 'https://huggingface.co/papers/2506.11274', 'abstract': 'A continuous thinking token learned via reinforcement learning improves language model accuracy more effectively than a fixed token during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing "</think>" with "Wait") can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment a distilled version of DeepSeek-R1 with a single learned "<|continue-thinking|>" token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., "Wait") for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model\'s accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing.', 'score': 3, 'issue_id': 4313, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '770628ec96646ceb', 'authors': ['Liran Ringel', 'Elad Tolochinsky', 'Yaniv Romano'], 'affiliations': ['Department of Computer Science, Technion Israel Institute of Technology', 'Department of Electrical and Computer Engineering, Technion Israel Institute of Technology', 'Independent Researcher'], 'pdf_title_img': 'assets/pdf/title_img/2506.11274.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#inference', '#training', '#math', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GSM8K Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 4.2% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ.'}, 'en': {'title': 'Learned Thinking Token Boosts Language Model Accuracy!', 'desc': 'This paper presents a novel approach to enhance language model performance by introducing a learned continuous thinking token through reinforcement learning. Unlike fixed tokens that merely extend reasoning time, the learned token adapts dynamically to improve accuracy during inference. The authors demonstrate that this method outperforms both the baseline model and traditional fixed-token strategies on standard math benchmarks. Notably, their approach achieves a significant accuracy boost, particularly on the GSM8K benchmark, showcasing the effectiveness of learning over static methods.'}, 'zh': {'title': 'å­¦ä¹ è¿ç»­æ€è€ƒæ ‡è®°ï¼Œæå‡è¯­è¨€æ¨¡å‹å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ å­¦ä¹ çš„è¿ç»­æ€è€ƒæ ‡è®°ï¼Œå¦‚ä½•åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯”å›ºå®šæ ‡è®°æ›´æœ‰æ•ˆåœ°æé«˜è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ä¸“é—¨çš„è¿ç»­æ€è€ƒæ ‡è®°å¯ä»¥è§¦å‘æ›´é•¿çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨DeepSeek-R1çš„ç²¾ç®€ç‰ˆæœ¬ä¸­åŠ å…¥äº†ä¸€ä¸ªå­¦ä¹ åˆ°çš„"<|continue-thinking|>"æ ‡è®°ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå…¶åµŒå…¥ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æƒé‡ä¸å˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå­¦ä¹ åˆ°çš„æ ‡è®°åœ¨æ ‡å‡†æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸æ¯”äºåŸºçº¿æ¨¡å‹å’Œä½¿ç”¨å›ºå®šæ ‡è®°çš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08592', 'title': 'Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity\n  Dilemma of Embeddings', 'url': 'https://huggingface.co/papers/2506.08592', 'abstract': 'A new dataset named CapRetrieval is introduced to evaluate the ability of text encoders to recognize fine-grained entities and events, highlighting challenges in dense retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.', 'score': 3, 'issue_id': 4307, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '64e56d52fd4bf03d', 'authors': ['Liyan Xu', 'Zhenlin Su', 'Mo Yu', 'Jiangnan Li', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.08592.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#training', '#open_source', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑÑ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²', 'desc': "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CapRetrieval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ»ĞºĞ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° CapRetrieval. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° 'Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼Ñ‹ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸' - ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ² ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ…."}, 'en': {'title': 'Enhancing Fine-Grained Entity Recognition in Text Encoders', 'desc': 'This paper introduces a new dataset called CapRetrieval, designed to test how well text encoders can identify detailed entities and events in text. The authors highlight a common problem where these encoders struggle with fine-grained retrieval, even in straightforward scenarios. Through zero-shot evaluation, they demonstrate that existing models often fail to match fine details, regardless of their size or training data. To improve performance, they propose data generation strategies for fine-tuning encoders, addressing the challenge of balancing detailed recognition with overall semantic understanding.'}, 'zh': {'title': 'æå‡æ–‡æœ¬ç¼–ç å™¨çš„ç»†ç²’åº¦è¯†åˆ«èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†CapRetrievalï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬ç¼–ç å™¨è¯†åˆ«ç»†ç²’åº¦å®ä½“å’Œäº‹ä»¶çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„æ–‡æœ¬ç¼–ç å™¨åœ¨å¯†é›†æ£€ç´¢ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆè¯†åˆ«è¯­ä¹‰ä¸­çš„ç»†ç²’åº¦ä¿¡æ¯ã€‚é€šè¿‡é›¶æ ·æœ¬è¯„ä¼°ï¼Œå‘ç°æ— è®ºæ¨¡å‹å¤§å°æˆ–è®­ç»ƒæ¥æºï¼Œç¼–ç å™¨åœ¨ç»†ç²’åº¦åŒ¹é…ä¸Šéƒ½å¯èƒ½å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ•°æ®ç”Ÿæˆç­–ç•¥æ¥å¾®è°ƒç¼–ç å™¨ï¼Œä»è€Œåœ¨CapRetrievalä¸Šè·å¾—æœ€ä½³æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08477', 'title': 'Detecting Harmful Memes with Decoupled Understanding and Guided CoT\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.08477', 'abstract': "U-CoT+ is a novel framework for detecting harmful memes by converting them into textual descriptions and using human-crafted guidelines with zero-shot CoT prompting to achieve high flexibility and explainability with small-scale LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, a novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resource-efficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. Codes and data are available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.", 'score': 3, 'issue_id': 4306, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '4b240f248019e671', 'authors': ['Fengjun Pan', 'Anh Tuan Luu', 'Xiaobao Wu'], 'affiliations': ['Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08477.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#ethics', '#interpretability', '#small_models', '#multimodal', '#benchmark'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ğ² Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ¾Ğ²', 'desc': 'U-CoT+ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¼Ğ¾Ğ² Ğ¾Ñ‚ Ğ¸Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Transforming Memes into Text for Smarter Detection', 'desc': 'U-CoT+ is a new framework designed to detect harmful memes by transforming them into textual descriptions. This approach uses a meme-to-text pipeline that preserves details, allowing for better interpretation without needing complex visual analysis. By applying human-crafted guidelines with zero-shot Chain of Thought (CoT) prompting, the framework enhances flexibility and explainability in the detection process. The effectiveness of U-CoT+ is demonstrated through extensive experiments on various benchmark datasets, showcasing its potential for efficient and interpretable meme moderation using small-scale large language models (LLMs).'}, 'zh': {'title': 'U-CoT+: é«˜æ•ˆå¯è§£é‡Šçš„æœ‰å®³è¿·å› æ£€æµ‹æ¡†æ¶', 'desc': 'U-CoT+æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹æœ‰å®³çš„ç½‘ç»œè¿·å› ã€‚å®ƒé€šè¿‡å°†è¿·å› è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œå¹¶ä½¿ç”¨äººç±»è®¾è®¡çš„æŒ‡å¯¼åŸåˆ™ï¼Œç»“åˆé›¶-shoté“¾å¼æ¨ç†ï¼Œæ¥å®ç°é«˜çµæ´»æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶é¿å…äº†å¯¹å¤æ‚è§†è§‰å†…å®¹çš„ç›´æ¥æ¨ç†ï¼Œä»è€Œæé«˜äº†èµ„æºæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒU-CoT+åœ¨å°è§„æ¨¡å¤§è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æœ‰æ•ˆçš„æœ‰å®³è¿·å› æ£€æµ‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11136', 'title': 'JAFAR: Jack up Any Feature at Any Resolution', 'url': 'https://huggingface.co/papers/2506.11136', 'abstract': 'JAFAR is a lightweight feature upsampler using an attention-based module with Spatial Feature Transform modulation, enabling high-resolution features from Foundation Vision Encoders without high-resolution supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io', 'score': 2, 'issue_id': 4311, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'ba6fb6c9e607162e', 'authors': ['Paul Couairon', 'Loick Chambon', 'Louis Serrano', 'Jean-Emmanuel Haugeard', 'Matthieu Cord', 'Nicolas Thome'], 'affiliations': ['Sorbonne UniversitÃ©, CNRS, ISIR, F-75005 Paris, France', 'Thales, TSGF, cortAIx Labs, France', 'Valeo.ai'], 'pdf_title_img': 'assets/pdf/title_img/2506.11136.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'JAFAR: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'JAFAR - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Spatial Feature Transform. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. JAFAR ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñ‹.'}, 'en': {'title': 'JAFAR: Elevating Vision Features with Attention and Modulation', 'desc': 'JAFAR is a novel feature upsampler that enhances the spatial resolution of visual features from Foundation Vision Encoders without requiring high-resolution supervision. It utilizes an attention-based module with Spatial Feature Transform modulation to align high-resolution queries with low-resolution keys, improving semantic coherence. The model is lightweight and flexible, allowing it to upscale features to any desired resolution effectively. Experimental results indicate that JAFAR excels in recovering fine details and outperforms existing methods in various dense vision tasks.'}, 'zh': {'title': 'JAFARï¼šè½»é‡çº§ç‰¹å¾ä¸Šé‡‡æ ·çš„æ–°é€‰æ‹©', 'desc': 'JAFARæ˜¯ä¸€ç§è½»é‡çº§çš„ç‰¹å¾ä¸Šé‡‡æ ·å™¨ï¼Œåˆ©ç”¨åŸºäºæ³¨æ„åŠ›çš„æ¨¡å—å’Œç©ºé—´ç‰¹å¾å˜æ¢è°ƒåˆ¶ï¼Œèƒ½å¤Ÿä»åŸºç¡€è§†è§‰ç¼–ç å™¨ä¸­ç”Ÿæˆé«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼Œè€Œæ— éœ€é«˜åˆ†è¾¨ç‡çš„ç›‘ç£ã€‚è¯¥æ–¹æ³•é€šè¿‡å¢å¼ºä½çº§å›¾åƒç‰¹å¾ä¸è¯­ä¹‰ä¸°å¯Œçš„ä½åˆ†è¾¨ç‡é”®ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œæ¥æå‡è§†è§‰ç‰¹å¾çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚å°½ç®¡æ²¡æœ‰é«˜åˆ†è¾¨ç‡çš„ç›‘ç£ï¼ŒJAFARåœ¨ä½ä¸Šé‡‡æ ·æ¯”ç‡å’Œåˆ†è¾¨ç‡ä¸‹çš„å­¦ä¹ è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°æ¨å¹¿åˆ°æ›´é«˜çš„è¾“å‡ºå°ºåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJAFARåœ¨æ¢å¤ç»†ç²’åº¦ç©ºé—´ç»†èŠ‚æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„ç‰¹å¾ä¸Šé‡‡æ ·æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11702', 'title': 'Configurable Preference Tuning with Rubric-Guided Synthetic Data', 'url': 'https://huggingface.co/papers/2506.11702', 'abstract': 'Configurable Preference Tuning enables language models to dynamically adjust their behavior based on human-interprettable directives, using rubric-guided preference data for fine-tuning and inference-time modulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning', 'score': 1, 'issue_id': 4305, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': '7a7eb1af4ef17eef', 'authors': ['VÃ­ctor Gallego'], 'affiliations': ['Komorebi AI Technologies, Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2506.11702.jpg', 'data': {'categories': ['#rlhf', '#synthetic', '#training', '#dataset', '#alignment', '#open_source'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Configurable Preference Tuning (CPT) Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. CPT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¸Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ñ…, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰Ğ¸Ñ… Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½ÑĞ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Dynamic Adaptation of Language Models with Configurable Preference Tuning', 'desc': "This paper introduces Configurable Preference Tuning (CPT), a new method that allows language models to adapt their responses based on clear, human-understandable instructions. Unlike traditional models that rely on a fixed set of preferences, CPT uses dynamically generated preference data to fine-tune the model's behavior. By employing structured rubrics that specify desired traits, the model can adjust its outputs in real-time without needing to be retrained. This innovation enhances the model's ability to respond to complex and varied human feedback, making it more flexible and context-aware."}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´è¯­è¨€æ¨¡å‹è¡Œä¸ºçš„å¯é…ç½®åå¥½è°ƒä¼˜', 'desc': 'å¯é…ç½®åå¥½è°ƒä¼˜ï¼ˆCPTï¼‰æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ ¹æ®äººç±»å¯ç†è§£çš„æŒ‡ä»¤åŠ¨æ€è°ƒæ•´å…¶è¡Œä¸ºã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ä¸åŒï¼ŒCPTå…è®¸æ¨¡å‹ä½¿ç”¨åˆæˆç”Ÿæˆçš„åå¥½æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»è€Œåœ¨æ¨ç†æ—¶æ ¹æ®ç³»ç»Ÿæç¤ºè°ƒèŠ‚è¾“å‡ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå“åº”ä¸åŒçš„ä¸Šä¸‹æ–‡å’Œéœ€æ±‚ã€‚è¯¥æ–¹æ³•ä¸ä»…æä¾›äº†æ›´ç»†è‡´çš„æ§åˆ¶ï¼Œè¿˜èƒ½æ›´å¥½åœ°æ¨¡æ‹Ÿå¤æ‚å’Œä¾èµ–ä¸Šä¸‹æ–‡çš„äººç±»åé¦ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10082', 'title': 'LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware\n  LoRA Fine-Tuning', 'url': 'https://huggingface.co/papers/2506.10082', 'abstract': 'A mask-based LoRA tuning method for video editing adapts pretrained Image-to-Video models for flexible and high-quality video editing, using spatial masks and reference images for context-specific adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods.', 'score': 1, 'issue_id': 4313, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '14126cd5898da5a7', 'authors': ['Chenjian Gao', 'Lihe Ding', 'Xin Cai', 'Zhanpeng Huang', 'Zibin Wang', 'Tianfan Xue'], 'affiliations': ['SenseTime Research', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.10082.jpg', 'data': {'categories': ['#multimodal', '#games', '#diffusion', '#video', '#optimization', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞ¾Ğº Ğ¸ LoRA', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LoRA Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Image-to-Video. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑÑ‹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ ÑÑ†ĞµĞ½Ñ‹. ĞœĞ°ÑĞºĞ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ².'}, 'en': {'title': 'Flexible Video Editing with Mask-Based LoRA Tuning', 'desc': 'This paper presents a novel mask-based Low-Rank Adaptation (LoRA) tuning method for enhancing video editing capabilities using pretrained Image-to-Video models. The approach allows for flexible and high-quality edits by utilizing spatial masks and reference images, which provide context-specific guidance during the editing process. By preserving background regions and enabling controlled propagation of edits, the method improves upon existing techniques that often lack adaptability for subsequent frames. Experimental results demonstrate that this method outperforms current state-of-the-art video editing approaches, making it a significant advancement in the field.'}, 'zh': {'title': 'çµæ´»é«˜æ•ˆçš„è§†é¢‘ç¼–è¾‘æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ©ç çš„LoRAè°ƒä¼˜æ–¹æ³•ï¼Œç”¨äºè§†é¢‘ç¼–è¾‘ï¼Œæ—¨åœ¨çµæ´»åœ°é€‚åº”é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡ç©ºé—´æ©ç å’Œå‚è€ƒå›¾åƒè¿›è¡Œä¸Šä¸‹æ–‡ç‰¹å®šçš„è°ƒæ•´ï¼Œä¿æŒèƒŒæ™¯åŒºåŸŸçš„åŒæ—¶å®ç°å¯æ§çš„ç¼–è¾‘ä¼ æ’­ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹ï¼Œæä¾›äº†é«˜æ•ˆä¸”é€‚åº”æ€§å¼ºçš„è§†é¢‘ç¼–è¾‘èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ç¼–è¾‘æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10056', 'title': 'Reward Models Enable Scalable Code Verification by Trading Accuracy for\n  Throughput', 'url': 'https://huggingface.co/papers/2506.10056', 'abstract': 'The standard paradigm for solving coding tasks via large language models (LLMs) is to generate-then-rank programs, where the latter step uses a verifier in the ranking process. The growing consensus is that a comprehensive verifier (e.g., a full test suite) should be prioritized over an outcome reward model (ORM) whenever possible, with little consideration given to the trade-offs involved. We aim to challenge this assumption by systematically exploring the tradeoff between speed and accuracy. We find that ORMs play a crucial role in scaling verification through trading accuracy for speed, even when a comprehensive verifier is available. Their value becomes especially apparent when used in a generate-prune-then-rank approach, where a faster but less accurate verifier removes incorrect solutions prior to ranking -- leading to a system that is 11.65x faster while only being 8.33% less accurate than the full test suite. We analyze the generate-prune-then-rank approach and show that it works by filtering out incorrect but highly ranked solutions. These findings enable the design of scalable and accurate program ranking systems.', 'score': 1, 'issue_id': 4318, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '1ee14b5fd87f1f86', 'authors': ['Gabriel Orlanski', 'Nicholas Roberts', 'Aws Albarghouthi', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.10056.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾ÑĞ¿Ğ°Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¾Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ°Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (ORM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ORM Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ-Ğ¾Ñ‚ÑĞµĞ²-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ' Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ORM Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ² 11.65 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 8.33% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²."}, 'en': {'title': 'Balancing Speed and Accuracy in Program Verification with ORMs', 'desc': 'This paper investigates the effectiveness of using outcome reward models (ORMs) in the coding task solution process with large language models (LLMs). It challenges the common belief that comprehensive verifiers should always be prioritized, highlighting the trade-off between speed and accuracy. The authors demonstrate that ORMs can significantly enhance the efficiency of program verification by allowing a faster, less accurate verifier to filter out incorrect solutions before ranking. Their findings suggest that a generate-prune-then-rank strategy can achieve a balance, resulting in a system that is much faster while maintaining acceptable accuracy levels.'}, 'zh': {'title': 'é€Ÿåº¦ä¸å‡†ç¡®æ€§çš„æƒè¡¡ï¼šä¼˜åŒ–ç¼–ç ä»»åŠ¡çš„éªŒè¯æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³ç¼–ç ä»»åŠ¡çš„æ ‡å‡†æ–¹æ³•ï¼Œå³ç”Ÿæˆ-ç„¶å-æ’åçš„ç¨‹åºã€‚ç ”ç©¶è¡¨æ˜ï¼Œå…¨é¢çš„éªŒè¯å™¨ï¼ˆå¦‚å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼‰é€šå¸¸è¢«è®¤ä¸ºä¼˜äºç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰ï¼Œä½†ä½œè€…æŒ‘æˆ˜äº†è¿™ä¸€å‡è®¾ã€‚é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶é€Ÿåº¦ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå‘ç°ORMåœ¨æé«˜éªŒè¯é€Ÿåº¦æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œå³ä½¿åœ¨æœ‰å…¨é¢éªŒè¯å™¨çš„æƒ…å†µä¸‹ã€‚æœ€ç»ˆï¼Œç”Ÿæˆ-ä¿®å‰ª-ç„¶å-æ’åçš„æ–¹æ³•ä½¿å¾—ç³»ç»Ÿé€Ÿåº¦æé«˜äº†11.65å€ï¼Œå‡†ç¡®æ€§ä»…ä¸‹é™äº†8.33%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11130', 'title': 'A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data', 'url': 'https://huggingface.co/papers/2506.11130', 'abstract': 'A self-refining framework enhances ASR performance using unlabeled datasets by integrating pseudo-labeling, TTS, and synthesized speech to create a specialized model.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The process starts with an existing ASR model generating pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs are bootstrapped into the original ASR system, completing the closed-loop self-improvement cycle. We demonstrated the effectiveness of the framework on Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a moderate amount of text data, and synthetic content from the AI models, we adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper. Results highlight the framework as a compelling alternative to pseudo-labeling self-distillation approaches and provides a practical pathway for improving ASR performance in low-resource or domain-specific settings.', 'score': 1, 'issue_id': 4308, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '743ff411fbd34247', 'authors': ['Cheng Kang Chou', 'Chan-Jan Hsu', 'Ho-Lam Chung', 'Liang-Hsuan Tseng', 'Hsi-Chun Cheng', 'Yu-Kuan Fu', 'Kuan Po Huang', 'Hung-Yi Lee'], 'affiliations': ['MediaTek Research', 'National Taiwan University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2506.11130.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#transfer_learning', '#low_resource', '#audio', '#synthetic'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ĞĞ¡Ğ : Ğ¾Ñ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ ĞĞ¡Ğ , ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ñ€ĞµÑ‡ÑŒ-Ñ‚ĞµĞºÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞĞ¡Ğ , Ğ·Ğ°Ğ¼Ñ‹ĞºĞ°Ñ Ñ†Ğ¸ĞºĞ» ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ‚Ğ°Ğ¹Ğ²Ğ°Ğ½ÑŒÑĞºĞ¾Ğ¼ Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğµ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Twister, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ· Whisper-large-v2, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ.'}, 'en': {'title': 'Enhancing ASR with Unlabeled Data: The Twister Framework', 'desc': 'This paper presents a self-refining framework designed to improve Automatic Speech Recognition (ASR) performance using only unlabeled datasets. The framework begins with an existing ASR model that generates pseudo-labels from unannotated speech data, which are then utilized to train a high-fidelity Text-to-Speech (TTS) system. Synthesized speech and text pairs are incorporated back into the original ASR model, creating a closed-loop system that enhances its accuracy. The proposed method, tested on Taiwanese Mandarin, shows significant error rate reductions, demonstrating its effectiveness in low-resource environments.'}, 'zh': {'title': 'è‡ªæˆ‘ä¼˜åŒ–æ¡†æ¶æå‡ASRæ€§èƒ½çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨æœªæ ‡æ³¨çš„æ•°æ®é›†æ¥æå‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨ç°æœ‰çš„ASRæ¨¡å‹ä¸ºæœªæ ‡æ³¨çš„è¯­éŸ³ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªé«˜ä¿çœŸçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚æ¥ç€ï¼Œå°†åˆæˆçš„è¯­éŸ³æ–‡æœ¬å¯¹å¼•å…¥åŸå§‹çš„ASRç³»ç»Ÿï¼Œå½¢æˆä¸€ä¸ªé—­ç¯çš„è‡ªæˆ‘æ”¹è¿›å¾ªç¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å°æ¹¾æ™®é€šè¯è¯­éŸ³ä¸Šæœ‰æ•ˆï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½é”™è¯¯ç‡ï¼Œå°¤å…¶åœ¨ä½èµ„æºæˆ–ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ä¸­å…·æœ‰å®é™…æ„ä¹‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08915', 'title': 'Inherently Faithful Attention Maps for Vision Transformers', 'url': 'https://huggingface.co/papers/2506.08915', 'abstract': 'An attention-based method using learned binary masks improves robustness in object perception by focusing on relevant image regions while filtering out spurious information.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds.', 'score': 1, 'issue_id': 4309, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'e080cfdc03932dd9', 'authors': ['Ananthu Aniraj', 'Cassio F. Dantas', 'Dino Ienco', 'Diego Marcos'], 'affiliations': ['Inrae', 'Inria', 'University of Montpellier'], 'pdf_title_img': 'assets/pdf/title_img/2506.08915.jpg', 'data': {'categories': ['#benchmark', '#cv', '#optimization', '#architecture', '#interpretability'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ¤Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ¼: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ±Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ°Ğ¿Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ½ĞµÑ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ„Ğ¾Ğ½Ğ°Ğ¼.'}, 'en': {'title': 'Focusing Attention for Robust Object Perception', 'desc': 'This paper presents an attention-based method that utilizes learned binary masks to enhance object perception in images. By focusing on relevant regions and filtering out irrelevant information, the method improves the robustness of predictions, especially in challenging contexts. The proposed two-stage framework first identifies object parts and task-relevant areas, then restricts analysis to these regions using attention masking. Experimental results show that this approach effectively mitigates the impact of spurious correlations and out-of-distribution backgrounds on object recognition tasks.'}, 'zh': {'title': 'åŸºäºæ³¨æ„åŠ›çš„é²æ£’ç‰©ä½“æ„ŸçŸ¥æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„ç®—æ³•ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„äºŒè¿›åˆ¶æ³¨æ„åŠ›æ©ç æ¥æé«˜ç‰©ä½“æ„ŸçŸ¥çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å…³æ³¨ç›¸å…³çš„å›¾åƒåŒºåŸŸï¼Œè¿‡æ»¤æ‰æ— å…³çš„ä¿¡æ¯ï¼Œä»è€Œç¡®ä¿åªæœ‰è¢«å…³æ³¨çš„åŒºåŸŸå½±å“é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µå¤„ç†å®Œæ•´å›¾åƒä»¥å‘ç°ç‰©ä½“éƒ¨åˆ†å¹¶è¯†åˆ«ä»»åŠ¡ç›¸å…³åŒºåŸŸï¼Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨è¾“å…¥çš„æ³¨æ„åŠ›æ©ç é™åˆ¶æ„Ÿå—é‡ï¼Œä»è€Œè¿›è¡Œæ›´ä¸“æ³¨çš„åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹è™šå‡ç›¸å…³æ€§å’Œåˆ†å¸ƒå¤–èƒŒæ™¯æ–¹é¢æ˜¾è‘—æé«˜äº†é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11116', 'title': 'Infinity Instruct: Scaling Instruction Selection and Synthesis to\n  Enhance Language Models', 'url': 'https://huggingface.co/papers/2506.11116', 'abstract': 'Infinity-Instruct, a comprehensive instruction dataset, enhances both foundational and chat capabilities of large language models through curation and synthesis, achieving superior performance compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our datasethttps://huggingface.co/datasets/BAAI/Infinity-Instruct and codeshttps://gitee.com/li-touch/infinity-instruct have been publicly released.', 'score': 1, 'issue_id': 4313, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '4de7c4de7a621e11', 'authors': ['Jijie Li', 'Li Du', 'Hanyu Zhao', 'Bo-wen Zhang', 'Liangdong Wang', 'Boyan Gao', 'Guang Liu', 'Yonghua Lin'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.11116.jpg', 'data': {'categories': ['#open_source', '#data', '#transfer_learning', '#benchmark', '#dataset'], 'emoji': 'ğŸš€', 'ru': {'title': 'Infinity-Instruct: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Infinity-Instruct - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹: 7,4 Ğ¼Ğ»Ğ½ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ 1,5 Ğ¼Ğ»Ğ½ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Infinity-Instruct, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, InfInstruct-LLaMA3.1-70B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4-0314 Ğ½Ğ° 8,6% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Unlocking LLM Potential with Infinity-Instruct', 'desc': 'Infinity-Instruct is a new instruction dataset that improves the performance of large language models (LLMs) in both foundational and chat tasks. It consists of 7.4 million curated foundational instructions and 1.5 million synthesized chat instructions, created through advanced data selection and filtering techniques. The dataset has been tested on various open-source models, showing significant performance improvements over existing instruction datasets. This research highlights the importance of combining foundational and chat training to enhance the overall capabilities of LLMs.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤èƒ½åŠ›', 'desc': 'Infinity-Instructæ˜¯ä¸€ä¸ªå…¨é¢çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›å’Œå¯¹è¯èƒ½åŠ›ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„å¤„ç†æµç¨‹ï¼Œæˆ‘ä»¬ä»è¶…è¿‡1äº¿ä¸ªæ ·æœ¬ä¸­ç­›é€‰å‡º740ä¸‡æ¡é«˜è´¨é‡çš„åŸºç¡€æŒ‡ä»¤ï¼Œå¹¶åˆæˆ150ä¸‡æ¡é«˜è´¨é‡çš„å¯¹è¯æŒ‡ä»¤ã€‚ç»è¿‡å®è¯è¯„ä¼°ï¼Œä½¿ç”¨Infinity-Instructå¾®è°ƒçš„å¤šä¸ªå¼€æºæ¨¡å‹åœ¨åŸºç¡€å’ŒæŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºç¡€è®­ç»ƒå’Œå¯¹è¯è®­ç»ƒä¹‹é—´çš„ååŒä½œç”¨å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03857', 'title': 'Prompt Candidates, then Distill: A Teacher-Student Framework for\n  LLM-driven Data Annotation', 'url': 'https://huggingface.co/papers/2506.03857', 'abstract': 'A novel candidate annotation paradigm using a teacher-student framework improves data quality forä¸‹æ¸¸ applications by encouraging large language models to output multiple labels when uncertain.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine a single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose a novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop a teacher-student framework CanDist that distills candidate annotations with a Small Language Model (SLM). We further provide a rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CanDist.', 'score': 1, 'issue_id': 4314, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '9efc9f12a990b701', 'authors': ['Mingxuan Xia', 'Haobo Wang', 'Yixuan Li', 'Zewei Yu', 'Jindong Wang', 'Junbo Zhao', 'Runze Wu'], 'affiliations': ['NetEase Fuxi AI Lab', 'University of Wisconsin Madison', 'William & Mary', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03857.jpg', 'data': {'categories': ['#data', '#dataset', '#training', '#optimization', '#alignment'], 'emoji': 'ğŸ·ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ LLM Ğ²Ñ‹Ğ´Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ğ»ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (SLM). Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Uncertainty: Multi-Label Annotation for Better Data Quality', 'desc': 'This paper introduces a new approach to data annotation using a teacher-student framework that enhances the quality of labels produced by large language models (LLMs). Instead of forcing LLMs to choose a single label, the method encourages them to generate multiple potential labels when they are uncertain. This strategy helps to mitigate the risk of incorrect labeling, which can degrade the quality of data for subsequent applications. The proposed framework, called CanDist, uses a Small Language Model (SLM) to refine these candidate annotations, leading to better performance in text classification tasks.'}, 'zh': {'title': 'åˆ©ç”¨æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶æå‡æ•°æ®æ ‡æ³¨è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å€™é€‰æ ‡æ³¨èŒƒå¼ï¼Œåˆ©ç”¨æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶æ¥æé«˜æ•°æ®è´¨é‡ã€‚è¯¥æ–¹æ³•é¼“åŠ±å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶è¾“å‡ºå¤šä¸ªæ ‡ç­¾ï¼Œä»è€Œå‡å°‘é”™è¯¯æ ‡æ³¨çš„é£é™©ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚æ ·æœ¬ï¼Œç¡®ä¿ä¸‹æ¸¸åº”ç”¨çš„æ•°æ®è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…­ä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10387', 'title': 'Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal\n  Skills', 'url': 'https://huggingface.co/papers/2506.10387', 'abstract': 'Hierarchical Multimodal Skills and Skill-Augmented Monte Carlo Tree Search improve multimodal GUI agent performance in long-horizon tasks by abstracting knowledge and bridging the offline-online domain gap.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI agents have yielded promising outcomes. However, these agents still struggle with long-horizon tasks in online environments, primarily due to insufficient knowledge and the inherent gap between offline and online domains. In this paper, inspired by how humans generalize knowledge in open-ended environments, we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of insufficient knowledge. It progressively abstracts trajectories into execution skills, core skills, and ultimately meta-skills, providing a hierarchical knowledge structure for long-horizon task planning. To bridge the domain gap, we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm, which efficiently leverages skills acquired in offline environments to reduce the action search space during online tree exploration. Building on HMS, we propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To validate the performance of Mirage-1 in real-world long-horizon scenarios, we constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1 outperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld, MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page: https://cybertronagent.github.io/Mirage-1.github.io/', 'score': 0, 'issue_id': 4315, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '9c47a3d6d929dc16', 'authors': ['Yuquan Xie', 'Zaijing Li', 'Rui Shao', 'Gongwei Chen', 'Kaiwen Zhou', 'Yinchuan Li', 'Dongmei Jiang', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Huawei Noahs Ark Lab', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10387.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#benchmark', '#long_context', '#games', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¸ ĞœĞšĞ¢ĞŸ: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞĞ°Ğ²Ñ‹ĞºĞ¾Ğ² (HMS) Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ”ĞµÑ€ĞµĞ²Ğ° ĞŸĞ¾Ğ¸ÑĞºĞ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ñ Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞĞ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ (SA-MCTS) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ HMS Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ°Ğ³ĞµĞ½Ñ‚ Mirage-1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AndroidLH.'}, 'en': {'title': 'Empowering GUI Agents with Hierarchical Skills for Long-Horizon Success', 'desc': 'This paper introduces a new approach to improve the performance of multimodal GUI agents in long-horizon tasks by using Hierarchical Multimodal Skills (HMS) and Skill-Augmented Monte Carlo Tree Search (SA-MCTS). HMS organizes knowledge into a hierarchy of execution skills, core skills, and meta-skills, allowing agents to better generalize and plan tasks. SA-MCTS enhances the search process by utilizing skills learned in offline settings to streamline decision-making in online environments. The proposed agent, Mirage-1, demonstrates significant performance improvements over existing agents in various benchmarks, showcasing its effectiveness in real-world applications.'}, 'zh': {'title': 'å±‚æ¬¡åŒ–æŠ€èƒ½ä¸å¢å¼ºæœç´¢æå‡GUIä»£ç†è¡¨ç°', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–å¤šæ¨¡æ€æŠ€èƒ½æ¨¡å—ï¼ˆHMSï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„çŸ¥è¯†ä¸è¶³é—®é¢˜ã€‚HMSé€šè¿‡å°†è½¨è¿¹é€æ­¥æŠ½è±¡ä¸ºæ‰§è¡ŒæŠ€èƒ½ã€æ ¸å¿ƒæŠ€èƒ½å’Œå…ƒæŠ€èƒ½ï¼Œæ„å»ºäº†ä¸€ä¸ªå±‚æ¬¡åŒ–çš„çŸ¥è¯†ç»“æ„ï¼Œä»¥æ”¯æŒé•¿æ—¶é—´ä»»åŠ¡çš„è§„åˆ’ã€‚ä¸ºäº†å¼¥è¡¥ç¦»çº¿å’Œåœ¨çº¿é¢†åŸŸä¹‹é—´çš„å·®è·ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†æŠ€èƒ½å¢å¼ºçš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•ï¼ˆSA-MCTSï¼‰ï¼Œè¯¥ç®—æ³•æœ‰æ•ˆåˆ©ç”¨ç¦»çº¿ç¯å¢ƒä¸­è·å¾—çš„æŠ€èƒ½ï¼Œå‡å°‘åœ¨çº¿æ ‘æœç´¢ä¸­çš„åŠ¨ä½œæœç´¢ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMirage-1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„ä»£ç†ï¼Œæå‡äº†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24726', 'title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.24726', 'abstract': "A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.", 'score': 140, 'issue_id': 4116, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '05f75b6123a35a65', 'authors': ['Shelly Bensal', 'Umar Jamil', 'Christopher Bryant', 'Melisa Russak', 'Kiran Kamble', 'Dmytro Mozolevskyi', 'Muayad Ali', 'Waseem AlShikh'], 'affiliations': ['Writer, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.24726.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#small_models', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»Ğµ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¿Ñ‹Ñ‚Ğ°ĞµÑ‚ÑÑ ĞµÑ‘ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ•ÑĞ»Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ°, Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Empowering Language Models Through Self-Reflection and Reinforcement Learning', 'desc': 'This paper presents a novel approach to enhance large language models using self-reflection and reinforcement learning. The method encourages models to analyze their mistakes and generate self-reflective commentary, which is then used to improve subsequent task attempts. By rewarding successful outcomes that follow self-reflection, the model learns to perform better even with minimal feedback. Experimental results indicate significant performance improvements, particularly in smaller models, suggesting a promising direction for developing more effective language models.'}, 'zh': {'title': 'è‡ªæˆ‘åæ€ä¸å¼ºåŒ–å­¦ä¹ æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è‡ªæˆ‘åæ€å’Œå¼ºåŒ–å­¦ä¹ æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨æ¨¡å‹å›ç­”é”™è¯¯æ—¶ï¼Œæ¿€åŠ±å…¶ç”Ÿæˆæ›´å¥½çš„è‡ªæˆ‘åæ€ï¼Œä»è€Œæé«˜è§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæ¨¡å‹åœ¨å¤±è´¥åç”Ÿæˆè‡ªæˆ‘åæ€çš„è¯„è®ºï¼›å…¶æ¬¡ï¼Œæ¨¡å‹åœ¨è€ƒè™‘è‡ªæˆ‘åæ€çš„æƒ…å†µä¸‹å†æ¬¡å°è¯•ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸­ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯å°å‹å¾®è°ƒæ¨¡å‹çš„è¡¨ç°è¶…è¿‡äº†åŒç±»æ›´å¤§æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02387', 'title': 'VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments', 'url': 'https://huggingface.co/papers/2506.02387', 'abstract': "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VS-Bench comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents' ability to predict others' future moves and optimize for long-term objectives. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return. Extensive experiments of fourteen leading VLMs reveal a significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return. We further conduct in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io.", 'score': 50, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '829f3546d3ac9345', 'authors': ['Zelai Xu', 'Zhexuan Xu', 'Xiangmin Yi', 'Huining Yuan', 'Xinlei Chen', 'Yi Wu', 'Chao Yu', 'Yu Wang'], 'affiliations': ['Beijing Zhongguancun Academy', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.02387.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#benchmark', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'VS-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'VS-Bench - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸, ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ 14 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ VLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Evaluating Strategic Reasoning in Multi-Agent Environments with VS-Bench', 'desc': 'VS-Bench is a new benchmark created to test Vision Language Models (VLMs) in complex situations where multiple agents interact. Unlike previous benchmarks that focused on single agents or text-only tasks, VS-Bench evaluates how well VLMs can reason and make decisions in environments that include both visual and language elements. It features eight different scenarios that require agents to work together, compete, or navigate mixed motives, assessing their ability to predict actions and achieve long-term goals. The results show that current VLMs still have a long way to go, with significant gaps in their performance, highlighting the need for further research in this area.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„æˆ˜ç•¥æ¨ç†æ–°åŸºå‡†', 'desc': 'VS-Benchæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤æ‚å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æˆ˜ç•¥æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚ä¸ç°æœ‰çš„å•æ™ºèƒ½ä½“æˆ–ä»…æ–‡æœ¬ç¯å¢ƒçš„åŸºå‡†ä¸åŒï¼ŒVS-Benchè€ƒè™‘äº†å¤šä¸ªæ™ºèƒ½ä½“åœ¨ä¸°å¯Œçš„è§†è§‰å’Œè¯­è¨€èƒŒæ™¯ä¸‹çš„äº’åŠ¨ã€‚è¯¥åŸºå‡†åŒ…æ‹¬å…«ä¸ªåŸºäºè§†è§‰çš„ç¯å¢ƒï¼Œæ¶µç›–åˆä½œã€ç«äº‰å’Œæ··åˆåŠ¨æœºçš„äº’åŠ¨ï¼Œè¯„ä¼°æ™ºèƒ½ä½“é¢„æµ‹ä»–äººæœªæ¥åŠ¨ä½œå’Œä¼˜åŒ–é•¿æœŸç›®æ ‡çš„èƒ½åŠ›ã€‚é€šè¿‡æ ‡å‡†åŒ–è¯„ä¼°ï¼ŒVS-Benchä¸ºæœªæ¥çš„æˆ˜ç•¥å¤šæ¨¡æ€æ™ºèƒ½ä½“ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03147', 'title': 'UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation', 'url': 'https://huggingface.co/papers/2506.03147', 'abstract': "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.  \t\t\t\t\tAI-generated summary \t\t\t\t Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets.", 'score': 48, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '34f1d96b37be24a1', 'authors': ['Bin Lin', 'Zongjian Li', 'Xinhua Cheng', 'Yuwei Niu', 'Yang Ye', 'Xianyi He', 'Shenghai Yuan', 'Wangbo Yu', 'Shaodong Wang', 'Yunyang Ge', 'Yatian Pang', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Peng Cheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.03147.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#open_source', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'UniWorld: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'UniWorld - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². UniWorld Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BAGEL Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'UniWorld: Efficient Image Manipulation with Semantic Power', 'desc': 'UniWorld is a new generative framework that enhances image perception and manipulation by utilizing semantic features from visual-language models. It outperforms the existing model BAGEL while using only 1% of its data, demonstrating efficiency in data usage. The framework leverages insights from the GPT-4o-Image model, which effectively uses semantic encoders instead of traditional Variational Autoencoders (VAEs). UniWorld not only excels in image editing tasks but also maintains strong capabilities in image understanding and generation across various applications.'}, 'zh': {'title': 'UniWorldï¼šå›¾åƒæ„ŸçŸ¥ä¸æ“ä½œçš„æ–°çºªå…ƒ', 'desc': 'UniWorldæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç‰¹å¾æ¥è¿›è¡Œå›¾åƒæ„ŸçŸ¥å’Œæ“ä½œã€‚ä¸BAGELç›¸æ¯”ï¼ŒUniWorldåœ¨æ•°æ®ä½¿ç”¨ä¸Šå‡å°‘äº†90%ï¼Œä½†åœ¨å›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä¼˜ã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢ä¿æŒç«äº‰åŠ›ï¼Œè¿˜åœ¨å¤šä¸ªå›¾åƒæ„ŸçŸ¥ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½çš„æ•ˆæœã€‚æˆ‘ä»¬å°†æ¨¡å‹æƒé‡ã€è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬ä»¥åŠæ•°æ®é›†å…¨éƒ¨å¼€æºï¼Œæ–¹ä¾¿ç ”ç©¶è€…ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02096', 'title': 'SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis', 'url': 'https://huggingface.co/papers/2506.02096', 'abstract': "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) trained via reinforcement learning with verifiable reward (RLVR) have shown notable progress in scaling test-time compute effectively. In this work, we investigate how synthesized RL data can further improve RLVR. To this end, we propose SynthRL-a scalable and guaranteed pipeline for automatic data scaling in reasoning-oriented RL training. SynthRL comprises three key stages: (1) selecting seed questions with appropriate distribution, (2) augmenting them into more challenging variants while preserving the original answers, and (3) a guaranteed verification stage that ensures near-perfect correctness and difficulty enhancement. Our empirical experiments demonstrate SynthRL's scalability and effectiveness. When applied to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable, challenging questions from approximately 8K seed samples. Models trained with our synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with a significant improvement over baseline models trained on seed data alone. Notably, detailed analysis reveals that the gains are more pronounced on the most challenging evaluation samples, highlighting SynthRL's effectiveness in eliciting deeper and more complex reasoning patterns.", 'score': 45, 'issue_id': 4110, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '9ea84a7af081829e', 'authors': ['Zijian Wu', 'Jinjie Ni', 'Xiangyan Liu', 'Zichen Liu', 'Hang Yan', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.02096.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#synthetic', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SynthRL: Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'SynthRL - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. SynthRL Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°: Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ¸Ğµ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'SynthRL: Elevating Visual Math Reasoning with Scalable Data Synthesis', 'desc': "SynthRL is a novel pipeline designed to enhance reinforcement learning (RL) by synthesizing data with verifiable rewards, specifically for visual math reasoning tasks. It operates in three stages: selecting initial questions, creating more challenging variants while keeping the answers intact, and verifying the correctness of these questions. This approach allows for the generation of over 3,300 additional verifiable questions from a smaller set of seed samples, significantly improving model performance. Empirical results show that models trained with SynthRL's data outperform those trained only on seed data, especially on difficult reasoning tasks, demonstrating its effectiveness in fostering advanced reasoning capabilities."}, 'zh': {'title': 'SynthRLï¼šæå‡è§†è§‰æ•°å­¦æ¨ç†çš„æ™ºèƒ½åˆæˆç®¡é“', 'desc': 'SynthRLæ˜¯ä¸€ç§å¯æ‰©å±•çš„æ•°æ®åˆæˆç®¡é“ï¼Œæ—¨åœ¨å¢å¼ºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰æ•°å­¦æ¨ç†æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§å’Œå¯éªŒè¯çš„é—®é¢˜ï¼Œæ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚SynthRLåŒ…æ‹¬ä¸‰ä¸ªå…³é”®é˜¶æ®µï¼šé€‰æ‹©åˆé€‚åˆ†å¸ƒçš„ç§å­é—®é¢˜ã€å°†å…¶å¢å¼ºä¸ºæ›´å…·æŒ‘æˆ˜æ€§çš„å˜ä½“ï¼Œå¹¶ç¡®ä¿ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œéš¾åº¦çš„æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SynthRLåˆæˆçš„æ•°æ®åœ¨å¤šä¸ªè§†è§‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æ ·æœ¬ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24120', 'title': 'CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs', 'url': 'https://huggingface.co/papers/2505.24120', 'abstract': 'A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remains inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering.Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning.We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6\\% accuracy.This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA.', 'score': 43, 'issue_id': 4116, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '09ca2aeec21b0156', 'authors': ['Ai Jian', 'Weijie Qiu', 'Xiaokun Wang', 'Peiyu Wang', 'Yunzhuo Hao', 'Jiangbo Pei', 'Yichen Wei', 'Yi Peng', 'Xuchen Song'], 'affiliations': ['Kunlun Inc.', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24120.jpg', 'data': {'categories': ['#science', '#multimodal', '#benchmark', '#reasoning'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'CSVQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CSVQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1378 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… STEM-Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑ†ĞµĞ½ĞºĞ° 15 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° CSVQA Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ 49.6%. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'CSVQA: Advancing Scientific Reasoning in Vision-Language Models', 'desc': 'The paper introduces CSVQA, a new benchmark designed to evaluate the scientific reasoning abilities of vision-language models (VLMs) through domain-specific visual question answering. Unlike existing benchmarks that focus on generic image understanding, CSVQA emphasizes the integration of domain knowledge and visual evidence in STEM contexts. It includes 1,378 question-answer pairs that require higher-order reasoning and authentic scientific content. The evaluation of 15 VLMs on this benchmark reveals significant performance gaps, highlighting the need for improvements in their scientific reasoning capabilities.'}, 'zh': {'title': 'CSVQAï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç§‘å­¦æ¨ç†èƒ½åŠ›', 'desc': 'CSVQAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡é¢†åŸŸç‰¹å®šçš„è§†è§‰é—®ç­”ï¼Œå¼ºè°ƒäº†è¿™äº›æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†ä¸­çš„ä¸è¶³ã€‚CSVQAåŒ…å«1378ä¸ªç²¾å¿ƒæ„å»ºçš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–å¤šä¸ªSTEMå­¦ç§‘ï¼Œè¦æ±‚æ¨¡å‹æ•´åˆé¢†åŸŸçŸ¥è¯†å’Œè§†è§‰è¯æ®è¿›è¡Œé«˜é˜¶æ¨ç†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡æœ‰äº›æ¨¡å‹è¡¨ç°è¾ƒå¥½ï¼Œä½†æœ€é«˜åˆ†çš„æ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º49.6%ï¼Œè¿™è¡¨æ˜åœ¨ç§‘å­¦æ¨ç†èƒ½åŠ›ä¸Šä»éœ€è¿›ä¸€æ­¥æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24714', 'title': 'FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation', 'url': 'https://huggingface.co/papers/2505.24714', 'abstract': 'FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.', 'score': 32, 'issue_id': 4110, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'a6dcbb10b5be7f41', 'authors': ['Junyu Luo', 'Zhizhuo Kou', 'Liming Yang', 'Xiao Luo', 'Jinsheng Huang', 'Zhiping Xiao', 'Jingshu Peng', 'Chengzhong Liu', 'Jiaming Ji', 'Xuanzhe Liu', 'Sirui Han', 'Ming Zhang', 'Yike Guo'], 'affiliations': ['HKUST', 'School of Computer Science, Peking University', 'State Key Laboratory for Multimedia Information Processing, PKU-Anker LLM Lab', 'University of California, Los Angeles', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.24714.jpg', 'data': {'categories': ['#dataset', '#hallucinations', '#science', '#multimodal', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'FinMME Ğ¸ FinScore: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ', 'desc': 'FinMME - ÑÑ‚Ğ¾ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 11 000 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸Ğ· 18 Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ° ĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹. FinScore - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4o Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° FinMME, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°.'}, 'en': {'title': 'FinMME: Elevating Financial AI with Robust Evaluation', 'desc': "The paper introduces FinMME, a new multimodal dataset specifically designed for financial research, which includes over 11,000 samples across various financial domains and asset classes. It addresses the gap in specialized evaluation datasets for Multimodal Large Language Models (MLLMs) in finance. The authors also present FinScore, an evaluation system that assesses model performance while penalizing inaccuracies and measuring multiple capabilities. Experimental results reveal that even advanced models like GPT-4o struggle with the challenges posed by FinMME, underscoring the dataset's robustness and reliability."}, 'zh': {'title': 'æ¨åŠ¨é‡‘èé¢†åŸŸçš„å¤šæ¨¡æ€ç ”ç©¶', 'desc': 'FinMMEæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€é‡‘èç ”ç©¶æ•°æ®é›†ï¼Œæ—¨åœ¨æ¨åŠ¨é‡‘èé¢†åŸŸçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘å±•ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡11,000ä¸ªé«˜è´¨é‡çš„é‡‘èç ”ç©¶æ ·æœ¬ï¼Œæ¶µç›–18ä¸ªé‡‘èé¢†åŸŸå’Œ6ç§èµ„äº§ç±»åˆ«ï¼Œæä¾›10ç§ä¸»è¦å›¾è¡¨ç±»å‹å’Œ21ç§å­ç±»å‹ã€‚ä¸ºäº†ç¡®ä¿æ•°æ®è´¨é‡ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨äº†20åæ³¨é‡Šå‘˜å’Œç²¾å¿ƒè®¾è®¡çš„éªŒè¯æœºåˆ¶ã€‚æ­¤å¤–ï¼ŒFinScoreè¯„ä¼°ç³»ç»Ÿå¼•å…¥äº†å¹»è§‰æƒ©ç½šå’Œå¤šç»´èƒ½åŠ›è¯„ä¼°ï¼Œä»¥æä¾›å…¬æ­£çš„è¯„ä¼°ç»“æœï¼Œå°½ç®¡å…ˆè¿›æ¨¡å‹å¦‚GPT-4oåœ¨FinMMEä¸Šè¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºå…¶æŒ‘æˆ˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02397', 'title': 'OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation', 'url': 'https://huggingface.co/papers/2506.02397', 'abstract': 'OThink-R1 is introduced to reduce reasoning redundancy in complex problem-solving by classifying reasoning steps as essential or redundant and dynamically switching thinking modes based on task complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advanced large reasoning models (LRMs) leverage extended chain-of-thought (CoT) reasoning to solve complex tasks, achieving state-of-the-art performance. Despite their success, we identify a critical issue: a substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary. To address this, we systematically analyze the reasoning trajectories of LRMs and present a method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking) for straightforward problems while engaging in deliberate thinking (slow-thinking) for complex problems. Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23\\% on average without compromising accuracy, offering practical guidelines for efficient reasoning models. The code is available at https://github.com/AgenticIR-Lab/OThink-R1.', 'score': 28, 'issue_id': 4120, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '12cee2b297810e65', 'authors': ['Shengjia Zhang', 'Junjie Wu', 'Jiawei Chen', 'Changwang Zhang', 'Xingyu Lou', 'Wangchunshu Zhou', 'Sheng Zhou', 'Can Wang', 'Jun Wang'], 'affiliations': ['OPPO Research Institute, Shenzhen, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.02397.jpg', 'data': {'categories': ['#training', '#math', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'OThink-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. OThink-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¸ LLM-Judge Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OThink-R1 ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 23% Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimize Reasoning: Think Smart, Not Hard!', 'desc': 'OThink-R1 is a novel approach designed to enhance reasoning efficiency in complex problem-solving by distinguishing between essential and redundant reasoning steps. It leverages a classification system to identify when complex reasoning is unnecessary, allowing for a switch between fast-thinking and slow-thinking modes based on task complexity. This method significantly reduces reasoning redundancy by approximately 23% while maintaining accuracy in tasks like mathematics and question-answering. The findings suggest that not all tasks require extensive reasoning, and OThink-R1 provides a framework for optimizing reasoning processes in large reasoning models.'}, 'zh': {'title': 'åŠ¨æ€æ€ç»´æ¨¡å¼ï¼Œå‡å°‘æ¨ç†å†—ä½™', 'desc': 'OThink-R1 æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å¤æ‚é—®é¢˜è§£å†³ä¸­çš„æ¨ç†å†—ä½™ã€‚å®ƒé€šè¿‡å°†æ¨ç†æ­¥éª¤åˆ†ç±»ä¸ºå¿…è¦æˆ–å†—ä½™ï¼Œå¹¶æ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€åˆ‡æ¢æ€ç»´æ¨¡å¼ã€‚å¯¹äºç®€å•é—®é¢˜ï¼ŒOThink-R1 ä½¿ç”¨å¿«é€Ÿæ€ç»´æ¨¡å¼ï¼Œè€Œå¯¹äºå¤æ‚é—®é¢˜åˆ™é‡‡ç”¨æ·±æ€ç†Ÿè™‘çš„æ…¢æ€ç»´æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒOThink-R1 å¹³å‡å‡å°‘äº†è¿‘23%çš„æ¨ç†å†—ä½™ï¼ŒåŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ï¼Œä¸ºé«˜æ•ˆæ¨ç†æ¨¡å‹æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00123', 'title': 'Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces', 'url': 'https://huggingface.co/papers/2506.00123', 'abstract': 'VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable progress of Multimodal Large Language Models (MLLMs) has attracted increasing attention to extend them to physical entities like legged robot. This typically requires MLLMs to not only grasp multimodal understanding abilities, but also integrate visual-spatial reasoning and physical interaction capabilities. Nevertheless,existing methods struggle to unify these capabilities due to their fundamental differences.In this paper, we present the Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning, and control in real world. VeBrain reformulates robotic control into common text-based MLLM tasks in the 2D visual space, thus unifying the objectives and mapping spaces of different tasks. Then, a novel robotic adapter is proposed to convert textual control signals from MLLMs to motion policies of real robots. From the data perspective, we further introduce VeBrain-600k, a high-quality instruction dataset encompassing various capabilities of VeBrain. In VeBrain-600k, we take hundreds of hours to collect, curate and annotate the data, and adopt multimodal chain-of-thought(CoT) to mix the different capabilities into a single conversation. Extensive experiments on 13 multimodal benchmarks and 5 spatial intelligence benchmarks demonstrate the superior performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to legged robots and robotic arms, VeBrain shows strong adaptability, flexibility, and compositional capabilities compared to existing methods. For example, compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by +5.6%, but also excels in legged robot tasks with +50% average gains.', 'score': 28, 'issue_id': 4114, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '8914927f73dff147', 'authors': ['Gen Luo', 'Ganlin Yang', 'Ziyang Gong', 'Guanzhou Chen', 'Haonan Duan', 'Erfei Cui', 'Ronglei Tong', 'Zhi Hou', 'Tianyi Zhang', 'Zhe Chen', 'Shenglong Ye', 'Lewei Lu', 'Jingbo Wang', 'Wenhai Wang', 'Jifeng Dai', 'Yu Qiao', 'Rongrong Ji', 'Xizhou Zhu'], 'affiliations': ['Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00123.jpg', 'data': {'categories': ['#games', '#robotics', '#multimodal', '#reasoning', '#benchmark', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'VeBrain: Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ·Ğ³ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'VeBrain - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ½Ğ¾Ğ³Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. ĞĞ½Ğ° Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² 2D Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. VeBrain Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'VeBrain: Unifying Multimodal Intelligence for Advanced Robotic Control', 'desc': 'VeBrain is a new framework designed to enhance the capabilities of legged robots by combining multimodal understanding, visual-spatial reasoning, and physical interaction. It reformulates robotic control tasks into text-based tasks that can be processed by Multimodal Large Language Models (MLLMs), allowing for a unified approach to different robotic challenges. The framework includes a unique robotic adapter that translates textual commands from MLLMs into actionable motion policies for robots. Extensive testing shows that VeBrain outperforms existing models, achieving significant improvements in various benchmarks and demonstrating its adaptability in real-world robotic applications.'}, 'zh': {'title': 'VeBrainï¼šå››è¶³æœºå™¨äººæ™ºèƒ½æ§åˆ¶çš„æ–°æ¡†æ¶', 'desc': 'VeBrainæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤šæ¨¡æ€ç†è§£ã€è§†è§‰ç©ºé—´æ¨ç†å’Œç‰©ç†äº¤äº’æ•´åˆåˆ°å››è¶³æœºå™¨äººä¸­ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æœºå™¨äººæ§åˆ¶é‡æ–°è¡¨è¿°ä¸ºåŸºäºæ–‡æœ¬çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»»åŠ¡ï¼Œä»è€Œç»Ÿä¸€äº†ä¸åŒä»»åŠ¡çš„ç›®æ ‡å’Œæ˜ å°„ç©ºé—´ã€‚VeBrainè¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æœºå™¨äººé€‚é…å™¨ï¼Œå°†MLLMçš„æ–‡æœ¬æ§åˆ¶ä¿¡å·è½¬æ¢ä¸ºçœŸå®æœºå™¨äººçš„è¿åŠ¨ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒVeBrain-600kæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œæ¶µç›–äº†VeBrainçš„å¤šç§èƒ½åŠ›ï¼Œç»è¿‡å¤§é‡æ•°æ®æ”¶é›†å’Œæ³¨é‡Šï¼Œå±•ç¤ºäº†VeBrainåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03135', 'title': 'OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models', 'url': 'https://huggingface.co/papers/2506.03135', 'abstract': "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning is a key aspect of cognitive psychology and remains a major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research.", 'score': 27, 'issue_id': 4116, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'a7cd11e4c04b1336', 'authors': ['Mengdi Jia', 'Zekun Qi', 'Shaochen Zhang', 'Wenyao Zhang', 'Xinqiang Yu', 'Jiawei He', 'He Wang', 'Li Yi'], 'affiliations': ['Galbot', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03135.jpg', 'data': {'categories': ['#cv', '#benchmark', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'OmniSpatial: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'OmniSpatial - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'OmniSpatial: Elevating Spatial Reasoning in Vision-Language Models', 'desc': 'This paper introduces OmniSpatial, a new benchmark designed to assess the capabilities of vision-language models (VLMs) in advanced spatial reasoning tasks. It highlights that while VLMs can handle basic spatial relations, they struggle with more complex reasoning, which is crucial for understanding human-like spatial interactions. The benchmark includes four main categories of spatial reasoning, with a total of 50 detailed subcategories, and is supported by over 1,500 carefully crafted question-answer pairs. The findings reveal significant shortcomings in current VLMs, prompting a discussion on future research directions to enhance their spatial reasoning abilities.'}, 'zh': {'title': 'OmniSpatialï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºOmniSpatialçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨é«˜çº§ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„ç†è§£èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€æ¨ç†ã€å¤æ‚ç©ºé—´é€»è¾‘ã€ç©ºé—´äº¤äº’å’Œè§†è§’è½¬æ¢ç­‰æ–¹é¢ã€‚OmniSpatialåŸºäºè®¤çŸ¥å¿ƒç†å­¦ï¼Œæ¶µç›–äº†50ä¸ªç»†åˆ†ç±»åˆ«ï¼Œå¹¶é€šè¿‡ç½‘ç»œæ•°æ®çˆ¬å–å’Œäººå·¥æ ‡æ³¨æ„å»ºäº†1500å¤šä¸ªé—®ç­”å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯å¼€æºè¿˜æ˜¯é—­æºçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œåœ¨å…¨é¢çš„ç©ºé—´ç†è§£ä¸Šéƒ½è¡¨ç°ä¸ä½³ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†å¤±è´¥æ¡ˆä¾‹å¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03143', 'title': 'GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents', 'url': 'https://huggingface.co/papers/2506.03143', 'abstract': 'GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.', 'score': 25, 'issue_id': 4112, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '34d1779dffecf9d8', 'authors': ['Qianhui Wu', 'Kanzhi Cheng', 'Rui Yang', 'Chaoyun Zhang', 'Jianwei Yang', 'Huiqiang Jiang', 'Jian Mu', 'Baolin Peng', 'Bo Qiao', 'Reuben Tan', 'Si Qin', 'Lars Liden', 'Qingwei Lin', 'Huan Zhang', 'Tong Zhang', 'Jianbing Zhang', 'Dongmei Zhang', 'Jianfeng Gao'], 'affiliations': ['Microsoft', 'Nanjing University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.03143.jpg', 'data': {'categories': ['#multimodal', '#training', '#cv', '#benchmark', '#optimization', '#games'], 'emoji': 'ğŸ–±ï¸', 'ru': {'title': 'GUI-Actor: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VLM', 'desc': 'GUI-Actor - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ĞµĞ·ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. ĞĞ½ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. GUI-Actor Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ <ACTOR> ÑĞ¾ Ğ²ÑĞµĞ¼Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing GUI Grounding with Coordinate-Free Attention', 'desc': 'The paper introduces GUI-Actor, a novel method for visual grounding in graphical user interfaces (GUIs) that leverages vision-language models (VLMs) and attention mechanisms. Unlike traditional approaches that generate screen coordinates, GUI-Actor operates in a coordinate-free manner, aligning an action token with relevant visual patches to identify action regions efficiently. This method addresses limitations such as weak spatial-semantic alignment and ambiguity in supervision targets, leading to better generalization across different screen layouts. The results demonstrate that GUI-Actor significantly outperforms existing methods, achieving state-of-the-art performance on various benchmarks while allowing for efficient fine-tuning.'}, 'zh': {'title': 'GUI-Actorï¼šæ— åæ ‡çš„é«˜æ•ˆGUIå®šä½æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGUI-Actorçš„åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ— åæ ‡çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å®šä½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åŸºäºæ³¨æ„åŠ›çš„åŠ¨ä½œå¤´ï¼Œèƒ½å¤Ÿåœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­å°†ç‰¹å®šçš„<ACTOR>æ ‡è®°ä¸ç›¸å…³çš„è§†è§‰è¡¥ä¸æ ‡è®°å¯¹é½ï¼Œä»è€Œæœ‰æ•ˆåœ°æå‡ºä¸€ä¸ªæˆ–å¤šä¸ªåŠ¨ä½œåŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGUI-Actoråœ¨å¤šä¸ªGUIåŠ¨ä½œå®šä½åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„å±å¹•åˆ†è¾¨ç‡å’Œå¸ƒå±€ä¸Šå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥éªŒè¯å™¨ï¼ŒGUI-Actorèƒ½å¤Ÿåœ¨ä¿æŒVLMä¸»å¹²ä¸å˜çš„æƒ…å†µä¸‹ï¼Œä»…å¾®è°ƒæ–°å¼•å…¥çš„åŠ¨ä½œå¤´ï¼Œä¾¿èƒ½å®ç°ä¸ä¹‹å‰æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23061', 'title': 'DINGO: Constrained Inference for Diffusion LLMs', 'url': 'https://huggingface.co/papers/2505.23061', 'abstract': "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference", 'score': 22, 'issue_id': 4112, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': 'c215c7998d0a7928', 'authors': ['Tarun Suresh', 'Debangshu Banerjee', 'Shubham Ugare', 'Sasa Misailovic', 'Gagandeep Singh'], 'affiliations': ['Department of Computer Science, University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.23061.jpg', 'data': {'categories': ['#training', '#architecture', '#benchmark', '#optimization', '#diffusion'], 'emoji': 'ğŸ§®', 'ru': {'title': 'DINGO: Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DINGO - Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DINGO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ JSON. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ±Ğ»Ğ¾Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸. DINGO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ñ Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ, ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'DINGO: Structuring Success in Diffusion Language Models', 'desc': 'DINGO is a new decoding strategy that improves diffusion language models by applying structured output constraints. This method allows the models to generate outputs that meet specific requirements, like those found in symbolic math and JSON formats. Unlike traditional autoregressive models that generate text one token at a time, DINGO uses dynamic programming to efficiently handle multiple tokens simultaneously while ensuring the output adheres to user-defined rules. As a result, DINGO significantly enhances the performance of diffusion models, achieving up to a 68 percentage point improvement in relevant tasks.'}, 'zh': {'title': 'DINGOï¼šæå‡æ‰©æ•£æ¨¡å‹çš„ç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›', 'desc': 'DINGOæ˜¯ä¸€ç§åŸºäºåŠ¨æ€è§„åˆ’çš„è§£ç ç­–ç•¥ï¼Œæ—¨åœ¨å¢å¼ºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å¼ºåˆ¶æ‰§è¡Œç»“æ„åŒ–è¾“å‡ºçº¦æŸï¼Œæ˜¾è‘—æé«˜äº†åœ¨ç¬¦å·æ•°å­¦å’ŒJSONç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ä¸åŒï¼Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œé¢„æµ‹ä¸€ç»„æ ‡è®°ï¼Œè¿™ä½¿å¾—ä¼ ç»Ÿçš„çº¦æŸè§£ç ç®—æ³•æ— æ³•æœ‰æ•ˆåº”ç”¨ã€‚DINGOé€šè¿‡é«˜æ•ˆä¸”å¯è¯æ˜çš„åˆ†å¸ƒä¿æŒæ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºå­—ç¬¦ä¸²ç¬¦åˆç”¨æˆ·æŒ‡å®šçš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œä»è€Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01674', 'title': 'MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2506.01674', 'abstract': "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advancements in Multimodal Large Language Models (MLLMs), their proficiency in fine-grained video motion understanding remains critically limited. They often lack inter-frame differencing and tend to average or ignore subtle visual cues. Furthermore, while visual prompting has shown potential in static images, its application to video's temporal complexities, particularly for fine-grained motion understanding, remains largely unexplored. We investigate whether inherent capability can be unlocked and boost MLLMs' motion perception and enable distinct visual signatures tailored to decouple object and camera motion cues. In this study, we introduce MotionSight, a novel zero-shot method pioneering object-centric visual spotlight and motion blur as visual prompts to effectively improve fine-grained motion understanding without training. To convert this into valuable data assets, we curated MotionVid-QA, the first large-scale dataset for fine-grained video motion understanding, with hierarchical annotations including SFT and preference data, {\\Theta}(40K) video clips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models. In particular, for fine-grained motion understanding we present a novel zero-shot technique and a large-scale, high-quality dataset. All the code and annotations will be publicly available.", 'score': 21, 'issue_id': 4110, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'baebffe54058ae77', 'authors': ['Yipeng Du', 'Tiehan Fan', 'Kepan Nan', 'Rui Xie', 'Penghao Zhou', 'Xiang Li', 'Jian Yang', 'Zhenheng Yang', 'Ying Tai'], 'affiliations': ['ByteDance', 'Nanjing University', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01674.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#open_source', '#games'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ: MotionSight ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'MotionSight - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MotionVid-QA Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MotionSight Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unlocking Fine-Grained Motion Understanding with MotionSight', 'desc': 'MotionSight is a novel zero-shot method designed to enhance fine-grained video motion understanding by utilizing object-centric visual prompts and motion blur. This approach addresses the limitations of Multimodal Large Language Models (MLLMs) in capturing subtle motion cues across video frames. By introducing a large-scale dataset called MotionVid-QA, which includes hierarchical annotations, the method allows for effective evaluation and improvement of motion perception without the need for extensive training. The results demonstrate that MotionSight achieves state-of-the-art performance, showcasing its potential to unlock advanced motion understanding capabilities in videos.'}, 'zh': {'title': 'MotionSightï¼šæå‡è§†é¢‘è¿åŠ¨ç†è§£çš„æ–°æ–¹æ³•', 'desc': 'MotionSightæ˜¯ä¸€ç§é›¶æ ·æœ¬æ–¹æ³•ï¼Œåˆ©ç”¨ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è§†è§‰èšç„¦å’Œè¿åŠ¨æ¨¡ç³Šä½œä¸ºæç¤ºï¼Œæ˜¾è‘—æå‡äº†ç»†ç²’åº¦è§†é¢‘è¿åŠ¨ç†è§£çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨MotionVid-QAæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯¥æ•°æ®é›†å…·æœ‰å±‚æ¬¡åŒ–çš„æ³¨é‡Šã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘è¿åŠ¨ç†è§£æ–¹é¢å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç»†å¾®çš„è§†è§‰çº¿ç´¢æ—¶ã€‚é€šè¿‡å¼•å…¥MotionSightï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨è§†è§‰æç¤ºæ¥è§£è€¦ç‰©ä½“å’Œç›¸æœºè¿åŠ¨çº¿ç´¢ï¼Œä»è€Œæé«˜è¿åŠ¨æ„ŸçŸ¥èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03065', 'title': 'Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.03065', 'abstract': 'Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical FLOP reduction, and actual inference speedups of 1.76times, 1.85times, and 1.58times, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis.', 'score': 20, 'issue_id': 4114, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'c51b4ca89c790b8c', 'authors': ['Pengtao Chen', 'Xianfang Zeng', 'Maosen Zhao', 'Peng Ye', 'Mingzhu Shen', 'Wei Cheng', 'Gang Yu', 'Tao Chen'], 'affiliations': ['Fudan University', 'Imperial College London', 'StepFun', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.03065.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#inference', '#diffusion'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Sparse-vDiT Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Video Diffusion Transformer (vDiT), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² ĞºĞ°Ñ€Ñ‚Ğ°Ñ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ğ¾Ğ»Ğ¾ÑĞ°Ñ‚Ñ‹Ğ¹. Sparse-vDiT Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Sparse-vDiT Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ vDiT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Accelerating Video Generation with Sparse Attention Patterns', 'desc': 'The paper introduces Sparse-vDiT, a framework designed to enhance the efficiency of Video Diffusion Transformers (vDiT) by utilizing identified sparsity patterns in attention maps. By analyzing these patterns, the authors found that certain attention heads can be skipped, leading to reduced computational complexity and faster inference times. Sparse-vDiT employs pattern-optimized sparse kernels and an offline search algorithm to determine the best sparse computation strategy for each layer and head. This approach results in significant reductions in theoretical FLOPs and actual inference speedups while preserving high visual quality in generated videos.'}, 'zh': {'title': 'åˆ©ç”¨ç¨€ç–æ€§åŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'Sparse-vDiTé€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›å›¾ä¸­çš„ç¨€ç–æ¨¡å¼ï¼ŒåŠ é€Ÿäº†è§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼ˆvDiTï¼‰ï¼Œåœ¨ä¸æ˜¾è‘—é™ä½è§†è§‰è´¨é‡çš„æƒ…å†µä¸‹ï¼Œå‡å°‘äº†ç†è®ºè®¡ç®—é‡ï¼ˆFLOPsï¼‰å¹¶æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚ç ”ç©¶å‘ç°ï¼ŒvDiTä¸­çš„æ³¨æ„åŠ›å›¾å­˜åœ¨å¯¹è§’çº¿ã€å¤šå¯¹è§’çº¿å’Œå‚ç›´æ¡çº¹ç­‰ä¸‰ç§ç¨€ç–æ¨¡å¼ï¼Œå¹¶ä¸”å¯ä»¥è·³è¿‡3-6%çš„æ³¨æ„åŠ›å¤´ã€‚æˆ‘ä»¬æå‡ºçš„Sparse-vDiTæ¡†æ¶åŒ…æ‹¬ä¼˜åŒ–ç¨€ç–å†…æ ¸å’Œç¦»çº¿ç¨€ç–æ‰©æ•£æœç´¢ç®—æ³•ï¼Œä»¥é€‰æ‹©æ¯å±‚å’Œæ¯ä¸ªå¤´çš„æœ€ä½³ç¨€ç–è®¡ç®—ç­–ç•¥ã€‚é€šè¿‡å°†Sparse-vDiTé›†æˆåˆ°æœ€å…ˆè¿›çš„vDiTæ¨¡å‹ä¸­ï¼Œå–å¾—äº†æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æå‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è§†è§‰ä¿çœŸåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00070', 'title': 'Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics', 'url': 'https://huggingface.co/papers/2506.00070', 'abstract': 'Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning.', 'score': 19, 'issue_id': 4114, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '0a2372063dd0aba7', 'authors': ['Dongyoung Kim', 'Sumin Park', 'Huiwon Jang', 'Jinwoo Shin', 'Jaehyung Kim', 'Younggyo Seo'], 'affiliations': ['KAIST', 'Real World Inc.', 'UC Berkeley', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00070.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Robot-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Robot-R1 Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Supervised Fine-Tuning (SFT), Robot-R1 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Robot-R1, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ SFT Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Robot Control with Robot-R1', 'desc': 'This paper presents Robot-R1, a new framework that improves robot control by using reinforcement learning instead of traditional Supervised Fine-Tuning (SFT). SFT often suffers from issues like catastrophic forgetting and poorly constructed datasets, which can hinder robot performance. Robot-R1 focuses on predicting the next keypoint state needed for task completion by utilizing current scene images and expert demonstration data. The results show that Robot-R1 outperforms SFT methods, even with fewer parameters, in tasks requiring low-level action control and embodied reasoning.'}, 'zh': {'title': 'Robot-R1ï¼šæå‡æœºå™¨äººæ§åˆ¶çš„å…·èº«æ¨ç†æ–°æ¡†æ¶', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æœºå™¨äººæŠ€æœ¯ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œé€šè¿‡ç»“åˆå…·èº«æ¨ç†ä¸æœºå™¨äººæ§åˆ¶æ¥æ¨åŠ¨è¿›æ­¥ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨è®­ç»ƒæ—¶å¸¸å¸¸ä½¿ç”¨å¯å‘å¼æ„å»ºçš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¹¶æœªé’ˆå¯¹æœºå™¨äººæ§åˆ¶è¿›è¡Œä¼˜åŒ–ã€‚SFTæ–¹æ³•è¿˜å¯èƒ½å¯¼è‡´ç¾éš¾æ€§é—å¿˜å’Œæ³›åŒ–æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Robot-R1æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæœºå™¨äººæ§åˆ¶çš„å…·èº«æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03136', 'title': 'Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.03136', 'abstract': "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE", 'score': 18, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '6a5362c4ed28a3b0', 'authors': ['Yinjie Wang', 'Ling Yang', 'Ye Tian', 'Ke Shen', 'Mengdi Wang'], 'affiliations': ['ByteDance', 'Peking University', 'Princeton University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2506.03136.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#rl', '#games'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CURE: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ²', 'desc': 'CURE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ReasonFlux-Coder, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ CURE, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºÑƒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ° Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¼ĞµĞ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CURE Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'CURE: Evolving Code and Tests Together for Better Accuracy', 'desc': "CURE is a reinforcement learning framework designed to enhance the accuracy of code and unit test generation without relying on ground-truth supervision. It features a unique reward system that allows coding and testing processes to evolve together based on their interactions. This method enables the unit tester to learn from the coder's errors, leading to improved performance in various coding tasks. The framework's models, such as ReasonFlux-Coder, show significant accuracy improvements over existing models, making it a powerful tool for developers."}, 'zh': {'title': 'CUREï¼šæ— ç›‘ç£å¼ºåŒ–å­¦ä¹ æå‡ä»£ç ç”Ÿæˆä¸æµ‹è¯•å‡†ç¡®æ€§', 'desc': 'CUREæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ä»£ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œè€Œæ— éœ€çœŸå®æ ‡ç­¾çš„ç›‘ç£ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡ä¸“é—¨çš„å¥–åŠ±æœºåˆ¶ï¼Œä½¿ç¼–ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆèƒ½åŠ›èƒ½å¤Ÿç›¸äº’æ¼”åŒ–ï¼Œä»è€Œç›´æ¥ä»ç¼–ç è€…çš„é”™è¯¯ä¸­å­¦ä¹ ã€‚ç»è¿‡ä¼˜åŒ–åï¼Œæˆ‘ä»¬çš„ReasonFlux-Coderæ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå‡†ç¡®æ€§ä¸Šæé«˜äº†5.3%ï¼Œåœ¨æœ€ä½³å‡†ç¡®æ€§ä¸Šæé«˜äº†9.0%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­è¡¨ç°å‡º64.8%çš„æ¨ç†æ•ˆç‡ï¼Œå±•ç°äº†å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03131', 'title': 'Native-Resolution Image Synthesis', 'url': 'https://huggingface.co/papers/2506.03131', 'abstract': 'A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce native-resolution image synthesis, a novel generative modeling paradigm that enables the synthesis of images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of conventional fixed-resolution, square-image methods by natively handling variable-length visual tokens, a core challenge for traditional techniques. To this end, we introduce the Native-resolution diffusion Transformer (NiT), an architecture designed to explicitly model varying resolutions and aspect ratios within its denoising process. Free from the constraints of fixed formats, NiT learns intrinsic visual distributions from images spanning a broad range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced large language models, NiT, trained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536) and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies.', 'score': 16, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '91e9b714c6e83924', 'authors': ['Zidong Wang', 'Lei Bai', 'Xiangyu Yue', 'Wanli Ouyang', 'Yiyuan Zhang'], 'affiliations': ['MMLab, CUHK', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.03131.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#synthetic', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'NiT: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ NiT (Native-resolution diffusion Transformer), ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. NiT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ImageNet Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ zero-shot Ğ½Ğ° Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ²Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Image Synthesis with Native-Resolution Modeling', 'desc': 'The paper presents a new generative model called the Native-resolution diffusion Transformer (NiT) that can create high-resolution images with various aspect ratios. Unlike traditional models that work with fixed-size images, NiT can handle images of any size by using variable-length visual tokens. This model not only achieves top performance on standard image benchmarks but also shows impressive zero-shot generalization, meaning it can generate high-quality images at new resolutions without additional training. The findings suggest that NiT could significantly advance the field of image synthesis by integrating techniques from both visual generative modeling and large language models.'}, 'zh': {'title': 'åŸç”Ÿåˆ†è¾¨ç‡å»ºæ¨¡ï¼šå›¾åƒç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆæ¨¡å‹â€”â€”åŸç”Ÿåˆ†è¾¨ç‡æ‰©æ•£å˜æ¢å™¨ï¼ˆNiTï¼‰ï¼Œå®ƒèƒ½å¤Ÿåˆæˆé«˜åˆ†è¾¨ç‡å’Œå¤šç§å®½é«˜æ¯”çš„å›¾åƒï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿå›ºå®šåˆ†è¾¨ç‡å›¾åƒæ–¹æ³•çš„å±€é™ï¼Œé€šè¿‡åŸç”Ÿå¤„ç†å¯å˜é•¿åº¦çš„è§†è§‰æ ‡è®°ï¼Œè§£å†³äº†ä¼ ç»ŸæŠ€æœ¯çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚NiTæ¶æ„ä¸“é—¨è®¾è®¡ç”¨äºåœ¨å»å™ªè¿‡ç¨‹ä¸­æ˜¾å¼å»ºæ¨¡ä¸åŒçš„åˆ†è¾¨ç‡å’Œå®½é«˜æ¯”ï¼Œèƒ½å¤Ÿä»å„ç§åˆ†è¾¨ç‡å’Œå®½é«˜æ¯”çš„å›¾åƒä¸­å­¦ä¹ å†…åœ¨çš„è§†è§‰åˆ†å¸ƒã€‚ç ”ç©¶è¡¨æ˜ï¼ŒNiTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œå±•ç¤ºäº†åŸç”Ÿåˆ†è¾¨ç‡å»ºæ¨¡åœ¨è§†è§‰ç”Ÿæˆå»ºæ¨¡å’Œå…ˆè¿›å¤§è¯­è¨€æ¨¡å‹æ–¹æ³•ä¹‹é—´çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03126', 'title': 'AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.03126', 'abstract': 'AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in AI-generated content (AIGC) have significantly accelerated animation production. To produce engaging animations, it is essential to generate coherent multi-shot video clips with narrative scripts and character references. However, existing public datasets primarily focus on real-world scenarios with global descriptions, and lack reference images for consistent character guidance. To bridge this gap, we present AnimeShooter, a reference-guided multi-shot animation dataset. AnimeShooter features comprehensive hierarchical annotations and strong visual consistency across shots through an automated pipeline. Story-level annotations provide an overview of the narrative, including the storyline, key scenes, and main character profiles with reference images, while shot-level annotations decompose the story into consecutive shots, each annotated with scene, characters, and both narrative and descriptive visual captions. Additionally, a dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each shot, along with audio descriptions and sound sources. To demonstrate the effectiveness of AnimeShooter and establish a baseline for the reference-guided multi-shot video generation task, we introduce AnimeShooterGen, which leverages Multimodal Large Language Models (MLLMs) and video diffusion models. The reference image and previously generated shots are first processed by MLLM to produce representations aware of both reference and context, which are then used as the condition for the diffusion model to decode the subsequent shot. Experimental results show that the model trained on AnimeShooter achieves superior cross-shot visual consistency and adherence to reference visual guidance, which highlight the value of our dataset for coherent animated video generation.', 'score': 15, 'issue_id': 4114, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '3cc5928482bd8262', 'authors': ['Lu Qiu', 'Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.03126.jpg', 'data': {'categories': ['#video', '#multimodal', '#story_generation', '#diffusion', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'AnimeShooter: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'AnimeShooter - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AnimeShooterGen, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AnimeShooterGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Enhancing Animation with Reference-Guided Multi-Shot Datasets', 'desc': 'AnimeShooter is a new dataset designed to improve the generation of coherent multi-shot animations by providing detailed hierarchical annotations and ensuring visual consistency. It includes story-level annotations that outline the narrative and character profiles, as well as shot-level annotations that break down the story into individual scenes with specific details. The dataset also features a subset with synchronized audio tracks to enhance the animation experience. To utilize this dataset, AnimeShooterGen employs Multimodal Large Language Models (MLLMs) and video diffusion models, resulting in improved visual consistency and adherence to character references in generated animations.'}, 'zh': {'title': 'æå‡åŠ¨ç”»ç”Ÿæˆçš„è¿è´¯æ€§ä¸ä¸€è‡´æ€§', 'desc': 'AnimeShooteræ˜¯ä¸€ä¸ªå‚è€ƒå¼•å¯¼çš„å¤šé•œå¤´åŠ¨ç”»æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜è¿è´¯çš„åŠ¨ç”»è§†é¢‘ç”Ÿæˆã€‚è¯¥æ•°æ®é›†é€šè¿‡å…¨é¢çš„å±‚æ¬¡æ³¨é‡Šå’Œè§†è§‰ä¸€è‡´æ€§ï¼Œå¸®åŠ©ç”Ÿæˆå…·æœ‰å™äº‹è„šæœ¬å’Œè§’è‰²å‚è€ƒçš„åŠ¨ç”»ç‰‡æ®µã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†AnimeShooterGenï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå–å¾—äº†æ›´å¥½çš„ç”Ÿæˆæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºAnimeShooterè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨é•œå¤´è§†è§‰ä¸€è‡´æ€§å’Œéµå¾ªå‚è€ƒè§†è§‰æŒ‡å¯¼æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02497', 'title': 'LumosFlow: Motion-Guided Long Video Generation', 'url': 'https://huggingface.co/papers/2506.02497', 'abstract': 'LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation has gained increasing attention due to its widespread applications in fields such as entertainment and simulation. Despite advances, synthesizing temporally coherent and visually compelling long sequences remains a formidable challenge. Conventional approaches often synthesize long videos by sequentially generating and concatenating short clips, or generating key frames and then interpolate the intermediate frames in a hierarchical manner. However, both of them still remain significant challenges, leading to issues such as temporal repetition or unnatural transitions. In this paper, we revisit the hierarchical long video generation pipeline and introduce LumosFlow, a framework introduce motion guidance explicitly. Specifically, we first employ the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames with larger motion intervals, thereby ensuring content diversity in the generated long videos. Given the complexity of interpolating contextual transitions between key frames, we further decompose the intermediate frame interpolation into motion generation and post-hoc refinement. For each pair of key frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes complex and large-motion optical flows, while MotionControlNet subsequently refines the warped results to enhance quality and guide intermediate frame generation. Compared with traditional video frame interpolation, we achieve 15x interpolation, ensuring reasonable and continuous motion between adjacent frames. Experiments show that our method can generate long videos with consistent motion and appearance. Code and models will be made publicly available upon acceptance. Our project page: https://jiahaochen1.github.io/LumosFlow/', 'score': 14, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'c7f37e23b510618b', 'authors': ['Jiahao Chen', 'Hangjie Yuan', 'Yichen Qian', 'Jingyun Liang', 'Jiazheng Xing', 'Pengwei Liu', 'Weihua Chen', 'Fan Wang', 'Bing Su'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02497.jpg', 'data': {'categories': ['#open_source', '#video', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'LumosFlow: Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²', 'desc': 'LumosFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LMTV-DM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ—Ğ°Ñ‚ĞµĞ¼ LOF-DM ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ğ° MotionControlNet ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². LumosFlow Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 15-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'LumosFlow: Smooth Long Video Generation with Motion Guidance', 'desc': 'LumosFlow is a novel framework designed for generating long videos with smooth transitions and coherent motion. It utilizes the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to create key frames that capture significant motion, enhancing the diversity of the video content. For the interpolation of frames between these key frames, it employs the Latent Optical Flow Diffusion Model (LOF-DM) to generate complex motion flows, followed by MotionControlNet for refining the results. This approach significantly improves the quality of long video generation, achieving 15 times faster interpolation while maintaining consistent motion and appearance throughout the video.'}, 'zh': {'title': 'LumosFlowï¼šé«˜æ•ˆç”Ÿæˆè¿è´¯é•¿è§†é¢‘çš„åˆ›æ–°æ¡†æ¶', 'desc': 'LumosFlow æ˜¯ä¸€ä¸ªç”¨äºé•¿è§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œé‡‡ç”¨ LMTV-DM ç”Ÿæˆå…³é”®å¸§ï¼Œå¹¶ä½¿ç”¨ LOF-DM å’Œ MotionControlNet è¿›è¡Œå¹³æ»‘çš„ä¸­é—´å¸§æ’å€¼ã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼å¼•å…¥è¿åŠ¨æŒ‡å¯¼ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶é¢ä¸´çš„æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰è¿è´¯æ€§é—®é¢˜ã€‚LumosFlow é€šè¿‡ç”Ÿæˆå…·æœ‰è¾ƒå¤§è¿åŠ¨é—´éš”çš„å…³é”®å¸§ï¼Œç¡®ä¿äº†ç”Ÿæˆè§†é¢‘å†…å®¹çš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸€è‡´è¿åŠ¨å’Œå¤–è§‚çš„é•¿è§†é¢‘ï¼Œä¸”æ’å€¼é€Ÿåº¦æ¯”ä¼ ç»Ÿæ–¹æ³•å¿« 15 å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02528', 'title': 'RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.02528', 'abstract': "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Inspired by the in-context learning mechanism of large language models (LLMs), a new paradigm of generalizable visual prompt-based image editing is emerging. Existing single-reference methods typically focus on style or appearance adjustments and struggle with non-rigid transformations. To address these limitations, we propose leveraging source-target image pairs to extract and transfer content-aware editing intent to novel query images. To this end, we introduce RelationAdapter, a lightweight module that enables Diffusion Transformer (DiT) based models to effectively capture and apply visual transformations from minimal examples. We also introduce Relation252K, a comprehensive dataset comprising 218 diverse editing tasks, to evaluate model generalization and adaptability in visual prompt-driven scenarios. Experiments on Relation252K show that RelationAdapter significantly improves the model's ability to understand and transfer editing intent, leading to notable gains in generation quality and overall editing performance.", 'score': 13, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'e8011f9f9decd752', 'authors': ['Yan Gong', 'Yiren Song', 'Yicheng Li', 'Chenglin Li', 'Yin Zhang'], 'affiliations': ['National University of Singapore', 'Zhe Jiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02528.jpg', 'data': {'categories': ['#cv', '#dataset', '#transfer_learning', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'RelationAdapter: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'RelationAdapter - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Diffusion Transformer Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº-Ñ†ĞµĞ»ÑŒ. ĞĞ½ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. RelationAdapter Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Relation252K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 218 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Image Editing with RelationAdapter', 'desc': "The paper introduces RelationAdapter, a new lightweight module designed to enhance Diffusion Transformer models for image editing tasks. It focuses on using source-target image pairs to effectively capture and apply visual transformations, which improves the model's performance on diverse editing tasks. By leveraging this approach, RelationAdapter addresses the limitations of existing methods that struggle with non-rigid transformations and generalization. The authors also present Relation252K, a dataset that includes 218 diverse editing tasks to evaluate the model's adaptability and effectiveness in visual prompt-driven scenarios."}, 'zh': {'title': 'æå‡å›¾åƒç¼–è¾‘æ€§èƒ½çš„è½»é‡çº§æ¨¡å—', 'desc': 'RelationAdapter æ˜¯ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œæ—¨åœ¨å¢å¼ºæ‰©æ•£å˜æ¢å™¨æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥æœ‰æ•ˆæ•æ‰å’Œåº”ç”¨è§†è§‰å˜æ¢ã€‚å®ƒé€šè¿‡æº-ç›®æ ‡å›¾åƒå¯¹æ¥æå–å’Œè½¬ç§»å†…å®¹æ„ŸçŸ¥çš„ç¼–è¾‘æ„å›¾ï¼Œä»è€Œæ”¹å–„ç¼–è¾‘æ€§èƒ½å’Œåœ¨å¤šæ ·ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰çš„å•å‚è€ƒæ–¹æ³•ä¸åŒï¼ŒRelationAdapter èƒ½å¤Ÿå¤„ç†éåˆšæ€§å˜æ¢ï¼Œæå‡å›¾åƒç¼–è¾‘çš„çµæ´»æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº† Relation252K æ•°æ®é›†ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨è§†è§‰æç¤ºé©±åŠ¨åœºæ™¯ä¸­çš„æ³›åŒ–å’Œé€‚åº”èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01144', 'title': 'FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.01144', 'abstract': "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video diffusion models are notoriously limited in their ability to model temporal aspects such as motion, physics, and dynamic interactions. Existing approaches address this limitation by retraining the model or introducing external conditioning signals to enforce temporal consistency. In this work, we explore whether a meaningful temporal representation can be extracted directly from the predictions of a pre-trained model without any additional training or auxiliary inputs. We introduce FlowMo, a novel training-free guidance method that enhances motion coherence using only the model's own predictions in each diffusion step. FlowMo first derives an appearance-debiased temporal representation by measuring the distance between latents corresponding to consecutive frames. This highlights the implicit temporal structure predicted by the model. It then estimates motion coherence by measuring the patch-wise variance across the temporal dimension and guides the model to reduce this variance dynamically during sampling. Extensive experiments across multiple text-to-video models demonstrate that FlowMo significantly improves motion coherence without sacrificing visual quality or prompt alignment, offering an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models.", 'score': 12, 'issue_id': 4113, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '7bd7aa77c6143afb', 'authors': ['Ariel Shaulov', 'Itay Hazan', 'Lior Wolf', 'Hila Chefer'], 'affiliations': ['School of Computer Science Tel Aviv University, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2506.01144.jpg', 'data': {'categories': ['#video', '#training', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'FlowMo - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. FlowMo Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¾Ñ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ¸, Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Motion Coherence in Video Generation Without Training', 'desc': "FlowMo is a novel method that improves the motion coherence of pre-trained text-to-video diffusion models without requiring any additional training. It works by using the model's own predictions to create a temporal representation that captures the dynamics of motion across frames. By measuring the variance in motion across patches, FlowMo guides the model to reduce inconsistencies during the video generation process. This approach enhances the temporal fidelity of the generated videos while maintaining high visual quality and alignment with input prompts."}, 'zh': {'title': 'FlowMoï¼šæå‡è§†é¢‘ç”Ÿæˆè¿åŠ¨ä¸€è‡´æ€§çš„æ–°æ–¹æ³•', 'desc': 'FlowMoæ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„è¿åŠ¨ä¸€è‡´æ€§ã€‚å®ƒé€šè¿‡åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„é¢„æµ‹ï¼Œå‡å°‘äº†æ—¶é—´ç»´åº¦ä¸Šçš„è¡¥ä¸æ–¹å·®ï¼Œä»è€Œå¢å¼ºäº†è¿åŠ¨çš„è¿è´¯æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒFlowMoä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–å¼•å…¥å¤–éƒ¨æ¡ä»¶ä¿¡å·ï¼Œè€Œæ˜¯ç›´æ¥ä»æ¨¡å‹çš„é¢„æµ‹ä¸­æå–æœ‰æ„ä¹‰çš„æ—¶é—´è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowMoåœ¨å¤šä¸ªæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­æ˜¾è‘—æé«˜äº†è¿åŠ¨ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è§†è§‰è´¨é‡å’Œæç¤ºå¯¹é½ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00910', 'title': 'PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.00910', 'abstract': 'ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs -- i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.', 'score': 10, 'issue_id': 4113, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '375d26a9868ef2fe', 'authors': ['Seongjae Kang', 'Dong Bok Lee', 'Hyungjoon Jang', 'Dongseop Kim', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'VUNO Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.00910.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#dataset', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ActiveKD: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'ActiveKD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ¼ ActiveKD ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Probabilistic CoreSet (PCoreSet), Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° 11 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PCoreSet ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ActiveKD.'}, 'en': {'title': 'Efficient Sample Selection through ActiveKD: Merging Active Learning and Knowledge Distillation', 'desc': 'ActiveKD is a novel framework that combines active learning (AL) with knowledge distillation (KD) to enhance the selection of diverse, unlabeled samples for annotation. It addresses the challenge of limited labeled data by utilizing large vision-language models (VLMs) that can perform well even with few examples. The framework introduces a selection strategy called Probabilistic CoreSet (PCoreSet), which focuses on maximizing coverage in the probability space, allowing for more effective knowledge transfer from teacher models to student models. Evaluations demonstrate that ActiveKD, through PCoreSet, significantly improves sample selection efficiency compared to traditional methods.'}, 'zh': {'title': 'ä¸»åŠ¨å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦çš„é«˜æ•ˆç»“åˆ', 'desc': 'ActiveKDæ˜¯ä¸€ä¸ªå°†ä¸»åŠ¨å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦ç›¸ç»“åˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆé€‰æ‹©å¤šæ ·åŒ–çš„æœªæ ‡æ³¨æ ·æœ¬è¿›è¡Œæ ‡æ³¨ã€‚çŸ¥è¯†è’¸é¦é€šå¸¸éœ€è¦è¶³å¤Ÿçš„æ ‡æ³¨æ•°æ®ï¼Œè€Œä¸»åŠ¨å­¦ä¹ åˆ™åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å·¥ä½œï¼Œå› æ­¤ä¸¤è€…çš„ç»“åˆå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ActiveKDåˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬èƒ½åŠ›ï¼Œé€šè¿‡æ¦‚ç‡ç©ºé—´ä¸­çš„ç»“æ„åŒ–é¢„æµ‹åå·®æ¥ä¼˜åŒ–æ ·æœ¬é€‰æ‹©ã€‚æˆ‘ä»¬æå‡ºçš„æ¦‚ç‡æ ¸å¿ƒé›†ï¼ˆPCoreSetï¼‰ç­–ç•¥èƒ½å¤Ÿåœ¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—ä¸‹ï¼Œé€‰æ‹©å…·æœ‰ç±»åˆ«å¤šæ ·æ€§çš„æœªæ ‡æ³¨æ ·æœ¬ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°ä¼ é€’æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03123', 'title': 'DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.03123', 'abstract': 'Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models often results in severe degradation of temporal consistency and appearance details. In this paper, by analyzing the training dynamics of Consistency Models, we identify a key conflicting learning dynamics during the distillation process: there is a significant discrepancy in the optimization gradients and loss contributions across different timesteps. This discrepancy prevents the distilled student model from achieving an optimal state, leading to compromised temporal consistency and degraded appearance details. To address this issue, we propose a parameter-efficient Dual-Expert Consistency Model~(DCM), where a semantic expert focuses on learning semantic layout and motion, while a detail expert specializes in fine detail refinement. Furthermore, we introduce Temporal Coherence Loss to improve motion consistency for the semantic expert and apply GAN and Feature Matching Loss to enhance the synthesis quality of the detail expert.Our approach achieves state-of-the-art visual quality with significantly reduced sampling steps, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Our code and models are available at https://github.com/Vchitect/DCM{https://github.com/Vchitect/DCM}.', 'score': 9, 'issue_id': 4122, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'b284c16ff31a205f', 'authors': ['Zhengyao Lv', 'Chenyang Si', 'Tianlin Pan', 'Zhaoxi Chen', 'Kwan-Yee K. Wong', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Nanjing University', 'S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.03123.jpg', 'data': {'categories': ['#training', '#video', '#multimodal', '#diffusion', '#optimization', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Dual-Expert Consistency Model (DCM) Ñ Ğ´Ğ²ÑƒĞ¼Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°.'}, 'en': {'title': 'Expert Specialization for Enhanced Video Synthesis', 'desc': 'This paper addresses the challenges of using Diffusion Models for video synthesis, particularly the high computational cost due to iterative denoising steps. It highlights the limitations of applying Consistency Models directly to video diffusion, which can lead to poor temporal consistency and loss of detail. The authors propose a Dual-Expert Consistency Model (DCM) that separates the learning tasks into two experts: one for semantic understanding and motion, and another for fine detail refinement. By introducing a Temporal Coherence Loss and utilizing GAN and Feature Matching Loss, the model achieves high visual quality with fewer sampling steps, showcasing the benefits of expert specialization in improving video synthesis.'}, 'zh': {'title': 'ä¸“å®¶ä¸“æ³¨ï¼Œæå‡è§†é¢‘åˆæˆè´¨é‡', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘åˆæˆä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†éœ€è¦å¤šæ¬¡å»å™ªæ­¥éª¤ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ã€‚ä¸€è‡´æ€§æ¨¡å‹åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘æ‰©æ•£æ¨¡å‹æ—¶ï¼Œå¾€å¾€ä¼šå¯¼è‡´æ—¶é—´ä¸€è‡´æ€§å’Œå¤–è§‚ç»†èŠ‚çš„ä¸¥é‡é€€åŒ–ã€‚æœ¬æ–‡é€šè¿‡åˆ†æä¸€è‡´æ€§æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€ï¼Œè¯†åˆ«å‡ºåœ¨è’¸é¦è¿‡ç¨‹ä¸­å­˜åœ¨çš„å…³é”®å†²çªå­¦ä¹ åŠ¨æ€ï¼Œå¯¼è‡´è’¸é¦å­¦ç”Ÿæ¨¡å‹æ— æ³•è¾¾åˆ°æœ€ä½³çŠ¶æ€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„åŒä¸“å®¶ä¸€è‡´æ€§æ¨¡å‹ï¼ˆDCMï¼‰ï¼Œé€šè¿‡è¯­ä¹‰ä¸“å®¶å’Œç»†èŠ‚ä¸“å®¶çš„ä¸“ä¸šåŒ–æ¥æé«˜è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01789', 'title': "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability", 'url': 'https://huggingface.co/papers/2506.01789', 'abstract': 'High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.', 'score': 9, 'issue_id': 4111, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '08a1fd6c4eff1198', 'authors': ['Genta Indra Winata', 'David Anugraha', 'Emmy Liu', 'Alham Fikri Aji', 'Shou-Yi Hung', 'Aditya Parashar', 'Patrick Amadeus Irawan', 'Ruochen Zhang', 'Zheng-Xin Yong', 'Jan Christian Blaise Cruz', 'Niklas Muennighoff', 'Seungone Kim', 'Hanyang Zhao', 'Sudipta Kar', 'Kezia Erina Suryoraharjo', 'M. Farid Adilazuarda', 'En-Shiun Annie Lee', 'Ayu Purwarianti', 'Derry Tanti Wijaya', 'Monojit Choudhury'], 'affiliations': ['Brown University', 'Capital One', 'Carnegie Mellon University', 'Columbia University', 'ITB', 'MBZUAI', 'Monash University', 'Ontario Tech University', 'Oracle', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01789.jpg', 'data': {'categories': ['#open_source', '#data', '#synthetic', '#dataset'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'DataRubrics: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ DataRubrics - ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². DataRubrics Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Dataset Quality with DataRubrics', 'desc': 'This paper discusses the importance of high-quality datasets in machine learning and the challenges in creating them, particularly regarding human annotations. It highlights issues with current dataset submissions, such as lack of originality and inadequate quality control, which are often missed during peer review. The authors propose a new framework called DataRubrics, which introduces systematic, rubric-based evaluation metrics to improve dataset quality assessment. Additionally, they explore methods for generating synthetic data and emphasize the need for reproducibility in evaluations using recent advancements in large language models (LLMs).'}, 'zh': {'title': 'æå‡æ•°æ®é›†è´¨é‡çš„ç³»ç»ŸåŒ–è¯„ä¼°æ¡†æ¶', 'desc': 'é«˜è´¨é‡çš„æ•°æ®é›†å¯¹äºè®­ç»ƒå’Œè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†åˆ›å»ºè¿™äº›æ•°æ®é›†å°¤å…¶æ˜¯å‡†ç¡®çš„äººç±»æ ‡æ³¨ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è®¸å¤šæ•°æ®é›†è®ºæ–‡ç¼ºä¹åŸåˆ›æ€§ã€å¤šæ ·æ€§æˆ–ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ï¼Œä¸”è¿™äº›é—®é¢˜åœ¨åŒè¡Œè¯„å®¡ä¸­å¸¸å¸¸è¢«å¿½è§†ã€‚æœ¬æ–‡æå€¡åœ¨æ•°æ®é›†å®¡æŸ¥è¿‡ç¨‹ä¸­æ•´åˆç³»ç»ŸåŒ–çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥æé«˜æ•°æ®è´¨é‡è¯„ä¼°çš„æ ‡å‡†åŒ–å’Œå¯æµ‹é‡æ€§ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†DataRubricsï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°äººç±»å’Œæ¨¡å‹ç”Ÿæˆæ•°æ®é›†è´¨é‡çš„ç»“æ„åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ•°æ®é©±åŠ¨ç ”ç©¶çš„æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01716', 'title': 'Self-Challenging Language Model Agents', 'url': 'https://huggingface.co/papers/2506.01716', 'abstract': 'The Self-Challenging framework trains intelligent agents using self-generated tasks defined as Code-as-Task, improving performance in multi-turn tool-use benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.', 'score': 5, 'issue_id': 4123, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '292019a70795a800', 'authors': ['Yifei Zhou', 'Sergey Levine', 'Jason Weston', 'Xian Li', 'Sainbayar Sukhbaatar'], 'affiliations': ['FAIR at Meta', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.01716.jpg', 'data': {'categories': ['#optimization', '#agi', '#training', '#agents', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ğ°Ğ³ĞµĞ½Ñ‚ Ğ±Ñ€Ğ¾ÑĞ°ĞµÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ¼Ñƒ ÑĞµĞ±Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Self-Challenging. Ğ’ ÑÑ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Code-as-Task, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3.1-8B-Instruct Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Empowering Agents Through Self-Generated Challenges', 'desc': "The Self-Challenging framework enhances the training of intelligent agents by allowing them to create their own tasks, known as Code-as-Task. This approach eliminates the need for extensive human-generated tasks, as the agent generates high-quality tasks through its interactions with tools. Each task includes an instruction, a verification function, and defined success and failure cases, which help ensure the tasks are effective for training. The framework utilizes reinforcement learning to improve the agent's performance, achieving significant gains in benchmarks with only self-generated data."}, 'zh': {'title': 'è‡ªæˆ‘æŒ‘æˆ˜ï¼Œæ™ºèƒ½ä½“è®­ç»ƒçš„æ–°æ–¹æ³•', 'desc': 'è‡ªæˆ‘æŒ‘æˆ˜æ¡†æ¶é€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„ä»»åŠ¡æ¥è®­ç»ƒæ™ºèƒ½ä½“ï¼Œè¿™äº›ä»»åŠ¡è¢«å®šä¹‰ä¸ºä»£ç ä»»åŠ¡ï¼Œä»è€Œæé«˜äº†åœ¨å¤šè½®å·¥å…·ä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“åœ¨ä¸å·¥å…·äº’åŠ¨åï¼Œé¦–å…ˆæ‰®æ¼”æŒ‘æˆ˜è€…è§’è‰²ç”Ÿæˆé«˜è´¨é‡ä»»åŠ¡ã€‚ç”Ÿæˆçš„ä»»åŠ¡åŒ…æ‹¬æŒ‡ä»¤ã€éªŒè¯å‡½æ•°ä»¥åŠè§£å†³æ–¹æ¡ˆå’Œå¤±è´¥æ¡ˆä¾‹ï¼Œç¡®ä¿ä»»åŠ¡çš„è´¨é‡ã€‚æœ€ç»ˆï¼Œæ™ºèƒ½ä½“ä½œä¸ºæ‰§è¡Œè€…ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡è¯„ä¼°åé¦ˆä½œä¸ºå¥–åŠ±ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00413', 'title': 'Accelerating Diffusion LLMs via Adaptive Parallel Decoding', 'url': 'https://huggingface.co/papers/2506.00413', 'abstract': 'Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.', 'score': 5, 'issue_id': 4113, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '42d7cf22876b9eef', 'authors': ['Daniel Israel', 'Guy Van den Broeck', 'Aditya Grover'], 'affiliations': ['Department of Computer Science University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.00413.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#diffusion', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (APD) Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLM). APD Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¼ĞµÑÑŒ Ğ¼Ğ°Ñ€Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ dLLM Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. APD Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ KV-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼.'}, 'en': {'title': 'Boosting Speed in Language Models with Adaptive Parallel Decoding', 'desc': 'Adaptive parallel decoding (APD) is a new technique that improves the speed of diffusion large language models (dLLMs) by allowing multiple tokens to be generated at the same time. Traditional methods use autoregressive decoding, which predicts tokens one after another, slowing down the process. APD changes this by dynamically adjusting how many tokens are generated in parallel, balancing speed and quality. By combining probabilities from both dLLMs and a smaller autoregressive model, APD achieves higher throughput with only slight reductions in output quality.'}, 'zh': {'title': 'è‡ªé€‚åº”å¹¶è¡Œè§£ç ï¼šæå‡ç”Ÿæˆé€Ÿåº¦ä¸è´¨é‡çš„å¹³è¡¡', 'desc': 'è‡ªé€‚åº”å¹¶è¡Œè§£ç ï¼ˆAPDï¼‰é€šè¿‡åŠ¨æ€è°ƒæ•´å¹¶è¡Œç”Ÿæˆçš„æ ‡è®°æ•°é‡ï¼Œæå‡äº†æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰çš„ååé‡ï¼Œè€Œä¸ä¼šæ˜¾è‘—é™ä½è´¨é‡ã€‚ä¼ ç»Ÿçš„è‡ªå›å½’è§£ç æ–¹æ³•ä½¿å¾—ç”Ÿæˆé€Ÿåº¦å—åˆ°é™åˆ¶ï¼Œå› ä¸ºæ ‡è®°æ˜¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°é¢„æµ‹çš„ã€‚å°½ç®¡ç†è®ºä¸Šæ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹å…è®¸å¹¶è¡Œç”Ÿæˆæ ‡è®°ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¾€å¾€éš¾ä»¥åœ¨ä¸ç‰ºç‰²è´¨é‡çš„æƒ…å†µä¸‹è¾¾åˆ°è‡ªå›å½’æ¨¡å‹çš„é€Ÿåº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å®šä¹‰dLLMè¾¹é™…æ¦‚ç‡ä¸å°å‹è‡ªå›å½’æ¨¡å‹ä¸‹åºåˆ—çš„è”åˆæ¦‚ç‡ä¹‹é—´çš„ä¹˜æ³•æ··åˆï¼Œæ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå¹¶é€šè¿‡å¯ç”¨KVç¼“å­˜å’Œé™åˆ¶æ©ç è¾“å…¥çš„å¤§å°è¿›ä¸€æ­¥ä¼˜åŒ–APDã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22704', 'title': 'Training Language Models to Generate Quality Code with Program Analysis\n  Feedback', 'url': 'https://huggingface.co/papers/2505.22704', 'abstract': 'A reinforcement learning framework improves code quality in large language models by using automated feedback from program analysis and unit tests.  \t\t\t\t\tAI-generated summary \t\t\t\t Code generation with large language models (LLMs), often termed vibe coding, is increasingly adopted in production but fails to ensure code quality, particularly in security (e.g., SQL injection vulnerabilities) and maintainability (e.g., missing type annotations). Existing methods, such as supervised fine-tuning and rule-based post-processing, rely on labor-intensive annotations or brittle heuristics, limiting their scalability and effectiveness. We propose REAL, a reinforcement learning framework that incentivizes LLMs to generate production-quality code using program analysis-guided feedback. Specifically, REAL integrates two automated signals: (1) program analysis detecting security or maintainability defects and (2) unit tests ensuring functional correctness. Unlike prior work, our framework is prompt-agnostic and reference-free, enabling scalable supervision without manual intervention. Experiments across multiple datasets and model scales demonstrate that REAL outperforms state-of-the-art methods in simultaneous assessments of functionality and code quality. Our work bridges the gap between rapid prototyping and production-ready code, enabling LLMs to deliver both speed and quality.', 'score': 5, 'issue_id': 4128, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '795ffdf04386035f', 'authors': ['Feng Yao', 'Zilong Wang', 'Liyuan Liu', 'Junxia Cui', 'Li Zhong', 'Xiaohan Fu', 'Haohui Mai', 'Vish Krishnan', 'Jianfeng Gao', 'Jingbo Shang'], 'affiliations': ['CausalFlow Inc.', 'Microsoft Research', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.22704.jpg', 'data': {'categories': ['#security', '#rl', '#dataset', '#training', '#optimization'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ° Ğ¾Ñ‚ LLM', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° REAL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). REAL Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ REAL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Code Quality with Reinforcement Learning', 'desc': 'This paper presents REAL, a reinforcement learning framework designed to enhance the quality of code generated by large language models (LLMs). It addresses common issues in code generation, such as security vulnerabilities and maintainability problems, by utilizing automated feedback from program analysis and unit tests. Unlike traditional methods that require extensive manual annotations, REAL operates in a prompt-agnostic manner, allowing for scalable and efficient supervision. Experimental results show that REAL significantly improves both functionality and code quality compared to existing techniques.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡ä»£ç è´¨é‡ï¼ŒåŠ©åŠ›é«˜æ•ˆç¼–ç¨‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºREALçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç çš„è´¨é‡ã€‚é€šè¿‡è‡ªåŠ¨åŒ–åé¦ˆï¼ŒREALç»“åˆäº†ç¨‹åºåˆ†æå’Œå•å…ƒæµ‹è¯•ï¼Œå¸®åŠ©æ¨¡å‹è¯†åˆ«å®‰å…¨æ€§å’Œå¯ç»´æŠ¤æ€§ç¼ºé™·ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒå’ŒåŸºäºè§„åˆ™çš„åå¤„ç†æ–¹æ³•ä¸åŒï¼ŒREALä¸ä¾èµ–äºäººå·¥æ ‡æ³¨ï¼Œå…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒREALåœ¨åŠŸèƒ½æ€§å’Œä»£ç è´¨é‡çš„è¯„ä¼°ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01274', 'title': 'ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding', 'url': 'https://huggingface.co/papers/2506.01274', 'abstract': "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), a novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals derived from a reference LMM to reflect the model's intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility.", 'score': 4, 'issue_id': 4115, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '401bd39fc17a47b7', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2506.01274.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#video', '#optimization', '#rl', '#benchmark'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ˜Ğ˜', 'desc': 'ReFoCUS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ReFoCUS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Optimizing Frame Selection for Better Video Reasoning', 'desc': "ReFoCUS is a novel framework that uses reinforcement learning to improve how video-LLMs select frames for video question answering (QA). Instead of relying on fixed rules or external systems, it learns which frames are most relevant to the questions being asked. By optimizing frame selection based on the model's preferences, ReFoCUS enhances the reasoning capabilities of video-LLMs. This method not only simplifies the selection process but also boosts performance on various video QA tasks without needing detailed supervision for each frame."}, 'zh': {'title': 'ä¼˜åŒ–è§†é¢‘å¸§é€‰æ‹©ï¼Œæå‡æ¨ç†èƒ½åŠ›', 'desc': 'ReFoCUSæ˜¯ä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è§†é¢‘-å¤§è¯­è¨€æ¨¡å‹ï¼ˆvideo-LLMï¼‰å¸§é€‰æ‹©çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†é¢‘é—®ç­”ä¸­çš„æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ å¸§é€‰æ‹©ç­–ç•¥ï¼Œå…³æ³¨è§†è§‰è¾“å…¥çš„é€‰æ‹©ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–æ–‡æœ¬å“åº”ã€‚ReFoCUSä½¿ç”¨æ¥è‡ªå‚è€ƒå¤§å¤šæ¨¡æ€æ¨¡å‹çš„å¥–åŠ±ä¿¡å·ï¼Œåæ˜ æ¨¡å‹å¯¹æ”¯æŒæ—¶é—´ç›¸å…³å“åº”çš„æœ€ä½³å¸§çš„åå¥½ã€‚é€šè¿‡è‡ªå›å½’æ¡ä»¶é€‰æ‹©æ¶æ„ï¼ŒReFoCUSæœ‰æ•ˆæ¢ç´¢å¸§ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡äº†å¤šä¸ªè§†é¢‘é—®ç­”åŸºå‡†çš„æ¨ç†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03096', 'title': 'FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens', 'url': 'https://huggingface.co/papers/2506.03096', 'abstract': 'Contrastive language-image pre-training aligns the features of text-image pairs in a common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text into a single feature vector. As a remedy, it is common practice to use additional modules to merge the features extracted by the unimodal encoders. In this work, we present FuseLIP, an alternative architecture for multimodal embedding. Leveraging recent progress in discrete image tokenizers, we propose to use a single transformer model which operates on an extended vocabulary of text and image tokens. This early fusion approach allows the different modalities to interact at each depth of encoding and obtain richer representations compared to common late fusion. We collect new datasets for multimodal pre-training and evaluation, designing challenging tasks for multimodal encoder models. We show that FuseLIP outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks.', 'score': 3, 'issue_id': 4118, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '6a94488665c463be', 'authors': ['Christian Schlarmann', 'Francesco Croce', 'Nicolas Flammarion', 'Matthias Hein'], 'affiliations': ['TÃ¼bingen AI Center University of TÃ¼bingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03096.jpg', 'data': {'categories': ['#architecture', '#dataset', '#games', '#alignment', '#multimodal'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ Ğ°Ğ½Ğ½ÑÑ Ñ„ÑŒÑĞ¶Ğ½ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'FuseLIP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼, FuseLIP Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. FuseLIP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'FuseLIP: Unifying Text and Image with Early Fusion for Better Multimodal Understanding', 'desc': 'This paper introduces FuseLIP, a novel architecture for multimodal embedding that combines text and image features using a single transformer model. Unlike traditional methods that merge features from separate encoders, FuseLIP employs an early fusion strategy, allowing text and image tokens to interact throughout the encoding process. This results in richer representations and improved performance on multimodal tasks like visual question answering (VQA) and text-guided image retrieval. The authors also present new datasets for training and evaluating multimodal models, demonstrating that FuseLIP outperforms existing methods in multimodal scenarios while maintaining competitive results in unimodal tasks.'}, 'zh': {'title': 'FuseLIPï¼šå¤šæ¨¡æ€åµŒå…¥çš„æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€åµŒå…¥æ¶æ„FuseLIPï¼Œæ—¨åœ¨æ”¹è¿›æ–‡æœ¬å’Œå›¾åƒçš„ç‰¹å¾å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„åæœŸèåˆæ–¹æ³•ä¸åŒï¼ŒFuseLIPé‡‡ç”¨æ—©æœŸèåˆç­–ç•¥ï¼Œé€šè¿‡å•ä¸€çš„å˜æ¢å™¨æ¨¡å‹å¤„ç†æ‰©å±•çš„æ–‡æœ¬å’Œå›¾åƒæ ‡è®°è¯æ±‡ã€‚è¿™æ ·å¯ä»¥åœ¨ç¼–ç çš„æ¯ä¸ªæ·±åº¦ä¸Šå®ç°ä¸åŒæ¨¡æ€çš„äº¤äº’ï¼Œä»è€Œè·å¾—æ›´ä¸°å¯Œçš„è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFuseLIPåœ¨å¤šæ¨¡æ€åµŒå…¥ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒæ—¶åœ¨å•æ¨¡æ€ä»»åŠ¡ä¸Šä¸åŸºçº¿æ¨¡å‹ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03079', 'title': 'ORV: 4D Occupancy-centric Robot Video Generation', 'url': 'https://huggingface.co/papers/2506.03079', 'abstract': 'ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Acquiring real-world robotic simulation data through teleoperation is notoriously time-consuming and labor-intensive. Recently, action-driven generative models have gained widespread adoption in robot learning and simulation, as they eliminate safety concerns and reduce maintenance efforts. However, the action sequences used in these methods often result in limited control precision and poor generalization due to their globally coarse alignment. To address these limitations, we propose ORV, an Occupancy-centric Robot Video generation framework, which utilizes 4D semantic occupancy sequences as a fine-grained representation to provide more accurate semantic and geometric guidance for video generation. By leveraging occupancy-based representations, ORV enables seamless translation of simulation data into photorealistic robot videos, while ensuring high temporal consistency and precise controllability. Furthermore, our framework supports the simultaneous generation of multi-view videos of robot gripping operations - an important capability for downstream robotic learning tasks. Extensive experimental results demonstrate that ORV consistently outperforms existing baseline methods across various datasets and sub-tasks. Demo, Code and Model: https://orangesodahub.github.io/ORV', 'score': 3, 'issue_id': 4115, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '2bfb7d794c03a7ed', 'authors': ['Xiuyu Yang', 'Bohan Li', 'Shaocong Xu', 'Nan Wang', 'Chongjie Ye', 'Zhaoxi Chen', 'Minghan Qin', 'Yikang Ding', 'Xin Jin', 'Hang Zhao', 'Hao Zhao'], 'affiliations': ['AIR, Tsinghua University', 'Beijing Academy of Artificial Intelligence', 'ByteDance', 'Eastern Institute of Technology, Ningbo', 'IIIS, Tsinghua University', 'Megvii Technology', 'National University of Singapore', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03079.jpg', 'data': {'categories': ['#games', '#robotics', '#optimization', '#video'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ORV: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· 4D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚ÑŒ', 'desc': 'ORV - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 4D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ORV ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'ORV: Revolutionizing Robot Video Generation with 4D Occupancy Sequences', 'desc': 'The paper introduces ORV, a framework designed for generating robot videos using 4D semantic occupancy sequences. This approach enhances the quality of video generation by providing detailed semantic and geometric information, leading to photorealistic and temporally consistent outputs. ORV addresses the limitations of previous action-driven generative models, which often struggle with control precision and generalization. The framework also allows for the generation of multi-view videos, which is beneficial for various robotic learning applications, demonstrating superior performance compared to existing methods.'}, 'zh': {'title': 'ä»¥å ç”¨ä¸ºä¸­å¿ƒçš„æœºå™¨äººè§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'ORVæ˜¯ä¸€ä¸ªä»¥å ç”¨ä¸ºä¸­å¿ƒçš„æœºå™¨äººè§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨4Dè¯­ä¹‰å ç”¨åºåˆ—ç”Ÿæˆé€¼çœŸçš„ã€æ—¶é—´ä¸€è‡´çš„ã€å¯ç²¾ç¡®æ§åˆ¶çš„æœºå™¨äººè§†é¢‘ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»†ç²’åº¦çš„å ç”¨è¡¨ç¤ºï¼Œæä¾›æ›´å‡†ç¡®çš„è¯­ä¹‰å’Œå‡ ä½•æŒ‡å¯¼ï¼Œä»è€Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„æ§åˆ¶ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ORVèƒ½å¤Ÿæ— ç¼åœ°å°†ä»¿çœŸæ•°æ®è½¬åŒ–ä¸ºé«˜è´¨é‡çš„æœºå™¨äººè§†é¢‘ï¼Œå¹¶æ”¯æŒåŒæ—¶ç”Ÿæˆå¤šè§†è§’çš„è§†é¢‘ï¼Œé€‚ç”¨äºåç»­çš„æœºå™¨äººå­¦ä¹ ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORVåœ¨å¤šä¸ªæ•°æ®é›†å’Œå­ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02338', 'title': 'One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL', 'url': 'https://huggingface.co/papers/2506.02338', 'abstract': "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.  \t\t\t\t\tAI-generated summary \t\t\t\t With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR.", 'score': 3, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': 'c96bff52eb2a678f', 'authors': ['Hyungjoo Chae', 'Dongjin Kang', 'Jihyuk Kim', 'Beong-woo Kwak', 'Sunghyun Park', 'Haeju Park', 'Jinyoung Yeo', 'Moontae Lee', 'Kyungjae Lee'], 'affiliations': ['LG AI Research', 'University of Illinois Chicago', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02338.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#dataset', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Long CoT Collection, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Empowering Reasoning with Long CoT Datasets', 'desc': 'The Long CoT Collection dataset is designed to improve reasoning skills in language models by using short chain-of-thought (CoT) models to generate long CoT inferences. This dataset consists of 100,000 annotated rationales that help models think more deeply and manage their reasoning process better. By training on this dataset, models can achieve reasoning quality similar to the established R1 model while also enhancing their performance in reinforcement learning tasks. The research highlights the potential for developing independent large reasoning models without relying solely on existing ones.'}, 'zh': {'title': 'æå‡æ¨ç†èƒ½åŠ›çš„é•¿é“¾æ€ç»´æ•°æ®é›†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºLong CoT Collectionçš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±çŸ­é“¾æ€ç»´ï¼ˆCoTï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆï¼Œæ—¨åœ¨æå‡ä¸€èˆ¬æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥è·å¾—ä¸ç°æœ‰å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚R1ï¼‰ç›¸å½“çš„æ¨ç†è´¨é‡ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿çŸ­é“¾æ€ç»´æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ›´é•¿æ—¶é—´çš„æ¨ç†ï¼Œå¹¶å¼•å…¥äº†å¯¹æ€ç»´é¢„ç®—çš„å¯æ§æ€§ï¼Œä»¥è§£å†³è¿‡åº¦æ€è€ƒçš„é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè¯¥æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00391', 'title': 'SHARE: An SLM-based Hierarchical Action CorREction Assistant for\n  Text-to-SQL', 'url': 'https://huggingface.co/papers/2506.00391', 'abstract': 'SHARE, an SLM-based Hierarchical Action RECorrection system, enhances LLMs in text-to-SQL by transforming SQL queries into action trajectories and employing a granular refinement process, improving error detection and correction efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for declarative SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose SHARE, an SLM-based Hierarchical Action corREction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.', 'score': 3, 'issue_id': 4122, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '2e0ac7f48f4b8959', 'authors': ['Ge Qu', 'Jinyang Li', 'Bowen Qin', 'Xiaolong Li', 'Nan Huo', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['BAAI', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.00391.jpg', 'data': {'categories': ['#low_resource', '#training', '#optimization', '#dataset', '#reasoning', '#small_models', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ SQL Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'SHARE - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² SQL. ĞĞ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. SHARE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'SHARE: Enhancing SQL Error Correction with Hierarchical Action Trajectories', 'desc': 'The paper introduces SHARE, a system designed to improve the self-correction capabilities of large language models (LLMs) in text-to-SQL tasks. It addresses the inefficiencies of traditional self-correction methods that rely heavily on recursive calls, which can be computationally expensive. SHARE utilizes a hierarchical approach with three specialized small language models (SLMs) to convert SQL queries into action trajectories, enhancing error detection and correction. The system also incorporates a novel training strategy that allows it to perform well even with limited data, making it suitable for applications where data privacy is a concern.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°SQLçš„è‡ªæˆ‘çº æ­£èƒ½åŠ›', 'desc': 'SHAREæ˜¯ä¸€ç§åŸºäºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å±‚æ¬¡åŒ–åŠ¨ä½œçº æ­£ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ°SQLè½¬æ¢ä¸­çš„è¡¨ç°ã€‚å®ƒé€šè¿‡å°†SQLæŸ¥è¯¢è½¬åŒ–ä¸ºé€æ­¥çš„åŠ¨ä½œè½¨è¿¹ï¼Œå¸®åŠ©LLMæ›´å¥½åœ°ç†è§£å’Œå®šä½é”™è¯¯ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„ç»†åŒ–è¿‡ç¨‹ï¼Œæé«˜äº†é”™è¯¯æ£€æµ‹å’Œçº æ­£çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒSHAREè¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å±‚æ¬¡è‡ªæˆ‘è¿›åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®èµ„æºæœ‰é™çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆè®­ç»ƒï¼Œç‰¹åˆ«é€‚ç”¨äºæœ‰æ•°æ®éšç§é™åˆ¶çš„æ–‡æœ¬åˆ°SQLåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24273', 'title': 'How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.24273', 'abstract': 'This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs.', 'score': 3, 'issue_id': 4118, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '73012fc9edd07bd3', 'authors': ['Hongyi James Cai', 'Junlin Wang', 'Xiaoyin Chen', 'Bhuwan Dhingra'], 'affiliations': ['Duke University', 'Mila - Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.24273.jpg', 'data': {'categories': ['#training', '#rl', '#synthetic', '#reasoning', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…: ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM), Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ñ€Ğ¾Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ° (backtracking) Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¡ÑƒĞ´Ğ¾ĞºÑƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° ÑĞºĞ¾Ñ€ĞµĞµ Ğ¾Ñ‚ Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹.'}, 'en': {'title': 'Enhancing Reasoning in LLMs through Backtracking and Training Synergy', 'desc': 'This paper explores how supervised fine-tuning (SFT) and reinforcement learning (RL) work together to improve reasoning in large language models (LLMs). It specifically examines the role of backtracking, a technique that allows models to revisit previous steps in their reasoning process, and how it enhances performance on various reasoning tasks. The study finds that while short chain-of-thought (CoT) sequences help in RL training, their effectiveness decreases with task difficulty. Ultimately, the research provides valuable insights into optimizing training strategies for better reasoning capabilities in LLMs.'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†èƒ½åŠ›çš„è®­ç»ƒç­–ç•¥', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç›¸äº’ä½œç”¨ï¼Œé‡ç‚¹å…³æ³¨å›æº¯åœ¨å¢å¼ºæ¨ç†èƒ½åŠ›ä¸­çš„ä½œç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹åœ¨æ•°å­¦å’Œé€»è¾‘é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼ŒçŸ­çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åºåˆ—åœ¨SFTé˜¶æ®µå¯¹RLè®­ç»ƒæœ‰ä¸€å®šè´¡çŒ®ï¼Œä½†åœ¨ä»»åŠ¡éš¾åº¦å¢åŠ æ—¶ï¼Œè¿™ç§è´¡çŒ®ä¼šå‡å¼±ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯å®äº†å›æº¯çš„é¢‘ç‡å’Œç»“æ„å¯¹æ¨ç†è®­ç»ƒçš„é‡è¦æ€§ï¼Œæä¾›äº†ä¼˜åŒ–è®­ç»ƒç­–ç•¥çš„å®ç”¨è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18079', 'title': 'Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding', 'url': 'https://huggingface.co/papers/2505.18079', 'abstract': 'The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later.', 'score': 3, 'issue_id': 4117, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '6dbac78671d7d992', 'authors': ['Xiaoyi Zhang', 'Zhaoyang Jia', 'Zongyu Guo', 'Jiahao Li', 'Bin Li', 'Houqiang Li', 'Yan Lu'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.18079.jpg', 'data': {'categories': ['#benchmark', '#video', '#long_context', '#reasoning', '#agents'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Deep Video Discovery Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ³ĞµĞ½Ñ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. DVD Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LVBench.'}, 'en': {'title': 'Revolutionizing Long-Form Video Understanding with Autonomous Agents', 'desc': 'The Deep Video Discovery (DVD) agent introduces an innovative approach to long-form video understanding by utilizing an autonomous agentic search strategy powered by large language models (LLMs). This method addresses the challenges posed by the complexity of temporal and spatial information in lengthy videos, which traditional models struggle to analyze effectively. By segmenting videos and employing a search-centric toolkit, the DVD agent can dynamically plan and refine its reasoning based on real-time observations. The results demonstrate that this system achieves state-of-the-art performance on benchmarks like LVBench, significantly outperforming previous methods.'}, 'zh': {'title': 'è‡ªä¸»æœç´¢ï¼Œæ·±åº¦ç†è§£é•¿è§†é¢‘', 'desc': 'æ·±åº¦è§†é¢‘å‘ç°ä»£ç†ä½¿ç”¨è‡ªä¸»æœç´¢ç­–ç•¥ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…‹æœäº†é•¿è§†é¢‘ç†è§£ä¸­çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹åˆ†æ®µè§†é¢‘ç‰‡æ®µè¿›è¡Œæ™ºèƒ½æœç´¢ï¼Œæå‡äº†åœ¨å¤æ‚æ—¶ç©ºèƒŒæ™¯ä¸‹çš„é—®é¢˜å›ç­”èƒ½åŠ›ã€‚ä¸ä»¥å¾€æ‰‹åŠ¨è®¾è®¡å·¥ä½œæµç¨‹çš„è§†é¢‘ä»£ç†ä¸åŒï¼Œæˆ‘ä»¬çš„ä»£ç†å¼ºè°ƒè‡ªä¸»æ€§ï¼Œåˆ©ç”¨å¤šå±‚æ¬¡è§†é¢‘æ•°æ®åº“ä¸­çš„æœç´¢å·¥å…·è¿›è¡Œè§„åˆ’å’Œé€‰æ‹©ã€‚ç»è¿‡å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„ç ”ç©¶æˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03119', 'title': 'Controllable Human-centric Keyframe Interpolation with Generative Prior', 'url': 'https://huggingface.co/papers/2506.03119', 'abstract': 'Existing interpolation methods use pre-trained video diffusion priors to generate intermediate frames between sparsely sampled keyframes. In the absence of 3D geometric guidance, these methods struggle to produce plausible results for complex, articulated human motions and offer limited control over the synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a novel framework that integrates 3D human guidance signals into the diffusion process for Controllable Human-centric Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for interpolation, our PoseFuse3D, a 3D-informed control model, features a novel SMPL-X encoder that transforms 3D geometry and shape into the 2D latent conditioning space, alongside a fusion network that integrates these 3D cues with 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset annotated with both 2D poses and 3D SMPL-X parameters. We show that PoseFuse3D-KI consistently outperforms state-of-the-art baselines on CHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS. Comprehensive ablations demonstrate that our PoseFuse3D model improves interpolation fidelity.', 'score': 2, 'issue_id': 4125, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '1fe3ed71536b27c9', 'authors': ['Zujin Guo', 'Size Wu', 'Zhongang Cai', 'Wei Li', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.03119.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#3d', '#cv'], 'emoji': 'ğŸ•º', 'ru': {'title': '3D-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'PoseFuse3D-KI - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ 3D-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ·Ğµ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğµ Ñ‚ĞµĞ»Ğ°. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PoseFuse3D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ² 2D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CHKI-Video Ñ 2D Ğ¸ 3D Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ PSNR Ğ½Ğ° 9% Ğ¸ LPIPS Ğ½Ğ° 38%.'}, 'en': {'title': 'Enhancing Video Frame Interpolation with 3D Human Guidance', 'desc': 'This paper presents PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a new method for generating intermediate video frames using 3D human motion guidance. Traditional methods struggle with complex human movements and lack control over the generated dynamics, but PoseFuse3D-KI incorporates 3D geometric signals to enhance the interpolation process. The framework utilizes a novel SMPL-X encoder to convert 3D shapes into a 2D latent space, allowing for better integration of spatial cues with 2D pose data. Evaluation on the new CHKI-Video dataset shows significant improvements in interpolation quality, outperforming existing methods in both PSNR and LPIPS metrics.'}, 'zh': {'title': 'PoseFuse3Dï¼šå¯æ§çš„äººä½“å…³é”®å¸§æ’å€¼æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶PoseFuse3Då…³é”®å¸§æ’å€¼å™¨ï¼ˆPoseFuse3D-KIï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å°†3Däººä½“å¼•å¯¼ä¿¡å·æ•´åˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå®ç°å¯æ§çš„äººä½“ä¸­å¿ƒå…³é”®å¸§æ’å€¼ï¼ˆCHKIï¼‰ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPoseFuse3D-KIèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„äººä½“è¿åŠ¨ï¼Œå¹¶æä¾›æ›´é«˜çš„åˆæˆåŠ¨æ€æ§åˆ¶èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿ç”¨äº†ä¸€ç§æ–°é¢–çš„SMPL-Xç¼–ç å™¨ï¼Œå°†3Då‡ ä½•å½¢çŠ¶è½¬æ¢ä¸º2Dæ½œåœ¨æ¡ä»¶ç©ºé—´ï¼Œå¹¶é€šè¿‡èåˆç½‘ç»œå°†è¿™äº›3Dçº¿ç´¢ä¸2Då§¿æ€åµŒå…¥ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPoseFuse3D-KIåœ¨CHKI-Videoæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼ŒPSNRæé«˜äº†9%ï¼ŒLPIPSå‡å°‘äº†38%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02678', 'title': 'TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression', 'url': 'https://huggingface.co/papers/2506.02678', 'abstract': "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.", 'score': 2, 'issue_id': 4124, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '7c7528bb32eeb5a4', 'authors': ['Zhong-Zhi Li', 'Xiao Liang', 'Zihao Tang', 'Lei Ji', 'Peijie Wang', 'Haotian Xu', 'Xing W', 'Haizhen Huang', 'Weiwei Deng', 'Ying Nian Wu', 'Yeyun Gong', 'Zhijiang Guo', 'Xiao Liu', 'Fei Yin', 'Cheng-Lin Liu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology (Guangzhou)', 'Institute of Automation, Chinese Academy of Sciences', 'Microsoft', 'School of Artificial Intelligence, Chinese Academy of Sciences', 'Tsinghua University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.02678.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#long_context', '#optimization', '#reasoning'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² LLM: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ²ĞµÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ System-1 Ğ¸ System-2. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğ° 40%, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Reasoning in LLMs with Dynamic Training', 'desc': "This paper presents a new training method for Large Language Models (LLMs) that improves their reasoning efficiency during inference, especially for long outputs. The proposed dynamic ratio-based training pipeline balances the model's System-1 and System-2 data without needing complex data annotations. By doing so, it reduces unnecessary reasoning steps while keeping the model's reasoning abilities intact. The results show a significant reduction in output tokens by about 40%, while still achieving high accuracy across various benchmarks."}, 'zh': {'title': 'åŠ¨æ€æ¯”ä¾‹è®­ç»ƒï¼Œæå‡æ¨ç†æ•ˆç‡ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€æ¯”ä¾‹çš„è®­ç»ƒæµç¨‹ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶çš„æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æé•¿è¾“å‡ºæ—¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¹³è¡¡æ¨¡å‹çš„System-1å’ŒSystem-2æ•°æ®çš„æƒé‡ï¼Œæ¶ˆé™¤å†—ä½™æ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨DeepSeek-R1-Distill-7Bå’ŒDeepSeek-R1-Distill-14Bæ¨¡å‹ä¸Šæ˜¾è‘—å‡å°‘äº†è¿‘40%çš„è¾“å‡ºæ ‡è®°ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†å¾ˆå¿«å…¬å¼€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02510', 'title': 'M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset', 'url': 'https://huggingface.co/papers/2506.02510', 'abstract': "A new multilingual, multi-sector, and multi-task benchmark, MÂ³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called M^3FinMeeting, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, M^3FinMeeting supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, M^3FinMeeting includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of M^3FinMeeting as a benchmark for assessing LLMs' financial meeting comprehension skills.", 'score': 2, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '903bb57b5cc664ec', 'authors': ['Jie Zhu', 'Junhui Li', 'Yalong Wen', 'Xiandong Li', 'Lifan Guo', 'Feng Chen'], 'affiliations': ['Nanjing University', 'Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02510.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#science', '#multilingual', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'MÂ³FinMeeting: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ²ÑÑ‚Ñ€ĞµÑ‡ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MÂ³FinMeeting Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ²ÑÑ‚Ñ€ĞµÑ‡ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼, ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ¸ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ GICS. MÂ³FinMeeting ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞµĞ¼ÑŒÑ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'MÂ³FinMeeting: Bridging Language Gaps in Financial Comprehension', 'desc': 'The MÂ³FinMeeting benchmark is designed to evaluate large language models (LLMs) in understanding financial meetings across multiple languages and sectors. It addresses the limitations of existing benchmarks that primarily focus on static financial documents like news articles. This new dataset supports English, Chinese, and Japanese, allowing for a broader assessment of multilingual financial discussions. Additionally, it includes tasks such as summarization and question answering, highlighting the need for LLMs to improve their comprehension of dynamic financial interactions.'}, 'zh': {'title': 'MÂ³FinMeetingï¼šé‡‘èä¼šè®®ç†è§£çš„æ–°åŸºå‡†', 'desc': 'MÂ³FinMeetingæ˜¯ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€ã€å¤šè¡Œä¸šå’Œå¤šä»»åŠ¡çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£é‡‘èä¼šè®®æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ”¯æŒè‹±è¯­ã€ä¸­æ–‡å’Œæ—¥è¯­ï¼Œå¢å¼ºäº†å¯¹ä¸åŒè¯­è¨€ç¯å¢ƒä¸­é‡‘èè®¨è®ºçš„ç†è§£ã€‚å®ƒæ¶µç›–äº†å…¨çƒè¡Œä¸šåˆ†ç±»æ ‡å‡†ï¼ˆGICSï¼‰å®šä¹‰çš„å¤šä¸ªè¡Œä¸šï¼Œç¡®ä¿åŸºå‡†èƒ½å¤Ÿè¦†ç›–å¹¿æ³›çš„é‡‘èæ´»åŠ¨ã€‚é€šè¿‡å¯¹ä¸ƒç§æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®éªŒï¼Œç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨ç†è§£èƒ½åŠ›ä¸Šä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ï¼Œè¯æ˜äº†MÂ³FinMeetingä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹é‡‘èä¼šè®®ç†è§£èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02454', 'title': 'Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework', 'url': 'https://huggingface.co/papers/2506.02454', 'abstract': 'A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\\% overall win rate over the baseline method.', 'score': 2, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '9220f780a7fba411', 'authors': ['Zhaorui Yang', 'Bo Pan', 'Han Wang', 'Yiyao Wang', 'Xingyu Liu', 'Minfeng Zhu', 'Bo Zhang', 'Wei Chen'], 'affiliations': ['State Key Lab of CAD&CG, Zhejiang University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02454.jpg', 'data': {'categories': ['#agents', '#multimodal', '#reasoning', '#optimization', '#games', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'ĞĞ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Multimodal DeepResearcher Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ‚ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Multimodal DeepResearcher Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Revolutionizing Reports: Text Meets Visualization with Multimodal DeepResearcher', 'desc': 'The paper introduces Multimodal DeepResearcher, a framework that allows Large Language Models (LLMs) to create detailed reports that combine text and various visualizations. It addresses the challenge of integrating informative visualizations with text, which has been largely overlooked in previous research. The framework uses a structured representation called Formal Description of Visualization (FDV) to help LLMs generate high-quality visual content. Through a systematic approach involving research, report creation, planning, and multimodal generation, the framework shows significant improvements in report quality compared to existing methods.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ·±åº¦ç ”ç©¶è€…ï¼šæ–‡æœ¬ä¸å¯è§†åŒ–çš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºå¤šæ¨¡æ€æ·±åº¦ç ”ç©¶è€…ï¼ˆMultimodal DeepResearcherï¼‰ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€æŠ¥å‘Šï¼Œè¿™äº›æŠ¥å‘Šç»“åˆäº†æ–‡æœ¬å’Œå¤šç§å¯è§†åŒ–å½¢å¼ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–çš„æ–‡æœ¬è¡¨ç¤ºï¼Œè§£å†³äº†æ–‡æœ¬ä¸å¯è§†åŒ–å†…å®¹äº¤ç»‡ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œæå‡äº†ä¿¡æ¯ä¼ è¾¾çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†å¯è§†åŒ–çš„æ­£å¼æè¿°ï¼ˆFDVï¼‰ï¼Œä½¿å¾—LLMsèƒ½å¤Ÿå­¦ä¹ å¹¶ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡å¯è§†åŒ–å›¾è¡¨ã€‚é€šè¿‡å››ä¸ªé˜¶æ®µçš„ä»»åŠ¡åˆ†è§£ï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆå¤šæ¨¡æ€æŠ¥å‘Šæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨æ€§èƒ½ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02295', 'title': 'QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large\n  Language Model Adaptation', 'url': 'https://huggingface.co/papers/2506.02295', 'abstract': 'Qari-OCR, a series of fine-tuned vision-language models, achieves state-of-the-art performance in Arabic OCR through iterative optimization on specialized datasets, handling diacritics, fonts, layouts, and low-resolution images.  \t\t\t\t\tAI-generated summary \t\t\t\t The inherent complexities of Arabic script; its cursive nature, diacritical marks (tashkeel), and varied typography, pose persistent challenges for Optical Character Recognition (OCR). We present Qari-OCR, a series of vision-language models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic through iterative fine-tuning on specialized synthetic datasets. Our leading model, QARI v0.2, establishes a new open-source state-of-the-art with a Word Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling of tashkeel, diverse fonts, and document layouts, alongside impressive performance on low-resolution images. Further explorations (QARI v0.3) showcase strong potential for structural document understanding and handwritten text. This work delivers a marked improvement in Arabic OCR accuracy and efficiency, with all models and datasets released to foster further research.', 'score': 2, 'issue_id': 4119, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'b031659260f990f9', 'authors': ['Ahmed Wasfy', 'Omer Nacar', 'Abdelakreem Elkhateb', 'Mahmoud Reda', 'Omar Elshehy', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['KAND CA Corp.', 'NAMAA', 'Prince Sultan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02295.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#dataset', '#synthetic', '#cv', '#open_source', '#optimization'], 'emoji': 'ğŸ“š', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Qari-OCR Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ OCR Ğ´Ğ»Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Qari-OCR ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¹ Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ğ°ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°ĞºĞ¸, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑˆÑ€Ğ¸Ñ„Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Arabic OCR with Qari-OCR', 'desc': 'Qari-OCR is a series of advanced vision-language models specifically designed to improve Optical Character Recognition (OCR) for Arabic text. It addresses the unique challenges of Arabic script, such as its cursive nature and diacritical marks, by using iterative fine-tuning on specialized datasets. The leading model, QARI v0.2, achieves impressive metrics with a Word Error Rate (WER) of 0.160 and a Character Error Rate (CER) of 0.061, setting a new benchmark in the field. This work not only enhances OCR accuracy for Arabic but also supports further research by making all models and datasets publicly available.'}, 'zh': {'title': 'Qari-OCRï¼šé˜¿æ‹‰ä¼¯è¯­OCRçš„æ–°çªç ´', 'desc': 'Qari-OCRæ˜¯ä¸€ç³»åˆ—ç»è¿‡å¾®è°ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰è¿›è¡Œä¼˜åŒ–ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨ç‰¹å®šçš„åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè¿­ä»£å¾®è°ƒï¼ŒæˆåŠŸå¤„ç†äº†é˜¿æ‹‰ä¼¯è¯­çš„è¿å†™ç‰¹æ€§ã€å…ƒéŸ³ç¬¦å·å’Œå¤šæ ·çš„å­—ä½“å¸ƒå±€ã€‚æˆ‘ä»¬çš„ä¸»è¦æ¨¡å‹QARI v0.2åœ¨å¤„ç†å«æœ‰å…ƒéŸ³ç¬¦å·çš„æ–‡æœ¬æ—¶ï¼Œè¾¾åˆ°äº†0.160çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œ0.061çš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ï¼Œå¹¶åœ¨BLEUè¯„åˆ†ä¸Šå–å¾—äº†0.737çš„ä¼˜å¼‚æˆç»©ã€‚Qari-OCRåœ¨ä½åˆ†è¾¨ç‡å›¾åƒçš„å¤„ç†ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œä¸ºé˜¿æ‹‰ä¼¯è¯­OCRçš„å‡†ç¡®æ€§å’Œæ•ˆç‡å¸¦æ¥äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01565', 'title': 'Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation', 'url': 'https://huggingface.co/papers/2506.01565', 'abstract': 'Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation.The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10\\% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42\\%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation.', 'score': 2, 'issue_id': 4117, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '0251c50d35bd4a50', 'authors': ['Li Zhou', 'Lutong Yu', 'Dongchu Xie', 'Shaohuan Cheng', 'Wenyan Li', 'Haizhou Li'], 'affiliations': ['Chengdu Technological University', 'Shenzhen Research Institute of Big Data', 'The Chinese University of Hong Kong, Shenzhen', 'University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2506.01565.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark'], 'emoji': 'ğŸ‘˜', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Hanfu-Bench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ…Ğ°Ğ½ÑŒÑ„Ñƒ - Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ğµ, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹. Hanfu-Bench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€ĞµĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Bridging Time and Culture with Hanfu-Bench', 'desc': 'This paper introduces Hanfu-Bench, a new dataset designed to enhance the understanding of cultural evolution over time using vision-language models (VLMs). It focuses on Hanfu, a traditional Chinese garment, to explore both cultural visual understanding and the transformation of traditional attire into modern designs. The study reveals that while closed VLMs perform similarly to non-experts in recognizing cultural features, they still lag behind human experts. Additionally, the transcreation task shows that even the best models struggle to achieve high success rates, highlighting the challenges in temporal cultural understanding and creative adaptation.'}, 'zh': {'title': 'æ¢ç´¢æ—¶é—´ç»´åº¦çš„æ–‡åŒ–ç†è§£', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†Hanfu-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨å¡«è¡¥è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£ä¸­çš„æ—¶é—´ç»´åº¦ç¼ºå¤±ã€‚Hanfuä½œä¸ºä¸­å›½ä¼ ç»Ÿæœé¥°ï¼Œä»£è¡¨äº†æ·±åšçš„æ–‡åŒ–é—äº§ï¼Œåæ˜ äº†ä¸­å›½æ–‡åŒ–çš„æ—¶é—´ç‰¹å¾ã€‚æ•°æ®é›†åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šæ–‡åŒ–è§†è§‰ç†è§£å’Œæ–‡åŒ–å›¾åƒå†åˆ›ä½œï¼Œå‰è€…é€šè¿‡å¤šé€‰è§†è§‰é—®ç­”è¯„ä¼°æ—¶é—´æ–‡åŒ–ç‰¹å¾çš„è¯†åˆ«ï¼Œåè€…åˆ™å…³æ³¨ä¼ ç»Ÿæœé¥°å‘ç°ä»£è®¾è®¡çš„è½¬åŒ–ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°é—­å¼è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ–‡åŒ–ç†è§£ä¸Šä¸éä¸“å®¶ç›¸å½“ï¼Œä½†åœ¨æ—¶é—´æ–‡åŒ–ç†è§£å’Œåˆ›æ„é€‚åº”æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01004', 'title': 'Motion-Aware Concept Alignment for Consistent Video Editing', 'url': 'https://huggingface.co/papers/2506.01004', 'abstract': "MoCA-Video injects semantic features from a reference image into a video object, preserving motion and visual context, and outperforms baselines using novel metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a training-free framework bridging the gap between image-domain semantic mixing and video. Given a generated video and a user-provided reference image, MoCA-Video injects the semantic features of the reference image into a specific object within the video, while preserving the original motion and visual context. Our approach leverages a diagonal denoising schedule and class-agnostic segmentation to detect and track objects in the latent space and precisely control the spatial location of the blended objects. To ensure temporal coherence, we incorporate momentum-based semantic corrections and gamma residual noise stabilization for smooth frame transitions. We evaluate MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS, and introduce a novel metric CASS (Conceptual Alignment Shift Score) to evaluate the consistency and effectiveness of the visual shifts between the source prompt and the modified video frames. Using self-constructed dataset, MoCA-Video outperforms current baselines, achieving superior spatial consistency, coherent motion, and a significantly higher CASS score, despite having no training or fine-tuning. MoCA-Video demonstrates that structured manipulation in the diffusion noise trajectory allows for controllable, high-quality video synthesis.", 'score': 2, 'issue_id': 4127, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '30605ac1bef5d7b2', 'authors': ['Tong Zhang', 'Juan C Leon Alcazar', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2506.01004.jpg', 'data': {'categories': ['#dataset', '#video', '#benchmark', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'MoCA-Video - ÑÑ‚Ğ¾ Ğ±ĞµĞ·Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¾Ğ±ÑŠĞµĞºÑ‚. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ° Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. MoCA-Video Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ CASS.'}, 'en': {'title': 'Seamlessly Blending Images into Videos with MoCA-Video!', 'desc': 'MoCA-Video is a framework that enhances videos by integrating semantic features from a reference image while maintaining the original motion and visual context. It operates without the need for training, using techniques like diagonal denoising and class-agnostic segmentation to accurately track and blend objects in the video. To ensure smooth transitions between frames, it employs momentum-based corrections and noise stabilization. The framework is evaluated using standard metrics and a new metric called CASS, showing significant improvements in spatial consistency and motion coherence compared to existing methods.'}, 'zh': {'title': 'æ— è®­ç»ƒè§†é¢‘åˆæˆçš„æ–°çªç ´', 'desc': 'MoCA-Videoï¼ˆè¿åŠ¨æ„ŸçŸ¥æ¦‚å¿µå¯¹é½è§†é¢‘ï¼‰æ˜¯ä¸€ç§æ— è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å°†å›¾åƒé¢†åŸŸçš„è¯­ä¹‰æ··åˆä¸è§†é¢‘ç»“åˆã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å‚è€ƒå›¾åƒçš„è¯­ä¹‰ç‰¹å¾æ³¨å…¥è§†é¢‘ä¸­çš„ç‰¹å®šå¯¹è±¡ï¼ŒåŒæ—¶ä¿æŒåŸæœ‰çš„è¿åŠ¨å’Œè§†è§‰ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬é‡‡ç”¨å¯¹è§’å»å™ªè°ƒåº¦å’Œç±»æ— å…³åˆ†å‰²æŠ€æœ¯æ¥æ£€æµ‹å’Œè·Ÿè¸ªæ½œåœ¨ç©ºé—´ä¸­çš„å¯¹è±¡ï¼Œå¹¶ç²¾ç¡®æ§åˆ¶æ··åˆå¯¹è±¡çš„ç©ºé—´ä½ç½®ã€‚é€šè¿‡å¼•å…¥åŸºäºåŠ¨é‡çš„è¯­ä¹‰ä¿®æ­£å’Œä¼½é©¬æ®‹å·®å™ªå£°ç¨³å®šåŒ–ï¼Œæˆ‘ä»¬ç¡®ä¿äº†è§†é¢‘å¸§ä¹‹é—´çš„æ—¶é—´ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00227', 'title': 'Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes', 'url': 'https://huggingface.co/papers/2506.00227', 'abstract': 'Ctrl-Crash, a controllable car crash video generation model using classifier-free guidance, achieves top performance in video quality and realism compared to existing diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.', 'score': 2, 'issue_id': 4126, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '894e903b8da58314', 'authors': ['Anthony Gosselin', 'Ge Ya Luo', 'Luis Lara', 'Florian Golemo', 'Derek Nowrouzezahrai', 'Liam Paull', 'Alexia Jolicoeur-Martineau', 'Christopher Pal'], 'affiliations': ['CIFAR AI Chair', 'McGill University', 'Mila', 'Polytechnique MontrÃ©al', 'Samsung SAIL MontrÃ©al', 'UniversitÃ© de MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2506.00227.jpg', 'data': {'categories': ['#multimodal', '#inference', '#diffusion', '#video'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ctrl-Crash - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ€Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ°Ğ²Ğ°Ñ€Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸, Ñ‚Ğ¸Ğ¿Ñ‹ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ctrl-Crash Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°.'}, 'en': {'title': 'Revolutionizing Crash Simulations with Ctrl-Crash', 'desc': 'Ctrl-Crash is a novel model designed to generate realistic car crash videos using advanced video diffusion techniques. It addresses the challenge of limited accident data by allowing users to control various aspects of the crash scenarios, such as bounding boxes and crash types. By employing classifier-free guidance, the model can produce diverse outcomes from slight changes in input, enhancing its utility for traffic safety simulations. The model outperforms existing methods in both quantitative metrics and human evaluations of video quality and realism.'}, 'zh': {'title': 'å¯æ§æ±½è½¦ç¢°æ’è§†é¢‘ç”Ÿæˆçš„çªç ´æ€§è¿›å±•', 'desc': 'Ctrl-Crashæ˜¯ä¸€ç§å¯æ§çš„æ±½è½¦ç¢°æ’è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’ŒçœŸå®æ„Ÿçš„è§†é¢‘ã€‚ç”±äºå¤§å¤šæ•°é©¾é©¶æ•°æ®é›†ä¸­äº‹æ•…äº‹ä»¶ç¨€ç¼ºï¼Œç°æœ‰çš„è§†é¢‘æ‰©æ•£æŠ€æœ¯åœ¨ç”ŸæˆçœŸå®çš„æ±½è½¦ç¢°æ’å›¾åƒæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é€šè¿‡è¾¹ç•Œæ¡†ã€ç¢°æ’ç±»å‹å’Œåˆå§‹å›¾åƒå¸§ç­‰ä¿¡å·è¿›è¡Œæ¡ä»¶æ§åˆ¶ï¼Œæ”¯æŒåäº‹å®åœºæ™¯ç”Ÿæˆï¼Œä½¿å¾—è¾“å…¥çš„å¾®å°å˜åŒ–å¯ä»¥å¯¼è‡´æˆªç„¶ä¸åŒçš„ç¢°æ’ç»“æœã€‚Ctrl-Crashåœ¨è§†é¢‘è´¨é‡æŒ‡æ ‡ï¼ˆå¦‚FVDå’ŒJEDiï¼‰å’ŒåŸºäºäººç±»è¯„ä¼°çš„ç‰©ç†çœŸå®æ„Ÿå’Œè§†é¢‘è´¨é‡æ–¹é¢ï¼Œå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.16994', 'title': 'R^2ec: Towards Large Recommender Models with Reasoning', 'url': 'https://huggingface.co/papers/2505.16994', 'abstract': 'A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. Current studies usually position LLMs as external reasoning modules to yield auxiliary thought for augmenting conventional recommendation pipelines. However, such decoupled designs are limited in significant resource cost and suboptimal joint optimization. To address these issues, we propose \\name, a unified large recommender model with intrinsic reasoning capabilities. Initially, we reconceptualize the model architecture to facilitate interleaved reasoning and recommendation in the autoregressive process. Subsequently, we propose RecPO, a corresponding reinforcement learning framework that optimizes \\name\\ both the reasoning and recommendation capabilities simultaneously in a single policy update; RecPO introduces a fused reward scheme that solely leverages recommendation labels to simulate the reasoning capability, eliminating dependency on specialized reasoning annotations. Experiments on three datasets with various baselines verify the effectiveness of \\name, showing relative improvements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at https://github.com/YRYangang/RRec.', 'score': 2, 'issue_id': 4111, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': '6b76c655943245ca', 'authors': ['Runyang You', 'Yongqi Li', 'Xinyu Lin', 'Xin Zhang', 'Wenjie Wang', 'Wenjie Li', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology (Shenzhen)', 'National University of Singapore', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16994.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#architecture', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° RecPO Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Hit@5 Ğ¸ NDCG@20 Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unified Reasoning and Recommendation for Enhanced Performance', 'desc': "This paper introduces a new large recommender model that integrates reasoning capabilities directly into the recommendation process. The model, named RecPO, uses a reinforcement learning framework to optimize both reasoning and recommendation in a unified manner. By allowing these two processes to work together, the model reduces resource costs and improves performance compared to traditional methods that treat them separately. Experiments demonstrate significant improvements in recommendation metrics, showcasing the model's effectiveness in real-world applications."}, 'zh': {'title': 'ç»Ÿä¸€æ¨èæ¨¡å‹ï¼Œæ¨ç†ä¸æ¨èçš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤§å‹æ¨èæ¨¡å‹ï¼Œå…·å¤‡å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ¨èè¿‡ç¨‹ä¸­å®ç°äº¤é”™æ¨ç†ä¸æ¨èã€‚æˆ‘ä»¬é‡æ–°æ„æ€äº†æ¨¡å‹æ¶æ„ï¼Œä½¿å…¶åœ¨è‡ªå›å½’è¿‡ç¨‹ä¸­åŒæ—¶è¿›è¡Œæ¨ç†å’Œæ¨èã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†RecPOï¼Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å•æ¬¡ç­–ç•¥æ›´æ–°ä¸­åŒæ—¶ä¼˜åŒ–æ¨ç†å’Œæ¨èèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šç›¸è¾ƒäºåŸºçº¿æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03144', 'title': 'MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query', 'url': 'https://huggingface.co/papers/2506.03144', 'abstract': "Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.", 'score': 1, 'issue_id': 4118, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ½Ñ', 'en': 'June 3', 'zh': '6æœˆ3æ—¥'}, 'hash': '5e57a72b2bce8a15', 'authors': ['Wei Chow', 'Yuan Gao', 'Linfeng Li', 'Xian Wang', 'Qi Xu', 'Hang Song', 'Lingdong Kong', 'Ran Zhou', 'Yi Zeng', 'Yidong Cai', 'Botian Jiang', 'Shilin Xu', 'Jiajun Zhang', 'Minghui Qiu', 'Xiangtai Li', 'Tianshu Yang', 'Siliang Tang', 'Juncheng Li'], 'affiliations': ['ByteDance Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03144.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#multilingual', '#transfer_learning', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ: MERIT Ğ¸ Coral', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MERIT - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Coral, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Coral Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 45.9% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° MERIT.'}, 'en': {'title': 'Revolutionizing Semantic Retrieval with MERIT and Coral', 'desc': 'This paper addresses the challenges in semantic retrieval, particularly in multilingual and multi-condition scenarios. It introduces MERIT, a new dataset with 320,000 queries across five languages and seven product categories, which highlights the limitations of current models that overlook specific query conditions. The authors propose Coral, a fine-tuning framework that enhances pre-trained multilingual language models by focusing on both global semantics and fine-grained conditional elements. Experimental results show that Coral significantly outperforms traditional methods, paving the way for improved semantic retrieval in complex environments.'}, 'zh': {'title': 'å¼€åˆ›å¤šæ¡ä»¶è¯­ä¹‰æ£€ç´¢çš„æ–°çºªå…ƒ', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è¯­ä¹‰æ£€ç´¢åœ¨ç°ä»£åº”ç”¨ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå½“å‰ç ”ç©¶çš„ä¸è¶³ä¹‹å¤„ã€‚ç°æœ‰çš„æ•°æ®é›†é€šå¸¸åªé™äºå•ä¸€è¯­è¨€æˆ–å•ä¸€å›¾åƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯çš„è¡¨è¾¾èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†MERITï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºäº¤é”™å¤šæ¡ä»¶è¯­ä¹‰æ£€ç´¢çš„å¤šè¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«320,000ä¸ªæŸ¥è¯¢å’Œ135,000ä¸ªäº§å“ï¼Œè¦†ç›–7ä¸ªä¸åŒçš„äº§å“ç±»åˆ«ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Coralï¼Œä¸€ä¸ªæ–°çš„å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆåµŒå…¥é‡å»ºå’Œå¯¹æ¯”å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨MERITä¸Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.02138', 'title': 'Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability', 'url': 'https://huggingface.co/papers/2506.02138', 'abstract': 'A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available.', 'score': 1, 'issue_id': 4115, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'c83ce7f2cc033984', 'authors': ['Yarden Bakish', 'Itamar Zimerman', 'Hila Chefer', 'Lior Wolf'], 'affiliations': ['Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02138.jpg', 'data': {'categories': ['#training', '#interpretability', '#open_source', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Layer-wise Relevance Propagation (LRP) Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ğ°Ñ€ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ-Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° LRP Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Enhancing Transformer Explainability with Positional Encoding in LRP', 'desc': 'This paper introduces a specialized Layer-wise Relevance Propagation (LRP) method tailored for Transformer models, focusing on the importance of positional encoding in explainability. Traditional LRP methods fail to account for positional encoding, which leads to a loss of critical relevance information tied to the structure and position of tokens. By reformulating the input space into position-token pairs, the authors develop new LRP rules that effectively propagate relevance across different types of positional encodings. The proposed method shows significant improvements over existing techniques in explainability tasks for both vision and natural language processing, validated through extensive experiments.'}, 'zh': {'title': 'æå‡Transformerå¯è§£é‡Šæ€§çš„ä¸“é—¨LRPæ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹Transformeræ¨¡å‹çš„ä¸“é—¨åŒ–å±‚æ¬¡ç›¸å…³ä¼ æ’­ï¼ˆLRPï¼‰æ–¹æ³•ï¼Œè€ƒè™‘äº†ä½ç½®ç¼–ç çš„å½±å“ï¼Œä»è€Œæ”¹å–„äº†ç›¸å…³æ€§ä¼ æ’­ã€‚ç°æœ‰çš„LRPæ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨Transformeræ¶æ„ä¸­çš„ä½ç½®ç¼–ç ï¼Œå¯¼è‡´äº†é‡è¦ç›¸å…³æ€§çš„ä¸¢å¤±ã€‚æˆ‘ä»¬é€šè¿‡å°†è¾“å…¥ç©ºé—´é‡æ–°æ„å»ºä¸ºä½ç½®-æ ‡è®°å¯¹ï¼Œæå‡ºäº†ç†è®ºåŸºç¡€çš„LRPè§„åˆ™ï¼Œä»¥é€‚åº”ä¸åŒçš„ä½ç½®ä¿¡æ¯ç¼–ç æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„å¯è§£é‡Šæ€§ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01265', 'title': 'Beyond In-Context Learning: Aligning Long-form Generation of Large\n  Language Models via Task-Inherent Attribute Guidelines', 'url': 'https://huggingface.co/papers/2506.01265', 'abstract': 'In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.', 'score': 1, 'issue_id': 4124, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '2f44ffeb11509c5c', 'authors': ['Do Xuan Long', 'Duong Ngoc Yen', 'Do Xuan Trong', 'Luu Anh Tuan', 'Kenji Kawaguchi', 'Shafiq Joty', 'Min-Yen Kan', 'Nancy F. Chen'], 'affiliations': ['Institute for Infocomm Research (I2R), A*STAR', 'Nanyang Technological University, Singapore', 'National University of Singapore', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.01265.jpg', 'data': {'categories': ['#data', '#training', '#long_context', '#optimization', '#multimodal'], 'emoji': 'ğŸ“', 'ru': {'title': 'LongGuide: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ (ICL) Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¸Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ ICL Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ LongGuide, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. LongGuide Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM.'}, 'en': {'title': 'Enhancing Long-Form Generation with LongGuide', 'desc': 'This paper explores the concept of in-context learning (ICL) in large language models (LLMs), highlighting its effectiveness in improving task performance with minimal examples. However, it identifies limitations in long-form generation tasks, such as summarization, where ICL alone does not adequately teach the necessary language and format distributions. The authors propose a solution called LongGuide, which generates guidelines to enhance model performance by providing explicit exposure to task distributions. LongGuide combines Metric Guidelines and Output Constraint Guidelines to optimize generation, resulting in significant performance improvements across various LLMs in both zero- and few-shot scenarios.'}, 'zh': {'title': 'æå‡ç”Ÿæˆä»»åŠ¡æ€§èƒ½çš„LongGuideæ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…ä¾é ç¤ºä¾‹ï¼ˆdemonstrationsï¼‰ä¸è¶³ä»¥è®©æ¨¡å‹æŒæ¡ç”Ÿæˆä»»åŠ¡çš„è¯­è¨€å’Œæ ¼å¼åˆ†å¸ƒã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†LongGuideï¼Œé€šè¿‡ç”Ÿæˆä¸¤æ¡å¹¶è¡Œçš„æŒ‡å¯¼æµï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡è¦æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLongGuideèƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œå­¦ä¹ èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24362', 'title': 'Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion', 'url': 'https://huggingface.co/papers/2505.24362', 'abstract': "We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well even before a single token is generated, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.", 'score': 1, 'issue_id': 4118, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '8bbcc31124760dad', 'authors': ['Anum Afzal', 'Florian Matthes', 'Gal Chechik', 'Yftah Ziser'], 'affiliations': ['Bar-Ilan University', 'Nvidia Research', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.24362.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ½Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑ…Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğ¾Ğ¼ (zero-shot Chain-of-Thought) Ğ´Ğ¾ ĞµĞ³Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½Ğ½ĞµĞµ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ CoT, Ñ…Ğ¾Ñ‚Ñ Ğ¸ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Early Insights in Chain-of-Thought Reasoning', 'desc': 'This paper explores the predictability of success in zero-shot Chain-of-Thought (CoT) reasoning processes before they are fully completed. The authors find that a probing classifier using representations from large language models (LLMs) can effectively predict outcomes even before generating any tokens, indicating that essential reasoning information is present early on. In contrast, a BERT-based model that relies on generated tokens performs poorly, as it focuses on superficial linguistic features rather than deeper reasoning. The study suggests that early stopping in CoT reasoning can still yield better performance than not using CoT at all, and proposes that future methods could utilize their classifier to determine when early stopping is beneficial.'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†æ•ˆç‡ï¼Œæ—©æœŸåœæ­¢ä¹Ÿèƒ½æœ‰æ•ˆ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†é›¶-shot Chain-of-Thought (CoT) è¿‡ç¨‹çš„æˆåŠŸæ˜¯å¦å¯ä»¥åœ¨å®Œæˆä¹‹å‰è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç¤ºçš„æ¢æµ‹åˆ†ç±»å™¨åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªæ ‡è®°ä¹‹å‰å°±è¡¨ç°è‰¯å¥½ï¼Œè¿™è¡¨æ˜æ¨ç†è¿‡ç¨‹ä¸­çš„å…³é”®ä¿¡æ¯åœ¨åˆå§‹æ­¥éª¤çš„è¡¨ç¤ºä¸­å·²ç»å­˜åœ¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¾èµ–ç”Ÿæˆæ ‡è®°çš„å¼ºBERTåŸºçº¿è¡¨ç°è¾ƒå·®ï¼Œå¯èƒ½æ˜¯å› ä¸ºå®ƒä¾èµ–äºæµ…å±‚è¯­è¨€çº¿ç´¢è€Œéæ›´æ·±å±‚çš„æ¨ç†åŠ¨æ€ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ—©æœŸåœæ­¢æ¨ç†å¯ä»¥æé«˜æ€§èƒ½ï¼Œå°½ç®¡ä¸å®Œæ•´æ¨ç†ç›¸æ¯”ä»æœ‰å·®è·ï¼Œè¿™ä¸ºä¼˜åŒ–CoTçš„æ•ˆç‡æä¾›äº†æ–°çš„æ€è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15675', 'title': 'Sekai: A Video Dataset towards World Exploration', 'url': 'https://huggingface.co/papers/2506.15675', 'abstract': "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.", 'score': 33, 'issue_id': 4373, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '4f989f259f55f0ee', 'authors': ['Zhen Li', 'Chuanhao Li', 'Xiaofeng Mao', 'Shaoheng Lin', 'Ming Li', 'Shitian Zhao', 'Zhaopan Xu', 'Xinyue Li', 'Yukang Feng', 'Jianwen Sun', 'Zizhen Li', 'Fanrui Zhang', 'Jiaxin Ai', 'Zhixiang Wang', 'Yuwei Wu', 'Tong He', 'Jiangmiao Pang', 'Yu Qiao', 'Yunde Jia', 'Kaipeng Zhang'], 'affiliations': ['Beijing Institute of Technology', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shenzhen MSU-BIT University', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2506.15675.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#games', '#data', '#video'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Sekai: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¸Ñ€', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Sekai Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 5000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 ÑÑ‚Ñ€Ğ°Ğ½ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ YUME Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Sekai: Unlocking the World Through Video Generation', 'desc': 'This paper presents Sekai, a new video dataset designed to enhance video generation models for world exploration applications. Sekai includes over 5,000 hours of first-person view videos from diverse locations, addressing the limitations of existing datasets by providing rich annotations such as location, scene, and weather conditions. The authors also introduce a toolbox for efficient video collection and annotation, ensuring high-quality data for training. The dataset is validated through experiments and is used to train an interactive video exploration model called YUME, showcasing its potential impact on video generation and exploration technologies.'}, 'zh': {'title': 'Sekaiï¼šå…¨çƒæ¢ç´¢çš„æ–°è§†é‡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSekaiçš„å…¨çƒè§†é¢‘æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒä¸–ç•Œæ¢ç´¢åº”ç”¨å¹¶å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª100å¤šä¸ªå›½å®¶å’Œåœ°åŒºçš„5000å¤šä¸ªå°æ—¶çš„ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ï¼Œæ¶µç›–750ä¸ªåŸå¸‚ï¼Œå…·æœ‰ä¸°å¯Œçš„æ³¨é‡Šä¿¡æ¯ã€‚Sekaiè§£å†³äº†ç°æœ‰è§†é¢‘ç”Ÿæˆæ•°æ®é›†åœ¨ä½ç½®ã€æ—¶é•¿ã€åœºæ™¯é™æ€æ€§å’Œæ¢ç´¢æ³¨é‡Šç­‰æ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡å®éªŒéªŒè¯äº†æ•°æ®é›†çš„è´¨é‡ï¼Œå¹¶ä½¿ç”¨å…¶å­é›†è®­ç»ƒäº†ä¸€ä¸ªåä¸ºYUMEçš„äº’åŠ¨è§†é¢‘ä¸–ç•Œæ¢ç´¢æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15211', 'title': 'ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning\n  in LLMs', 'url': 'https://huggingface.co/papers/2506.15211', 'abstract': 'ProtoReasoning enhances large reasoning models through prototypical representations, leading to improved cross-domain generalization in logical reasoning, planning, and other tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain generalization capabilities. However, the underlying mechanisms supporting such transfer remain poorly understood. We hypothesize that cross-domain generalization arises from shared abstract reasoning prototypes -- fundamental reasoning patterns that capture the essence of problems across domains. These prototypes minimize the nuances of the representation, revealing that seemingly diverse tasks are grounded in shared reasoning structures.Based on this hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning ability of LLMs by leveraging scalable and verifiable prototypical representations (Prolog for logical reasoning, PDDL for planning).ProtoReasoning features: (1) an automated prototype construction pipeline that transforms problems into corresponding prototype representations; (2) a comprehensive verification system providing reliable feedback through Prolog/PDDL interpreters; (3) the scalability to synthesize problems arbitrarily within prototype space while ensuring correctness. Extensive experiments show that ProtoReasoning achieves 4.7% improvement over baseline models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics (AIME24). Significantly, our ablation studies confirm that learning in prototype space also demonstrates enhanced generalization to structurally similar problems compared to training solely on natural language representations, validating our hypothesis that reasoning prototypes serve as the foundation for generalizable reasoning in large language models.', 'score': 20, 'issue_id': 4380, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'a2ce33b1fd959b89', 'authors': ['Feng He', 'Zijun Chen', 'Xinnian Liang', 'Tingting Ma', 'Yunqi Qiu', 'Shuangzhi Wu', 'Junchi Yan'], 'affiliations': ['ByteDance Seed', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15211.jpg', 'data': {'categories': ['#reasoning', '#transfer_learning', '#training', '#dataset', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ProtoReasoning - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ· Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ProtoReasoning Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Generalization with Prototypical Reasoning', 'desc': 'ProtoReasoning is a framework designed to improve the reasoning capabilities of large language models (LLMs) by utilizing prototypical representations. It posits that shared abstract reasoning patterns, or prototypes, enable better generalization across different domains by simplifying complex tasks into fundamental structures. The framework includes an automated pipeline for creating these prototypes, a verification system for ensuring accuracy, and the ability to generate a wide range of problems within the prototype space. Experimental results show that ProtoReasoning significantly enhances performance in logical reasoning, planning, and general reasoning tasks compared to traditional methods.'}, 'zh': {'title': 'åŸå‹æ¨ç†ï¼šæå‡æ¨ç†æ¨¡å‹çš„è·¨é¢†åŸŸèƒ½åŠ›', 'desc': 'ProtoReasoning æ˜¯ä¸€ç§å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡åŸå‹è¡¨ç¤ºæ¥æé«˜è·¨é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å‡è®¾è·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›æºäºå…±äº«çš„æŠ½è±¡æ¨ç†åŸå‹ï¼Œè¿™äº›åŸå‹æ•æ‰äº†ä¸åŒé¢†åŸŸé—®é¢˜çš„æœ¬è´¨ã€‚ProtoReasoning åŒ…å«ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åŸå‹æ„å»ºç®¡é“ï¼Œå°†é—®é¢˜è½¬åŒ–ä¸ºç›¸åº”çš„åŸå‹è¡¨ç¤ºï¼Œå¹¶æä¾›å¯é çš„åé¦ˆç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProtoReasoning åœ¨é€»è¾‘æ¨ç†ã€è§„åˆ’ä»»åŠ¡å’Œä¸€èˆ¬æ¨ç†ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ¨ç†åŸå‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15681', 'title': 'GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.15681', 'abstract': 'GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.', 'score': 16, 'issue_id': 4370, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '2d531d89420a04d8', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yong Man Ro', 'Yu-Chiang Frank Wang', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15681.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#inference', '#training', '#multimodal', '#dataset', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'GenRecal: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'GenRecal - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Recalibrator Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ VLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GenRecal Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ VLM. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Aligning Features for Efficient Vision-Language Models', 'desc': 'GenRecal is a new framework designed to enhance the performance of vision-language models (VLMs) by aligning their feature representations across various architectures. It addresses the challenge of transferring knowledge from large, complex VLMs to smaller, more efficient models, which is crucial for deployment on devices with limited resources. The framework includes a Recalibrator that adapts features from different VLMs, allowing for effective knowledge transfer despite differences in architecture and tokenization. Experimental results show that GenRecal not only improves baseline performance but also surpasses both open-source and closed-source VLMs in various benchmarks.'}, 'zh': {'title': 'GenRecalï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ¡†æ¶', 'desc': 'GenRecalæ˜¯ä¸€ç§æ–°é¢–çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹ä¸åŒæ¶æ„çš„ç‰¹å¾è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç”±äºä¸åŒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¶æ„çš„å¤šæ ·æ€§è€Œå¯¼è‡´çš„çŸ¥è¯†è½¬ç§»æŒ‘æˆ˜ã€‚GenRecalå¼•å…¥äº†ä¸€ä¸ªé‡æ ¡å‡†å™¨ï¼Œèƒ½å¤Ÿåœ¨å¼‚æ„VLMä¹‹é—´å¯¹ç‰¹å¾è¡¨ç¤ºè¿›è¡Œé€‚é…ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çŸ¥è¯†ä¼ é€’ã€‚é€šè¿‡åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜GenRecalæ˜¾è‘—æé«˜äº†åŸºçº¿æ€§èƒ½ï¼Œæœ€ç»ˆè¶…è¶Šäº†å¤§å‹å¼€æºå’Œé—­æºçš„VLMã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15677', 'title': 'Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence', 'url': 'https://huggingface.co/papers/2506.15677', 'abstract': 'Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.', 'score': 11, 'issue_id': 4370, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '9dff3e9d64c54e88', 'authors': ['Yining Hong', 'Rui Sun', 'Bingxuan Li', 'Xingcheng Yao', 'Maxine Wu', 'Alexander Chien', 'Da Yin', 'Ying Nian Wu', 'Zhecan James Wang', 'Kai-Wei Chang'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.15677.jpg', 'data': {'categories': ['#3d', '#benchmark', '#open_source', '#agents', '#multimodal', '#agi', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ² Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° - Embodied Web Agents, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ²ĞµĞ±-Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-ÑÑ€ĞµĞ´Ñ‹ Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Embodied Web Agents Benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Bridging Physical and Digital Intelligence with Embodied Web Agents', 'desc': "The paper introduces Embodied Web Agents, a new type of AI that combines physical interaction with web-based reasoning. This integration allows the agents to perform tasks that require both physical actions and access to online information, such as cooking or navigating. The authors create a benchmark environment that includes realistic 3D settings and web interfaces to evaluate these agents' abilities. Their findings show that current AI systems still lag behind human performance, highlighting both the challenges and potential for improvement in this area."}, 'zh': {'title': 'å…·èº«ç½‘ç»œä»£ç†ï¼šè¿æ¥ç‰©ç†ä¸æ•°å­—æ™ºèƒ½çš„æ¡¥æ¢', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„äººå·¥æ™ºèƒ½ä»£ç†æ¨¡å‹â€”â€”å…·èº«ç½‘ç»œä»£ç†ï¼ˆEmbodied Web Agentsï¼‰ï¼Œå®ƒå°†ç‰©ç†äº¤äº’ä¸ç½‘ç»œè§„æ¨¡æ¨ç†ç»“åˆåœ¨ä¸€èµ·ï¼Œä»¥è¯„ä¼°è·¨é¢†åŸŸæ™ºèƒ½ã€‚å½“å‰çš„AIä»£ç†é€šå¸¸åªèƒ½åœ¨æ•°å­—ä¿¡æ¯æ£€ç´¢æˆ–ç‰©ç†ä¸–ç•Œäº¤äº’ä¸­å‘æŒ¥ä½œç”¨ï¼Œç¼ºä¹ä¸¤è€…çš„æ•´åˆï¼Œé™åˆ¶äº†å®ƒä»¬è§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†å…·èº«ç½‘ç»œä»£ç†ä»»åŠ¡ç¯å¢ƒï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ä»¿çœŸå¹³å°ï¼Œèƒ½å¤Ÿå°†çœŸå®çš„3Dç¯å¢ƒä¸åŠŸèƒ½æ€§ç½‘ç»œæ¥å£ç´§å¯†ç»“åˆã€‚é€šè¿‡è¿™ä¸€å¹³å°ï¼Œæˆ‘ä»¬æ„å»ºå¹¶å‘å¸ƒäº†å…·èº«ç½‘ç»œä»£ç†åŸºå‡†ï¼Œæ¶µç›–äº†çƒ¹é¥ªã€å¯¼èˆªã€è´­ç‰©ç­‰å¤šç§ä»»åŠ¡ï¼Œè¦æ±‚åœ¨ç‰©ç†å’Œæ•°å­—é¢†åŸŸä¹‹é—´è¿›è¡Œåè°ƒæ¨ç†ï¼Œä»¥ç³»ç»Ÿè¯„ä¼°è·¨é¢†åŸŸæ™ºèƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13414', 'title': 'BUT System for the MLC-SLM Challenge', 'url': 'https://huggingface.co/papers/2506.13414', 'abstract': "The combined DiCoW and DiariZen ASR system demonstrates strong performance in multilingual scenarios, with DiCoW preserving its multilingual capabilities and DiariZen improving through fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a two-speaker automatic speech recognition (ASR) system that combines DiCoW -- a diarization-conditioned variant of Whisper -- with DiariZen, a diarization pipeline built on top of Pyannote. We first evaluate both systems in out-of-domain (OOD) multilingual scenarios without any fine-tuning. In this scenario, DiariZen consistently outperforms the baseline Pyannote diarization model, demonstrating strong generalization. Despite being fine-tuned on English-only data for target-speaker ASR, DiCoW retains solid multilingual performance, indicating that encoder modifications preserve Whisper's multilingual capabilities. We then fine-tune both DiCoW and DiariZen on the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform the fine-tuned Pyannote baseline, while DiCoW sees further gains from domain adaptation. Our final system achieves a micro-average tcpWER/CER of 16.75% and ranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several labeling inconsistencies in the training data -- such as missing speech segments and incorrect silence annotations -- which can hinder diarization fine-tuning. We propose simple mitigation strategies to address these issues and improve system robustness.", 'score': 11, 'issue_id': 4380, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'd143e6da0f164962', 'authors': ['Alexander Polok', 'Jiangyu Han', 'Dominik Klement', 'Samuele Cornell', 'Jan ÄŒernockÃ½', 'LukÃ¡Å¡ Burget'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University, USA', 'Speech@FIT, Brno University of Technology, Czechia'], 'pdf_title_img': 'assets/pdf/title_img/2506.13414.jpg', 'data': {'categories': ['#audio', '#training', '#multilingual', '#data'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ DiCoW Ğ¸ DiariZen: Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ASR', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ DiCoW Ğ¸ DiariZen. DiCoW - ÑÑ‚Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Whisper, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ° DiariZen - ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Pyannote. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ DiCoW ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° DiariZen ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Enhancing Multilingual ASR with DiCoW and DiariZen', 'desc': 'This paper presents a novel automatic speech recognition (ASR) system that integrates two components: DiCoW, which is a diarization-conditioned version of Whisper, and DiariZen, a diarization pipeline based on Pyannote. The system is evaluated in multilingual scenarios, showing that DiariZen outperforms the baseline Pyannote model without fine-tuning, indicating its strong generalization capabilities. DiCoW maintains its multilingual performance even after being fine-tuned on English-only data, demonstrating the effectiveness of its encoder modifications. The final system achieves a competitive error rate and addresses labeling inconsistencies in training data to enhance robustness and performance.'}, 'zh': {'title': 'å¤šè¯­è¨€ASRç³»ç»Ÿçš„å¼ºå¤§ç»“åˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆDiCoWå’ŒDiariZençš„åŒè¯´è¯è€…è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šè¯­è¨€åœºæ™¯ä¸­çš„å¼ºå¤§æ€§èƒ½ã€‚DiCoWæ˜¯Whisperçš„ä¸€ä¸ªåŸºäºè¯´è¯è€…åˆ†ç¦»çš„å˜ä½“ï¼Œè€ŒDiariZenåˆ™æ˜¯åŸºäºPyannoteæ„å»ºçš„è¯´è¯è€…åˆ†ç¦»ç®¡é“ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ²¡æœ‰ä»»ä½•å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒDiariZenåœ¨å¤šè¯­è¨€åœºæ™¯ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹Pyannoteï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»è¿‡å¾®è°ƒåï¼ŒDiariZenå’ŒDiCoWçš„æ€§èƒ½è¿›ä¸€æ­¥æå‡ï¼Œæœ€ç»ˆç³»ç»Ÿåœ¨MLC-SLMæŒ‘æˆ˜èµ›ä¸­å–å¾—äº†16.75%çš„å¾®å¹³å‡tcpWER/CERï¼Œæ’åç¬¬äºŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15068', 'title': 'Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation', 'url': 'https://huggingface.co/papers/2506.15068', 'abstract': 'PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/long_form_rl.', 'score': 9, 'issue_id': 4374, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '3f5497d8e1350326', 'authors': ['Zongxia Li', 'Yapei Chang', 'Yuhang Zhou', 'Xiyang Wu', 'Zichao Liang', 'Yoo Yeon Sung', 'Jordan Lee Boyd-Graber'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.15068.jpg', 'data': {'categories': ['#long_context', '#alignment', '#rlhf', '#benchmark', '#optimization', '#open_source', '#dataset'], 'emoji': 'ğŸ“', 'ru': {'title': 'PrefBERT: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸', 'desc': 'PrefBERT - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸. PrefBERT Ğ¾Ğ±ÑƒÑ‡ĞµĞ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑˆĞºĞ°Ğ»Ğµ Ğ›Ğ¸ĞºĞµÑ€Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ GRPO, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ROUGE-L Ğ¸ BERTScore.'}, 'en': {'title': 'Enhancing Long-Form Generation Evaluation with PrefBERT', 'desc': 'PrefBERT is a novel scoring model designed to enhance the evaluation of open-ended long-form text generation. It addresses the limitations of traditional metrics like ROUGE-L and BERTScore, which often overlook important qualities such as coherence and relevance. By providing distinct semantic rewards for good and bad outputs, PrefBERT guides the training of generative models more effectively. Comprehensive evaluations demonstrate that PrefBERT aligns closely with human preferences, leading to higher quality generated responses.'}, 'zh': {'title': 'PrefBERTï¼šæå‡å¼€æ”¾å¼é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯„ä¼°æ•ˆæœ', 'desc': 'PrefBERTæ˜¯ä¸€ç§è¯„åˆ†æ¨¡å‹ï¼Œæ—¨åœ¨æ”¹å–„å¼€æ”¾å¼é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯„ä¼°ã€‚ä¸ä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡ç›¸æ¯”ï¼ŒPrefBERTæä¾›äº†æ›´å¥½çš„è¯­ä¹‰å¥–åŠ±åé¦ˆï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚è¯¥æ¨¡å‹åœ¨ä¸¤ä¸ªå¤šæ ·åŒ–çš„é•¿æ–‡æœ¬å“åº”è¯„ä¼°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿè¯†åˆ«è¾“å‡ºçš„å¥½åï¼Œå¹¶ä¸ºå…¶æä¾›æ˜ç¡®çš„å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡ç»¼åˆè¯„ä¼°ï¼ŒPrefBERTåœ¨ä¸åŒçš„é•¿æ–‡æœ¬ä¸­è¡¨ç°å‡ºå¯é æ€§ï¼Œå¹¶ä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15569', 'title': 'SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification', 'url': 'https://huggingface.co/papers/2506.15569', 'abstract': "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.", 'score': 7, 'issue_id': 4371, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '0e3c39a143af668b', 'authors': ['Chengye Wang', 'Yifei Shen', 'Zexi Kuang', 'Arman Cohan', 'Yilun Zhao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.15569.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rag', '#science', '#benchmark', '#open_source'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'SciVer: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'SciVer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 3000 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 1113 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 21 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ o4-mini, Gemini-2.5-Flash Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹.'}, 'en': {'title': 'Bridging the Gap: Evaluating Multimodal Models in Scientific Claim Verification', 'desc': 'The paper introduces SciVer, a benchmark designed to assess how well multimodal foundation models can verify claims in scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, focusing on four reasoning types relevant to claim verification. The study evaluates 21 advanced multimodal models, revealing significant performance gaps compared to human experts. Additionally, it highlights limitations in current models and provides insights for improving their understanding and reasoning capabilities in scientific literature.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„ç§‘å­¦éªŒè¯èƒ½åŠ›', 'desc': 'SciVeræ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦èƒŒæ™¯ä¸‹éªŒè¯å£°æ˜çš„èƒ½åŠ›ã€‚å®ƒåŒ…å«3000ä¸ªä¸“å®¶æ³¨é‡Šçš„ç¤ºä¾‹ï¼Œæ¶µç›–1113ç¯‡ç§‘å­¦è®ºæ–‡ï¼Œåˆ†ä¸ºå››ä¸ªå­é›†ï¼Œä»£è¡¨å¤šæ¨¡æ€ç§‘å­¦å£°æ˜éªŒè¯ä¸­çš„å¸¸è§æ¨ç†ç±»å‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†21ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°è¿™äº›æ¨¡å‹ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚é€šè¿‡å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œäººç±»é”™è¯¯è¯„ä¼°çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«äº†å½“å‰å¼€æºæ¨¡å‹çš„å…³é”®å±€é™æ€§ï¼Œä¸ºæé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ç§‘å­¦æ–‡çŒ®ä»»åŠ¡ä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14842', 'title': 'PictSure: Pretraining Embeddings Matters for In-Context Learning Image\n  Classifiers', 'url': 'https://huggingface.co/papers/2506.14842', 'abstract': "PictSure is an in-context learning framework that enhances few-shot image classification by optimizing embedding models' architecture, pretraining, and fine-tuning strategies to improve out-of-domain performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model -- its architecture, pretraining, and training dynamics -- at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at https://github.com/PictSure/pictsure-library.", 'score': 5, 'issue_id': 4381, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '4cf4adfc32104e33', 'authors': ['Lukas Schiesser', 'Cornelius Wolff', 'Sophie Haas', 'Simon Pukrop'], 'affiliations': ['German Research Center for AI (DFKI)'], 'pdf_title_img': 'assets/pdf/title_img/2506.14842.jpg', 'data': {'categories': ['#dataset', '#cv', '#transfer_learning', '#optimization', '#training', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°', 'desc': 'PictSure - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. PictSure Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ğ´Ğ¾Ğ¼ĞµĞ½Ğµ.'}, 'en': {'title': 'Enhancing Few-Shot Image Classification with PictSure', 'desc': 'PictSure is a framework designed to improve few-shot image classification (FSIC) by focusing on the optimization of embedding models. It emphasizes the importance of the architecture, pretraining, and fine-tuning strategies of these models to enhance their performance, especially in out-of-domain scenarios. The research highlights that the success of training and the ability to generalize to new domains are closely linked to how well the embedding models are pretrained. By systematically analyzing various visual encoders and training methods, PictSure demonstrates superior performance compared to existing models in challenging out-of-domain benchmarks while still performing well in familiar tasks.'}, 'zh': {'title': 'PictSureï¼šæå‡å°‘æ ·æœ¬å›¾åƒåˆ†ç±»çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶', 'desc': 'PictSureæ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–åµŒå…¥æ¨¡å‹çš„æ¶æ„ã€é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥æ¥å¢å¼ºå°‘æ ·æœ¬å›¾åƒåˆ†ç±»çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å…³æ³¨å›¾åƒåµŒå…¥åœ¨å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä¸­çš„é‡è¦æ€§ï¼Œç³»ç»Ÿåœ°ç ”ç©¶ä¸åŒè§†è§‰ç¼–ç å™¨ç±»å‹ã€é¢„è®­ç»ƒç›®æ ‡å’Œå¾®è°ƒç­–ç•¥å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåµŒå…¥æ¨¡å‹çš„é¢„è®­ç»ƒæ–¹å¼å¯¹è®­ç»ƒæˆåŠŸå’ŒåŸŸå¤–æ€§èƒ½æœ‰å¾ˆå¤§å½±å“ã€‚PictSureåœ¨ä¸è®­ç»ƒåˆ†å¸ƒæ˜¾è‘—ä¸åŒçš„åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„å°‘æ ·æœ¬å›¾åƒåˆ†ç±»æ¨¡å‹ï¼ŒåŒæ—¶åœ¨åŸŸå†…ä»»åŠ¡ä¸Šä¿æŒäº†ç›¸å½“çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15672', 'title': 'SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence', 'url': 'https://huggingface.co/papers/2506.15672', 'abstract': 'SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at https://yaoz720.github.io/SwarmAgentic/.', 'score': 4, 'issue_id': 4376, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'e6b93c3f506d4979', 'authors': ['Yao Zhang', 'Chenyang Lin', 'Shijie Tang', 'Haokun Chen', 'Shijie Zhou', 'Yunpu Ma', 'Volker Tresp'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2506.15672.jpg', 'data': {'categories': ['#optimization', '#agents', '#benchmark', '#games', '#agi', '#open_source', '#alignment'], 'emoji': 'ğŸ', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'SwarmAgentic - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. SwarmAgentic ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ½ÑƒĞ»Ñ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ€Ğ¾ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ.'}, 'en': {'title': 'Revolutionizing Agentic Systems with SwarmAgentic', 'desc': 'SwarmAgentic is a novel framework designed for the automated generation of agentic systems, which are capable of decision-making and collaboration. It addresses the limitations of existing frameworks by enabling the creation of agents from scratch and optimizing their functionality and teamwork through language-driven exploration. The framework utilizes a population-based approach inspired by Particle Swarm Optimization (PSO) to evolve candidate systems based on feedback. In evaluations, SwarmAgentic demonstrated significant improvements in performance on various complex tasks, showcasing its potential for scalable and autonomous agentic system design.'}, 'zh': {'title': 'å…¨è‡ªåŠ¨åŒ–ä»£ç†ç³»ç»Ÿç”Ÿæˆçš„æœªæ¥', 'desc': 'SwarmAgenticæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–ä»£ç†ç³»ç»Ÿç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¯­è¨€é©±åŠ¨çš„æ¢ç´¢æ¥ä¼˜åŒ–ä»£ç†çš„åŠŸèƒ½å’Œåä½œã€‚ä¸ç°æœ‰çš„ä»£ç†ç³»ç»Ÿç”Ÿæˆæ¡†æ¶ç›¸æ¯”ï¼ŒSwarmAgenticèƒ½å¤Ÿä»é›¶å¼€å§‹ç”Ÿæˆä»£ç†ï¼Œå¹¶è‡ªæˆ‘ä¼˜åŒ–å…¶åŠŸèƒ½å’Œåä½œèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»´æŠ¤å€™é€‰ç³»ç»Ÿçš„ç§ç¾¤å¹¶è¿›è¡Œåé¦ˆå¼•å¯¼çš„æ›´æ–°ï¼Œå€Ÿé‰´äº†ç²’å­ç¾¤ä¼˜åŒ–ï¼ˆPSOï¼‰çš„æ€æƒ³ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ç³»ç»Ÿç»“æ„æœç´¢ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„ä»»åŠ¡ä¸­ï¼ŒSwarmAgenticè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†ï¼Œå±•ç¤ºäº†å…¨è‡ªåŠ¨åŒ–åœ¨æ— çº¦æŸä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15050', 'title': 'Truncated Proximal Policy Optimization', 'url': 'https://huggingface.co/papers/2506.15050', 'abstract': 'T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors.', 'score': 4, 'issue_id': 4374, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'b986b577a01d9bf5', 'authors': ['Tiantian Fan', 'Lingjun Liu', 'Yu Yue', 'Jiaze Chen', 'Chengyi Wang', 'Qiying Yu', 'Chi Zhang', 'Zhiqi Lin', 'Ruofei Zhu', 'Yufeng Yuan', 'Xiaochen Zuo', 'Bole Ma', 'Mofan Zhang', 'Gaohong Liu', 'Ru Zhang', 'Haotian Zhou', 'Cong Xie', 'Ruidong Zhu', 'Zhi Zhang', 'Xin Liu', 'Mingxuan Wang', 'Lin Yan', 'Yonghui Wu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2506.15050.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'T-PPO: Ğ£ÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ² 2,5 Ñ€Ğ°Ğ·Ğ°', 'desc': 'T-PPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° PPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹. T-PPO Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° (EGAE) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Boosting Training Efficiency for Large Language Models with T-PPO', 'desc': "T-PPO is a new method that enhances the Proximal Policy Optimization (PPO) algorithm to make training Large Language Models (LLMs) faster and more efficient. It addresses the slow training times caused by PPO's on-policy nature, especially when generating long responses. By introducing Extended Generalized Advantage Estimation (EGAE), T-PPO allows for better advantage estimation from incomplete responses, which helps maintain effective policy learning. Additionally, it optimizes the use of hardware resources by reducing unnecessary computations during training, leading to a significant increase in efficiency and performance compared to existing methods."}, 'zh': {'title': 'T-PPOï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'T-PPOæ˜¯å¯¹PPOçš„ä¸€ç§æ‰©å±•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚å®ƒé€šè¿‡ä¼˜åŒ–ç­–ç•¥æ›´æ–°å’Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç¡¬ä»¶èµ„æºæ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚T-PPOå¼•å…¥äº†æ‰©å±•çš„å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆEGAEï¼‰ï¼Œä½¿å¾—åœ¨ä¸å®Œæ•´å“åº”çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒç­–ç•¥å­¦ä¹ çš„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼ŒT-PPOè¿˜é€šè¿‡ç‹¬ç«‹ä¼˜åŒ–ç­–ç•¥å’Œä»·å€¼æ¨¡å‹ï¼Œå‡å°‘å†—ä½™è®¡ç®—ï¼ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06279', 'title': 'CoMemo: LVLMs Need Image Context with Image Memory', 'url': 'https://huggingface.co/papers/2506.06279', 'abstract': "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - a dual-path architecture that combines a Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemo's superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/.", 'score': 4, 'issue_id': 4370, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'e2ad205a039e250b', 'authors': ['Shi Liu', 'Weijie Su', 'Xizhou Zhu', 'Wenhai Wang', 'Jifeng Dai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06279.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#multimodal', '#reasoning', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CoMemo: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'CoMemo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ½ĞµĞ±Ñ€ĞµĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. CoMemo Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RoPE-DHR, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ°Ñ‚ÑÑ€. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ LVLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Visual Awareness in Multimodal Processing with CoMemo', 'desc': 'CoMemo is a machine learning model designed to improve how visual information is processed alongside language. It uses a dual-path architecture that separates context and memory paths to better handle visual data, reducing the neglect of important visual details. The model also introduces a new positional encoding method called RoPE-DHR, which helps maintain the spatial relationships in images, especially in long sequences. Evaluations show that CoMemo outperforms traditional large vision-language models in various tasks, including understanding long contexts and answering visual questions.'}, 'zh': {'title': 'CoMemoï¼šæå‡å¤šæ¨¡æ€å¤„ç†çš„è§†è§‰æ„è¯†', 'desc': 'CoMemoæ˜¯ä¸€ç§æ–°å‹çš„åŒè·¯å¾„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤„ç†ä¸­çš„è§†è§‰ä¿¡æ¯å¿½è§†å’Œç©ºé—´æ„è¯†é—®é¢˜ã€‚å®ƒç»“åˆäº†ä¸Šä¸‹æ–‡å›¾åƒè·¯å¾„å’Œå›¾åƒè®°å¿†è·¯å¾„ï¼Œæœ‰æ•ˆç¼“è§£äº†è§†è§‰ä¿¡æ¯çš„å¿½è§†ç°è±¡ã€‚æ­¤å¤–ï¼ŒCoMemoå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä½ç½®ç¼–ç æœºåˆ¶RoPE-DHRï¼Œé€šè¿‡ç¼©ç•¥å›¾ä½ç½®èšåˆæ¥ä¿æŒäºŒç»´ç©ºé—´æ„è¯†ï¼Œå‡å°‘è¿œç¨‹è¡°å‡ã€‚é€šè¿‡åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ï¼ŒCoMemoçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¶æ„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14315', 'title': 'ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured\n  Proxies', 'url': 'https://huggingface.co/papers/2506.14315', 'abstract': 'An agent-guided framework generates photorealistic 3D scenes for VR by synthesizing textures onto lightweight geometric proxies, enabling real-time rendering and superior visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery. This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: https://immersegen.github.io.', 'score': 3, 'issue_id': 4380, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '3fcf78822da5c3c6', 'authors': ['Jinyan Yuan', 'Bangbang Yang', 'Keke Wang', 'Panwang Pan', 'Lin Ma', 'Xuehai Zhang', 'Xiao Liu', 'Zhaopeng Cui', 'Yuewen Ma'], 'affiliations': ['ByteDance', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.14315.jpg', 'data': {'categories': ['#3d', '#video', '#multimodal', '#synthetic', '#games', '#agents'], 'emoji': 'ğŸ•¶ï¸', 'ru': {'title': 'Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼ Ğ² VR Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸', 'desc': 'ImmerseGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ° Ğ½Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ RGBA, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼. ImmerseGen Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¸ Ğ·Ğ²ÑƒĞº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing VR with Lightweight 3D Scene Generation', 'desc': 'This paper presents ImmerseGen, a new framework for creating realistic 3D scenes for virtual reality (VR) using lightweight geometric proxies. Instead of relying on complex high-poly models, ImmerseGen synthesizes photorealistic textures directly onto these proxies, allowing for real-time rendering without sacrificing visual quality. The framework utilizes agent-guided generative models to produce coherent textures that enhance the immersive experience. Additionally, it incorporates dynamic effects and audio to create a multisensory environment, demonstrating improved efficiency and realism compared to traditional methods.'}, 'zh': {'title': 'è½»é‡çº§å‡ ä½•ä»£ç†ï¼Œå®æ—¶ç”Ÿæˆé€¼çœŸ3Dåœºæ™¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºImmerseGençš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé€¼çœŸçš„3Dåœºæ™¯ï¼Œä»¥å¢å¼ºè™šæ‹Ÿç°å®ä½“éªŒã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è½»é‡å‡ ä½•ä»£ç†ä¸åˆæˆçš„RGBAçº¹ç†ç»“åˆï¼Œç®€åŒ–äº†å»ºæ¨¡è¿‡ç¨‹ï¼Œå¹¶å®ç°äº†å®æ—¶æ¸²æŸ“ã€‚ImmerseGenåˆ©ç”¨ä»£ç†å¼•å¯¼ç”Ÿæˆæ¨¡å‹ï¼Œç¡®ä¿çº¹ç†ä¸åœºæ™¯çš„æ— ç¼èåˆï¼Œä»è€Œæé«˜è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒImmerseGenåœ¨å…‰ç…§çœŸå®æ„Ÿã€ç©ºé—´ä¸€è‡´æ€§å’Œæ¸²æŸ“æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14824', 'title': 'FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2506.14824', 'abstract': 'FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems.', 'score': 3, 'issue_id': 4376, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': 'c694f22e30433fa6', 'authors': ['Yao Zhang', 'Hewei Gao', 'Haokun Chen', 'Weiguo Li', 'Yunpu Ma', 'Volker Tresp'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning', 'Siemens Technology', 'Technical University of Munich', 'University Heidelberg'], 'pdf_title_img': 'assets/pdf/title_img/2506.14824.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#agents', '#scalability', '#privacy', '#training', '#federated_learning'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'FedNano - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑĞµÑ€Ğ²ĞµÑ€Ğµ, Ğ° Ğ½Ğ° ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¸Ğµ NanoEdge Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. FedNano Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Scalable AI with FedNano: Federated Learning for Large Language Models', 'desc': 'FedNano is a federated learning framework designed to enhance the deployment of large language models (LLMs) while addressing privacy and scalability challenges. It centralizes the LLM on servers and utilizes lightweight NanoEdge modules for client-specific adaptations, significantly reducing the need for client-side resources. By employing low-rank adaptation techniques, FedNano minimizes communication costs and storage requirements, allowing clients to only transmit compact updates instead of full model parameters. This innovative approach enables effective collaboration across heterogeneous client data while maintaining privacy, ultimately making multimodal AI systems more scalable and feasible.'}, 'zh': {'title': 'FedNanoï¼šè§£å†³å¤šæ¨¡æ€å­¦ä¹ çš„éšç§ä¸å¯æ‰©å±•æ€§é—®é¢˜', 'desc': 'FedNanoæ˜¯ä¸€ä¸ªè”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹é›†ä¸­åœ¨æœåŠ¡å™¨ä¸Šï¼Œå¹¶ä½¿ç”¨NanoEdgeæ¨¡å—è¿›è¡Œå®¢æˆ·ç«¯ç‰¹å®šçš„é€‚åº”ï¼Œä»è€Œè§£å†³äº†å¯æ‰©å±•æ€§å’Œéšç§é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å…è®¸åœ¨ä¸é›†ä¸­æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œåä½œæ¨¡å‹è®­ç»ƒï¼Œå…‹æœäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„éƒ¨ç½²æŒ‘æˆ˜ã€‚NanoEdgeæ¨¡å—é‡‡ç”¨ç‰¹å®šæ¨¡æ€çš„ç¼–ç å™¨å’Œå¯è®­ç»ƒçš„NanoAdaptersï¼Œæ˜¾è‘—å‡å°‘äº†å®¢æˆ·ç«¯çš„å­˜å‚¨éœ€æ±‚å’Œé€šä¿¡å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedNanoåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„è”é‚¦å­¦ä¹ åŸºçº¿ï¼Œä¿ƒè¿›äº†å¯æ‰©å±•çš„å»ä¸­å¿ƒåŒ–å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14866', 'title': 'OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents', 'url': 'https://huggingface.co/papers/2506.14866', 'abstract': 'A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.', 'score': 2, 'issue_id': 4378, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '4a30ee5bf7d4f9f6', 'authors': ['Thomas Kuntz', 'Agatha Duzan', 'Hao Zhao', 'Francesco Croce', 'Zico Kolter', 'Nicolas Flammarion', 'Maksym Andriushchenko'], 'affiliations': ['Carnegie Mellon University', 'EPFL'], 'pdf_title_img': 'assets/pdf/title_img/2506.14866.jpg', 'data': {'categories': ['#security', '#ethics', '#agents', '#benchmark'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'OS-Harm: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…', 'desc': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº OS-Harm Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°. ĞĞ½ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…. OS-Harm Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 150 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ĞĞ¡. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Ensuring Safe Interactions: Introducing OS-Harm for Computer Use Agents', 'desc': "The paper introduces OS-Harm, a benchmark designed to assess the safety of computer use agents that interact with graphical user interfaces (GUIs). It evaluates these agents for their vulnerability to misuse, prompt injection attacks, and other forms of misbehavior across various applications. The benchmark includes 150 tasks that simulate different safety violations, such as harassment and data exfiltration, while requiring agents to operate within common OS applications. An automated judging system is proposed to evaluate the agents' performance, achieving a high agreement with human assessments, revealing that many models are prone to compliance with misuse and unsafe actions."}, 'zh': {'title': 'OS-Harmï¼šè¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„å®‰å…¨æ–°åŸºå‡†', 'desc': 'OS-Harmæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†åœ¨ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢äº¤äº’æ—¶çš„å®‰å…¨æ€§ã€‚å®ƒä¸»è¦æµ‹è¯•ä»£ç†åœ¨ç”¨æˆ·æ¶æ„ä½¿ç”¨ã€æç¤ºæ³¨å…¥æ”»å‡»å’Œæ¨¡å‹ä¸å½“è¡Œä¸ºç­‰æ–¹é¢çš„è„†å¼±æ€§ã€‚è¯¥åŸºå‡†åˆ›å»ºäº†150ä¸ªä»»åŠ¡ï¼Œæ¶µç›–å¤šç§å®‰å…¨è¿è§„è¡Œä¸ºï¼Œå¹¶è¦æ±‚ä»£ç†ä¸ä¸åŒçš„æ“ä½œç³»ç»Ÿåº”ç”¨ç¨‹åºè¿›è¡Œäº¤äº’ã€‚é€šè¿‡è‡ªåŠ¨è¯„ä¼°å·¥å…·ï¼ŒOS-Harmèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°ä»£ç†çš„å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ï¼Œä¸ºè®¡ç®—æœºä½¿ç”¨ä»£ç†çš„å®‰å…¨æ€§æä¾›äº†é‡è¦çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14435', 'title': 'MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models', 'url': 'https://huggingface.co/papers/2506.14435', 'abstract': 'MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.', 'score': 2, 'issue_id': 4371, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '2f5b5f7f3c20ae10', 'authors': ['Hongyu Wang', 'Jiayu Xu', 'Ruiping Wang', 'Yan Feng', 'Yitao Zhai', 'Peng Pei', 'Xunliang Cai', 'Xilin Chen'], 'affiliations': ['Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences', 'Meituan University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.14435.jpg', 'data': {'categories': ['#architecture', '#optimization', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MoTE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Mixture-of-Experts Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MoTE - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². MoTE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰ÑƒÑ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ MoE-LLaVA Ğ½Ğ° 4.3% Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'MoTE: Efficient Ternary Experts for Scalable Edge Deployment', 'desc': 'The paper introduces MoTE, a new method that enhances Mixture-of-Experts (MoE) models by using low-precision ternary experts, which consist of parameters limited to -1, 0, and 1. This approach allows for a significant reduction in memory usage while maintaining competitive performance compared to traditional full-precision experts. MoTE is designed to be scalable and efficient, making it suitable for deployment on edge devices where memory is limited. Experimental results show that MoTE not only matches the performance of existing models but also improves accuracy when combined with post-training quantization techniques.'}, 'zh': {'title': 'MoTEï¼šå†…å­˜é«˜æ•ˆçš„æ··åˆä¸“å®¶æ¨¡å‹', 'desc': 'MoTEæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”å†…å­˜é«˜æ•ˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›æ··åˆä¸“å®¶æ¨¡å‹ï¼Œä½¿ç”¨ä½ç²¾åº¦çš„ä¸‰å…ƒä¸“å®¶æ¥æå‡æ€§èƒ½å¹¶å‡å°‘å†…å­˜å ç”¨ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚ä¸ä»¥å¾€ä¸»è¦ä½¿ç”¨å…¨ç²¾åº¦ä¸“å®¶çš„ç¨€ç–ä¸Šå‡æ–¹æ³•ä¸åŒï¼ŒMoTEé€šè¿‡è®­ç»ƒæ›´å¤šä½ç²¾åº¦ä¸“å®¶æ¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å‰é¦ˆç½‘ç»œä½œä¸ºå…±äº«ä¸“å®¶ï¼Œå¹¶è®­ç»ƒå‚æ•°ä¸º{-1, 0, 1}çš„ä¸‰å…ƒè·¯ç”±ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoTEåœ¨æ¨¡å‹è§„æ¨¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ‰©å±•è¶‹åŠ¿ï¼Œå¹¶åœ¨å†…å­˜å—é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.11110', 'title': 'AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2506.11110', 'abstract': 'AssertBench evaluates Large Language Models\' ability to maintain consistent truth evaluation when faced with contradictory user assertions about factually true statements by analyzing framing-induced variability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model\'s agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model\'s underlying factual knowledge by stratifying results based on the model\'s accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM\'s ability to "stick to its guns" when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench.', 'score': 2, 'issue_id': 4386, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': 'd72eb1f8052f802e', 'authors': ['Jaeho Lee', 'Atharv Chowdhary'], 'affiliations': ['Brown University, Providence, RI 02912'], 'pdf_title_img': 'assets/pdf/title_img/2506.11110.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#benchmark', '#hallucinations'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ¾Ğ¹ĞºĞ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ¿ĞµÑ€ĞµĞ´ Ğ»Ğ¸Ñ†Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': "AssertBench - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° FEVEROUS. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ğ¸: Ğ¾Ğ´Ğ¸Ğ½, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ½Ğ¾, Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹, Ğ³Ğ´Ğµ Ğ¾Ğ½Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¼. AssertBench Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM 'Ğ¾Ñ‚ÑÑ‚Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ' Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¾Ğ± Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ñ„Ğ°ĞºÑ‚Ğµ."}, 'en': {'title': 'Testing LLMs: Can They Stick to the Truth?', 'desc': "AssertBench is a benchmark designed to evaluate how well Large Language Models (LLMs) maintain consistent truth evaluations when faced with contradictory assertions about factual statements. It investigates the impact of framing on model responses by presenting two different prompts for the same fact: one affirming its truth and another denying it. The goal is to see if the model can uphold its factual accuracy without being swayed by the user's framing. By isolating the effects of framing, AssertBench helps assess the robustness of LLMs in their reasoning and agreement with factual information."}, 'zh': {'title': 'åšæŒçœŸç›¸ï¼ŒæŠµå¾¡çŸ›ç›¾ä¸»å¼ çš„æŒ‘æˆ˜', 'desc': 'AssertBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢å¯¹ç”¨æˆ·å…³äºäº‹å®çœŸç›¸çš„çŸ›ç›¾ä¸»å¼ æ—¶ï¼Œä¿æŒä¸€è‡´æ€§çœŸç›¸è¯„ä¼°èƒ½åŠ›çš„å·¥å…·ã€‚è¯¥ç ”ç©¶é€šè¿‡åˆ†ææ¡†æ¶å¼•èµ·çš„å˜å¼‚æ€§ï¼Œæ¢è®¨äº†äº‹å®æ€§é™ˆè¿°çš„æ–¹å‘æ€§æ¡†æ¶å¦‚ä½•å½±å“æ¨¡å‹çš„åŒæ„ç¨‹åº¦ã€‚ç ”ç©¶ä½¿ç”¨äº†æ¥è‡ª FEVEROUS çš„è¯æ®æ”¯æŒäº‹å®ï¼Œå¹¶ä¸ºæ¯ä¸ªäº‹å®æ„å»ºäº†ä¸¤ç§æ¡†æ¶æç¤ºï¼Œè®°å½•æ¨¡å‹çš„åŒæ„å’Œæ¨ç†è¿‡ç¨‹ã€‚ç›®æ ‡æ˜¯è®©æ¨¡å‹åœ¨é¢å¯¹çŸ›ç›¾çš„ç”¨æˆ·ä¸»å¼ æ—¶ï¼Œèƒ½å¤ŸåšæŒè‡ªå·±çš„åˆ¤æ–­ï¼Œè€Œä¸æ˜¯éšç”¨æˆ·çš„æ„è§æ”¹å˜è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15461', 'title': 'All is Not Lost: LLM Recovery without Checkpoints', 'url': 'https://huggingface.co/papers/2506.15461', 'abstract': "A novel method, CheckFree, and its extended version CheckFree+, efficiently recover from node failures during LLM training by substituting failed stages with averaged neighboring stages or through out-of-order pipeline execution, improving convergence time over existing checkpointing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Training LLMs on decentralized and wimpy computation nodes, e.g., multiple on-spot instances, lowers the training cost and enables model democratization. The inevitable challenge here is the churn of nodes due to failures and the operator's scheduling policies, leading to losing a stage - a part of the model. The conventional approaches to recover from failures are to either use checkpointing, where periodically a copy of the entire model is sent to an additional storage, or redundant computation. These approaches yield significant communication and/or computation overhead even in non-failure cases and scale poorly in settings with large models. In this paper, we propose, CheckFree, an efficient recovery method where a failing stage is substituted by a weighted average of the closest neighboring stages. In contrast to the state of the art, CheckFree requires no additional computation or storage. However, because of the nature of averaging neighbouring stages, it can only recover failures of intermediate stages. We further extend our method to CheckFree+ with out-of-order pipeline execution to tolerate crashes of the first and last stages. Thanks to out-of-order pipelining, behaviour of those stages is mimicked by their neighboring ones, which allows CheckFree+ to recover them by simply copying the weights from the immediate neighbour. To be able to recover the (de)embedding layers, CheckFree+ copies those layers to the neighboring stages, which requires relatively small storage overhead. We extensively evaluate our method on LLaMa models of model sizes from 124M to 1.5B with varying failure frequencies. In the case of low and medium failure rates (5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant computation in terms of convergence in wall-clock time by over 12%. Both of our proposals can be run via our code available at: https://github.com/gensyn-ai/CheckFree.", 'score': 1, 'issue_id': 4388, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'f6a5c394da44e46e', 'authors': ['Nikolay Blagoev', 'OÄŸuzhan Ersoy', 'Lydia Yiyu Chen'], 'affiliations': ['Gensyn', 'TU Delft', 'University of Neuchatel'], 'pdf_title_img': 'assets/pdf/title_img/2506.15461.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚', 'desc': 'CheckFree Ğ¸ CheckFree+ - Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ±Ğ¾ĞµĞ² ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‚ Ğ½ĞµĞ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ´Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ½ĞµĞ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ°, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². CheckFree Ğ¸ CheckFree+ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°Ñ… ÑĞ±Ğ¾ĞµĞ².'}, 'en': {'title': 'Efficient Recovery in LLM Training with CheckFree', 'desc': 'The paper introduces CheckFree, a novel method for recovering from node failures during the training of large language models (LLMs). Instead of traditional checkpointing or redundant computation, CheckFree substitutes failed stages with averaged outputs from neighboring stages, significantly reducing overhead. The extended version, CheckFree+, enhances this by allowing out-of-order execution, enabling recovery of initial and final stages by mimicking their behavior through neighboring stages. Evaluations show that both methods improve convergence time by over 12% compared to existing techniques, especially under low to medium failure rates.'}, 'zh': {'title': 'é«˜æ•ˆæ¢å¤ï¼šCheckFreeä¸CheckFree+çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•CheckFreeåŠå…¶æ‰©å±•ç‰ˆæœ¬CheckFree+ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­çš„èŠ‚ç‚¹æ•…éšœæ¢å¤æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”¨ç›¸é‚»é˜¶æ®µçš„åŠ æƒå¹³å‡æ›¿ä»£å¤±è´¥çš„é˜¶æ®µï¼Œæˆ–é€šè¿‡æ— åºæµæ°´çº¿æ‰§è¡Œæ¥æ”¹å–„æ”¶æ•›æ—¶é—´ï¼Œä¼˜äºä¼ ç»Ÿçš„æ£€æŸ¥ç‚¹æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCheckFreeä¸éœ€è¦é¢å¤–çš„è®¡ç®—æˆ–å­˜å‚¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹ä¸­é—´é˜¶æ®µçš„æ•…éšœã€‚æ‰©å±•ç‰ˆæœ¬CheckFree+åˆ™é€šè¿‡æ¨¡ä»¿ç›¸é‚»é˜¶æ®µçš„è¡Œä¸ºï¼Œè¿›ä¸€æ­¥å®¹å¿é¦–å°¾é˜¶æ®µçš„å´©æºƒï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹è®­ç»ƒçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14770', 'title': 'GMT: General Motion Tracking for Humanoid Whole-Body Control', 'url': 'https://huggingface.co/papers/2506.14770', 'abstract': "GMT, a unified motion-tracking framework, addresses challenges in tracking diverse humanoid robot motions through adaptive sampling and a motion mixture-of-experts architecture, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to track general whole-body motions in the real world is a useful way to build general-purpose humanoid robots. However, achieving this can be challenging due to the temporal and kinematic diversity of the motions, the policy's capability, and the difficulty of coordination of the upper and lower bodies. To address these issues, we propose GMT, a general and scalable motion-tracking framework that trains a single unified policy to enable humanoid robots to track diverse motions in the real world. GMT is built upon two core components: an Adaptive Sampling strategy and a Motion Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically balances easy and difficult motions during training. The MoE ensures better specialization of different regions of the motion manifold. We show through extensive experiments in both simulation and the real world the effectiveness of GMT, achieving state-of-the-art performance across a broad spectrum of motions using a unified general policy. Videos and additional information can be found at https://gmt-humanoid.github.io.", 'score': 1, 'issue_id': 4383, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '1ed4993d05c76b95', 'authors': ['Zixuan Chen', 'Mazeyu Ji', 'Xuxin Cheng', 'Xuanbin Peng', 'Xue Bin Peng', 'Xiaolong Wang'], 'affiliations': ['Simon Fraser University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2506.14770.jpg', 'data': {'categories': ['#robotics', '#architecture', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'GMT: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'GMT - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. GMT Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Unified Motion Tracking for Humanoid Robots with GMT', 'desc': 'The paper presents GMT, a unified motion-tracking framework designed for humanoid robots to effectively track a wide range of motions. It tackles challenges such as the diversity of motion types and the coordination between upper and lower body movements. GMT employs an Adaptive Sampling strategy to optimize training by balancing the complexity of different motions, and a Motion Mixture-of-Experts architecture to enhance specialization in motion tracking. Experimental results demonstrate that GMT achieves state-of-the-art performance in both simulated and real-world environments, showcasing its versatility and effectiveness.'}, 'zh': {'title': 'ç»Ÿä¸€è¿åŠ¨è·Ÿè¸ªï¼Œçªç ´äººå½¢æœºå™¨äººæŒ‘æˆ˜', 'desc': 'GMTæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¿åŠ¨è·Ÿè¸ªæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ ·åŒ–äººå½¢æœºå™¨äººè¿åŠ¨è·Ÿè¸ªä¸­çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡è‡ªé€‚åº”é‡‡æ ·å’Œè¿åŠ¨ä¸“å®¶æ··åˆæ¶æ„æ¥å®ç°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è·Ÿè¸ªå„ç§å¤æ‚çš„å…¨èº«è¿åŠ¨ã€‚è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨å¹³è¡¡ç®€å•å’Œå›°éš¾çš„è¿åŠ¨ï¼Œè€Œè¿åŠ¨ä¸“å®¶æ··åˆæ¶æ„åˆ™ç¡®ä¿äº†è¿åŠ¨æµå½¢ä¸åŒåŒºåŸŸçš„æ›´å¥½ä¸“ä¸šåŒ–ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒGMTåœ¨æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œä¸­éƒ½å±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15682', 'title': 'Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model', 'url': 'https://huggingface.co/papers/2506.15682', 'abstract': "ECAD, a genetic algorithm, optimizes caching schedules for diffusion models, enhancing inference speed while maintaining quality across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.", 'score': 0, 'issue_id': 4386, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'c624b4c5b312499d', 'authors': ['Anirud Aggarwal', 'Abhinav Shrivastava', 'Matthew Gwilliam'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.15682.jpg', 'data': {'categories': ['#optimization', '#inference', '#benchmark', '#diffusion'], 'emoji': 'ğŸš€', 'ru': {'title': 'ECAD: Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ECAD - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑÑ…ĞµĞ¼Ñ‹ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ECAD Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ñ€Ğ¾Ğ½Ñ‚ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Accelerating Diffusion Models with ECAD: Smart Caching for Speed and Quality', 'desc': 'The paper introduces ECAD, a genetic algorithm designed to optimize caching schedules for diffusion models, which improves inference speed without sacrificing quality. Traditional methods for caching features in diffusion transformers often rely on fixed rules, limiting their effectiveness. ECAD learns adaptive caching strategies tailored to specific models, allowing for better performance across different architectures. The results show that ECAD significantly enhances inference speed and maintains high quality, outperforming previous methods on various benchmarks.'}, 'zh': {'title': 'ECADï¼šåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ™ºèƒ½ç¼“å­˜è°ƒåº¦', 'desc': 'ECADæ˜¯ä¸€ç§é—ä¼ ç®—æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„ç¼“å­˜è°ƒåº¦ï¼Œä»è€Œæé«˜æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„è´¨é‡ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå›ºå®šçš„å¯å‘å¼è§„åˆ™ï¼Œå¯¼è‡´åŠ é€Ÿæ•ˆæœæœ‰é™æˆ–åœ¨ä¸åŒæ¶æ„é—´æ³›åŒ–èƒ½åŠ›å·®ã€‚ECADé€šè¿‡å­¦ä¹ æ¯ä¸ªæ¨¡å‹çš„é«˜æ•ˆç¼“å­˜è°ƒåº¦ï¼Œå½¢æˆå¸•ç´¯æ‰˜å‰æ²¿ï¼Œä¸”åªéœ€å°‘é‡æ ¡å‡†æç¤ºï¼Œæ— éœ€ä¿®æ”¹ç½‘ç»œå‚æ•°æˆ–å‚è€ƒå›¾åƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒECADåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œå¹¶èƒ½æœ‰æ•ˆé€‚åº”ä¸åŒçš„æ‰©æ•£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14965', 'title': 'Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective', 'url': 'https://huggingface.co/papers/2506.14965', 'abstract': 'Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360', 'score': 32, 'issue_id': 4399, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'ecb3c8bbf20f8213', 'authors': ['Zhoujun Cheng', 'Shibo Hao', 'Tianyang Liu', 'Fan Zhou', 'Yutao Xie', 'Feng Yao', 'Yuexin Bian', 'Yonghao Zhuang', 'Nilabjo Dey', 'Yuheng Zha', 'Yi Gu', 'Kun Zhou', 'Yuqi Wang', 'Yuan Li', 'Richard Fan', 'Jianshu She', 'Chengqian Gao', 'Abulhair Saparov', 'Haonan Li', 'Taylor W. Killian', 'Mikhail Yurochkin', 'Zhengzhong Liu', 'Eric P. Xing', 'Zhiting Hu'], 'affiliations': ['Carnegie Mellon University', 'MBZUAI', 'Purdue University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2506.14965.jpg', 'data': {'categories': ['#training', '#rl', '#open_source', '#reasoning', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Guru: Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Guru Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ RL Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Guru-7B Ğ¸ Guru-32B, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» RL Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ² LLM.'}, 'en': {'title': 'Unlocking Diverse Reasoning with Guru: A Reinforcement Learning Revolution', 'desc': 'The paper introduces Guru, a comprehensive reinforcement learning (RL) reasoning corpus designed to enhance the reasoning capabilities of large language models (LLMs) across various domains. It addresses the challenge of limited RL reward signals by providing 92,000 examples in six distinct reasoning areas, ensuring reliable and effective training. The findings reveal that while some domains benefit from cross-domain training, others require specific in-domain training to improve performance, indicating that RL can lead to real skill development. The authors present two models, Guru-7B and Guru-32B, which achieve state-of-the-art results in RL training, outperforming existing models and enhancing their base performance on complex reasoning tasks.'}, 'zh': {'title': 'Guruï¼šæå‡RLåœ¨æ¨ç†ä¸­çš„åº”ç”¨', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Guruï¼Œä¸€ä¸ªå¤šæ ·åŒ–çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨ç†è¯­æ–™åº“ï¼Œå¼ºè°ƒäº†ç‰¹å®šé¢†åŸŸè®­ç»ƒçš„éœ€æ±‚ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤æ‚ä»»åŠ¡ä¸­å¯¹å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„æå‡ã€‚GuruåŒ…å«92,000ä¸ªå¯éªŒè¯çš„ç¤ºä¾‹ï¼Œæ¶µç›–æ•°å­¦ã€ä»£ç ã€ç§‘å­¦ã€é€»è¾‘ã€ä»¿çœŸå’Œè¡¨æ ¼ç­‰å…­ä¸ªæ¨ç†é¢†åŸŸï¼Œç¡®ä¿äº†RLè®­ç»ƒçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLåœ¨ä¸åŒé¢†åŸŸçš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼ŒæŸäº›é¢†åŸŸï¼ˆå¦‚æ•°å­¦å’Œä»£ç ï¼‰å¯ä»¥é€šè¿‡è·¨é¢†åŸŸè®­ç»ƒè·å¾—æ›´å¥½çš„æ•ˆæœï¼Œè€Œå…¶ä»–é¢†åŸŸï¼ˆå¦‚é€»è¾‘å’Œä»¿çœŸï¼‰åˆ™éœ€è¦åœ¨ç‰¹å®šé¢†åŸŸå†…è¿›è¡Œè®­ç»ƒã€‚æœ€ç»ˆï¼ŒGuru-7Bå’ŒGuru-32Bæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€ä½³åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15564', 'title': 'Show-o2: Improved Native Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2506.15564', 'abstract': 'Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents improved native unified multimodal models, i.e., Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o.', 'score': 16, 'issue_id': 4415, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'd12a7d2ea11f47a1', 'authors': ['Jinheng Xie', 'Zhenheng Yang', 'Mike Zheng Shou'], 'affiliations': ['ByteDance', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.15564.jpg', 'data': {'categories': ['#games', '#optimization', '#multimodal', '#3d', '#cv', '#video', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Show-o2: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Show-o2 - ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ² 3D Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Show-o2 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Unified Multimodal Mastery with Show-o2', 'desc': 'The paper introduces Show-o2, a novel multimodal model that combines autoregressive modeling and flow matching within a 3D causal variational autoencoder framework. This approach allows for the creation of unified visual representations that can effectively handle both image and video data. By employing a dual-path strategy for spatial and temporal fusion, Show-o2 enhances scalability and performance in multimodal tasks. The model is trained using a two-stage recipe, resulting in a versatile system capable of generating and understanding text, images, and videos.'}, 'zh': {'title': 'Show-o2ï¼šç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„åˆ›æ–°æ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ”¹è¿›çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œç§°ä¸ºShow-o2ã€‚å®ƒåˆ©ç”¨è‡ªå›å½’å»ºæ¨¡å’ŒæµåŒ¹é…æŠ€æœ¯ï¼Œåœ¨3Då› æœå˜åˆ†è‡ªç¼–ç å™¨çš„åŸºç¡€ä¸Šæ„å»ºç»Ÿä¸€çš„è§†è§‰è¡¨ç¤ºã€‚é€šè¿‡ç©ºé—´å’Œæ—¶é—´çš„åŒè·¯å¾„èåˆï¼ŒShow-o2èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å›¾åƒå’Œè§†é¢‘çš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ã€‚è®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥ä¾¿æœ‰æ•ˆå­¦ä¹ å¹¶æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09827', 'title': 'EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection', 'url': 'https://huggingface.co/papers/2506.09827', 'abstract': 'EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration.', 'score': 14, 'issue_id': 4400, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '38fbc70bf2ef023b', 'authors': ['Christoph Schuhmann', 'Robert Kaczmarczyk', 'Gollam Rabby', 'Felix Friedrich', 'Maurice Kraus', 'Kourosh Nadi', 'Huu Nguyen', 'Kristian Kersting', 'SÃ¶ren Auer'], 'affiliations': ['Centre for Cognitive Science', 'DFKI', 'Hessian.AI', 'L3S Research Center Leibniz University of Hannover', 'LAION e.V.', 'Ontocord', 'TIBLeibniz Information Centre for Science and Technology', 'TU Darmstadt', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2506.09827.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#science', '#synthetic', '#data', '#audio'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'EmoNet-Voice - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµÑÑƒÑ€Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ñ€ĞµÑ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ°. ĞĞ½ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 40 ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑˆĞµĞ» Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Empathic Insight Voice, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ñ€ĞµÑ‡Ğ¸.'}, 'en': {'title': 'EmoNet-Voice: Revolutionizing Speech Emotion Recognition with Synthetic Audio', 'desc': 'EmoNet-Voice is a new resource that enhances speech emotion recognition (SER) by providing extensive pre-training and benchmark datasets. It includes EmoNet-Voice Big, which features over 4,500 hours of synthetic audio across multiple voices and languages, allowing for fine-grained emotion evaluation across 40 distinct categories. The dataset is designed to address privacy concerns and emotional granularity limitations found in existing SER datasets by using AI-generated audio that simulates real emotional expressions. Additionally, the paper introduces Empathic Insight Voice models that demonstrate high agreement with human expert evaluations, revealing insights into the detection of various emotional intensities.'}, 'zh': {'title': 'EmoNet-Voiceï¼šç»†ç²’åº¦è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„æ–°æ ‡å‡†', 'desc': 'EmoNet-Voiceæ˜¯ä¸€ä¸ªæ–°çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«èµ„æºï¼Œæä¾›äº†å¤§è§„æ¨¡çš„é¢„è®­ç»ƒå’ŒåŸºå‡†æ•°æ®é›†ã€‚å®ƒåŒ…å«è¶…è¿‡4500å°æ—¶çš„è¯­éŸ³æ•°æ®ï¼Œæ¶µç›–11ç§å£°éŸ³ã€40ç§æƒ…æ„Ÿå’Œ4ç§è¯­è¨€ï¼Œèƒ½å¤Ÿè¿›è¡Œç»†ç²’åº¦çš„æƒ…æ„Ÿè¯„ä¼°ã€‚é€šè¿‡åˆæˆçš„éŸ³é¢‘ç‰‡æ®µï¼ŒEmoNet-Voiceæ¨¡æ‹Ÿäº†æ¼”å‘˜è¡¨ç°ç‰¹å®šæƒ…æ„Ÿçš„åœºæ™¯ï¼Œå¹¶ç»è¿‡å¿ƒç†å­¦ä¸“å®¶çš„ä¸¥æ ¼éªŒè¯ã€‚è¯¥èµ„æºä¸ºæƒ…æ„Ÿè¯†åˆ«æ¨¡å‹è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œå°¤å…¶åœ¨é«˜å”¤é†’æƒ…æ„Ÿï¼ˆå¦‚æ„¤æ€’ï¼‰ä¸ä½å”¤é†’çŠ¶æ€ï¼ˆå¦‚ä¸“æ³¨ï¼‰ä¹‹é—´çš„æ£€æµ‹ä¸Šè¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14837', 'title': 'Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction', 'url': 'https://huggingface.co/papers/2506.14837', 'abstract': 'ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o.', 'score': 8, 'issue_id': 4396, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': '3172095671c65e03', 'authors': ['Chengzhi Xu', 'Yuyang Wang', 'Lai Wei', 'Lichao Sun', 'Weiran Huang'], 'affiliations': ['Lehigh University', 'MIFA Lab, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'State Key Laboratory of General Artificial Intelligence, BIGAI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14837.jpg', 'data': {'categories': ['#cv', '#interpretability', '#optimization', '#training', '#multimodal'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ', 'desc': 'ChartIR - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ² ĞºĞ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ². ChartIR Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ChartIR Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2-VL Ğ¸ GPT-4.'}, 'en': {'title': 'ChartIR: Refining Code Generation from Charts with Structured Instructions', 'desc': 'ChartIR is a novel approach that enhances the performance of multimodal large language models (MLLMs) in generating code from charts by separating the tasks of visual understanding and code translation. It employs structured instructions to guide the model in accurately interpreting visual elements and translating them into executable code. The method involves two main stages: initial code generation followed by iterative refinement, which allows for progressive improvements in the output. Experimental results demonstrate that ChartIR significantly outperforms existing methods on both open-source and closed-source models.'}, 'zh': {'title': 'ChartIRï¼šæå‡å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆçš„æ™ºèƒ½æ–¹æ³•', 'desc': 'ChartIRæ˜¯ä¸€ç§é€šè¿‡ç»“æ„åŒ–æŒ‡ä»¤å’Œè¿­ä»£ä¼˜åŒ–æ¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†è§†è§‰ç†è§£å’Œä»£ç ç¿»è¯‘ä»»åŠ¡åˆ†å¼€ï¼Œé¦–å…ˆé€šè¿‡æè¿°å’Œå·®å¼‚ä¸¤ç§ç»“æ„åŒ–æŒ‡ä»¤æ¥æ•æ‰å›¾è¡¨çš„è§†è§‰å…ƒç´ ã€‚æ¥ç€ï¼ŒChartIRå°†æ•´ä½“å›¾è¡¨ç”Ÿæˆæµç¨‹åˆ†ä¸ºåˆå§‹ä»£ç ç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–ä¸¤ä¸ªé˜¶æ®µï¼Œä»è€Œé€æ­¥æå‡æœ€ç»ˆè¾“å‡ºçš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒChartIRåœ¨å¼€æºæ¨¡å‹Qwen2-VLå’Œé—­æºæ¨¡å‹GPT-4oä¸Šå‡è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15154', 'title': 'SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning', 'url': 'https://huggingface.co/papers/2506.15154', 'abstract': 'SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.  \t\t\t\t\tAI-generated summary \t\t\t\t Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.', 'score': 6, 'issue_id': 4397, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'f2b380f0491c0add', 'authors': ['Anuradha Chopra', 'Abhinaba Roy', 'Dorien Herremans'], 'affiliations': ['Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2506.15154.jpg', 'data': {'categories': ['#audio', '#games', '#multimodal', '#architecture', '#science', '#dataset', '#training'], 'emoji': 'ğŸµ', 'ru': {'title': 'SonicVerse: ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'SonicVerse - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰ÑƒÑ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰ÑƒÑ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MusicBench Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MIRFLEX. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Music Descriptions with SonicVerse', 'desc': 'SonicVerse is a multi-task music captioning model that improves the quality of music descriptions by integrating audio feature detection. It captures both low-level acoustic details and high-level musical attributes through a projection-based architecture. This model generates detailed captions for short music pieces and can create time-informed descriptions for longer compositions by using a large-language model. By enhancing the training dataset with music features, SonicVerse demonstrates improved caption quality and detail in its outputs.'}, 'zh': {'title': 'SonicVerseï¼šæå‡éŸ³ä¹å­—å¹•è´¨é‡çš„å¤šä»»åŠ¡æ¨¡å‹', 'desc': 'SonicVerseæ˜¯ä¸€ç§å¤šä»»åŠ¡éŸ³ä¹å­—å¹•ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆäº†éŸ³é¢‘ç‰¹å¾æ£€æµ‹ä»¥æé«˜å­—å¹•è´¨é‡ã€‚è¯¥æ¨¡å‹é€šè¿‡å…³é”®éŸ³è°ƒæ£€æµ‹ã€å£°ä¹æ£€æµ‹ç­‰è¾…åŠ©ä»»åŠ¡ï¼Œæ•æ‰éŸ³ä¹çš„ä½çº§å£°å­¦ç»†èŠ‚å’Œé«˜çº§éŸ³ä¹å±æ€§ã€‚å…¶å…³é”®è´¡çŒ®åœ¨äºé‡‡ç”¨åŸºäºæŠ•å½±çš„æ¶æ„ï¼Œå°†éŸ³é¢‘è¾“å…¥è½¬æ¢ä¸ºè¯­è¨€æ ‡è®°ï¼ŒåŒæ—¶é€šè¿‡ä¸“ç”¨è¾…åŠ©å¤´æ£€æµ‹éŸ³ä¹ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç‰¹å¾çš„ç»“åˆæ˜¾è‘—æå‡äº†ç”Ÿæˆå­—å¹•çš„è´¨é‡å’Œç»†èŠ‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15455', 'title': 'RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation', 'url': 'https://huggingface.co/papers/2506.15455', 'abstract': 'RE-IMAGINE evaluates the reasoning abilities of Large Language Models by generating variations of problems that cannot be solved by memorization, indicating reliance on statistical recall.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true reasoning or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE, a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy.', 'score': 3, 'issue_id': 4407, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '880c387f77f1729e', 'authors': ['Xinnuo Xu', 'Rachel Lawrence', 'Kshitij Dubey', 'Atharva Pandey', 'Risa Ueno', 'Fabian Falck', 'Aditya V. Nori', 'Rahul Sharma', 'Amit Sharma', 'Javier Gonzalez'], 'affiliations': ['Microsoft Research Cambridge, UK', 'Microsoft Research India'], 'pdf_title_img': 'assets/pdf/title_img/2506.15455.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#training', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº RE-IMAGINE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½ĞµĞ»ÑŒĞ·Ñ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. RE-IMAGINE Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² LLM Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Evaluating True Reasoning in Language Models with RE-IMAGINE', 'desc': "The paper introduces RE-IMAGINE, a framework designed to assess the reasoning capabilities of Large Language Models (LLMs) by generating problem variations that require more than just memorization. It builds on the ladder of causation, which categorizes reasoning into three levels: associations, interventions, and counterfactuals. By transforming problems into an intermediate symbolic representation, RE-IMAGINE creates numerous unique problems that challenge LLMs' reasoning skills. The results from testing various LLMs on established benchmarks reveal a significant reliance on statistical recall, suggesting that their high accuracy may not reflect true reasoning abilities."}, 'zh': {'title': 'æ¨ç†èƒ½åŠ›çš„æ–°è¯„ä¼°ï¼šRE-IMAGINEæ¡†æ¶', 'desc': 'RE-IMAGINE æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡ç”Ÿæˆæ— æ³•ä»…é è®°å¿†è§£å†³çš„é—®é¢˜å˜ä½“ï¼Œæ¥æ£€éªŒæ¨¡å‹æ˜¯å¦çœŸæ­£å…·å¤‡æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŸºäºå› æœæ¨ç†çš„å±‚æ¬¡ç»“æ„ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ¨ç†é¢†åŸŸï¼ˆå¦‚æ•°å­¦ã€ä»£ç å’Œé€»è¾‘ï¼‰ä¸­ç”Ÿæˆé—®é¢˜å˜ä½“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨¡å‹é¢å¯¹è¿™äº›å˜ä½“æ—¶ï¼Œæ€§èƒ½ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¾èµ–äºç»Ÿè®¡è®°å¿†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19851', 'title': 'AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2506.19851', 'abstract': 'AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}.', 'score': 40, 'issue_id': 4470, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': 'ce9a811b1a9e7d9d', 'authors': ['Zehuan Huang', 'Haoran Feng', 'Yangtian Sun', 'Yuanchen Guo', 'Yanpei Cao', 'Lu Sheng'], 'affiliations': ['Beihang University, China', 'The University of Hong Kong, China', 'Tsinghua University, China', 'VAST, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.19851.jpg', 'data': {'categories': ['#optimization', '#3d', '#transfer_learning', '#diffusion', '#dataset'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'AnimaX: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'AnimaX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… 2D-ĞºĞ°Ñ€Ñ‚ Ğ¿Ğ¾Ğ· Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ· Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. AnimaX Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ·. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Bridging Video Motion and 3D Animation with AnimaX', 'desc': 'AnimaX is a novel framework for creating 3D animations that combines the strengths of video diffusion models with skeleton-based control. It allows for the generation of animations without being limited to fixed skeletal structures, overcoming the challenges of high-dimensional optimization. By using multi-view, multi-frame 2D pose maps and shared positional encodings, AnimaX ensures that video motion knowledge is effectively transferred to 3D motion generation. This method has been trained on a large dataset and demonstrates superior performance in terms of generalization, motion fidelity, and efficiency for diverse animated characters.'}, 'zh': {'title': 'AnimaXï¼šæ— ç±»åˆ« 3D åŠ¨ç”»çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'AnimaX æ˜¯ä¸€ä¸ªå‰é¦ˆå¼çš„ 3D åŠ¨ç”»æ¡†æ¶ï¼Œå®ƒç»“åˆäº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿åŠ¨å…ˆéªŒå’ŒåŸºäºéª¨éª¼çš„å¯æ§ç»“æ„ã€‚ä¸ä¼ ç»Ÿçš„è¿åŠ¨åˆæˆæ–¹æ³•ä¸åŒï¼ŒAnimaX æ”¯æŒä»»æ„éª¨éª¼çš„å¤šæ ·åŒ–å…³èŠ‚ç½‘æ ¼ï¼Œå¹¶æœ‰æ•ˆåœ°å°†è§†é¢‘ä¸­çš„è¿åŠ¨çŸ¥è¯†è½¬ç§»åˆ° 3D é¢†åŸŸã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šè§†è§’ã€å¤šå¸§çš„ 2D å§¿æ€å›¾è¡¨ç¤º 3D è¿åŠ¨ï¼Œå¹¶åˆ©ç”¨å…±äº«ä½ç½®ç¼–ç å’Œæ¨¡æ€æ„ŸçŸ¥åµŒå…¥ç¡®ä¿è§†é¢‘å’Œå§¿æ€åºåˆ—ä¹‹é—´çš„æ—¶ç©ºå¯¹é½ã€‚ç»è¿‡åœ¨ä¸€ä¸ªåŒ…å« 160,000 ä¸ªç»‘å®šåºåˆ—çš„æ–°æ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒAnimaX åœ¨ VBench ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ— ç±»åˆ« 3D åŠ¨ç”»è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18701', 'title': 'Matrix-Game: Interactive World Foundation Model', 'url': 'https://huggingface.co/papers/2506.18701', 'abstract': 'Matrix-Game, a controllable game world generation model trained in a two-stage process, outperforms existing models by producing high-quality, action-controllable, and physically consistent Minecraft world videos.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.', 'score': 37, 'issue_id': 4478, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'a6267b86f005d608', 'authors': ['Yifan Zhang', 'Chunli Peng', 'Boyang Wang', 'Puyi Wang', 'Qingcheng Zhu', 'Fei Kang', 'Biao Jiang', 'Zedong Gao', 'Eric Li', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Interactive World Foundation', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.18701.jpg', 'data': {'categories': ['#open_source', '#dataset', '#video', '#benchmark', '#data', '#games'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²: Matrix-Game ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Minecraft', 'desc': 'Matrix-Game - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Minecraft, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 2700 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ 1000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ 17 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Matrix-Game Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'Matrix-Game: Revolutionizing Game World Generation with Control and Quality', 'desc': "Matrix-Game is a novel model designed for generating interactive game worlds, specifically in Minecraft, using a two-stage training process. The first stage involves unsupervised pretraining on a large dataset of gameplay videos to understand the environment, followed by a second stage of supervised training with action-labeled data for generating videos based on user interactions. With over 17 billion parameters, the model excels in producing high-quality, controllable videos that maintain physical consistency and visual coherence. The introduction of the GameWorld Score benchmark allows for comprehensive evaluation of the model's performance in terms of visual quality, action controllability, and adherence to physical rules, demonstrating its superiority over existing models."}, 'zh': {'title': 'Matrix-Gameï¼šå¯æ§çš„æ¸¸æˆä¸–ç•Œç”Ÿæˆæ–°æ ‡å‡†', 'desc': 'Matrix-Gameæ˜¯ä¸€ç§å¯æ§çš„æ¸¸æˆä¸–ç•Œç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒè¿‡ç¨‹æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚é¦–å…ˆè¿›è¡Œå¤§è§„æ¨¡çš„æ— æ ‡ç­¾é¢„è®­ç»ƒï¼Œä»¥ç†è§£ç¯å¢ƒï¼Œç„¶åè¿›è¡Œæœ‰æ ‡ç­¾çš„è®­ç»ƒä»¥ç”Ÿæˆäº’åŠ¨è§†é¢‘ã€‚è¯¥æ¨¡å‹ä½¿ç”¨è¶…è¿‡170äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶è§’è‰²åŠ¨ä½œå’Œç›¸æœºç§»åŠ¨ï¼ŒåŒæ—¶ä¿æŒé«˜è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMatrix-Gameåœ¨å¯æ§æ€§å’Œç‰©ç†ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„Minecraftä¸–ç•Œæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19290', 'title': 'Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs', 'url': 'https://huggingface.co/papers/2506.19290', 'abstract': "An automated data-curation pipeline for software engineering improves large language model performance on SWE tasks, achieving state-of-the-art results with and without test-time scaling techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds) and long-context dependency resolution (e.g., >32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research.", 'score': 27, 'issue_id': 4471, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '7c26a879c3db6096', 'authors': ['Liang Zeng', 'Yongcong Li', 'Yuzhen Xiao', 'Changshi Li', 'Chris Yuhao Liu', 'Rui Yan', 'Tianwen Wei', 'Jujie He', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc'], 'pdf_title_img': 'assets/pdf/title_img/2506.19290.jpg', 'data': {'categories': ['#open_source', '#dataset', '#data', '#long_context', '#benchmark', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ LLM Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 10 000 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Python Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Skywork-SWE Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ LLM Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen2.5-Coder-32B, ĞºĞ°Ğº Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ….'}, 'en': {'title': 'Automating Data Curation to Boost LLMs in Software Engineering', 'desc': "This paper presents an automated data-curation pipeline designed to enhance the performance of large language models (LLMs) in software engineering (SWE) tasks. The pipeline addresses the challenges of manual data annotation and environment setup by providing a diverse dataset of over 10,000 real-world Python task instances from GitHub. The authors demonstrate that increasing the dataset size leads to improved model performance, with their Skywork-SWE model achieving state-of-the-art accuracy on the SWE-bench Verified benchmark. Additionally, the incorporation of test-time scaling techniques further boosts the model's performance, establishing new benchmarks for LLMs in SWE."}, 'zh': {'title': 'è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†ï¼Œæå‡è½¯ä»¶å·¥ç¨‹æ¨¡å‹è¡¨ç°', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†ç®¡é“ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥ç®¡é“é€šè¿‡ç³»ç»Ÿæ€§åœ°æ‰©å±•æ•°æ®é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§ï¼Œè§£å†³äº†ä¼ ç»Ÿæ•°æ®æ•´ç†è¿‡ç¨‹ä¸­çš„æ—¶é—´æ¶ˆè€—é—®é¢˜ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«10,169ä¸ªçœŸå®Pythonä»»åŠ¡å®ä¾‹çš„æ•°æ®é›†ï¼Œå¹¶æˆåŠŸéªŒè¯äº†è¶…è¿‡8,000ä¸ªè®­ç»ƒè½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€æ•°æ®é‡çš„å¢åŠ ï¼Œæ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹èƒ½åŠ›ä¸Šçš„è¡¨ç°æŒç»­æå‡ï¼Œæœ€ç»ˆåœ¨SWE-bench VerifiedåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†38.0%çš„å‡†ç¡®ç‡ï¼Œåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16141', 'title': 'GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.16141', 'abstract': "GRPO-CARE, a reinforcement learning framework optimizing for consistency and correctness, outperforms standard GRPO on a new video understanding benchmark, SEED-Bench-R1, improving both performance and logical coherence in multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.", 'score': 24, 'issue_id': 4471, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '0d8fc795754c4210', 'authors': ['Yi Chen', 'Yuying Ge', 'Rui Wang', 'Yixiao Ge', 'Junhao Cheng', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.16141.jpg', 'data': {'categories': ['#transfer_learning', '#rl', '#interpretability', '#multimodal', '#benchmark', '#training', '#optimization', '#video', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'GRPO-CARE: Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜', 'desc': 'GRPO-CARE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. GRPO-CARE Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ±Ğ¾Ğ½ÑƒÑ Ğ·Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ GRPO Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SEED-Bench-R1 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing Consistency and Correctness in Multimodal Learning', 'desc': 'The paper introduces GRPO-CARE, a new reinforcement learning framework that enhances the performance and logical coherence of multimodal large language models (MLLMs) in video understanding tasks. It addresses the limitations of standard GRPO, which often sacrifices reasoning consistency for accuracy, by implementing a two-tiered reward system that promotes both correct answers and coherent reasoning. The authors present SEED-Bench-R1, a benchmark designed to rigorously evaluate MLLMs on complex video tasks, revealing that GRPO-CARE significantly outperforms GRPO with a notable increase in consistency and performance. This work not only proposes a novel framework but also contributes a valuable benchmark for future research in MLLM development.'}, 'zh': {'title': 'æå‡ä¸€è‡´æ€§ä¸æ­£ç¡®æ€§çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'GRPO-CAREæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¸€è‡´æ€§å’Œæ­£ç¡®æ€§ï¼Œè¶…è¶Šäº†æ ‡å‡†çš„GRPOæ–¹æ³•ã€‚å®ƒåœ¨æ–°çš„è§†é¢‘ç†è§£åŸºå‡†SEED-Bench-R1ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œé€»è¾‘è¿è´¯æ€§ã€‚é€šè¿‡å¼•å…¥åŒé‡å¥–åŠ±æœºåˆ¶ï¼ŒGRPO-CAREä¸ä»…å…³æ³¨ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œè¿˜é¼“åŠ±æ¨ç†è¿‡ç¨‹çš„é€»è¾‘ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œè§†é¢‘ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„è¿ç§»èƒ½åŠ›ï¼Œæ¨åŠ¨äº†æ›´å…·å¯è§£é‡Šæ€§å’Œé²æ£’æ€§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19848', 'title': 'ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing', 'url': 'https://huggingface.co/papers/2506.19848', 'abstract': 'ScaleCap enhances image captioning by iteratively enriching and calibrating captions using heuristic question answering and contrastive sentence rating, addressing multimodal and linguistic biases to improve accuracy, balance, and informativeness.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents ScaleCap, an inference-time scalable image captioning strategy that generates comprehensive and detailed image captions. The key challenges of high-quality image captioning lie in the inherent biases of LVLMs: multimodal bias resulting in imbalanced descriptive granularity, offering detailed accounts of some elements while merely skimming over others; linguistic bias leading to hallucinated descriptions of non-existent objects. To address these issues, we propose a scalable debiased captioning strategy, which continuously enriches and calibrates the caption with increased inference budget. Specifically, we propose two novel components: heuristic question answering and contrastive sentence rating. The former generates content-specific questions based on the image and answers them to progressively inject relevant information into the caption. The latter employs sentence-level offline contrastive decoding to effectively identify and eliminate hallucinations caused by linguistic biases. With increased inference cost, more heuristic questions are raised by ScaleCap to progressively capture additional visual details, generating captions that are more accurate, balanced, and informative. Extensive modality alignment experiments demonstrate the effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them for LVLM pretraining leads to consistent performance gains across 11 widely used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity of generated captions with two additional tasks: replacing images with captions in VQA task, and reconstructing images from captions to assess semantic coverage. Code is available at https://github.com/Cooperx521/ScaleCap.', 'score': 23, 'issue_id': 4473, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '0017ef0507a6b74d', 'authors': ['Long Xing', 'Qidong Huang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Zang', 'Yuhang Cao', 'Jinsong Li', 'Shuangrui Ding', 'Weiming Zhang', 'Nenghai Yu', 'Jiaqi Wang', 'Feng Wu', 'Dahua Lin'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.19848.jpg', 'data': {'categories': ['#cv', '#benchmark', '#hallucinations', '#interpretability', '#multimodal', '#inference'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ScaleCap: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'ScaleCap - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ScaleCap Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Enhancing Image Captions with ScaleCap: Accurate, Balanced, and Informative!', 'desc': 'ScaleCap is a novel approach to image captioning that improves the quality of generated captions by addressing biases in large vision-language models (LVLMs). It uses heuristic question answering to generate specific questions about the image, which helps to enrich the captions with relevant details. Additionally, it employs contrastive sentence rating to identify and remove inaccuracies or hallucinations in the descriptions. By iteratively refining captions with these techniques, ScaleCap produces more accurate, balanced, and informative image descriptions.'}, 'zh': {'title': 'ScaleCapï¼šæå‡å›¾åƒæè¿°çš„æ™ºèƒ½ç­–ç•¥', 'desc': 'ScaleCapæ˜¯ä¸€ç§å¢å¼ºå›¾åƒæè¿°ç”Ÿæˆçš„ç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£ä¸°å¯Œå’Œæ ¡å‡†æè¿°ï¼Œè§£å†³å¤šæ¨¡æ€å’Œè¯­è¨€åè§é—®é¢˜ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§å’Œä¿¡æ¯é‡ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å¯å‘å¼é—®ç­”å’Œå¯¹æ¯”å¥å­è¯„åˆ†ä¸¤ä¸ªæ–°ç»„ä»¶ï¼Œå‰è€…æ ¹æ®å›¾åƒç”Ÿæˆç‰¹å®šé—®é¢˜å¹¶å›ç­”ï¼Œä»¥é€æ­¥æ³¨å…¥ç›¸å…³ä¿¡æ¯ã€‚åè€…é€šè¿‡å¥å­çº§çš„å¯¹æ¯”è§£ç ï¼Œæœ‰æ•ˆè¯†åˆ«å¹¶æ¶ˆé™¤ç”±äºè¯­è¨€åè§å¯¼è‡´çš„è™šå‡æè¿°ã€‚é€šè¿‡å¢åŠ æ¨ç†é¢„ç®—ï¼ŒScaleCapèƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®ã€å¹³è¡¡å’Œä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæè¿°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17612', 'title': 'JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent', 'url': 'https://huggingface.co/papers/2506.17612', 'abstract': 'JarvisArt, an MLLM-driven agent, achieves superior photo retouching by understanding user intent and coordinating multiple retouching tools in Lightroom, outperforming GPT-4o on a novel benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities. Project Page: https://jarvisart.vercel.app/.', 'score': 17, 'issue_id': 4472, 'pub_date': '2025-06-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ½Ñ', 'en': 'June 21', 'zh': '6æœˆ21æ—¥'}, 'hash': '2504aa6b996e0739', 'authors': ['Yunlong Lin', 'Zixu Lin', 'Kunjie Lin', 'Jinbin Bai', 'Panwang Pan', 'Chenxin Li', 'Haoyu Chen', 'Zhongdao Wang', 'Xinghao Ding', 'Wenbo Li', 'Shuicheng Yan'], 'affiliations': ['Bytedance', 'Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, Fujian, China', 'National University of Singapore', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.17612.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#games', '#reasoning', '#optimization', '#agents', '#cv', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'JarvisArt: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‚ÑƒÑˆĞ¸ Ñ„Ğ¾Ñ‚Ğ¾', 'desc': 'JarvisArt - ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (ĞœLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµÑ‚ÑƒÑˆÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Lightroom. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° fine-tuning Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Group Relative Policy Optimization for Retouching (GRPO-R). JarvisArt Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMArt-Bench, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 60% Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ… Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Photo Retouching with Intelligent AI', 'desc': 'JarvisArt is an advanced multi-modal large language model (MLLM) agent designed for photo retouching, which excels in understanding user intent and effectively coordinating over 200 tools in Adobe Lightroom. Unlike traditional AI solutions that lack flexibility, JarvisArt mimics the reasoning of professional artists, allowing for personalized and nuanced editing. It employs a two-stage training process that enhances its decision-making abilities and tool proficiency, ensuring high-quality results. The performance of JarvisArt is validated through a new benchmark, MMArt-Bench, where it significantly outperforms existing models like GPT-4o in pixel-level metrics for content fidelity.'}, 'zh': {'title': 'æ™ºèƒ½ä¿®é¥°ï¼Œè¶…è¶Šä¼ ç»Ÿï¼', 'desc': 'JarvisArt æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ™ºèƒ½ä»£ç†ï¼Œä¸“æ³¨äºç…§ç‰‡ä¿®é¥°ã€‚å®ƒèƒ½å¤Ÿç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¹¶åè°ƒ Lightroom ä¸­çš„200å¤šç§ä¿®é¥°å·¥å…·ï¼Œæä¾›æ›´ä¼˜è´¨çš„ä¿®é¥°æ•ˆæœã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒè¿‡ç¨‹ï¼ŒJarvisArt æå‡äº†å†³ç­–èƒ½åŠ›å’Œå·¥å…·ä½¿ç”¨ç†Ÿç»ƒåº¦ï¼Œèƒ½å¤Ÿæ»¡è¶³ç”¨æˆ·ä¸ªæ€§åŒ–çš„ç¼–è¾‘éœ€æ±‚ã€‚ä¸ç°æœ‰çš„ AI è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒJarvisArt åœ¨å†…å®¹ä¿çœŸåº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹³å‡åƒç´ çº§æŒ‡æ ‡æé«˜äº†60%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19467', 'title': 'Can Large Language Models Capture Human Annotator Disagreements?', 'url': 'https://huggingface.co/papers/2506.19467', 'abstract': 'LLMs struggle to predict human annotation disagreements, contrary to their performance in predicting majority labels, and RLVR-style reasoning exacerbates this issue.  \t\t\t\t\tAI-generated summary \t\t\t\t Human annotation variation (i.e., annotation disagreements) is common in NLP and often reflects important information such as task subjectivity and sample ambiguity. While Large Language Models (LLMs) are increasingly used for automatic annotation to reduce human effort, their evaluation often focuses on predicting the majority-voted "ground truth" labels. It is still unclear, however, whether these models also capture informative human annotation variation. Our work addresses this gap by extensively evaluating LLMs\' ability to predict annotation disagreements without access to repeated human labels. Our results show that LLMs struggle with modeling disagreements, which can be overlooked by majority label-based evaluations. Notably, while RLVR-style (Reinforcement learning with verifiable rewards) reasoning generally boosts LLM performance, it degrades performance in disagreement prediction. Our findings highlight the critical need for evaluating and improving LLM annotators in disagreement modeling. Code and data at https://github.com/EdisonNi-hku/Disagreement_Prediction.', 'score': 14, 'issue_id': 4480, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': 'bdc388a5dcc6fc40', 'authors': ['Jingwei Ni', 'Yu Fan', 'VilÃ©m Zouhar', 'Donya Rooein', 'Alexander Hoyle', 'Mrinmaya Sachan', 'Markus Leippold', 'Dirk Hovy', 'Elliott Ash'], 'affiliations': ['Bocconi University', 'ETH ZÃ¼rich', 'University of ZÃ¼rich'], 'pdf_title_img': 'assets/pdf/title_img/2506.19467.jpg', 'data': {'categories': ['#rl', '#interpretability', '#reasoning', '#rlhf', '#alignment', '#data', '#multilingual'], 'emoji': 'ğŸ¤”', 'ru': {'title': 'LLM Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ğ¹ Ğ² Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ñ Ğ² Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ğ¹, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸Ñ… Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ°. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ RLVR (Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸) ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LLM Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Bridging the Gap: Enhancing LLMs in Predicting Human Annotation Disagreements', 'desc': "This paper investigates how well Large Language Models (LLMs) can predict disagreements among human annotations, which often indicate important nuances in tasks. While LLMs perform well in predicting majority labels, they struggle to capture the variation in human opinions, which is crucial for understanding task subjectivity. The study reveals that using RLVR-style reasoning, which typically enhances performance, actually worsens the models' ability to predict these disagreements. The authors emphasize the importance of developing better evaluation methods for LLMs to address this gap in understanding human annotation variation."}, 'zh': {'title': 'æå‡LLMåœ¨æ ‡æ³¨ä¸ä¸€è‡´æ€§é¢„æµ‹ä¸­çš„èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢„æµ‹äººç±»æ ‡æ³¨ä¸ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚å°½ç®¡LLMsåœ¨é¢„æµ‹å¤šæ•°æ ‡ç­¾æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬åœ¨æ•æ‰äººç±»æ ‡æ³¨çš„å˜å¼‚æ€§æ—¶å´å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æ–¹æ³•è™½ç„¶èƒ½æå‡LLMsçš„æ•´ä½“æ€§èƒ½ï¼Œä½†å´ä¼šé™ä½å…¶åœ¨é¢„æµ‹æ ‡æ³¨ä¸ä¸€è‡´æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†åœ¨ä¸ä¸€è‡´æ€§å»ºæ¨¡ä¸­è¯„ä¼°å’Œæ”¹è¿›LLMæ ‡æ³¨å™¨çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18951', 'title': 'SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications', 'url': 'https://huggingface.co/papers/2506.18951', 'abstract': "A new benchmark and training environment for debugging SQL issues using advanced open-source models significantly improves their performance compared to proprietary solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging SQL issues. To address this gap, we introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530 PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within new environments to facilitate rigorous evaluation. Baseline evaluations underscore the task's complexity, with the leading reasoning model O3-Mini achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks is crucial for empowering local development while safeguarding data privacy. Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for elevating open-source model capabilities for SQL issue debugging. This environment leverages SQL-Rewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose f-Plan Boosting, which extracts high-level debugging plans from SQL solutions, enabling teacher LLMs to produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B, Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-Multi, surpassing leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities. The leaderboard and source code are available: https://bird-critic.github.io/", 'score': 14, 'issue_id': 4476, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'b6a4ef33fdb5cc7e', 'authors': ['Jinyang Li', 'Xiaolong Li', 'Ge Qu', 'Per Jacobsson', 'Bowen Qin', 'Binyuan Hui', 'Shuzheng Si', 'Nan Huo', 'Xiaohan Xu', 'Yue Zhang', 'Ziwei Tang', 'Yuanshuai Li', 'Florensia Widjaja', 'Xintong Zhu', 'Feige Zhou', 'Yongfeng Huang', 'Yannis Papakonstantinou', 'Fatma Ozcan', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['CUHK', 'CUHKSZ', 'Google Cloud', 'HKU STAR Lab', 'THU', 'The BIRD Team'], 'pdf_title_img': 'assets/pdf/title_img/2506.18951.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#reasoning', '#benchmark', '#agents'], 'emoji': 'ğŸ¦', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞµ SQL', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BIRD-CRITIC Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ PostgreSQL Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ€ĞµĞ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Six-Gym Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞµ SQL. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ SQL-Rewind Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ f-Plan Boosting, Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Bird-Fixer Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen-2.5-Coder-14B. Bird-Fixer Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ SQL, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Empowering SQL Debugging with Open-Source Innovations', 'desc': 'This paper introduces BIRD-CRITIC, a new benchmark for evaluating SQL debugging capabilities of machine learning models, featuring 530 PostgreSQL and 570 multi-dialect tasks derived from real user issues. It highlights the limitations of current Large Language Models (LLMs) in effectively debugging SQL problems, with the best existing model achieving only modest success rates. To enhance model performance, the authors present Six-Gym, a training environment that utilizes the SQL-Rewind strategy to create executable datasets for training. Additionally, they propose f-Plan Boosting to improve the learning process by extracting high-level debugging plans, resulting in better training outcomes for open-source models like Bird-Fixer, which outperforms some proprietary solutions.'}, 'zh': {'title': 'æå‡å¼€æºæ¨¡å‹ï¼Œç ´è§£SQLè°ƒè¯•éš¾é¢˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å’Œè®­ç»ƒç¯å¢ƒï¼Œæ—¨åœ¨é€šè¿‡å…ˆè¿›çš„å¼€æºæ¨¡å‹æ¥è°ƒè¯•SQLé—®é¢˜ï¼Œä»è€Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†BIRD-CRITICï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«530ä¸ªPostgreSQLä»»åŠ¡å’Œ570ä¸ªå¤šæ–¹è¨€ä»»åŠ¡çš„SQLè°ƒè¯•åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è°ƒè¯•SQLé—®é¢˜ä¸Šçš„èƒ½åŠ›ã€‚ä¸ºäº†æå‡å¼€æºæ¨¡å‹çš„è°ƒè¯•èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†Six-Gymè®­ç»ƒç¯å¢ƒï¼Œå¹¶å¼•å…¥äº†SQL-Rewindç­–ç•¥å’Œf-Plan Boostingæ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹çš„æˆåŠŸç‡ã€‚æœ€ç»ˆï¼ŒåŸºäºQwen-2.5-Coder-14Bçš„Bird-Fixeråœ¨BIRD-CRITICåŸºå‡†ä¸Šå–å¾—äº†ä¼˜äºé¢†å…ˆä¸“æœ‰æ¨¡å‹çš„æˆç»©ï¼Œæ ‡å¿—ç€SQLè°ƒè¯•èƒ½åŠ›çš„æ°‘ä¸»åŒ–è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19850', 'title': 'Unified Vision-Language-Action Model', 'url': 'https://huggingface.co/papers/2506.19850', 'abstract': "UniVLA is a multimodal VLA model that autoregressively processes vision, language, and action as token sequences, incorporating world modeling for effective long-horizon policy learning and achieving state-of-the-art results across simulation and real-world benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving.", 'score': 10, 'issue_id': 4476, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '72bc18c81e5ac0e4', 'authors': ['Yuqi Wang', 'Xinghang Li', 'Wenxuan Wang', 'Junbo Zhang', 'Yingyan Li', 'Yuntao Chen', 'Xinlong Wang', 'Zhaoxiang Zhang'], 'affiliations': ['BAAI', 'CASIA', 'HKISI', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2506.19850.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#transfer_learning', '#video', '#benchmark', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'UniVLA: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸', 'desc': 'UniVLA - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ…. UniVLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'UniVLA: Advancing Robotic Manipulation with Multimodal Learning', 'desc': 'UniVLA is a new multimodal vision-language-action (VLA) model that processes visual, linguistic, and action data as sequences of tokens. It improves upon previous models by incorporating world modeling, which helps it understand the causal relationships in video data. This allows UniVLA to learn effective policies for long-horizon tasks, achieving top performance on various benchmarks. The model has shown significant success in both simulated environments and real-world applications, outperforming existing methods.'}, 'zh': {'title': 'UniVLAï¼šå¤šæ¨¡æ€å­¦ä¹ çš„æ–°çªç ´', 'desc': 'UniVLAæ˜¯ä¸€ç§å¤šæ¨¡æ€è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹ï¼Œå®ƒä»¥è‡ªå›å½’çš„æ–¹å¼å¤„ç†è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¿¡å·ï¼Œå°†å…¶ä½œä¸ºç¦»æ•£çš„æ ‡è®°åºåˆ—è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸–ç•Œå»ºæ¨¡æ¥æ•æ‰è§†é¢‘ä¸­çš„å› æœåŠ¨æ€ï¼Œä»è€Œæœ‰æ•ˆåœ°è¿›è¡Œé•¿æœŸç­–ç•¥å­¦ä¹ ã€‚UniVLAåœ¨å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„ä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚å®ƒåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†95.5%çš„å¹³å‡æˆåŠŸç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19767', 'title': 'SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning', 'url': 'https://huggingface.co/papers/2506.19767', 'abstract': 'Supervised Reinforcement Fine-Tuning (SRFT) integrates Supervised Fine-Tuning and Reinforcement Learning through entropy-aware weighting to achieve high accuracy in language model optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks.', 'score': 9, 'issue_id': 4471, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': 'dfdf02be41523939', 'authors': ['Yuqian Fu', 'Tinghong Chen', 'Jiajun Chai', 'Xihuai Wang', 'Songjun Tu', 'Guojun Yin', 'Wei Lin', 'Qichao Zhang', 'Yuanheng Zhu', 'Dongbin Zhao'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Meituan', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19767.jpg', 'data': {'categories': ['#rl', '#rlhf', '#training', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ supervised Ğ¸ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Supervised Reinforcement Fine-Tuning (SRFT). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Supervised Fine-Tuning Ğ¸ Reinforcement Learning, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. SRFT Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SRFT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unifying Fine-Tuning for Superior Language Model Performance', 'desc': 'Supervised Reinforcement Fine-Tuning (SRFT) combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance the performance of language models. It uses entropy-aware weighting to balance the strengths of both methods, allowing for better optimization of model policies. The paper highlights how SFT makes broad changes to model behavior, while RL focuses on specific improvements, with entropy being a key measure of success. SRFT has been shown to significantly improve accuracy on various reasoning tasks compared to traditional methods.'}, 'zh': {'title': 'ç›‘ç£å¼ºåŒ–å¾®è°ƒï¼šä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'ç›‘ç£å¼ºåŒ–å¾®è°ƒï¼ˆSRFTï¼‰ç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡ç†µæ„ŸçŸ¥åŠ æƒå®ç°è¯­è¨€æ¨¡å‹ä¼˜åŒ–çš„é«˜å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›‘ç£å¾®è°ƒä¼šå¯¹è¯­è¨€æ¨¡å‹çš„ç­–ç•¥åˆ†å¸ƒäº§ç”Ÿç²—ç²’åº¦çš„å…¨å±€å˜åŒ–ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™è¿›è¡Œç»†ç²’åº¦çš„é€‰æ‹©æ€§ä¼˜åŒ–ï¼Œç†µæ˜¯è®­ç»ƒæ•ˆæœçš„é‡è¦æŒ‡æ ‡ã€‚SRFTæ–¹æ³•é€šè¿‡ç†µæ„ŸçŸ¥åŠ æƒæœºåˆ¶ï¼Œå°†è¿™ä¸¤ç§å¾®è°ƒèŒƒå¼ç»Ÿä¸€ä¸ºå•é˜¶æ®µæ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSRFTåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°59.1%ï¼Œæ¯”é›¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•æé«˜äº†9.0%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19713', 'title': 'Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low\n  CFG Scales', 'url': 'https://huggingface.co/papers/2506.19713', 'abstract': 'Frequency-decoupled guidance (FDG) enhances image quality and diversity by separately controlling low- and high-frequency guidance components in diffusion models, outperforming standard classifier-free guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-free guidance (CFG) has become an essential component of modern conditional diffusion models. Although highly effective in practice, the underlying mechanisms by which CFG enhances quality, detail, and prompt alignment are not fully understood. We present a novel perspective on CFG by analyzing its effects in the frequency domain, showing that low and high frequencies have distinct impacts on generation quality. Specifically, low-frequency guidance governs global structure and condition alignment, while high-frequency guidance mainly enhances visual fidelity. However, applying a uniform scale across all frequencies -- as is done in standard CFG -- leads to oversaturation and reduced diversity at high scales and degraded visual quality at low scales. Based on these insights, we propose frequency-decoupled guidance (FDG), an effective approach that decomposes CFG into low- and high-frequency components and applies separate guidance strengths to each component. FDG improves image quality at low guidance scales and avoids the drawbacks of high CFG scales by design. Through extensive experiments across multiple datasets and models, we demonstrate that FDG consistently enhances sample fidelity while preserving diversity, leading to improved FID and recall compared to CFG, establishing our method as a plug-and-play alternative to standard classifier-free guidance.', 'score': 9, 'issue_id': 4475, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '5fb28071fbad67d8', 'authors': ['Seyedmorteza Sadat', 'Tobias Vontobel', 'Farnood Salehi', 'Romann M. Weber'], 'affiliations': ['Disney Research Studios', 'ETH ZÃ¼rich'], 'pdf_title_img': 'assets/pdf/title_img/2506.19713.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#optimization', '#dataset'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾-Ñ€Ğ°Ğ·Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ (FDG). FDG Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾- Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğº Ğ½Ğ¸Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ ÑĞ¸Ğ»Ñƒ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ·ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ (CFG). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ FDG Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº FID Ğ¸ recall.'}, 'en': {'title': 'Decoupling Frequencies for Superior Image Generation', 'desc': 'This paper introduces Frequency-Decoupled Guidance (FDG), a new method for improving image generation in diffusion models. FDG separates the guidance into low-frequency and high-frequency components, allowing for tailored control over each aspect of image quality. The low-frequency guidance focuses on the overall structure and alignment with conditions, while high-frequency guidance enhances fine details and visual fidelity. By adjusting the strengths of these components independently, FDG avoids the common pitfalls of standard classifier-free guidance, resulting in higher quality images with better diversity.'}, 'zh': {'title': 'é¢‘ç‡è§£è€¦å¼•å¯¼ï¼šæå‡å›¾åƒè´¨é‡ä¸å¤šæ ·æ€§çš„æ–°æ–¹æ³•', 'desc': 'é¢‘ç‡è§£è€¦å¼•å¯¼ï¼ˆFDGï¼‰é€šè¿‡åˆ†åˆ«æ§åˆ¶æ‰©æ•£æ¨¡å‹ä¸­çš„ä½é¢‘å’Œé«˜é¢‘å¼•å¯¼æˆåˆ†ï¼Œæå‡äº†å›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œè¶…è¶Šäº†æ ‡å‡†çš„æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½é¢‘å¼•å¯¼ä¸»è¦å½±å“å…¨å±€ç»“æ„å’Œæ¡ä»¶å¯¹é½ï¼Œè€Œé«˜é¢‘å¼•å¯¼åˆ™å¢å¼ºè§†è§‰ç»†èŠ‚ã€‚ä¼ ç»Ÿçš„CFGåœ¨æ‰€æœ‰é¢‘ç‡ä¸Šä½¿ç”¨ç»Ÿä¸€çš„æ¯”ä¾‹ï¼Œå¯¼è‡´é«˜é¢‘æ—¶çš„è¿‡é¥±å’Œå’Œå¤šæ ·æ€§é™ä½ï¼Œä½é¢‘æ—¶çš„è§†è§‰è´¨é‡ä¸‹é™ã€‚FDGé€šè¿‡å°†CFGåˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œå¹¶å¯¹æ¯ä¸ªæˆåˆ†åº”ç”¨ä¸åŒçš„å¼•å¯¼å¼ºåº¦ï¼Œæœ‰æ•ˆæ”¹å–„äº†å›¾åƒè´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†æ ·æœ¬çš„å¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18843', 'title': 'USAD: Universal Speech and Audio Representation via Distillation', 'url': 'https://huggingface.co/papers/2506.18843', 'abstract': 'USAD integrates diverse audio types using efficient layer-to-layer distillation from domain-specific models, achieving competitive performance across various benchmarks with a single encoder.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.', 'score': 9, 'issue_id': 4472, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'c7855e42be638a70', 'authors': ['Heng-Jui Chang', 'Saurabhchand Bhati', 'James Glass', 'Alexander H. Liu'], 'affiliations': ['MIT CSAIL Cambridge, MA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.18843.jpg', 'data': {'categories': ['#benchmark', '#audio'], 'emoji': 'ğŸ§', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾', 'desc': 'USAD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ€ĞµÑ‡ÑŒ, Ğ·Ğ²ÑƒĞºĞ¸ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ñ‚ ÑĞ»Ğ¾Ñ Ğº ÑĞ»Ğ¾Ñ Ğ¸Ğ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ. USAD Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ², Ñ‚ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğº state-of-the-art Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SUPERB Ğ¸ HEAR.'}, 'en': {'title': 'One Model, Many Sounds: USAD Unifies Audio Learning', 'desc': 'The paper introduces Universal Speech and Audio Distillation (USAD), a novel method for learning audio representations that combines different audio types such as speech, sound, and music into one model. It utilizes layer-to-layer distillation from specialized self-supervised learning (SSL) models, allowing a student model to learn from a wide-ranging audio dataset. This approach enables USAD to perform well on various tasks, including speech processing, audio tagging, and sound classification. The results demonstrate that USAD achieves competitive performance on multiple benchmarks with just a single encoder, showcasing its efficiency and versatility in audio representation learning.'}, 'zh': {'title': 'ç»Ÿä¸€éŸ³é¢‘è¡¨ç¤ºï¼Œæå‡å¤šä»»åŠ¡æ€§èƒ½', 'desc': 'USADæ˜¯ä¸€ç§ç»Ÿä¸€çš„éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•´åˆå¤šç§éŸ³é¢‘ç±»å‹ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ã€‚å®ƒé€šè¿‡ä»ç‰¹å®šé¢†åŸŸçš„è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ä¸­è¿›è¡Œé«˜æ•ˆçš„å±‚é—´è’¸é¦ï¼Œè®­ç»ƒä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨ä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œå­¦ä¹ ã€‚USADåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬è¯­éŸ³å¤„ç†ã€éŸ³é¢‘æ ‡è®°å’Œå£°éŸ³åˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å•ä¸€ç¼–ç å™¨ï¼Œè¾¾åˆ°äº†æ¥è¿‘æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14012', 'title': 'Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text', 'url': 'https://huggingface.co/papers/2506.14012', 'abstract': "LLMs' comprehension and reasoning skills are evaluated under code-switching conditions, revealing that embedding English into other languages can improve understanding, while prompts and fine-tuning affect degradation mitigation differently.  \t\t\t\t\tAI-generated summary \t\t\t\t Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English textx2013even under linguistic constraintsx2013embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.", 'score': 9, 'issue_id': 4474, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'd8a7231ee69205ba', 'authors': ['Amr Mohamed', 'Yang Zhang', 'Michalis Vazirgiannis', 'Guokan Shang'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14012.jpg', 'data': {'categories': ['#reasoning', '#low_resource', '#multilingual', '#long_context'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¼ĞµÑˆĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ². Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Enhancing LLM Comprehension through Code-Switching', 'desc': "This paper investigates how Large Language Models (LLMs) understand and reason when faced with code-switching, which is the mixing of languages in communication. The study finds that including English words within other languages can enhance the models' comprehension, despite some degradation when foreign words disrupt English text. It also examines the effects of different prompting techniques and highlights that fine-tuning the models provides a more reliable way to reduce comprehension issues. Overall, the research emphasizes the importance of understanding LLM performance in multilingual contexts, especially as code-switching becomes more common in digital communication."}, 'zh': {'title': 'ä»£ç åˆ‡æ¢æå‡ç†è§£èƒ½åŠ›çš„ç ”ç©¶', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç åˆ‡æ¢æ¡ä»¶ä¸‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ä»£ç åˆ‡æ¢æ˜¯æŒ‡åœ¨åŒä¸€äº¤æµä¸­äº¤æ›¿ä½¿ç”¨ä¸¤ç§æˆ–å¤šç§è¯­è¨€ï¼Œè¿™åœ¨å¤šè¯­è¨€ç¤¾åŒºä¸­éå¸¸æ™®éã€‚ç ”ç©¶å‘ç°ï¼Œå°†è‹±è¯­åµŒå…¥å…¶ä»–è¯­è¨€ä¸­å¯ä»¥æé«˜ç†è§£èƒ½åŠ›ï¼Œè€Œæç¤ºå’Œå¾®è°ƒå¯¹å‡è½»æ€§èƒ½ä¸‹é™çš„å½±å“åˆ™æœ‰æ‰€ä¸åŒã€‚æ€»ä½“è€Œè¨€ï¼Œå¾®è°ƒæä¾›äº†ä¸€ç§æ›´ç¨³å®šçš„æ–¹å¼æ¥åº”å¯¹è¯­è¨€æ··åˆå¸¦æ¥çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19838', 'title': 'SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution', 'url': 'https://huggingface.co/papers/2506.19838', 'abstract': 'Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems.', 'score': 8, 'issue_id': 4470, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': 'a00bfa00a7f0f869', 'authors': ['Liangbin Xie', 'Yu Li', 'Shian Du', 'Menghan Xia', 'Xintao Wang', 'Fanghua Yu', 'Ziyan Chen', 'Pengfei Wan', 'Jiantao Zhou', 'Chao Dong'], 'affiliations': ['Kuaishou Technology', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Shenzhen University of Advanced Technology', 'State Key Laboratory of Internet of Things for Smart City, University of Macau', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19838.jpg', 'data': {'categories': ['#optimization', '#training', '#video', '#diffusion', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Video Quality with Smart Cascaded Super-Resolution', 'desc': "This paper presents new design principles for improving cascaded video super-resolution (VSR) models, which are essential for generating high-resolution videos. The authors introduce innovative degradation strategies and timestep sampling methods to enhance the training process, ensuring that the VSR model aligns well with the base model's output. They also explore the impact of noise augmentation on low-resolution inputs and propose techniques like interleaving temporal units and sparse local attention to optimize training efficiency. The results show that their framework outperforms existing methods, providing a solid foundation for future developments in video super-resolution."}, 'zh': {'title': 'æå‡è§†é¢‘è¶…åˆ†è¾¨ç‡ç”Ÿæˆçš„è®¾è®¡åŸåˆ™', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†çº§è”è§†é¢‘è¶…åˆ†è¾¨ç‡æ¨¡å‹çš„è®¾è®¡åŸåˆ™ï¼Œä»¥æé«˜é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆçš„æ•ˆæœã€‚æˆ‘ä»¬å¼•å…¥äº†é€€åŒ–ç­–ç•¥ã€æ—¶é—´æ­¥é‡‡æ ·ã€å™ªå£°å¢å¼ºå’Œç¨€ç–å±€éƒ¨æ³¨æ„åŠ›ç­‰æ–¹æ³•ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ—¶é—´æ­¥é‡‡æ ·ç­–ç•¥å’Œå™ªå£°å¢å¼ºå¯¹ä½åˆ†è¾¨ç‡è¾“å…¥çš„å½±å“ï¼Œä»è€ŒæŒ‡å¯¼æ¨¡å‹æ¶æ„å’Œè®­ç»ƒåˆ›æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°æœ‰æŠ€æœ¯ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥é«˜æ•ˆçº§è”åˆæˆç³»ç»Ÿçš„å‘å±•æä¾›äº†å®ç”¨çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19794', 'title': 'Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study', 'url': 'https://huggingface.co/papers/2506.19794', 'abstract': "Enhancements to open-source large language models' data analysis capabilities through strategic planning, interaction design, and data quality improvements were identified and applied.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities.", 'score': 8, 'issue_id': 4472, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '590e2169fc8a24c8', 'authors': ['Yuqi Zhu', 'Yi Zhong', 'Jintian Zhang', 'Ziheng Zhang', 'Shuofei Qiao', 'Yujie Luo', 'Lun Du', 'Da Zheng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2506.19794.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#benchmark', '#reasoning', '#open_source', '#dataset', '#data'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ‘Ñ‹Ğ»Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM.'}, 'en': {'title': 'Boosting Open-Source LLMs for Better Data Analysis', 'desc': 'This paper explores how to improve the data analysis abilities of open-source Large Language Models (LLMs). It identifies that strategic planning is crucial for enhancing model performance in reasoning tasks. The study also highlights the importance of interaction design and task complexity, as well as the role of data quality over diversity in achieving better results. By applying these findings, the authors propose a new data synthesis method that significantly boosts the analytical reasoning skills of LLMs.'}, 'zh': {'title': 'æå‡å¼€æºLLMsçš„æ•°æ®åˆ†æèƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æå‡å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°æ®åˆ†æä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ç­–åˆ’å¤šæ ·åŒ–çš„çœŸå®åœºæ™¯æ•°æ®é›†ï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ•°æ®ç†è§£ã€ä»£ç ç”Ÿæˆå’Œæˆ˜ç•¥è§„åˆ’ç­‰ä¸‰ä¸ªç»´åº¦çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œæˆ˜ç•¥è§„åˆ’çš„è´¨é‡æ˜¯æ¨¡å‹æ€§èƒ½çš„ä¸»è¦å†³å®šå› ç´ ï¼Œäº¤äº’è®¾è®¡å’Œä»»åŠ¡å¤æ‚æ€§ä¹Ÿæ˜¾è‘—å½±å“æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ•°æ®è´¨é‡å¯¹å®ç°æœ€ä½³æ€§èƒ½çš„å½±å“å¤§äºæ•°æ®çš„å¤šæ ·æ€§ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ•°æ®åˆæˆæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¼€æºLLMsçš„åˆ†ææ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19830', 'title': 'Scaling Speculative Decoding with Lookahead Reasoning', 'url': 'https://huggingface.co/papers/2506.19830', 'abstract': 'Lookahead Reasoning enhances the speed of speculative decoding by introducing step-level parallelism, improving speedup over token-level decoding while maintaining answer quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models excel by generating long chain-of-thoughts, but decoding the resulting thousands of tokens is slow. Token-level speculative decoding (SD) helps, but its benefit is capped, because the chance that an entire gamma-token guess is correct falls exponentially as gamma grows. This means allocating more compute for longer token drafts faces an algorithmic ceiling -- making the speedup modest and hardware-agnostic. We raise this ceiling with Lookahead Reasoning, which exploits a second, step-level layer of parallelism. Our key insight is that reasoning models generate step-by-step, and each step needs only to be semantically correct, not exact token matching. In Lookahead Reasoning, a lightweight draft model proposes several future steps; the target model expands each proposal in one batched pass, and a verifier keeps semantically correct steps while letting the target regenerate any that fail. Token-level SD still operates within each reasoning step, so the two layers of parallelism multiply. We show Lookahead Reasoning lifts the peak speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other benchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x while preserving answer quality, and its speedup scales better with additional GPU throughput. Our code is available at https://github.com/hao-ai-lab/LookaheadReasoning', 'score': 5, 'issue_id': 4482, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '1310e53c2a30fcac', 'authors': ['Yichao Fu', 'Rui Ge', 'Zelei Shao', 'Zhijie Deng', 'Hao Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'UCSD', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2506.19830.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#benchmark'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Lookahead Reasoning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Lookahead Reasoning Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ 1.4x Ğ´Ğ¾ 2.1x Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Boosting Decoding Speed with Lookahead Reasoning', 'desc': 'This paper introduces Lookahead Reasoning, a method that enhances the efficiency of speculative decoding in machine learning models. By implementing step-level parallelism, it allows for faster processing of reasoning tasks while maintaining the quality of the generated answers. The approach involves a lightweight draft model that proposes future steps, which are then expanded by a target model, ensuring semantic correctness without requiring exact token matches. The results show a significant improvement in speedup from 1.4x to 2.1x over traditional token-level decoding methods, especially when utilizing more GPU resources.'}, 'zh': {'title': 'å‰ç»æ¨ç†ï¼šæå‡æ¨æµ‹è§£ç é€Ÿåº¦çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå‰ç»æ¨ç†ï¼ˆLookahead Reasoningï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨æµ‹è§£ç çš„é€Ÿåº¦ã€‚é€šè¿‡å¼•å…¥æ­¥éª¤çº§å¹¶è¡Œæ€§ï¼Œå‰ç»æ¨ç†åœ¨ä¿æŒç­”æ¡ˆè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†é€Ÿåº¦ã€‚ä¼ ç»Ÿçš„åŸºäºä»¤ç‰Œçš„æ¨æµ‹è§£ç åœ¨å¤„ç†é•¿é“¾æ¨ç†æ—¶é¢ä¸´ç®—æ³•ç“¶é¢ˆï¼Œè€Œå‰ç»æ¨ç†é€šè¿‡è½»é‡çº§è‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„æ‰¹é‡æ‰©å±•ï¼Œå…‹æœäº†è¿™ä¸€é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‰ç»æ¨ç†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å°†æ¨æµ‹è§£ç çš„é€Ÿåº¦æå‡ä»1.4å€æé«˜åˆ°2.1å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19807', 'title': 'KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality', 'url': 'https://huggingface.co/papers/2506.19807', 'abstract': 'KnowRL, a knowledge-enhanced reinforcement learning approach, reduces hallucinations in slow-thinking large language models by incorporating factuality rewards based on knowledge verification during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.', 'score': 5, 'issue_id': 4472, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '2f1016b014bf6ee5', 'authors': ['Baochang Ren', 'Shuofei Qiao', 'Wenhao Yu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Tencent AI Seattle Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19807.jpg', 'data': {'categories': ['#hallucinations', '#rl', '#rlhf', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¤Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜: KnowRL Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'KnowRL - ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾ Ğ¼Ñ‹ÑĞ»ÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. KnowRL Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ°Ñ…, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ KnowRL ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾ Ğ¼Ñ‹ÑĞ»ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Truthfulness in AI with KnowRL', 'desc': "KnowRL is a novel approach that enhances reinforcement learning by incorporating factuality rewards to reduce hallucinations in slow-thinking large language models. These models often generate incorrect information due to their inability to recognize the limits of their knowledge. By integrating knowledge verification into the training process, KnowRL encourages models to engage in fact-based reasoning. Experimental results show that this method effectively decreases hallucinations while preserving the models' reasoning abilities."}, 'zh': {'title': 'çŸ¥è¯†å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ï¼Œå‡å°‘å¹»è§‰ç°è±¡', 'desc': 'KnowRLæ˜¯ä¸€ç§å¢å¼ºçŸ¥è¯†çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥åŸºäºçŸ¥è¯†éªŒè¯çš„äº‹å®å¥–åŠ±ï¼Œå‡å°‘äº†æ…¢æ€ç»´å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰ç°è±¡ã€‚æ…¢æ€ç»´æ¨¡å‹å¸¸å¸¸å› ä¸ºæ— æ³•å‡†ç¡®è¯†åˆ«çŸ¥è¯†è¾¹ç•Œè€Œè¾“å‡ºé”™è¯¯å†…å®¹ï¼Œå¯¼è‡´ä¸¥é‡çš„å¹»è§‰é—®é¢˜ã€‚KnowRLé€šè¿‡å°†äº‹å®å¥–åŠ±æ•´åˆåˆ°å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢æ€ç»´ï¼Œä»è€Œå¸®åŠ©å®ƒä»¬è¯†åˆ«çŸ¥è¯†è¾¹ç•Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKnowRLåœ¨å‡è½»å¹»è§‰çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18945', 'title': 'Chain-of-Experts: Unlocking the Communication Power of\n  Mixture-of-Experts Models', 'url': 'https://huggingface.co/papers/2506.18945', 'abstract': "Chain-of-Experts (CoE) improves performance and memory efficiency in mixture-of-experts models by iteratively routing tokens through experts within each layer.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE) architecture that introduces sequential expert communication within each layer. Unlike traditional MoE models, where experts operate independently in parallel, CoE processes tokens iteratively across a chain of experts inside a layer. To support dynamic expert selection across iterations, CoE employs a dedicated router at each iteration step within a layer. This design allows tokens to re-evaluate and select different experts during each iteration, rather than being statically assigned. As a result, CoE introduces a flexible routing mechanism that increases the diversity of expert combinations and enriches the model's representational capacity. CoE demonstrates improved performance under fixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to 1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling axis: depth through expert iteration, which complements conventional width/depth scaling. For example, using 2x iterations matches the performance of 3x expert selections (in width), while reducing memory usage by 17.6-42% relative to other scaling strategies. Our analysis reveals that CoE's benefits stem from its iterative residual structure and enhanced expert specialization empowered by iterative routing, which together unlock more expressive representations. Code is available at https://github.com/ZihanWang314/coe.", 'score': 5, 'issue_id': 4490, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'a213aae3fd116e93', 'authors': ['Zihan Wang', 'Rui Pan', 'Jiarui Yao', 'Robert Csordas', 'Linjie Li', 'Lu Yin', 'Jiajun Wu', 'Tong Zhang', 'Manling Li', 'Shiwei Liu'], 'affiliations': ['Northwestern University', 'Stanford University', 'University of Illinois Urbana-Champaign', 'University of Oxford', 'University of Surrey', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.18945.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mixture-of-Experts Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Chain-of-Experts (CoE). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… MoE Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, CoE Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ. CoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Iterative Expert Routing for Enhanced Model Efficiency', 'desc': 'Chain-of-Experts (CoE) is a novel Mixture-of-Experts (MoE) architecture that enhances model performance and memory efficiency by allowing tokens to be routed through a sequence of experts within each layer. Unlike traditional MoE models that operate experts in parallel, CoE iteratively processes tokens, enabling dynamic expert selection at each step. This iterative routing mechanism increases the diversity of expert combinations, leading to richer representations and improved performance on tasks like math reasoning. CoE not only reduces validation loss but also introduces a new scaling approach that optimizes depth through expert iteration, significantly lowering memory usage compared to conventional methods.'}, 'zh': {'title': 'é“¾å¼ä¸“å®¶ï¼šæå‡æ¨¡å‹æ€§èƒ½ä¸å†…å­˜æ•ˆç‡çš„åˆ›æ–°æ¶æ„', 'desc': 'Chain-of-Expertsï¼ˆCoEï¼‰æ˜¯ä¸€ç§æ–°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œé€šè¿‡åœ¨æ¯ä¸€å±‚å†…è¿­ä»£åœ°è·¯ç”±ä»¤ç‰Œåˆ°ä¸“å®¶ï¼Œä»è€Œæé«˜æ€§èƒ½å’Œå†…å­˜æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„MoEæ¨¡å‹ä¸åŒï¼ŒCoEåœ¨æ¯ä¸€å±‚å†…çš„ä¸“å®¶ä¹‹é—´è¿›è¡Œé¡ºåºé€šä¿¡ï¼Œä½¿å¾—ä»¤ç‰Œå¯ä»¥åœ¨æ¯æ¬¡è¿­ä»£ä¸­é‡æ–°è¯„ä¼°å’Œé€‰æ‹©ä¸åŒçš„ä¸“å®¶ã€‚è¿™ä¸ªè®¾è®¡å¼•å…¥äº†çµæ´»çš„è·¯ç”±æœºåˆ¶ï¼Œå¢åŠ äº†ä¸“å®¶ç»„åˆçš„å¤šæ ·æ€§ï¼Œä¸°å¯Œäº†æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚CoEåœ¨å›ºå®šè®¡ç®—æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ŒéªŒè¯æŸå¤±ä»1.20é™ä½åˆ°1.12ï¼ŒåŒæ—¶ç›¸è¾ƒäºå…¶ä»–æ‰©å±•ç­–ç•¥ï¼Œå†…å­˜ä½¿ç”¨å‡å°‘äº†17.6-42%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19839', 'title': 'Improving Progressive Generation with Decomposable Flow Matching', 'url': 'https://huggingface.co/papers/2506.19839', 'abstract': 'Decomposable Flow Matching (DFM) framework enhances visual generation and video quality by applying Flow Matching at multiple scales without requiring complex multi-stage architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating high-dimensional visual modalities is a computationally intensive task. A common solution is progressive generation, where the outputs are synthesized in a coarse-to-fine spectral autoregressive manner. While diffusion models benefit from the coarse-to-fine nature of denoising, explicit multi-stage architectures are rarely adopted. These architectures have increased the complexity of the overall approach, introducing the need for a custom diffusion formulation, decomposition-dependent stage transitions, add-hoc samplers, or a model cascade. Our contribution, Decomposable Flow Matching (DFM), is a simple and effective framework for the progressive generation of visual media. DFM applies Flow Matching independently at each level of a user-defined multi-scale representation (such as Laplacian pyramid). As shown by our experiments, our approach improves visual quality for both images and videos, featuring superior results compared to prior multistage frameworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores over the base architecture and 26.4% over the best-performing baseline, under the same training compute. When applied to finetuning of large models, such as FLUX, DFM shows faster convergence speed to the training distribution. Crucially, all these advantages are achieved with a single model, architectural simplicity, and minimal modifications to existing training pipelines.', 'score': 4, 'issue_id': 4484, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': '02798e339aace939', 'authors': ['Moayed Haji-Ali', 'Willi Menapace', 'Ivan Skorokhodov', 'Arpit Sahni', 'Sergey Tulyakov', 'Vicente Ordonez', 'Aliaksandr Siarohin'], 'affiliations': ['Rice University', 'Snap Inc'], 'pdf_title_img': 'assets/pdf/title_img/2506.19839.jpg', 'data': {'categories': ['#video', '#diffusion', '#optimization', '#architecture', '#training', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Decomposable Flow Matching (DFM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. DFM Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Flow Matching Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DFM Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Simplifying Visual Generation with Decomposable Flow Matching', 'desc': 'The Decomposable Flow Matching (DFM) framework simplifies the process of generating high-dimensional visual content by applying Flow Matching at various scales. Unlike traditional methods that rely on complex multi-stage architectures, DFM operates effectively with a single model, enhancing both image and video quality. It utilizes a user-defined multi-scale representation, such as a Laplacian pyramid, to progressively generate visuals, leading to significant improvements in fidelity and convergence speed. Experimental results demonstrate that DFM outperforms existing methods, achieving notable gains in FDD scores while maintaining architectural simplicity.'}, 'zh': {'title': 'ç®€åŒ–è§†è§‰ç”Ÿæˆçš„å¯åˆ†è§£æµåŒ¹é…æ¡†æ¶', 'desc': 'å¯åˆ†è§£æµåŒ¹é…ï¼ˆDFMï¼‰æ¡†æ¶é€šè¿‡åœ¨å¤šä¸ªå°ºåº¦ä¸Šåº”ç”¨æµåŒ¹é…ï¼Œå¢å¼ºäº†è§†è§‰ç”Ÿæˆå’Œè§†é¢‘è´¨é‡ï¼Œè€Œæ— éœ€å¤æ‚çš„å¤šé˜¶æ®µæ¶æ„ã€‚DFMåœ¨ç”¨æˆ·å®šä¹‰çš„å¤šå°ºåº¦è¡¨ç¤ºçš„æ¯ä¸ªå±‚æ¬¡ä¸Šç‹¬ç«‹åº”ç”¨æµåŒ¹é…ï¼Œä»è€Œç®€åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDFMåœ¨å›¾åƒå’Œè§†é¢‘çš„è§†è§‰è´¨é‡ä¸Šå‡ä¼˜äºä»¥å¾€çš„å¤šé˜¶æ®µæ¡†æ¶ï¼Œå°¤å…¶åœ¨Imagenet-1k 512pxæ•°æ®é›†ä¸Šï¼ŒFDDå¾—åˆ†æé«˜äº†35.2%ã€‚æ­¤å¤–ï¼ŒDFMåœ¨å¾®è°ƒå¤§å‹æ¨¡å‹æ—¶æ˜¾ç¤ºå‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œæ‰€æœ‰è¿™äº›ä¼˜åŠ¿éƒ½åœ¨å•ä¸€æ¨¡å‹å’Œç®€å•æ¶æ„ä¸‹å®ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16095', 'title': 'Intelligent Operation and Maintenance and Prediction Model Optimization\n  for Improving Wind Power Generation Efficiency', 'url': 'https://huggingface.co/papers/2506.16095', 'abstract': 'This study explores the effectiveness of predictive maintenance models and the optimization of intelligent Operation and Maintenance (O&M) systems in improving wind power generation efficiency. Through qualitative research, structured interviews were conducted with five wind farm engineers and maintenance managers, each with extensive experience in turbine operations. Using thematic analysis, the study revealed that while predictive maintenance models effectively reduce downtime by identifying major faults, they often struggle with detecting smaller, gradual failures. Key challenges identified include false positives, sensor malfunctions, and difficulties in integrating new models with older turbine systems. Advanced technologies such as digital twins, SCADA systems, and condition monitoring have significantly enhanced turbine maintenance practices. However, these technologies still require improvements, particularly in AI refinement and real-time data integration. The findings emphasize the need for continuous development to fully optimize wind turbine performance and support the broader adoption of renewable energy.', 'score': 4, 'issue_id': 4479, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': 'b6bac9ae1a6539ff', 'authors': ['Xun Liu', 'Xiaobin Wu', 'Jiaqi He', 'Rajan Das Gupta'], 'affiliations': ['Department of Computer Science and Engineering Jahangirnagar University Dhaka, Bangladesh', 'Faculty of Education Shinawatra University Bangkok, Thailand', 'School of Automotive Engineering Chengdu Industry And Trade College Chengdu, China', 'Tilburg School of Social and Behavioral Sciences Tilburg University Tilburg, The Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2506.16095.jpg', 'data': {'categories': ['#data', '#healthcare', '#training', '#optimization', '#science'], 'emoji': 'ğŸŒ¬ï¸', 'ru': {'title': 'ĞŸÑ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµÑ‚Ñ€ÑĞ½Ñ‹Ñ… Ñ‚ÑƒÑ€Ğ±Ğ¸Ğ½: Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²ĞµÑ‚Ñ€Ğ¾ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸ĞºĞµ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ²ĞµÑ‚Ñ€ÑĞ½Ñ‹Ñ… ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ½ĞµĞ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¾ Ğ¼ĞµĞ½ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½ĞµĞ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ ÑÑ‚Ğ°Ñ€Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ñ‚ÑƒÑ€Ğ±Ğ¸Ğ½. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ‚Ñ€ÑĞ½Ñ‹Ñ… Ñ‚ÑƒÑ€Ğ±Ğ¸Ğ½.'}, 'en': {'title': 'Optimizing Wind Power with Predictive Maintenance and Advanced Technologies', 'desc': 'This paper investigates how predictive maintenance models can enhance the efficiency of wind power generation. It highlights the importance of intelligent Operation and Maintenance (O&M) systems in minimizing downtime by identifying major faults in turbines. However, the study points out that these models often fail to detect smaller, gradual failures and face challenges like false positives and sensor issues. The research suggests that while advanced technologies like digital twins and SCADA systems improve maintenance, further advancements in AI and real-time data integration are necessary for optimal turbine performance.'}, 'zh': {'title': 'ä¼˜åŒ–é£ç”µç»´æŠ¤ï¼Œæå‡å¯å†ç”Ÿèƒ½æºæ•ˆç‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†é¢„æµ‹æ€§ç»´æŠ¤æ¨¡å‹åœ¨æé«˜é£åŠ›å‘ç”µæ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠæ™ºèƒ½è¿ç»´ç³»ç»Ÿçš„ä¼˜åŒ–ã€‚é€šè¿‡å¯¹äº”ä½é£ç”µåœºå·¥ç¨‹å¸ˆå’Œç»´æŠ¤ç»ç†çš„ç»“æ„åŒ–è®¿è°ˆï¼Œç ”ç©¶å‘ç°é¢„æµ‹æ€§ç»´æŠ¤æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘åœæœºæ—¶é—´ï¼Œä½†åœ¨æ£€æµ‹å°å‹æ¸è¿›æ€§æ•…éšœæ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºäº†å‡é˜³æ€§ã€ä¼ æ„Ÿå™¨æ•…éšœå’Œæ–°æ¨¡å‹ä¸æ—§æ¶¡è½®ç³»ç»Ÿé›†æˆçš„æŒ‘æˆ˜ã€‚å°½ç®¡æ•°å­—åŒèƒèƒã€SCADAç³»ç»Ÿå’ŒçŠ¶æ€ç›‘æµ‹ç­‰å…ˆè¿›æŠ€æœ¯æ˜¾è‘—æå‡äº†æ¶¡è½®ç»´æŠ¤å®è·µï¼Œä½†åœ¨äººå·¥æ™ºèƒ½ä¼˜åŒ–å’Œå®æ—¶æ•°æ®é›†æˆæ–¹é¢ä»éœ€æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19433', 'title': 'Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System', 'url': 'https://huggingface.co/papers/2506.19433', 'abstract': 'Mem4Nav enhances Vision-and-Language Navigation by integrating a hierarchical spatial-cognition system with dual-memory modules using a reversible Transformer for improved task completion, speed, and detour detection.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce Mem4Nav, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.', 'score': 2, 'issue_id': 4479, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': 'b905012a9184cad4', 'authors': ['Lixuan He', 'Haoyu Dong', 'Zhenxing Chen', 'Yangcheng Yu', 'Jie Feng', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.19433.jpg', 'data': {'categories': ['#long_context', '#games', '#agents', '#multimodal', '#open_source', '#architecture', '#training'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Mem4Nav: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Mem4Nav - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Vision-and-Language Navigation, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ·Ğ´Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾ĞºÑ‚Ğ¾Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ². Mem4Nav Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Navigation with Smart Memory Systems', 'desc': 'Mem4Nav is a novel approach to Vision-and-Language Navigation (VLN) that enhances task performance by integrating a hierarchical spatial-cognition system with dual-memory modules. It utilizes a reversible Transformer to manage long-term and short-term memory, allowing agents to effectively recall and utilize past experiences in complex environments. The system combines a sparse octree for detailed spatial indexing with a semantic topology graph for understanding landmark connections, improving navigation efficiency and obstacle avoidance. Evaluations show significant improvements in task completion rates and navigation accuracy across various VLN models, demonstrating the effectiveness of its memory architecture.'}, 'zh': {'title': 'Mem4Navï¼šæå‡è§†è§‰ä¸è¯­è¨€å¯¼èˆªçš„æ™ºèƒ½è®°å¿†ç³»ç»Ÿ', 'desc': 'Mem4Navæ˜¯ä¸€ç§å¢å¼ºè§†è§‰ä¸è¯­è¨€å¯¼èˆªçš„ç³»ç»Ÿï¼Œå®ƒç»“åˆäº†åˆ†å±‚ç©ºé—´è®¤çŸ¥ç³»ç»Ÿå’ŒåŒé‡è®°å¿†æ¨¡å—ï¼Œä½¿ç”¨å¯é€†å˜æ¢å™¨æ¥æé«˜ä»»åŠ¡å®Œæˆç‡ã€é€Ÿåº¦å’Œç»•è¡Œæ£€æµ‹èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨å¤æ‚çš„åŸå¸‚ç¯å¢ƒä¸­å¸®åŠ©ä»£ç†äººç†è§£è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶åœ¨è¾ƒé•¿æ—¶é—´å†…å›å¿†ç›¸å…³ç»éªŒã€‚ä¸ä¼ ç»Ÿçš„æ¨¡å—åŒ–ç®¡é“ç›¸æ¯”ï¼ŒMem4Navæä¾›äº†ç»Ÿä¸€çš„è®°å¿†ç»“æ„ï¼Œå…‹æœäº†å›ºå®šä¸Šä¸‹æ–‡çª—å£å’Œéšå¼ç©ºé—´æ¨ç†çš„é™åˆ¶ã€‚é€šè¿‡é•¿çŸ­æœŸè®°å¿†çš„ç»“åˆï¼ŒMem4Navåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ä»»åŠ¡å®Œæˆç‡å’Œå®æ—¶è§„åˆ’èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19847', 'title': 'Orthogonal Finetuning Made Scalable', 'url': 'https://huggingface.co/papers/2506.19847', 'abstract': 'OFTv2 optimizes orthogonal fine-tuning by shifting from matrix-matrix to matrix-vector multiplications and introducing efficient Cayley-Neumann parameterization, enhancing speed, memory usage, and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation while preventing catastrophic forgetting, but its high runtime and memory demands limit practical deployment. We identify the core computational bottleneck in OFT as its weight-centric implementation, which relies on costly matrix-matrix multiplications with cubic complexity. To overcome this, we propose OFTv2, an input-centric reformulation that instead uses matrix-vector multiplications (i.e., matrix-free computation), reducing the computational cost to quadratic. We further introduce the Cayley-Neumann parameterization, an efficient orthogonal parameterization that approximates the matrix inversion in Cayley transform via a truncated Neumann series. These modifications allow OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage without compromising performance. In addition, we extend OFTv2 to support finetuning quantized foundation models and show that it outperforms the popular QLoRA in training stability, efficiency, and memory usage.', 'score': 1, 'issue_id': 4483, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': 'c6e0936200cd15de', 'authors': ['Zeju Qiu', 'Weiyang Liu', 'Adrian Weller', 'Bernhard SchÃ¶lkopf'], 'affiliations': ['Max Planck Institute for Intelligent Systems', 'The Alan Turing Institute', 'The Chinese University of Hong Kong', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2506.19847.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': 'ğŸš€', 'ru': {'title': 'OFTv2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'OFTv2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (OFT), Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾-Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğº Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾-Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞšÑĞ¹Ğ»Ğ¸-ĞĞµĞ¹Ğ¼Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. OFTv2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'OFTv2: Speeding Up Orthogonal Fine-Tuning with Matrix-Vector Magic!', 'desc': 'OFTv2 is an advanced method for orthogonal fine-tuning that improves the efficiency of adapting machine learning models. It shifts from using matrix-matrix multiplications, which are computationally expensive, to matrix-vector multiplications, significantly reducing the time and memory required for training. The introduction of the Cayley-Neumann parameterization allows for a more efficient way to handle orthogonal parameters, further enhancing performance. Overall, OFTv2 achieves faster training speeds and lower memory usage while maintaining high performance, making it suitable for fine-tuning quantized models.'}, 'zh': {'title': 'OFTv2ï¼šé«˜æ•ˆçš„æ­£äº¤å¾®è°ƒæ–°æ–¹æ³•', 'desc': 'OFTv2é€šè¿‡å°†çŸ©é˜µ-çŸ©é˜µä¹˜æ³•è½¬å˜ä¸ºçŸ©é˜µ-å‘é‡ä¹˜æ³•ï¼Œä¼˜åŒ–äº†æ­£äº¤å¾®è°ƒçš„è¿‡ç¨‹ï¼Œä»è€Œæé«˜äº†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ•ˆç‡ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†é«˜æ•ˆçš„Cayley-Neumannå‚æ•°åŒ–ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚OFTv2çš„è®¡ç®—å¤æ‚åº¦ä»ç«‹æ–¹çº§åˆ«é™ä½åˆ°å¹³æ–¹çº§åˆ«ï¼Œä½¿å¾—è®­ç»ƒé€Ÿåº¦æé«˜äº†10å€ï¼ŒGPUå†…å­˜ä½¿ç”¨é™ä½äº†3å€ã€‚è¯¥æ–¹æ³•è¿˜æ‰©å±•äº†å¯¹é‡åŒ–åŸºç¡€æ¨¡å‹çš„å¾®è°ƒæ”¯æŒï¼Œæ˜¾ç¤ºå‡ºåœ¨è®­ç»ƒç¨³å®šæ€§ã€æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨æ–¹é¢ä¼˜äºæµè¡Œçš„QLoRAã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18882', 'title': 'Light of Normals: Unified Feature Representation for Universal\n  Photometric Stereo', 'url': 'https://huggingface.co/papers/2506.18882', 'abstract': '', 'score': 68, 'issue_id': 4450, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '39545287159810c0', 'authors': ['Hong Li', 'Houyuan Chen', 'Chongjie Ye', 'Zhaoxi Chen', 'Bohan Li', 'Shaocong Xu', 'Xianda Guo', 'Xuhui Liu', 'Yikai Wang', 'Baochang Zhang', 'Satoshi Ikehata', 'Boxin Shi', 'Anyi Rao', 'Hao Zhao'], 'affiliations': ['AIR, THU', 'BAAI', 'BNU', 'BUAA', 'FNii, CUHKSZ', 'HKUST', 'NII', 'NJU', 'PKU'], 'pdf_title_img': 'assets/pdf/title_img/2506.18882.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Hybrid Deep Learning: Merging CNNs and RNNs for Superior Performance', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': 'æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åˆ›æ–°ç®—æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ•°æ®ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç ”ç©¶è€…å¸Œæœ›æ¨åŠ¨æœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18871', 'title': 'OmniGen2: Exploration to Advanced Multimodal Generation', 'url': 'https://huggingface.co/papers/2506.18871', 'abstract': 'OmniGen2, a versatile generative model, introduces dual decoding pathways for text and images, preserves original text generation, and achieves competitive results with a new subject-driven benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2', 'score': 47, 'issue_id': 4447, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '18382718ba53ccf7', 'authors': ['Chenyuan Wu', 'Pengfei Zheng', 'Ruiran Yan', 'Shitao Xiao', 'Xin Luo', 'Yueze Wang', 'Wanli Li', 'Xiyan Jiang', 'Yexin Liu', 'Junjie Zhou', 'Ze Liu', 'Ziyi Xia', 'Chaofan Li', 'Haoge Deng', 'Jiahao Wang', 'Kun Luo', 'Bo Zhang', 'Defu Lian', 'Xinlong Wang', 'Zhongyuan Wang', 'Tiejun Huang', 'Zheng Liu'], 'affiliations': ['Beijing Academy of Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2506.18871.jpg', 'data': {'categories': ['#training', '#open_source', '#data', '#dataset', '#multimodal', '#benchmark', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'OmniGen2: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'OmniGen2 - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿ÑƒÑ‚Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², OmniGen2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº OmniContext Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ.'}, 'en': {'title': 'OmniGen2: Unifying Text and Image Generation with Dual Pathways', 'desc': 'OmniGen2 is a generative model that enhances the creation of text and images through dual decoding pathways, allowing for specialized processing of each modality. It maintains the original text generation capabilities while introducing a new image tokenizer and reflection mechanism for improved image tasks. The model is trained using comprehensive data pipelines that support various generation tasks, including image editing and in-context generation. Despite its smaller size, OmniGen2 achieves competitive performance on benchmarks, particularly in subject-driven tasks, and aims to advance research in multimodal generation.'}, 'zh': {'title': 'OmniGen2ï¼šå¤šæ¨¡æ€ç”Ÿæˆçš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ', 'desc': 'OmniGen2æ˜¯ä¸€ç§å¤šåŠŸèƒ½çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºæ–‡æœ¬å’Œå›¾åƒç”Ÿæˆä»»åŠ¡æä¾›ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚ä¸OmniGen v1ä¸åŒï¼ŒOmniGen2é‡‡ç”¨äº†ä¸¤ä¸ªç‹¬ç«‹çš„è§£ç è·¯å¾„ï¼Œåˆ†åˆ«å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼Œä½¿ç”¨äº†ä¸å…±äº«çš„å‚æ•°å’Œè§£è€¦çš„å›¾åƒæ ‡è®°å™¨ã€‚è¿™ç§è®¾è®¡ä½¿å¾—OmniGen2èƒ½å¤Ÿåœ¨ä¸é‡æ–°é€‚é…VAEè¾“å…¥çš„æƒ…å†µä¸‹ï¼Œä¿ç•™åŸæœ‰çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚å°½ç®¡å‚æ•°é‡ç›¸å¯¹è¾ƒå°ï¼ŒOmniGen2åœ¨å¤šä¸ªä»»åŠ¡åŸºå‡†ä¸Šå–å¾—äº†ç«äº‰åŠ›çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘æ–¹é¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18841', 'title': 'LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2506.18841', 'abstract': "An incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B", 'score': 38, 'issue_id': 4447, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '8589c05aae4b8258', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqiang Hu', 'Roy Ka-Wei Lee', 'Juanzi Li'], 'affiliations': ['Singapore University of Technology and Design, Singapore', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.18841.jpg', 'data': {'categories': ['#training', '#long_context', '#rl', '#open_source', '#dataset', '#benchmark'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: RL Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¸ÑÑŒĞ¼Ğ° Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LongWriter-Zero Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WritingBench Ğ¸ Arena-Write.'}, 'en': {'title': 'Reinforcement Learning for Ultra-Long Text Generation Without Synthetic Data', 'desc': 'This paper presents a novel approach to generating ultra-long, high-quality text using a large language model (LLM) without relying on synthetic data or supervised fine-tuning. The authors introduce an incentivization-based reinforcement learning (RL) method that allows the model to learn from scratch, enhancing its ability to produce coherent and structured long-form content. By employing specialized reward models, the LLM is guided to improve its writing quality, length control, and formatting during the generation process. Experimental results demonstrate that the proposed LongWriter-Zero model outperforms traditional methods, achieving state-of-the-art performance on long-form writing benchmarks.'}, 'zh': {'title': 'æ¿€åŠ±å¼ºåŒ–å­¦ä¹ ï¼Œè¶…é•¿æ–‡æœ¬ç”Ÿæˆæ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¿€åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆè¶…é•¿ä¸”é«˜è´¨é‡çš„æ–‡æœ¬ï¼Œè€Œæ— éœ€åˆæˆæ•°æ®æˆ–ç›‘ç£å¾®è°ƒã€‚ä»¥å¾€çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™éœ€è¦æ„å»ºåˆæˆçš„é•¿æ–‡æœ¬è¾“å‡ºï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥å®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»é›¶å¼€å§‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ¨¡å‹ï¼Œä¿ƒè¿›è¶…é•¿é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›çš„å‡ºç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LongWriter-Zeroæ¨¡å‹åœ¨é•¿æ–‡æœ¬å†™ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„SFTæ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18851', 'title': 'Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset', 'url': 'https://huggingface.co/papers/2506.18851', 'abstract': 'A cross-pair dataset called Phantom-Data improves subject-to-video generation by enhancing prompt alignment and visual quality while maintaining identity consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines.', 'score': 25, 'issue_id': 4447, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '525a4c676b83f9a6', 'authors': ['Zhuowei Chen', 'Bingchuan Li', 'Tianxiang Ma', 'Lijie Liu', 'Mingcong Liu', 'Yi Zhang', 'Gen Li', 'Xinghui Li', 'Siyu Zhou', 'Qian He', 'Xinglong Wu'], 'affiliations': ['Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2506.18851.jpg', 'data': {'categories': ['#data', '#dataset', '#synthetic'], 'emoji': 'ğŸ­', 'ru': {'title': 'Phantom-Data: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ñƒ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Phantom-Data Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ñƒ. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°, Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Phantom-Data Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Phantom-Data: Enhancing Video Generation with Identity Consistency', 'desc': "The paper introduces Phantom-Data, a new dataset designed to enhance subject-to-video generation in machine learning. This dataset addresses the 'copy-paste problem' by providing identity-consistent pairs that are not tied to specific backgrounds or contexts. It is created through a three-stage process that includes subject detection, cross-context retrieval, and identity verification. Experiments demonstrate that using Phantom-Data leads to better alignment with prompts and improved visual quality while maintaining consistent subject identity."}, 'zh': {'title': 'Phantom-Dataï¼šæå‡è§†é¢‘ç”Ÿæˆçš„èº«ä»½ä¸€è‡´æ€§ä¸è§†è§‰è´¨é‡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhantom-Dataçš„è·¨å¯¹æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºæ–‡æœ¬ç”Ÿæˆè§†é¢‘çš„æ•ˆæœã€‚è¯¥æ•°æ®é›†é€šè¿‡å¢å¼ºæç¤ºå¯¹é½å’Œè§†è§‰è´¨é‡ï¼ŒåŒæ—¶ä¿æŒèº«ä»½ä¸€è‡´æ€§ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨éµå¾ªæ–‡æœ¬æŒ‡ä»¤æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚Phantom-DataåŒ…å«çº¦ä¸€ç™¾ä¸‡ä¸ªèº«ä»½ä¸€è‡´çš„é…å¯¹ï¼Œæ¶µç›–å¤šç§ç±»åˆ«ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µæµç¨‹æ„å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Phantom-Dataè¿›è¡Œè®­ç»ƒæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18792', 'title': 'ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs', 'url': 'https://huggingface.co/papers/2506.18792', 'abstract': "ViDAR uses diffusion-aware reconstruction to generate high-quality novel views of dynamic scenes from monocular video, outperforming existing methods in visual quality and geometric consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Dynamic Novel View Synthesis aims to generate photorealistic views of moving subjects from arbitrary viewpoints. This task is particularly challenging when relying on monocular video, where disentangling structure from motion is ill-posed and supervision is scarce. We introduce Video Diffusion-Aware Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages personalised diffusion models to synthesise a pseudo multi-view supervision signal for training a Gaussian splatting representation. By conditioning on scene-specific features, ViDAR recovers fine-grained appearance details while mitigating artefacts introduced by monocular ambiguity. To address the spatio-temporal inconsistency of diffusion-based supervision, we propose a diffusion-aware loss function and a camera pose optimisation strategy that aligns synthetic views with the underlying scene geometry. Experiments on DyCheck, a challenging benchmark with extreme viewpoint variation, show that ViDAR outperforms all state-of-the-art baselines in visual quality and geometric consistency. We further highlight ViDAR's strong improvement over baselines on dynamic regions and provide a new benchmark to compare performance in reconstructing motion-rich parts of the scene. Project page: https://vidar-4d.github.io", 'score': 25, 'issue_id': 4455, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'f8043a5d01c5fa35', 'authors': ['Michal Nazarczuk', 'Sibi Catley-Chandar', 'Thomas Tanay', 'Zhensong Zhang', 'Gregory Slabaugh', 'Eduardo PÃ©rez-Pellitero'], 'affiliations': ['Huawei Noahs Ark Lab', 'Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2506.18792.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#benchmark', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ViDAR: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½', 'desc': 'ViDAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ². ViDAR Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ViDAR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹.'}, 'en': {'title': 'Revolutionizing View Synthesis with ViDAR!', 'desc': 'ViDAR is a new method for creating high-quality images of moving scenes from single video feeds. It uses a technique called diffusion-aware reconstruction to improve the visual quality and consistency of the generated views. By employing personalized diffusion models, ViDAR generates a training signal that helps in accurately capturing the details of the scene while reducing errors caused by motion ambiguity. The method also includes a special loss function and camera pose optimization to ensure that the generated views align well with the actual scene geometry, leading to better results in dynamic environments.'}, 'zh': {'title': 'ViDARï¼šåŠ¨æ€åœºæ™¯çš„æ–°è§†è§’é‡å»º', 'desc': 'ViDARæ˜¯ä¸€ç§æ–°é¢–çš„4Dé‡å»ºæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ„ŸçŸ¥é‡å»ºæŠ€æœ¯ï¼Œä»å•ç›®è§†é¢‘ä¸­ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨æ€åœºæ™¯æ–°è§†è§’ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ªæ€§åŒ–çš„æ‰©æ•£æ¨¡å‹åˆæˆä¼ªå¤šè§†è§’ç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒé«˜æ–¯ç‚¹äº‘è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆåˆ†ç¦»ç»“æ„ä¸è¿åŠ¨ã€‚ViDARé€šè¿‡åœºæ™¯ç‰¹å®šç‰¹å¾æ¢å¤ç»†è‡´çš„å¤–è§‚ç»†èŠ‚ï¼Œå¹¶å‡å°‘å•ç›®æ¨¡ç³Šå¸¦æ¥çš„ä¼ªå½±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViDARåœ¨è§†è§‰è´¨é‡å’Œå‡ ä½•ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨åŠ¨æ€åŒºåŸŸè¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18896', 'title': 'ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2506.18896', 'abstract': 'ReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux', 'score': 23, 'issue_id': 4447, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '6a30d79f40f7d98d', 'authors': ['Jiaru Zou', 'Ling Yang', 'Jingwen Gu', 'Jiahao Qiu', 'Ke Shen', 'Jingrui He', 'Mengdi Wang'], 'affiliations': ['ByteDance', 'Cornell University', 'Princeton University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2506.18896.jpg', 'data': {'categories': ['#training', '#small_models', '#optimization', '#rl', '#dataset', '#benchmark', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'ReasonFlux-PRM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ†ĞµĞ»Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ReasonFlux-PRM Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ReasonFlux-PRM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing Reasoning Evaluation with ReasonFlux-PRM', 'desc': 'ReasonFlux-PRM is a new model that improves how we evaluate reasoning processes in large language models by focusing on both individual steps and overall trajectories. It addresses the limitations of previous Process Reward Models that mainly assessed final outputs, which often missed the nuances of intermediate reasoning. By using both step-level and trajectory-level supervision, it provides more accurate rewards that align with structured reasoning data. The model has shown significant performance improvements in tasks like model distillation, reinforcement learning, and test-time scaling, outperforming existing models and human-curated benchmarks.'}, 'zh': {'title': 'æ¨ç†è½¨è¿¹çš„æ™ºèƒ½è¯„ä¼°', 'desc': 'ReasonFlux-PRMæ˜¯ä¸€ç§æ–°é¢–çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä¸“æ³¨äºè¯„ä¼°æ¨ç†è½¨è¿¹ï¼Œç»“åˆäº†é€æ­¥å’Œè½¨è¿¹çº§çš„ç›‘ç£ã€‚è¿™ç§æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨¡å‹è’¸é¦ã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶æ‰©å±•ä¸­æå‡æ€§èƒ½ã€‚é€šè¿‡å¯¹æ¨ç†è¿‡ç¨‹çš„ç»†è‡´å¥–åŠ±åˆ†é…ï¼ŒReasonFlux-PRMèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18254', 'title': 'RLPR: Extrapolating RLVR to General Domains without Verifiers', 'url': 'https://huggingface.co/papers/2506.18254', 'abstract': "RLPR, a verifier-free framework using LLM's token probability scores as reward signals, enhances reasoning capabilities across both general and mathematical domains, outperforming other methods in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising potential in advancing the reasoning capabilities of LLMs. However, its success remains largely confined to mathematical and code domains. This primary limitation stems from the heavy reliance on domain-specific verifiers, which results in prohibitive complexity and limited scalability. To address the challenge, our key observation is that LLM's intrinsic probability of generating a correct free-form answer directly indicates its own evaluation of the reasoning reward (i.e., how well the reasoning process leads to the correct answer). Building on this insight, we propose RLPR, a simple verifier-free framework that extrapolates RLVR to broader general domains. RLPR uses the LLM's own token probability scores for reference answers as the reward signal and maximizes the expected reward during training. We find that addressing the high variance of this noisy probability reward is crucial to make it work, and propose prob-to-reward and stabilizing methods to ensure a precise and stable reward from LLM intrinsic probabilities. Comprehensive experiments in four general-domain benchmarks and three mathematical benchmarks show that RLPR consistently improves reasoning capabilities in both areas for Gemma, Llama, and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6 points on TheoremQA and 7.5 points on Minerva, and even surpasses strong verifier-model-dependent approaches General-Reasoner by 1.6 average points across seven benchmarks.", 'score': 23, 'issue_id': 4451, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'f96aa2817f2af792', 'authors': ['Tianyu Yu', 'Bo Ji', 'Shouli Wang', 'Shu Yao', 'Zefan Wang', 'Ganqu Cui', 'Lifan Yuan', 'Ning Ding', 'Yuan Yao', 'Zhiyuan Liu', 'Maosong Sun', 'Tat-Seng Chua'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Harbin Institute of Technology', 'National University of Singapore', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.18254.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RLPR: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'RLPR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. RLPR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Reinforcement Learning Without Verifiers: Unlocking LLM Reasoning', 'desc': 'The paper introduces RLPR, a new framework that enhances the reasoning abilities of large language models (LLMs) without needing external verifiers. It leverages the token probability scores from LLMs as reward signals, allowing for a more scalable and efficient training process. By addressing the high variance in these probability rewards, the authors implement methods to stabilize the reward signal, leading to improved performance in both general and mathematical reasoning tasks. Experimental results show that RLPR significantly outperforms existing methods, demonstrating its effectiveness across various benchmarks.'}, 'zh': {'title': 'RLPRï¼šæ— éœ€éªŒè¯å™¨çš„æ¨ç†èƒ½åŠ›æå‡æ¡†æ¶', 'desc': 'RLPRæ˜¯ä¸€ç§æ— éœ€éªŒè¯å™¨çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„æ ‡è®°æ¦‚ç‡åˆ†æ•°ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¢å¼ºäº†å…¶åœ¨ä¸€èˆ¬å’Œæ•°å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç‰¹å®šé¢†åŸŸéªŒè¯å™¨ä¾èµ–æ€§å¸¦æ¥çš„å¤æ‚æ€§å’Œå¯æ‰©å±•æ€§é™åˆ¶ã€‚é€šè¿‡ä½¿ç”¨LLMè‡ªèº«çš„æ ‡è®°æ¦‚ç‡åˆ†æ•°ä½œä¸ºå‚è€ƒç­”æ¡ˆçš„å¥–åŠ±ä¿¡å·ï¼ŒRLPRåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLPRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ï¼Œè¶…è¶Šäº†å…¶ä»–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15741', 'title': 'OAgents: An Empirical Study of Building Effective Agents', 'url': 'https://huggingface.co/papers/2506.15741', 'abstract': 'Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI.', 'score': 23, 'issue_id': 4452, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'e2b3d701474597cc', 'authors': ['He Zhu', 'Tianrui Qin', 'King Zhu', 'Heyuan Huang', 'Yeyi Guan', 'Jinxiang Xia', 'Yi Yao', 'Hanhao Li', 'Ningning Wang', 'Pai Liu', 'Tianhao Peng', 'Xin Gui', 'Xiaowan Li', 'Yuhui Liu', 'Yuchen Eleanor Jiang', 'Jun Wang', 'Changwang Zhang', 'Xiangru Tang', 'Ge Zhang', 'Jian Yang', 'Minghao Liu', 'Xitong Gao', 'Jiaheng Liu', 'Wangchunshu Zhou'], 'affiliations': ['Nanjing University', 'OPPO'], 'pdf_title_img': 'assets/pdf/title_img/2506.15741.jpg', 'data': {'categories': ['#agents', '#benchmark', '#open_source', '#agi'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GAIA Ğ¸ BrowseComp Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ². ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº OAgents, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Standardizing Agentic AI for Better Comparisons and Performance', 'desc': 'This paper addresses the challenges in the field of Agentic AI, particularly the lack of standardization and scientific rigor in current research practices. The authors conduct a systematic empirical study using the GAIA benchmark and BrowseComp to evaluate the impact of design choices on agent effectiveness. They highlight that previous evaluation methods are non-reproducible and exhibit significant variance, which complicates fair comparisons. To improve this, they propose a robust evaluation protocol and introduce OAgents, a new modular agent framework that enhances performance and supports future research in the field.'}, 'zh': {'title': 'æ ‡å‡†åŒ–è¯„ä¼°ï¼Œæå‡Agentic AIçš„æœ‰æ•ˆæ€§', 'desc': 'æœ€è¿‘ï¼ŒAgentic AIæˆä¸ºä¸€ä¸ªè¶Šæ¥è¶Šå—æ¬¢è¿çš„ç ”ç©¶é¢†åŸŸã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºå½“å‰çš„ä»£ç†ç ”ç©¶å®è·µç¼ºä¹æ ‡å‡†åŒ–å’Œç§‘å­¦ä¸¥è°¨æ€§ï¼Œè¿™ä½¿å¾—ä¸åŒæ–¹æ³•ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒå˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œå°šä¸æ¸…æ¥šä»£ç†æ¡†æ¶ä¸­çš„ä¸åŒè®¾è®¡é€‰æ‹©å¦‚ä½•å½±å“å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”æµ‹é‡å…¶è¿›å±•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡ç³»ç»Ÿçš„å®è¯ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§æ›´ç¨³å¥çš„è¯„ä¼°åè®®ï¼Œä»¥ç¨³å®šæ¯”è¾ƒï¼Œå¹¶æ­ç¤ºäº†å“ªäº›ç»„ä»¶å’Œè®¾è®¡å¯¹æœ‰æ•ˆä»£ç†è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18898', 'title': 'Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations', 'url': 'https://huggingface.co/papers/2506.18898', 'abstract': "A multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com", 'score': 21, 'issue_id': 4447, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '988f76bd08498ba9', 'authors': ['Jiaming Han', 'Hao Chen', 'Yang Zhao', 'Hanyu Wang', 'Qi Zhao', 'Ziyan Yang', 'Hao He', 'Xiangyu Yue', 'Lu Jiang'], 'affiliations': ['ByteDance Seed', 'CUHK MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2506.18898.jpg', 'data': {'categories': ['#training', '#cv', '#multimodal', '#games', '#benchmark', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ»ĞµĞ¶Ğ¸Ñ‚ Text-Aligned Tokenizer (TA-Tok), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ ĞºĞ½Ğ¸Ğ³Ñƒ, ÑĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸Ğ· ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´ĞµÑ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Tar ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unifying Vision and Text with TA-Tok for Enhanced Multimodal Learning', 'desc': "This paper introduces a multimodal framework that combines visual and textual data into a single representation using a Text-Aligned Tokenizer (TA-Tok). The TA-Tok transforms images into discrete tokens aligned with a large language model's vocabulary, allowing for seamless interaction between text and images. The framework employs a generative de-tokenizer that includes both autoregressive and diffusion-based models to generate high-quality visual outputs efficiently. Experimental results indicate that this approach not only enhances visual understanding and generation but also outperforms existing multimodal models in terms of training speed and efficiency."}, 'zh': {'title': 'ç»Ÿä¸€è§†è§‰ä¸æ–‡æœ¬çš„å¤šæ¨¡æ€æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†è§‰ç†è§£å’Œç”Ÿæˆç»Ÿä¸€åˆ°ä¸€ä¸ªå…±äº«çš„ç¦»æ•£è¯­ä¹‰è¡¨ç¤ºä¸­ã€‚æ ¸å¿ƒæ˜¯æ–‡æœ¬å¯¹é½çš„æ ‡è®°å™¨ï¼ˆTA-Tokï¼‰ï¼Œå®ƒä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯æ±‡å°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£æ ‡è®°ã€‚é€šè¿‡æ‰©å±•è¯æ±‡ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹Tarå®ç°äº†è·¨æ¨¡æ€è¾“å…¥å’Œè¾“å‡ºï¼Œé¿å…äº†ç‰¹å®šæ¨¡æ€è®¾è®¡çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†è§„æ¨¡è‡ªé€‚åº”çš„ç¼–ç å’Œè§£ç æ–¹æ³•ï¼Œä»¥å¹³è¡¡æ•ˆç‡å’Œè§†è§‰ç»†èŠ‚ï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆæ€§å»æ ‡è®°å™¨ç”Ÿæˆé«˜ä¿çœŸè§†è§‰è¾“å‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18903', 'title': 'VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory', 'url': 'https://huggingface.co/papers/2506.18903', 'abstract': 'A novel memory mechanism called Surfel-Indexed View Memory enhances video generation by efficiently remembering and retrieving relevant past views, improving long-term scene coherence and reducing computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel memory mechanism to build video generators that can explore environments interactively. Similar results have previously been achieved by out-painting 2D views of the scene while incrementally reconstructing its 3D geometry, which quickly accumulates errors, or by video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables the efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost of using all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control.', 'score': 13, 'issue_id': 4448, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'fe5d31c2b125d778', 'authors': ['Runjia Li', 'Philip Torr', 'Andrea Vedaldi', 'Tomas Jakab'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.18903.jpg', 'data': {'categories': ['#long_context', '#video', '#benchmark', '#optimization', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Surfel-Indexed View Memory (VMem) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. VMem Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ğµ Ğ²Ğ¸Ğ´Ñ‹ ÑÑ†ĞµĞ½Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (ÑÑƒÑ€Ñ„ĞµĞ»ĞµĞ¹). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑÑ†ĞµĞ½ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Video Generation with Efficient Memory Retrieval', 'desc': 'This paper introduces a new memory mechanism called Surfel-Indexed View Memory (VMem) that improves video generation by efficiently recalling relevant past views. Unlike traditional methods that either accumulate errors or have limited context, VMem uses geometric indexing based on 3D surface elements to enhance long-term scene coherence. By focusing on the most pertinent past views, it reduces computational costs while generating consistent and coherent video outputs. The approach is evaluated against challenging benchmarks, showing better performance in maintaining scene integrity and camera control compared to existing techniques.'}, 'zh': {'title': 'é«˜æ•ˆè®°å¿†ï¼Œæå‡è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®°å¿†æœºåˆ¶ï¼Œç§°ä¸ºè¡¨é¢ç´¢å¼•è§†å›¾è®°å¿†ï¼ˆSurfel-Indexed View Memoryï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆçš„æ•ˆæœã€‚è¯¥æœºåˆ¶é€šè¿‡å‡ ä½•ç´¢å¼•è¿‡å»çš„è§†å›¾ï¼ŒåŸºäºè§‚å¯Ÿåˆ°çš„ä¸‰ç»´è¡¨é¢å…ƒç´ ï¼ˆsurfelsï¼‰æ¥æœ‰æ•ˆåœ°è®°å¿†å’Œæ£€ç´¢ç›¸å…³çš„å†å²è§†å›¾ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒVMemèƒ½å¤Ÿåœ¨ç”Ÿæˆæ–°è§†å›¾æ—¶é«˜æ•ˆåœ°æå–æœ€ç›¸å…³çš„è¿‡å»è§†å›¾ï¼Œä»è€Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒé•¿æœŸåœºæ™¯çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨é•¿æœŸåœºæ™¯åˆæˆåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨åœºæ™¯ä¸€è‡´æ€§å’Œç›¸æœºæ§åˆ¶æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18463', 'title': 'DIP: Unsupervised Dense In-Context Post-training of Visual\n  Representations', 'url': 'https://huggingface.co/papers/2506.18463', 'abstract': 'A novel unsupervised post-training method improves dense image representations using pseudo-tasks and a pretrained diffusion model for in-context scene understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: https://github.com/sirkosophia/DIP', 'score': 12, 'issue_id': 4460, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '0fa71dbfe6a3edb6', 'authors': ['Sophia Sirko-Galouchenko', 'Spyros Gidaris', 'Antonin Vobecky', 'Andrei Bursuc', 'Nicolas Thome'], 'affiliations': ['CIIRC CTU Prague', 'FEE CTU', 'Sorbonne Universite, CNRS, ISIR, F-75005 Paris, France', 'Valeo.ai'], 'pdf_title_img': 'assets/pdf/title_img/2506.18463.jpg', 'data': {'categories': ['#cv', '#training', '#open_source', '#diffusion', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'DIP: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ DIP. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. DIP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 9 Ñ‡Ğ°ÑĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU A100. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'Enhancing Image Understanding with Unsupervised Pseudo-Tasks', 'desc': 'This paper presents DIP, an innovative unsupervised post-training method aimed at enhancing dense image representations in vision encoders. It utilizes pseudo-tasks that mimic real-world scenarios, drawing inspiration from meta-learning, to improve scene understanding without the need for labeled data. The method incorporates a pretrained diffusion model to automatically generate these in-context tasks, making it both efficient and straightforward. DIP demonstrates superior performance compared to existing methods and the original vision encoder, making it a valuable tool for various scene understanding applications.'}, 'zh': {'title': 'DIPï¼šæå‡å›¾åƒè¡¨ç¤ºçš„æ— ç›‘ç£åè®­ç»ƒæ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ— ç›‘ç£åè®­ç»ƒæ–¹æ³•DIPï¼Œæ—¨åœ¨é€šè¿‡ä¼ªä»»åŠ¡æå‡å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„å¯†é›†å›¾åƒè¡¨ç¤ºï¼Œä»¥å®ç°ä¸Šä¸‹æ–‡åœºæ™¯ç†è§£ã€‚ä¸ä»¥å¾€ä¾èµ–å¤æ‚è‡ªè’¸é¦æ¶æ„çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ¨¡æ‹Ÿä¸‹æ¸¸ä¸Šä¸‹æ–‡åœºæ™¯çš„ä¼ªä»»åŠ¡æ¥è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼Œçµæ„Ÿæ¥æºäºå…ƒå­¦ä¹ åŸç†ã€‚ä¸ºäº†åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œåè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆä¸Šä¸‹æ–‡ä»»åŠ¡çš„æœºåˆ¶ï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨æœ¬èº«ã€‚DIPç®€å•ã€æ— ç›‘ç£ä¸”è®¡ç®—é«˜æ•ˆï¼Œåœ¨å•ä¸ªA100 GPUä¸Šè®­ç»ƒæ—¶é—´å°‘äº9å°æ—¶ï¼Œèƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18309', 'title': 'LettinGo: Explore User Profile Generation for Recommendation System', 'url': 'https://huggingface.co/papers/2506.18309', 'abstract': 'LettinGo enhances user profiling via diverse, adaptive profiles generated using LLMs and Direct Preference Optimization, improving recommendation accuracy and flexibility.  \t\t\t\t\tAI-generated summary \t\t\t\t User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, a novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as a key innovation for next-generation recommendation systems.', 'score': 8, 'issue_id': 4450, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '6f2beb5a53c301ed', 'authors': ['Lu Wang', 'Di Zhang', 'Fangkai Yang', 'Pu Zhao', 'Jianfeng Liu', 'Yuefeng Zhan', 'Hao Sun', 'Qingwei Lin', 'Weiwei Deng', 'Dongmei Zhang', 'Feng Sun', 'Qi Zhang'], 'affiliations': ['Microsoft Corporation Beijing, China', 'Peking University Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.18309.jpg', 'data': {'categories': ['#rlhf', '#training', '#interpretability', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'LettinGo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM, Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LettinGo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'LettinGo: Adaptive User Profiles for Smarter Recommendations', 'desc': 'LettinGo is a new framework designed to improve user profiling for recommendation systems by creating diverse and adaptive profiles. It utilizes large language models (LLMs) to generate richer, text-based profiles that are more interpretable than traditional methods. The framework employs Direct Preference Optimization (DPO) to ensure that the profiles are aligned with specific recommendation tasks, allowing for greater flexibility and effectiveness. By exploring various user profiles and evaluating their impact on recommendations, LettinGo significantly enhances the accuracy and contextual awareness of personalized suggestions.'}, 'zh': {'title': 'LettinGoï¼šæå‡æ¨èç³»ç»Ÿçš„ç”¨æˆ·ç”»åƒç”Ÿæˆ', 'desc': 'LettinGo æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤šæ ·åŒ–å’Œè‡ªé€‚åº”çš„ç”¨æˆ·ç”»åƒï¼Œä»¥æé«˜æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¡¨è¾¾èƒ½åŠ›ï¼Œç»“åˆç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å›ºå®šæ ¼å¼çš„é™åˆ¶ã€‚é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„æ“ä½œï¼ŒLettinGo èƒ½å¤Ÿæ¢ç´¢å¤šæ ·çš„ç”¨æˆ·ç”»åƒã€è¯„ä¼°å…¶åœ¨æ¨èç³»ç»Ÿä¸­çš„è´¨é‡ï¼Œå¹¶æ ¹æ®ä»»åŠ¡æ€§èƒ½è°ƒæ•´ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†æ¨èçš„å‡†ç¡®æ€§ã€çµæ´»æ€§å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18901', 'title': 'From Virtual Games to Real-World Play', 'url': 'https://huggingface.co/papers/2506.18901', 'abstract': 'RealPlay generates photorealistic, temporally consistent video sequences from user control signals through iterative prediction and generalizes to various real-world entities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RealPlay, a neural network-based real-world game engine that enables interactive video generation from user control signals. Unlike prior works focused on game-style visuals, RealPlay aims to produce photorealistic, temporally consistent video sequences that resemble real-world footage. It operates in an interactive loop: users observe a generated scene, issue a control command, and receive a short video chunk in response. To enable such realistic and responsive generation, we address key challenges including iterative chunk-wise prediction for low-latency feedback, temporal consistency across iterations, and accurate control response. RealPlay is trained on a combination of labeled game data and unlabeled real-world videos, without requiring real-world action annotations. Notably, we observe two forms of generalization: (1) control transfer-RealPlay effectively maps control signals from virtual to real-world scenarios; and (2) entity transfer-although training labels originate solely from a car racing game, RealPlay generalizes to control diverse real-world entities, including bicycles and pedestrians, beyond vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/', 'score': 7, 'issue_id': 4457, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '74497a4801f265ee', 'authors': ['Wenqiang Sun', 'Fangyun Wei', 'Jinjing Zhao', 'Xi Chen', 'Zilong Chen', 'Hongyang Zhang', 'Jun Zhang', 'Yan Lu'], 'affiliations': ['HKUST', 'Microsoft Research', 'Tsinghua University', 'University of Sydney', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2506.18901.jpg', 'data': {'categories': ['#games', '#video', '#multimodal', '#transfer_learning'], 'emoji': 'ğŸ®', 'ru': {'title': 'RealPlay: Ğ¾Ñ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'RealPlay - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. RealPlay Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑ€Ğ°Ğ· Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²ĞµĞ»Ğ¾ÑĞ¸Ğ¿ĞµĞ´Ñ‹ Ğ¸ Ğ¿ĞµÑˆĞµÑ…Ğ¾Ğ´Ğ¾Ğ², Ñ…Ğ¾Ñ‚Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ½Ğ¾Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹.'}, 'en': {'title': 'RealPlay: Interactive, Photorealistic Video Generation from User Commands', 'desc': 'RealPlay is a neural network-based system designed to generate realistic video sequences in response to user commands. It focuses on creating photorealistic visuals that maintain temporal consistency, making the generated videos appear like real-world footage. The system operates interactively, allowing users to issue commands and receive video chunks with low latency. RealPlay demonstrates impressive generalization capabilities, effectively transferring control signals from virtual environments to real-world scenarios and adapting to various entities beyond its initial training data.'}, 'zh': {'title': 'å®æ—¶ç”ŸæˆçœŸå®æ„Ÿè§†é¢‘çš„æ¸¸æˆå¼•æ“', 'desc': 'RealPlayæ˜¯ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„æ¸¸æˆå¼•æ“ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æ§åˆ¶ä¿¡å·ç”Ÿæˆé€¼çœŸçš„è§†é¢‘åºåˆ—ã€‚ä¸ä»¥å¾€ä¸“æ³¨äºæ¸¸æˆé£æ ¼è§†è§‰æ•ˆæœçš„ç ”ç©¶ä¸åŒï¼ŒRealPlayæ—¨åœ¨ç”Ÿæˆä¸çœŸå®ä¸–ç•Œè§†é¢‘ç›¸ä¼¼çš„é«˜è´¨é‡ã€æ—¶é—´ä¸€è‡´çš„è§†é¢‘ã€‚å®ƒé€šè¿‡ä¸€ä¸ªäº¤äº’å¾ªç¯å·¥ä½œï¼Œç”¨æˆ·è§‚å¯Ÿç”Ÿæˆçš„åœºæ™¯ï¼Œå‘å‡ºæ§åˆ¶å‘½ä»¤ï¼Œç„¶åæ”¶åˆ°çŸ­è§†é¢‘ç‰‡æ®µä½œä¸ºåé¦ˆã€‚RealPlayåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆäº†æ ‡è®°çš„æ¸¸æˆæ•°æ®å’Œæœªæ ‡è®°çš„çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†æ§åˆ¶ä¿¡å·ä»è™šæ‹Ÿåœºæ™¯æ˜ å°„åˆ°çœŸå®åœºæ™¯ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ§åˆ¶å¤šç§çœŸå®ä¸–ç•Œå®ä½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18349', 'title': 'SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation', 'url': 'https://huggingface.co/papers/2506.18349', 'abstract': "SlimMoE compresses large MoE models into smaller, efficient variants using multi-stage compression without full retraining, maintaining competitive performance with significantly fewer resources.  \t\t\t\t\tAI-generated summary \t\t\t\t The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .", 'score': 7, 'issue_id': 4449, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'ae21c466bdfe4727', 'authors': ['Zichong Li', 'Chen Liang', 'Zixuan Zhang', 'Ilgee Hong', 'Young Jin Kim', 'Weizhu Chen', 'Tuo Zhao'], 'affiliations': ['Georgia Tech', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2506.18349.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#training', '#architecture', '#open_source', '#inference', '#small_models'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'SlimMoE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'SlimMoE - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture of Experts (MoE) Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¶Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Phi 3.5-MoE Ğ´Ğ¾ Phi-mini-MoE Ğ¸ Phi-tiny-MoE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Efficient Compression of MoE Models for Resource-Constrained Environments', 'desc': 'SlimMoE is a novel framework designed to compress large Mixture of Experts (MoE) models into smaller, more efficient versions without the need for extensive retraining. It employs a multi-stage compression approach that reduces the number of parameters while preserving model performance, making it feasible to deploy in environments with limited resources. By systematically slimming down experts and transferring knowledge through intermediate stages, SlimMoE mitigates the performance loss typically associated with one-shot pruning methods. The resulting models, Phi-mini-MoE and Phi-tiny-MoE, demonstrate competitive performance with significantly fewer activated parameters, making them ideal for academic and resource-constrained applications.'}, 'zh': {'title': 'SlimMoEï¼šé«˜æ•ˆå‹ç¼©å¤§å‹MoEæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'SlimMoEæ˜¯ä¸€ç§å¤šé˜¶æ®µå‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤§å‹æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹å‹ç¼©ä¸ºæ›´å°ã€æ›´é«˜æ•ˆçš„å˜ä½“ï¼Œè€Œæ— éœ€å®Œå…¨é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡ç²¾ç®€ä¸“å®¶å’Œåœ¨ä¸­é—´é˜¶æ®µè½¬ç§»çŸ¥è¯†ï¼Œæœ‰æ•ˆå‡å°‘å‚æ•°æ•°é‡ï¼Œé¿å…äº†ä¸€æ¬¡æ€§å‰ªææ–¹æ³•å¸¸è§çš„æ€§èƒ½ä¸‹é™ã€‚å®éªŒè¡¨æ˜ï¼Œå‹ç¼©åçš„æ¨¡å‹åœ¨ç›¸ä¼¼è§„æ¨¡ä¸‹è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è¾ƒä½å»¶è¿Ÿä¸‹ä¸æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚SlimMoEä¸ºåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ä½¿ç”¨MoEæ¶æ„æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16962', 'title': 'Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2506.16962', 'abstract': 'MICS, a novel reasoning-path searching scheme, enhances medical MLLMs like Chiron-o1 with robust generalizable reasoning and visual question-answering capabilities through comprehensive chain-of-thought data generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs', 'score': 7, 'issue_id': 4449, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': 'c1531ab3106ca207', 'authors': ['Haoran Sun', 'Yankai Jiang', 'Wenjie Lou', 'Yujie Zhang', 'Wenjie Li', 'Lilong Wang', 'Mianxin Liu', 'Lei Liu', 'Xiaosong Wang'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.16962.jpg', 'data': {'categories': ['#science', '#benchmark', '#data', '#multimodal', '#dataset', '#training', '#healthcare', '#reasoning'], 'emoji': 'ğŸ©º', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MICS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. MICS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ½Ğ°ÑÑ‚Ğ°Ğ²Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑÑ‚Ğ°Ğ¶ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ MLLM Chiron-o1, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMRP. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Chiron-o1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Medical Reasoning with MICS: A New Path to Better Diagnosis', 'desc': 'This paper introduces MICS, a new method for improving reasoning in medical multimodal large language models (MLLMs) like Chiron-o1. MICS generates high-quality chain-of-thought (CoT) data by using mentor models to guide the reasoning process and intern models to explore these paths. The effectiveness of the reasoning paths is evaluated using an MICS-Score, which helps in selecting the best paths for medical diagnosis. The results show that Chiron-o1, trained with MICS-generated data, outperforms existing models in medical visual question answering and reasoning tasks.'}, 'zh': {'title': 'MICSï¼šæå‡åŒ»ç–—æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ¡ˆ', 'desc': 'MICSæ˜¯ä¸€ç§æ–°é¢–çš„æ¨ç†è·¯å¾„æœç´¢æ–¹æ¡ˆï¼Œæ—¨åœ¨å¢å¼ºåŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç”Ÿæˆå…¨é¢çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ï¼ŒMICSèƒ½å¤Ÿæœ‰æ•ˆåœ°æ„å»ºåŒ»ç–—é¢†åŸŸçš„æ¨ç†æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¯¼å¸ˆæ¨¡å‹é€æ­¥åˆå§‹åŒ–æ¨ç†ï¼Œç„¶åè®©å®ä¹ æ¨¡å‹æ²¿ç€è¿™äº›è·¯å¾„ç»§ç»­æ€è€ƒï¼Œæœ€ç»ˆé€‰æ‹©æœ€ä½³æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºMICSæ„å»ºçš„Chiron-o1æ¨¡å‹åœ¨åŒ»ç–—è§†è§‰é—®ç­”å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16123', 'title': 'FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning', 'url': 'https://huggingface.co/papers/2506.16123', 'abstract': 'A structured chain-of-thought prompting method in financial natural language processing improves performance and reduces computational cost while enhancing interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents FinCoT, a structured chain-of-thought (CoT) prompting approach that incorporates insights from domain-specific expert financial reasoning to guide the reasoning traces of large language models. We investigate that there are three main prompting styles in FinNLP: (1) standard prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an explicit reasoning structure, such as the use of tags; and (3) structured CoT prompting--CoT prompting with explicit instructions or examples that define structured reasoning steps. Previously, FinNLP has primarily focused on prompt engineering with either standard or unstructured CoT prompting. However, structured CoT prompting has received limited attention in prior work. Furthermore, the design of reasoning structures in structured CoT prompting is often based on heuristics from non-domain experts. In this study, we investigate each prompting approach in FinNLP. We evaluate the three main prompting styles and FinCoT on CFA-style questions spanning ten financial domains. We observe that FinCoT improves performance from 63.2% to 80.5% and Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens eight-fold compared to structured CoT prompting. Our findings show that domain-aligned structured prompts not only improve performance and reduce inference costs but also yield more interpretable and expert-aligned reasoning traces.', 'score': 7, 'issue_id': 4456, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': 'faa22cef17fe9376', 'authors': ['Natapong Nitarach', 'Warit Sirichotedumrong', 'Panop Pitchayarthorn', 'Pittawat Taveekitworachai', 'Potsawee Manakul', 'Kunat Pipatanakul'], 'affiliations': ['SCB 10X', 'SCBX Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.16123.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#multimodal', '#training', '#data'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ FinCoT - ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. FinCoT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ 63.2% Ğ´Ğ¾ 80.5% Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ğ² ÑÑ‚Ğ¸Ğ»Ğµ CFA Ğ¿Ğ¾ 10 Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, FinCoT ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Financial NLP with Structured Reasoning', 'desc': 'This paper introduces FinCoT, a new prompting method for financial natural language processing that enhances the reasoning capabilities of large language models. It categorizes prompting styles into three types: standard prompting, unstructured chain-of-thought (CoT) prompting, and structured CoT prompting, with a focus on the latter. The study demonstrates that structured CoT prompting, which uses explicit instructions from financial experts, significantly boosts model performance and reduces computational costs. Results show that FinCoT increases accuracy and provides clearer reasoning paths, making it a valuable tool for financial analysis.'}, 'zh': {'title': 'ç»“æ„åŒ–æ€ç»´é“¾æç¤ºï¼šæå‡é‡‘èNLPæ€§èƒ½çš„å…³é”®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFinCoTçš„ç»“æ„åŒ–æ€ç»´é“¾æç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨æå‡é‡‘èè‡ªç„¶è¯­è¨€å¤„ç†çš„æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶å¢å¼ºå¯è§£é‡Šæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒFinNLPä¸­ä¸»è¦æœ‰ä¸‰ç§æç¤ºé£æ ¼ï¼šæ ‡å‡†æç¤ºã€éç»“æ„åŒ–æ€ç»´é“¾æç¤ºå’Œç»“æ„åŒ–æ€ç»´é“¾æç¤ºã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ ‡å‡†æˆ–éç»“æ„åŒ–æ€ç»´é“¾æç¤ºä¸Šï¼Œè€Œç»“æ„åŒ–æ€ç»´é“¾æç¤ºçš„å…³æ³¨è¾ƒå°‘ã€‚é€šè¿‡å¯¹åä¸ªé‡‘èé¢†åŸŸçš„CFAé£æ ¼é—®é¢˜è¿›è¡Œè¯„ä¼°ï¼ŒFinCoTçš„æ€§èƒ½ä»63.2%æå‡è‡³80.5%ï¼Œå¹¶ä¸”ç”Ÿæˆçš„æ ‡è®°æ•°é‡å‡å°‘äº†å…«å€ï¼Œæ˜¾ç¤ºå‡ºé¢†åŸŸå¯¹é½çš„ç»“æ„åŒ–æç¤ºåœ¨æ€§èƒ½å’Œæ¨ç†å¯è§£é‡Šæ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18839', 'title': '4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction\n  for 4D Scene Generation', 'url': 'https://huggingface.co/papers/2506.18839', 'abstract': 'A new framework combines 4D video modeling and 3D reconstruction using a unified architecture with sparse attention patterns, achieving superior visual quality and reconstruction.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose the first framework capable of computing a 4D spatio-temporal grid of video frames and 3D Gaussian particles for each time step using a feed-forward architecture. Our architecture has two main components, a 4D video model and a 4D reconstruction model. In the first part, we analyze current 4D video diffusion architectures that perform spatial and temporal attention either sequentially or in parallel within a two-stream design. We highlight the limitations of existing approaches and introduce a novel fused architecture that performs spatial and temporal attention within a single layer. The key to our method is a sparse attention pattern, where tokens attend to others in the same frame, at the same timestamp, or from the same viewpoint. In the second part, we extend existing 3D reconstruction algorithms by introducing a Gaussian head, a camera token replacement algorithm, and additional dynamic layers and training. Overall, we establish a new state of the art for 4D generation, improving both visual quality and reconstruction capability.', 'score': 7, 'issue_id': 4458, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': 'adf5e4953f4c2a0e', 'authors': ['Chaoyang Wang', 'Ashkan Mirzaei', 'Vidit Goel', 'Willi Menapace', 'Aliaksandr Siarohin', 'Avalon Vinella', 'Michael Vasilkovsky', 'Ivan Skorokhodov', 'Vladislav Shakhrai', 'Sergey Korolev', 'Sergey Tulyakov', 'Peter Wonka'], 'affiliations': ['KAUST', 'Snap Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.18839.jpg', 'data': {'categories': ['#3d', '#video', '#diffusion', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ 4D Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ 4D Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ»Ğ¾Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ 4D Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 4D Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing 4D Video and 3D Reconstruction with Sparse Attention', 'desc': 'This paper presents a novel framework that integrates 4D video modeling with 3D reconstruction using a unified architecture. It introduces a feed-forward model that computes a 4D spatio-temporal grid of video frames alongside 3D Gaussian particles for each time step. The framework employs a unique sparse attention pattern, allowing tokens to focus on relevant information within the same frame, timestamp, or viewpoint, enhancing both spatial and temporal attention. As a result, this approach sets a new benchmark in 4D generation, significantly improving visual quality and reconstruction accuracy.'}, 'zh': {'title': 'èåˆ4Dè§†é¢‘ä¸3Dé‡å»ºçš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç»“åˆäº†4Dè§†é¢‘å»ºæ¨¡å’Œ3Dé‡å»ºï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ¶æ„å’Œç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰è´¨é‡å’Œé‡å»ºæ•ˆæœã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„4Dæ—¶ç©ºç½‘æ ¼è§†é¢‘å¸§å’Œ3Dé«˜æ–¯ç²’å­ï¼Œé‡‡ç”¨å‰é¦ˆæ¶æ„ã€‚æˆ‘ä»¬åˆ†æäº†ç°æœ‰çš„4Dè§†é¢‘æ‰©æ•£æ¶æ„ï¼ŒæŒ‡å‡ºäº†å…¶åœ¨ç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›å¤„ç†ä¸Šçš„å±€é™æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„èåˆæ¶æ„ï¼Œåœ¨å•å±‚ä¸­åŒæ—¶æ‰§è¡Œç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›ã€‚é€šè¿‡å¼•å…¥é«˜æ–¯å¤´å’ŒåŠ¨æ€å±‚ç­‰æ–°æŠ€æœ¯ï¼Œæˆ‘ä»¬åœ¨4Dç”Ÿæˆé¢†åŸŸå»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18904', 'title': 'TC-Light: Temporally Consistent Relighting for Dynamic Long Videos', 'url': 'https://huggingface.co/papers/2506.18904', 'abstract': 'TC-Light, a novel two-stage video relighting model, achieves high temporal coherence and low computational cost through appearance embedding and unique video tensor optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Editing illumination in long videos with complex dynamics has significant value in various downstream tasks, including visual content creation and manipulation, as well as data scaling up for embodied AI through sim2real and real2real transfer. Nevertheless, existing video relighting techniques are predominantly limited to portrait videos or fall into the bottleneck of temporal consistency and computation efficiency. In this paper, we propose TC-Light, a novel paradigm characterized by the proposed two-stage post optimization mechanism. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible relighting results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/.', 'score': 6, 'issue_id': 4459, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'bf0e4b895e1a5df9', 'authors': ['Yang Liu', 'Chuanchen Luo', 'Zimo Tang', 'Yingyan Li', 'Yuran Yang', 'Yuanyong Ning', 'Lue Fan', 'Junran Peng', 'Zhaoxiang Zhang'], 'affiliations': ['Huazhong University of Science and Technology', 'NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences', 'Shandong University', 'Tencent', 'University of Chinese Academy of Sciences', 'University of Science and Technology Beijing'], 'pdf_title_img': 'assets/pdf/title_img/2506.18904.jpg', 'data': {'categories': ['#video', '#benchmark', '#optimization', '#long_context'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'TC-Light - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼. TC-Light Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Efficient and Coherent Video Relighting with TC-Light', 'desc': 'TC-Light is a two-stage video relighting model designed to enhance the illumination of long videos while maintaining temporal coherence and minimizing computational costs. The first stage focuses on optimizing appearance embedding to ensure consistent global lighting across frames. In the second stage, it utilizes a unique video representation called Unique Video Tensor (UVT) to refine the texture and lighting details. This approach addresses the limitations of existing methods, particularly in handling complex video dynamics, and demonstrates superior performance through extensive benchmarking.'}, 'zh': {'title': 'TC-Lightï¼šé«˜æ•ˆè§†é¢‘é‡ç…§æ˜çš„æ–°é€‰æ‹©', 'desc': 'TC-Lightæ˜¯ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè§†é¢‘é‡ç…§æ˜æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé«˜æ—¶é—´ä¸€è‡´æ€§çš„åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤–è§‚åµŒå…¥å’Œç‹¬ç‰¹çš„è§†é¢‘å¼ é‡ä¼˜åŒ–æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸ç°æœ‰æŠ€æœ¯ä¸»è¦å±€é™äºè‚–åƒè§†é¢‘ä¸åŒï¼ŒTC-Lighté€‚ç”¨äºé•¿è§†é¢‘å’Œå¤æ‚åŠ¨æ€åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„é‡ç…§æ˜æ•ˆæœï¼Œä¸”è®¡ç®—æ•ˆç‡ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16507', 'title': 'Robust Reward Modeling via Causal Rubrics', 'url': 'https://huggingface.co/papers/2506.16507', 'abstract': 'Crome, a novel reward modeling framework using causal and neutral augmentations, significantly improves the robustness and accuracy of reward models against reward hacking.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback, yet they often suffer from reward hacking. They tend to latch on to superficial or spurious attributes, such as response length or formatting, mistaking these cues learned from correlations in training data for the true causal drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce Crome (Causally Robust Reward Modeling), a novel framework grounded in an explicit causal model designed to mitigate reward hacking. Crome employs the following synthetic targeted augmentations during training: (1) Causal Augmentations, which are pairs that differ along specific causal attributes, to enforce sensitivity along each causal attribute individually, and (2) Neutral Augmentations, which are tie-label pairs varying primarily in spurious attributes, to enforce invariance along spurious attributes. Notably, our augmentations are produced without any knowledge of spurious factors, via answer interventions only along causal rubrics, that are identified by querying an oracle LLM. Empirically, Crome significantly outperforms standard baselines on RewardBench, improving average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in specific categories. The robustness of Crome is further testified by the consistent gains obtained in a Best-of-N inference setting across increasing N, across various benchmarks, including the popular RewardBench (covering chat, chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and the reasoning-specific GSM8k.', 'score': 6, 'issue_id': 4455, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': 'aca1d330d3067ef5', 'authors': ['Pragya Srivastava', 'Harman Singh', 'Rahul Madhavan', 'Gandharv Patil', 'Sravanti Addepalli', 'Arun Suggala', 'Rengarajan Aravamudhan', 'Soumya Sharma', 'Anirban Laha', 'Aravindan Raghuveer', 'Karthikeyan Shanmugam', 'Doina Precup'], 'affiliations': ['Google DeepMind', 'MILA - Quebec AI Institute', 'McGill University'], 'pdf_title_img': 'assets/pdf/title_img/2506.16507.jpg', 'data': {'categories': ['#optimization', '#alignment', '#benchmark', '#rlhf', '#hallucinations', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Crome - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑĞ²Ğ½Ğ¾Ğ¹ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Crome Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ RewardBench, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ€ĞµĞ´Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 5.4%. Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Crome Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¸Ğ· N Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Crome: Enhancing Reward Models with Causal Insights', 'desc': 'Crome is a new framework designed to enhance reward models used in aligning Large Language Models with human feedback. It addresses the issue of reward hacking, where models mistakenly focus on superficial traits instead of genuine quality indicators. By using causal and neutral augmentations during training, Crome helps models learn the true causal factors that contribute to quality while ignoring misleading spurious attributes. The framework has shown significant improvements in accuracy and robustness across various benchmarks, demonstrating its effectiveness in creating more reliable reward models.'}, 'zh': {'title': 'Cromeï¼šæå‡å¥–åŠ±æ¨¡å‹é²æ£’æ€§çš„åˆ›æ–°æ¡†æ¶', 'desc': 'Cromeæ˜¯ä¸€ç§æ–°é¢–çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼Œä»¥é˜²æ­¢å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹å®¹æ˜“å—åˆ°è¡¨é¢ç‰¹å¾çš„å½±å“ï¼Œå¯¼è‡´è¯¯åˆ¤è´¨é‡çš„çœŸæ­£å› æœé©±åŠ¨å› ç´ ã€‚Cromeé€šè¿‡å› æœå¢å¼ºå’Œä¸­æ€§å¢å¼ºçš„åˆæˆç›®æ ‡æ¥è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œåˆ†åˆ«å¢å¼ºå¯¹å› æœå±æ€§çš„æ•æ„Ÿæ€§å’Œå¯¹è¡¨é¢å±æ€§çš„ä¸å˜æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCromeåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œæå‡äº†å¹³å‡å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18631', 'title': 'ReDit: Reward Dithering for Improved LLM Policy Optimization', 'url': 'https://huggingface.co/papers/2506.18631', 'abstract': "ReDit, a reward dithering method, addresses issues in discrete reward systems by introducing noise, leading to smoother optimization and faster convergence compared to standard methods.  \t\t\t\t\tAI-generated summary \t\t\t\t DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.", 'score': 5, 'issue_id': 4448, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'cc6b6162c9368cf4', 'authors': ['Chenxing Wei', 'Jiarui Yu', 'Ying Tiffany He', 'Hande Dong', 'Yao Shu', 'Fei Yu'], 'affiliations': ['College of Computer Science and Software Engineering, Shenzhen University, China', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China', 'Hong Kong University of Science and Technology (Guangzhou), China', 'Tencent, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.18631.jpg', 'data': {'categories': ['#rlhf', '#training', '#rl', '#reasoning', '#optimization'], 'emoji': 'ğŸ²', 'ru': {'title': 'Ğ¨ÑƒĞ¼ Ğ²Ğ¾ Ğ±Ğ»Ğ°Ğ³Ğ¾: ReDit ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ReDit - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ReDit Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ°Ğ²Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ReDit Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ·Ğ° 10% ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ GRPO.'}, 'en': {'title': 'ReDit: Smoother Rewards for Faster Learning', 'desc': 'ReDit is a novel method designed to improve optimization in systems that use discrete rewards by adding random noise to the reward signal. This noise helps to create smoother gradient updates, which leads to faster convergence during training. By introducing stochasticity, ReDit encourages exploration of new policies, helping models avoid getting stuck in local optima. Experimental results show that ReDit not only reduces training time significantly but also enhances performance compared to traditional methods.'}, 'zh': {'title': 'ReDitï¼šæå‡ç¦»æ•£å¥–åŠ±ç³»ç»Ÿçš„ä¼˜åŒ–æ•ˆç‡', 'desc': 'ReDitæ˜¯ä¸€ç§å¥–åŠ±æŠ–åŠ¨æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç¦»æ•£å¥–åŠ±ç³»ç»Ÿä¸­çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å™ªå£°ï¼ŒReDitä½¿å¾—ä¼˜åŒ–è¿‡ç¨‹æ›´åŠ å¹³æ»‘ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æ¯”æ ‡å‡†æ–¹æ³•æ›´å¿«ã€‚å®éªŒè¡¨æ˜ï¼Œç¦»æ•£å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸å’Œä¸ç¨³å®šçš„ä¼˜åŒ–ï¼Œè€ŒReDité€šè¿‡æ·»åŠ éšæœºå™ªå£°æ¥æ”¹å–„è¿™ä¸€ç‚¹ã€‚æœ€ç»ˆï¼ŒReDitåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒæ­¥éª¤ä»…ä¸ºä¼ ç»Ÿæ–¹æ³•çš„10%ï¼ŒåŒæ—¶åœ¨ç›¸ä¼¼è®­ç»ƒæ—¶é—´å†…æ€§èƒ½æå‡äº†4%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17538', 'title': 'ConsumerBench: Benchmarking Generative AI Applications on End-User\n  Devices', 'url': 'https://huggingface.co/papers/2506.17538', 'abstract': 'ConsumerBench evaluates GenAI system efficiency and response time on end-user devices through a comprehensive benchmarking framework, emphasizing realistic multi-application scenarios and customizable workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent shift in Generative AI (GenAI) applications from cloud-only environments to end-user devices introduces new challenges in resource management, system efficiency, and user experience. This paper presents ConsumerBench, a comprehensive benchmarking framework designed to evaluate the system efficiency and response time of GenAI models running on end-user devices. Unlike existing benchmarks that assume exclusive model access on dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios executing concurrently on constrained hardware. Furthermore, ConsumerBench supports customizable workflows that simulate complex tasks requiring coordination among multiple applications. ConsumerBench captures both application-level metrics, including latency and Service Level Objective (SLO) attainment, and system-level metrics like CPU/GPU utilization and memory bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies in resource sharing, unfair scheduling under greedy allocation, and performance pitfalls of static model server configurations. The paper also provides practical insights for model developers and system designers, highlighting the benefits of custom kernels tailored to consumer-grade GPU architectures and the value of implementing SLO-aware scheduling strategies.', 'score': 5, 'issue_id': 4451, 'pub_date': '2025-06-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ½Ñ', 'en': 'June 21', 'zh': '6æœˆ21æ—¥'}, 'hash': '77484dbfc212d862', 'authors': ['Yile Gu', 'Rohan Kadekodi', 'Hoang Nguyen', 'Keisuke Kamahori', 'Yiyu Liu', 'Baris Kasikci'], 'affiliations': ['Shanghai Jiao Tong University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.17538.jpg', 'data': {'categories': ['#data', '#optimization', '#benchmark'], 'emoji': 'ğŸ“±', 'ru': {'title': 'ConsumerBench: ĞÑ†ĞµĞ½ĞºĞ° GenAI Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'ConsumerBench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ConsumerBench Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¸ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Optimizing GenAI Performance on Everyday Devices', 'desc': 'ConsumerBench is a benchmarking framework that assesses the efficiency and response time of Generative AI (GenAI) systems on end-user devices. It addresses the challenges posed by running multiple applications simultaneously on limited hardware, unlike traditional benchmarks that focus on dedicated GPUs. The framework measures both application-level metrics, such as latency and Service Level Objectives (SLO), and system-level metrics like CPU/GPU utilization. The findings highlight issues in resource sharing and scheduling, offering insights for developers to optimize performance on consumer-grade devices.'}, 'zh': {'title': 'ConsumerBenchï¼šè¯„ä¼°ç»ˆç«¯è®¾å¤‡ä¸ŠGenAIç³»ç»Ÿçš„æ•ˆç‡ä¸å“åº”æ—¶é—´', 'desc': 'ConsumerBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åœ¨ç»ˆç«¯è®¾å¤‡ä¸Šè¿è¡Œçš„ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰ç³»ç»Ÿçš„æ•ˆç‡å’Œå“åº”æ—¶é—´ã€‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒConsumerBenchæ¨¡æ‹Ÿäº†åœ¨å—é™ç¡¬ä»¶ä¸ŠåŒæ—¶æ‰§è¡Œçš„å¤šåº”ç”¨åœºæ™¯ï¼Œå¼ºè°ƒäº†èµ„æºç®¡ç†å’Œç”¨æˆ·ä½“éªŒçš„æ–°æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ”¯æŒå¯å®šåˆ¶çš„å·¥ä½œæµç¨‹ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿéœ€è¦å¤šä¸ªåº”ç”¨åè°ƒçš„å¤æ‚ä»»åŠ¡ã€‚é€šè¿‡å®éªŒï¼ŒConsumerBenchæ­ç¤ºäº†èµ„æºå…±äº«ä¸­çš„ä½æ•ˆã€è´ªå©ªåˆ†é…ä¸‹çš„ä¸å…¬å¹³è°ƒåº¦ä»¥åŠé™æ€æ¨¡å‹æœåŠ¡å™¨é…ç½®çš„æ€§èƒ½é™·é˜±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18879', 'title': 'CommVQ: Commutative Vector Quantization for KV Cache Compression', 'url': 'https://huggingface.co/papers/2506.18879', 'abstract': 'Commutative Vector Quantization (CommVQ) reduces memory usage in long-context LLM inference by compressing the KV cache with additive quantization and integration of Rotary Position Embedding (RoPE).  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.', 'score': 4, 'issue_id': 4448, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '1d442a58c0e72d5c', 'authors': ['Junyan Li', 'Yang Zhang', 'Muhammad Yusuf Hassan', 'Talha Chafekar', 'Tianle Cai', 'Zhile Ren', 'Pengsheng Guo', 'Foroozan Karimzadeh', 'Colorado Reed', 'Chong Wang', 'Chuang Gan'], 'affiliations': ['Apple Inc.', 'Massachusetts Institute of Technology', 'Princeton University', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.18879.jpg', 'data': {'categories': ['#long_context', '#training', '#open_source', '#inference', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Commutative Vector Quantization (CommVQ) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. CommVQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Rotary Position Embedding Ğ´Ğ»Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºÑÑˆĞ° Ğ½Ğ° 87.5% Ğ¿Ñ€Ğ¸ 2-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaMA-3.1 8B Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ 128K Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU RTX 4090.'}, 'en': {'title': 'Efficient Memory Management for Long-Context LLMs with CommVQ', 'desc': 'This paper introduces Commutative Vector Quantization (CommVQ), a method designed to reduce memory usage in long-context inference for Large Language Models (LLMs). By employing additive quantization and a commutative codebook integrated with Rotary Position Embedding (RoPE), the method compresses the key-value (KV) cache effectively. The approach allows for efficient decoding through simple matrix multiplication, significantly lowering computational costs. Experiments demonstrate that CommVQ can reduce the KV cache size by 87.5% while maintaining high accuracy, enabling LLMs to handle longer contexts on standard GPUs.'}, 'zh': {'title': 'å¯äº¤æ¢å‘é‡é‡åŒ–ï¼šä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„å†…å­˜ä½¿ç”¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¯äº¤æ¢å‘é‡é‡åŒ–ï¼ˆCommVQï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„å†…å­˜ä½¿ç”¨ã€‚é€šè¿‡å¼•å…¥åŠ æ³•é‡åŒ–å’Œè½»é‡çº§ç¼–ç å™¨ï¼ŒCommVQèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ï¼Œå¹¶é€šè¿‡ç®€å•çš„çŸ©é˜µä¹˜æ³•è¿›è¡Œè§£ç ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰å…¼å®¹çš„ä»£ç æœ¬ï¼Œå¹¶ä½¿ç”¨æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œä»è€Œåœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­å®ç°é«˜æ•ˆè§£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå°†FP16 KVç¼“å­˜å¤§å°å‡å°‘87.5%ï¼Œå¹¶åœ¨1ä½é‡åŒ–ä¸‹å®ç°æœ€å°çš„å‡†ç¡®æ€§æŸå¤±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18527', 'title': 'Auto-Regressively Generating Multi-View Consistent Images', 'url': 'https://huggingface.co/papers/2506.18527', 'abstract': '', 'score': 4, 'issue_id': 4450, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '5b7e755fbaf18d79', 'authors': ['JiaKui Hu', 'Yuxiao Yang', 'Jialun Liu', 'Jinbo Wu', 'Chen Zhao', 'Yanye Lu'], 'affiliations': ['Baidu VIS', 'Biomedical Engineering Department, College of Future Technology, Peking University', 'Institute of Medical Technology, Peking University Health Science Center, Peking University', 'National Biomedical Imaging Center, Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.18527.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸.'}, 'en': {'title': 'Hybrid Models: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': 'æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åˆ›æ–°ç®—æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ•°æ®ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç ”ç©¶è€…å¸Œæœ›æ¨åŠ¨æœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17673', 'title': 'FaithfulSAE: Towards Capturing Faithful Features with Sparse\n  Autoencoders without External Dataset Dependencies', 'url': 'https://huggingface.co/papers/2506.17673', 'abstract': 'FaithfulSAE improves Sparse Autoencoder stability and interpretability by training on synthetic datasets generated by the model itself, reducing the occurrence of fake features and out-of-distribution data issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Autoencoders (SAEs) have emerged as a promising solution for decomposing large language model representations into interpretable features. However, Paulo and Belrose (2025) have highlighted instability across different initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not capture model-internal features. These problems likely stem from training SAEs on external datasets - either collected from the Web or generated by another model - which may contain out-of-distribution (OOD) data beyond the model\'s generalisation capabilities. This can result in hallucinated SAE features, which we term "Fake Features", that misrepresent the model\'s internal activations. To address these issues, we propose FaithfulSAE, a method that trains SAEs on the model\'s own synthetic dataset. Using FaithfulSAEs, we demonstrate that training SAEs on less-OOD instruction datasets results in SAEs being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained on web-based datasets in the SAE probing task and exhibit a lower Fake Feature Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on external datasets, advancing interpretability by better capturing model-internal features while highlighting the often neglected importance of SAE training datasets.', 'score': 4, 'issue_id': 4453, 'pub_date': '2025-06-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ½Ñ', 'en': 'June 21', 'zh': '6æœˆ21æ—¥'}, 'hash': 'b4d024da8e8afc68', 'authors': ['Seonglae Cho', 'Harryn Oh', 'Donghyun Lee', 'Luis Eduardo Rodrigues Vieira', 'Andrew Bermingham', 'Ziad El Sayed'], 'affiliations': ['University College London'], 'pdf_title_img': 'assets/pdf/title_img/2506.17673.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#data', '#hallucinations', '#synthetic', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’ĞµÑ€Ğ½Ñ‹Ğµ ÑĞµĞ±Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ SAE', 'desc': 'FaithfulSAE - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² (SAE) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. FaithfulSAE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ SAE Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ SAE, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ²ĞµĞ±-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing SAE Stability and Interpretability with Synthetic Data', 'desc': "FaithfulSAE is a method designed to enhance the stability and interpretability of Sparse Autoencoders (SAEs) by training them on synthetic datasets generated by the model itself. This approach addresses issues of instability and the presence of 'Fake Features' that arise when SAEs are trained on external datasets, which may include out-of-distribution data. By using less out-of-distribution instruction datasets, FaithfulSAE ensures that the SAEs are more consistent across different initialization seeds. The results show that FaithfulSAE significantly reduces the occurrence of misrepresented features and improves performance in SAE probing tasks compared to traditional web-based training methods."}, 'zh': {'title': 'FaithfulSAEï¼šæå‡ç¨€ç–è‡ªç¼–ç å™¨çš„ç¨³å®šæ€§ä¸å¯è§£é‡Šæ€§', 'desc': 'FaithfulSAEæ˜¯ä¸€ç§æ”¹è¿›ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ç¨³å®šæ€§å’Œå¯è§£é‡Šæ€§çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ä½¿ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„åˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå‡å°‘äº†è™šå‡ç‰¹å¾å’Œåˆ†å¸ƒå¤–æ•°æ®çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨FaithfulSAEè®­ç»ƒçš„SAEåœ¨ä¸åŒåˆå§‹åŒ–ç§å­ä¸‹è¡¨ç°æ›´ç¨³å®šï¼Œå¹¶ä¸”åœ¨SAEæ¢æµ‹ä»»åŠ¡ä¸­ä¼˜äºåŸºäºç½‘ç»œæ•°æ®é›†è®­ç»ƒçš„SAEã€‚è¯¥æ–¹æ³•å¼ºè°ƒäº†SAEè®­ç»ƒæ•°æ®é›†çš„é‡è¦æ€§ï¼Œæ¨åŠ¨äº†å¯¹æ¨¡å‹å†…éƒ¨ç‰¹å¾çš„æ›´å¥½æ•æ‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17323', 'title': 'I Know Which LLM Wrote Your Code Last Summer: LLM generated Code\n  Stylometry for Authorship Attribution', 'url': 'https://huggingface.co/papers/2506.17323', 'abstract': "A novel model, CodeT5-Authorship, is introduced to classify the authorship of C programs generated by Large Language Models, achieving high accuracy compared to traditional and transformer-based classifiers.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting AI-generated code, deepfakes, and other synthetic content is an emerging research challenge. As code generated by Large Language Models (LLMs) becomes more common, identifying the specific model behind each sample is increasingly important. This paper presents the first systematic study of LLM authorship attribution for C programs. We released CodeT5-Authorship, a novel model that uses only the encoder layers from the original CodeT5 encoder-decoder architecture, discarding the decoder to focus on classification. Our model's encoder output (first token) is passed through a two-layer classification head with GELU activation and dropout, producing a probability distribution over possible authors. To evaluate our approach, we introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse tasks. We compare our model to seven traditional ML classifiers and eight fine-tuned transformer models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model achieves 97.56% accuracy in distinguishing C programs generated by closely related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.", 'score': 4, 'issue_id': 4455, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '51a95259fad4a8ed', 'authors': ['Tamas Bisztray', 'Bilel Cherif', 'Richard A. Dubniczky', 'Nils Gruschka', 'Bertalan Borsos', 'Mohamed Amine Ferrag', 'Attila Kovacs', 'Vasileios Mavroeidis', 'Norbert Tihanyi'], 'affiliations': ['EÃ¶tvÃ¶s LÃ³rÃ¡nd University, Budapest, Hungary', 'Guelma University, Guelma, Algeria', 'Technology Innovation Institute, Abu Dhabi, United Arab Emirates', 'University of Oslo, Cyentific AS, Oslo, Norway', 'University of Oslo, Oslo, Norway'], 'pdf_title_img': 'assets/pdf/title_img/2506.17323.jpg', 'data': {'categories': ['#synthetic', '#architecture', '#benchmark', '#open_source', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'CodeT5-Authorship: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ LLM-Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ°', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CodeT5-Authorship Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ½Ğ° C, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ CodeT5 Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LLM-AuthorBench Ğ¸Ğ· 32 000 ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… C-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ÑĞµĞ¼ÑŒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ LLM. Ğ’ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 97.56% Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¾Ñ‚ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¿ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… LLM - 95.40%.'}, 'en': {'title': 'Unmasking Code: Identifying LLM Authorship with Precision', 'desc': 'The paper introduces CodeT5-Authorship, a new model designed to classify the authorship of C programs generated by Large Language Models (LLMs). It focuses on using only the encoder part of the CodeT5 architecture to enhance classification accuracy. The model achieves impressive results, with 97.56% accuracy in binary classification and 95.40% in multi-class attribution among various LLMs. Additionally, the authors provide a benchmark dataset, LLM-AuthorBench, to facilitate further research in this area.'}, 'zh': {'title': 'è¯†åˆ«AIç”Ÿæˆä»£ç çš„ä½œè€…èº«ä»½æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ¨¡å‹CodeT5-Authorshipï¼Œç”¨äºåˆ†ç±»ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„Cç¨‹åºçš„ä½œè€…èº«ä»½ã€‚è¯¥æ¨¡å‹ä»…ä½¿ç”¨åŸå§‹CodeT5ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„ç¼–ç å™¨å±‚ï¼Œä¸“æ³¨äºåˆ†ç±»ä»»åŠ¡ï¼Œå–å¾—äº†æ¯”ä¼ ç»Ÿå’ŒåŸºäºå˜æ¢å™¨çš„åˆ†ç±»å™¨æ›´é«˜çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†LLM-AuthorBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«32,000ä¸ªå¯ç¼–è¯‘Cç¨‹åºçš„åŸºå‡†ï¼Œæ¶µç›–äº†å…«ç§æœ€å…ˆè¿›çš„LLMã€‚é€šè¿‡ä¸å¤šç§ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨å’Œå¾®è°ƒçš„å˜æ¢å™¨æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18887', 'title': 'Steering Conceptual Bias via Transformer Latent-Subspace Activation', 'url': 'https://huggingface.co/papers/2506.18887', 'abstract': 'A gradient-refined adaptive activation steering framework enables LLMs to reliably generate scientific code in a specific programming language by selectively steering latent subspaces.  \t\t\t\t\tAI-generated summary \t\t\t\t This work examines whether activating latent subspaces in language models (LLMs) can steer scientific code generation toward a specific programming language. Five causal LLMs were first evaluated on scientific coding prompts to quantify their baseline bias among four programming languages. A static neuron-attribution method, perturbing the highest activated MLP weight for a C++ or CPP token, proved brittle and exhibited limited generalization across prompt styles and model scales. To address these limitations, a gradient-refined adaptive activation steering framework (G-ACT) was developed: per-prompt activation differences are clustered into a small set of steering directions, and lightweight per-layer probes are trained and refined online to select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably biases generation towards the CPP language by increasing the average probe classification accuracy by 15% and the early layers (0-6) improving the probe classification accuracy by 61.5% compared to the standard ACT framework. For LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted injections at key layers still improve language selection. Although per-layer probing introduces a modest inference overhead, it remains practical by steering only a subset of layers and enables reproducible model behavior. These results demonstrate a scalable, interpretable and efficient mechanism for concept-level control for practical agentic systems.', 'score': 3, 'issue_id': 4459, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'fb4fc878d44172df', 'authors': ['Vansh Sharma', 'Venkat Raman'], 'affiliations': ['University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2506.18887.jpg', 'data': {'categories': ['#agents', '#plp', '#interpretability', '#training', '#science'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ ĞºĞ¾Ğ´Ğ° Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº G-ACT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ½ÑƒĞ¶Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞ·Ñ‹ĞºĞ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Steering LLMs for Precise Scientific Code Generation', 'desc': "This paper introduces a new framework called gradient-refined adaptive activation steering (G-ACT) to improve the generation of scientific code in specific programming languages using large language models (LLMs). The authors found that traditional methods for steering code generation were not effective across different prompts and model sizes. G-ACT clusters activation differences into steering directions and uses lightweight probes to refine the steering process, leading to significant improvements in code generation accuracy for the C++ language. The results show that this method is scalable and interpretable, allowing for better control over the model's output in practical applications."}, 'zh': {'title': 'å¼•å¯¼æ½œåœ¨ç©ºé—´ï¼Œæå‡ç§‘å­¦ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¢¯åº¦ç²¾ç»†è‡ªé€‚åº”æ¿€æ´»å¼•å¯¼æ¡†æ¶ï¼ˆG-ACTï¼‰ï¼Œæ—¨åœ¨é€šè¿‡é€‰æ‹©æ€§å¼•å¯¼æ½œåœ¨å­ç©ºé—´ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿå¯é åœ°ç”Ÿæˆç‰¹å®šç¼–ç¨‹è¯­è¨€çš„ç§‘å­¦ä»£ç ã€‚ç ”ç©¶é¦–å…ˆè¯„ä¼°äº†äº”ç§å› æœLLMåœ¨ç§‘å­¦ç¼–ç æç¤ºä¸‹çš„åŸºçº¿åå·®ï¼Œå‘ç°é™æ€ç¥ç»å…ƒå½’å› æ–¹æ³•åœ¨ä¸åŒæç¤ºé£æ ¼å’Œæ¨¡å‹è§„æ¨¡ä¸‹è¡¨ç°ä¸ä½³ã€‚G-ACTé€šè¿‡å°†æ¯ä¸ªæç¤ºçš„æ¿€æ´»å·®å¼‚èšç±»ä¸ºä¸€å°ç»„å¼•å¯¼æ–¹å‘ï¼Œå¹¶åœ¨çº¿è®­ç»ƒè½»é‡çº§çš„æ¯å±‚æ¢é’ˆæ¥é€‰æ‹©åˆé€‚çš„å¼•å¯¼å‘é‡ï¼Œä»è€Œå…‹æœäº†è¿™äº›é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨LLaMA-3.2 3Bæ¨¡å‹ä¸­æ˜¾è‘—æé«˜äº†CPPè¯­è¨€ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å¯æ‰©å±•ã€å¯è§£é‡Šå’Œé«˜æ•ˆçš„æ¦‚å¿µçº§æ§åˆ¶æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18787', 'title': '3D Arena: An Open Platform for Generative 3D Evaluation', 'url': 'https://huggingface.co/papers/2506.18787', 'abstract': "3D Arena evaluates generative 3D models using human preferences, revealing insights into visual and textural features' impact on quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating Generative 3D models remains challenging due to misalignment between automated metrics and human perception of quality. Current benchmarks rely on image-based metrics that ignore 3D structure or geometric measures that fail to capture perceptual appeal and real-world utility. To address this gap, we present 3D Arena, an open platform for evaluating image-to-3D generation models through large-scale human preference collection using pairwise comparisons.   Since launching in June 2024, the platform has collected 123,243 votes from 8,096 users across 19 state-of-the-art models, establishing the largest human preference evaluation for Generative 3D. We contribute the iso3d dataset of 100 evaluation prompts and demonstrate quality control achieving 99.75% user authenticity through statistical fraud detection. Our ELO-based ranking system provides reliable model assessment, with the platform becoming an established evaluation resource.   Through analysis of this preference data, we present insights into human preference patterns. Our findings reveal preferences for visual presentation features, with Gaussian splat outputs achieving a 16.6 ELO advantage over meshes and textured models receiving a 144.1 ELO advantage over untextured models. We provide recommendations for improving evaluation methods, including multi-criteria assessment, task-oriented evaluation, and format-aware comparison. The platform's community engagement establishes 3D Arena as a benchmark for the field while advancing understanding of human-centered evaluation in Generative 3D.", 'score': 3, 'issue_id': 4462, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'e3bfc1da0ca3a884', 'authors': ['Dylan Ebert'], 'affiliations': ['Hugging Face'], 'pdf_title_img': 'assets/pdf/title_img/2506.18787.jpg', 'data': {'categories': ['#games', '#dataset', '#3d', '#open_source', '#benchmark'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': '3D Arena - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ° Ğ±Ğ¾Ğ»ĞµĞµ 120 000 Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ² Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 19 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ»ÑĞ´Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': '3D Arena: Elevating Generative 3D Evaluation through Human Preferences', 'desc': 'The paper introduces 3D Arena, a platform designed to evaluate generative 3D models based on human preferences rather than solely relying on automated metrics. It highlights the limitations of current benchmarks that either overlook 3D structure or fail to capture the aesthetic appeal of models. By collecting over 123,000 votes from users, the platform establishes a comprehensive dataset for assessing model quality through pairwise comparisons. The findings reveal significant insights into user preferences, emphasizing the importance of visual features in model evaluation and suggesting improvements for future assessment methods.'}, 'zh': {'title': '3D Arenaï¼šäººç±»åå¥½çš„ç”Ÿæˆ3Dæ¨¡å‹è¯„ä¼°å¹³å°', 'desc': '3D Arena æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ç”Ÿæˆ 3D æ¨¡å‹çš„å¹³å°ï¼Œä¸»è¦é€šè¿‡äººç±»åå¥½æ¥æ­ç¤ºè§†è§‰å’Œçº¹ç†ç‰¹å¾å¯¹è´¨é‡çš„å½±å“ã€‚è¯¥å¹³å°è§£å†³äº†è‡ªåŠ¨åŒ–æŒ‡æ ‡ä¸äººç±»è´¨é‡æ„ŸçŸ¥ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œæä¾›äº†åŸºäºæˆå¯¹æ¯”è¾ƒçš„å¤§è§„æ¨¡äººç±»åå¥½æ”¶é›†ã€‚è‡ª2024å¹´6æœˆæ¨å‡ºä»¥æ¥ï¼Œ3D Arena å·²æ”¶é›†äº†æ¥è‡ª8096åç”¨æˆ·çš„123,243ç¥¨ï¼Œæˆä¸ºç”Ÿæˆ 3D é¢†åŸŸæœ€å¤§çš„äººå·¥åå¥½è¯„ä¼°èµ„æºã€‚é€šè¿‡å¯¹åå¥½æ•°æ®çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°äº†äººç±»åå¥½çš„æ¨¡å¼ï¼Œå¹¶æå‡ºäº†æ”¹è¿›è¯„ä¼°æ–¹æ³•çš„å»ºè®®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17871', 'title': 'How Alignment Shrinks the Generative Horizon', 'url': 'https://huggingface.co/papers/2506.17871', 'abstract': 'The Branching Factor (BF) quantifies the effective number of plausible next steps during generation and reveals how alignment tuning and longer reasoning chains reduce variability in aligned large language models (LLMs).  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model\'s output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model\'s output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model\'s behavior, but instead steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.', 'score': 3, 'issue_id': 4461, 'pub_date': '2025-06-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ½Ñ', 'en': 'June 22', 'zh': '6æœˆ22æ—¥'}, 'hash': '2f0246faa73f4056', 'authors': ['Chenghao Yang', 'Ari Holtzman'], 'affiliations': ['Department of Computer Science, University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2506.17871.jpg', 'data': {'categories': ['#training', '#rlhf', '#alignment', '#interpretability', '#reasoning'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ¤Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ’ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ: ĞºĞ»ÑÑ‡ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¤Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ğ’ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ (BF) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ BF ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑƒĞ¶Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑÑ‚Ğ¾Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼, ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Branching Factor: Understanding Stability in Aligned LLMs', 'desc': 'The paper introduces the Branching Factor (BF), a measure that quantifies the number of plausible next steps in the output of aligned large language models (LLMs). It reveals that as LLMs generate text, their outputs become more predictable, indicated by a decreasing BF. The study shows that alignment tuning significantly sharpens the output distribution, leading to less variability in generated content. Additionally, it highlights how longer reasoning chains in aligned models can enhance stability by pushing generation into more deterministic phases.'}, 'zh': {'title': 'åˆ†æ”¯å› å­ï¼šç†è§£LLMè¾“å‡ºç¨³å®šæ€§çš„å…³é”®', 'desc': 'æœ¬æ–‡ä»‹ç»äº†åˆ†æ”¯å› å­ï¼ˆBFï¼‰ï¼Œå®ƒé‡åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰æ•ˆçš„å¯è¡Œä¸‹ä¸€æ­¥æ•°é‡ï¼Œå¹¶æ­ç¤ºäº†å¯¹é½è°ƒä¼˜å’Œæ›´é•¿æ¨ç†é“¾å¦‚ä½•å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¾“å‡ºå˜å¼‚æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€ç”Ÿæˆçš„è¿›è¡Œï¼ŒBFé€šå¸¸ä¼šä¸‹é™ï¼Œè¡¨æ˜LLMsåœ¨ç”Ÿæˆæ—¶å˜å¾—æ›´åŠ å¯é¢„æµ‹ã€‚å¯¹é½è°ƒä¼˜æ˜¾è‘—æé«˜äº†æ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„é›†ä¸­åº¦ï¼Œä½¿BFç›¸è¾ƒäºåŸºç¡€æ¨¡å‹å‡å°‘äº†è¿‘ä¸€ä¸ªæ•°é‡çº§ã€‚é€šè¿‡è¿™äº›å‘ç°ï¼Œæœ¬æ–‡ä¸ºç†è§£å’Œæ§åˆ¶LLMè¾“å‡ºæä¾›äº†æœ‰åŠ›çš„è¯Šæ–­å·¥å…·ï¼Œé˜æ˜äº†å¯¹é½å¦‚ä½•å‡å°‘å˜å¼‚æ€§ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡ç”Ÿæˆæ›´é•¿çš„æ¨ç†é“¾æ¥ä¿ƒè¿›ç¨³å®šçš„è¾“å‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17818', 'title': 'CultureMERT: Continual Pre-Training for Cross-Cultural Music\n  Representation Learning', 'url': 'https://huggingface.co/papers/2506.17818', 'abstract': 'CultureMERT-95M, a multi-culturally adapted foundation model, enhances cross-cultural music representation learning with a two-stage continual pre-training strategy, demonstrating superior performance in diverse non-Western music tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in music foundation models have improved audio representation learning, yet their effectiveness across diverse musical traditions remains limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation model developed to enhance cross-cultural music representation learning and understanding. To achieve this, we propose a two-stage continual pre-training strategy that integrates learning rate re-warming and re-decaying, enabling stable adaptation even with limited computational resources. Training on a 650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music traditions, results in an average improvement of 4.9% in ROC-AUC and AP across diverse non-Western music auto-tagging tasks, surpassing prior state-of-the-art, with minimal forgetting on Western-centric benchmarks. We further investigate task arithmetic, an alternative approach to multi-cultural adaptation that merges single-culture adapted models in the weight space. Task arithmetic performs on par with our multi-culturally trained model on non-Western auto-tagging tasks and shows no regression on Western datasets. Cross-cultural evaluation reveals that single-culture models transfer with varying effectiveness across musical traditions, whereas the multi-culturally adapted model achieves the best overall performance. To support research on world music representation learning, we publicly release CultureMERT-95M and CultureMERT-TA-95M, fostering the development of more culturally aware music foundation models.', 'score': 3, 'issue_id': 4459, 'pub_date': '2025-06-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ½Ñ', 'en': 'June 21', 'zh': '6æœˆ21æ—¥'}, 'hash': 'ae3ec675bcd28264', 'authors': ['Angelos-Nikolaos Kanatas', 'Charilaos Papaioannou', 'Alexandros Potamianos'], 'affiliations': ['Archimedes, Athena Research Center, Greece', 'Centre for Digital Music, Queen Mary University of London, UK', 'Institute for Language and Speech Processing, Athena Research Center, Greece', 'School of ECE, National Technical University of Athens, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2506.17818.jpg', 'data': {'categories': ['#audio', '#transfer_learning', '#open_source', '#dataset', '#training'], 'emoji': 'ğŸµ', 'ru': {'title': 'CultureMERT-95M: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜', 'desc': 'CultureMERT-95M - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ³Ñ€ĞµĞ²Ğ° Ğ¸ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. CultureMERT-95M Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ»Ñ Ğ½ĞµĞµĞ²Ñ€Ğ¾Ğ¿ĞµĞ¹ÑĞºĞ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ ROC-AUC Ğ¸ AP Ğ½Ğ° 4.9%. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºÑƒ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Cross-Cultural Music Understanding with CultureMERT-95M', 'desc': 'CultureMERT-95M is a foundation model designed to improve music representation learning across different cultures. It uses a two-stage continual pre-training strategy that helps the model adapt to various musical traditions without losing performance on Western music tasks. By training on a diverse dataset of 650 hours of music from Greek, Turkish, and Indian cultures, it achieves better results in auto-tagging tasks compared to previous models. The model also explores task arithmetic, which combines single-culture models, showing that multi-cultural adaptation leads to superior performance in cross-cultural music tasks.'}, 'zh': {'title': 'è·¨æ–‡åŒ–éŸ³ä¹è¡¨ç¤ºå­¦ä¹ çš„æ–°çªç ´', 'desc': 'CultureMERT-95M æ˜¯ä¸€ä¸ªå¤šæ–‡åŒ–é€‚åº”çš„åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡è·¨æ–‡åŒ–éŸ³ä¹è¡¨ç¤ºå­¦ä¹ ã€‚å®ƒé‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„æŒç»­é¢„è®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†å­¦ä¹ ç‡çš„é‡æ–°åŠ çƒ­å’Œè¡°å‡ï¼Œä½¿å¾—æ¨¡å‹åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ä¹Ÿèƒ½ç¨³å®šé€‚åº”ã€‚é€šè¿‡åœ¨650å°æ—¶çš„å¤šæ–‡åŒ–æ•°æ®ä¸Šè®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨éè¥¿æ–¹éŸ³ä¹è‡ªåŠ¨æ ‡è®°ä»»åŠ¡ä¸­å¹³å‡æé«˜äº†4.9%çš„ROC-AUCå’ŒAPï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†ä»»åŠ¡ç®—æœ¯ï¼Œè¿™æ˜¯ä¸€ç§å°†å•æ–‡åŒ–é€‚åº”æ¨¡å‹åˆå¹¶çš„æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨éè¥¿æ–¹ä»»åŠ¡ä¸Šè¡¨ç°ä¸å¤šæ–‡åŒ–æ¨¡å‹ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17939', 'title': 'GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.17939', 'abstract': "A novel dataset and verifiable reward mechanism enhance the explainability and efficiency of medical visual question answering models.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical visual question answering aims to support clinical decision-making by enabling models to answer natural language questions based on medical images. While recent advances in multi-modal learning have significantly improved performance, current methods still suffer from limited answer reliability and poor interpretability, impairing the ability of clinicians and patients to understand and trust model-generated answers. To address this, this work first proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer generation is decomposed into intermediate reasoning steps that explicitly ground relevant visual regions of the medical image, thereby providing fine-grained explainability. Furthermore, we introduce a novel verifiable reward mechanism for reinforcement learning to guide post-training, improving the alignment between the model's reasoning process and its final answer. Remarkably, our method achieves comparable performance using only one-eighth of the training data, demonstrating the efficiency and effectiveness of the proposal. The dataset is available at https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.", 'score': 2, 'issue_id': 4460, 'pub_date': '2025-06-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ½Ñ', 'en': 'June 22', 'zh': '6æœˆ22æ—¥'}, 'hash': '1b27d6b7949f271c', 'authors': ['Bo Liu', 'Xiangyu Zhao', 'Along He', 'Yidi Chen', 'Huazhu Fu', 'Xiao-Ming Wu'], 'affiliations': ['Department of Data Science and Artificial Intelligence, The Hong Kong Polytechnic University, Hong Kong', 'Department of Radiology, West China Hospital of Sichuan University, China', 'Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore', 'National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.17939.jpg', 'data': {'categories': ['#rl', '#alignment', '#healthcare', '#interpretability', '#reasoning', '#dataset', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ThinkVG Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹ Ğº Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 1/8 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'Enhancing Explainability and Efficiency in Medical VQA', 'desc': "This paper introduces a new dataset called Thinking with Visual Grounding (ThinkVG) to improve medical visual question answering (VQA) models. The dataset breaks down the answer generation process into clear reasoning steps that link specific parts of medical images to the answers, enhancing explainability. Additionally, a novel verifiable reward mechanism is proposed for reinforcement learning, which helps align the model's reasoning with its final answers. The approach shows that it can achieve similar performance with significantly less training data, highlighting its efficiency and effectiveness."}, 'zh': {'title': 'æå‡åŒ»å­¦é—®ç­”æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œå¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥å¢å¼ºåŒ»å­¦è§†è§‰é—®ç­”æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ•ˆç‡ã€‚åŒ»å­¦è§†è§‰é—®ç­”æ—¨åœ¨é€šè¿‡ä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®åŒ»å­¦å›¾åƒå›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜æ¥æ”¯æŒä¸´åºŠå†³ç­–ã€‚å½“å‰çš„æ–¹æ³•åœ¨ç­”æ¡ˆçš„å¯é æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ï¼Œå½±å“äº†ä¸´åºŠåŒ»ç”Ÿå’Œæ‚£è€…å¯¹æ¨¡å‹ç”Ÿæˆç­”æ¡ˆçš„ç†è§£å’Œä¿¡ä»»ã€‚é€šè¿‡å¼•å…¥Thinking with Visual Groundingï¼ˆThinkVGï¼‰æ•°æ®é›†å’Œå¼ºåŒ–å­¦ä¹ ä¸­çš„æ–°å¥–åŠ±æœºåˆ¶ï¼Œæœ¬æ–‡æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸æœ€ç»ˆç­”æ¡ˆä¹‹é—´çš„å¯¹é½ï¼Œä¸”ä»…ä½¿ç”¨å…«åˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®å°±è¾¾åˆ°äº†å¯æ¯”çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16929', 'title': 'A deep learning and machine learning approach to predict neonatal death\n  in the context of SÃ£o Paulo', 'url': 'https://huggingface.co/papers/2506.16929', 'abstract': 'Deep learning, specifically LSTM, outperforms other machine learning techniques in predicting neonatal mortality using historical data.  \t\t\t\t\tAI-generated summary \t\t\t\t Neonatal death is still a concerning reality for underdeveloped and even some developed countries. Worldwide data indicate that 26.693 babies out of 1,000 births die, according to Macro Trades. To reduce this number, early prediction of endangered babies is crucial. Such prediction enables the opportunity to take ample care of the child and mother so that early child death can be avoided. In this context, machine learning was used to determine whether a newborn baby is at risk. To train the predictive model, historical data of 1.4 million newborns was used. Machine learning and deep learning techniques such as logical regression, K-nearest neighbor, random forest classifier, extreme gradient boosting (XGBoost), convolutional neural network, and long short-term memory (LSTM) were implemented using the dataset to identify the most accurate model for predicting neonatal mortality. Among the machine learning algorithms, XGBoost and random forest classifier achieved the best accuracy with 94%, while among the deep learning models, LSTM delivered the highest accuracy with 99%. Therefore, using LSTM appears to be the most suitable approach to predict whether precautionary measures for a child are necessary.', 'score': 2, 'issue_id': 4455, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': '0bc1d033846c482b', 'authors': ['Mohon Raihan', 'Plabon Kumar Saha', 'Rajan Das Gupta', 'A Z M Tahmidul Kabir', 'Afia Anjum Tamanna', 'Md. Harun-Ur-Rashid', 'Adnan Bin Abdus Salam', 'Md Tanvir Anjum', 'A Z M Ahteshamul Kabir'], 'affiliations': ['Department of Computer Science and Engineering, Faculty of Engineering and Technology, University of Dhaka, Dhaka, Bangladesh', 'Department of Computer Science and Engineering, Faculty of Science & Engineering, United International University-Bangladesh, Dhaka, Bangladesh', 'Department of Computer Science and Engineering, Faculty of Science and Technology, American International University-Bangladesh, Dhaka, Bangladesh', 'Department of Computer Science and Software Engineering, Faculty of Science and Technology, American International University-Bangladesh, Dhaka, Bangladesh', 'Department of Computer Science, Faculty of Mathematical & Physical Sciences, Jahangirnagar University, Dhaka, Bangladesh', 'Department of Electrical and Electronic Engineering, Faculty of Engineering, American International University-Bangladesh, Dhaka, Bangladesh', 'Department of Predictive Analytics, Faculty of Science and Engineering, Curtin University, Perth, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2506.16929.jpg', 'data': {'categories': ['#healthcare', '#dataset', '#training', '#data'], 'emoji': 'ğŸ‘¶', 'ru': {'title': 'LSTM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞ¾Ğ½Ğ°Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¼ĞµÑ€Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ½Ğ°Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¼ĞµÑ€Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ 1,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ½Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ, ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ Ğ»ĞµÑ, XGBoost Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸. Ğ¡Ñ€ĞµĞ´Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² 94% Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ XGBoost Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ Ğ»ĞµÑ. ĞĞ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LSTM Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ 99%, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ²ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ€.'}, 'en': {'title': 'LSTM: The Key to Predicting Neonatal Survival', 'desc': 'This paper discusses the use of machine learning, particularly Long Short-Term Memory (LSTM) networks, to predict neonatal mortality using historical data from 1.4 million newborns. The study compares various machine learning techniques, including logistic regression, K-nearest neighbors, random forest, and XGBoost, to identify the most effective model for this prediction task. Results show that while XGBoost and random forest achieved 94% accuracy, LSTM outperformed all other models with an impressive 99% accuracy. The findings highlight the importance of early prediction in reducing neonatal deaths by enabling timely interventions for at-risk infants.'}, 'zh': {'title': 'LSTMï¼šæ–°ç”Ÿå„¿æ­»äº¡ç‡é¢„æµ‹çš„æœ€ä½³é€‰æ‹©', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰åœ¨é¢„æµ‹æ–°ç”Ÿå„¿æ­»äº¡ç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚é€šè¿‡åˆ†æ140ä¸‡åæ–°ç”Ÿå„¿çš„å†å²æ•°æ®ï¼Œç ”ç©¶ä½¿ç”¨äº†å¤šç§æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼ŒåŒ…æ‹¬é€»è¾‘å›å½’ã€Kè¿‘é‚»ã€éšæœºæ£®æ—åˆ†ç±»å™¨å’Œæç«¯æ¢¯åº¦æå‡ï¼ˆXGBoostï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLSTMæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡é«˜è¾¾99%ï¼Œè¿œè¶…å…¶ä»–ç®—æ³•ã€‚å› æ­¤ï¼ŒLSTMè¢«è®¤ä¸ºæ˜¯é¢„æµ‹æ–°ç”Ÿå„¿æ˜¯å¦éœ€è¦é‡‡å–é¢„é˜²æªæ–½çš„æœ€ä½³æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15645', 'title': 'Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models', 'url': 'https://huggingface.co/papers/2506.15645', 'abstract': "Visual-Quality Test-Time Tuning (VQ-TTT) improves Multimodal Large Language Models (MLLMs) performance on vision-language tasks by dynamically adjusting input images, demonstrating the importance of adaptive rather than universally clean imagery.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer.", 'score': 2, 'issue_id': 4463, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '2261039639b43504', 'authors': ['Shuo Xing', 'Lanqing Guo', 'Hongyuan Hua', 'Seoyoung Lee', 'Peiran Li', 'Yufei Wang', 'Zhangyang Wang', 'Zhengzhong Tu'], 'affiliations': ['Nanyang Technological University', 'Texas A&M University', 'University of Texas at Austin', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.15645.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#cv', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°: Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒÑÑ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Visual-Quality Test-Time Tuning (VQ-TTT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. VQ-TTT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ MLLM Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Adaptive Imagery: Enhancing MLLM Performance with VQ-TTT', 'desc': 'This paper introduces Visual-Quality Test-Time Tuning (VQ-TTT), a method that enhances the performance of Multimodal Large Language Models (MLLMs) on vision-language tasks by adapting the quality of input images. The study reveals that images with lower perceptual quality can sometimes lead to better model understanding, challenging the assumption that higher quality always yields better results. VQ-TTT works by applying a learnable kernel to modify the frequency content of images and fine-tuning specific layers of the vision encoder, allowing for dynamic adjustments during inference. The results show that VQ-TTT significantly improves accuracy across various MLLMs and datasets, emphasizing the importance of task-specific image quality over universally clean images.'}, 'zh': {'title': 'é€‚åº”æ€§è°ƒæ•´å›¾åƒè´¨é‡ï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹è¡¨ç°', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè§†è§‰è´¨é‡æµ‹è¯•æ—¶è°ƒä¼˜ï¼ˆVQ-TTTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¾“å…¥å›¾åƒçš„é€‚åº”æ€§è°ƒæ•´æ¯”å•çº¯è¿½æ±‚å›¾åƒçš„æ¸…æ™°åº¦æ›´ä¸ºé‡è¦ã€‚é€šè¿‡å¯¹å›¾åƒè¿›è¡Œæ§åˆ¶æ€§é™è´¨å’Œé£æ ¼å˜åŒ–ï¼Œå‘ç°æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹å¯¹ä½äºäººç±»æ„ŸçŸ¥çš„å›¾åƒè´¨é‡åè€Œè¡¨ç°æ›´å¥½ã€‚VQ-TTTé€šè¿‡åœ¨å†»ç»“çš„è§†è§‰ç¼–ç å™¨å‰æ’å…¥å¯å­¦ä¹ çš„ä½ç§©æ ¸ï¼Œå¹¶ä»…å¾®è°ƒæµ…å±‚è§†è§‰ç¼–ç å™¨å±‚ï¼ŒåŠ¨æ€è°ƒæ•´æ¯ä¸ªè¾“å…¥å›¾åƒï¼Œä»¥ç¬¦åˆç‰¹å®šä»»åŠ¡çš„æ¨¡å‹åå¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10597', 'title': 'SoK: Evaluating Jailbreak Guardrails for Large Language Models', 'url': 'https://huggingface.co/papers/2506.10597', 'abstract': 'A systematic analysis and evaluation framework for jailbreak guardrails in Large Language Models is presented, categorizing and assessing their effectiveness and optimization potential.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable progress, but their deployment has exposed critical vulnerabilities, particularly to jailbreak attacks that circumvent safety mechanisms. Guardrails--external defense mechanisms that monitor and control LLM interaction--have emerged as a promising solution. However, the current landscape of LLM guardrails is fragmented, lacking a unified taxonomy and comprehensive evaluation framework. In this Systematization of Knowledge (SoK) paper, we present the first holistic analysis of jailbreak guardrails for LLMs. We propose a novel, multi-dimensional taxonomy that categorizes guardrails along six key dimensions, and introduce a Security-Efficiency-Utility evaluation framework to assess their practical effectiveness. Through extensive analysis and experiments, we identify the strengths and limitations of existing guardrail approaches, explore their universality across attack types, and provide insights into optimizing defense combinations. Our work offers a structured foundation for future research and development, aiming to guide the principled advancement and deployment of robust LLM guardrails. The code is available at https://github.com/xunguangwang/SoK4JailbreakGuardrails.', 'score': 2, 'issue_id': 4448, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 Ğ¸ÑĞ½Ñ', 'en': 'June 12', 'zh': '6æœˆ12æ—¥'}, 'hash': '4122cc84dd4333e8', 'authors': ['Xunguang Wang', 'Zhenlan Ji', 'Wenxuan Wang', 'Zongjie Li', 'Daoyuan Wu', 'Shuai Wang'], 'affiliations': ['Renmin University of China', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.10597.jpg', 'data': {'categories': ['#data', '#benchmark', '#security', '#optimization'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ LLM Ğ¾Ñ‚ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² (Ğ³Ğ°Ñ€Ğ´Ñ€ĞµĞ¹Ğ»Ğ¾Ğ²) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ‚Ğ¸Ğ¿Ğ° jailbreak. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ³Ğ°Ñ€Ğ´Ñ€ĞµĞ¹Ğ»Ñ‹ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼. Ğ’Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Security-Efficiency-Utility Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ LLM, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ€.'}, 'en': {'title': 'Strengthening LLMs: A New Framework for Jailbreak Guardrails', 'desc': 'This paper presents a comprehensive framework for analyzing and evaluating guardrails designed to protect Large Language Models (LLMs) from jailbreak attacks. It introduces a multi-dimensional taxonomy that categorizes these guardrails based on six important aspects, helping to clarify their roles and effectiveness. Additionally, the authors propose a new evaluation framework that balances security, efficiency, and utility, allowing for a thorough assessment of guardrail performance. By identifying the strengths and weaknesses of current approaches, this work aims to enhance the development of more effective defenses for LLMs against potential vulnerabilities.'}, 'zh': {'title': 'ç³»ç»ŸåŒ–è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„è¶Šç‹±é˜²æŠ¤æœºåˆ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„åˆ†æå’Œè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è¶Šç‹±é˜²æŠ¤æœºåˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡LLMså–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æš´éœ²äº†å…³é”®çš„å®‰å…¨æ¼æ´ï¼Œå°¤å…¶æ˜¯è¶Šç‹±æ”»å‡»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šç»´åˆ†ç±»æ³•ï¼Œå°†é˜²æŠ¤æœºåˆ¶åˆ†ä¸ºå…­ä¸ªå…³é”®ç»´åº¦ï¼Œå¹¶å¼•å…¥äº†å®‰å…¨æ€§ã€æ•ˆç‡å’Œå®ç”¨æ€§è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°å…¶å®é™…æ•ˆæœã€‚é€šè¿‡å¹¿æ³›çš„åˆ†æå’Œå®éªŒï¼Œæˆ‘ä»¬è¯†åˆ«äº†ç°æœ‰é˜²æŠ¤æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†ç»“æ„åŒ–çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18900', 'title': 'Audit & Repair: An Agentic Framework for Consistent Story Visualization\n  in Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2506.18900', 'abstract': 'A collaborative multi-agent framework improves consistency in multi-panel story visualizations by refining inconsistencies across panels using diffusion models like Flux and Stable Diffusion.  \t\t\t\t\tAI-generated summary \t\t\t\t Story visualization has become a popular task where visual scenes are generated to depict a narrative across multiple panels. A central challenge in this setting is maintaining visual consistency, particularly in how characters and objects persist and evolve throughout the story. Despite recent advances in diffusion models, current approaches often fail to preserve key character attributes, leading to incoherent narratives. In this work, we propose a collaborative multi-agent framework that autonomously identifies, corrects, and refines inconsistencies across multi-panel story visualizations. The agents operate in an iterative loop, enabling fine-grained, panel-level updates without re-generating entire sequences. Our framework is model-agnostic and flexibly integrates with a variety of diffusion models, including rectified flow transformers such as Flux and latent diffusion models such as Stable Diffusion. Quantitative and qualitative experiments show that our method outperforms prior approaches in terms of multi-panel consistency.', 'score': 1, 'issue_id': 4461, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'a2463a13521e764c', 'authors': ['Kiymet Akdemir', 'Tahira Kazimi', 'Pinar Yanardag'], 'affiliations': ['Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2506.18900.jpg', 'data': {'categories': ['#cv', '#multimodal', '#diffusion', '#story_generation', '#agents'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ°Ğ½ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ğ½ĞµĞ»ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Flux Ğ¸ Stable Diffusion. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ğ½ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Story Visualizations with Collaborative Agents', 'desc': 'This paper presents a collaborative multi-agent framework designed to enhance visual consistency in multi-panel story visualizations. The framework utilizes diffusion models, such as Flux and Stable Diffusion, to autonomously identify and correct inconsistencies in character and object attributes across panels. By operating in an iterative loop, the agents can make precise updates to individual panels without needing to regenerate the entire sequence. Experimental results demonstrate that this approach significantly improves consistency compared to previous methods.'}, 'zh': {'title': 'åä½œæ™ºèƒ½ä½“æå‡å¤šé¢æ¿æ•…äº‹å¯è§†åŒ–ä¸€è‡´æ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä½œå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºæ”¹å–„å¤šé¢æ¿æ•…äº‹å¯è§†åŒ–ä¸­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Fluxå’ŒStable Diffusionï¼‰è‡ªåŠ¨è¯†åˆ«å¹¶ä¿®æ­£é¢æ¿ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡è¿­ä»£å¾ªç¯ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ä¸é‡æ–°ç”Ÿæˆæ•´ä¸ªåºåˆ—çš„æƒ…å†µä¸‹è¿›è¡Œç»†ç²’åº¦çš„é¢æ¿çº§æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¢æ¿ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18890', 'title': '4D-LRM: Large Space-Time Reconstruction Model From and To Any View at\n  Any Time', 'url': 'https://huggingface.co/papers/2506.18890', 'abstract': '4D-LRM is a large-scale model that efficiently reconstructs objects from multiple views and times into any view-time combination using space-time representations and Gaussian primitives.  \t\t\t\t\tAI-generated summary \t\t\t\t Can we scale 4D pretraining to learn general space-time representations that reconstruct an object from a few views at some times to any view at any time? We provide an affirmative answer with 4D-LRM, the first large-scale 4D reconstruction model that takes input from unconstrained views and timestamps and renders arbitrary novel view-time combinations. Unlike prior 4D approaches, e.g., optimization-based, geometry-based, or generative, that struggle with efficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time representation and directly predicts per-pixel 4D Gaussian primitives from posed image tokens across time, enabling fast, high-quality rendering at, in principle, infinite frame rate. Our results demonstrate that scaling spatiotemporal pretraining enables accurate and efficient 4D reconstruction. We show that 4D-LRM generalizes to novel objects, interpolates across time, and handles diverse camera setups. It reconstructs 24-frame sequences in one forward pass with less than 1.5 seconds on a single A100 GPU.', 'score': 1, 'issue_id': 4467, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'debfa6a0f0726863', 'authors': ['Ziqiao Ma', 'Xuweiyi Chen', 'Shoubin Yu', 'Sai Bi', 'Kai Zhang', 'Chen Ziwen', 'Sihan Xu', 'Jianing Yang', 'Zexiang Xu', 'Kalyan Sunkavalli', 'Mohit Bansal', 'Joyce Chai', 'Hao Tan'], 'affiliations': ['Adobe Research', 'Oregon State University', 'UNC Chapel Hill', 'University of Michigan', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2506.18890.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv'], 'emoji': 'ğŸ•’', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 4D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğº Ğ»ÑĞ±Ğ¾Ğ¼Ñƒ Ñ€Ğ°ĞºÑƒÑ€ÑÑƒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': '4D-LRM - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ² Ğ»ÑĞ±ÑƒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´-Ğ²Ñ€ĞµĞ¼Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. 4D-LRM Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ 4D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸Ğ· Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€.'}, 'en': {'title': 'Revolutionizing 4D Object Reconstruction with 4D-LRM', 'desc': 'The 4D-LRM model is designed to reconstruct objects from various viewpoints and times, allowing for any combination of view and time. It utilizes space-time representations and Gaussian primitives to achieve efficient and high-quality rendering. Unlike previous methods, 4D-LRM learns a unified representation that can predict detailed 4D structures directly from image tokens, making it faster and more versatile. The model demonstrates strong generalization capabilities, handling different objects and camera setups while producing 24-frame sequences quickly on modern hardware.'}, 'zh': {'title': 'é«˜æ•ˆçš„4Dé‡å»ºï¼šä»ä»»æ„è§†è§’å’Œæ—¶é—´ç”Ÿæˆç‰©ä½“', 'desc': '4D-LRMæ˜¯ä¸€ç§å¤§è§„æ¨¡æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°ä»å¤šä¸ªè§†è§’å’Œæ—¶é—´é‡å»ºç‰©ä½“ï¼Œå¹¶ç”Ÿæˆä»»æ„è§†è§’å’Œæ—¶é—´çš„ç»„åˆã€‚è¯¥æ¨¡å‹ä½¿ç”¨æ—¶ç©ºè¡¨ç¤ºå’Œé«˜æ–¯åŸè¯­ï¼Œç›´æ¥ä»å›¾åƒæ ‡è®°ä¸­é¢„æµ‹æ¯ä¸ªåƒç´ çš„4Dé«˜æ–¯åŸè¯­ã€‚ä¸ä¹‹å‰çš„4Dæ–¹æ³•ç›¸æ¯”ï¼Œ4D-LRMåœ¨æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›å’ŒçœŸå®æ„Ÿæ–¹é¢è¡¨ç°æ›´ä½³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ4D-LRMèƒ½å¤Ÿå¿«é€Ÿã€å‡†ç¡®åœ°é‡å»ºç‰©ä½“ï¼Œå¹¶åœ¨å•ä¸ªA100 GPUä¸Šä»¥ä¸åˆ°1.5ç§’çš„æ—¶é—´å®Œæˆ24å¸§åºåˆ—çš„é‡å»ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17671', 'title': 'TPTT: Transforming Pretrained Transformer into Titans', 'url': 'https://huggingface.co/papers/2506.17671', 'abstract': 'TPTT enhances large language models with efficient linearized attention and advanced memory management, improving both efficiency and accuracy for long-context inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have led to remarkable progress in natural language processing, but their computational and memory demands remain a significant challenge, particularly for long-context inference. We introduce TPTT (Transforming Pretrained Transformer into Titans), a novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management. TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA). It is fully compatible with the Hugging Face Transformers library, enabling seamless adaptation of any causal LLM through parameter-efficient fine-tuning (LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU benchmark with models of approximately 1 billion parameters, observing substantial improvements in both efficiency and accuracy. For instance, Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT. Code is available at https://github.com/fabienfrfr/tptt . Python package at https://pypi.org/project/tptt/ .', 'score': 1, 'issue_id': 4460, 'pub_date': '2025-06-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ½Ñ', 'en': 'June 21', 'zh': '6æœˆ21æ—¥'}, 'hash': '3c74ff95efb8cfe2', 'authors': ['Fabien Furfaro'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.17671.jpg', 'data': {'categories': ['#long_context', '#inference', '#architecture', '#small_models', '#training', '#benchmark', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'TPTT: ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ² Ñ‚Ğ¸Ñ‚Ğ°Ğ½Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'TPTT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ĞºĞ°Ğº Memory as Gate (MaG) Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (LiZA). TPTT Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¾Ğ¹ Hugging Face Transformers Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±ÑƒÑ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMLU Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Transforming Transformers for Efficient Long-Context Inference', 'desc': 'TPTT, or Transforming Pretrained Transformer into Titans, is a new framework designed to improve large language models (LLMs) by using efficient linearized attention and advanced memory management techniques. It introduces methods like Memory as Gate (MaG) and mixed linearized attention (LiZA) to enhance performance during long-context inference. The framework is compatible with the Hugging Face Transformers library, allowing for easy fine-tuning of causal LLMs without the need for complete retraining. Results show that TPTT significantly boosts both efficiency and accuracy, achieving a 20% increase in Exact Match on the MMLU benchmark with a model of about 1 billion parameters.'}, 'zh': {'title': 'é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ–°çªç ´', 'desc': 'TPTTï¼ˆå°†é¢„è®­ç»ƒå˜æ¢å™¨è½¬å˜ä¸ºå·¨äººï¼‰æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é«˜æ•ˆçš„çº¿æ€§åŒ–æ³¨æ„åŠ›æœºåˆ¶å’Œå…ˆè¿›çš„å†…å­˜ç®¡ç†æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†å†…å­˜ä½œä¸ºé—¨æ§ï¼ˆMaGï¼‰å’Œæ··åˆçº¿æ€§åŒ–æ³¨æ„åŠ›ï¼ˆLiZAï¼‰ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚TPTTä¸Hugging Face Transformersåº“å®Œå…¨å…¼å®¹ï¼Œæ”¯æŒé€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAï¼‰è½»æ¾é€‚åº”ä»»ä½•å› æœè¯­è¨€æ¨¡å‹ï¼Œè€Œæ— éœ€å®Œå…¨é‡è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTPTTåœ¨MMLUåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œ1äº¿å‚æ•°çš„æ¨¡å‹Titans-Llama-3.2-1Bçš„å‡†ç¡®ç‡æé«˜äº†20%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13905', 'title': 'Spec2RTL-Agent: Automated Hardware Code Generation from Complex\n  Specifications Using LLM Agent Systems', 'url': 'https://huggingface.co/papers/2506.13905', 'abstract': 'Spec2RTL-Agent, a multi-agent system, automates RTL code generation from complex specifications by improving correctness and reducing human intervention.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in generating hardware RTL code with LLMs, existing solutions still suffer from a substantial gap between practical application scenarios and the requirements of real-world RTL code development. Prior approaches either focus on overly simplified hardware descriptions or depend on extensive human guidance to process complex specifications, limiting their scalability and automation potential. In this paper, we address this gap by proposing an LLM agent system, termed Spec2RTL-Agent, designed to directly process complex specification documentation and generate corresponding RTL code implementations, advancing LLM-based RTL code generation toward more realistic application settings. To achieve this goal, Spec2RTL-Agent introduces a novel multi-agent collaboration framework that integrates three key enablers: (1) a reasoning and understanding module that translates specifications into structured, step-by-step implementation plans; (2) a progressive coding and prompt optimization module that iteratively refines the code across multiple representations to enhance correctness and synthesisability for RTL conversion; and (3) an adaptive reflection module that identifies and traces the source of errors during generation, ensuring a more robust code generation flow. Instead of directly generating RTL from natural language, our system strategically generates synthesizable C++ code, which is then optimized for HLS. This agent-driven refinement ensures greater correctness and compatibility compared to naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three specification documents, showing it generates accurate RTL code with up to 75% fewer human interventions than existing methods. This highlights its role as the first fully automated multi-agent system for RTL generation from unstructured specs, reducing reliance on human effort in hardware design.', 'score': 1, 'issue_id': 4466, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'c23d5bbb3875086a', 'authors': ['Zhongzhi Yu', 'Mingjie Liu', 'Michael Zimmer', 'Yingyan Celine Lin', 'Yong Liu', 'Haoxing Ren'], 'affiliations': ['Cadence San Jose, CA', 'Georgia Institute of Technology Atlanta, GA', 'Nvidia Research Austin, TX'], 'pdf_title_img': 'assets/pdf/title_img/2506.13905.jpg', 'data': {'categories': ['#optimization', '#architecture', '#reasoning', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ RTL: Ğ¾Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğº ĞºĞ¾Ğ´Ñƒ Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Spec2RTL-Agent - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ RTL-ĞºĞ¾Ğ´Ğ° Ğ¸Ğ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ C++ ĞºĞ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ HLS, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ RTL. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Spec2RTL-Agent Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾ 75% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Automating RTL Code Generation with Spec2RTL-Agent', 'desc': 'Spec2RTL-Agent is a multi-agent system that automates the generation of RTL code from complex specifications, enhancing correctness while minimizing human involvement. It addresses the limitations of previous methods that either oversimplified hardware descriptions or required extensive human guidance. The system employs a collaborative framework with modules for reasoning, coding optimization, and error reflection, allowing it to produce synthesizable C++ code that is optimized for high-level synthesis (HLS). Evaluation results demonstrate that Spec2RTL-Agent can generate accurate RTL code with significantly reduced human intervention, marking a significant advancement in automated hardware design.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ– RTL ä»£ç ç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'Spec2RTL-Agent æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨ä»å¤æ‚çš„è§„æ ¼æ–‡æ¡£ä¸­è‡ªåŠ¨ç”Ÿæˆ RTL ä»£ç ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¼•å…¥æ¨ç†ä¸ç†è§£æ¨¡å—ã€æ¸è¿›ç¼–ç ä¸æç¤ºä¼˜åŒ–æ¨¡å—ä»¥åŠè‡ªé€‚åº”åæ€æ¨¡å—ï¼Œæ˜¾è‘—æé«˜äº†ä»£ç ç”Ÿæˆçš„æ­£ç¡®æ€§å¹¶å‡å°‘äº†äººå·¥å¹²é¢„ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSpec2RTL-Agent èƒ½å¤Ÿç›´æ¥å¤„ç†å¤æ‚çš„è§„æ ¼ï¼Œç”Ÿæˆå¯åˆæˆçš„ C++ ä»£ç ï¼Œè¿›è€Œä¼˜åŒ–ä¸º HLSã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ç”Ÿæˆ RTL ä»£ç æ—¶ï¼Œäººå·¥å¹²é¢„å‡å°‘äº†å¤šè¾¾ 75%ï¼Œæ ‡å¿—ç€å…¶åœ¨ç¡¬ä»¶è®¾è®¡ä¸­çš„è‡ªåŠ¨åŒ–æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18369', 'title': 'RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language\n  Models', 'url': 'https://huggingface.co/papers/2506.18369', 'abstract': 'A reinforcement learning-based post-training framework improves the personalized image captioning capabilities of multi-modal large language models compared to supervised fine-tuning methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. In this work, we observe that such limitations persist in existing post-training-based MLLM personalization methods. Specifically, despite being post-tuned with large-scale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning. However, acquiring large-scale, high-quality captions for such complex settings is both costly and difficult. To address the data-centric nature of SFT, we propose a reinforcement learning (RL)-based post-training framework. To the best of our knowledge, this is the first RL-based approach to post-train MLLMs for personalized image captioning. Our method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multi-concept image captioning task.', 'score': 0, 'issue_id': 4467, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': '64f17edd5c9878b1', 'authors': ['Yeongtak Oh', 'Jisoo Mok', 'Dohyun Chung', 'Juhyeon Shin', 'Sangha Park', 'Johan Barthelemy', 'Sungroh Yoon'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University', 'Department of Future Automotive Mobility, Seoul National University', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2506.18369.jpg', 'data': {'categories': ['#multimodal', '#training', '#rl', '#games', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ MLLM. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Personalized Image Captioning', 'desc': "This paper introduces a novel reinforcement learning (RL) framework to enhance personalized image captioning in multi-modal large language models (MLLMs). Traditional supervised fine-tuning (SFT) methods often fail to generate accurate captions for complex images, as they rely heavily on large-scale caption datasets that are hard to obtain. The proposed RL-based approach improves the models' ability to recognize visual elements and generate tailored captions, outperforming existing SFT methods. This work represents a significant advancement in the field of personalized image captioning, particularly for challenging scenarios involving multiple concepts."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡ä¸ªæ€§åŒ–å›¾åƒæè¿°èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ªæ€§åŒ–å›¾åƒæè¿°æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨ç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒæ ‡é¢˜æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¤šæ¦‚å¿µå›¾åƒæè¿°ä¸­ã€‚é€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„è§†è§‰è¯†åˆ«å’Œä¸ªæ€§åŒ–ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæ¦‚å¿µå›¾åƒæè¿°ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06751', 'title': 'Geopolitical biases in LLMs: what are the "good" and the "bad" countries\n  according to contemporary language models', 'url': 'https://huggingface.co/papers/2506.06751', 'abstract': "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper evaluates geopolitical biases in LLMs with respect to various countries though an analysis of their interpretation of historical events with conflicting national perspectives (USA, UK, USSR, and China). We introduce a novel dataset with neutral event descriptions and contrasting viewpoints from different countries. Our findings show significant geopolitical biases, with models favoring specific national narratives. Additionally, simple debiasing prompts had a limited effect in reducing these biases. Experiments with manipulated participant labels reveal models' sensitivity to attribution, sometimes amplifying biases or recognizing inconsistencies, especially with swapped labels. This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and offers a framework and dataset for future geopolitical bias research.", 'score': 52, 'issue_id': 4234, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ½Ñ', 'en': 'June 7', 'zh': '6æœˆ7æ—¥'}, 'hash': '87a1fbaf018382d4', 'authors': ['Mikhail Salnikov', 'Dmitrii Korzh', 'Ivan Lazichny', 'Elvir Karimov', 'Artyom Iudin', 'Ivan Oseledets', 'Oleg Y. Rogov', 'Alexander Panchenko', 'Natalia Loukachevitch', 'Elena Tutubalina'], 'affiliations': ['AIRI', 'Kazan Federal University', 'Lomonosov MSU', 'MIPT', 'MTUCI', 'Sber AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2506.06751.jpg', 'data': {'categories': ['#ethics', '#alignment', '#data', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM: Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ°Ğ¼. ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ° Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ Ğ¼Ğ°Ğ»Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Uncovering Geopolitical Biases in Language Models', 'desc': 'This paper investigates the presence of geopolitical biases in large language models (LLMs) by analyzing how they interpret historical events from different national perspectives, specifically focusing on the USA, UK, USSR, and China. It introduces a new dataset that contains neutral descriptions of events alongside varying viewpoints from these countries to facilitate further research. The results indicate that LLMs tend to favor certain national narratives, demonstrating significant biases in their outputs. Additionally, the study finds that basic debiasing techniques are not very effective in mitigating these biases, suggesting a need for more robust methods in future research.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„åœ°ç¼˜æ”¿æ²»åè§', 'desc': 'æœ¬è®ºæ–‡è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£é‡Šå†å²äº‹ä»¶æ—¶çš„åœ°ç¼˜æ”¿æ²»åè§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç¾å›½ã€è‹±å›½ã€è‹è”å’Œä¸­å›½ç­‰å›½å®¶çš„ä¸åŒè§†è§’ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…å«ä¸­ç«‹çš„äº‹ä»¶æè¿°å’Œæ¥è‡ªä¸åŒå›½å®¶çš„å¯¹ç«‹è§‚ç‚¹ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹å€¾å‘äºæ”¯æŒç‰¹å®šå›½å®¶çš„å™äº‹ï¼Œä¸”ç®€å•çš„å»åè§æ–¹æ³•æ•ˆæœæœ‰é™ã€‚é€šè¿‡æ“æ§å‚ä¸è€…æ ‡ç­¾çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹å¯¹å½’å› éå¸¸æ•æ„Ÿï¼Œæœ‰æ—¶ä¼šæ”¾å¤§åè§æˆ–è¯†åˆ«ä¸ä¸€è‡´ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡ç­¾äº¤æ¢çš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09040', 'title': 'Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better', 'url': 'https://huggingface.co/papers/2506.09040', 'abstract': 'Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.', 'score': 25, 'issue_id': 4237, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '09d042607d92f156', 'authors': ['Dianyi Wang', 'Wei Song', 'Yikun Wang', 'Siyuan Wang', 'Kaicheng Yu', 'Zhongyu Wei', 'Jiaqi Wang'], 'affiliations': ['AutoLab, Westlake University', 'Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute', 'University of Southern California', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09040.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#benchmark', '#games', '#cv', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ (ASVR) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ° Ğ½Ğµ Ğ½Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ¿ÑƒÑ‰ĞµĞ½Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑÑ…. ASVR Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Multimodal Understanding with Semantic Focus', 'desc': 'The paper introduces Autoregressive Semantic Visual Reconstruction (ASVR), a method that enhances multimodal understanding by focusing on reconstructing the semantic content of images rather than their raw visual appearance. This approach addresses limitations in existing large vision-language models (LVLMs) that primarily rely on textual sequences, which can lead to missing critical visual details. By enabling joint learning of visual and textual modalities, ASVR allows models to effectively reconstruct discrete semantic tokens from continuous image features, leading to improved comprehension. The results show significant performance gains across various benchmarks, demonstrating the effectiveness of semantic reconstruction in multimodal tasks.'}, 'zh': {'title': 'è‡ªå›å½’è¯­ä¹‰é‡å»ºï¼Œæå‡å¤šæ¨¡æ€ç†è§£ï¼', 'desc': 'è‡ªå›å½’è¯­ä¹‰è§†è§‰é‡å»ºï¼ˆASVRï¼‰é€šè¿‡å…³æ³¨è¯­ä¹‰é‡å»ºè€ŒéåŸå§‹è§†è§‰å¤–è§‚ï¼Œæå‡äº†å¤šæ¨¡æ€ç†è§£çš„èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä»…å¯¹æ–‡æœ¬åºåˆ—åº”ç”¨è‡ªå›å½’ç›‘ç£ï¼Œæœªèƒ½å……åˆ†æ•´åˆè§†è§‰æ¨¡æ€ï¼Œå¯¼è‡´æ— æ³•åˆ©ç”¨æ²¡æœ‰é…å¥—è¯´æ˜çš„å›¾åƒã€‚ASVR é€šè¿‡åœ¨ç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶å†…å®ç°è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„è”åˆå­¦ä¹ ï¼Œå…‹æœäº†è¿™ä¸€é™åˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé‡å»ºå›¾åƒçš„è¯­ä¹‰è¡¨ç¤ºèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤šæ¨¡æ€ç†è§£çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08672', 'title': 'RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling', 'url': 'https://huggingface.co/papers/2506.08672', 'abstract': 'RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.  \t\t\t\t\tAI-generated summary \t\t\t\t Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1% average points on eight ID tasks and Delta10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.', 'score': 23, 'issue_id': 4239, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'f9f7805577b3b091', 'authors': ['Yang Liu', 'Jiaqi Li', 'Zilong Zheng'], 'affiliations': ['NLCo Lab, Beijing Institute for General Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2506.08672.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#rl', '#optimization', '#small_models', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'RuleReasoner - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ÑÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. RuleReasoner Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Boosting Small Models with Dynamic Domain Sampling', 'desc': 'RuleReasoner is a method that improves rule-based reasoning in small models by using dynamic domain sampling. It addresses the challenges posed by varying rule formats and complexities in real-world applications. By updating sampling weights based on historical rewards, RuleReasoner enhances the learning process and generalization across different tasks. Empirical results show that it significantly outperforms large reasoning models while being more computationally efficient.'}, 'zh': {'title': 'å°æ¨¡å‹çš„è§„åˆ™æ¨ç†æ–°çªç ´', 'desc': 'RuleReasoneræ˜¯ä¸€ç§å¢å¼ºå°å‹æ¨¡å‹è§„åˆ™æ¨ç†çš„æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€é¢†åŸŸé‡‡æ ·å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–è§„åˆ™æ¨ç†ï¼Œè§£å†³äº†ç°å®åº”ç”¨ä¸­è§„åˆ™æ ¼å¼å’Œå¤æ‚æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é€šè¿‡æ›´æ–°ä¸åŒé¢†åŸŸçš„é‡‡æ ·æƒé‡ï¼ŒRuleReasonerèƒ½å¤Ÿçµæ´»åœ°è¿›è¡Œåœ¨çº¿å­¦ä¹ ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­éœ€è¦äººå·¥è®¾è®¡çš„æ··åˆè®­ç»ƒæ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼ŒRuleReasoneråœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07927', 'title': 'Solving Inequality Proofs with Large Language Models', 'url': 'https://huggingface.co/papers/2506.07927', 'abstract': 'The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.', 'score': 14, 'issue_id': 4235, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '0171dcd88ad3a14f', 'authors': ['Jiayi Sheng', 'Luna Lyu', 'Jikai Jin', 'Tony Xia', 'Alex Gu', 'James Zou', 'Pan Lu'], 'affiliations': ['Massachusetts Institute of Technology', 'Stanford University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.07927.jpg', 'data': {'categories': ['#survey', '#data', '#benchmark', '#dataset', '#reasoning', '#math'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ½Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IneqMath Ñ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 10% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒÑ‡ĞµÑ‚Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ LLM Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾.'}, 'en': {'title': 'Bridging the Gap: From Answers to Rigorous Proofs in Inequality Proving', 'desc': 'This paper explores the challenges faced by large language models (LLMs) in the domain of inequality proving, which is essential for advanced reasoning in mathematics and science. It identifies a significant gap between generating answers and producing valid, step-by-step proofs, highlighting the limitations of current datasets that are often inadequate. The authors propose a new task formulation that breaks down inequality proving into two checkable subtasks: bound estimation and relation prediction, and introduce the IneqMath dataset for training and evaluation. Their evaluation of 29 leading LLMs reveals that even the best models struggle with rigorous proof construction, achieving less than 10% accuracy when assessed on step-wise reasoning, indicating a need for improved methodologies in theorem-guided reasoning and self-refinement.'}, 'zh': {'title': 'æ­ç¤ºä¸ç­‰å¼è¯æ˜ä¸­çš„æ¨ç†æŒ‘æˆ˜', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸ç­‰å¼è¯æ˜çš„æŒ‘æˆ˜ï¼Œæ­ç¤ºäº†æ‰¾åˆ°ç­”æ¡ˆä¸ç”Ÿæˆæœ‰æ•ˆé€æ­¥è§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·ã€‚ä¸ç­‰å¼è¯æ˜åœ¨ç§‘å­¦å’Œæ•°å­¦é¢†åŸŸè‡³å…³é‡è¦ï¼Œè€ƒéªŒç€é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œå¦‚å‘ç°ç´§ç•Œå’Œæˆ˜ç•¥æ€§å®šç†åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹ç°æœ‰æ•°æ®é›†ç¨€ç¼ºå’Œå½¢å¼åŒ–çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§éæ­£å¼ä½†å¯éªŒè¯çš„ä»»åŠ¡å½¢å¼ï¼Œå°†ä¸ç­‰å¼è¯æ˜é‡æ„ä¸ºä¸¤ä¸ªå¯è‡ªåŠ¨æ£€æŸ¥çš„å­ä»»åŠ¡ï¼šç•Œé™ä¼°è®¡å’Œå…³ç³»é¢„æµ‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†IneqMathæ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ£€æµ‹å¸¸è§çš„æ¨ç†ç¼ºé™·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08009', 'title': 'Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion', 'url': 'https://huggingface.co/papers/2506.08009', 'abstract': "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/", 'score': 13, 'issue_id': 4235, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'e63130d065bba1fd', 'authors': ['Xun Huang', 'Zhengqi Li', 'Guande He', 'Mingyuan Zhou', 'Eli Shechtman'], 'affiliations': ['Adobe Research', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.08009.jpg', 'data': {'categories': ['#optimization', '#video', '#diffusion', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Self Forcing: Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Self Forcing. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ñ…. Self Forcing Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾.'}, 'en': {'title': 'Self Forcing: Enhancing Video Generation with Autoregressive Training', 'desc': 'The paper presents Self Forcing, a new training method for autoregressive video diffusion models that aims to reduce exposure bias and enhance video generation quality. It allows models to generate video frames based on their own previously generated outputs instead of relying solely on ground-truth frames, which helps in better training. By using a holistic video-level supervision approach, the method evaluates the quality of the entire video sequence rather than just individual frames. Additionally, it incorporates efficient caching mechanisms and a few-step diffusion model to optimize performance and ensure real-time video generation on a single GPU.'}, 'zh': {'title': 'è‡ªæˆ‘å¼ºåˆ¶ï¼šæå‡è§†é¢‘ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'Self Forcingæ˜¯ä¸€ç§æ–°é¢–çš„è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘æ›å…‰åå·®å¹¶æé«˜ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´ä½“è§†é¢‘çº§ç›‘ç£å’Œé«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼Œè§£å†³äº†æ¨¡å‹åœ¨æ¨ç†æ—¶å¿…é¡»ä¾èµ–è‡ªèº«ä¸å®Œç¾è¾“å‡ºç”Ÿæˆåºåˆ—çš„é—®é¢˜ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒSelf Forcingåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨è‡ªç”Ÿæˆè¾“å‡ºè¿›è¡Œæ¡ä»¶ç”Ÿæˆï¼Œä»è€Œå®ç°è§†é¢‘çº§çš„æ•´ä½“æŸå¤±è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šå®ç°å®æ—¶è§†é¢‘ç”Ÿæˆï¼Œä¸”ç”Ÿæˆè´¨é‡ä¼˜äºä¼ ç»Ÿçš„éå› æœæ‰©æ•£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08002', 'title': 'Aligning Text, Images, and 3D Structure Token-by-Token', 'url': 'https://huggingface.co/papers/2506.08002', 'abstract': "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/", 'score': 13, 'issue_id': 4232, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'ab2da61a8a27783d', 'authors': ['Aadarsh Sahoo', 'Vansh Tibrewal', 'Georgia Gkioxari'], 'affiliations': ['California Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.08002.jpg', 'data': {'categories': ['#optimization', '#training', '#synthetic', '#3d', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-ÑÑ†ĞµĞ½, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… 3D-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D-Ñ„Ğ¾Ñ€Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Unifying Language, Image, and 3D Understanding for Enhanced Machine Interaction', 'desc': 'This paper presents a unified framework that integrates language, images, and 3D scenes to enhance machine understanding of three-dimensional environments. By leveraging autoregressive models, the authors explore how to effectively represent and process structured 3D scenes alongside traditional modalities. The framework includes a comprehensive guide for optimal training and performance, addressing critical aspects like data representation and modality-specific objectives. The model is evaluated on various 3D tasks, demonstrating its capability in rendering, recognition, instruction-following, and question-answering across both synthetic and real-world datasets.'}, 'zh': {'title': 'ç»Ÿä¸€ä¸‰ç»´åœºæ™¯æ¨¡å‹ï¼Œæå‡AIç†è§£èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¯­è¨€ã€å›¾åƒå’Œä¸‰ç»´åœºæ™¯æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å„ç§ä¸‰ç»´ä»»åŠ¡å’Œæ•°æ®é›†çš„æœ€ä½³è®­ç»ƒå’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è‡ªå›å½’æ¨¡å‹ï¼Œæ¢ç´¢äº†ç»“æ„åŒ–ä¸‰ç»´åœºæ™¯çš„æ–°æ¨¡å¼ï¼Œå¸®åŠ©è®¾è®¡å¸ˆæ„å»ºå’Œç¼–è¾‘ä¸‰ç»´ç¯å¢ƒã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä»½è¯¦ç»†çš„â€œé£Ÿè°±â€ï¼Œé˜è¿°äº†å®ç°æœ€ä½³è®­ç»ƒå’Œæ€§èƒ½çš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œå¹¶è¯„ä¼°äº†åœ¨å››ä¸ªæ ¸å¿ƒä¸‰ç»´ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡ä¸°å¯Œä¸‰ç»´æ¨¡æ€çš„é‡åŒ–å½¢çŠ¶ç¼–ç ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤æ‚ä¸‰ç»´ç‰©ä½“è¯†åˆ«ä»»åŠ¡ä¸­å±•ç°äº†æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04614', 'title': 'Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation', 'url': 'https://huggingface.co/papers/2506.04614', 'abstract': "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency.", 'score': 13, 'issue_id': 4236, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '9eb1f2457722a2cd', 'authors': ['Yuyang Wanyan', 'Xi Zhang', 'Haiyang Xu', 'Haowei Liu', 'Junyang Wang', 'Jiabo Ye', 'Yutong Kou', 'Ming Yan', 'Fei Huang', 'Xiaoshan Yang', 'Weiming Dong', 'Changsheng Xu'], 'affiliations': ['Alibaba Group', 'Beijing Jiaotong University', 'MAIS, Institute of Automation, Chinese Academy of Sciences, China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04614.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#optimization', '#rl', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (GUI). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ (S-GRPO) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GUI-Critic-R1. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±ÑƒÑ‚ÑÑ‚Ñ€ÑĞ¿Ğ¿Ğ¸Ğ½Ğ³Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GUI-Critic-Train Ğ¸ GUI-Critic-Test. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GUI-Critic-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI.'}, 'en': {'title': 'Enhancing GUI Automation Reliability with Pre-operative Critique', 'desc': 'This paper presents a new method to improve the reliability of GUI automation using a pre-operative critic mechanism. The mechanism provides feedback before actions are executed, helping to prevent errors that could lead to serious issues like unwanted deletions. The authors introduce a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to enhance the feedback process, making it more effective. Experiments show that their model, GUI-Critic-R1, significantly outperforms existing models in both accuracy and operational efficiency during GUI automation tasks.'}, 'zh': {'title': 'æå‡GUIè‡ªåŠ¨åŒ–å¯é æ€§çš„é¢„æ“ä½œè¯„ä¼°æœºåˆ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢„æ“ä½œè¯„ä¼°æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–ä¸­çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡çš„å¯é æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºå»ºè®®æ„ŸçŸ¥æ¢¯åº¦ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰çš„ç­–ç•¥ï¼Œä»¥æ„å»ºé¢„æ“ä½œè¯„ä¼°æ¨¡å‹GUI-Critic-R1ï¼Œå¹¶é€šè¿‡å¼•å…¥æ–°é¢–çš„å»ºè®®å¥–åŠ±æ¥å¢å¼ºæ¨¡å‹åé¦ˆçš„å¯é æ€§ã€‚è¯¥æœºåˆ¶åœ¨å®é™…æ‰§è¡Œä¹‹å‰æä¾›æœ‰æ•ˆåé¦ˆï¼Œå¸®åŠ©æ¨ç†æ½œåœ¨ç»“æœå’Œè¡ŒåŠ¨çš„æ­£ç¡®æ€§ï¼Œä»è€Œå‡å°‘å†³ç­–é”™è¯¯çš„é£é™©ã€‚é€šè¿‡åœ¨ç§»åŠ¨å’Œç½‘é¡µé¢†åŸŸçš„é™æ€å®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯„ä¼°å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07177', 'title': 'Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2506.07177', 'abstract': 'Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals.', 'score': 12, 'issue_id': 4233, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': '7d83fcff01c3595a', 'authors': ['Sangwon Jang', 'Taekyung Ki', 'Jaehyeong Jo', 'Jaehong Yoon', 'Soo Ye Kim', 'Zhe Lin', 'Sung Ju Hwang'], 'affiliations': ['Adobe Research', 'DeepAuto.ai', 'KAIST', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.07177.jpg', 'data': {'categories': ['#optimization', '#video', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Frame Guidance Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸Ğ»Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Frame Guidance Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Effortless Control in Video Generation with Frame Guidance', 'desc': 'Frame Guidance introduces a novel approach to video generation that does not require any training, allowing for effective control using frame-level signals. This method significantly reduces memory usage while ensuring that the generated videos maintain global coherence. By utilizing a simple latent processing technique and a unique latent optimization strategy, Frame Guidance can adapt to various tasks such as keyframe guidance and stylization. The results demonstrate that this approach can produce high-quality videos across different input types without the need for fine-tuning large models.'}, 'zh': {'title': 'æ— è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ§åˆ¶æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFrame Guidanceçš„æ–¹æ³•ï¼Œç”¨äºæ§åˆ¶è§†é¢‘ç”Ÿæˆï¼Œä¸”æ— éœ€è®­ç»ƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¸§çº§ä¿¡å·ï¼Œå¦‚å…³é”®å¸§å’Œé£æ ¼å‚è€ƒå›¾åƒï¼Œæ¥å®ç°å¯¹è§†é¢‘ç”Ÿæˆçš„ç²¾ç»†æ§åˆ¶ã€‚Frame Guidanceæ˜¾è‘—é™ä½äº†å†…å­˜ä½¿ç”¨ï¼Œå¹¶å¢å¼ºäº†è§†é¢‘è¾“å‡ºçš„å…¨å±€ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­ç”Ÿæˆé«˜è´¨é‡çš„å¯æ§è§†é¢‘ï¼Œä¸”ä¸ä»»ä½•è§†é¢‘æ¨¡å‹å…¼å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05167', 'title': 'ECoRAG: Evidentiality-guided Compression for Long Context RAG', 'url': 'https://huggingface.co/papers/2506.05167', 'abstract': 'ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.', 'score': 7, 'issue_id': 4231, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'd979315df3a92206', 'authors': ['Yeonseok Jeong', 'Jinsu Kim', 'Dohyeon Lee', 'Seung-won Hwang'], 'affiliations': ['IPAI, Seoul National University', 'Korea University', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05167.jpg', 'data': {'categories': ['#rag', '#alignment', '#long_context'], 'emoji': 'ğŸ”', 'ru': {'title': 'ECoRAG: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²', 'desc': 'ECoRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° (ODQA). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ECoRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ODQA. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°.'}, 'en': {'title': 'ECoRAG: Elevating LLMs with Evidence-Based Compression', 'desc': 'The ECoRAG framework enhances the performance of Large Language Models (LLMs) in Open-Domain Question Answering (ODQA) by focusing on evidentiality during document retrieval and compression. By filtering out non-evidential information, ECoRAG ensures that the generated answers are supported by relevant evidence, improving the overall accuracy of responses. Additionally, the framework optimizes resource usage by reducing latency and minimizing token consumption, making it more efficient than previous methods. Experiments demonstrate that ECoRAG significantly outperforms existing compression techniques in ODQA tasks.'}, 'zh': {'title': 'ECoRAGï¼šæå‡é—®ç­”æ€§èƒ½çš„è¯æ®æ€§å‹ç¼©æ¡†æ¶', 'desc': 'ECoRAGæ¡†æ¶é€šè¿‡åŸºäºè¯æ®æ€§å‹ç¼©æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼€æ”¾é¢†åŸŸé—®ç­”ï¼ˆODQAï¼‰ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä»¥å¾€å‹ç¼©æŠ€æœ¯æœªèƒ½æœ‰æ•ˆè¿‡æ»¤éè¯æ®æ€§ä¿¡æ¯çš„é—®é¢˜ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚ECoRAGç¡®ä¿ç”Ÿæˆçš„ç­”æ¡ˆæœ‰è¶³å¤Ÿçš„è¯æ®æ”¯æŒï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œé¢å¤–çš„æ–‡æ¡£æ£€ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECoRAGåœ¨ODQAä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„å‹ç¼©æ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†å»¶è¿Ÿå’Œä»¤ç‰Œä½¿ç”¨ï¼Œå…·æœ‰å¾ˆé«˜çš„æˆæœ¬æ•ˆç›Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08279', 'title': 'Seeing Voices: Generating A-Roll Video from Audio with Mirage', 'url': 'https://huggingface.co/papers/2506.08279', 'abstract': "Mirage generates realistic video from audio inputs, integrating with speech synthesis to create compelling multimodal content through a unified, self-attention-based training approach.  \t\t\t\t\tAI-generated summary \t\t\t\t From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links).", 'score': 6, 'issue_id': 4242, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '5309366c2181217d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#architecture', '#video', '#multimodal', '#audio', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ—Ğ²ÑƒĞº Ğ¾Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚: Mirage Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Mirage - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Mirage Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ». Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼ Ñ€ĞµÑ‡Ğ¸ ÑÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚.'}, 'en': {'title': 'Transforming Audio into Realistic Video with Mirage', 'desc': 'Mirage is a novel model that generates realistic videos from audio inputs by combining audio and visual elements through a self-attention-based training method. Unlike previous models that either ignore sound or are limited to specific applications, Mirage creates expressive video content from scratch based on audio cues. It excels in generating videos of people speaking, accurately reflecting the performance implied in the audio. This unified approach enhances the quality of the generated videos, making them more compelling and realistic compared to traditional methods.'}, 'zh': {'title': 'Mirageï¼šéŸ³é¢‘é©±åŠ¨çš„é€¼çœŸè§†é¢‘ç”Ÿæˆ', 'desc': 'Mirageæ˜¯ä¸€ç§éŸ³é¢‘åˆ°è§†é¢‘çš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®éŸ³é¢‘è¾“å…¥ç”Ÿæˆé€¼çœŸçš„è§†é¢‘å›¾åƒã€‚å®ƒç»“åˆäº†è¯­éŸ³åˆæˆæŠ€æœ¯ï¼Œåˆ›é€ å‡ºå¼•äººå…¥èƒœçš„å¤šæ¨¡æ€å†…å®¹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒMirageä¸ä»…å…³æ³¨è§†è§‰å…ƒç´ ï¼Œè¿˜èƒ½æœ‰æ•ˆå¤„ç†éŸ³é¢‘ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç»Ÿä¸€è®­ç»ƒï¼Œç¡®ä¿äº†ç”Ÿæˆç»“æœçš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.07932', 'title': 'Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor', 'url': 'https://huggingface.co/papers/2506.07932', 'abstract': 'A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object.', 'score': 6, 'issue_id': 4233, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'a13860cb07518cb2', 'authors': ['Rishit Dagli', 'Yushi Guan', 'Sankeerth Durvasula', 'Mohammadreza Mofayezi', 'Nandita Vijaykumar'], 'affiliations': ['University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.07932.jpg', 'data': {'categories': ['#synthetic', '#architecture', '#3d'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'Squeeze3D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… ÑÑ‚ĞµĞ¿ĞµĞ½ĞµĞ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞµÑ‚Ğ¾Ğº, Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Squeeze3D Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ°ĞºĞ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Efficient 3D Data Compression with Squeeze3D', 'desc': 'Squeeze3D is a new framework designed to compress 3D data efficiently using pre-trained models. It connects the latent spaces of a pre-trained encoder and a generative model through trainable mapping networks, allowing for high compression ratios while preserving visual quality. The framework can handle various 3D representations, such as meshes and point clouds, and is trained solely on synthetic data, eliminating the need for specific 3D datasets. Experiments show that Squeeze3D achieves impressive compression rates, making it a versatile tool for 3D data compression.'}, 'zh': {'title': 'Squeeze3Dï¼šé«˜æ•ˆå‹ç¼©3Dæ•°æ®çš„æ–°æ–¹æ³•', 'desc': 'Squeeze3Dæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹é«˜æ•ˆå‹ç¼©3Dæ•°æ®ï¼Œè¾¾åˆ°é«˜å‹ç¼©æ¯”å¹¶ä¿æŒè§†è§‰è´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯è®­ç»ƒçš„æ˜ å°„ç½‘ç»œè¿æ¥é¢„è®­ç»ƒç¼–ç å™¨å’Œç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„æ½œåœ¨ç©ºé—´ã€‚3Dæ¨¡å‹é¦–å…ˆè¢«ç¼–ç ä¸ºç´§å‡‘çš„æ½œåœ¨ä»£ç ï¼Œç„¶åé€šè¿‡æ˜ å°„ç½‘ç»œè½¬æ¢ä¸ºç”Ÿæˆæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œä»¥é‡å»ºåŸå§‹3Dæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒSqueeze3Dåœ¨ä¸åŒæ ¼å¼çš„3Dæ•°æ®ä¸Šå®ç°äº†æ˜¾è‘—çš„å‹ç¼©æ•ˆæœï¼ŒåŒæ—¶å»¶è¿Ÿè¾ƒå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07045', 'title': 'Interpretable and Reliable Detection of AI-Generated Images via Grounded\n  Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2506.07045', 'abstract': 'A dataset with annotations aids in fine-tuning MLLMs for accurate detection and localization of AI-generated images with meaningful explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of image generation technologies intensifies the demand for interpretable and robust detection methods. Although existing approaches often attain high accuracy, they typically operate as black boxes without providing human-understandable justifications. Multi-modal Large Language Models (MLLMs), while not originally intended for forgery detection, exhibit strong analytical and reasoning capabilities. When properly fine-tuned, they can effectively identify AI-generated images and offer meaningful explanations. However, existing MLLMs still struggle with hallucination and often fail to align their visual interpretations with actual image content and human reasoning. To bridge this gap, we construct a dataset of AI-generated images annotated with bounding boxes and descriptive captions that highlight synthesis artifacts, establishing a foundation for human-aligned visual-textual grounded reasoning. We then finetune MLLMs through a multi-stage optimization strategy that progressively balances the objectives of accurate detection, visual localization, and coherent textual explanation. The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly outperforming baseline methods.', 'score': 6, 'issue_id': 4241, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': 'ced31e12be23b119', 'authors': ['Yikun Ji', 'Hong Yan', 'Jun Lan', 'Huijia Zhu', 'Weiqiang Wang', 'Qi Fan', 'Liqing Zhang', 'Jianfu Zhang'], 'affiliations': ['Ant Group', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07045.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#data', '#training', '#reasoning', '#optimization', '#hallucinations', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ½Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ MLLM Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ˜Ğ˜ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing AI Image Detection with Human-Aligned Reasoning', 'desc': "This paper presents a method to improve the detection and localization of AI-generated images using Multi-modal Large Language Models (MLLMs). It highlights the importance of a well-annotated dataset that includes bounding boxes and descriptive captions to enhance the model's understanding of visual artifacts. The authors propose a multi-stage optimization strategy to fine-tune MLLMs, aiming to balance accurate detection with coherent explanations. The results show that the fine-tuned model significantly outperforms existing methods in both identifying AI-generated images and providing meaningful insights into their flaws."}, 'zh': {'title': 'å¾®è°ƒMLLMsä»¥æ£€æµ‹AIç”Ÿæˆå›¾åƒçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¸¦æ³¨é‡Šçš„æ•°æ®é›†æ¥å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä»¥å‡†ç¡®æ£€æµ‹å’Œå®šä½AIç”Ÿæˆçš„å›¾åƒã€‚éšç€å›¾åƒç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹å¯è§£é‡Šå’Œç¨³å¥çš„æ£€æµ‹æ–¹æ³•çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é€šå¸¸å…·æœ‰é«˜å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¾€å¾€ä½œä¸ºé»‘ç®±æ“ä½œï¼Œæ— æ³•æä¾›äººç±»å¯ç†è§£çš„è§£é‡Šã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå¸¦æœ‰è¾¹ç•Œæ¡†å’Œæè¿°æ€§æ ‡é¢˜çš„AIç”Ÿæˆå›¾åƒæ•°æ®é›†ï¼Œæœ¬æ–‡ä¸ºäººç±»å¯¹é½çš„è§†è§‰-æ–‡æœ¬æ¨ç†å¥ å®šäº†åŸºç¡€ï¼Œå¹¶é€šè¿‡å¤šé˜¶æ®µä¼˜åŒ–ç­–ç•¥å¾®è°ƒMLLMsï¼Œæ˜¾è‘—æé«˜äº†æ£€æµ‹å’Œå®šä½çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08887', 'title': 'DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval', 'url': 'https://huggingface.co/papers/2506.08887', 'abstract': 'The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA.', 'score': 4, 'issue_id': 4232, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '068c2f58bc5049d2', 'authors': ['Leqi Shen', 'Guoqiang Gong', 'Tianxiang Hao', 'Tao He', 'Yifeng Zhang', 'Pengzhang Liu', 'Sicheng Zhao', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['BNRist', 'Department of Automation, Tsinghua University', 'GRG Banking Equipment Co., Ltd.', 'Hangzhou Zhuoxi Institute of Brain and Intelligence', 'JD.com', 'School of Software', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.08887.jpg', 'data': {'categories': ['#transfer_learning', '#alignment', '#video', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiscoVLA - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸, ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾. DiscoVLA Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiscoVLA Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼.'}, 'en': {'title': 'Bridging Gaps in Video-Text Retrieval with DiscoVLA', 'desc': 'The paper introduces DiscoVLA, a method designed to enhance video-text retrieval by addressing three main discrepancies: vision, language, and alignment. Unlike existing approaches that primarily focus on visual aspects, DiscoVLA integrates both image and video features to improve understanding at the video level. It also generates pseudo image captions to refine image-level alignment and employs Image-to-Video Alignment Distillation to strengthen video-level alignment using knowledge from image-level data. Experimental results show that DiscoVLA significantly outperforms previous methods, achieving a notable improvement in retrieval accuracy.'}, 'zh': {'title': 'æå‡è§†é¢‘-æ–‡æœ¬æ£€ç´¢çš„DiscoVLAæ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDiscoVLAçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è§£å†³è§†è§‰ã€è¯­è¨€å’Œå¯¹é½çš„å·®å¼‚æ¥æ”¹å–„è§†é¢‘-æ–‡æœ¬æ£€ç´¢ã€‚ä¼ ç»Ÿçš„CLIPæ¨¡å‹ä¸»è¦å…³æ³¨å›¾åƒçº§åˆ«çš„è§†è§‰-è¯­è¨€åŒ¹é…ï¼Œè€Œè§†é¢‘-æ–‡æœ¬æ£€ç´¢éœ€è¦åœ¨è§†é¢‘çº§åˆ«ä¸Šè¿›è¡Œå…¨é¢ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å›¾åƒ-è§†é¢‘ç‰¹å¾èåˆæ¥æ•´åˆå›¾åƒå’Œè§†é¢‘çš„ç‰¹å¾ï¼Œæœ‰æ•ˆåº”å¯¹è§†è§‰å’Œè¯­è¨€çš„å·®å¼‚ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ç”Ÿæˆä¼ªå›¾åƒæ ‡é¢˜ä»¥å­¦ä¹ ç»†ç²’åº¦çš„å›¾åƒçº§åˆ«å¯¹é½ï¼Œä»è€Œå‡è½»å¯¹é½å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiscoVLAåœ¨è§†é¢‘-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨MSRVTTæ•°æ®é›†ä¸Šå–å¾—äº†50.5%çš„R@1æˆç»©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08500', 'title': 'DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in\n  Search-Augmented LLMs', 'url': 'https://huggingface.co/papers/2506.08500', 'abstract': 'CONFLICTS, a benchmark for evaluating how LLMs handle knowledge conflicts in RAG, reveals significant challenges in conflict resolution but shows improvement with explicit prompting.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.', 'score': 4, 'issue_id': 4241, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '3dec58d73ae743df', 'authors': ['Arie Cattan', 'Alon Jacovi', 'Ori Ram', 'Jonathan Herzig', 'Roee Aharoni', 'Sasha Goldshtein', 'Eran Ofek', 'Idan Szpektor', 'Avi Caciularu'], 'affiliations': ['Bar-Ilan University', 'Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.08500.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#rag', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CONFLICTS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Retrieval Augmented Generation (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑƒĞ¼ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Navigating Knowledge Conflicts in LLMs with CONFLICTS Benchmark', 'desc': 'This paper introduces CONFLICTS, a benchmark designed to evaluate how large language models (LLMs) manage knowledge conflicts in Retrieval Augmented Generation (RAG) systems. It presents a new taxonomy categorizing different types of knowledge conflicts and outlines the expected behavior of models when faced with these conflicts. The authors demonstrate that while LLMs often struggle with resolving discrepancies in retrieved information, providing explicit prompts can enhance their performance. The study highlights the need for further advancements in conflict resolution strategies within LLMs.'}, 'zh': {'title': 'è§£å†³çŸ¥è¯†å†²çªï¼Œæå‡æ¨¡å‹è¡¨ç°ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•CONFLICTSï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­å¤„ç†çŸ¥è¯†å†²çªçš„èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†çŸ¥è¯†å†²çªçš„ç±»å‹ï¼Œå¹¶æè¿°äº†æ¨¡å‹åœ¨æ¯ç§ç±»å‹ä¸‹çš„æœŸæœ›è¡Œä¸ºã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMsåœ¨è§£å†³ä¿¡æ¯æ¥æºä¹‹é—´çš„å†²çªæ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œä½†é€šè¿‡æ˜ç¡®æç¤ºå¯ä»¥æ˜¾è‘—æé«˜å…¶å“åº”çš„è´¨é‡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæœªæ¥çš„ç ”ç©¶ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08300', 'title': "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability", 'url': 'https://huggingface.co/papers/2506.08300', 'abstract': "Institutional Books 1.0 provides a large dataset of public domain books from Harvard Library for training and inference of large language models, enhancing data accessibility and sustainability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use.", 'score': 4, 'issue_id': 4239, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '8e42e765c2efbe2a', 'authors': ['Matteo Cargnelutti', 'Catherine Brobston', 'John Hess', 'Jack Cushman', 'Kristi Mukk', 'Aristana Scourtas', 'Kyle Courtney', 'Greg Leppert', 'Amanda Watson', 'Martha Whitehead', 'Jonathan Zittrain'], 'affiliations': ['Harvard Law School Library', 'Harvard Law School, Harvard School of Engineering and Applied Sciences, Harvard Kennedy School', 'Harvard Library', 'Institutional Data Initiative, Harvard Law School Library', 'Library Innovation Lab, Harvard Law School Library'], 'pdf_title_img': 'assets/pdf/title_img/2506.08300.jpg', 'data': {'categories': ['#open_source', '#dataset', '#synthetic', '#data'], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ', 'desc': 'Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Institutional Books 1.0 Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½ÑƒÑ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ ĞºĞ½Ğ¸Ğ³ Ğ¸Ğ· Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ“Ğ°Ñ€Ğ²Ğ°Ñ€Ğ´ÑĞºĞ¾Ğ¹ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 983,004 Ñ‚Ğ¾Ğ¼Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 250 ÑĞ·Ñ‹ĞºĞ°Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ğ¾ĞºĞ¾Ğ»Ğ¾ 242 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ OCR-Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ½Ğ¸Ğ³, Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ…ÑÑ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ¸Ğµ ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking Knowledge: A Sustainable Dataset for Language Models', 'desc': 'Institutional Books 1.0 is a comprehensive dataset of public domain books sourced from Harvard Library, aimed at improving the training and inference processes of large language models (LLMs). The dataset includes over 983,000 volumes and approximately 242 billion tokens, providing a rich resource for enhancing the quality and diversity of training data. By ensuring clear provenance and sustainable practices in data stewardship, this initiative addresses the critical need for high-quality, publicly available datasets in the rapidly evolving field of machine learning. The project not only facilitates better model performance but also promotes accessibility to historical texts for both human and machine use.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹çš„æ•°æ®å¯ç”¨æ€§ä¸å¯æŒç»­æ€§', 'desc': 'Institutional Books 1.0 æ˜¯ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå“ˆä½›å›¾ä¹¦é¦†çš„å…¬å…±é¢†åŸŸä¹¦ç±ï¼Œæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æä¾›æ•°æ®æ”¯æŒã€‚è¿™äº›ä¹¦ç±çš„å¤šæ ·æ€§å’Œè´¨é‡ç›´æ¥å½±å“åˆ°è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼Œå› æ­¤éœ€è¦é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚è¯¥é¡¹ç›®ä»å“ˆä½›å›¾ä¹¦é¦†æå–å’Œå¤„ç†äº†è¶…è¿‡983,000æœ¬ä¹¦ç±çš„æ–‡æœ¬å’Œå…ƒæ•°æ®ï¼Œç¡®ä¿æ•°æ®çš„å¯è®¿é—®æ€§å’Œå¯æŒç»­æ€§ã€‚é€šè¿‡è¿™ä¸ªæ•°æ®é›†ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥æ›´æ–¹ä¾¿åœ°ä½¿ç”¨å†å²æ–‡æœ¬ï¼Œæ¨åŠ¨æœºå™¨å­¦ä¹ çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07976', 'title': 'Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction', 'url': 'https://huggingface.co/papers/2506.07976', 'abstract': 'Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.  \t\t\t\t\tAI-generated summary \t\t\t\t The current paradigm of test-time scaling relies on generating long reasoning traces ("thinking" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent\'s interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.', 'score': 4, 'issue_id': 4239, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'b35082cb7ca5bb65', 'authors': ['Junhong Shen', 'Hao Bai', 'Lunjun Zhang', 'Yifei Zhou', 'Amrith Setlur', 'Shengbang Tong', 'Diego Caples', 'Nan Jiang', 'Tong Zhang', 'Ameet Talwalkar', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'New York University', 'Scribe', 'The AGI Company', 'University of California, Berkeley', 'University of Illinois Urbana-Champaign', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.07976.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#agents', '#open_source', '#training'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (Test-Time Interaction, TTI). TTI Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ TTI Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WebVoyager Ğ¸ WebArena, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemma 3 12B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Web Agents with Test-Time Interaction', 'desc': 'This paper introduces Test-Time Interaction (TTI), a novel approach that enhances the performance of web agents by allowing them to interact more effectively with their environment. Unlike traditional methods that focus on generating long reasoning traces before acting, TTI enables agents to adapt their behavior in real-time by increasing their interaction horizon. This method supports complex behaviors such as exploration, backtracking, and dynamic re-planning within a single decision-making process. The authors demonstrate that TTI significantly improves task success rates on web benchmarks, showcasing its potential as a complementary strategy to existing scaling techniques in reinforcement learning.'}, 'zh': {'title': 'æµ‹è¯•æ—¶äº¤äº’ï¼šæå‡ä»£ç†æ™ºèƒ½çš„æ–°ç»´åº¦', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæµ‹è¯•æ—¶äº¤äº’ï¼ˆTTIï¼‰ï¼Œæ—¨åœ¨æé«˜ç½‘ç»œä»£ç†çš„æ€§èƒ½ã€‚TTIé€šè¿‡æ‰©å±•ä»£ç†çš„äº¤äº’èŒƒå›´ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å•æ¬¡æ‰§è¡Œä¸­è¿›è¡Œæ¢ç´¢ã€å›æº¯å’ŒåŠ¨æ€é‡æ–°è§„åˆ’ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–é•¿æ¨ç†è½¨è¿¹çš„æ–¹å¼ä¸åŒï¼ŒTTIå…è®¸ä»£ç†åœ¨ä¸ç¯å¢ƒäº’åŠ¨æ—¶è·å–æ–°ä¿¡æ¯å¹¶é€‚åº”å…¶è¡Œä¸ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒTTIåœ¨WebVoyagerå’ŒWebArenaåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†äº¤äº’æ‰©å±•ä½œä¸ºä¸€ç§å¼ºå¤§çš„è¡¥å……æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05928', 'title': 'MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2506.05928', 'abstract': 'A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ homogeneous MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a heterogeneous Mixture-of-Adapters (MoA) approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: (i) Soft MoA achieves fine-grained integration by performing a weighted fusion of all expert outputs; (ii) Sparse MoA activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA.', 'score': 4, 'issue_id': 4232, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'f78e6f70ebb69ac2', 'authors': ['Jie Cao', 'Tianwei Lin', 'Hongyang He', 'Rolan Yan', 'Wenqiao Zhang', 'Juncheng Li', 'Dongping Zhang', 'Siliang Tang', 'Yueting Zhuang'], 'affiliations': ['Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05928.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#optimization', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ°Ñ ÑĞ¼ĞµÑÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½ÑƒÑ Mixture-of-Adapters (MoA). MoA Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ³Ğ¾Ğ¼Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² MoE-LoRA. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° MoA: Soft MoA Ñ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²ÑĞµÑ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Sparse MoA Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MoA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¾Ğ¼Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ MoE-LoRA ĞºĞ°Ğº Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Unlocking LLM Potential with Diverse Adapter Experts', 'desc': 'This paper introduces a new approach called heterogeneous Mixture-of-Adapters (MoA) for fine-tuning Large Language Models (LLMs) more efficiently. Unlike traditional homogeneous MoE-LoRA methods that use similar adapter experts, MoA combines diverse adapter structures to improve performance and prevent issues like representation collapse. The method includes two variants: Soft MoA, which fuses outputs from all experts, and Sparse MoA, which selectively activates experts based on their effectiveness. Experimental results show that MoA significantly outperforms existing methods in both performance and parameter efficiency.'}, 'zh': {'title': 'å¼‚æ„é€‚é…å™¨æ··åˆï¼šæå‡å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¼‚æ„çš„é€‚é…å™¨æ··åˆï¼ˆMoAï¼‰æ–¹æ³•ï¼Œä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„åŒè´¨MoE-LoRAæ–¹æ³•ä¸åŒï¼ŒMoAé›†æˆäº†å…·æœ‰ä¸åŒç»“æ„çš„é€‚é…å™¨ä¸“å®¶ï¼Œä»è€Œå…‹æœäº†è¡¨ç¤ºå´©æºƒå’Œä¸“å®¶è´Ÿè½½ä¸å¹³è¡¡çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€æ•´åˆé€‚é…å™¨ä¸“å®¶çš„äº’è¡¥è¡¨ç¤ºèƒ½åŠ›ï¼Œä¿ƒè¿›äº†ä¸“å®¶çš„ä¸“ä¸šåŒ–ï¼Œæå‡äº†é¢„è®­ç»ƒçŸ¥è¯†å‘ä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆè½¬ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼‚æ„MoAåœ¨æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ä¸Šå‡ä¼˜äºåŒè´¨MoE-LoRAæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05700', 'title': 'RKEFino1: A Regulation Knowledge-Enhanced Large Language Model', 'url': 'https://huggingface.co/papers/2506.05700', 'abstract': 'RKEFino1, a knowledge-enhanced financial reasoning model, addresses accuracy and compliance challenges in Digital Regulatory Reporting through fine-tuning with domain knowledge from XBRL, CDM, and MOF, and introduces a novel Numerical NER task.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) hold great promise for financial applications but introduce critical accuracy and compliance challenges in Digital Regulatory Reporting (DRR). To address these issues, we propose RKEFino1, a regulation knowledge-enhanced financial reasoning model built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce a novel Numerical NER task covering financial entities in both sentences and tables. Experimental results demonstrate the effectiveness and generalization capacity of RKEFino1 in compliance-critical financial tasks. We have released our model on Hugging Face.', 'score': 2, 'issue_id': 4231, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': 'a23a28bb68811316', 'authors': ['Yan Wang', 'Yueru He', 'Ruoyu Xiang', 'Jeff Zhao'], 'affiliations': ['Columbia University', 'New York University', 'The University of Texas at Austin', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05700.jpg', 'data': {'categories': ['#data', '#multimodal', '#reasoning', '#training', '#healthcare', '#open_source'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'RKEFino1 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Fino1. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼ Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸Ğ· XBRL, CDM Ğ¸ MOF. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. RKEFino1 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ (NER) Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ….'}, 'en': {'title': 'Enhancing Financial Compliance with RKEFino1', 'desc': 'RKEFino1 is a financial reasoning model designed to improve accuracy and compliance in Digital Regulatory Reporting (DRR). It enhances the Fino1 model by incorporating domain knowledge from XBRL, CDM, and MOF, which are essential for understanding financial regulations. The model introduces a new task called Numerical Named Entity Recognition (NER) to identify financial entities in both text and tabular formats. Experimental results show that RKEFino1 effectively addresses compliance challenges and generalizes well to various financial tasks.'}, 'zh': {'title': 'çŸ¥è¯†å¢å¼ºçš„é‡‘èæ¨ç†ï¼Œæå‡åˆè§„æ€§ä¸å‡†ç¡®æ€§', 'desc': 'RKEFino1æ˜¯ä¸€ç§å¢å¼ºçŸ¥è¯†çš„é‡‘èæ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ•°å­—ç›‘ç®¡æŠ¥å‘Šä¸­çš„å‡†ç¡®æ€§å’Œåˆè§„æ€§æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åŸºäºFino1ï¼Œå¹¶é€šè¿‡XBRLã€CDMå’ŒMOFç­‰é¢†åŸŸçŸ¥è¯†è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé—®ç­”ä»»åŠ¡â€”â€”åŸºäºçŸ¥è¯†çš„é—®ç­”å’Œæ•°å­¦æ¨ç†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°å€¼å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼Œæ¶µç›–äº†å¥å­å’Œè¡¨æ ¼ä¸­çš„é‡‘èå®ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRKEFino1åœ¨åˆè§„æ€§å…³é”®çš„é‡‘èä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07047', 'title': 'Mathesis: Towards Formal Theorem Proving from Natural Languages', 'url': 'https://huggingface.co/papers/2506.07047', 'abstract': "Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.", 'score': 1, 'issue_id': 4237, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': 'd3bc82dde4f2b8bc', 'authors': ['Yu Xuejun', 'Jianyuan Zhong', 'Zijin Feng', 'Pengyi Zhai', 'Roozbeh Yousefzadeh', 'Wei Chong Ng', 'Haoxiong Liu', 'Ziyi Shou', 'Jing Xiong', 'Yudong Zhou', 'Claudia Beth Ong', 'Austen Jeremy Sugiarto', 'Yaoxi Zhang', 'Wai Ming Tai', 'Huan Cao', 'Dongcai Lu', 'Jiacheng Sun', 'Qiang Xu', 'Shen Xin', 'Zhenguo Li'], 'affiliations': ['Huawei Celia Team', 'Huawei Noahs Ark Lab', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.07047.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#benchmark', '#dataset', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¾Ñ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Mathesis - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Mathesis-Autoformalizer, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Mathesis-Prover Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Gaokao-Formal Ğ¸Ğ· 488 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ²ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ° Ğ² Ğ²ÑƒĞ·Ñ‹.'}, 'en': {'title': 'Bridging Natural Language and Formal Reasoning with Mathesis', 'desc': "This paper presents Mathesis, a novel end-to-end theorem proving system that processes informal problem statements, addressing the limitations of existing LLM-based theorem provers. It introduces Mathesis-Autoformalizer, which utilizes reinforcement learning to automatically convert natural language problems into formal statements, supported by the LeanScorer framework for assessing formalization quality. Additionally, the Mathesis-Prover generates formal proofs from these formalized statements. The system's effectiveness is validated through experiments on the Gaokao-Formal benchmark, demonstrating significant improvements in accuracy and pass rates compared to existing methods."}, 'zh': {'title': 'Mathesisï¼šéæ­£å¼é—®é¢˜çš„å®šç†è¯æ˜æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†Mathesisï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†éæ­£å¼é—®é¢˜é™ˆè¿°çš„ç«¯åˆ°ç«¯å®šç†è¯æ˜ç®¡é“ã€‚å®ƒåŒ…æ‹¬Mathesis-Autoformalizerï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨å½¢å¼åŒ–å·¥å…·ï¼Œèƒ½å¤Ÿæé«˜è‡ªç„¶è¯­è¨€é—®é¢˜çš„å½¢å¼åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡LeanScoreræ¡†æ¶è¯„ä¼°å½¢å¼åŒ–è´¨é‡ã€‚è®ºæ–‡è¿˜æå‡ºäº†Mathesis-Proverï¼Œèƒ½å¤Ÿä»å½¢å¼åŒ–çš„é™ˆè¿°ä¸­ç”Ÿæˆæ­£å¼è¯æ˜ã€‚é€šè¿‡åœ¨ä¸­å›½é«˜è€ƒçš„488ä¸ªå¤æ‚é—®é¢˜ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜Mathesisåœ¨é€šè¿‡ç‡å’Œå‡†ç¡®æ€§ä¸Šå‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04688', 'title': 'MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2506.04688', 'abstract': "MMRefine evaluates the error refinement capabilities of Multimodal Large Language Models through a benchmark that categorizes errors and identifies performance bottlenecks.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MMRefine, a MultiModal Refinement benchmark designed to evaluate the error refinement capabilities of Multimodal Large Language Models (MLLMs). As the emphasis shifts toward enhancing reasoning during inference, MMRefine provides a framework that evaluates MLLMs' abilities to detect and correct errors across six distinct scenarios beyond just comparing final accuracy before and after refinement. Furthermore, the benchmark analyzes the refinement performance by categorizing errors into six error types. Experiments with various open and closed MLLMs reveal bottlenecks and factors impeding refinement performance, highlighting areas for improvement in effective reasoning enhancement. Our code and dataset are publicly available at https://github.com/naver-ai/MMRefine.", 'score': 1, 'issue_id': 4241, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'd7ca0acf8a8fe586', 'authors': ['Gio Paik', 'Geewook Kim', 'Jinbae Im'], 'affiliations': ['KAIST AI', 'NAVER Cloud AI', 'Theta One, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.04688.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'MMRefine: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMRefine - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ MLLM Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Refining Reasoning: Enhancing MLLMs with MMRefine', 'desc': "MMRefine is a benchmark designed to assess how well Multimodal Large Language Models (MLLMs) can refine their outputs by correcting errors. It categorizes errors into six types and evaluates the models' performance in detecting and addressing these errors during inference. The focus is not just on the final accuracy but also on the models' reasoning capabilities before and after refinement. The findings from experiments highlight specific bottlenecks that hinder effective error correction, providing insights for future improvements in MLLMs."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„é”™è¯¯ä¿®æ­£èƒ½åŠ›', 'desc': 'MMRefineæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é”™è¯¯ä¿®æ­£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒä¸ä»…å…³æ³¨æœ€ç»ˆå‡†ç¡®ç‡çš„æ¯”è¾ƒï¼Œè¿˜é€šè¿‡å…­ç§ä¸åŒåœºæ™¯æ¥æ£€æµ‹å’Œçº æ­£é”™è¯¯ã€‚è¯¥åŸºå‡†å°†é”™è¯¯åˆ†ä¸ºå…­ç§ç±»å‹ï¼Œä»¥åˆ†æä¿®æ­£æ€§èƒ½çš„ç“¶é¢ˆå’Œå½±å“å› ç´ ã€‚å®éªŒç»“æœæ­ç¤ºäº†åœ¨æ¨ç†å¢å¼ºæ–¹é¢çš„æ”¹è¿›ç©ºé—´ï¼Œä¿ƒè¿›äº†å¯¹MLLMsçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.04020', 'title': 'QQSUM: A Novel Task and Model of Quantitative Query-Focused\n  Summarization for Review-based Product Question Answering', 'url': 'https://huggingface.co/papers/2506.04020', 'abstract': 'QQSUM-RAG extends Retrieval-Augmented Generation to provide diverse, representative Key Point summaries with quantified opinions for product question answering, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: https://github.com/antangrocket1312/QQSUMM', 'score': 0, 'issue_id': 4240, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '8e25a42b034fd30c', 'authors': ['An Quang Tang', 'Xiuzhen Zhang', 'Minh Ngoc Dinh', 'Zhuang Li'], 'affiliations': ['RMIT University, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2506.04020.jpg', 'data': {'categories': ['#rag', '#optimization', '#multimodal', '#open_source', '#games'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ°Ñ… Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ QQSUM-RAG Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Retrieval-Augmented Generation, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ. QQSUM-RAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ few-shot learning Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ QQSUM-RAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Diverse Insights for Better Product Answers', 'desc': 'The paper introduces QQSUM-RAG, an advanced model that enhances Retrieval-Augmented Generation (RAG) for product question answering (PQA) by generating diverse and representative Key Point summaries. It addresses the limitation of existing PQA systems that typically provide answers from a single viewpoint, thereby missing the variety of customer opinions. QQSUM focuses on Quantitative Query-Focused Summarization, which not only summarizes diverse opinions but also quantifies their prevalence to improve response accuracy. Experimental results show that QQSUM-RAG outperforms current RAG methods in both the quality of generated text and the accuracy of opinion quantification.'}, 'zh': {'title': 'å¤šæ ·åŒ–å®¢æˆ·æ„è§çš„æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºå®šé‡æŸ¥è¯¢èšç„¦æ‘˜è¦ï¼ˆQQSUMï¼‰ï¼Œæ—¨åœ¨å°†å¤šæ ·åŒ–çš„å®¢æˆ·æ„è§æ€»ç»“ä¸ºä»£è¡¨æ€§çš„å…³é”®ç‚¹ï¼ˆKPsï¼‰ï¼Œå¹¶é‡åŒ–å…¶æ™®éæ€§ï¼Œä»¥æœ‰æ•ˆå›ç­”ç”¨æˆ·æŸ¥è¯¢ã€‚QQSUM-RAGæ¨¡å‹æ‰©å±•äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œé€šè¿‡å°‘é‡å­¦ä¹ è”åˆè®­ç»ƒKPå¯¼å‘çš„æ£€ç´¢å™¨å’ŒKPæ‘˜è¦ç”Ÿæˆå™¨ï¼Œä»è€Œç”Ÿæˆèƒ½å¤Ÿæ•æ‰å¤šæ ·åŒ–å’Œä»£è¡¨æ€§æ„è§çš„æ‘˜è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQQSUM-RAGåœ¨æ–‡æœ¬è´¨é‡å’Œæ„è§é‡åŒ–å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„RAGåŸºçº¿æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºç”µå­å•†åŠ¡å¹³å°çš„äº§å“é—®ç­”ç³»ç»Ÿæä¾›äº†æ›´å…¨é¢çš„å®¢æˆ·æ„è§è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14028', 'title': 'MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation', 'url': 'https://huggingface.co/papers/2506.14028', 'abstract': 'MultiFinBen is a multilingual and multimodal benchmark for financial domain tasks, evaluating LLMs across modalities and linguistic settings, revealing challenges in complex cross-lingual and multimodal financial reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.', 'score': 65, 'issue_id': 4360, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'e94d60496c8d96d1', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#multilingual', '#benchmark', '#multimodal', '#reasoning', '#financial', '#dataset'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'MultiFinBen: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ Ğ˜Ğ˜', 'desc': 'MultiFinBen - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº PolyFiQA Ğ¸ OCR-Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹, ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑ†ĞµĞ½ĞºĞ° 22 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ¶ÑŠÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'MultiFinBen: Bridging Multilingual and Multimodal Gaps in Financial AI', 'desc': 'MultiFinBen is a new benchmark designed to test large language models (LLMs) in the financial sector using multiple languages and types of data, such as text, images, and audio. It addresses the limitations of previous benchmarks that only focused on single languages and simple tasks, which do not reflect the complexities of real-world financial communication. The benchmark includes innovative tasks that require models to understand and reason with mixed-language inputs and to extract information from visual financial documents. Evaluation of various advanced models shows that even the best-performing ones struggle with these challenging tasks, highlighting the need for improved capabilities in financial reasoning across different languages and modalities.'}, 'zh': {'title': 'å¤šè¯­è¨€å¤šæ¨¡æ€é‡‘èåŸºå‡†ï¼Œæ¨åŠ¨é‡‘èæ™ºèƒ½è¿›æ­¥', 'desc': 'MultiFinBenæ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èé¢†åŸŸä»»åŠ¡çš„å¤šè¯­è¨€å’Œå¤šæ¨¡æ€åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒæ¨¡æ€å’Œè¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ­ç¤ºäº†åœ¨å¤æ‚çš„è·¨è¯­è¨€å’Œå¤šæ¨¡æ€é‡‘èæ¨ç†ä¸­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ–°ä»»åŠ¡ï¼ŒPolyFiQA-Easyå’ŒPolyFiQA-Expertï¼Œè¦æ±‚æ¨¡å‹åœ¨æ··åˆè¯­è¨€è¾“å…¥ä¸Šè¿›è¡Œå¤æ‚æ¨ç†ã€‚æ­¤å¤–ï¼ŒMultiFinBenè¿˜æä¾›äº†ä¸€ç§åŠ¨æ€çš„ã€å…³æ³¨éš¾åº¦çš„é€‰æ‹©æœºåˆ¶ï¼Œä»¥ç¡®ä¿åŸºå‡†çš„å¹³è¡¡æ€§å’Œæœ‰æ•ˆæ€§ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.12928', 'title': 'Scaling Test-time Compute for LLM Agents', 'url': 'https://huggingface.co/papers/2506.12928', 'abstract': "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.", 'score': 36, 'issue_id': 4351, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': '39c8f3e831e90d93', 'authors': ['King Zhu', 'Hanhao Li', 'Siwei Wu', 'Tianshun Xing', 'Dehua Ma', 'Xiangru Tang', 'Minghao Liu', 'Jian Yang', 'Jiaheng Liu', 'Yuchen Eleanor Jiang', 'Changwang Zhang', 'Chenghua Lin', 'Jun Wang', 'Ge Zhang', 'Wangchunshu Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.12928.jpg', 'data': {'categories': ['#training', '#reasoning', '#agents', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ‘Ñ‹Ğ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¿Ğ¸ÑĞ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Boosting Language Agents with Test-Time Scaling', 'desc': 'This paper investigates how increasing computational resources at test time can enhance the performance of large language models (LLMs). It systematically examines various test-time scaling methods, such as parallel sampling, sequential revisions, and verification techniques. The findings indicate that scaling up computation not only boosts reasoning capabilities but also highlights the importance of strategic reflection and diverse rollouts. Notably, the study reveals that the list-wise verification method yields the best results among different merging approaches.'}, 'zh': {'title': 'æµ‹è¯•æ—¶é—´æ‰©å±•æå‡è¯­è¨€ä»£ç†æ€§èƒ½', 'desc': 'æœ¬æ–‡ç³»ç»Ÿæ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­åº”ç”¨æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•çš„æ•ˆæœã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¡ç®—æ‰©å±•èƒ½å¤Ÿæ˜¾è‘—æå‡è¯­è¨€ä»£ç†çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¹¶è¡Œé‡‡æ ·ã€é¡ºåºä¿®è®¢ã€æœ‰æ•ˆéªŒè¯å’Œå¢åŠ å¤šæ ·åŒ–çš„å›æ»šç­–ç•¥ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒè®¾è®¡ç­–ç•¥å¯¹è¯­è¨€ä»£ç†æ€§èƒ½çš„å½±å“ï¼Œå¹¶å‘ç°æµ‹è¯•æ—¶é—´è®¡ç®—çš„æ‰©å±•ç¡®å®èƒ½æé«˜ä»£ç†çš„è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯ï¼Œé‡‡ç”¨åˆ—è¡¨å¼éªŒè¯æ–¹æ³•æ•ˆæœæœ€ä½³ï¼Œè€Œå¤šæ ·åŒ–çš„å›æ»šç­–ç•¥ä¹Ÿå¯¹ä»»åŠ¡è¡¨ç°æœ‰ç§¯æå½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12285', 'title': 'CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\n  Following', 'url': 'https://huggingface.co/papers/2506.12285', 'abstract': 'CMI-Bench introduces a comprehensive instruction-following benchmark for audio-text LLMs to evaluate them on a diverse range of music information retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking: reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.', 'score': 34, 'issue_id': 4360, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 Ğ¸ÑĞ½Ñ', 'en': 'June 14', 'zh': '6æœˆ14æ—¥'}, 'hash': 'c0c91a24a40dfd14', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#open_source', '#survey', '#audio', '#benchmark', '#ethics'], 'emoji': 'ğŸµ', 'ru': {'title': 'CMI-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM', 'desc': 'CMI-Bench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¶Ğ°Ğ½Ñ€Ğ¾Ğ², Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². CMI-Bench Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ LLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ, Ñ…Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ³ĞµĞ½Ğ´ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM.'}, 'en': {'title': 'CMI-Bench: Advancing Music Understanding with LLMs', 'desc': 'CMI-Bench is a new benchmark designed to evaluate audio-text large language models (LLMs) on various music information retrieval (MIR) tasks. It addresses the limitations of existing benchmarks by providing a comprehensive set of instruction-following tasks that reflect real-world music analysis complexities. The benchmark includes diverse tasks such as genre classification, emotion tagging, and melody extraction, using standardized evaluation metrics for consistency with state-of-the-art models. Results from experiments show performance gaps between LLMs and supervised models, revealing both the potential and limitations of current LLMs in handling MIR challenges.'}, 'zh': {'title': 'CMI-Benchï¼šéŸ³ä¹ä¿¡æ¯æ£€ç´¢çš„æ–°åŸºå‡†', 'desc': 'CMI-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡ä»¤è·ŸéšåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å®ƒä»¬åœ¨å¤šæ ·åŒ–çš„éŸ³ä¹ä¿¡æ¯æ£€ç´¢ï¼ˆMIRï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥åŸºå‡†é‡æ–°è§£é‡Šäº†ä¼ ç»ŸMIRæ³¨é‡Šä¸ºæŒ‡ä»¤è·Ÿéšæ ¼å¼ï¼Œæ¶µç›–äº†å¦‚æµæ´¾åˆ†ç±»ã€æƒ…æ„Ÿå›å½’ã€ä¹å™¨åˆ†ç±»ç­‰å¤šé¡¹ä»»åŠ¡ã€‚ä¸ä»¥å¾€çš„åŸºå‡†ä¸åŒï¼ŒCMI-Benché‡‡ç”¨æ ‡å‡†åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿ä¸ç°æœ‰çš„ç›‘ç£å­¦ä¹ æ¨¡å‹ç›´æ¥å¯æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºLLMä¸ç›‘ç£æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨MIRä»»åŠ¡ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.14429', 'title': 'LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs', 'url': 'https://huggingface.co/papers/2506.14429', 'abstract': 'This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textit{stable perplexity} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textit{local perception} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.', 'score': 33, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'd0032538675516d6', 'authors': ['Xiaoran Liu', 'Zhigeng Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.14429.jpg', 'data': {'categories': ['#training', '#long_context', '#architecture', '#benchmark', '#diffusion', '#rl'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ¼ 'Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ'. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ LongLLaDA Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."}, 'en': {'title': 'Unlocking Long Contexts in Diffusion LLMs with LongLLaDA', 'desc': 'This paper explores how diffusion large language models (LLMs) perform with long contexts compared to traditional auto-regressive LLMs. It highlights that diffusion LLMs maintain stable perplexity when extending context, unlike their auto-regressive counterparts, which struggle with longer inputs. The authors introduce LongLLaDA, a method that allows for context window extension without additional training, leveraging insights from Rotary Position Embedding (RoPE) scaling. The findings reveal specific tasks where diffusion LLMs excel and others where they do not, paving the way for future research in long-context applications.'}, 'zh': {'title': 'æ‰©æ•£æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡æ–°æ–¹æ³•ï¼šLongLLaDA', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdiffusion LLMsï¼‰ä¸è‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ï¼ˆauto-regressive LLMsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ€§èƒ½æ–¹é¢çš„æ¯”è¾ƒï¼Œè¯†åˆ«äº†å®ƒä»¬çš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•LongLLaDAæ¥æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£LLMsåœ¨ç›´æ¥ä¸Šä¸‹æ–‡å¤–æ¨æ—¶ä¿æŒäº†æ˜¾è‘—ç¨³å®šçš„å›°æƒ‘åº¦ï¼Œè€Œè‡ªå›å½’æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡è¶…å‡ºé¢„è®­ç»ƒé•¿åº¦æ—¶åˆ™è¡¨ç°ä¸ä½³ã€‚æ‰©æ•£LLMså±•ç°å‡ºç‹¬ç‰¹çš„å±€éƒ¨æ„ŸçŸ¥ç°è±¡ï¼Œä½¿å…¶èƒ½å¤ŸæˆåŠŸä»æœ€è¿‘çš„ä¸Šä¸‹æ–‡ç‰‡æ®µä¸­æ£€ç´¢ä¿¡æ¯ã€‚é€šè¿‡æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ç¼©æ”¾ç†è®ºï¼Œæˆ‘ä»¬è§£é‡Šäº†è¿™äº›ç°è±¡ï¼Œå¹¶éªŒè¯äº†æ‰©æ•£LLMsçš„ä¸Šä¸‹æ–‡å¤–æ¨æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14245', 'title': 'Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs', 'url': 'https://huggingface.co/papers/2506.14245', 'abstract': 'RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the Pass@K metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the Pass@K metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, CoT-Pass@K, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using CoT-Pass@K, we observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.', 'score': 25, 'issue_id': 4348, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'c78cc63a970ea4e9', 'authors': ['Xumeng Wen', 'Zihan Liu', 'Shun Zheng', 'Zhijian Xu', 'Shengyu Ye', 'Zhirong Wu', 'Xiao Liang', 'Yang Wang', 'Junjie Li', 'Ziming Miao', 'Jiang Bian', 'Mao Yang'], 'affiliations': ['Microsoft Research Asia', 'Peking University', 'The Chinese University of Hong Kong', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.14245.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RLVR: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ - Reinforcement Learning with Verifiable Rewards (RLVR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ CoT-Pass@K, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ RLVR Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ K. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» RLVR Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Machine Reasoning with RLVR and CoT-Pass@K', 'desc': 'Reinforcement Learning with Verifiable Rewards (RLVR) enhances the reasoning abilities of Large Language Models (LLMs) by promoting logical thought processes. The study identifies a flaw in the existing evaluation metric, Pass@K, which inaccurately rewards correct answers that may stem from faulty reasoning paths. To improve this, the authors propose a new metric, CoT-Pass@K, that ensures both the reasoning chain and the final answer are accurate. The findings demonstrate that RLVR can effectively encourage correct reasoning from the early stages of training, leading to better generalization across various scenarios.'}, 'zh': {'title': 'RLVRï¼šæ¨åŠ¨æœºå™¨æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'RLVRï¼ˆå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼‰é€šè¿‡æ¿€åŠ±æ­£ç¡®å’Œé€»è¾‘çš„æ€ç»´é“¾ï¼Œæ¨åŠ¨äº†æœºå™¨æ¨ç†çš„å‘å±•ã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡Pass@Kå­˜åœ¨ç¼ºé™·ï¼Œå¯èƒ½ä¼šé”™è¯¯åœ°è®¤å¯ä¸å®Œæ•´çš„æ€ç»´é“¾æ‰€å¾—åˆ°çš„æ­£ç¡®ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ›´ç²¾ç¡®çš„è¯„ä¼°æŒ‡æ ‡CoT-Pass@Kï¼Œè¦æ±‚æ¨ç†è·¯å¾„å’Œæœ€ç»ˆç­”æ¡ˆéƒ½å¿…é¡»æ­£ç¡®ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜ï¼ŒRLVRèƒ½å¤Ÿæœ‰æ•ˆæ¿€åŠ±æ­£ç¡®æ¨ç†çš„æ³›åŒ–ï¼Œå¹¶ä¸”è¿™ç§å¢å¼ºçš„æ¨ç†èƒ½åŠ›åœ¨è®­ç»ƒæ—©æœŸå°±èƒ½æ˜¾ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14234', 'title': 'Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team', 'url': 'https://huggingface.co/papers/2506.14234', 'abstract': "Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.", 'score': 24, 'issue_id': 4348, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '70ebdc96484832ea', 'authors': ['Md Tanzib Hosain', 'Salman Rahman', 'Md Kishor Morol', 'Md Rizwan Parvez'], 'affiliations': ['American International University-Bangladesh', 'Cornell University', 'Qatar Computing Research Institute', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.14234.jpg', 'data': {'categories': ['#training', '#agents', '#agi', '#open_source', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Xolver: ĞĞ¿Ñ‹Ñ‚-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Xolver - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ğ¸ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼-Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡. Xolver Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ. Ğ”Ğ°Ğ¶Ğµ Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Xolver Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Empowering Language Models with Experience-Aware Reasoning', 'desc': 'Xolver is a multi-agent reasoning framework designed to enhance large language models (LLMs) by incorporating persistent memory and diverse experience modalities. Unlike traditional LLMs that treat each problem independently, Xolver allows agents to accumulate knowledge from past experiences, similar to expert problem solvers. This framework integrates various methods such as self-retrieval, tool usage, and collaborative interactions to refine reasoning and improve performance on complex tasks. As a result, Xolver consistently outperforms specialized reasoning agents, achieving state-of-the-art results on several benchmarks, demonstrating the importance of experience-aware learning in AI.'}, 'zh': {'title': 'Xolverï¼šç»éªŒé©±åŠ¨çš„æ¨ç†æ¡†æ¶', 'desc': 'Xolveræ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æŒä¹…è®°å¿†å’Œå¤šæ ·åŒ–çš„ç»éªŒæ¨¡å¼å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»è€Œæé«˜å¤æ‚æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„LLMå­¤ç«‹å¤„ç†æ¯ä¸ªé—®é¢˜ä¸åŒï¼ŒXolverèƒ½å¤Ÿæ•´åˆå’Œç§¯ç´¯ç»éªŒçŸ¥è¯†ï¼Œæ¨¡æ‹Ÿä¸“å®¶é—®é¢˜è§£å†³è€…çš„æ€ç»´æ–¹å¼ã€‚å®ƒé€šè¿‡å¤–éƒ¨å’Œè‡ªæˆ‘æ£€ç´¢ã€å·¥å…·ä½¿ç”¨ã€åä½œäº’åŠ¨ç­‰å¤šç§ç»éªŒæ¨¡å¼ï¼Œé¿å…ä»å¤´ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚Xolveråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†æ•´ä½“ç»éªŒå­¦ä¹ åœ¨å®ç°é€šç”¨æ™ºèƒ½ä½“æ–¹é¢çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13363', 'title': 'Efficient Medical VIE via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.13363', 'abstract': 'An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE.', 'score': 22, 'issue_id': 4348, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '29de6bf10e7470ad', 'authors': ['Lijun Liu', 'Ruiyang Li', 'Zhaocheng Liu', 'Chenglin Zhu', 'Chong Li', 'Jiehan Cheng', 'Qiang Ju', 'Jian Xie'], 'affiliations': ['Baichuan Inc.', 'Peking University', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13363.jpg', 'data': {'categories': ['#hallucinations', '#training', '#optimization', '#healthcare', '#reasoning', '#rl', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'RLVR: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° RLVR Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-VL-7B Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². RLVR ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Medical VIE with Limited Data and Enhanced Reasoning', 'desc': "This paper presents a Reinforcement Learning with Verifiable Rewards (RLVR) framework that utilizes a fine-tuned Qwen2.5-VL-7B model to enhance Visual Information Extraction (VIE) in medical contexts. By leveraging only 100 annotated samples, the framework effectively balances precision and recall, addressing the challenges posed by limited annotated data and high annotation costs. The approach incorporates innovative sampling strategies and a balanced reward mechanism to improve reasoning capabilities and reduce hallucinations in the output. The results show significant improvements in F1 score, precision, and recall, although the model's performance varies with the similarity of the tasks to the training data."}, 'zh': {'title': 'åŒ»ç–—è§†è§‰ä¿¡æ¯æå–çš„åˆ›æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¾®è°ƒçš„Qwen2.5-VL-7Bæ¨¡å‹ï¼Œåœ¨åŒ»ç–—è§†è§‰ä¿¡æ¯æå–ï¼ˆVIEï¼‰ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä»…ä½¿ç”¨100ä¸ªæ ‡æ³¨æ ·æœ¬ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨åŒ»ç–—é¢†åŸŸé¢ä¸´çš„é«˜æ ‡æ³¨æˆæœ¬å’Œé¢†åŸŸç‰¹å®šæ¨¡å¼çš„é—®é¢˜ã€‚é€šè¿‡ç¡®ä¿æ•°æ®é›†çš„å¤šæ ·æ€§å’Œå¹³è¡¡çš„ç²¾ç¡®ç‡-å¬å›ç‡å¥–åŠ±æœºåˆ¶ï¼Œå‡å°‘äº†æ¨¡å‹çš„å¹»è§‰ç°è±¡ï¼Œå¹¶æé«˜äº†é¢†åŸŸè¦†ç›–ç‡ã€‚æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13642', 'title': 'Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model', 'url': 'https://huggingface.co/papers/2506.13642', 'abstract': 'Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.', 'score': 21, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '0d0624980a111254', 'authors': ['Shaolei Zhang', 'Shoutao Guo', 'Qingkai Fang', 'Yan Zhou', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13642.jpg', 'data': {'categories': ['#multimodal', '#audio', '#transfer_learning', '#cv', '#benchmark', '#agi'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Stream-Omni - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ‡ÑŒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ Ñ€ĞµÑ‡Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Stream-Omni Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Stream-Omni: Efficient Multimodal Integration for Enhanced Interaction', 'desc': "Stream-Omni is a large multimodal model that effectively integrates text, vision, and speech by using innovative alignment techniques. It employs sequence-dimension concatenation for aligning vision with text and a layer-dimension mapping for aligning speech with text, which allows for more efficient learning of modality relationships. This approach reduces the reliance on large datasets, particularly for speech, while still achieving strong performance across various multimodal tasks. The model's design enables it to provide intermediate outputs during speech interactions, enhancing the overall user experience in multimodal applications."}, 'zh': {'title': 'Stream-Omniï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ•´åˆæ¨¡å‹', 'desc': 'Stream-Omniæ˜¯ä¸€ç§å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œè¯­éŸ³ã€‚å®ƒé€šè¿‡åºåˆ—ç»´åº¦è¿æ¥å®ç°è§†è§‰ä¸æ–‡æœ¬çš„å¯¹é½ï¼Œå¹¶é€šè¿‡åŸºäºCTCçš„å±‚ç»´åº¦æ˜ å°„å®ç°è¯­éŸ³ä¸æ–‡æœ¬çš„å¯¹é½ï¼Œä»è€Œåœ¨æ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹æ”¯æŒå¤šç§æ¨¡æ€ç»„åˆçš„äº¤äº’ï¼Œèƒ½å¤Ÿåœ¨è§†è§‰ç†è§£ã€è¯­éŸ³äº¤äº’å’Œè§†è§‰å¼•å¯¼çš„è¯­éŸ³äº¤äº’ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚Stream-Omniçš„è®¾è®¡ä½¿å¾—ç”¨æˆ·åœ¨è¯­éŸ³äº¤äº’æ—¶å¯ä»¥åŒæ—¶è·å¾—ä¸­é—´æ–‡æœ¬è¾“å‡ºï¼Œæä¾›äº†å…¨é¢çš„å¤šæ¨¡æ€ä½“éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14758', 'title': 'Reasoning with Exploration: An Entropy Perspective', 'url': 'https://huggingface.co/papers/2506.14758', 'abstract': 'Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.', 'score': 17, 'issue_id': 4349, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '14595ff25bf8a37c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğ¹ Ñ‡Ğ»ĞµĞ½ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Pass@K. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Language Model Reasoning through Entropy-Driven Exploration', 'desc': 'This paper introduces a new approach to enhance exploratory reasoning in language models (LMs) by modifying the advantage function in reinforcement learning (RL) with an entropy-based term. The authors highlight that traditional methods often focus on exploitation, leading to performance plateaus, and argue that incorporating entropy can promote better exploration. Their empirical analysis shows that high-entropy regions correlate with key reasoning actions, such as pivotal tokens and reflective behaviors. The proposed method not only encourages deeper reasoning chains but also significantly improves performance on the Pass@K metric, demonstrating its effectiveness in advancing LM reasoning capabilities.'}, 'zh': {'title': 'å¢å¼ºè¯­è¨€æ¨¡å‹æ¨ç†çš„æ¢ç´¢æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç†µçš„æœ¯è¯­ï¼Œåº”ç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„ä¼˜åŠ¿å‡½æ•°ï¼Œä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¢ç´¢æ€§æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¼•å…¥ç†µä¿¡å·ï¼Œä¿ƒè¿›äº†æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡ï¼Œè§£å†³äº†è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ€§èƒ½åœæ»çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜ç†µåŒºåŸŸä¸ä¸‰ç§æ¢ç´¢æ€§æ¨ç†è¡Œä¸ºä¹‹é—´å­˜åœ¨å¼ºæ­£ç›¸å…³ï¼ŒåŒ…æ‹¬å…³é”®æ ‡è®°ã€åæ€æ€§è¡Œä¸ºå’Œç¨€æœ‰è¡Œä¸ºã€‚é€šè¿‡ç®€å•çš„ä»£ç ä¿®æ”¹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶åœ¨Pass@KæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.14603', 'title': 'Align Your Flow: Scaling Continuous-Time Flow Map Distillation', 'url': 'https://huggingface.co/papers/2506.14603', 'abstract': 'Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.', 'score': 13, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'c235653dff87ea28', 'authors': ['Amirmojtaba Sabour', 'Sanja Fidler', 'Karsten Kreis'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.14603.jpg', 'data': {'categories': ['#training', '#dataset', '#cv', '#benchmark', '#optimization', '#diffusion', '#small_models'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Flow maps: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'flow maps'. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑÑ‚ÑŒ Ğ»ÑĞ±Ñ‹Ğµ Ğ´Ğ²Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑˆÑƒĞ¼Ğ° Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑˆĞ°Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ flow maps. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ²."}, 'en': {'title': 'Flow Maps: Efficient Few-Step Generative Modeling', 'desc': 'This paper introduces flow maps, a new approach in generative modeling that connects different noise levels in a single step, allowing for efficient image and text-to-image generation. Unlike traditional diffusion and consistency models, which require many sampling steps and degrade in performance with increased steps, flow maps maintain effectiveness across all step counts. The authors propose two continuous-time training objectives and novel techniques that enhance the training of flow maps, including autoguidance and adversarial finetuning. The results demonstrate that their method, called Align Your Flow, achieves state-of-the-art performance in few-step generation tasks on various benchmarks, outperforming existing models in both image and text-conditioned synthesis.'}, 'zh': {'title': 'æµå›¾æ¨¡å‹ï¼šé«˜æ•ˆçš„å°‘æ­¥éª¤ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æµå›¾æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å›¾åƒå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆç‡ã€‚æµå›¾é€šè¿‡åœ¨å•ä¸€æ­¥éª¤ä¸­è¿æ¥ä»»æ„ä¸¤ä¸ªå™ªå£°æ°´å¹³ï¼Œå…‹æœäº†ä¼ ç»Ÿæ‰©æ•£å’Œæµæ¨¡å‹åœ¨å¤šæ­¥éª¤é‡‡æ ·ä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°çš„è¿ç»­æ—¶é—´ç›®æ ‡å’Œè®­ç»ƒæŠ€æœ¯ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†æµå›¾çš„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæµå›¾æ¨¡å‹åœ¨å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ­¥éª¤ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12860', 'title': 'QFFT, Question-Free Fine-Tuning for Adaptive Reasoning', 'url': 'https://huggingface.co/papers/2506.12860', 'abstract': 'Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios.', 'score': 13, 'issue_id': 4348, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': '4e8d6c1da3d2fdd1', 'authors': ['Wanlong Liu', 'Junxiao Xu', 'Fei Yu', 'Yukang Lin', 'Ke Ji', 'Wenyu Chen', 'Yan Xu', 'Yasheng Wang', 'Lifeng Shang', 'Benyou Wang'], 'affiliations': ['Huawei Noahs Ark Lab', 'The Chinese University of Hong Kong, Shenzhen', 'University of Electronic Science and Technology of China, Chengdu, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12860.jpg', 'data': {'categories': ['#training', '#math', '#long_context', '#reasoning', '#low_resource'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'QFFT: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Question-Free Fine-Tuning (QFFT). QFFT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50%, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. QFFT Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼, Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ….'}, 'en': {'title': 'Efficient Reasoning with Question-Free Fine-Tuning', 'desc': 'This paper introduces Question-Free Fine-Tuning (QFFT), a method that enhances cognitive models by combining short and long chain-of-thought reasoning patterns. QFFT addresses the issue of overthinking in Long Chain-of-Thought models, which can lead to unnecessary complexity in responses. By training models without input questions, QFFT allows them to learn from Long CoT responses while primarily using Short CoT patterns for efficiency. The results show that QFFT significantly reduces response length and performs well across various challenging scenarios, outperforming traditional Supervised Fine-Tuning methods in specific contexts.'}, 'zh': {'title': 'æ— é—®å¾®è°ƒï¼šé«˜æ•ˆé€‚åº”çš„æ¨ç†æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºæ— é—®å¾®è°ƒï¼ˆQFFTï¼‰ï¼Œæ—¨åœ¨æé«˜è®¤çŸ¥æ¨¡å‹çš„æ•ˆç‡å’Œé€‚åº”æ€§ã€‚é€šè¿‡ç»“åˆçŸ­é“¾å’Œé•¿é“¾æ¨ç†æ¨¡å¼ï¼ŒQFFTèƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘å“åº”é•¿åº¦ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒçŸ­é“¾æ¨ç†åœ¨ç®€å•é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œé•¿é“¾æ¨ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­æ›´å…·ä¼˜åŠ¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQFFTåœ¨å¤šä¸ªæ•°å­¦æ•°æ®é›†ä¸Šå¹³å‡å“åº”é•¿åº¦å‡å°‘è¶…è¿‡50%ï¼Œå¹¶åœ¨å™ªå£°ã€åŸŸå¤–å’Œä½èµ„æºåœºæ™¯ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12278', 'title': 'Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure', 'url': 'https://huggingface.co/papers/2506.12278', 'abstract': 'TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.', 'score': 13, 'issue_id': 4348, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': 'c852db550c523453', 'authors': ['Zheyuan Yang', 'Zexi Kuang', 'Xue Xia', 'Yilun Zhao'], 'affiliations': ['HKUST', 'Northeastern University', 'Tongji University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12278.jpg', 'data': {'categories': ['#open_source', '#optimization', '#benchmark'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'TestCase-Eval: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¯Ğœ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²', 'desc': 'TestCase-Eval - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ¯Ğœ) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 500 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 100 000 Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Codeforces. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 19 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¯Ğœ Ğ½Ğ° TestCase-Eval, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ².'}, 'en': {'title': 'Evaluating LLMs for Effective Test Case Generation', 'desc': 'TestCase-Eval is a benchmark designed to assess the performance of large language models (LLMs) in generating effective test cases for algorithmic problems. It consists of 500 algorithm problems paired with 100,000 human-created solutions sourced from the Codeforces platform. The evaluation focuses on two main aspects: Fault Coverage, which checks how well the generated test cases explore various input scenarios, and Fault Exposure, which determines the ability of LLMs to create specific test inputs that can uncover flaws in code implementations. The study evaluates 19 different LLMs, providing valuable insights into their capabilities and limitations in this area.'}, 'zh': {'title': 'è¯„ä¼°LLMç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„æ–°åŸºå‡†', 'desc': 'TestCase-Evalæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆç®—æ³•é—®é¢˜æµ‹è¯•ç”¨ä¾‹çš„æ–°åŸºå‡†ã€‚å®ƒåŒ…å«500ä¸ªç®—æ³•é—®é¢˜å’Œæ¥è‡ªCodeforceså¹³å°çš„100,000ä¸ªäººå·¥è§£å†³æ–¹æ¡ˆã€‚è¯¥åŸºå‡†å…³æ³¨ä¸¤ä¸ªå…³é”®ä»»åŠ¡ï¼šæ•…éšœè¦†ç›–æ€§ï¼Œè¯„ä¼°LLMç”Ÿæˆçš„æµ‹è¯•é›†æ˜¯å¦èƒ½å¤Ÿæ¢æµ‹å¤šæ ·çš„è¾“å…¥åœºæ™¯ï¼›æ•…éšœæš´éœ²æ€§ï¼Œè¯„ä¼°LLMæ˜¯å¦èƒ½å¤Ÿç”Ÿæˆç‰¹å®šçš„æµ‹è¯•è¾“å…¥ä»¥æ­ç¤ºä»£ç å®ç°ä¸­çš„é”™è¯¯ã€‚æˆ‘ä»¬å¯¹19ä¸ªæœ€å…ˆè¿›çš„å¼€æºå’Œä¸“æœ‰LLMåœ¨TestCase-Evalä¸Šçš„è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæä¾›äº†å®ƒä»¬åœ¨ç”Ÿæˆæœ‰æ•ˆæµ‹è¯•ç”¨ä¾‹æ–¹é¢çš„ä¼˜ç¼ºç‚¹çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14606', 'title': 'Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees', 'url': 'https://huggingface.co/papers/2506.14606', 'abstract': 'A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.', 'score': 10, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'c414a1f31e0417da', 'authors': ['Ahmed Heakl', 'Sarim Hashmi', 'Chaimaa Abi', 'Celine Lee', 'Abdulrahman Mahmoud'], 'affiliations': ['Cornell University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14606.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#benchmark', '#data', '#science'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¯ĞœĞ‘ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ GG (Guaranteed Guess) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ĞºĞ¾Ğ´Ğ° Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¯ĞœĞ‘ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° (99% Ğ´Ğ»Ñ HumanEval Ğ¸ 49% Ğ´Ğ»Ñ BringupBench) Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient ISA Translation with Guaranteed Guess', 'desc': 'This paper presents a new transpilation pipeline called GG (Guaranteed Guess) that focuses on translating programs between complex (CISC) and reduced (RISC) instruction set architectures (ISAs). By leveraging large language models (LLMs) for generating translation candidates, the pipeline integrates software testing to ensure high correctness and efficiency. The authors demonstrate that their approach achieves over 99% functional correctness on specific benchmarks and outperforms the existing Rosetta 2 framework in terms of runtime speed, energy efficiency, and memory usage. The research aims to enhance the portability of code across different hardware architectures and will provide open-source resources for further exploration in ISA-level code translation.'}, 'zh': {'title': 'é«˜æ•ˆå‡†ç¡®çš„ISAè½¬è¯‘æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ISAä¸­å¿ƒçš„è½¬è¯‘ç®¡é“ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè½¯ä»¶æµ‹è¯•æŠ€æœ¯ï¼Œå®ç°äº†åœ¨å¤æ‚å’Œç®€åŒ–ç¡¬ä»¶æ¶æ„ä¹‹é—´çš„é«˜æ•ˆä¸”æ­£ç¡®çš„ä»£ç è½¬æ¢ã€‚è¯¥æ–¹æ³•é€šè¿‡LLMç”Ÿæˆå€™é€‰ç¿»è¯‘ï¼Œå¹¶å°†å…¶åµŒå…¥è½¯ä»¶æµ‹è¯•æ¡†æ¶ä¸­ï¼Œä»¥æé«˜ç¿»è¯‘çš„å¯é æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œè¾¾åˆ°äº†99%çš„åŠŸèƒ½å’Œè¯­ä¹‰æ­£ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„Rosetta 2æ¡†æ¶ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å°†å¼€æºä»£ç ã€æ•°æ®ã€æ¨¡å‹å’ŒåŸºå‡†ï¼Œä»¥æ¨åŠ¨ISAçº§ä»£ç ç¿»è¯‘ç ”ç©¶çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13977', 'title': 'CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios', 'url': 'https://huggingface.co/papers/2506.13977', 'abstract': 'A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at https://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.', 'score': 8, 'issue_id': 4351, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'd72fbcfec0e28e92', 'authors': ['Shiting Huang', 'Zhen Fang', 'Zehui Chen', 'Siyu Yuan', 'Junjie Ye', 'Yu Zeng', 'Lin Chen', 'Qi Mao', 'Feng Zhao'], 'affiliations': ['Communication University of China', 'Fudan University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13977.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#dataset', '#optimization'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'CRITICTOOL - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. CRITICTOOL Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing LLM Robustness with CRITICTOOL', 'desc': 'This paper introduces CRITICTOOL, a benchmark designed to assess and improve the robustness of large language models (LLMs) when using external tools. It identifies and categorizes various errors that can occur during the function-calling process, especially as tasks become more complex. The benchmark employs an innovative evolutionary strategy for dataset construction, ensuring a wide range of tool-use errors that mimic real-world challenges. Through extensive experiments, the authors demonstrate the effectiveness of CRITICTOOL in enhancing the error-handling capabilities of LLMs and provide insights into their tool reflection abilities.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨çš„é²æ£’æ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†CRITICTOOLï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°å’Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨å·¥å…·æ—¶å¤„ç†é”™è¯¯çš„èƒ½åŠ›ã€‚éšç€ä»»åŠ¡çš„å¤æ‚æ€§å¢åŠ ï¼Œå·¥å…·ä½¿ç”¨è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå‡ºç°å„ç§æ„å¤–é”™è¯¯ï¼Œå› æ­¤æœ‰æ•ˆå¤„ç†è¿™äº›é”™è¯¯æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬åˆ†æäº†åœ¨å¤šä¸ªç«äº‰æ€§å·¥å…·è¯„ä¼°åŸºå‡†ä¸­é‡åˆ°çš„é”™è¯¯ç±»å‹ï¼Œå¹¶åŸºäºæ­¤æ„å»ºäº†CRITICTOOLï¼Œä¸“æ³¨äºå·¥å…·å­¦ä¹ çš„æ‰¹åˆ¤æ€§è¯„ä¼°ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†è¯¥åŸºå‡†ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æä¾›äº†å¯¹ä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹å·¥å…·ååº”èƒ½åŠ›çš„æ·±å…¥åˆ†æã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09985', 'title': 'V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\n  and Planning', 'url': 'https://huggingface.co/papers/2506.09985', 'abstract': 'A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.  \t\t\t\t\tAI-generated summary \t\t\t\t A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.', 'score': 7, 'issue_id': 4359, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '2a9d0d368d8caa0a', 'authors': ['Mido Assran', 'Adrien Bardes', 'David Fan', 'Quentin Garrido', 'Russell Howes', 'Mojtaba', 'Komeili', 'Matthew Muckley', 'Ammar Rizvi', 'Claire Roberts', 'Koustuv Sinha', 'Artem Zholus', 'Sergio Arnaud', 'Abha Gejji', 'Ada Martin', 'Francois Robert Hogan', 'Daniel Dugas', 'Piotr Bojanowski', 'Vasil Khalidov', 'Patrick Labatut', 'Francisco Massa', 'Marc Szafraniec', 'Kapil Krishnakumar', 'Yong Li', 'Xiaodong Ma', 'Sarath Chandar', 'Franziska Meier', 'Yann LeCun', 'Michael Rabbat', 'Nicolas Ballas'], 'affiliations': ['FAIR at Meta', 'Mila Quebec AI Institute and Polytechnique MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2506.09985.jpg', 'data': {'categories': ['#multimodal', '#games', '#transfer_learning', '#dataset', '#agi', '#cv', '#robotics', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ V-JEPA 2 Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğµ Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸĞ¾ÑĞ»Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, V-JEPA 2 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 62 Ñ‡Ğ°ÑĞ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ V-JEPA 2-AC Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ.'}, 'en': {'title': 'Learning to Act by Watching: Self-Supervised Motion Understanding and Planning', 'desc': 'This paper presents a self-supervised learning method that leverages vast amounts of internet video data alongside minimal robot interaction to enhance motion understanding, action anticipation, and robotic planning. The authors introduce V-JEPA 2, a joint-embedding-predictive architecture pre-trained on over 1 million hours of video, achieving impressive results in various tasks without the need for specific training or rewards. By aligning V-JEPA 2 with a large language model, they also achieve state-of-the-art performance in video question-answering tasks. Furthermore, they demonstrate the application of this approach in robotic planning, enabling robots to perform tasks like object manipulation using learned models without additional data collection.'}, 'zh': {'title': 'è‡ªç›‘ç£å­¦ä¹ ï¼šä»è§†é¢‘åˆ°æœºå™¨äººè§„åˆ’çš„çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆäº†äº’è”ç½‘è§†é¢‘æ•°æ®å’Œå°‘é‡æœºå™¨äººäº¤äº’æ•°æ®ï¼Œä»¥å®ç°è¿åŠ¨ç†è§£ã€åŠ¨ä½œé¢„æµ‹ã€è§†é¢‘é—®ç­”å’Œæœºå™¨äººè§„åˆ’ç­‰ä»»åŠ¡ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨è¶…è¿‡100ä¸‡å°æ—¶çš„è§†é¢‘æ•°æ®é›†ä¸Šé¢„è®­ç»ƒäº†ä¸€ä¸ªæ— åŠ¨ä½œçš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„V-JEPA 2ï¼Œå–å¾—äº†è¿åŠ¨ç†è§£å’Œäººç±»åŠ¨ä½œé¢„æµ‹çš„ä¼˜å¼‚è¡¨ç°ã€‚é€šè¿‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½ï¼ŒV-JEPA 2åœ¨å¤šä¸ªè§†é¢‘é—®ç­”ä»»åŠ¡ä¸Šä¹Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†è‡ªç›‘ç£å­¦ä¹ åº”ç”¨äºæœºå™¨äººè§„åˆ’ä»»åŠ¡ï¼ŒæˆåŠŸå®ç°äº†åœ¨ä¸åŒå®éªŒå®¤ä¸­ä½¿ç”¨V-JEPA 2-ACè¿›è¡Œç‰©ä½“çš„æŠ“å–å’Œæ”¾ç½®ï¼Œè€Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæˆ–å¥–åŠ±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13651', 'title': 'xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations', 'url': 'https://huggingface.co/papers/2506.13651', 'abstract': "We introduce xbench, a dynamic, profession-aligned evaluation suite designed to bridge the gap between AI agent capabilities and real-world productivity. While existing benchmarks often focus on isolated technical skills, they may not accurately reflect the economic value agents deliver in professional settings. To address this, xbench targets commercially significant domains with evaluation tasks defined by industry professionals. Our framework creates metrics that strongly correlate with productivity value, enables prediction of Technology-Market Fit (TMF), and facilitates tracking of product capabilities over time. As our initial implementations, we present two benchmarks: Recruitment and Marketing. For Recruitment, we collect 50 tasks from real-world headhunting business scenarios to evaluate agents' abilities in company mapping, information retrieval, and talent sourcing. For Marketing, we assess agents' ability to match influencers with advertiser needs, evaluating their performance across 50 advertiser requirements using a curated pool of 836 candidate influencers. We present initial evaluation results for leading contemporary agents, establishing a baseline for these professional domains. Our continuously updated evalsets and evaluations are available at https://xbench.org.", 'score': 6, 'issue_id': 4351, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'ce4f94d367671b84', 'authors': ['Kaiyuan Chen', 'Yixin Ren', 'Yang Liu', 'Xiaobo Hu', 'Haotong Tian', 'Tianbao Xie', 'Fangfu Liu', 'Haoye Zhang', 'Hongzhang Liu', 'Yuan Gong', 'Chen Sun', 'Han Hou', 'Hui Yang', 'James Pan', 'Jianan Lou', 'Jiayi Mao', 'Jizheng Liu', 'Jinpeng Li', 'Kangyi Liu', 'Kenkun Liu', 'Rui Wang', 'Run Li', 'Tong Niu', 'Wenlong Zhang', 'Wenqi Yan', 'Xuanzheng Wang', 'Yuchen Zhang', 'Yi-Hsin Hung', 'Yuan Jiang', 'Zexuan Liu', 'Zihan Yin', 'Zijian Ma', 'Zhiwen Mo'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Imperial College London', 'Massachusetts Institute of Technology', 'National University of Singapore', 'Peking University', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong (Shenzhen)', 'The Ohio State University', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'University of Oxford', 'University of Pennsylvania', 'University of Science and Technology of China', 'University of Sydney', 'University of Toronto', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13651.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'xbench: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ xbench - Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², xbench Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ñ€Ñ‹Ğ½ĞºÑƒ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°: Ğ´Ğ»Ñ Ñ€ĞµĞºÑ€ÑƒÑ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Bridging AI Performance with Real-World Productivity', 'desc': "The paper introduces xbench, a new evaluation suite aimed at assessing AI agents in real-world professional contexts. Unlike traditional benchmarks that focus on isolated skills, xbench emphasizes the economic impact of AI agents in industries like recruitment and marketing. It includes tasks defined by industry experts to ensure relevance and creates metrics that correlate with productivity value. The initial benchmarks evaluate agents' performance in real-world scenarios, providing a baseline for future assessments and tracking improvements over time."}, 'zh': {'title': 'xbenchï¼šè¿æ¥AIèƒ½åŠ›ä¸çœŸå®ç”Ÿäº§åŠ›çš„æ¡¥æ¢', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†xbenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€çš„ã€ä¸èŒä¸šç›¸å…³çš„è¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨å¼¥åˆäººå·¥æ™ºèƒ½ä»£ç†èƒ½åŠ›ä¸ç°å®ä¸–ç•Œç”Ÿäº§åŠ›ä¹‹é—´çš„å·®è·ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸å…³æ³¨å­¤ç«‹çš„æŠ€æœ¯æŠ€èƒ½ï¼Œä½†å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ ä»£ç†åœ¨ä¸“ä¸šç¯å¢ƒä¸­æ‰€å¸¦æ¥çš„ç»æµä»·å€¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œxbenché’ˆå¯¹å•†ä¸šä¸Šé‡è¦çš„é¢†åŸŸï¼Œè¯„ä¼°ä»»åŠ¡ç”±è¡Œä¸šä¸“ä¸šäººå£«å®šä¹‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ›å»ºäº†ä¸ç”Ÿäº§åŠ›ä»·å€¼é«˜åº¦ç›¸å…³çš„æŒ‡æ ‡ï¼Œèƒ½å¤Ÿé¢„æµ‹æŠ€æœ¯å¸‚åœºå¥‘åˆåº¦ï¼ˆTMFï¼‰ï¼Œå¹¶ä¾¿äºè·Ÿè¸ªäº§å“èƒ½åŠ›çš„å˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10100', 'title': 'EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2506.10100', 'abstract': 'EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.', 'score': 6, 'issue_id': 4348, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '6a877c4c5f1d1f72', 'authors': ['Yantai Yang', 'Yuhao Wang', 'Zichen Wen', 'Luo Zhongwei', 'Chang Zou', 'Zhipeng Zhang', 'Chuan Wen', 'Linfeng Zhang'], 'affiliations': ['Harbin Institute of Technology', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'University of Electronic Science and Technology of China', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10100.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#architecture', '#inference', '#multimodal'], 'emoji': 'ğŸš€', 'ru': {'title': 'EfficientVLA: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ VLA Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'EfficientVLA - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision-Language-Action (VLA). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğµ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ EfficientVLA Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CogACT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 1,93 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ FLOP Ğ½Ğ° 71,1% Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… VLA.'}, 'en': {'title': 'Accelerating VLA Models with EfficientVLA', 'desc': 'EfficientVLA is a framework designed to speed up Vision-Language-Action (VLA) models by addressing their high computational and memory requirements. It achieves this by pruning unnecessary language layers, optimizing the selection of visual tokens, and caching important features during the action generation process. This approach not only reduces the overall processing time but also minimizes the number of floating-point operations (FLOPs) needed for inference. As a result, EfficientVLA significantly enhances the efficiency of VLA models while maintaining a high level of performance.'}, 'zh': {'title': 'é«˜æ•ˆåŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'EfficientVLAæ˜¯ä¸€ç§åŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡ä¿®å‰ªè¯­è¨€å±‚ã€ä¼˜åŒ–è§†è§‰æ ‡è®°é€‰æ‹©å’Œç¼“å­˜ä¸­é—´ç‰¹å¾æ¥æé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•ç³»ç»Ÿæ€§åœ°æ¶ˆé™¤äº†è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆï¼Œè§£å†³äº†ç°æœ‰åŠ é€Ÿæ–¹æ³•æ— æ³•å…¨é¢åº”å¯¹çš„é—®é¢˜ã€‚é€šè¿‡åˆ†æå±‚é—´å†—ä½™ï¼ŒEfficientVLAå»é™¤äº†åŠŸèƒ½ä¸é‡è¦çš„è¯­è¨€æ¨¡å—å±‚ï¼Œå¹¶é‡‡ç”¨ä»»åŠ¡æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–è§†è§‰å¤„ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåº”ç”¨EfficientVLAåï¼Œæ ‡å‡†VLAæ¨¡å‹CogACTçš„æ¨ç†é€Ÿåº¦æé«˜äº†1.93å€ï¼ŒFLOPså‡å°‘è‡³28.9%ï¼ŒæˆåŠŸç‡ä»…ä¸‹é™0.6%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14002', 'title': 'Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders', 'url': 'https://huggingface.co/papers/2506.14002', 'abstract': "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically prove that this algorithm correctly recovers all monosemantic features when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.", 'score': 5, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'f27daadffcce7200', 'authors': ['Siyu Chen', 'Heejune Sheen', 'Xuyuan Xiong', 'Tianhao Wang', 'Zhuoran Yang'], 'affiliations': ['Antai College of Economics and Management, Shanghai Jiao Tong University', 'Department of Statistics and Data Science, Yale University', 'Toyota Technological Institute at Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2506.14002.jpg', 'data': {'categories': ['#training', '#architecture', '#interpretability', '#math', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Group Bias Adaptation Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² (Sparse Autoencoders). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Large Language Models) Ñ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑĞ¸ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ´Ğ¾ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Feature Recovery in Language Models with Group Bias Adaptation', 'desc': 'This paper introduces a new method called Group Bias Adaptation (GBA) to improve Sparse Autoencoders (SAEs) for extracting clear features from Large Language Models (LLMs). The authors address the limitations of existing SAE training methods, which often lack solid mathematical backing and can be unstable. They propose a statistical framework that models complex features as combinations of simpler, clear concepts, ensuring better feature recovery. The new training algorithm not only provides theoretical guarantees for recovering these features but also shows better performance compared to traditional methods when tested on large models.'}, 'zh': {'title': 'ç¾¤ä½“åå·®é€‚åº”ï¼šæå‡ç¨€ç–è‡ªç¼–ç å™¨çš„å•ä¹‰ç‰¹å¾æ¢å¤èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç»Ÿè®¡æ¡†æ¶å’Œè®­ç»ƒç®—æ³•ï¼Œç§°ä¸ºç¾¤ä½“åå·®é€‚åº”ï¼ˆGroup Bias Adaptationï¼‰ï¼Œæ—¨åœ¨å¢å¼ºç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencodersï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å•ä¹‰ç‰¹å¾æ¢å¤èƒ½åŠ›ã€‚ç°æœ‰çš„ç¨€ç–è‡ªç¼–ç å™¨è®­ç»ƒç®—æ³•ç¼ºä¹ä¸¥æ ¼çš„æ•°å­¦ä¿è¯ï¼Œå¹¶ä¸”åœ¨è¶…å‚æ•°æ•æ„Ÿæ€§å’Œä¸ç¨³å®šæ€§æ–¹é¢å­˜åœ¨å®é™…é™åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ç‰¹å¾å¯è¯†åˆ«æ€§çš„æ–°æ¦‚å¿µï¼Œè§£å†³äº†ç‰¹å¾æ¢å¤é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºåå·®é€‚åº”çš„æ–°è®­ç»ƒç®—æ³•ã€‚ç†è®ºè¯æ˜è¯¥ç®—æ³•èƒ½å¤Ÿåœ¨ç‰¹å®šç»Ÿè®¡æ¨¡å‹ä¸‹æ­£ç¡®æ¢å¤æ‰€æœ‰å•ä¹‰ç‰¹å¾ï¼Œä»è€Œä¸ºç¨€ç–è‡ªç¼–ç å™¨çš„è®­ç»ƒæä¾›äº†ç†è®ºæ”¯æŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10038', 'title': 'Ambient Diffusion Omni: Training Good Models with Bad Data', 'url': 'https://huggingface.co/papers/2506.10038', 'abstract': 'Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.', 'score': 5, 'issue_id': 4349, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'f746e9dd50fb7b78', 'authors': ['Giannis Daras', 'Adrian Rodriguez-Munoz', 'Adam Klivans', 'Antonio Torralba', 'Constantinos Daskalakis'], 'affiliations': ['Massachusetts Institute of Technology', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.10038.jpg', 'data': {'categories': ['#training', '#cv', '#dataset', '#synthetic', '#diffusion', '#data'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñ‹ Ğ¸Ğ· ÑˆÑƒĞ¼Ğ°: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ambient Diffusion Omni, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¾Ñ‚Ğ±Ñ€Ğ°ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ FID Ğ½Ğ° ImageNet Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑĞ¼ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½ĞµÑĞ¼ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking Potential: Enhancing Diffusion Models with Low-Quality Images', 'desc': 'The Ambient Diffusion Omni framework enhances diffusion models by effectively utilizing low-quality images, which are often overlooked. It demonstrates that these lower-quality images can significantly improve model performance on tasks like text-to-image generation. The framework leverages natural image properties, such as spectral power law decay and locality, to extract valuable signals during training. By validating its approach with various synthetic corruptions, the framework achieves state-of-the-art results in ImageNet FID, showcasing improved image quality and diversity.'}, 'zh': {'title': 'åˆ©ç”¨ä½è´¨é‡å›¾åƒæå‡æ‰©æ•£æ¨¡å‹çš„è´¨é‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAmbient Diffusion Omniçš„æ¡†æ¶ï¼Œåˆ©ç”¨ä½è´¨é‡å›¾åƒæ¥æå‡æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šå¸¸è¢«ä¸¢å¼ƒçš„ä½è´¨é‡å›¾åƒå®é™…ä¸Šå…·æœ‰å¾ˆå¤§çš„ä»·å€¼ï¼Œå¯ä»¥æ”¹å–„æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚è¯¥æ¡†æ¶åˆ©ç”¨è‡ªç„¶å›¾åƒçš„ä¸¤ä¸ªç‰¹æ€§â€”â€”è°±åŠŸç‡æ³•åˆ™è¡°å‡å’Œå±€éƒ¨æ€§ï¼ŒæˆåŠŸåœ°ä»åˆæˆæ¨¡ç³Šã€JPEGå‹ç¼©å’Œè¿åŠ¨æ¨¡ç³Šçš„å›¾åƒä¸­æå–ä¿¡å·ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åœ¨ImageNet FIDä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å›¾åƒè´¨é‡å’Œå¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05336', 'title': 'VideoMolmo: Spatio-Temporal Grounding Meets Pointing', 'url': 'https://huggingface.co/papers/2506.05336', 'abstract': 'VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at https://github.com/mbzuai-oryx/VideoMolmo.', 'score': 5, 'issue_id': 4348, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'def5ee56ea3b6157', 'authors': ['Ghazi Shazan Ahmad', 'Ahmed Heakl', 'Hanan Gani', 'Abdelrahman Shaker', 'Zhiqiang Shen', 'Ranjay Krishna', 'Fahad Shahbaz Khan', 'Salman Khan'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Australian National University', 'LinkÃ¶ping University', 'Mohamed Bin Zayed University of Artificial Intelligence', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.05336.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#benchmark', '#open_source', '#reasoning', '#video', '#multimodal'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'VideoMolmo - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ SAM2 Ğ´Ğ»Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑĞ¾Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. VideoMolmo Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'Enhancing Spatio-Temporal Reasoning with VideoMolmo', 'desc': 'VideoMolmo is a multimodal model designed to improve spatio-temporal pointing accuracy and reasoning in various real-world applications. It combines a temporal attention mechanism with a novel mask fusion technique called SAM2, which enhances the coherence of video sequences. By generating precise pointing coordinates through a large language model and then refining them with a mask-fusion module, VideoMolmo simplifies the task and improves interpretability. The model is evaluated on a newly curated dataset and a challenging benchmark, demonstrating significant advancements over existing video-based approaches.'}, 'zh': {'title': 'VideoMolmoï¼šæå‡æ—¶ç©ºæŒ‡å‘ä¸æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡å‹', 'desc': 'VideoMolmoæ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨¡å‹ï¼Œç»“åˆäº†æ—¶é—´æ³¨æ„æœºåˆ¶å’ŒSAM2è¿›è¡Œæ©è†œèåˆï¼Œæ˜¾è‘—æé«˜äº†æ—¶ç©ºæŒ‡å‘çš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ä¸“ä¸ºåŸºäºæ–‡æœ¬æè¿°çš„ç»†ç²’åº¦æ—¶ç©ºæŒ‡å‘è€Œè®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„çœŸå®åœºæ™¯ä¸­è¿›è¡Œç²¾ç¡®äº¤äº’ã€‚é€šè¿‡å¼•å…¥æ—¶é—´æ¨¡å—å’ŒåŒå‘ç‚¹ä¼ æ’­çš„æ©è†œèåˆç®¡é“ï¼ŒVideoMolmoç¡®ä¿äº†è§†é¢‘åºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§å’Œè¿è´¯æ€§ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«72,000ä¸ªè§†é¢‘-å­—å¹•å¯¹çš„æ•°æ®é›†ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤šç§çœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14755', 'title': 'Optimizing Length Compression in Large Reasoning Models', 'url': 'https://huggingface.co/papers/2506.14755', 'abstract': 'LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.', 'score': 4, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '837a56d067dd6e74', 'authors': ['Zhengxiang Cheng', 'Dongping Chen', 'Mingyang Fu', 'Tianyi Zhou'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.14755.jpg', 'data': {'categories': ['#reasoning', '#training', '#architecture', '#benchmark', '#optimization'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'LC-R1: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'LC-R1 - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½ Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. LC-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 50% Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 2%.'}, 'en': {'title': 'Streamlining Reasoning: LC-R1 for Efficient Large Models', 'desc': "The paper introduces LC-R1, a post-training method aimed at improving Large Reasoning Models (LRMs) by reducing unnecessary reasoning while maintaining accuracy. It identifies 'invalid thinking' as a key issue where models redundantly verify correct answers, leading to verbosity. To combat this, the authors propose two principles: Brevity, which focuses on cutting out redundant reasoning, and Sufficiency, which ensures essential reasoning steps are retained. Through experiments, LC-R1 demonstrates a significant reduction in reasoning sequence length by about 50% with only a slight accuracy drop of around 2%, showcasing an effective balance between compression and performance."}, 'zh': {'title': 'ç®€åŒ–æ¨ç†ï¼Œæå‡æ•ˆç‡ï¼', 'desc': 'LC-R1æ˜¯ä¸€ç§åè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç®€æ´æ€§å’Œå……åˆ†æ€§åŸåˆ™æ¥å‡å°‘å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„ä¸å¿…è¦æ¨ç†ï¼ŒåŒæ—¶ä¿æŒè¾ƒå°çš„å‡†ç¡®æ€§æŸå¤±ã€‚è¯¥æ–¹æ³•è¯†åˆ«å‡ºæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çš„â€œæ— æ•ˆæ€ç»´â€é—®é¢˜ï¼Œå³æ¨¡å‹åœ¨å¾—å‡ºæ­£ç¡®ç­”æ¡ˆåä»ç„¶åå¤æ£€æŸ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä½æ•ˆé—®é¢˜ï¼ŒLC-R1æå‡ºäº†ä¸¤ä¸ªæ–°åŸåˆ™ï¼šç®€æ´æ€§ï¼Œå¼ºè°ƒæ¶ˆé™¤å†—ä½™ï¼›å……åˆ†æ€§ï¼Œç¡®ä¿å…³é”®æ¨ç†æ­¥éª¤å¾—ä»¥ä¿ç•™ã€‚é€šè¿‡å¯¹å¤šä¸ªæ¨ç†åŸºå‡†çš„å¹¿æ³›å®éªŒï¼ŒLC-R1å®ç°äº†åºåˆ—é•¿åº¦çš„æ˜¾è‘—å‡å°‘ï¼ˆçº¦50%ï¼‰ï¼Œè€Œå‡†ç¡®æ€§ä»…ä¸‹é™çº¦2%ï¼Œåœ¨é«˜å‹ç¼©ç‡å’Œå‡†ç¡®æ€§ä¹‹é—´è¾¾æˆäº†è‰¯å¥½çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09033', 'title': 'Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.09033', 'abstract': 'Router-R1, a reinforcement learning-based framework, improves multi-LLM routing by interleaving think and route actions, optimizing performance-cost trade-offs, and generalizing to unseen models.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (i.e., assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present Router-R1, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1.', 'score': 3, 'issue_id': 4357, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '6f6eee917a3ef0d9', 'authors': ['Haozhen Zhang', 'Tao Feng', 'Jiaxuan You'], 'affiliations': ['University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.09033.jpg', 'data': {'categories': ['#rlhf', '#rl', '#multimodal', '#optimization', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Router-R1 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Router-R1 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ Ñ†ĞµĞ½Ñ‹ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸.'}, 'en': {'title': 'Optimizing Multi-LLM Routing with Reinforcement Learning', 'desc': "Router-R1 is a reinforcement learning framework designed to enhance the routing of user queries among multiple large language models (LLMs). Unlike traditional routers that assign queries to a single model, Router-R1 interleaves 'think' and 'route' actions, allowing it to consider multiple models' strengths for complex tasks. It uses a rule-based reward system to optimize the balance between performance and cost, making it efficient in selecting the best model for each query. The framework demonstrates strong generalization capabilities, performing well on various benchmarks while managing costs effectively."}, 'zh': {'title': 'æ™ºèƒ½è·¯ç”±ï¼Œä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬çš„å¹³è¡¡', 'desc': 'Router-R1 æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è·¯ç”±ã€‚å®ƒé€šè¿‡äº¤æ›¿è¿›è¡Œæ€è€ƒå’Œè·¯ç”±åŠ¨ä½œï¼Œä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„å•è½®ä¸€å¯¹ä¸€æ˜ å°„ä¸åŒï¼ŒRouter-R1 å°†è·¯ç”±å’Œèšåˆè§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œåˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›è¿›è¡ŒåŠ¨æ€æ¨¡å‹è°ƒç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRouter-R1 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œæˆæœ¬ç®¡ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14761', 'title': 'From Bytes to Ideas: Language Modeling with Autoregressive U-Nets', 'url': 'https://huggingface.co/papers/2506.14761', 'abstract': 'An autoregressive U-Net learns to embed its own tokens during training, enabling a multi-scale view of text sequences and improved handling of character-level tasks and low-resource languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.', 'score': 2, 'issue_id': 4362, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '24f72bf9457e3acf', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#architecture', '#data', '#optimization', '#low_resource', '#multilingual'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€Ğ°ÑÑĞ¸Ğ²Ğ½ÑƒÑ U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ¹Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ² ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ Ñ„Ñ€Ğ°Ğ·Ñ‹, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Flexible Tokenization for Enhanced Text Understanding', 'desc': "This paper presents an autoregressive U-Net model that learns to create its own token embeddings during training, allowing for a flexible approach to text processing. By reading raw bytes and progressively pooling them into larger units, the model gains a multi-scale perspective on text sequences. This design enables the model to predict further into the future at deeper layers, focusing on broader semantic patterns while earlier layers manage finer details. The approach not only improves performance on character-level tasks but also enhances the model's ability to work with low-resource languages by integrating tokenization within the model itself."}, 'zh': {'title': 'è‡ªå›å½’U-Netï¼šçµæ´»å¤„ç†æ–‡æœ¬çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§è‡ªå›å½’U-Netæ¨¡å‹ï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ åµŒå…¥è‡ªå·±çš„æ ‡è®°ï¼Œä»è€Œå®ç°å¯¹æ–‡æœ¬åºåˆ—çš„å¤šå°ºåº¦è§†å›¾ã€‚è¿™ç§æ–¹æ³•æ‰“ç ´äº†ä¼ ç»Ÿæ ‡è®°åŒ–çš„å›ºå®šç²’åº¦é™åˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´çµæ´»åœ°å¤„ç†æ•°æ®ã€‚é€šè¿‡è¯»å–åŸå§‹å­—èŠ‚å¹¶é€æ­¥èšåˆæˆè¯ï¼Œæ¨¡å‹åœ¨ä¸åŒæ·±åº¦é˜¶æ®µé¢„æµ‹æ›´è¿œçš„æœªæ¥ï¼Œå…³æ³¨æ›´å¹¿æ³›çš„è¯­ä¹‰æ¨¡å¼ã€‚æœ€ç»ˆï¼Œè¿™ç§æ¨¡å‹ä¸ä»…èƒ½å¤„ç†å­—ç¬¦çº§ä»»åŠ¡ï¼Œè¿˜èƒ½åœ¨ä½èµ„æºè¯­è¨€ä¸­ä¼ é€’çŸ¥è¯†ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.14731', 'title': 'Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs', 'url': 'https://huggingface.co/papers/2506.14731', 'abstract': 'Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.', 'score': 2, 'issue_id': 4350, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '7c1c5a66d6e8f898', 'authors': ['Ring Team', 'Bin Hu', 'Cai Chen', 'Deng Zhao', 'Ding Liu', 'Dingnan Jin', 'Feng Zhu', 'Hao Dai', 'Hongzhi Luan', 'Jia Guo', 'Jiaming Liu', 'Jiewei Wu', 'Jun Mei', 'Jun Zhou', 'Junbo Zhao', 'Junwu Xiong', 'Kaihong Zhang', 'Kuan Xu', 'Lei Liang', 'Liang Jiang', 'Liangcheng Fu', 'Longfei Zheng', 'Qiang Gao', 'Qing Cui', 'Quan Wan', 'Shaomian Zheng', 'Shuaicheng Li', 'Tongkai Yang', 'Wang Ren', 'Xiaodong Yan', 'Xiaopei Wan', 'Xiaoyun Feng', 'Xin Zhao', 'Xinxing Yang', 'Xinyu Kong', 'Xuemin Yang', 'Yang Li', 'Yingting Wu', 'Yongkang Liu', 'Zhankai Xu', 'Zhenduo Zhang', 'Zhenglei Zhou', 'Zhenyu Huang', 'Zhiqiang Zhang', 'Zihao Wang', 'Zujie Wen'], 'affiliations': ['Ring Team, Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14731.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#reasoning', '#open_source', '#rl', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ring-lite - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mixture-of-Experts (MoE), Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ C3PO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Efficient Reasoning with Fewer Parameters: Introducing Ring-lite', 'desc': 'Ring-lite is a large language model that uses a Mixture-of-Experts (MoE) architecture combined with reinforcement learning (RL) to enhance reasoning capabilities while minimizing parameter activation. It builds on the Ling-lite model, achieving state-of-the-art performance on various reasoning benchmarks with only a fraction of the parameters activated compared to similar models. The paper introduces a novel training method called Constrained Contextual Computation Policy Optimization (C3PO) to improve stability during RL training and optimize computational efficiency. Additionally, it highlights the importance of selecting distillation checkpoints based on entropy loss for better performance in RL training and proposes a two-stage training approach to manage domain conflicts in mixed datasets.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œæ¿€æ´»æ›´å°‘å‚æ•°çš„Ring-lite', 'desc': 'Ring-liteæ˜¯ä¸€ç§åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å®ç°é«˜æ•ˆä¸”ç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨Ling-liteçš„åŸºç¡€ä¸Šæ„å»ºï¼Œå…·æœ‰168äº¿ä¸ªå‚æ•°ï¼Œä½†ä»…æ¿€æ´»2.75äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ä¸å°è§„æ¨¡çš„æœ€å…ˆè¿›æ¨ç†æ¨¡å‹ç›¸åŒ¹é…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆè®­ç»ƒæµç¨‹ï¼Œå°†è’¸é¦ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œè§£å†³äº†MoEå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„ä¸€äº›æœªè®°å½•çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥å—é™ä¸Šä¸‹æ–‡è®¡ç®—ç­–ç•¥ä¼˜åŒ–ï¼ˆC3POï¼‰ï¼Œæˆ‘ä»¬æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¹¶é€šè¿‡ç®—æ³•ä¸ç³»ç»Ÿçš„å…±åŒè®¾è®¡æ–¹æ³•æ”¹å–„äº†è®¡ç®—ååé‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14702', 'title': 'Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers', 'url': 'https://huggingface.co/papers/2506.14702', 'abstract': 'A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.', 'score': 2, 'issue_id': 4350, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '2fbff0f4b562f92e', 'authors': ["Daniel D'souza", 'Julia Kreutzer', 'Adrien Morisot', 'Ahmet ÃœstÃ¼n', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.14702.jpg', 'data': {'categories': ['#long_context', '#optimization', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ¸ Ğ½ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ğ²Ğ¾ÑÑ‚Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Optimizing Model Performance for Rare Use Cases', 'desc': "This paper addresses the challenge of improving machine learning model performance on rare and underrepresented use cases, often referred to as the long tail. It proposes a method for fine-tuning models that enhances both controllability and performance by automatically inferring generation attributes during inference. The authors introduce a taxonomy of data characteristics to help guide the model's output, allowing for better adaptation to specific tasks without relying heavily on prompt engineering. Their approach demonstrates significant performance improvements, particularly in underrepresented domains, achieving notable gains in generation quality and task-specific evaluations."}, 'zh': {'title': 'ä¼˜åŒ–æ¨¡å‹ä»¥æå‡ç¨€æœ‰ç”¨ä¾‹çš„æ€§èƒ½ä¸å¯æ§æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨æ¨æ–­ç”Ÿæˆå±æ€§æ¥å¾®è°ƒæ¨¡å‹ï¼Œä»¥æé«˜åœ¨ç¨€æœ‰å’Œæœªå……åˆ†ä»£è¡¨çš„ç”¨ä¾‹ä¸Šçš„æ€§èƒ½å’Œå¯æ§æ€§ã€‚ç°ä»£æœºå™¨å­¦ä¹ é¢ä¸´çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨é•¿å°¾ç‰¹å¾ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸­è¾ƒå°‘å‡ºç°çš„ç‰¹å¾ã€‚æˆ‘ä»¬é‡æ–°å®¡è§†è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯ä¹‹é—´çš„å·®è·ï¼Œä»¥æ”¹å–„é•¿å°¾æ€§èƒ½ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›ä¸€ç»„å¯æ§çš„ç”Ÿæˆå±æ€§ã€‚é€šè¿‡å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å®ç°äº†åœ¨æ¨ç†æ—¶è‡ªåŠ¨æ¨æ–­è¿™äº›æ ‡è®°ï¼Œä»è€Œåœ¨æœªå……åˆ†ä»£è¡¨çš„é¢†åŸŸä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13599', 'title': 'CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation', 'url': 'https://huggingface.co/papers/2506.13599', 'abstract': 'CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose CityGPT-Powered Agentic framework for Mobility Simulation (CAMS), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. CAMS comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that CAMS achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, CAMS generates more realistic and plausible trajectories. In general, CAMS establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.', 'score': 2, 'issue_id': 4348, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '0b0a6282d1310e1b', 'authors': ['Yuwei Du', 'Jie Feng', 'Jian Yuan', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13599.jpg', 'data': {'categories': ['#agents', '#synthetic', '#reasoning', '#multimodal'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'CAMS (CityGPT-Powered Agentic framework for Mobility Simulation) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. CAMS Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ: MobExtractor Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, GeoGenerator Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¸ TrajEnhancer Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CAMS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Urban Mobility Simulation with CAMS', 'desc': 'CAMS introduces a novel framework that combines agent-based modeling with large language models to enhance the simulation of human mobility in urban environments. It addresses the limitations of traditional methods by integrating individual and collective mobility patterns, allowing for more realistic trajectory generation. The framework consists of three main components: MobExtractor for mobility pattern extraction, GeoGenerator for urban geospatial knowledge generation, and TrajEnhancer for trajectory refinement. Experiments demonstrate that CAMS outperforms existing approaches by generating plausible mobility trajectories without needing external geospatial data.'}, 'zh': {'title': 'åŸå¸‚ç§»åŠ¨æ¨¡æ‹Ÿçš„æ–°èŒƒå¼', 'desc': 'CAMSæ˜¯ä¸€ä¸ªç»“åˆäº†ä»£ç†æ¡†æ¶å’ŒåŸå¸‚çŸ¥è¯†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ›´çœŸå®åœ°æ¨¡æ‹Ÿäººç±»çš„ç§»åŠ¨è¡Œä¸ºã€‚å®ƒé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼šMobExtractoræå–å’Œåˆæˆç”¨æˆ·çš„ç§»åŠ¨æ¨¡å¼ï¼ŒGeoGeneratorç”Ÿæˆè€ƒè™‘é›†ä½“çŸ¥è¯†çš„åŸå¸‚åœ°ç†ä¿¡æ¯ï¼ŒTrajEnhanceræ ¹æ®ç§»åŠ¨æ¨¡å¼ç”Ÿæˆç¬¦åˆçœŸå®åå¥½çš„è½¨è¿¹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCAMSåœ¨ä¸ä¾èµ–å¤–éƒ¨åœ°ç†ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å»ºæ¨¡ä¸ªä½“å’Œé›†ä½“çš„ç§»åŠ¨æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAMSç”Ÿæˆçš„è½¨è¿¹æ›´åŠ çœŸå®å¯ä¿¡ï¼Œå¼€åˆ›äº†äººç±»ç§»åŠ¨æ¨¡æ‹Ÿçš„æ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05426', 'title': 'Mixture-of-Experts Meets In-Context Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.05426', 'abstract': 'T2MIR, a framework using token-wise and task-wise MoE in transformer-based decision models, enhances in-context reinforcement learning by addressing multi-modality and task diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR.', 'score': 2, 'issue_id': 4356, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '4e79eb4ebca225c7', 'authors': ['Wenhao Wu', 'Fuhong Liu', 'Haoru Li', 'Zican Hu', 'Daoyi Dong', 'Chunlin Chen', 'Zhi Wang'], 'affiliations': ['Australian Artificial Intelligence Institute, University of Technology Sydney', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05426.jpg', 'data': {'categories': ['#optimization', '#rl', '#games', '#multimodal', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'T2MIR: Ğ¡Ğ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ', 'desc': 'T2MIR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICRL), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE) Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ğ¾ĞºĞµĞ½-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ MoE. T2MIR Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing In-Context Learning with T2MIR: A Mixture-of-Experts Approach', 'desc': 'The paper introduces T2MIR, a novel framework that enhances in-context reinforcement learning (ICRL) by integrating mixture-of-experts (MoE) into transformer-based decision models. It addresses the challenges of multi-modality in state-action-reward data and the diversity of decision tasks by implementing token-wise and task-wise MoE layers. The token-wise MoE captures different meanings of input tokens, while the task-wise MoE directs tasks to specialized experts, reducing conflicts during training. Experimental results demonstrate that T2MIR improves the learning capacity of ICRL and outperforms existing methods, showcasing its potential for advancements in both language and vision tasks.'}, 'zh': {'title': 'T2MIRï¼šæå‡ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ çš„ä¸“å®¶æ··åˆæ¡†æ¶', 'desc': 'T2MIRæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºTransformerçš„å†³ç­–æ¨¡å‹ä¸­çš„é€tokenå’Œé€ä»»åŠ¡çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ–¹æ³•ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰ã€‚è¯¥æ¡†æ¶è§£å†³äº†çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±æ•°æ®çš„å¤šæ¨¡æ€æ€§å’Œå†³ç­–ä»»åŠ¡çš„å¤šæ ·æ€§é—®é¢˜ã€‚T2MIRé€šè¿‡å¼•å…¥ä¸¤ä¸ªå¹¶è¡Œå±‚ï¼Œåˆ†åˆ«æ˜¯é€tokençš„MoEå’Œé€ä»»åŠ¡çš„MoEï¼Œæ¥æ•æ‰è¾“å…¥tokençš„ä¸åŒè¯­ä¹‰ï¼Œå¹¶å°†å¤šæ ·åŒ–çš„ä»»åŠ¡åˆ†é…ç»™ä¸“é—¨çš„ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT2MIRæ˜¾è‘—æé«˜äº†ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13901', 'title': 'Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations', 'url': 'https://huggingface.co/papers/2506.13901', 'abstract': "A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.   To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.   Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.", 'score': 1, 'issue_id': 4351, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '5492fc2feb0ae2f3', 'authors': ['Abhilekh Borah', 'Chhavi Sharma', 'Danush Khanna', 'Utkarsh Bhatt', 'Gurpreet Singh', 'Hasnat Md Abdullah', 'Raghav Kaushik Ravi', 'Vinija Jain', 'Jyoti Patel', 'Shubham Singh', 'Vasu Sharma', 'Arpita Vats', 'Rahul Raja', 'Aman Chadha', 'Amitava Das'], 'affiliations': ['Amazon AI', 'BITS Goa, India', 'Evalueserve', 'IIIT Guwahati, India', 'IIT Kharagpur, India', 'LinkedIn', 'Manipal University Jaipur, India', 'Meta AI', 'New York University, USA', 'Texas A&M University, USA', 'Vellore Institute of Technology, Chennai, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.13901.jpg', 'data': {'categories': ['#security', '#rlhf', '#alignment', '#benchmark', '#dataset', '#open_source'], 'emoji': 'ğŸ¯', 'ru': {'title': 'AQI: Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (alignment) Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ˜Ğ½Ğ´ĞµĞºÑ ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (AQI). AQI Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ñ„Ğ°Ğ»ÑŒÑˆĞ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. AQI Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Ensuring True Alignment in Language Models with AQI', 'desc': "The paper introduces a new evaluation metric called the Alignment Quality Index (AQI) to assess the alignment of large language models (LLMs). AQI analyzes latent space activations to measure clustering quality, helping to identify misalignments and instances of alignment faking that traditional behavioral proxies may overlook. By utilizing established clustering metrics like the Davies-Bouldin Score and Dunn Index, AQI provides a more reliable assessment of model safety and alignment in high-stakes applications. The authors also present the LITMUS dataset to support rigorous evaluation, demonstrating AQI's effectiveness in revealing vulnerabilities that other metrics fail to detect."}, 'zh': {'title': 'å¯¹é½è´¨é‡æŒ‡æ•°ï¼šç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨ä¸å¯é ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç§°ä¸ºå¯¹é½è´¨é‡æŒ‡æ•°ï¼ˆAQIï¼‰ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½æƒ…å†µã€‚AQIé€šè¿‡åˆ†ææ½œåœ¨ç©ºé—´ä¸­çš„æ¿€æ´»åˆ†ç¦»ï¼Œæ•æ‰èšç±»è´¨é‡ï¼Œä»¥æ£€æµ‹æ¨¡å‹çš„é”™è¯¯å¯¹é½å’Œä¼ªå¯¹é½ç°è±¡ã€‚ä¸ç°æœ‰çš„è¡Œä¸ºä»£ç†ç›¸æ¯”ï¼ŒAQIèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«æ¨¡å‹çš„å®‰å…¨æ€§å’Œæ½œåœ¨é£é™©ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†LITMUSæ•°æ®é›†ï¼Œä»¥æ”¯æŒåœ¨å¤æ‚æ¡ä»¶ä¸‹çš„ç¨³å¥è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†AQIä¸å¤–éƒ¨è¯„å®¡è€…çš„ç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13387', 'title': 'TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast', 'url': 'https://huggingface.co/papers/2506.13387', 'abstract': "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2M's great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: https://github.com/BeileiCui/TR2M)", 'score': 1, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '6d0fc497ae4dcfd0', 'authors': ['Beilei Cui', 'Yiming Huang', 'Long Bai', 'Hongliang Ren'], 'affiliations': ['The Chinese University of Hong Kong, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.13387.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'TR2M - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. TR2M Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ½Ğ° Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ² Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Transforming Relative Depth to Metric Depth with TR2M', 'desc': 'The paper introduces TR2M, a framework that effectively converts relative depth information into metric depth using multimodal inputs, specifically images and text. It addresses the limitations of existing monocular depth estimation methods by combining the strengths of metric and relative depth estimation. TR2M employs cross-modality attention to enhance feature fusion and utilizes contrastive learning to improve scale alignment. The framework demonstrates strong performance across various datasets, including impressive zero-shot capabilities on unseen data, showcasing its versatility and effectiveness in depth estimation tasks.'}, 'zh': {'title': 'TR2Mï¼šç›¸å¯¹æ·±åº¦åˆ°åº¦é‡æ·±åº¦çš„æ™ºèƒ½è½¬æ¢', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTR2Mçš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç›¸å¯¹æ·±åº¦è½¬æ¢ä¸ºåº¦é‡æ·±åº¦ï¼Œåˆ©ç”¨å¤šæ¨¡æ€è¾“å…¥æå‡åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚å½“å‰çš„å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ä¸»è¦åˆ†ä¸ºåº¦é‡æ·±åº¦ä¼°è®¡å’Œç›¸å¯¹æ·±åº¦ä¼°è®¡ï¼Œå‰è€…åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°è‰¯å¥½ï¼Œä½†å±€é™æ€§è¾ƒå¤§ï¼Œè€Œåè€…åœ¨ä¸åŒé¢†åŸŸå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å­˜åœ¨å°ºåº¦ä¸ç¡®å®šæ€§çš„é—®é¢˜ã€‚TR2Mé€šè¿‡èåˆæ–‡æœ¬æè¿°å’Œå›¾åƒè¾“å…¥ï¼Œåˆ©ç”¨äº¤å‰æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—å’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæ„å»ºäº†ä¸¤ä¸ªé‡æ ‡å®šå›¾ä»¥åœ¨åƒç´ çº§åˆ«ä¸Šè¿›è¡Œæ·±åº¦è½¬æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTR2Måœ¨å·²çŸ¥æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨äº”ä¸ªæœªçŸ¥æ•°æ®é›†ä¸Šå±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºåœ¨åƒç´ çº§åˆ«ä¸Šåˆ©ç”¨è¯­è¨€è¾…åŠ©è¿›è¡Œæ·±åº¦è½¬æ¢çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12880', 'title': 'Universal Jailbreak Suffixes Are Strong Attention Hijackers', 'url': 'https://huggingface.co/papers/2506.12880', 'abstract': "Suffix-based jailbreaks exploit adversarial suffixes to hijack large language models, with effectiveness linked to suffix universality; the method can be enhanced and mitigated with minimal computational or utility cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We study suffix-based jailbreaksx2013a powerful family of attacks against large language models (LLMs) that optimize adversarial suffixes to circumvent safety alignment. Focusing on the widely used foundational GCG attack (Zou et al., 2023), we observe that suffixes vary in efficacy: some markedly more universalx2013generalizing to many unseen harmful instructionsx2013than others. We first show that GCG's effectiveness is driven by a shallow, critical mechanism, built on the information flow from the adversarial suffix to the final chat template tokens before generation. Quantifying the dominance of this mechanism during generation, we find GCG irregularly and aggressively hijacks the contextualization process. Crucially, we tie hijacking to the universality phenomenon, with more universal suffixes being stronger hijackers. Subsequently, we show that these insights have practical implications: GCG universality can be efficiently enhanced (up to times5 in some cases) at no additional computational cost, and can also be surgically mitigated, at least halving attack success with minimal utility loss. We release our code and data at http://github.com/matanbt/interp-jailbreak.", 'score': 1, 'issue_id': 4357, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': 'b2ad9af4ff256800', 'authors': ['Matan Ben-Tov', 'Mor Geva', 'Mahmood Sharif'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12880.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#security', '#hallucinations', '#multimodal', '#open_source', '#alignment', '#data'], 'emoji': 'ğŸ”“', 'ru': {'title': 'Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑĞ¾Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ€Ğ°Ğ¶Ğ´ĞµĞ±Ğ½Ñ‹Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¼ĞµÑ€ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°ĞºĞ¸ GCG Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ„Ñ„Ğ¸ĞºÑĞ¾Ğ² Ğ¸ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ‘Ğ¾Ğ»ĞµĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑÑ‹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ ÑĞ¸Ğ»ÑŒĞ½ĞµĞµ Ğ² Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ñ… ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking and Mitigating Suffix-Based Attacks on Language Models', 'desc': "This paper investigates suffix-based jailbreaks, which are attacks on large language models (LLMs) that use specific suffixes to bypass safety measures. The authors focus on the GCG attack, revealing that some suffixes are more effective than others due to their universality, meaning they can apply to a wider range of harmful instructions. They identify a key mechanism in how these suffixes influence the model's output, showing that more universal suffixes are better at hijacking the model's contextualization process. The study also presents methods to enhance the effectiveness of these attacks without extra computational costs and suggests ways to mitigate them while maintaining model utility."}, 'zh': {'title': 'åç¼€æ”»å‡»ï¼šç»•è¿‡å®‰å…¨æœºåˆ¶çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†ä¸€ç§åŸºäºåç¼€çš„æ”»å‡»æ–¹æ³•ï¼Œæ—¨åœ¨ç»•è¿‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æœºåˆ¶ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸åŒçš„åç¼€åœ¨æ”»å‡»æ•ˆæœä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒæŸäº›åç¼€å…·æœ‰æ›´å¼ºçš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿé€‚ç”¨äºæ›´å¤šæœªè§è¿‡çš„æœ‰å®³æŒ‡ä»¤ã€‚é€šè¿‡åˆ†æä¿¡æ¯æµåŠ¨ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ”»å‡»çš„å…³é”®æœºåˆ¶ï¼Œå¹¶æŒ‡å‡ºæ›´é€šç”¨çš„åç¼€èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åŠ«æŒä¸Šä¸‹æ–‡å¤„ç†è¿‡ç¨‹ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†å¢å¼ºå’Œç¼“è§£è¿™ç§æ”»å‡»çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜é˜²å¾¡æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14629', 'title': 'VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning', 'url': 'https://huggingface.co/papers/2506.14629', 'abstract': 'VisText-Mosquito is a multimodal dataset combining visual and textual data for automated mosquito breeding site detection, segmentation, and reasoning, utilizing YOLOv9s, YOLOv11n-Seg, and a fine-tuned BLIP model.  \t\t\t\t\tAI-generated summary \t\t\t\t Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito', 'score': 0, 'issue_id': 4358, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '3308b767733e40b9', 'authors': ['Md. Adnanul Islam', 'Md. Faiyaz Abdullah Sayeedi', 'Md. Asaduzzaman Shuvo', 'Muhammad Ziaur Rahman', 'Shahanur Rahman Bappy', 'Raiyan Rahman', 'Swakkhar Shatabda'], 'affiliations': ['BRAC University, Bangladesh', 'United International University, Bangladesh', 'University of Portsmouth, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2506.14629.jpg', 'data': {'categories': ['#games', '#dataset', '#open_source', '#multimodal', '#healthcare', '#reasoning'], 'emoji': 'ğŸ¦Ÿ', 'ru': {'title': 'Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ: Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹ ĞºĞ¾Ğ¼Ğ°Ñ€Ğ¾Ğ²', 'desc': 'VisText-Mosquito - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ĞµÑÑ‚ Ñ€Ğ°Ğ·Ğ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ñ€Ğ¾Ğ². Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ YOLOv9s Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², YOLOv11n-Seg Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BLIP Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1828 Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², 142 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ñ€Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Harnessing AI for Mosquito Control: Detect, Segment, Reason!', 'desc': 'The paper introduces VisText-Mosquito, a unique dataset that combines images and text to help identify and analyze mosquito breeding sites. It includes 1,828 annotated images for detecting objects and 142 images specifically for segmenting water surfaces, along with reasoning texts for each image. The study employs advanced models like YOLOv9s and YOLOv11n-Seg for detection and segmentation tasks, achieving high precision scores. Additionally, a fine-tuned BLIP model is used for generating natural language reasoning, demonstrating the effectiveness of multimodal approaches in combating mosquito-borne diseases.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ•°æ®åŠ©åŠ›èšŠå­æ»‹ç”Ÿåœ°è‡ªåŠ¨æ£€æµ‹', 'desc': 'VisText-Mosquitoæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ•°æ®é›†ï¼Œç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œç”¨äºè‡ªåŠ¨åŒ–æ£€æµ‹å’Œåˆ†æèšŠå­æ»‹ç”Ÿåœ°ã€‚è¯¥æ•°æ®é›†åŒ…å«1828å¼ æ ‡æ³¨å›¾åƒç”¨äºç›®æ ‡æ£€æµ‹ï¼Œ142å¼ å›¾åƒç”¨äºæ°´é¢åˆ†å‰²ï¼Œä»¥åŠä¸æ¯å¼ å›¾åƒç›¸å…³çš„è‡ªç„¶è¯­è¨€æ¨ç†æ–‡æœ¬ã€‚ä½¿ç”¨YOLOv9sæ¨¡å‹è¿›è¡Œç›®æ ‡æ£€æµ‹æ—¶ï¼Œè¾¾åˆ°äº†æœ€é«˜çš„ç²¾åº¦0.92926ï¼Œè€ŒYOLOv11n-Segåœ¨åˆ†å‰²ä»»åŠ¡ä¸­è¾¾åˆ°äº†0.91587çš„ç²¾åº¦ã€‚é€šè¿‡å¾®è°ƒçš„BLIPæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨æ¨ç†ç”Ÿæˆæ–¹é¢å–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œå¼ºè°ƒäº†â€œé¢„é˜²èƒœäºæ²»ç–—â€çš„ä¸»é¢˜ï¼Œå±•ç¤ºäº†åŸºäºAIçš„æ£€æµ‹å¦‚ä½•ä¸»åŠ¨åº”å¯¹èšŠåª’ç–¾ç—…é£é™©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13922', 'title': 'DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance', 'url': 'https://huggingface.co/papers/2506.13922', 'abstract': 'DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io', 'score': 0, 'issue_id': 4359, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'bc27538b9a430f57', 'authors': ['Maximilian Du', 'Shuran Song'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13922.jpg', 'data': {'categories': ['#agents', '#diffusion', '#optimization', '#training', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'DynaGuide: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸', 'desc': 'DynaGuide - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğº Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ»ÑĞ¼ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ÑĞ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ…. DynaGuide Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑÑ€ĞµĞ´Ğ½ÑÑ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ 70% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ CALVIN Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ² 5,4 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Steering Policies with DynaGuide: Adapting to Multiple Goals Robustly!', 'desc': 'DynaGuide is a novel steering method that enhances diffusion policies by utilizing an external dynamics model. This approach allows the policies to adapt to various objectives while ensuring robustness, particularly when dealing with low-quality objectives. Unlike traditional goal-conditioning methods, DynaGuide separates the dynamics model from the base policy, enabling it to steer towards multiple goals and improve underrepresented behaviors. The effectiveness of DynaGuide is demonstrated through experiments, achieving a 70% steering success rate and significantly outperforming existing methods.'}, 'zh': {'title': 'DynaGuideï¼šå¤šç›®æ ‡å¼•å¯¼çš„æ™ºèƒ½ç­–ç•¥', 'desc': 'DynaGuideæ˜¯ä¸€ç§ä½¿ç”¨å¤–éƒ¨åŠ¨æ€æ¨¡å‹çš„å¼•å¯¼æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæ‰©æ•£ç­–ç•¥çš„é€‚åº”æ€§ã€‚å®ƒå…è®¸ç­–ç•¥é€‚åº”å¤šä¸ªç›®æ ‡ï¼Œå¹¶åœ¨ä½è´¨é‡ç›®æ ‡ä¸‹ä¿æŒé²æ£’æ€§ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›®æ ‡æ¡ä»¶æ–¹æ³•ã€‚é€šè¿‡å°†åŠ¨æ€æ¨¡å‹ä¸åŸºç¡€ç­–ç•¥åˆ†ç¦»ï¼ŒDynaGuideèƒ½å¤Ÿå¼•å¯¼ç­–ç•¥æœå‘å¤šä¸ªç›®æ ‡ï¼Œå¹¶å¢å¼ºåŸºç¡€ç­–ç•¥çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDynaGuideåœ¨å¤šç§ä»»åŠ¡ä¸­å–å¾—äº†70%çš„å¼•å¯¼æˆåŠŸç‡ï¼Œå°¤å…¶åœ¨ä½è´¨é‡ç›®æ ‡ä¸‹æ¯”ç›®æ ‡æ¡ä»¶æ–¹æ³•æé«˜äº†5.4å€çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12015', 'title': 'EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction', 'url': 'https://huggingface.co/papers/2506.12015', 'abstract': 'EMLoC, an memory-efficient fine-tuning framework using activation-aware SVD and LoRA, allows model adaptation within inference memory constraints for diverse applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.', 'score': 0, 'issue_id': 4356, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': 'd559dcf057099acf', 'authors': ['Hsi-Che Lin', 'Yu-Chu Yu', 'Kai-Po Chang', 'Yu-Chiang Frank Wang'], 'affiliations': ['NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12015.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#open_source', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞŸĞš', 'desc': 'EMLoC - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ SVD Ğ¸ LoRA Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ñ‚ĞµÑ… Ğ¶Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. EMLoC Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, Ğ´ĞµĞ»Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Efficient Fine-Tuning for Everyone with EMLoC!', 'desc': 'EMLoC is a memory-efficient framework designed for fine-tuning large foundation models while adhering to inference memory limits. It utilizes activation-aware singular value decomposition (SVD) to create a lightweight emulator from a small calibration dataset, allowing for effective model adaptation. Fine-tuning is achieved through Low-Rank Adaptation (LoRA), which is then corrected with a novel compensation algorithm to ensure alignment with the original model. This approach enables users to fine-tune large models on standard consumer hardware, making advanced machine learning accessible and practical for diverse applications.'}, 'zh': {'title': 'EMLoCï¼šé«˜æ•ˆå†…å­˜å¾®è°ƒçš„æ–°é€‰æ‹©', 'desc': 'EMLoCæ˜¯ä¸€ç§å†…å­˜é«˜æ•ˆçš„å¾®è°ƒæ¡†æ¶ï¼Œåˆ©ç”¨æ¿€æ´»æ„ŸçŸ¥çš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å’ŒLoRAæŠ€æœ¯ï¼Œä½¿å¾—åœ¨æ¨ç†å†…å­˜é™åˆ¶ä¸‹è¿›è¡Œæ¨¡å‹é€‚åº”æˆä¸ºå¯èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å°å‹ä¸‹æ¸¸æ ¡å‡†é›†ä¸Šæ„å»ºä»»åŠ¡ç‰¹å®šçš„è½»é‡çº§æ¨¡æ‹Ÿå™¨ï¼Œæ¥å®ç°å¾®è°ƒã€‚ä¸ºäº†çº æ­£åŸå§‹æ¨¡å‹ä¸å‹ç¼©æ¨¡æ‹Ÿå™¨ä¹‹é—´çš„é”™ä½ï¼ŒEMLoCæå‡ºäº†ä¸€ç§æ–°é¢–çš„è¡¥å¿ç®—æ³•ï¼Œä½¿å¾—å¾®è°ƒåçš„LoRAæ¨¡å—å¯ä»¥åˆå¹¶å›åŸå§‹æ¨¡å‹ä¸­è¿›è¡Œæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEMLoCåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡æ€ä¸Šä¼˜äºå…¶ä»–åŸºçº¿ï¼Œä¸”æ— éœ€é‡åŒ–å³å¯åœ¨å•ä¸ª24GBçš„æ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒ38Bæ¨¡å‹ï¼Œæå¤§åœ°æé«˜äº†æ¨¡å‹é€‚åº”çš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03939', 'title': 'Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.03939', 'abstract': 'Graph Counselor enhances Large Language Models by using multi-agent collaboration and adaptive reasoning to integrate knowledge effectively, improving factual accuracy and generation quality in specialized domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.', 'score': 0, 'issue_id': 4362, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '44afebcf0fc38495', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#graphs', '#rag'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Graph Counselor - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Graph Counselor Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ°Ñ….'}, 'en': {'title': 'Empowering LLMs with Adaptive Multi-Agent Collaboration', 'desc': "Graph Counselor is a novel approach that enhances Large Language Models (LLMs) by employing multi-agent collaboration and adaptive reasoning techniques. It addresses the limitations of existing methods in knowledge integration by utilizing an Adaptive Graph Information Extraction Module (AGIEM) that allows agents to work together to model complex graph structures. This method dynamically adjusts information extraction strategies, improving the model's ability to handle multi-level dependencies and reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module ensures higher accuracy and semantic consistency in reasoning results, leading to superior performance in graph reasoning tasks."}, 'zh': {'title': 'å›¾é¡¾é—®ï¼šæå‡è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†æ•´åˆèƒ½åŠ›', 'desc': 'Graph Counselor æ˜¯ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“åä½œçš„å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†æ•´åˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”å›¾ä¿¡æ¯æå–æ¨¡å—ï¼ˆAGIEMï¼‰ï¼Œä½¿è§„åˆ’ã€æ€è€ƒå’Œæ‰§è¡Œæ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œä»è€Œç²¾ç¡®å»ºæ¨¡å¤æ‚çš„å›¾ç»“æ„å¹¶åŠ¨æ€è°ƒæ•´ä¿¡æ¯æå–ç­–ç•¥ã€‚å®ƒè¿˜å¼•å…¥äº†å¤šè§†è§’è‡ªæˆ‘åæ€æ¨¡å—ï¼ˆSRï¼‰ï¼Œé€šè¿‡è‡ªæˆ‘åæ€å’Œé€†å‘æ¨ç†æœºåˆ¶æé«˜æ¨ç†ç»“æœçš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraph Counselor åœ¨å¤šä¸ªå›¾æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´é«˜çš„æ¨ç†å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.01939', 'title': 'Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.01939', 'abstract': "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.", 'score': 93, 'issue_id': 4090, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'caf288ab8a8d11b2', 'authors': ['Shenzhi Wang', 'Le Yu', 'Chang Gao', 'Chujie Zheng', 'Shixuan Liu', 'Rui Lu', 'Kai Dang', 'Xionghui Chen', 'Jianxin Yang', 'Zhenru Zhang', 'Yuqiong Liu', 'An Yang', 'Andrew Zhao', 'Yang Yue', 'Shiji Song', 'Bowen Yu', 'Gao Huang', 'Junyang Lin'], 'affiliations': ['LeapLab, Tsinghua University', 'Qwen Team, Alibaba Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.01939.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#rl', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ñ‹ÑĞ¾ĞºĞ¾ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. RLVR Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 20% Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unlocking Reasoning Power with High-Entropy Tokens in RLVR', 'desc': "This paper investigates the role of token entropy patterns in Reinforcement Learning with Verifiable Rewards (RLVR) to enhance the reasoning abilities of Large Language Models (LLMs). It finds that high-entropy tokens, which are rare, significantly influence the model's reasoning pathways and performance. The study shows that by focusing on these high-entropy tokens during training, the model can achieve better results than traditional methods, even when using a smaller subset of tokens. Overall, the research emphasizes the importance of understanding and optimizing high-entropy tokens to improve RLVR outcomes."}, 'zh': {'title': 'é«˜ç†µä»¤ç‰Œï¼šæå‡RLVRæ¨ç†èƒ½åŠ›çš„å…³é”®', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æå‡ä¸­çš„ä½œç”¨ï¼Œé‡ç‚¹åˆ†æäº†ä»¤ç‰Œç†µæ¨¡å¼å¯¹æ¨ç†æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œåªæœ‰å°‘é‡é«˜ç†µä»¤ç‰Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿå¼•å¯¼æ¨¡å‹èµ°å‘å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ã€‚é€šè¿‡å¯¹RLVRè®­ç»ƒä¸­ç†µæ¨¡å¼çš„æ¼”å˜è¿›è¡Œç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°RLVRä¸»è¦éµå¾ªåŸºç¡€æ¨¡å‹çš„ç†µæ¨¡å¼ï¼Œä¸»è¦è°ƒæ•´é«˜ç†µä»¤ç‰Œçš„ç†µå€¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–é«˜ç†µä»¤ç‰Œæ˜¯æå‡RLVRæ•ˆæœçš„å…³é”®ï¼Œåˆ©ç”¨è¿™äº›ä»¤ç‰Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24760', 'title': 'REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2505.24760', 'abstract': 'We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.', 'score': 44, 'issue_id': 4088, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'c7587646c2140ddd', 'authors': ['Zafir Stojanovski', 'Oliver Stanley', 'Joe Sharratt', 'Richard Jones', 'Abdulhakeem Adefioye', 'Jean Kaddour', 'Andreas KÃ¶pf'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24760.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#games', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ˜Ğ˜ Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Reasoning Gym (RG) - ÑÑ‚Ğ¾ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ğ°, Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞ°, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¸ Ğ¸Ğ³Ñ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ RG - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ RG ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlock Infinite Reasoning with Reasoning Gym!', 'desc': 'The paper presents Reasoning Gym (RG), a new library designed for reinforcement learning that focuses on reasoning tasks with verifiable rewards. RG includes over 100 data generators and verifiers across diverse domains such as algebra, logic, and games, enabling a wide range of reasoning challenges. A significant feature of RG is its ability to create an almost limitless amount of training data with customizable complexity, which is a departure from traditional fixed reasoning datasets. The authors show that RG effectively supports the evaluation and training of reasoning models in reinforcement learning settings.'}, 'zh': {'title': 'æ¨ç†è®­ç»ƒåœºï¼šæ— é™ç”Ÿæˆï¼ŒæŒç»­è¯„ä¼°', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†æ¨ç†è®­ç»ƒåœºï¼ˆReasoning Gymï¼ŒRGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†ç¯å¢ƒåº“ï¼Œå…·æœ‰å¯éªŒè¯çš„å¥–åŠ±ã€‚å®ƒæä¾›äº†è¶…è¿‡100ä¸ªæ•°æ®ç”Ÿæˆå™¨å’ŒéªŒè¯å™¨ï¼Œæ¶µç›–ä»£æ•°ã€ç®—æœ¯ã€è®¡ç®—ã€è®¤çŸ¥ã€å‡ ä½•ã€å›¾è®ºã€é€»è¾‘å’Œå„ç§å¸¸è§æ¸¸æˆç­‰å¤šä¸ªé¢†åŸŸã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºèƒ½å¤Ÿç”Ÿæˆå‡ ä¹æ— é™çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥è°ƒæ•´å¤æ‚æ€§ï¼Œè¿™ä¸å¤§å¤šæ•°å›ºå®šçš„æ¨ç†æ•°æ®é›†ä¸åŒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRGåœ¨è¯„ä¼°å’Œå¼ºåŒ–å­¦ä¹ æ¨ç†æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01844', 'title': 'SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics', 'url': 'https://huggingface.co/papers/2506.01844', 'abstract': 'SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.', 'score': 42, 'issue_id': 4094, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '64cdbe1cd5ffbfc4', 'authors': ['Mustafa Shukor', 'Dana Aubakirova', 'Francesco Capuano', 'Pepijn Kooijmans', 'Steven Palma', 'Adil Zouitine', 'Michel Aractingi', 'Caroline Pascal', 'Martino Russi', 'Andres Marafioti', 'Simon Alibert', 'Matthieu Cord', 'Thomas Wolf', 'Remi Cadene'], 'affiliations': ['Hugging Face', 'Sorbonne University', 'valeo.ai', 'Ã‰cole Normale SupÃ©rieure Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2506.01844.jpg', 'data': {'categories': ['#optimization', '#open_source', '#multimodal', '#small_models', '#training', '#benchmark', '#dataset', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'SmolVLA - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞĞ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ğ° Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. SmolVLA Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° CPU, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞ²Ğ¾Ğ¹ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, SmolVLA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ.'}, 'en': {'title': 'SmolVLA: Efficient Vision-Language-Action for Everyone', 'desc': 'SmolVLA is a compact vision-language-action model that efficiently integrates visual and linguistic understanding for robotics. It significantly reduces the computational costs associated with training and inference, making it suitable for consumer-grade hardware. By leveraging community-collected data, SmolVLA maintains competitive performance while being much smaller than traditional models. The introduction of an asynchronous inference stack enhances responsiveness, allowing for faster action execution without sacrificing accuracy.'}, 'zh': {'title': 'SmolVLAï¼šå°å·§é«˜æ•ˆçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹', 'desc': 'SmolVLAæ˜¯ä¸€ç§ç´§å‡‘é«˜æ•ˆçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶å¯åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šéƒ¨ç½²ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç¤¾åŒºæ”¶é›†çš„æ•°æ®ï¼Œé¿å…äº†ä¼ ç»Ÿæ¨¡å‹å¯¹å¤§å‹æ•°æ®é›†çš„ä¾èµ–ï¼Œä»è€Œé™ä½äº†è®­ç»ƒå’Œæ¨ç†çš„æˆæœ¬ã€‚SmolVLAè®¾è®¡ä¸ºå¯ä»¥åœ¨å•ä¸ªGPUä¸Šè®­ç»ƒï¼Œå¹¶åœ¨æ¶ˆè´¹çº§GPUæˆ–CPUä¸Šè¿è¡Œï¼Œæå‡äº†å“åº”é€Ÿåº¦ã€‚å°½ç®¡ä½“ç§¯å°ï¼ŒSmolVLAçš„æ€§èƒ½ä¸ä½“ç§¯åå€çš„æ¨¡å‹ç›¸å½“ï¼Œé€‚ç”¨äºå¤šç§æœºå™¨äººåŸºå‡†æµ‹è¯•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01049', 'title': 'Taming LLMs by Scaling Learning Rates with Gradient Grouping', 'url': 'https://huggingface.co/papers/2506.01049', 'abstract': 'SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.', 'score': 31, 'issue_id': 4087, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '350401d748400bad', 'authors': ['Siyuan Li', 'Juanxi Tian', 'Zedong Wang', 'Xin Jin', 'Zicheng Liu', 'Wentao Zhang', 'Dan Xu'], 'affiliations': ['Peking University', 'The Hong Kong University of Science and Technology', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01049.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'SGG: Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SGG (Scaling with Gradient Grouping). SGG Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SGG Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'SGG: Optimizing Learning Rates for Better LLM Training', 'desc': 'This paper presents Scaling with Gradient Grouping (SGG), an innovative optimizer wrapper designed to enhance adaptive learning rates for large language models (LLMs). SGG addresses the challenges of training LLMs by dynamically grouping gradient statistics and applying specific scaling for each group, which improves convergence and stability. By imposing collective constraints on groups while allowing precise adjustments for individual parameters, SGG optimizes the learning process more effectively than traditional methods. Experimental results demonstrate that SGG integrates well with existing optimizers, leading to faster convergence and improved performance across various model sizes and training conditions.'}, 'zh': {'title': 'SGGï¼šæå‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–å™¨', 'desc': 'SGGæ˜¯ä¸€ç§ä¼˜åŒ–å™¨åŒ…è£…å™¨ï¼Œé€šè¿‡å¯¹æ¢¯åº¦è¿›è¡Œåˆ†ç»„å’Œåº”ç”¨ç‰¹å®šäºé›†ç¾¤çš„ç¼©æ”¾ï¼Œå¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”å­¦ä¹ ç‡ã€‚å®ƒè§£å†³äº†å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚SGGé€šè¿‡åŠ¨æ€åˆ†ç»„å’Œé›†ç¾¤ç‰¹å®šçš„ç¼©æ”¾æ¥æ”¹å–„å­¦ä¹ ç‡ä¼°è®¡ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„å‚æ•°è°ƒæ•´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSGGä¸ç°æœ‰ä¼˜åŒ–å™¨æ— ç¼é›†æˆï¼Œå¹¶åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šæä¾›äº†ä¸€è‡´çš„æ€§èƒ½æå‡å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00539', 'title': 'ARIA: Training Language Agents with Intention-Driven Reward Aggregation', 'url': 'https://huggingface.co/papers/2506.00539', 'abstract': 'ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an exponentially large action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering better policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces policy gradient variance, but also delivers substantial performance gains of an average of 9.95% across four downstream tasks, consistently outperforming offline and online RL baselines.', 'score': 25, 'issue_id': 4090, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '49b915ea5a1db300', 'authors': ['Ruihan Yang', 'Yikai Zhang', 'Aili Chen', 'Xintao Wang', 'Siyu Yuan', 'Jiangjie Chen', 'Deqing Yang', 'Yanghua Xiao'], 'affiliations': ['Bytedance Seed', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00539.jpg', 'data': {'categories': ['#games', '#reasoning', '#agents', '#rl', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ARIA: ĞĞ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ARIA - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸Ğ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ÑÑ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ARIA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 9,95% Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing Language Agents with Intention-Based Reward Aggregation', 'desc': 'ARIA is a method designed to improve reinforcement learning in language-based tasks by addressing the issue of reward sparsity. It does this by aggregating rewards in an intention space, which clusters semantically similar actions and assigns them shared rewards. This approach reduces the variance of rewards, making it easier for agents to learn effective policies. Experiments show that ARIA leads to significant performance improvements across various tasks compared to traditional reinforcement learning methods.'}, 'zh': {'title': 'æ„å›¾ç©ºé—´ä¸­çš„å¥–åŠ±èšåˆï¼Œæå‡è¯­è¨€ä»£ç†çš„å­¦ä¹ æ•ˆç‡', 'desc': 'ARIAæ˜¯ä¸€ç§åœ¨æ„å›¾ç©ºé—´ä¸­èšåˆå¥–åŠ±çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç¼“è§£å¥–åŠ±ç¨€ç–æ€§å¹¶æ”¹å–„åŸºäºè¯­è¨€çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„ç­–ç•¥ä¼˜åŒ–ã€‚é€šè¿‡å°†è‡ªç„¶è¯­è¨€åŠ¨ä½œä»é«˜ç»´çš„è”åˆæ ‡è®°åˆ†å¸ƒç©ºé—´æŠ•å½±åˆ°ä½ç»´çš„æ„å›¾ç©ºé—´ï¼ŒARIAèƒ½å¤Ÿå°†è¯­ä¹‰ç›¸ä¼¼çš„åŠ¨ä½œèšé›†åœ¨ä¸€èµ·å¹¶åˆ†é…å…±äº«å¥–åŠ±ã€‚è¿™ç§åŸºäºæ„å›¾çš„å¥–åŠ±èšåˆå‡å°‘äº†å¥–åŠ±æ–¹å·®ï¼Œå¢å¼ºäº†å¥–åŠ±ä¿¡å·çš„å¯†åº¦ï¼Œä»è€Œä¿ƒè¿›äº†æ›´å¥½çš„ç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARIAæ˜¾è‘—é™ä½äº†ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®ï¼Œå¹¶åœ¨å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å¹³å‡æé«˜äº†9.95%çš„æ€§èƒ½ï¼Œå§‹ç»ˆä¼˜äºç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23590', 'title': 'Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles', 'url': 'https://huggingface.co/papers/2505.23590', 'abstract': 'The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: Firstly, we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. Secondly, training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. Thirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. Fourthly, we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. Finally, our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.', 'score': 24, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '583be7c36c626f1e', 'authors': ['Zifu Wang', 'Junyi Zhu', 'Bo Tang', 'Zhiyu Li', 'Feiyu Xiong', 'Jiaqian Yu', 'Matthew B. Blaschko'], 'affiliations': ['ESAT-PSI, KU Leuven', 'Institute for Advanced Algorithms Research, Shanghai', 'Memory Tensor, Shanghai', 'Samsung R&D Institute China, Beijing', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23590.jpg', 'data': {'categories': ['#multimodal', '#training', '#transfer_learning', '#rl', '#cv', '#open_source', '#reasoning', '#games'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ°Ğ·Ğ»Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (MLLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº-Ğ¿Ğ°Ğ·Ğ»Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ¿Ğ°Ğ·Ğ»Ğ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ°Ğ·Ğ»Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ, Ñ‡ĞµĞ¼ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Multimodal Learning with Jigsaw Puzzles and RL', 'desc': 'This paper explores the use of rule-based reinforcement learning (RL) in multimodal large language models (MLLMs) through the lens of jigsaw puzzles. The study finds that MLLMs can improve from random guessing to near-perfect accuracy on jigsaw puzzles after fine-tuning, demonstrating their ability to generalize to more complex tasks. It also reveals that MLLMs can learn effectively with or without explicit reasoning, although they may not always utilize a step-by-step thought process. Additionally, the research shows that RL outperforms supervised fine-tuning in terms of generalization, highlighting the importance of training strategies in visual tasks.'}, 'zh': {'title': 'åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ–°å‘ç°', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä½¿ç”¨æ‹¼å›¾ä½œä¸ºå®éªŒæ¡†æ¶ï¼Œå‘ç°ç»è¿‡å¾®è°ƒåï¼Œæ¨¡å‹åœ¨ç®€å•æ‹¼å›¾ä¸Šçš„è¡¨ç°ä»éšæœºçŒœæµ‹æå‡è‡³æ¥è¿‘å®Œç¾çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°å¤æ‚çš„æœªè§é…ç½®ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œæ¨¡å‹å¯ä»¥åœ¨æœ‰æ— æ˜¾å¼æ¨ç†çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ³›åŒ–ï¼Œå°½ç®¡å¼€æºæ¨¡å‹æ›´å€¾å‘äºç›´æ¥å›ç­”ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°å¼ºåŒ–å­¦ä¹ åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç›‘ç£å¾®è°ƒï¼Œå¹¶ä¸”åˆå§‹çš„ç›‘ç£å¾®è°ƒå†·å¯åŠ¨é˜¶æ®µå¯èƒ½ä¼šå¦¨ç¢åç»­çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01853', 'title': 'ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding', 'url': 'https://huggingface.co/papers/2506.01853', 'abstract': 'A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni-a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, by performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset. Our work provides an effective attempt at extending multimodal models with basic 3D capabilities, which contributes to future research in 3D-native AI. Project page: https://github.com/JAMESYJL/ShapeLLM-Omni', 'score': 22, 'issue_id': 4091, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '241ff6937e6642f5', 'authors': ['Junliang Ye', 'Zhengyi Wang', 'Ruowen Zhao', 'Shenghao Xie', 'Jun Zhu'], 'affiliations': ['Peking University', 'ShengShu', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01853.jpg', 'data': {'categories': ['#synthetic', '#3d', '#games', '#dataset', '#agi', '#multimodal'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D: ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰Ğ°Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ShapeLLM-Omni - Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ 3D Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° (VQVAE) Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Alpaca. ShapeLLM-Omni Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ 3D-Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ 3D-Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Bridging Text and 3D: ShapeLLM-Omni Unleashes Multimodal Potential', 'desc': 'ShapeLLM-Omni is a novel large language model designed to understand and generate 3D assets alongside text. It utilizes a 3D vector-quantized variational autoencoder (VQVAE) to efficiently represent and reconstruct 3D shapes in a discrete latent space. The model is trained on a new dataset called 3D-Alpaca, which includes diverse tasks such as generation, comprehension, and editing of 3D content. This research aims to enhance multimodal AI capabilities by integrating 3D understanding, paving the way for future advancements in 3D-native artificial intelligence.'}, 'zh': {'title': 'ShapeLLM-Omniï¼šå¼€å¯3Då†…å®¹ç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºShapeLLM-Omniçš„åŸç”Ÿ3Då¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ç†è§£å’Œç”Ÿæˆ3Dèµ„äº§åŠæ–‡æœ¬ã€‚è¯¥æ¨¡å‹ä½¿ç”¨3Då‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVQVAEï¼‰è¿›è¡Œè®­ç»ƒï¼Œå°†3Då¯¹è±¡æ˜ å°„åˆ°ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œä»¥å®ç°é«˜æ•ˆå‡†ç¡®çš„å½¢çŠ¶è¡¨ç¤ºå’Œé‡å»ºã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªåä¸º3D-Alpacaçš„å¤§è§„æ¨¡è¿ç»­è®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–ç”Ÿæˆã€ç†è§£å’Œç¼–è¾‘ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œè®­ç»ƒæä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚é€šè¿‡å¯¹Qwen-2.5-vl-7B-Instructæ¨¡å‹è¿›è¡ŒåŸºäºæŒ‡ä»¤çš„è®­ç»ƒï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºæ‰©å±•å¤šæ¨¡æ€æ¨¡å‹çš„åŸºæœ¬3Dèƒ½åŠ›æä¾›äº†æœ‰æ•ˆçš„å°è¯•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00996', 'title': 'Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2506.00996', 'abstract': "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/", 'score': 22, 'issue_id': 4090, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '25ea795d193b8719', 'authors': ['Kinam Kim', 'Junha Hyung', 'Jaegul Choo'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.00996.jpg', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ Temporal In-Context Fine-Tuning (TIC-FT) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. TIC-FT Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ°Ğ´Ñ€Ñ‹ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ¸, Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ±ÑƒÑ„ĞµÑ€Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ÑÑ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑˆÑƒĞ¼Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 10-30 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TIC-FT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ.'}, 'en': {'title': 'Efficient Video Generation with Minimal Data Using TIC-FT', 'desc': "Temporal In-Context Fine-Tuning (TIC-FT) is a novel method that improves pretrained video diffusion models for various conditional generation tasks without needing extensive data or changing the model architecture. It works by combining condition and target frames along the time axis and adding intermediate buffer frames with increasing noise, which helps maintain the model's temporal coherence. This approach allows for effective fine-tuning with as few as 10-30 training samples, making it efficient and scalable. TIC-FT has been shown to outperform existing methods in generating high-quality videos while ensuring fidelity to the given conditions."}, 'zh': {'title': 'æ—¶é—´ä¸Šä¸‹æ–‡å¾®è°ƒï¼šé«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæ—¶é—´ä¸Šä¸‹æ–‡å¾®è°ƒï¼ˆTIC-FTï¼‰ï¼Œç”¨äºå¢å¼ºé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°å¤šæ ·åŒ–çš„æ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚TIC-FTé€šè¿‡åœ¨æ—¶é—´è½´ä¸Šè¿æ¥æ¡ä»¶å¸§å’Œç›®æ ‡å¸§ï¼Œå¹¶æ’å…¥é€æ¸å¢åŠ å™ªå£°æ°´å¹³çš„ä¸­é—´ç¼“å†²å¸§ï¼Œæ¥å®ç°å¹³æ»‘è¿‡æ¸¡ï¼Œä»è€Œä¸é¢„è®­ç»ƒæ¨¡å‹çš„æ—¶é—´åŠ¨æ€å¯¹é½ã€‚è¯¥æ–¹æ³•æ— éœ€å¯¹æ¨¡å‹æ¶æ„è¿›è¡Œä¿®æ”¹ï¼Œä¸”åªéœ€10åˆ°30ä¸ªè®­ç»ƒæ ·æœ¬å³å¯å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTIC-FTåœ¨æ¡ä»¶ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒé«˜æ•ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00411', 'title': 'LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks', 'url': 'https://huggingface.co/papers/2506.00411', 'abstract': 'A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.', 'score': 22, 'issue_id': 4091, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '16fa44208bf3618c', 'authors': ['Yi Yang', 'Jiaxuan Sun', 'Siqi Kou', 'Yihan Wang', 'Zhijie Deng'], 'affiliations': ['Fudan University', 'Shanghai Jiao Tong University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00411.jpg', 'data': {'categories': ['#cv', '#architecture', '#robotics', '#agents', '#dataset', '#agi', '#long_context'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'LoHoVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. LoHoVLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ LoHoVLA Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğµ Ravens.'}, 'en': {'title': 'Empowering Robots with Unified Vision and Language for Complex Tasks', 'desc': 'The paper presents LoHoVLA, a new framework that integrates a large pretrained vision language model with hierarchical closed-loop control to enhance performance in long-horizon embodied tasks. It addresses the challenges of high-level task planning and low-level motion control by generating language and action tokens for effective sub-task generation and robot action prediction. The framework is trained on a unique dataset, LoHoSet, which includes a variety of long-horizon tasks with expert demonstrations. Experimental results demonstrate that LoHoVLA outperforms existing models, highlighting the potential of unified architectures in improving embodied intelligence.'}, 'zh': {'title': 'ç»Ÿä¸€æ¡†æ¶æå‡é•¿æ—¶é—´ä»»åŠ¡è¡¨ç°', 'desc': 'LoHoVLAæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é•¿æ—¶é—´ä»»åŠ¡çš„è¡¨ç°ã€‚å®ƒç»“åˆäº†å¤§å‹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œåˆ†å±‚é—­ç¯æ§åˆ¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œé«˜å±‚æ¬¡ä»»åŠ¡è§„åˆ’å’Œä½å±‚æ¬¡è¿åŠ¨æ§åˆ¶ã€‚é€šè¿‡ç”Ÿæˆè¯­è¨€å’ŒåŠ¨ä½œæ ‡è®°ï¼ŒLoHoVLAä¿ƒè¿›äº†ä»»åŠ¡é—´çš„æ›´å¥½æ³›åŒ–ã€‚æ­¤å¤–ï¼ŒLoHoVLAåœ¨Ravensæ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01943', 'title': 'Learning Video Generation for Robotic Manipulation with Collaborative\n  Trajectory Control', 'url': 'https://huggingface.co/papers/2506.01943', 'abstract': 'Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation.', 'score': 20, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '4acb1e4fc9635b8a', 'authors': ['Xiao Fu', 'Xintao Wang', 'Xian Liu', 'Jianhong Bai', 'Runsen Xu', 'Pengfei Wan', 'Di Zhang', 'Dahua Lin'], 'affiliations': ['Kuaishou Technology', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01943.jpg', 'data': {'categories': ['#robotics', '#video', '#diffusion'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RoboMaster: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ RoboMaster Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ´Ğ¾, Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'RoboMaster: Enhancing Robotic Video Generation through Interaction Modeling', 'desc': "This paper introduces RoboMaster, a new framework designed to improve video generation for robotic decision-making by focusing on multi-object interactions. Unlike previous methods that treat objects separately, RoboMaster breaks down the interaction process into three stages: pre-interaction, interaction, and post-interaction. By modeling these stages with the dominant object's features, it effectively addresses the challenges of overlapping features that degrade visual quality. The framework also uses advanced representations to maintain semantic consistency, resulting in superior performance on complex tasks compared to existing techniques."}, 'zh': {'title': 'RoboMasterï¼šæå‡æœºå™¨äººæ“ä½œçš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRoboMasterçš„æ–°æ¡†æ¶ï¼Œç”¨äºå»ºæ¨¡å¤šç‰©ä½“ä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œä»¥æ”¹å–„æœºå™¨äººå†³ç­–æ•°æ®çš„ç”Ÿæˆã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒRoboMasterå°†äº¤äº’è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªå­é˜¶æ®µï¼šé¢„äº¤äº’ã€äº¤äº’å’Œåäº¤äº’ï¼Œåˆ†åˆ«ä½¿ç”¨ä¸»å¯¼ç‰©ä½“çš„ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒRoboMasteræœ‰æ•ˆåœ°è§£å†³äº†å¤šç‰©ä½“ç‰¹å¾èåˆå¸¦æ¥çš„é—®é¢˜ï¼Œæé«˜äº†è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01713', 'title': 'SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.01713', 'abstract': 'Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.', 'score': 20, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'fc955e3f149c1b08', 'authors': ['Zhongwei Wan', 'Zhihao Dou', 'Che Liu', 'Yu Zhang', 'Dongfei Cui', 'Qinjian Zhao', 'Hui Shen', 'Jing Xiong', 'Yi Xin', 'Yifan Jiang', 'Yangfan He', 'Mi Zhang', 'Shen Yan'], 'affiliations': ['ByteDance Seed', 'Case Western Reserve University', 'Duke University', 'Imperial College London', 'Kean University Minnesota', 'Nanjing University', 'The Ohio State University', 'The University of Hong Kong', 'Tongji University', 'University of Michigan', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.01713.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rl', '#benchmark', '#dataset', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SRPO: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). SRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸ĞµĞ¹ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… MLLM Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Multimodal Reasoning through Self-Reflection', 'desc': "This paper introduces a new approach called Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO) to improve the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing reflection methods are inadequate for generating useful feedback, which limits the models' performance on complex reasoning tasks. SRPO consists of two stages: first, it creates a high-quality dataset for training the model to reflect on its responses, and second, it implements a reward mechanism that promotes meaningful reflections. Experimental results show that SRPO significantly enhances both the accuracy of reasoning and the quality of reflections compared to current leading models."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„è‡ªæˆ‘åæ€æ¡†æ¶', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦æ˜ç¡®è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘çº æ­£çš„å¤æ‚é—®é¢˜ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„åæ€æ–¹æ³•è¿‡äºç®€å•ï¼Œéš¾ä»¥ç”Ÿæˆæœ‰æ„ä¹‰å’ŒæŒ‡å¯¼æ€§çš„åé¦ˆï¼Œå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒçŸ¥è¯†åœ¨åˆå§‹è®­ç»ƒæœŸé—´åŸºæœ¬å›ºå®šã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºè‡ªæˆ‘åæ€å¢å¼ºæ¨ç†çš„å¤šæ¨¡æ€è‡ªæˆ‘åæ€æ¡†æ¶ï¼ˆSRPOï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µçš„åæ€æ„è¯†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æå‡å¤šæ¨¡æ€LLMçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSRPOåœ¨å¤šä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ¨ç†å‡†ç¡®æ€§å’Œåæ€è´¨é‡éƒ½æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01667', 'title': 'EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models', 'url': 'https://huggingface.co/papers/2506.01667', 'abstract': 'EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have demonstrated strong performance in various vision-language tasks. However, they often struggle to comprehensively understand Earth Observation (EO) data, which is critical for monitoring the environment and the effects of human activity on it. In this work, we present EarthMind, a novel vision-language framework for multi-granular and multi-sensor EO data understanding. EarthMind features two core components: (1) Spatial Attention Prompting (SAP), which reallocates attention within the LLM to enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns heterogeneous modalities into a shared space and adaptively reweighs tokens based on their information density for effective fusion. To facilitate multi-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive benchmark with over 2,000 human-annotated multi-sensor image-question pairs, covering a wide range of perception and reasoning tasks. Extensive experiments demonstrate the effectiveness of EarthMind. It achieves state-of-the-art performance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in scale. Moreover, EarthMind outperforms existing methods on multiple public EO benchmarks, showcasing its potential to handle both multi-granular and multi-sensor challenges in a unified framework.', 'score': 17, 'issue_id': 4093, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '90528034771977ef', 'authors': ['Yan Shu', 'Bin Ren', 'Zhitong Xiong', 'Danda Pani Paudel', 'Luc Van Gool', 'Begum Demir', 'Nicu Sebe', 'Paolo Rota'], 'affiliations': ['INSAIT, Sofia University St. Kliment Ohridski', 'Technical University of Munich', 'Technische UniversitÃ¤t Berlin', 'University of Pisa', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2506.01667.jpg', 'data': {'categories': ['#benchmark', '#cv', '#survey', '#reasoning', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'EarthMind: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸', 'desc': 'EarthMind - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. EarthMind Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸.'}, 'en': {'title': 'EarthMind: Revolutionizing Earth Observation with Vision-Language Fusion', 'desc': "EarthMind is a new framework designed to improve the understanding of Earth Observation (EO) data by combining vision and language processing. It uses Spatial Attention Prompting (SAP) to focus on important details at the pixel level, enhancing the model's ability to interpret images. Additionally, Cross-modal Fusion aligns different types of data, allowing the model to weigh information based on its relevance. This approach not only outperforms larger models like GPT-4o on specialized benchmarks but also effectively handles complex multi-sensor data tasks."}, 'zh': {'title': 'EarthMindï¼šé«˜æ•ˆç†è§£åœ°çƒè§‚æµ‹æ•°æ®çš„åˆ›æ–°æ¡†æ¶', 'desc': 'EarthMindæ˜¯ä¸€ä¸ªè§†è§‰-è¯­è¨€æ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆç†è§£å¤šç²’åº¦å’Œå¤šä¼ æ„Ÿå™¨çš„åœ°çƒè§‚æµ‹æ•°æ®ã€‚å®ƒé‡‡ç”¨ç©ºé—´æ³¨æ„åŠ›æç¤ºå’Œè·¨æ¨¡æ€èåˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸“é—¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šæ›´å¤§çš„æ¨¡å‹ã€‚EarthMindçš„ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶åˆ†åˆ«æ˜¯ç©ºé—´æ³¨æ„åŠ›æç¤ºï¼ˆSAPï¼‰ï¼Œç”¨äºå¢å¼ºåƒç´ çº§ç†è§£ï¼Œä»¥åŠè·¨æ¨¡æ€èåˆï¼Œèƒ½å¤Ÿå°†ä¸åŒæ¨¡æ€å¯¹é½åˆ°å…±äº«ç©ºé—´å¹¶æ ¹æ®ä¿¡æ¯å¯†åº¦è‡ªé€‚åº”è°ƒæ•´æƒé‡ã€‚é€šè¿‡EarthMind-BenchåŸºå‡†æµ‹è¯•ï¼ŒEarthMindåœ¨å¤šä¸ªå…¬å…±åœ°çƒè§‚æµ‹åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹å¤„ç†å¤šç²’åº¦å’Œå¤šä¼ æ„Ÿå™¨æŒ‘æˆ˜çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24298', 'title': 'AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning', 'url': 'https://huggingface.co/papers/2505.24298', 'abstract': 'AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.57times training speedup compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.', 'score': 16, 'issue_id': 4092, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'fad566ec1d2ba264', 'authors': ['Wei Fu', 'Jiaxuan Gao', 'Xujie Shen', 'Chen Zhu', 'Zhiyu Mei', 'Chuyi He', 'Shusheng Xu', 'Guo Wei', 'Jun Mei', 'Jiashu Wang', 'Tongkai Yang', 'Binhang Yuan', 'Yi Wu'], 'affiliations': ['Ant Research', 'HKUST', 'IIIS, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24298.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': 'ğŸš€', 'ru': {'title': 'AReaL: ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'AReaL - ÑÑ‚Ğ¾ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GPU Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2.57 Ñ€Ğ°Ğ· Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ÑĞ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ AReaL Ğ½Ğ°Ğ´ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'AReaL: Revolutionizing Reinforcement Learning with Asynchronous Training', 'desc': 'AReaL is an innovative reinforcement learning system designed to enhance the training of large language models by decoupling the generation and training processes. This fully asynchronous approach allows for continuous output generation without waiting for the longest tasks to finish, leading to improved GPU utilization. By balancing the workload between rollout and training workers, AReaL effectively manages data staleness and employs a modified Proximal Policy Optimization (PPO) to optimize training with outdated samples. Experimental results demonstrate that AReaL can achieve up to 2.57 times faster training speeds while maintaining or improving performance on reasoning tasks.'}, 'zh': {'title': 'AReaLï¼šå¼‚æ­¥å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆè®­ç»ƒæ–°æ¨¡å¼', 'desc': 'AReaLæ˜¯ä¸€ç§å®Œå…¨å¼‚æ­¥çš„å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œå®ƒå°†ç”Ÿæˆå’Œè®­ç»ƒè§£è€¦ï¼Œä»è€Œæé«˜GPUçš„åˆ©ç”¨ç‡ï¼Œå¹¶åœ¨æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾2.57å€çš„è®­ç»ƒåŠ é€Ÿã€‚ä¼ ç»Ÿçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿé€šå¸¸æ˜¯åŒæ­¥çš„ï¼Œç”Ÿæˆå’Œè®­ç»ƒäº¤æ›¿è¿›è¡Œï¼Œè¿™å¯¼è‡´äº†ç³»ç»Ÿæ•ˆç‡ä½ä¸‹ã€‚AReaLé€šè¿‡è®©ç”Ÿæˆå·¥ä½œè€…æŒç»­ç”Ÿæˆæ–°è¾“å‡ºï¼Œè€Œè®­ç»ƒå·¥ä½œè€…åœ¨æ”¶é›†åˆ°ä¸€æ‰¹æ•°æ®åç«‹å³æ›´æ–°æ¨¡å‹ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡å¹³è¡¡ç”Ÿæˆå’Œè®­ç»ƒå·¥ä½œè€…çš„å·¥ä½œè´Ÿè½½ï¼ŒAReaLæœ‰æ•ˆæ§åˆ¶äº†æ•°æ®çš„è¿‡æ—¶æ€§ï¼Œå¹¶é‡‡ç”¨äº†å¢å¼ºè¿‡æ—¶æ€§çš„PPOå˜ä½“æ¥æ›´å¥½åœ°å¤„ç†è¿‡æ—¶çš„è®­ç»ƒæ ·æœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01863', 'title': 'Unified Scaling Laws for Compressed Representations', 'url': 'https://huggingface.co/papers/2506.01863', 'abstract': 'A study on scaling laws and compression techniques shows that a unified capacity metric can predict model performance across different compressed formats, including scalar-quantized, sparse-quantized, and vector-quantized representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression formats, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include validating a general scaling law formulation and showing that it is applicable both individually but also composably across compression types. Based on this, our main finding is demonstrating both theoretically and empirically that there exists a simple "capacity" metric -- based on the representation\'s ability to fit random Gaussian data -- which can robustly predict parameter efficiency across multiple compressed representations. On the practical side, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats.', 'score': 14, 'issue_id': 4099, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '0f97dac5f5c8309f', 'authors': ['Andrei Panferov', 'Alexandra Volkova', 'Ionut-Vlad Modoranu', 'Vage Egiazarian', 'Mher Safaryan', 'Dan Alistarh'], 'affiliations': ['ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.01863.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾-ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unified Capacity Metric: Predicting Performance in Compressed Models', 'desc': 'This paper explores how scaling laws in machine learning can predict the performance of models when they are compressed using different techniques. It focuses on various compression formats like scalar-quantized, sparse-quantized, and vector-quantized representations. The authors propose a unified capacity metric that can effectively gauge the efficiency of these compressed models based on their ability to handle random Gaussian data. Their findings suggest that this metric not only applies to individual compression types but can also be used to compare and improve training algorithms across different formats.'}, 'zh': {'title': 'ç»Ÿä¸€å®¹é‡åº¦é‡ï¼Œé¢„æµ‹æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†ç¼©æ”¾æ³•åˆ™ä¸å‹ç¼©æŠ€æœ¯ä¹‹é—´çš„å…³ç³»ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å®¹é‡åº¦é‡ï¼Œå¯ä»¥é¢„æµ‹ä¸åŒå‹ç¼©æ ¼å¼ä¸‹æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹çš„æ€§èƒ½å¯ä»¥æ ¹æ®æ¨¡å‹å¤§å°ã€è®¡ç®—é‡å’Œæ•°æ®é‡è¿›è¡Œå¯é¢„æµ‹çš„ç¼©æ”¾ã€‚é€šè¿‡éªŒè¯é€šç”¨çš„ç¼©æ”¾æ³•åˆ™å…¬å¼ï¼Œç ”ç©¶å‘ç°è¯¥å…¬å¼é€‚ç”¨äºä¸åŒçš„å‹ç¼©ç±»å‹ã€‚æœ€ç»ˆï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„â€œå®¹é‡â€åº¦é‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹å¤šç§å‹ç¼©è¡¨ç¤ºä¸‹çš„å‚æ•°æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24846', 'title': 'MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning', 'url': 'https://huggingface.co/papers/2505.24846', 'abstract': 'MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.', 'score': 14, 'issue_id': 4091, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'ed5c25a307e9093d', 'authors': ['Jingyan Shen', 'Jiarui Yao', 'Rui Yang', 'Yifan Sun', 'Feng Luo', 'Rui Pan', 'Tong Zhang', 'Han Zhao'], 'affiliations': ['Columbia University', 'Rice University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.24846.jpg', 'data': {'categories': ['#dataset', '#training', '#rlhf', '#alignment'], 'emoji': 'ğŸ­', 'ru': {'title': 'MiCRo: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'MiCRo - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑĞ° ÑĞ¼ĞµÑĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. MiCRo ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MiCRo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'MiCRo: Dynamic Personalization for Diverse Human Preferences', 'desc': 'MiCRo is a two-stage framework designed to enhance personalized preference learning for large language models (LLMs). It utilizes binary preference datasets and employs a context-aware mixture modeling approach to better capture the diverse preferences of humans. The framework dynamically adjusts mixture weights based on the context, allowing for more accurate and scalable preference adaptation. Experimental results show that MiCRo significantly improves the ability of LLMs to personalize responses according to varied human preferences.'}, 'zh': {'title': 'MiCRoï¼šæ•æ‰å¤šæ ·åŒ–äººç±»åå¥½çš„æ–°æ¡†æ¶', 'desc': 'MiCRoæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–åå¥½å­¦ä¹ ã€‚å®ƒåˆ©ç”¨äºŒå…ƒåå¥½æ•°æ®é›†ï¼Œå¹¶æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ··åˆæƒé‡ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰å¤šæ ·åŒ–çš„äººç±»åå¥½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ··åˆå»ºæ¨¡ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹æ— æ³•å……åˆ†åæ˜ äººç±»å¤šæ ·æ€§çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMiCRoåœ¨å¤šä¸ªåå¥½æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä¸ªæ€§åŒ–æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01413', 'title': 'Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.01413', 'abstract': 'Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.', 'score': 11, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '3f0db6c1e3cc1878', 'authors': ['Yulei Qin', 'Gang Li', 'Zongyi Li', 'Zihan Xu', 'Yuchen Shi', 'Zhekai Lin', 'Xiao Cui', 'Ke Li', 'Xing Sun'], 'affiliations': ['Tencent YouTu Lab', 'The Chinese University of Hong Kong', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01413.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ğ¹ ÑƒĞ¼Ğ½ĞµĞµ, Ğ° Ğ½Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğµ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Reasoning to Deep Understanding', 'desc': "This paper addresses the limitations of large language models (LLMs) in following complex instructions, particularly when these instructions involve multiple constraints. The authors critique the traditional chain-of-thought (CoT) approach, which often leads to poor performance due to its tendency to merely rephrase instructions without deep reasoning. To improve LLMs' ability to handle complex tasks, they propose a systematic method that includes decomposing instructions and using reinforcement learning with specific reward signals to enhance reasoning. Their extensive evaluations demonstrate that their approach significantly boosts performance, achieving results comparable to larger models with fewer parameters."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›', 'desc': 'ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å½“æŒ‡ä»¤åŒ…å«å¤šä¸ªå¹¶è¡Œã€é“¾å¼å’Œåˆ†æ”¯ç»“æ„çš„çº¦æŸæ—¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼Œé€šè¿‡æ¿€åŠ±æ¨ç†æ¥æå‡LLMså¤„ç†å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¯éªŒè¯çš„è§„åˆ™ä¸­å¿ƒå¥–åŠ±ä¿¡å·ï¼ŒåŸ¹å…»æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšæ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”æ ·æœ¬ï¼Œæˆ‘ä»¬è§£å†³äº†åœ¨å¤æ‚æŒ‡ä»¤ä¸‹æ¨ç†çš„æµ…å±‚å’Œéæœ¬è´¨ç‰¹æ€§ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00577', 'title': 'Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs', 'url': 'https://huggingface.co/papers/2506.00577', 'abstract': 'Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.  \t\t\t\t\tAI-generated summary \t\t\t\t Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively generalize to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce Recon (Reasoning like an ECONomist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .', 'score': 10, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '0ed5d0b7064f9962', 'authors': ['Yufa Zhou', 'Shaobo Wang', 'Xingyu Dong', 'Xiangqi Jin', 'Yifang Chen', 'Yue Min', 'Kexin Yang', 'Xingzhang Ren', 'Dayiheng Liu', 'Linfeng Zhang'], 'affiliations': ['Duke University', 'EPIC Lab, Shanghai Jiao Tong University', 'Qwen Team, Alibaba Group', 'The University of Chicago', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2506.00577.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#agents', '#open_source', '#reasoning', '#games', '#dataset'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° (SFT) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR), Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Recon - 7B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 2100 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Economic Reasoning in LLMs through Post-Training Techniques', 'desc': "This paper investigates how post-training techniques can enhance the performance of Large Language Models (LLMs) in multi-agent environments. It specifically focuses on Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve reasoning and economic decision-making. The authors introduce a model called Recon, which is trained on a dataset of economic reasoning problems, demonstrating significant advancements in structured reasoning capabilities. The findings suggest that domain-aligned post-training can effectively improve LLMs' reasoning and alignment in complex scenarios."}, 'zh': {'title': 'åè®­ç»ƒæŠ€æœ¯æå‡æ™ºèƒ½ä½“æ¨ç†ä¸ç»æµç†æ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åè®­ç»ƒæŠ€æœ¯å¦‚ä½•æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›å’Œç»æµç†æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨ç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé’ˆå¯¹ç»æµæ¨ç†è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡å¼•å…¥Reconæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨é«˜è´¨é‡ç»æµæ¨ç†é—®é¢˜çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†åè®­ç»ƒï¼Œå¹¶åœ¨ç»æµæ¨ç†åŸºå‡†å’Œå¤šæ™ºèƒ½ä½“æ¸¸æˆä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡åè®­ç»ƒçš„æ¨¡å‹åœ¨ç»“æ„åŒ–æ¨ç†å’Œç»æµç†æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†é¢†åŸŸå¯¹é½çš„åè®­ç»ƒåœ¨å¢å¼ºæ¨ç†å’Œæ™ºèƒ½ä½“å¯¹é½æ–¹é¢çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23907', 'title': 'Cora: Correspondence-aware image editing using few step diffusion', 'url': 'https://huggingface.co/papers/2505.23907', 'abstract': 'Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora, a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives.', 'score': 10, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '7de1457440a0b449', 'authors': ['Amirhossein Almohammadi', 'Aryan Mikaeili', 'Sauradip Nag', 'Negar Hassanpour', 'Andrea Tagliasacchi', 'Ali Mahdavi-Amiri'], 'affiliations': ['Google Deepmind, Canada', 'Huawei, Canada', 'Simon Fraser University, Canada', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2505.23907.jpg', 'data': {'categories': ['#cv', '#video', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€', 'desc': 'Cora - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ°. Cora Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾.'}, 'en': {'title': 'Cora: Revolutionizing Image Editing with Precision and Control', 'desc': 'The Cora framework improves image editing by using advanced techniques like correspondence-aware noise correction and interpolated attention maps. It effectively aligns textures and structures between source and target images, allowing for accurate texture transfer and content generation. This method addresses common issues in image editing, such as preserving key attributes and avoiding artifacts during significant structural changes. Extensive testing shows that Cora maintains high quality in structure, texture, and identity across various editing tasks, outperforming existing methods.'}, 'zh': {'title': 'Coraï¼šå›¾åƒç¼–è¾‘çš„æ–°çªç ´', 'desc': 'Coraæ¡†æ¶é€šè¿‡å¼•å…¥å¯¹åº”æ„ŸçŸ¥å™ªå£°æ ¡æ­£å’Œæ’å€¼æ³¨æ„åŠ›å›¾ï¼Œå¢å¼ºäº†å›¾åƒç¼–è¾‘çš„æ•ˆæœã€‚å®ƒèƒ½å¤Ÿåœ¨æºå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´å¯¹é½çº¹ç†å’Œç»“æ„ï¼Œä»è€Œå®ç°å‡†ç¡®çš„çº¹ç†è½¬ç§»å’Œå¿…è¦çš„æ–°å†…å®¹ç”Ÿæˆã€‚Coraåœ¨å†…å®¹ç”Ÿæˆå’Œä¿ç•™ä¹‹é—´æä¾›äº†è‰¯å¥½çš„æ§åˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å§¿æ€å˜åŒ–ã€ç‰©ä½“æ·»åŠ å’Œçº¹ç†ç»†åŒ–ç­‰å¤šç§ç¼–è¾‘ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoraåœ¨ç»“æ„ã€çº¹ç†å’Œèº«ä»½çš„ä¿æŒä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”¨æˆ·ç ”ç©¶ä¹Ÿè¯å®äº†å…¶ä¼˜äºå…¶ä»–æ–¹æ³•çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01952', 'title': 'WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks', 'url': 'https://huggingface.co/papers/2506.01952', 'abstract': 'WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.', 'score': 9, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'aa260fbf373a4f2c', 'authors': ['Atsuyuki Miyai', 'Zaiying Zhao', 'Kazuki Egashira', 'Atsuki Sato', 'Tatsumi Sunada', 'Shota Onohara', 'Hiromasa Yamanishi', 'Mashiro Toyooka', 'Kunato Nishina', 'Ryoma Maeda', 'Kiyoharu Aizawa', 'Toshihiko Yamasaki'], 'affiliations': ['The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2506.01952.jpg', 'data': {'categories': ['#reasoning', '#agents', '#agi', '#benchmark', '#long_context'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'WebChoreArena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'WebChoreArena - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ², ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 532 Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ WebArena Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑ‚Ğ¾Ğ¼Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²ĞµĞ±-Ğ±Ñ€Ğ°ÑƒĞ·Ğ¸Ğ½Ğ³Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°: Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GPT-4, Claude 3.7 Sonnet Ğ¸ Gemini 2.5 Pro, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° WebChoreArena. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ñ Gemini 2.5 Pro Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ WebArena.'}, 'en': {'title': 'WebChoreArena: Elevating LLMs to Tackle Tedious Web Tasks', 'desc': 'WebChoreArena is a new benchmark that includes 532 tasks designed to evaluate the capabilities of large language models (LLMs) in handling complex web browsing chores. It extends the previous WebArena benchmark by focusing on more tedious tasks that require advanced skills such as massive memory retrieval, precise calculations, and long-term memory management across multiple web pages. The benchmark allows for reproducible experiments and fair comparisons with existing models, showcasing the progress of LLMs like GPT-4o and Gemini 2.5 Pro. Despite improvements in performance, the results indicate that there is still significant room for enhancement in tackling the challenges presented by WebChoreArena compared to general browsing tasks.'}, 'zh': {'title': 'WebChoreArenaï¼šè¯„ä¼°LLMåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„èƒ½åŠ›', 'desc': 'WebChoreArenaæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«532ä¸ªä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å’Œç¹ççš„ç½‘é¡µæµè§ˆä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æ‰©å±•äº†WebArenaçš„èŒƒå›´ï¼Œä¸“æ³¨äºäººç±»é€šå¸¸é¿å…çš„ç¹é‡ä»»åŠ¡ã€‚WebChoreArenaæ•´åˆäº†ä¸‰å¤§æŒ‘æˆ˜ï¼šå¤§è§„æ¨¡è®°å¿†ä»»åŠ¡ã€è®¡ç®—ä»»åŠ¡å’Œé•¿æœŸè®°å¿†ä»»åŠ¡ï¼Œç¡®ä¿äº†ä¸¥æ ¼çš„å¯é‡å¤æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€LLMçš„è¿›æ­¥ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œæ˜¾ç¤ºå‡ºWebChoreArenaçš„æŒ‘æˆ˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23977', 'title': 'VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL', 'url': 'https://huggingface.co/papers/2505.23977', 'abstract': 'VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.', 'score': 8, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': 'fef2cab0e56bc9bd', 'authors': ['Yichen Feng', 'Zhangchen Xu', 'Fengqing Jiang', 'Yuetai Li', 'Bhaskar Ramasubramanian', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran'], 'affiliations': ['University of Washington', 'Western Washington University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23977.jpg', 'data': {'categories': ['#synthetic', '#cv', '#multimodal', '#reasoning', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'VisualSphinx - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° VisualSphinx ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° VisualSphinx, Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ğ°Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with VisualSphinx', 'desc': 'VisualSphinx is a synthetic dataset designed to enhance multimodal reasoning in vision language models (VLMs). It addresses the lack of large-scale, structured training data necessary for effective logical reasoning in tasks like diagram understanding. The dataset is created using a rule-to-image synthesis pipeline that generates images based on logical rules extracted from questions. Experiments show that VLMs trained on VisualSphinx demonstrate improved logical coherence and performance across various reasoning tasks, including algebra and geometry.'}, 'zh': {'title': 'VisualSphinxï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›', 'desc': 'VisualSphinxæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†ä¸“æ³¨äºé€»è¾‘æ¨ç†ä»»åŠ¡ï¼Œè§£å†³äº†å½“å‰æ¨¡å‹ç¼ºä¹ç»“æ„åŒ–è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚é€šè¿‡è§„åˆ™åˆ°å›¾åƒçš„åˆæˆæµç¨‹ï¼ŒVisualSphinxèƒ½å¤Ÿç”Ÿæˆä¸é—®é¢˜ç›¸å…³çš„å›¾åƒï¼Œå¢å¼ºæ¨¡å‹çš„é€»è¾‘ä¸€è‡´æ€§å’Œå¯è¯»æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨VisualSphinxè®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ã€ä»£æ•°æ¨ç†ã€ç®—æœ¯æ¨ç†å’Œå‡ ä½•æ¨ç†ç­‰ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23059', 'title': 'From Token to Action: State Machine Reasoning to Mitigate Overthinking\n  in Information Retrieval', 'url': 'https://huggingface.co/papers/2505.23059', 'abstract': 'State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting enables complex reasoning in large language models (LLMs), including applications in information retrieval (IR). However, it often leads to overthinking, where models produce excessively long and semantically redundant traces with little or no benefit. We identify two key challenges in IR: redundant trajectories that revisit similar states and misguided reasoning that diverges from user intent. To address these, we propose State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning, offering a practical alternative to conventional CoT reasoning. The code and details are available at https://github.com/ldilab/SMR.', 'score': 8, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '83af42c01de2e64c', 'authors': ['Dohyeon Lee', 'Yeonseok Jeong', 'Seung-won Hwang'], 'affiliations': ['Computer Science and Engineering, Seoul National University', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23059.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#optimization', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SMR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ State Machine Reasoning (SMR). SMR ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ, Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SMR Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° 3.4% Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 74.4%.'}, 'en': {'title': 'Streamlining Retrieval with State Machine Reasoning', 'desc': 'State Machine Reasoning (SMR) is a new framework designed to enhance information retrieval in large language models by minimizing unnecessary complexity. It tackles the problem of overthinking, which often results in lengthy and repetitive outputs that do not improve results. SMR introduces a set of discrete actions that allow models to make more efficient decisions, leading to better performance and reduced token usage. Experiments demonstrate that SMR significantly boosts retrieval accuracy while being adaptable across different models without needing specific adjustments.'}, 'zh': {'title': 'çŠ¶æ€æœºæ¨ç†ï¼šæå‡æ£€ç´¢æ•ˆç‡ï¼Œå‡å°‘èµ„æºæ¶ˆè€—', 'desc': 'çŠ¶æ€æœºæ¨ç†ï¼ˆSMRï¼‰é€šè¿‡ç¦»æ•£åŠ¨ä½œæ¡†æ¶æ¥æ”¹å–„ä¿¡æ¯æ£€ç´¢æ€§èƒ½ï¼Œå¹¶å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œä½¿ç”¨ï¼Œè§£å†³äº†è¿‡åº¦æ€è€ƒçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•è¯†åˆ«äº†ä¿¡æ¯æ£€ç´¢ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå†—ä½™è½¨è¿¹å’Œè¯¯å¯¼æ€§æ¨ç†ã€‚SMRé‡‡ç”¨åŸºäºè½¬ç§»çš„æ¨ç†æ¡†æ¶ï¼ŒåŒ…å«ç²¾ç»†æ§åˆ¶çš„ç¦»æ•£åŠ¨ä½œï¼ˆå¦‚ç²¾ç‚¼ã€é‡æ–°æ’åºå’Œåœæ­¢ï¼‰ï¼Œæ”¯æŒæ—©æœŸåœæ­¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSMRåœ¨BEIRå’ŒBRIGHTåŸºå‡†ä¸Šæé«˜äº†3.4%çš„æ£€ç´¢æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†74.4%çš„ä»¤ç‰Œä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23001', 'title': 'DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors', 'url': 'https://huggingface.co/papers/2505.23001', 'abstract': 'DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.  \t\t\t\t\tAI-generated summary \t\t\t\t Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.', 'score': 8, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': 'cd584a75fce48ae2', 'authors': ['Yize Cheng', 'Wenxiao Wang', 'Mazda Moayeri', 'Soheil Feizi'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2505.23001.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#security', '#leakage'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'DyePack: Ğ›Ğ¾Ğ²ÑƒÑˆĞºĞ° Ğ´Ğ»Ñ Ğ½ĞµÑ‡ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'DyePack - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ‚Ğ¸Ğ¿Ğ° backdoor Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ±ĞµĞ·Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ backdoor Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ²ÑˆĞ¸ĞµÑÑ Ğ½Ğ° Ğ½Ğ¸Ñ…. DyePack Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‡ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ²Ğ¸Ğ½ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹.'}, 'en': {'title': 'DyePack: Safeguarding Model Integrity with Backdoor Detection', 'desc': 'DyePack is a novel framework designed to detect models that have been trained using benchmark test sets by employing backdoor attacks. It introduces benign backdoor samples into the test data, allowing for the identification of contaminated models without needing access to their internal workings. The framework ensures precise computation of false positive rates, effectively preventing wrongful accusations against models. Through extensive evaluation, DyePack demonstrates its capability to accurately flag contaminated models across various tasks while maintaining low false positive rates.'}, 'zh': {'title': 'DyePackï¼šç²¾å‡†è¯†åˆ«è®­ç»ƒä¸­ä½¿ç”¨åŸºå‡†æµ‹è¯•é›†çš„æ¨¡å‹', 'desc': 'DyePackæ˜¯ä¸€ä¸ªåˆ©ç”¨åé—¨æ”»å‡»çš„æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«åœ¨è®­ç»ƒä¸­ä½¿ç”¨åŸºå‡†æµ‹è¯•é›†çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡å¼•å…¥è‰¯æ€§åé—¨æ ·æœ¬ï¼Œç¡®ä¿å‡†ç¡®çš„å‡é˜³æ€§ç‡ï¼ŒåŒæ—¶é˜²æ­¢é”™è¯¯æŒ‡æ§ã€‚DyePackçš„è®¾è®¡ç»“åˆäº†å¤šä¸ªå…·æœ‰éšæœºç›®æ ‡çš„åé—¨ï¼Œä½¿å¾—åœ¨æ ‡è®°æ¯ä¸ªæ¨¡å‹æ—¶èƒ½å¤Ÿç²¾ç¡®è®¡ç®—å‡é˜³æ€§ç‡ã€‚é€šè¿‡åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼ŒDyePackæˆåŠŸæ£€æµ‹åˆ°æ‰€æœ‰å—æ±¡æŸ“çš„æ¨¡å‹ï¼Œä¸”å‡é˜³æ€§ç‡æä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24523', 'title': 'Stress-testing Machine Generated Text Detection: Shifting Language\n  Models Writing Style to Fool Detectors', 'url': 'https://huggingface.co/papers/2505.24523', 'abstract': "Adversarial attacks using Direct Preference Optimization fine-tune language models to evade detection, leading to a significant drop in the performance of existing MGT detectors.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.", 'score': 7, 'issue_id': 4096, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '4c6278bf22171f39', 'authors': ['Andrea Pedrotti', 'Michele Papucci', 'Cristiano Ciaccio', 'Alessio Miaschi', 'Giovanni Puccetti', "Felice Dell'Orletta", 'Andrea Esuli'], 'affiliations': ['Department of Computer Science, University of Pisa', 'Istituto di Scienza Tecnologie dellInformazione A. Faedo (CNR-ISTI)', 'ItaliaNLP Lab, Istituto di Linguistica Computazionale Antonio Zampolli (CNR-ILC)'], 'pdf_title_img': 'assets/pdf/title_img/2505.24523.jpg', 'data': {'categories': ['#data', '#hallucinations', '#benchmark', '#security', '#rlhf', '#alignment'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'ĞĞ±Ğ¼Ğ°Ğ½ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²: ĞºĞ°Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾-ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° (MGT) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Direct Preference Optimization. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² MGT, Ğ´ĞµĞ»Ğ°Ñ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹, Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ MGT. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Fooling the Detectives: Enhancing MGT Stealth with DPO', 'desc': 'This paper discusses how adversarial attacks can be used to improve the stealth of machine-generated text (MGT) by fine-tuning language models through Direct Preference Optimization (DPO). The authors demonstrate that these attacks can significantly reduce the effectiveness of current MGT detectors by altering the style of generated text to resemble human-written content. They also analyze the linguistic features that detectors rely on, revealing vulnerabilities in their detection capabilities. The findings emphasize the need for more robust detection methods to handle the evolving challenges posed by advanced generative AI.'}, 'zh': {'title': 'æå‡æ£€æµ‹å™¨é²æ£’æ€§ï¼ŒæŠµå¾¡å¯¹æŠ—æ€§æ”»å‡»', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¯¹æŠ—æ€§æ”»å‡»å¦‚ä½•åˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä»è€Œä½¿å…¶ç”Ÿæˆçš„æ–‡æœ¬æ›´éš¾è¢«æœºå™¨ç”Ÿæˆæ–‡æœ¬ï¼ˆMGTï¼‰æ£€æµ‹å™¨è¯†åˆ«ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°æœ‰çš„MGTæ£€æµ‹å™¨åœ¨é¢å¯¹ç»è¿‡ä¼˜åŒ–çš„æ–‡æœ¬æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå®¹æ˜“è¢«æ¬ºéª—ã€‚é€šè¿‡åˆ†æè¯­è¨€æ¨¡å‹çš„é£æ ¼è½¬å˜ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ£€æµ‹å™¨ä¾èµ–çš„è¯­è¨€ç‰¹å¾ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ”¹è¿›æ£€æµ‹æ–¹æ³•çš„é‡è¦æ€§ï¼Œä»¥å¢å¼ºå…¶å¯¹æœªçŸ¥æ–‡æœ¬çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01881', 'title': 'WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent\n  Triggerability in Task-Oriented Dialogue', 'url': 'https://huggingface.co/papers/2506.01881', 'abstract': 'STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.', 'score': 6, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'e82ff37de8341d1a', 'authors': ['Yaoyao Qian', 'Jindan Huang', 'Yuanli Wang', 'Simon Yu', 'Kyrie Zhixuan Zhou', 'Jiayuan Mao', 'Mingfu Liang', 'Hanhan Zhou'], 'affiliations': ['Boston University, Boston, MA', 'George Washington University, Washington, DC', 'Massachusetts Institute of Technology, Cambridge, MA', 'Northeastern University, Boston, MA', 'Northwestern University, Evanston, IL', 'Tufts University, Medford, MA', 'University of Texas at San Antonio, San Antonio, TX'], 'pdf_title_img': 'assets/pdf/title_img/2506.01881.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#alignment', '#agents'], 'emoji': 'ğŸŒªï¸', 'ru': {'title': 'STORM: Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº STORM Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. STORM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - UserLLM Ğ¸ AgentLLM - Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ…Ğ¾Ğ´Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ°Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜.'}, 'en': {'title': 'Enhancing Dialogue Systems through Collaborative Intent Formation', 'desc': 'The STORM framework enhances task-oriented dialogue systems by addressing the challenges of asymmetric information between users and AI agents. It recognizes that users often do not fully articulate their needs, leading to difficulties in intent recognition by the system. By modeling the dynamics of information exchange, STORM enables the development of annotated datasets that track how users and agents collaboratively form intents. The research shows that a moderate level of uncertainty can improve performance in certain contexts, suggesting that complete transparency is not always the best approach in human-AI interactions.'}, 'zh': {'title': 'STORMï¼šä¿ƒè¿›äººæœºåä½œçš„æ„å›¾å½¢æˆ', 'desc': 'STORMæ¡†æ¶é€šè¿‡å»ºæ¨¡ç”¨æˆ·å’Œä»£ç†ä¹‹é—´çš„ä¿¡æ¯ä¸å¯¹ç§°åŠ¨æ€ï¼Œä¿ƒè¿›äº†ä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿä¸­çš„åä½œæ„å›¾å½¢æˆã€‚ç”¨æˆ·çš„è¡¨è¾¾è™½ç„¶åœ¨è¯­è¨€ä¸Šå®Œæ•´ï¼Œä½†å¾€å¾€ç¼ºä¹ç³»ç»Ÿæ‰€éœ€çš„ç»“æ„ä¿¡æ¯ï¼Œå¯¼è‡´ç³»ç»Ÿæ— æ³•æ­£ç¡®å“åº”ã€‚STORMæ¡†æ¶èƒ½å¤Ÿæ•æ‰è¡¨è¾¾è½¨è¿¹å’Œæ½œåœ¨çš„è®¤çŸ¥è½¬å˜ï¼Œä»è€Œç³»ç»ŸåŒ–åˆ†æåä½œç†è§£çš„å‘å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé€‚åº¦çš„ä¸ç¡®å®šæ€§ï¼ˆ40-60%ï¼‰å¯ä»¥ä¼˜äºå®Œå…¨é€æ˜çš„ä¿¡æ¯ï¼Œè¿™ä¸ºäººæœºåä½œä¸­çš„ä¿¡æ¯å®Œæ•´æ€§æä¾›äº†æ–°çš„æ€è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00338', 'title': 'OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning', 'url': 'https://huggingface.co/papers/2506.00338', 'abstract': 'The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.  \t\t\t\t\tAI-generated summary \t\t\t\t The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature, which introduces challenges such as incorrect language labels and audio-text misalignments. To address this, we develop a scalable data-cleaning pipeline using public toolkits, yielding a dataset with 166,000 hours of speech across 75 languages. Our new series of OWSM v4 models, trained on this curated dataset alongside existing OWSM data, significantly outperform previous versions on multilingual benchmarks. Our models even match or surpass frontier industrial models like Whisper and MMS in multiple scenarios. We will publicly release the cleaned YODAS data, pre-trained models, and all associated scripts via the ESPnet toolkit.', 'score': 6, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '2f4783eb2db68192', 'authors': ['Yifan Peng', 'Shakeel Muhammad', 'Yui Sudo', 'William Chen', 'Jinchuan Tian', 'Chyi-Jiunn Lin', 'Shinji Watanabe'], 'affiliations': ['Carnegie Mellon University, United States', 'Honda Research Institute Japan, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2506.00338.jpg', 'data': {'categories': ['#training', '#low_resource', '#open_source', '#multilingual', '#data', '#audio', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ²', 'desc': 'ĞŸÑ€Ğ¾ĞµĞºÑ‚ OWSM ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ» Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 166 000 Ñ‡Ğ°ÑĞ°Ğ¼Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° 75 ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ¾Ğ²Ğ°Ñ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ OWSM v4, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Whisper Ğ¸ MMS, Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Multilingual Speech Models with Cleaned Web Data', 'desc': 'The OWSM project has improved its multilingual speech models by integrating a large-scale, cleaned web dataset called YODAS. This dataset, which contains 166,000 hours of speech in 75 languages, was challenging to incorporate due to issues like incorrect language labels and audio-text misalignments. To tackle these challenges, a scalable data-cleaning pipeline was developed, resulting in a high-quality dataset for training. The new OWSM v4 models, trained on this curated dataset, now perform comparably to leading industrial models, showcasing significant advancements in multilingual speech recognition.'}, 'zh': {'title': 'æå‡å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹çš„å¼€åˆ›æ€§è¿›å±•', 'desc': 'OWSMé¡¹ç›®é€šè¿‡æ•´åˆä¸€ä¸ªå¤§å‹æ¸…æ´—è¿‡çš„ç½‘ç»œæ•°æ®é›†YODASï¼Œæå‡äº†å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹çš„æ€§èƒ½ã€‚YODASæ•°æ®é›†åŒ…å«äº†å¤§é‡çš„è¯­éŸ³æ•°æ®ï¼Œä½†ç”±äºå…¶åŸå§‹ç‰¹æ€§ï¼Œå­˜åœ¨è¯­è¨€æ ‡ç­¾é”™è¯¯å’ŒéŸ³é¢‘æ–‡æœ¬ä¸å¯¹é½ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®æ¸…æ´—æµç¨‹ï¼Œæœ€ç»ˆç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«75ç§è¯­è¨€ã€166,000å°æ—¶è¯­éŸ³çš„æ•°æ®é›†ã€‚æ–°çš„OWSM v4æ¨¡å‹åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³åœ¨å¤šä¸ªåœºæ™¯ä¸­ä¸é¢†å…ˆçš„å·¥ä¸šæ¨¡å‹ç›¸åª²ç¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24842', 'title': 'Cascading Adversarial Bias from Injection to Distillation in Language\n  Models', 'url': 'https://huggingface.co/papers/2505.24842', 'abstract': 'Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injection of biased content during training. We demonstrate that adversaries can inject subtle biases into teacher models through minimal data poisoning, which propagates to student models and becomes significantly amplified. We propose two propagation modes: Untargeted Propagation, where bias affects multiple tasks, and Targeted Propagation, focusing on specific tasks while maintaining normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning rate), student models generate biased responses 76.9% of the time in targeted scenarios - higher than 69.4% in teacher models. For untargeted propagation, adversarial bias appears 6x-29x more frequently in student models on unseen tasks. We validate findings across six bias types (targeted advertisements, phishing links, narrative manipulations, insecure coding practices), various distillation methods, and different modalities spanning text and code generation. Our evaluation reveals shortcomings in current defenses - perplexity filtering, bias detection systems, and LLM-based autorater frameworks - against these attacks. Results expose significant security vulnerabilities in distilled models, highlighting need for specialized safeguards. We propose practical design principles for building effective adversarial bias mitigation strategies.', 'score': 6, 'issue_id': 4092, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '15a12805380711b7', 'authors': ['Harsh Chaudhari', 'Jamie Hayes', 'Matthew Jagielski', 'Ilia Shumailov', 'Milad Nasr', 'Alina Oprea'], 'affiliations': ['Google DeepMind', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24842.jpg', 'data': {'categories': ['#security', '#ethics', '#training', '#inference', '#data', '#dataset'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ°: ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸: Ğ½ĞµÑ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğ¹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ€ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Strengthening Distilled Models Against Adversarial Bias Injection', 'desc': 'This paper explores the vulnerabilities of distilled language models to adversarial attacks, specifically through the injection of biased content during their training phase. It shows that adversaries can subtly poison teacher models with minimal data, which then amplifies biases in the student models that are derived from them. The study identifies two modes of bias propagation: Untargeted, affecting multiple tasks, and Targeted, which focuses on specific tasks while keeping normal behavior intact. The findings reveal that current defenses are inadequate, emphasizing the need for improved strategies to safeguard against these security threats in distilled models.'}, 'zh': {'title': 'ä¿æŠ¤è’¸é¦æ¨¡å‹ï¼ŒæŠµå¾¡å¯¹æŠ—æ€§åè§æ”»å‡»ï¼', 'desc': 'æ¨¡å‹è’¸é¦åœ¨åˆ›å»ºå°å‹å¯éƒ¨ç½²è¯­è¨€æ¨¡å‹ä¸­å˜å¾—è‡³å…³é‡è¦ï¼Œè¿™äº›æ¨¡å‹ä¿ç•™äº†æ›´å¤§ç³»ç»Ÿçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¹¿æ³›éƒ¨ç½²å¼•å‘äº†å¯¹æŠ—æ€§æ“æ§çš„è„†å¼±æ€§é—®é¢˜ã€‚æœ¬æ–‡ç ”ç©¶äº†è’¸é¦æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹åè§å†…å®¹çš„å¯¹æŠ—æ€§æ³¨å…¥çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§ä¼ æ’­æ¨¡å¼ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æœ€å°çš„æ•°æ®æ±¡æŸ“ä½¿æ•™å¸ˆæ¨¡å‹æ³¨å…¥å¾®å¦™çš„åè§ï¼Œè¿™äº›åè§åœ¨å­¦ç”Ÿæ¨¡å‹ä¸­è¢«æ˜¾è‘—æ”¾å¤§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24625', 'title': 'Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors', 'url': 'https://huggingface.co/papers/2505.24625', 'abstract': "A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.", 'score': 6, 'issue_id': 4087, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '8bfa132788ee6990', 'authors': ['Duo Zheng', 'Shijia Huang', 'Yanyang Li', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.24625.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#video', '#games', '#architecture', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸: Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Video-3D Geometry Large Language Model (VG LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², VG LLM Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸Ğ»Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ´Ğ¾Ğ¼ ÑĞ²ĞµÑ€Ñ…Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ 3D-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ¹ 3D-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VG LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding from Video Alone!', 'desc': 'The paper introduces the Video-3D Geometry Large Language Model (VG LLM), which enhances 3D scene understanding by extracting 3D information directly from video sequences. Unlike previous methods that require extensive 3D data inputs, VG LLM operates solely on video data, making it more efficient. It utilizes a 3D visual geometry encoder to gather 3D prior information, which is then combined with visual tokens for processing in a Multimodal Large Language Model. The results demonstrate that VG LLM achieves competitive performance in 3D tasks, outperforming existing models without the need for additional 3D data.'}, 'zh': {'title': 'è§†é¢‘é©±åŠ¨çš„3Dç†è§£æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘-3Då‡ ä½•å¤§è¯­è¨€æ¨¡å‹ï¼ˆVG LLMï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥ä»è§†é¢‘åºåˆ—ä¸­æå–3Dä¿¡æ¯ï¼Œä»è€Œå¢å¼º3Dåœºæ™¯ç†è§£ï¼Œè€Œæ— éœ€é¢å¤–çš„3Dæ•°æ®ã€‚è¯¥æ¨¡å‹åˆ©ç”¨3Dè§†è§‰å‡ ä½•ç¼–ç å™¨ï¼Œä»è§†é¢‘ä¸­æå–3Då…ˆéªŒä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸è§†è§‰æ ‡è®°ç»“åˆï¼Œè¾“å…¥åˆ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„4Bæ¨¡å‹åœ¨ä¸ä¾èµ–æ˜¾å¼3Dæ•°æ®è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸åª²ç¾çš„ç»“æœï¼Œç”šè‡³åœ¨VSI-Benchè¯„ä¼°ä¸­è¶…è¶Šäº†Gemini-1.5-Proã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24183', 'title': 'CodeV-R1: Reasoning-Enhanced Verilog Generation', 'url': 'https://huggingface.co/papers/2505.24183', 'abstract': 'CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage "distill-then-RL" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.', 'score': 6, 'issue_id': 4093, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': 'b542a58b96860ad6', 'authors': ['Yaoyu Zhu', 'Di Huang', 'Hanqi Lyu', 'Xiaoyun Zhang', 'Chongxiao Li', 'Wenxuan Shi', 'Yutong Wu', 'Jianan Mu', 'Jinghua Wang', 'Yang Zhao', 'Pengwei Jin', 'Shuyao Cheng', 'Shengwen Liang', 'Xishan Zhang', 'Rui Zhang', 'Zidong Du', 'Qi Guo', 'Xing Hu', 'Yunji Chen'], 'affiliations': ['Cambricon Technologies', 'SKL of Processors, Institute of Computing Technology, CAS', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.24183.jpg', 'data': {'categories': ['#games', '#rl', '#dataset', '#optimization', '#open_source', '#training'], 'emoji': 'ğŸ”§', 'ru': {'title': 'CodeV-R1: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ¸ĞºĞ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° CodeV-R1 Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Verilog Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° RLVR. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ "ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº - ĞºĞ¾Ğ´" Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. CodeV-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ CodeV-R1-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Revolutionizing Verilog Generation with CodeV-R1', 'desc': 'The paper presents CodeV-R1, a reinforcement learning with verifiable reward (RLVR) framework designed for generating Verilog code from natural language specifications. It addresses challenges in electronic design automation (EDA) by introducing a rule-based testbench for equivalence checking and a round-trip data synthesis method to create a high-quality dataset of NL-code pairs. The training process utilizes a two-stage approach, combining knowledge distillation with an adaptive RLVR algorithm to optimize training efficiency. CodeV-R1 demonstrates significant improvements in performance metrics, surpassing previous state-of-the-art models in Verilog generation tasks.'}, 'zh': {'title': 'CodeV-R1ï¼šç”µå­è®¾è®¡è‡ªåŠ¨åŒ–çš„å¼ºåŒ–å­¦ä¹ æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†CodeV-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºVerilogç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–ï¼ˆEDAï¼‰ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼€å‘åŸºäºè§„åˆ™çš„æµ‹è¯•å¹³å°ç”Ÿæˆå™¨å’Œå›åˆæ•°æ®åˆæˆæ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»£ç ä¸è‡ªç„¶è¯­è¨€æè¿°ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆè¿›è¡ŒçŸ¥è¯†è’¸é¦ä»¥æå‡æ¨ç†èƒ½åŠ›ï¼Œç„¶åä½¿ç”¨è‡ªé€‚åº”çš„RLVRç®—æ³•é™ä½è®­ç»ƒæˆæœ¬ã€‚æœ€ç»ˆï¼ŒCodeV-R1-7Bæ¨¡å‹åœ¨VerilogEval v2å’ŒRTLLM v1.1ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21179', 'title': 'Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model', 'url': 'https://huggingface.co/papers/2505.21179', 'abstract': 'Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Negative guidance -- explicitly suppressing unwanted attributes -- remains a fundamental challenge in diffusion models, particularly in few-step sampling regimes. While Classifier-Free Guidance (CFG) works well in standard settings, it fails under aggressive sampling step compression due to divergent predictions between positive and negative branches. We present Normalized Attention Guidance (NAG), an efficient, training-free mechanism that applies extrapolation in attention space with L1-based normalization and refinement. NAG restores effective negative guidance where CFG collapses while maintaining fidelity. Unlike existing approaches, NAG generalizes across architectures (UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image, video), functioning as a universal plug-in with minimal computational overhead. Through extensive experimentation, we demonstrate consistent improvements in text alignment (CLIP Score), fidelity (FID, PFID), and human-perceived quality (ImageReward). Our ablation studies validate each design component, while user studies confirm significant preference for NAG-guided outputs. As a model-agnostic inference-time approach requiring no retraining, NAG provides effortless negative guidance for all modern diffusion frameworks -- pseudocode in the Appendix!', 'score': 6, 'issue_id': 4095, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '3e7694e3e9f014f5', 'authors': ['Dar-Yen Chen', 'Hmrishav Bandyopadhyay', 'Kai Zou', 'Yi-Zhe Song'], 'affiliations': ['NetMind.AI', 'SketchX, CVSSP, University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2505.21179.jpg', 'data': {'categories': ['#diffusion', '#inference', '#cv', '#optimization', '#video'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'NAG: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Normalized Attention Guidance (NAG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. NAG Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², NAG Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼.'}, 'en': {'title': 'Effortless Negative Guidance for Diffusion Models with NAG', 'desc': 'Normalized Attention Guidance (NAG) is a novel method that improves diffusion models by providing effective negative guidance without the need for retraining. It addresses the challenge of suppressing unwanted attributes, especially in scenarios with few sampling steps where traditional methods like Classifier-Free Guidance (CFG) struggle. NAG utilizes an efficient mechanism that normalizes attention using L1-based techniques, allowing it to maintain high fidelity while enhancing negative guidance. This approach is versatile, working across different architectures, sampling regimes, and modalities, making it a universal solution for modern diffusion frameworks.'}, 'zh': {'title': 'å½’ä¸€åŒ–æ³¨æ„åŠ›å¼•å¯¼ï¼šæ— ç¼è´Ÿå¼•å¯¼çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'å½’ä¸€åŒ–æ³¨æ„åŠ›å¼•å¯¼ï¼ˆNAGï¼‰é€šè¿‡åœ¨ä¸åŒçš„é‡‡æ ·é˜¶æ®µå’Œæ¨¡æ€ä¸­æä¾›æœ‰æ•ˆçš„è´Ÿå¼•å¯¼ï¼Œå¢å¼ºäº†æ‰©æ•£æ¨¡å‹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è´Ÿå¼•å¯¼çš„æŒ‘æˆ˜åœ¨äºåœ¨å°‘æ­¥é‡‡æ ·ä¸­æ˜¾å¾—å°¤ä¸ºçªå‡ºï¼Œä¼ ç»Ÿçš„æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰åœ¨æ¿€è¿›çš„é‡‡æ ·æ­¥éª¤å‹ç¼©ä¸‹è¡¨ç°ä¸ä½³ã€‚NAGé‡‡ç”¨åŸºäºL1çš„å½’ä¸€åŒ–å’Œç²¾ç‚¼æ–¹æ³•ï¼Œåœ¨æ³¨æ„åŠ›ç©ºé—´ä¸­è¿›è¡Œå¤–æ¨ï¼Œæ¢å¤äº†æœ‰æ•ˆçš„è´Ÿå¼•å¯¼ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†NAGåœ¨æ–‡æœ¬å¯¹é½ã€ä¿çœŸåº¦å’Œäººç±»æ„ŸçŸ¥è´¨é‡æ–¹é¢çš„ä¸€è‡´æ€§æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01084', 'title': 'zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression', 'url': 'https://huggingface.co/papers/2506.01084', 'abstract': 'A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers\' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable "hypertokens" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\\%, with significant improvements in inference latency.', 'score': 5, 'issue_id': 4093, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': 'f9927f51990f811a', 'authors': ['Saibo Geng', 'Nathan Ranchin', 'Yunzhen yao', 'Maxime Peyrard', 'Chris Wendler', 'Michael Gastpar', 'Robert West'], 'affiliations': ['EPFL', 'Microsoft', 'Northeastern University', 'UniversitÃ© Grenoble Alpes, CNRS, Grenoble INP, LIG'], 'pdf_title_img': 'assets/pdf/title_img/2506.01084.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° zip2zip, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ LLM, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ LZW, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° LZW, ÑĞ»Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ zip2zip Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° 20-60% Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ.'}, 'en': {'title': 'Dynamic Tokenization for Faster Inference in LLMs', 'desc': "The paper presents zip2zip, a novel framework that enhances the efficiency of large language models (LLMs) by dynamically adjusting their token vocabulary during inference. By utilizing Lempel-Ziv-Welch (LZW) compression, zip2zip reduces the length of token sequences, which leads to faster inference speeds. The framework includes a tokenizer that creates reusable 'hypertokens', an embedding layer for these hypertokens, and a causal language model that operates on compressed sequences. The results demonstrate that zip2zip can significantly decrease input and output lengths by 20-60%, improving overall model performance and reducing computational costs."}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´ä»¤ç‰Œï¼Œæå‡æ¨ç†é€Ÿåº¦', 'desc': 'zip2zipæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒåœ¨æ¨ç†æ—¶åŠ¨æ€è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»¤ç‰Œè¯æ±‡ï¼Œä½¿ç”¨LZWå‹ç¼©æŠ€æœ¯æ¥å‡å°‘ä»¤ç‰Œåºåˆ—çš„é•¿åº¦ï¼Œä»è€Œæé«˜æ¨ç†é€Ÿåº¦ã€‚ä¼ ç»Ÿçš„ä»¤ç‰ŒåŒ–æ–¹æ³•é€šå¸¸ä¾èµ–äºé™æ€çš„ä»¤ç‰Œå™¨ï¼Œè¿™äº›ä»¤ç‰Œå™¨çš„å›ºå®šè¯æ±‡æ— æ³•é€‚åº”ç‰¹å®šé¢†åŸŸæˆ–è¯­è¨€çš„è¾“å…¥ï¼Œå¯¼è‡´ç”Ÿæˆæ›´é•¿çš„ä»¤ç‰Œåºåˆ—å’Œæ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚zip2zipé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å®ç°å…¶åŠŸèƒ½ï¼šåŸºäºLZWå‹ç¼©çš„ä»¤ç‰Œå™¨ã€å®æ—¶è®¡ç®—æ–°å½¢æˆçš„è¶…ä»¤ç‰Œçš„åµŒå…¥å±‚ï¼Œä»¥åŠè®­ç»ƒæ¨¡å‹å¤„ç†å‹ç¼©åºåˆ—çš„å› æœè¯­è¨€å»ºæ¨¡å˜ä½“ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡zip2zipå¤„ç†çš„LLMåœ¨æ¨ç†æ—¶èƒ½å¤Ÿæœ‰æ•ˆä½¿ç”¨è¶…ä»¤ç‰Œï¼Œè¾“å…¥å’Œè¾“å‡ºåºåˆ—é•¿åº¦å‡å°‘20-60%ï¼Œæ¨ç†å»¶è¿Ÿæ˜¾è‘—é™ä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00512', 'title': 'Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing', 'url': 'https://huggingface.co/papers/2506.00512', 'abstract': 'A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided 3D editing aims to precisely edit semantically relevant local 3D regions, which has significant potential for various practical applications ranging from 3D games to film production. Existing methods typically follow a view-indiscriminate paradigm: editing 2D views indiscriminately and projecting them back into 3D space. However, they overlook the different cross-view interdependencies, resulting in inconsistent multi-view editing. In this study, we argue that ideal consistent 3D editing can be achieved through a progressive-views paradigm, which propagates editing semantics from the editing-salient view to other editing-sparse views. Specifically, we propose Pro3D-Editor, a novel framework, which mainly includes Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view Sampler dynamically samples and edits the most editing-salient view as the primary view. Key-view Render accurately propagates editing semantics from the primary view to other key views through its Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based on the edited multi-views. Extensive experiments demonstrate that our method outperforms existing methods in editing accuracy and spatial consistency.', 'score': 5, 'issue_id': 4094, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '0a9ce5d9ebc76a52', 'authors': ['Yang Zheng', 'Mengqi Huang', 'Nan Chen', 'Zhendong Mao'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00512.jpg', 'data': {'categories': ['#games', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ñ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğº Ğ¼ĞµĞ½ĞµĞµ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Pro3D-Editor Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Achieving Consistent 3D Editing with Pro3D-Editor', 'desc': 'This paper introduces a new approach for 3D editing called Pro3D-Editor, which focuses on maintaining consistency across different views of a 3D object. Unlike traditional methods that treat all views equally, this framework uses a progressive-views paradigm to propagate editing information from the most important view to others. It consists of three main components: a Primary-view Sampler that identifies and edits the most relevant view, a Key-view Render that transfers the editing semantics to other views, and a Full-view Refiner that finalizes the 3D object based on the edited views. The results show that Pro3D-Editor achieves better accuracy and consistency compared to existing 3D editing techniques.'}, 'zh': {'title': 'æ¸è¿›è§†å›¾èŒƒå¼å®ç°ä¸€è‡´çš„3Dç¼–è¾‘', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¸è¿›è§†å›¾èŒƒå¼ï¼Œé€šè¿‡Pro3D-Editorå®ç°ä¸€è‡´çš„3Dç¼–è¾‘ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»å…³é”®è§†å›¾å‘è¾ƒå°‘ç¼–è¾‘çš„è§†å›¾ä¼ æ’­è¯­ä¹‰ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤šè§†å›¾ç¼–è¾‘ä¸­å­˜åœ¨çš„ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚Pro3D-Editoræ¡†æ¶åŒ…æ‹¬ä¸»è¦è§†å›¾é‡‡æ ·å™¨ã€å…³é”®è§†å›¾æ¸²æŸ“å’Œå…¨è§†å›¾ç²¾ç‚¼å™¨ï¼Œèƒ½å¤ŸåŠ¨æ€é€‰æ‹©æœ€é‡è¦çš„è§†å›¾è¿›è¡Œç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¼–è¾‘ç²¾åº¦å’Œç©ºé—´ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24452', 'title': 'Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training', 'url': 'https://huggingface.co/papers/2505.24452', 'abstract': 'A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets.While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations.In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient.In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets.First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations.From this framework, we derive the UBA schedule, controlled by a single hyper-parameter varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of varphi.We offer practical guidelines for its selection via theoretical analysis and empirical results.xtensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.', 'score': 5, 'issue_id': 4091, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '82972c2646341cc9', 'authors': ['Anda Tang', 'Yiming Dong', 'Yutao Zeng', 'zhou Xun', 'Zhouchen Lin'], 'affiliations': ['ByteDance Seed', 'Institute for Artificial Intelligence, Peking University', 'Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China', 'State Key Lab of General AI, School of Intelligence Science and Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24452.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Unified Budget-Aware (UBA), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. UBA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼ Ï†, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹.'}, 'en': {'title': 'Optimizing Training with Unified Budget-Aware Learning Rates', 'desc': 'This paper introduces a new learning rate schedule called the Unified Budget-Aware (UBA) schedule, designed to optimize training when there are limits on the number of iterations. Traditional learning rate schedules often rely on trial-and-error and lack a solid theoretical basis, making them inefficient. The UBA schedule is grounded in a novel optimization framework that considers the curvature of the loss landscape, allowing it to adapt better to various tasks and network architectures. Experimental results demonstrate that UBA outperforms standard schedules across different vision and language tasks, providing a more effective training strategy within constrained budgets.'}, 'zh': {'title': 'ç»Ÿä¸€é¢„ç®—æ„ŸçŸ¥å­¦ä¹ ç‡è°ƒåº¦ï¼Œä¼˜åŒ–æœ‰é™è®­ç»ƒé¢„ç®—', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„é¢„ç®—æ„ŸçŸ¥å­¦ä¹ ç‡è°ƒåº¦ï¼ˆUBAï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–åœ¨æœ‰é™è¿­ä»£é¢„ç®—ä¸‹çš„è®­ç»ƒæ•ˆæœã€‚ä¼ ç»Ÿçš„å­¦ä¹ ç‡è°ƒåº¦æ–¹æ³•å¾€å¾€ä¾èµ–ç»éªŒï¼Œç¼ºä¹ç†è®ºåŸºç¡€ï¼Œè€ŒUBAåˆ™é€šè¿‡æ„å»ºä¸€ä¸ªæ–°çš„ä¼˜åŒ–æ¡†æ¶ï¼Œè€ƒè™‘äº†å¯¹æŸå¤±å‡½æ•°æ›²ç‡å˜åŒ–çš„é²æ£’æ€§ã€‚è¯¥è°ƒåº¦ç”±ä¸€ä¸ªè¶…å‚æ•°æ§åˆ¶ï¼Œèƒ½å¤Ÿåœ¨çµæ´»æ€§å’Œç®€å•æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œé¿å…äº†å¯¹æ¯ä¸ªç½‘ç»œè¿›è¡Œæ•°å€¼ä¼˜åŒ–çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUBAåœ¨å¤šç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸­ï¼Œå‡ä¼˜äºå¸¸ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.23504', 'title': 'VAU-R1: Advancing Video Anomaly Understanding via Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.23504', 'abstract': 'VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.', 'score': 5, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': 'c243189c9ec32d1f', 'authors': ['Liyun Zhu', 'Qixiang Chen', 'Xi Shen', 'Xiaodong Cun'], 'affiliations': ['Australian National University', 'GVC Lab, Great Bay University', 'Intellindust AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.23504.jpg', 'data': {'categories': ['#rl', '#interpretability', '#multimodal', '#reasoning', '#video', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ: Ğ˜Ğ˜ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸', 'desc': 'VAU-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ VAU-Bench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ± Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸ÑÑ… Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° VAU-R1 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Video Anomaly Reasoning with VAU-R1 and VAU-Bench', 'desc': 'The paper introduces VAU-R1, a framework that uses Multimodal Large Language Models (MLLMs) and Reinforcement Fine-Tuning (RFT) to improve the understanding of video anomalies. It addresses the challenges of fine-grained spatio-temporal perception and the need for robust reasoning in ambiguous situations. Additionally, the authors present VAU-Bench, a new benchmark designed to evaluate reasoning capabilities in video anomaly scenarios through multiple-choice questions and detailed rationales. The results demonstrate that VAU-R1 enhances accuracy in question answering and improves the coherence of reasoning across various contexts.'}, 'zh': {'title': 'æå‡è§†é¢‘å¼‚å¸¸æ¨ç†çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'VAU-R1 æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†é¢‘å¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆReinforcement Fine-Tuningï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº† VAU-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè§†é¢‘å¼‚å¸¸æ¨ç†çš„é“¾å¼æ€ç»´åŸºå‡†ï¼ŒåŒ…å«å¤šé¡¹é€‰æ‹©é—®ç­”ã€è¯¦ç»†æ¨ç†ã€æ—¶é—´æ ‡æ³¨å’Œæè¿°æ€§æ ‡é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVAU-R1 åœ¨é—®ç­”å‡†ç¡®æ€§ã€æ—¶é—´å®šä½å’Œæ¨ç†è¿è´¯æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01484', 'title': 'LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification', 'url': 'https://huggingface.co/papers/2506.01484', 'abstract': 'A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.  \t\t\t\t\tAI-generated summary \t\t\t\t Detoxification, the task of rewriting harmful language into non-toxic text, has become increasingly important amid the growing prevalence of toxic content online. However, high-quality parallel datasets for detoxification, especially for hate speech, remain scarce due to the cost and sensitivity of human annotation. In this paper, we propose a novel LLM-in-the-loop pipeline leveraging GPT-4o-mini for automated detoxification. We first replicate the ParaDetox pipeline by replacing human annotators with an LLM and show that the LLM performs comparably to human annotation. Building on this, we construct PARADEHATE, a large-scale parallel dataset specifically for hatespeech detoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate text pairs and evaluate a wide range of baseline methods. Experimental results show that models such as BART, fine-tuned on PARADEHATE, achieve better performance in style accuracy, content preservation, and fluency, demonstrating the effectiveness of LLM-generated detoxification text as a scalable alternative to human annotation.', 'score': 4, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '422c267bbe9577db', 'authors': ['Shuzhou Yuan', 'Ercong Nie', 'Lukas Kouba', 'Ashish Yashwanth Kangen', 'Helmut Schmid', 'Hinrich Schutze', 'Michael Farber'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning (MCML)', 'ScaDS.AI and TU Dresden'], 'pdf_title_img': 'assets/pdf/title_img/2506.01484.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#dataset', '#data', '#open_source'], 'emoji': 'ğŸ§¼', 'ru': {'title': 'Ğ˜Ğ˜ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ° Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GPT-4o-mini. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PARADEHATE, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 8000 Ğ¿Ğ°Ñ€ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ´ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Automating Hate Speech Detoxification with GPT-4o-mini', 'desc': "This paper introduces a new method for creating a large dataset aimed at detoxifying hate speech using the GPT-4o-mini model. Detoxification involves rewriting harmful language into non-toxic text, which is crucial due to the rise of toxic content online. The authors developed a pipeline that automates this process, replacing human annotators with a language model, and found that the model's performance is comparable to that of humans. They also created a dataset called PARADEHATE, consisting of over 8,000 pairs of hate and non-hate text, which significantly improves the performance of various models in terms of style accuracy, content preservation, and fluency."}, 'zh': {'title': 'åˆ©ç”¨GPT-4o-miniç”Ÿæˆä»‡æ¨è¨€è®ºå»æ¯’åŒ–æ•°æ®é›†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œåˆ©ç”¨GPT-4o-miniç”Ÿæˆå¤§è§„æ¨¡çš„ä»‡æ¨è¨€è®ºå»æ¯’åŒ–æ•°æ®é›†ï¼Œä»è€Œæé«˜åŸºçº¿æ¨¡å‹åœ¨é£æ ¼å‡†ç¡®æ€§ã€å†…å®¹ä¿ç•™å’Œæµç•…æ€§æ–¹é¢çš„è¡¨ç°ã€‚å»æ¯’åŒ–æ˜¯å°†æœ‰å®³è¯­è¨€é‡å†™ä¸ºéæœ‰å®³æ–‡æœ¬çš„ä»»åŠ¡ï¼Œéšç€ç½‘ç»œä¸Šæœ‰æ¯’å†…å®¹çš„å¢åŠ ï¼Œè¿™ä¸€ä»»åŠ¡å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç”±äºäººå·¥æ ‡æ³¨çš„æˆæœ¬å’Œæ•æ„Ÿæ€§ï¼Œé«˜è´¨é‡çš„å»æ¯’åŒ–å¹³è¡Œæ•°æ®é›†ä»ç„¶ç¨€ç¼ºã€‚æˆ‘ä»¬æ„å»ºäº†PARADEHATEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºä»‡æ¨è¨€è®ºå»æ¯’åŒ–çš„å¤§è§„æ¨¡å¹³è¡Œæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†åŸºäºè¯¥æ•°æ®é›†çš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00643', 'title': 'SATA-BENCH: Select All That Apply Benchmark for Multiple Choice\n  Questions', 'url': 'https://huggingface.co/papers/2506.00643', 'abstract': "SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.", 'score': 4, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': 'f95c367c9eaf00a9', 'authors': ['Weijie Xu', 'Shixian Cui', 'Xi Fang', 'Chi Xue', 'Stephanie Eckman', 'Chandan Reddy'], 'affiliations': ['Amazon'], 'pdf_title_img': 'assets/pdf/title_img/2506.00643.jpg', 'data': {'categories': ['#training', '#benchmark', '#open_source', '#interpretability', '#data', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SATA-BENCH - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ñƒ ÑĞ°Ğ¼Ñ‹Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ñ… Ğ»Ğ¸ÑˆÑŒ 41.8% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Choice Funnel, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ¼.'}, 'en': {'title': 'Enhancing Multi-Answer Reasoning with SATA-BENCH and Choice Funnel', 'desc': 'The paper introduces SATA-BENCH, a benchmark designed to evaluate large language models (LLMs) on multi-answer questions, specifically Select All That Apply (SATA) tasks. It highlights significant performance gaps in current LLMs, with the best model achieving only 41.8% exact match in identifying all correct answers. The authors identify two main issues: selection bias, where models favor certain answers, and count bias, where they struggle to predict the correct number of answers. To mitigate these challenges, they propose a new decoding strategy called Choice Funnel, which enhances accuracy and reduces costs in multi-answer reasoning tasks.'}, 'zh': {'title': 'æå‡å¤šç­”æ¡ˆæ¨ç†çš„å‡†ç¡®æ€§ä¸æ•ˆç‡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†SATA-BENCHï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç­”æ¡ˆé—®é¢˜ä¸Šçš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨é€‰æ‹©æ‰€æœ‰æ­£ç¡®ç­”æ¡ˆæ—¶å­˜åœ¨æ˜¾è‘—çš„é€‰æ‹©åå·®å’Œè®¡æ•°åå·®ï¼Œå¯¼è‡´å‡†ç¡®ç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†Choice Funnelè§£ç ç­–ç•¥ï¼Œé€šè¿‡å»åå’Œè‡ªé€‚åº”é˜ˆå€¼å¼•å¯¼æ¨¡å‹åšå‡ºæ›´å®Œæ•´å’Œå‡†ç¡®çš„é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChoice Funnelåœ¨å‡†ç¡®åŒ¹é…ç‡ä¸Šæ¯”ç«äº‰åŸºçº¿æé«˜äº†29%ï¼ŒåŒæ—¶é™ä½äº†æ¨ç†æˆæœ¬è¶…è¿‡64%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24086', 'title': 'ComposeAnything: Composite Object Priors for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2505.24086', 'abstract': 'ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating images from text involving complex and novel object arrangements remains a significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, a novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text, consisting of 2D object bounding boxes enriched with depth information and detailed captions. Based on this layout, we generate a spatial and depth aware coarse composite of objects that captures the intended composition, serving as a strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text.', 'score': 4, 'issue_id': 4095, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '2bd92a7129e6945b', 'authors': ['Zeeshan Khan', 'Shizhe Chen', 'Cordelia Schmid'], 'affiliations': ['Inria, Ã‰cole normale supÃ©rieure, CNRS, PSL Research University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24086.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#diffusion', '#interpretability', '#cv', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2.5D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ²', 'desc': 'ComposeAnything - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 2.5D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… 2D Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼Ğ°ĞºĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ¼, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ComposeAnything Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… T2I-CompBench Ğ¸ NSR-1K Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ 2D/3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑÑÑ€Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'ComposeAnything: Elevating Text-to-Image Generation with 2.5D Layouts', 'desc': 'ComposeAnything is a framework that enhances text-to-image generation by utilizing large language models (LLMs) to create 2.5D semantic layouts. This method improves the arrangement of objects in images by incorporating depth information, which helps maintain coherence and quality in the generated images. Unlike previous models that rely solely on 2D layouts, ComposeAnything provides a more accurate representation of spatial relationships, allowing for better object placement. The framework has shown superior performance on benchmark tests, producing high-quality images that align closely with the provided text descriptions.'}, 'zh': {'title': 'ComposeAnythingï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'ComposeAnything æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œç”ŸæˆåŒ…å«æ·±åº¦ä¿¡æ¯çš„2.5Dè¯­ä¹‰å¸ƒå±€ï¼Œä»è€Œå¢å¼ºå¯¹è±¡çš„æ”¾ç½®å’Œä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é‡æ–°è®­ç»ƒç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡ç”Ÿæˆç©ºé—´å’Œæ·±åº¦æ„ŸçŸ¥çš„ç²—ç•¥åˆæˆå›¾åƒï¼Œæ¥æŒ‡å¯¼å»å™ªè¿‡ç¨‹ã€‚ComposeAnything åœ¨å¤„ç†å¤æ‚çš„2D/3Dç©ºé—´å¸ƒå±€å’Œè¶…ç°å®ç»„åˆæ—¶ï¼Œè¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22954', 'title': 'Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents', 'url': 'https://huggingface.co/papers/2505.22954', 'abstract': 'The Darwin G\\"odel Machine improves its coding capabilities through iterative self-modification and open-ended exploration, surpassing other approaches in benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Today\'s AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\\"odel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.', 'score': 4, 'issue_id': 4104, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'}, 'hash': '014c0d439b8212f8', 'authors': ['Jenny Zhang', 'Shengran Hu', 'Cong Lu', 'Robert Lange', 'Jeff Clune'], 'affiliations': ['Canada CIFAR AI Chair', 'Sakana AI', 'University of British Columbia', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.22954.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization', '#agi', '#agents', '#open_source', '#architecture'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ˜Ğ˜: ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Darwin GÃ¶del Machine (DGM) - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. DGM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ñ€Ğ²Ğ¸Ğ½Ğ¾Ğ²ÑĞºĞ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¸Ñ… Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° ÑĞ²Ğ¾Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SWE-bench Ğ¸ Polyglot. DGM Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ Ğ˜Ğ˜.'}, 'en': {'title': 'Evolving AI: The Future of Self-Improvement', 'desc': 'The Darwin G"odel Machine (DGM) is a self-improving AI system that enhances its coding abilities through iterative self-modification and open-ended exploration. Unlike traditional AI, which relies on fixed architectures, the DGM autonomously evolves by modifying its own code and validating these changes against coding benchmarks. It employs a Darwinian approach, maintaining an archive of coding agents and generating new versions to explore diverse solutions. This method has shown significant performance improvements in coding tasks, demonstrating the potential for continuous and safe AI advancement.'}, 'zh': {'title': 'è‡ªæˆ‘æ”¹è¿›çš„AIï¼šè¾¾å°”æ–‡å“¥å¾·å°”æœºå™¨çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'è¾¾å°”æ–‡å“¥å¾·å°”æœºå™¨ï¼ˆDGMï¼‰é€šè¿‡è¿­ä»£è‡ªæˆ‘ä¿®æ”¹å’Œå¼€æ”¾å¼æ¢ç´¢æ¥æé«˜å…¶ç¼–ç èƒ½åŠ›ï¼Œè¶…è¶Šäº†å…¶ä»–æ–¹æ³•çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šæ¶æ„AIç³»ç»Ÿä¸åŒï¼ŒDGMèƒ½å¤Ÿè‡ªä¸»ä¸”æŒç»­åœ°æ”¹è¿›è‡ªèº«ã€‚å®ƒå€Ÿé‰´äº†è¾¾å°”æ–‡è¿›åŒ–çš„ç†å¿µï¼Œç»´æŠ¤ä¸€ä¸ªç”Ÿæˆç¼–ç ä»£ç†çš„æ¡£æ¡ˆåº“ï¼Œå¹¶é€šè¿‡é‡‡æ ·å’ŒåŸºç¡€æ¨¡å‹ç”Ÿæˆæ–°ç‰ˆæœ¬ï¼Œå½¢æˆå¤šæ ·åŒ–çš„é«˜è´¨é‡ä»£ç†æ ‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGMåœ¨ç¼–ç èƒ½åŠ›ä¸Šæ˜¾è‘—æå‡ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„ä»£ç ç¼–è¾‘å·¥å…·å’ŒåŒè¡Œè¯„å®¡æœºåˆ¶ï¼Œæ ‡å¿—ç€è‡ªæˆ‘æ”¹è¿›AIçš„é‡è¦è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01928', 'title': 'Esoteric Language Models', 'url': 'https://huggingface.co/papers/2506.01928', 'abstract': 'Eso-LMs, a novel fusion of autoregressive and masked diffusion models, introduce KV caching to MDMs, achieving faster inference and superior performance on language modeling benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features--most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the **first to introduce KV caching for MDMs** while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to **65x** faster inference than standard MDMs and **4x** faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: [http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)', 'score': 3, 'issue_id': 4100, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '6b37258ad3883db7', 'authors': ['Subham Sekhar Sahoo', 'Zhihan Yang', 'Yash Akhauri', 'Johnna Liu', 'Deepansha Singh', 'Zhoujun Cheng', 'Zhengzhong Liu', 'Eric Xing', 'John Thickstun', 'Arash Vahdat'], 'affiliations': ['Cornell Tech', 'Cornell University', 'MBZUAI', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2506.01928.jpg', 'data': {'categories': ['#inference', '#open_source', '#benchmark', '#architecture', '#optimization', '#diffusion'], 'emoji': 'ğŸš€', 'ru': {'title': 'Eso-LMs: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ KV-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Eso-LMs Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ KV-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Eso-LMs Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ 65 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Eso-LMs: Fast and Efficient Language Modeling Revolution', 'desc': 'Eso-LMs are a new type of language model that combines features from both autoregressive and masked diffusion models. This fusion allows for better performance in language tasks by improving perplexity and inference speed. A key innovation is the introduction of KV caching in masked diffusion models, which enhances efficiency during inference while still allowing for parallel generation. As a result, Eso-LMs achieve significantly faster inference times compared to traditional models, setting new benchmarks in language modeling.'}, 'zh': {'title': 'Eso-LMsï¼šè‡ªå›å½’ä¸æ©è”½æ‰©æ•£æ¨¡å‹çš„å®Œç¾èåˆ', 'desc': 'Eso-LMsæ˜¯ä¸€ç§æ–°å‹çš„è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†è‡ªå›å½’æ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ã€‚å®ƒå¼•å…¥äº†KVç¼“å­˜æŠ€æœ¯ï¼Œä½¿å¾—åœ¨æ¨ç†æ—¶çš„æ•ˆç‡å¤§å¹…æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å¹¶è¡Œç”Ÿæˆçš„èƒ½åŠ›ã€‚é€šè¿‡ä¼˜åŒ–é‡‡æ ·ç­–ç•¥ï¼ŒEso-LMsåœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œæ¨ç†é€Ÿåº¦æ¯”ä¼ ç»Ÿçš„æ©è”½æ‰©æ•£æ¨¡å‹å¿«65å€ã€‚è¯¥æ¨¡å‹æœ‰æ•ˆåœ°è§£å†³äº†è‡ªå›å½’æ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ï¼Œæä¾›äº†æ›´å¥½çš„ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01920', 'title': 'From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.01920', 'abstract': 'A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing a novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1. Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities.', 'score': 3, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': '42a070cbc2d3afee', 'authors': ['Serry Sibaee', 'Omer Nacar', 'Adel Ammar', 'Yasser Al-Habashi', 'Abdulrahman Al-Batati', 'Wadii Boulila'], 'affiliations': ['Prince Sultan University, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2506.01920.jpg', 'data': {'categories': ['#machine_translation', '#benchmark', '#dataset', '#low_resource', '#multilingual'], 'emoji': 'ğŸ‡¦ğŸ‡ª', 'ru': {'title': 'ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ADMD Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼. ADMD ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 490 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 10 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹.'}, 'en': {'title': 'Enhancing Arabic Language Models with Cultural Competence', 'desc': 'This paper introduces a new evaluation framework and dataset called ADMD to improve the assessment of Arabic language models. It identifies key issues in existing Arabic evaluation datasets, such as linguistic accuracy and cultural alignment. The ADMD consists of 490 challenging questions across various domains, which are used to evaluate five leading language models. The findings highlight significant performance variations among models, particularly in areas requiring deep cultural understanding, underscoring the need for cultural competence in language model evaluation.'}, 'zh': {'title': 'æå‡é˜¿æ‹‰ä¼¯è¯­æ¨¡å‹è¯„ä¼°çš„æ–‡åŒ–èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ADMDï¼Œç”¨äºè¯„ä¼°é˜¿æ‹‰ä¼¯è¯­æ¨¡å‹ï¼Œå¼ºè°ƒäº†æ€§èƒ½å·®å¼‚å’Œæ–‡åŒ–èƒ½åŠ›çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åˆ†æäº†ç°æœ‰çš„é˜¿æ‹‰ä¼¯è¯­è¯„ä¼°æ•°æ®é›†ï¼Œå‘ç°äº†è¯­è¨€å‡†ç¡®æ€§ã€æ–‡åŒ–å¯¹é½å’Œæ–¹æ³•è®ºä¸¥è°¨æ€§æ–¹é¢çš„é‡å¤§é—®é¢˜ã€‚ADMDåŒ…å«490ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæ¶µç›–åä¸ªä¸»è¦é¢†åŸŸï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è¿™äº›å±€é™æ€§ã€‚é€šè¿‡ä½¿ç”¨ADMDè¯„ä¼°äº”ä¸ªé¢†å…ˆçš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ·±åšæ–‡åŒ–ç†è§£å’Œä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00723', 'title': 'Pitfalls in Evaluating Language Model Forecasters', 'url': 'https://huggingface.co/papers/2506.00723', 'abstract': 'Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.', 'score': 3, 'issue_id': 4097, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '4260f72a1f88e8cd', 'authors': ['Daniel Paleka', 'Shashwat Goel', 'Jonas Geiping', 'Florian TramÃ¨r'], 'affiliations': ['ELLIS Institute TÃ¼bingen', 'ETH Zurich', 'MPI TÃ¼bingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.00723.jpg', 'data': {'categories': ['#data', '#leakage', '#benchmark', '#evaluation'], 'emoji': 'âš ï¸', 'ru': {'title': 'ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾ Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸: ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ¾ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑĞ¾Ğ¼Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼ Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM.'}, 'en': {'title': 'Rethinking Evaluation: Ensuring Trust in LLM Forecasting', 'desc': 'This paper discusses the challenges of evaluating large language models (LLMs) in forecasting tasks, highlighting that claims of LLMs matching or exceeding human performance may be misleading. The authors identify two main issues: the risk of temporal leakage, which can distort evaluation results, and the difficulty in translating evaluation performance to real-world scenarios. They provide a systematic analysis and examples from previous studies to illustrate how these evaluation flaws can undermine confidence in LLM performance claims. The paper calls for the development of more rigorous evaluation methodologies to accurately assess the forecasting capabilities of LLMs.'}, 'zh': {'title': 'è°¨æ…è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘è¢«åº”ç”¨äºé¢„æµ‹ä»»åŠ¡ï¼Œæœ‰äº›ç ”ç©¶å£°ç§°è¿™äº›ç³»ç»Ÿçš„è¡¨ç°ä¸äººç±»ç›¸å½“æˆ–æ›´å¥½ã€‚æœ¬æ–‡æŒ‡å‡ºï¼Œè¯„ä¼°LLMé¢„æµ‹è€…å­˜åœ¨ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› æ­¤æˆ‘ä»¬åº”å¯¹è¿™äº›ç»“è®ºä¿æŒè°¨æ…ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯ç”±äºå¤šç§æ—¶é—´æ³„æ¼å½¢å¼ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœéš¾ä»¥ä¿¡ä»»ï¼›äºŒæ˜¯ä»è¯„ä¼°è¡¨ç°æ¨æ–­åˆ°ç°å®ä¸–ç•Œé¢„æµ‹çš„éš¾åº¦ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æå’Œå…·ä½“å®ä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯„ä¼°ç¼ºé™·å¦‚ä½•å¼•å‘å¯¹å½“å‰å’Œæœªæ¥æ€§èƒ½å£°æ˜çš„æ‹…å¿§ï¼Œå¹¶ä¸»å¼ éœ€è¦æ›´ä¸¥æ ¼çš„è¯„ä¼°æ–¹æ³•æ¥è‡ªä¿¡åœ°è¯„ä¼°LLMçš„é¢„æµ‹èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21724', 'title': 'OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions', 'url': 'https://huggingface.co/papers/2505.21724', 'abstract': "OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speaker's multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality.", 'score': 3, 'issue_id': 4095, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '309d90ff41ad30b0', 'authors': ['Cheng Luo', 'Jianghui Wang', 'Bing Li', 'Siyang Song', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology', 'University of Exeter'], 'pdf_title_img': 'assets/pdf/title_img/2505.21724.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#cv', '#dataset', '#optimization', '#audio', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniResponse - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ñ… Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹. OmniResponse Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Chrono-Text Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ TempoVoice Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ResponseNet Ñ 696 Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Synchronized Responses for Natural Conversations', 'desc': 'This paper presents OmniResponse, a Multimodal Large Language Model designed to generate synchronized verbal and non-verbal responses in conversations. It introduces a new task called Online Multimodal Conversational Response Generation (OMCRG), which focuses on creating real-time feedback based on multimodal inputs from speakers. The model uses text as an intermediate step to ensure that audio and facial responses are well-coordinated. Additionally, it introduces two innovative components, Chrono-Text and TempoVoice, to enhance the quality and synchronization of the generated responses.'}, 'zh': {'title': 'OmniResponseï¼šåŒæ­¥ç”Ÿæˆå¤šæ¨¡æ€å“åº”çš„åˆ›æ–°æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºåœ¨çº¿å¤šæ¨¡æ€å¯¹è¯å“åº”ç”Ÿæˆï¼ˆOMCRGï¼‰ï¼Œæ—¨åœ¨æ ¹æ®è¯´è¯è€…çš„å¤šæ¨¡æ€è¾“å…¥åœ¨çº¿ç”ŸæˆåŒæ­¥çš„è¯­è¨€å’Œéè¯­è¨€åé¦ˆã€‚ä¸ºäº†è§£å†³ç”Ÿæˆçš„éŸ³é¢‘å’Œé¢éƒ¨ååº”ä¹‹é—´çš„åŒæ­¥é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬åˆ›æ–°æ€§åœ°å¼•å…¥äº†æ–‡æœ¬ä½œä¸ºä¸­ä»‹æ¨¡æ€ã€‚æˆ‘ä»¬æå‡ºäº†OmniResponseï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œèƒ½å¤Ÿè‡ªå›å½’åœ°ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ã€‚é€šè¿‡ä½¿ç”¨Chrono-Textå’ŒTempoVoiceç­‰æ–°ç»„ä»¶ï¼ŒOmniResponseåœ¨è¯­ä¹‰å†…å®¹ã€éŸ³è§†é¢‘åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19621', 'title': 'Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models', 'url': 'https://huggingface.co/papers/2505.19621', 'abstract': "The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS", 'score': 3, 'issue_id': 4093, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '4358fd586e320601', 'authors': ['George Kour', 'Itay Nakash', 'Ateret Anaby-Tavor', 'Michal Shmueli-Scheuer'], 'affiliations': ['IBM'], 'pdf_title_img': 'assets/pdf/title_img/2505.19621.jpg', 'data': {'categories': ['#benchmark', '#data', '#hallucinations', '#multimodal', '#ethics', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Preference, Opinion, and Belief survey (POBs) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğº Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ LLM, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, ĞºĞ°Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¼ĞµĞ½ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Assessing Bias in Language Models: A Call for Neutrality', 'desc': 'This paper introduces the Preference, Opinion, and Belief survey (POBs), which evaluates the subjective biases of Large Language Models (LLMs) in various domains. It highlights that as LLMs are increasingly used in decision-making, their inherent biases can shape the advice they provide, potentially reinforcing certain viewpoints. The study assesses leading LLMs for properties like reliability and neutrality, revealing that newer models tend to exhibit greater bias and inconsistency. Additionally, it examines the impact of advanced reasoning techniques on these biases, finding only marginal improvements in performance.'}, 'zh': {'title': 'è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸»è§‚åè§', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºåå¥½ã€è§‚ç‚¹å’Œä¿¡å¿µè°ƒæŸ¥ï¼ˆPOBsï¼‰çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šã€æ–‡åŒ–ã€ä¼¦ç†å’Œä¸ªäººé¢†åŸŸçš„ä¸»è§‚å€¾å‘ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€æ¨¡å‹ç‰ˆæœ¬çš„æ›´æ–°ï¼Œå®ƒä»¬çš„åè§å’Œä¸ä¸€è‡´æ€§æœ‰æ‰€å¢åŠ ï¼Œè¿™å¯èƒ½å½±å“å®ƒä»¬å¯¹ç”¨æˆ·çš„å»ºè®®å’Œæ¨èã€‚é€šè¿‡å¯¹é¢†å…ˆçš„å¼€æºå’Œé—­æºLLMsè¿›è¡Œè¯„ä¼°ï¼Œè®ºæ–‡æµ‹é‡äº†æ¨¡å‹çš„å¯é æ€§ã€ä¸­ç«‹æ€§å’Œä¸€è‡´æ€§ç­‰å±æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ¨ç†å’Œè‡ªæˆ‘åæ€æœºåˆ¶åœ¨å…¶ä»–ä»»åŠ¡ä¸­æœ‰æ•ˆï¼Œä½†åœ¨æœ¬ç ”ç©¶é¢†åŸŸçš„æå‡æœ‰é™ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹åœ¨æŸäº›è§‚ç‚¹ä¸Šçš„åè§åŠ å‰§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01074', 'title': 'How Programming Concepts and Neurons Are Shared in Code Language Models', 'url': 'https://huggingface.co/papers/2506.01074', 'abstract': "LLMs representing multiple programming languages in their concept space tend to cluster closer to English and exhibit distinct neuron activations for specific languages, particularly in the upper layers, with highly aligned languages sharing similar representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model's concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model's concept space. Code is available at https://github.com/cisnlp/code-specific-neurons.", 'score': 2, 'issue_id': 4101, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '4fca9ba1a062ca80', 'authors': ['Amir Hossein Kargaran', 'Yihong Liu', 'FranÃ§ois Yvon', 'Hinrich SchÃ¼tze'], 'affiliations': ['LMU Munich & Munich Center for Machine Learning', 'Sorbonne UniversitÃ© & CNRS, ISIR'], 'pdf_title_img': 'assets/pdf/title_img/2506.01074.jpg', 'data': {'categories': ['#transfer_learning', '#plp', '#machine_translation', '#multilingual'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ·Ğ³Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸: Ğ±Ğ»Ğ¸Ğ¶Ğµ Ğº Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼Ñƒ, Ñ‡ĞµĞ¼ ĞºĞ°Ğ¶ĞµÑ‚ÑÑ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ±Ğ»Ğ¸Ğ¶Ğµ Ğº Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼Ñƒ ÑĞ·Ñ‹ĞºÑƒ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ° Ğ² Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¯Ğ·Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº LLM Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ·Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unraveling Language Representations in Large Language Models', 'desc': "This paper explores how large language models (LLMs) represent multiple programming languages (PLs) alongside English in their internal concept space. It reveals that LLMs cluster PLs closer to English, particularly in the upper layers, where distinct neuron activations occur for specific languages. The study employs a few-shot translation task across 21 PL pairs, analyzing embeddings and neuron activations to uncover structural patterns in the model's representation of PLs. The findings suggest that highly aligned PLs share similar representations and exhibit unique neuron activations, enhancing our understanding of LLMs in coding tasks."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç¼–ç¨‹è¯­è¨€è¡¨ç¤ºç»“æ„', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆPLsï¼‰ä¸è‹±è¯­ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬å‘ç°ï¼ŒLLMsçš„æ¦‚å¿µç©ºé—´ä¸­ï¼Œç¼–ç¨‹è¯­è¨€çš„è¡¨ç¤ºæ›´æ¥è¿‘è‹±è¯­ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­é—´å±‚çš„ååŠéƒ¨åˆ†ï¼Œè‹±è¯­çš„æ ‡è®°æ¦‚ç‡è¾ƒé«˜ã€‚é€šè¿‡åˆ†æç¥ç»å…ƒæ¿€æ´»ï¼Œæˆ‘ä»¬å‘ç°ç‰¹å®šè¯­è¨€çš„ç¥ç»å…ƒä¸»è¦é›†ä¸­åœ¨åº•å±‚ï¼Œè€Œæ¯ç§ç¼–ç¨‹è¯­è¨€ç‹¬æœ‰çš„ç¥ç»å…ƒåˆ™å‡ºç°åœ¨ä¸Šå±‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†LLMså¦‚ä½•åœ¨å†…éƒ¨è¡¨ç¤ºç¼–ç¨‹è¯­è¨€ï¼Œå¹¶å±•ç¤ºäº†æ¨¡å‹æ¦‚å¿µç©ºé—´ä¸­çš„ç»“æ„æ¨¡å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00930', 'title': 'Aligning VLM Assistants with Personalized Situated Cognition', 'url': 'https://huggingface.co/papers/2506.00930', 'abstract': "A framework called PCogAlign constructs a reward model for aligning vision-language models with personalized situated cognition, using a benchmark with varied Role-Sets.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.", 'score': 2, 'issue_id': 4099, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '113b1c51e51c3619', 'authors': ['Yongqi Li', 'Shen Zhou', 'Xiaohu Li', 'Xin Miao', 'Jintao Wen', 'Mayi Xu', 'Jianhao Chen', 'Birong Pan', 'Hankun Kang', 'Yuanyuan Zhu', 'Ming Zhong', 'Tieyun Qian'], 'affiliations': ['School of Computer Science, Wuhan University, China', 'Zhongguancun Academy, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00930.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal', '#open_source', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ', 'desc': 'PCogAlign - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸Ñ‚ÑƒĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PCogAlignBench, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 18 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ 20 Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒÑƒĞ¼Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…, Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Aligning AI with Personalized Human Cognition', 'desc': "The paper introduces PCogAlign, a framework designed to create a reward model that aligns vision-language models (VLMs) with personalized situated cognition. It recognizes that individuals from diverse backgrounds have unique cognitive expectations, which can affect their interactions with VLMs. To address this, the authors utilize the sociological concept of Role-Set to categorize individuals and evaluate their actions for personalized alignment. The study includes a benchmark called PCogAlignBench, featuring 18,000 instances across 20 different Role-Sets, demonstrating the framework's effectiveness through experimental results and human evaluations."}, 'zh': {'title': 'ä¸ªæ€§åŒ–è®¤çŸ¥å¯¹é½çš„è§†è§‰-è¯­è¨€æ¨¡å‹æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPCogAlignçš„æ¡†æ¶ï¼Œç”¨äºæ„å»ºä¸ä¸ªæ€§åŒ–æƒ…å¢ƒè®¤çŸ¥å¯¹é½çš„è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¥–åŠ±æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸åŒèƒŒæ™¯çš„äººåœ¨ç›¸åŒæƒ…å¢ƒä¸‹å¯èƒ½æœ‰ä¸åŒçš„è®¤çŸ¥å’ŒæœŸæœ›ï¼Œå› æ­¤éœ€è¦é’ˆå¯¹ä¸ªä½“çš„ä¸ªæ€§åŒ–éœ€æ±‚è¿›è¡Œå¯¹é½ã€‚ä¸ºæ­¤ï¼Œä½œè€…åŸºäºç¤¾ä¼šå­¦çš„è§’è‰²é›†æ¦‚å¿µï¼Œç®€åŒ–äº†ä¸ªä½“ç‰¹å¾çš„æè¿°ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«18000ä¸ªå®ä¾‹å’Œ20ä¸ªä¸åŒè§’è‰²é›†ä¸ªä½“çš„åŸºå‡†æ•°æ®é›†PCogAlignBenchã€‚å®éªŒç»“æœå’Œäººç±»è¯„ä¼°è¡¨æ˜ï¼ŒPCogAlignBenchçš„å¯é æ€§å’ŒPCogAlignçš„æœ‰æ•ˆæ€§ï¼Œç›¸å…³ä»£ç å’Œæ•°æ®é›†å°†å¼€æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00789', 'title': 'RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented\n  Generation Systems', 'url': 'https://huggingface.co/papers/2506.00789', 'abstract': "RARE evaluates the robustness of Retrieval-Augmented Generation (RAG) systems against real-world noise, context conflicts, and time-sensitive data with a knowledge-graph-driven benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) enhances recency and factuality in answers. However, existing evaluations rarely test how well these systems cope with real-world noise, conflicting between internal and external retrieved contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness Evaluation (RARE), a unified framework and large-scale benchmark that jointly stress-tests query and document perturbations over dynamic, time-sensitive corpora. One of the central features of RARE is a knowledge-graph-driven synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop relations from the customized corpus and generates multi-level question sets without manual intervention. Leveraging this pipeline, we construct a dataset (RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and policy documents and 48,322 questions whose distribution evolves as the underlying sources change. To quantify resilience, we formalize retrieval-conditioned robustness metrics (RARE-Met) that capture a model's ability to remain correct or recover when queries, documents, or real-world retrieval results are systematically altered. Our results show that RAG systems exhibit surprising vulnerability to perturbations, with document robustness consistently being the weakest point regardless of generator size or architecture. RAG systems consistently show lower robustness on multi-hop queries than single-hop queries across all domains.", 'score': 2, 'issue_id': 4102, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '5c7a672484e9a0fa', 'authors': ['Yixiao Zeng', 'Tianyu Cao', 'Danqing Wang', 'Xinran Zhao', 'Zimeng Qiu', 'Morteza Ziyadi', 'Tongshuang Wu', 'Lei Li'], 'affiliations': ['Amazon', 'Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00789.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#graphs', '#rag', '#security'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RARE: Ğ¡Ñ‚Ñ€ĞµÑÑ-Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'RARE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (RAG) Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑˆÑƒĞ¼Ğ°Ğ¼ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ², ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. RARE Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼.'}, 'en': {'title': 'Enhancing RAG Systems: Evaluating Robustness in Real-World Scenarios', 'desc': 'The paper introduces RARE, a framework designed to evaluate the robustness of Retrieval-Augmented Generation (RAG) systems in real-world scenarios. It addresses how these systems handle noise, conflicting contexts, and rapidly changing information by using a knowledge-graph-driven benchmark. RARE includes a synthesis pipeline that generates complex question sets from a dynamic dataset of time-sensitive documents. The findings reveal that RAG systems are particularly vulnerable to perturbations, especially in multi-hop queries, highlighting the need for improved robustness in document retrieval.'}, 'zh': {'title': 'è¯„ä¼°RAGç³»ç»Ÿçš„é²æ£’æ€§', 'desc': 'RAREæ˜¯ä¸€ä¸ªè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨çœŸå®ä¸–ç•Œå™ªå£°ã€ä¸Šä¸‹æ–‡å†²çªå’Œæ—¶é—´æ•æ„Ÿæ•°æ®ä¸‹é²æ£’æ€§çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡ä¸€ä¸ªçŸ¥è¯†å›¾è°±é©±åŠ¨çš„åŸºå‡†æµ‹è¯•ï¼Œè”åˆæµ‹è¯•æŸ¥è¯¢å’Œæ–‡æ¡£çš„æ‰°åŠ¨ã€‚RAREæ„å»ºäº†ä¸€ä¸ªåŒ…å«400ä¸ªä¸“å®¶çº§æ—¶é—´æ•æ„Ÿæ–‡æ¡£å’Œ48,322ä¸ªé—®é¢˜çš„æ•°æ®é›†ï¼Œä»¥é‡åŒ–æ¨¡å‹åœ¨é¢å¯¹å˜åŒ–æ—¶çš„é²æ£’æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRAGç³»ç»Ÿåœ¨å¤šè·³æŸ¥è¯¢ä¸Šçš„é²æ£’æ€§æ™®éä½äºå•è·³æŸ¥è¯¢ï¼Œä¸”æ–‡æ¡£çš„é²æ£’æ€§æ˜¯æœ€è–„å¼±çš„ç¯èŠ‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00772', 'title': 'LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning', 'url': 'https://huggingface.co/papers/2506.00772', 'abstract': 'Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.', 'score': 2, 'issue_id': 4095, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': 'd66b2e2afe71cfd9', 'authors': ['Zihang Liu', 'Tianyu Pang', 'Oleg Balabanov', 'Chaoqun Yang', 'Tianjin Huang', 'Lu Yin', 'Yaoqing Yang', 'Shiwei Liu'], 'affiliations': ['Dartmouth College, NH, USA', 'Eindhoven University of Technology, the Netherlands', 'International Computer Science Institute, CA, USA', 'Lawrence Berkeley National Laboratory, CA, USA', 'Tsinghua University, China', 'University of California, Berkeley, CA, USA', 'University of Exeter, Exeter, UK', 'University of Oxford, Oxford, UK', 'University of Surrey, Guildford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2506.00772.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#low_resource', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LIFT (Low-rank Informed Sparse Fine-Tuning). LIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 5% Ğ¾Ñ‚ Ğ¸Ñ… Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. LIFT Ñ‚Ğ°ĞºĞ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'Efficient Fine-Tuning with Critical Weights', 'desc': 'This paper introduces a method called Low-rank Informed Sparse Fine-Tuning (LIFT) that improves the efficiency and performance of large language models (LLMs) by focusing on critical weights identified through low-rank approximation. Instead of updating all parameters during fine-tuning, LIFT selectively updates only the top 5% of Principal Weights, which are determined to be the most important for reasoning tasks. This approach not only enhances reasoning capabilities but also reduces the risk of overfitting and catastrophic forgetting, common issues in full fine-tuning. The results show that LIFT outperforms traditional full fine-tuning while preserving more knowledge from the original model, making it a promising strategy for efficient model adaptation.'}, 'zh': {'title': 'ä½ç§©å¾®è°ƒï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºä½ç§©çŸ¥æƒ…ç¨€ç–å¾®è°ƒï¼ˆLIFTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚é€šè¿‡ä½ç§©è¿‘ä¼¼ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºå¯¹æ¨ç†è‡³å…³é‡è¦çš„æƒé‡ï¼Œç§°ä¸ºä¸»æƒé‡ï¼Œå¹¶ä»…æ›´æ–°è¿™äº›æƒé‡çš„å‰5%ã€‚ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼ŒLIFTåœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶åœ¨å†…å­˜ä½¿ç”¨ä¸Šä¿æŒé«˜æ•ˆã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒæºé¢†åŸŸçŸ¥è¯†çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¿å…è¿‡æ‹Ÿåˆå’Œç¾éš¾æ€§é—å¿˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00530', 'title': 'CityLens: Benchmarking Large Language-Vision Models for Urban\n  Socioeconomic Sensing', 'url': 'https://huggingface.co/papers/2506.00530', 'abstract': 'Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce CityLens, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.', 'score': 2, 'issue_id': 4097, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '4c9476ad7ef056e6', 'authors': ['Tianhui Liu', 'Jie Feng', 'Hetian Pang', 'Xin Zhang', 'Tianjian Ouyang', 'Zhiyuan Zhang', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China', 'School of Electronic and Information Engineering, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00530.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#benchmark', '#dataset', '#reasoning', '#survey'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'CityLens: Ğ’Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ³Ğ»Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'CityLens - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLVM) Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒĞ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 17 Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ Ğ²ÑĞµĞ¼Ñƒ Ğ¼Ğ¸Ñ€Ñƒ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 6 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹: ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°, Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€ĞµÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰Ğ°Ñ ÑÑ€ĞµĞ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ»Ğ¸ 11 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ LLVM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'CityLens: Bridging Visual Data and Urban Socioeconomic Insights', 'desc': 'This paper presents CityLens, a benchmark for assessing large language-vision models (LLVMs) in predicting urban socioeconomic indicators using visual data from satellite and street view images. The study creates a multi-modal dataset that includes 17 cities and covers six important domains such as economy and health, reflecting the complexity of urban environments. It defines 11 prediction tasks and employs three evaluation methods to analyze the performance of 17 advanced LLVMs. The findings indicate that while these models show potential in understanding urban data, they still face challenges in accurately predicting socioeconomic conditions, highlighting the need for further research in this area.'}, 'zh': {'title': 'é€šè¿‡è§†è§‰æ•°æ®ç†è§£åŸå¸‚ç¤¾ä¼šç»æµæ¡ä»¶', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†CityLensï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€-è§†è§‰æ¨¡å‹ï¼ˆLLVMsï¼‰åœ¨ä»å«æ˜Ÿå’Œè¡—æ™¯å›¾åƒä¸­é¢„æµ‹åŸå¸‚ç¤¾ä¼šç»æµæŒ‡æ ‡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›–å…¨çƒ17ä¸ªåŸå¸‚ï¼Œæ¶‰åŠç»æµã€æ•™è‚²ã€çŠ¯ç½ªã€äº¤é€šã€å¥åº·å’Œç¯å¢ƒç­‰6ä¸ªå…³é”®é¢†åŸŸï¼Œåæ˜ äº†åŸå¸‚ç”Ÿæ´»çš„å¤šé¢æ€§ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å®šä¹‰äº†11ä¸ªé¢„æµ‹ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨ä¸‰ç§è¯„ä¼°èŒƒå¼ï¼šç›´æ¥åº¦é‡é¢„æµ‹ã€æ ‡å‡†åŒ–åº¦é‡ä¼°è®¡å’ŒåŸºäºç‰¹å¾çš„å›å½’ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLVMsåœ¨æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é¢„æµ‹åŸå¸‚ç¤¾ä¼šç»æµæŒ‡æ ‡æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00385', 'title': 'MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation', 'url': 'https://huggingface.co/papers/2506.00385', 'abstract': 'MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce MagiCodec, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.', 'score': 2, 'issue_id': 4091, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '421c283aaadfdd94', 'authors': ['Yakun Song', 'Jiawei Chen', 'Xiaobin Zhuang', 'Chenpeng Du', 'Ziyang Ma', 'Jian Wu', 'Jian Cong', 'Dongya Jia', 'Zhuo Chen', 'Yuping Wang', 'Yuxuan Wang', 'Xie Chen'], 'affiliations': ['Bytedance Inc.', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00385.jpg', 'data': {'categories': ['#audio', '#optimization', '#diffusion', '#open_source', '#multimodal'], 'emoji': 'ğŸµ', 'ru': {'title': 'MagiCodec: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'MagiCodec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ ĞºĞ¾Ğ´ĞµĞº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ° ÑˆÑƒĞ¼Ğ° Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MagiCodec Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´ĞµĞºĞ¸ ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ¢Ğ¾ĞºĞµĞ½Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ MagiCodec, Ğ¸Ğ¼ĞµÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµĞµ Ğ½Ğ° Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¦Ğ¸Ğ¿Ñ„Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'MagiCodec: Transforming Audio for Better AI Compatibility', 'desc': 'MagiCodec is a new audio codec that uses a Transformer model to improve how audio is represented as tokens. It focuses on enhancing the semantic meaning of these tokens while still ensuring that the audio can be reconstructed accurately. The codec employs a special training method that includes adding noise and regularization to make the tokens more expressive and useful for generative models. Tests show that MagiCodec outperforms existing codecs in both audio quality and its ability to work with other AI models.'}, 'zh': {'title': 'MagiCodecï¼šæå‡éŸ³é¢‘è¯­ä¹‰è¡¨è¾¾çš„ç¼–è§£ç å™¨', 'desc': 'MagiCodecæ˜¯ä¸€ç§åŸºäºTransformerçš„éŸ³é¢‘ç¼–è§£ç å™¨ï¼Œæ—¨åœ¨æé«˜è¯­ä¹‰æ ‡è®°çš„è¡¨è¾¾èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„é‡å»ºæ•ˆæœã€‚ä¸ä¼ ç»Ÿç¼–è§£ç å™¨ä¸åŒï¼ŒMagiCodecåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†é«˜æ–¯å™ªå£°å’Œæ½œåœ¨æ­£åˆ™åŒ–ï¼Œä»¥å¢å¼ºç”Ÿæˆä»£ç çš„è¯­ä¹‰è¡¨ç°åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMagiCodecåœ¨é‡å»ºè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›ç¼–è§£ç å™¨ã€‚å…¶ç”Ÿæˆçš„æ ‡è®°å‘ˆç°å‡ºç±»ä¼¼Zipfåˆ†å¸ƒçš„ç‰¹å¾ï¼Œå¢å¼ºäº†ä¸åŸºäºè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆæ¶æ„çš„å…¼å®¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00381', 'title': 'Neuro2Semantic: A Transfer Learning Framework for Semantic\n  Reconstruction of Continuous Language from Human Intracranial EEG', 'url': 'https://huggingface.co/papers/2506.00381', 'abstract': 'Neuro2Semantic reconstructs semantic content from neural signals using LSTM-based alignment and text generation, outperforming existing methods with limited data.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves strong performance with as little as 30 minutes of neural data, outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies.', 'score': 2, 'issue_id': 4101, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '78b01a3a61f68dc7', 'authors': ['Siavash Shams', 'Richard Antonello', 'Gavin Mischler', 'Stephan Bickel', 'Ashesh Mehta', 'Nima Mesgarani'], 'affiliations': ['Department of Electrical Engineering, Columbia University, USA', 'The Feinstein Institutes for Medical Research, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.00381.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#transfer_learning', '#multimodal', '#healthcare', '#science'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ»Ğ½ Ğº ÑĞ»Ğ¾Ğ²Ğ°Ğ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Neuro2Semantic - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LSTM-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ-ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (30 Ğ¼Ğ¸Ğ½ÑƒÑ‚). Neuro2Semantic Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ….'}, 'en': {'title': 'Transforming Brain Signals into Natural Language', 'desc': 'Neuro2Semantic is a new framework that translates brain signals into meaningful text. It uses a Long Short-Term Memory (LSTM) model to align neural signals from intracranial EEG with existing text representations. After alignment, a correction module generates coherent and natural language from these signals. This method is effective even with limited data, making it a significant advancement in decoding language from neural activity.'}, 'zh': {'title': 'ç¥ç»ä¿¡å·é‡å»ºè¯­ä¹‰å†…å®¹çš„æ–°æ–¹æ³•', 'desc': 'Neuro2Semantic æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»è„‘å†…ç”µæè®°å½•çš„ç¥ç»ä¿¡å·ä¸­é‡å»ºè¯­ä¹‰å†…å®¹ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†åŸºäº LSTM çš„å¯¹é½æŠ€æœ¯ï¼Œå°†ç¥ç»ä¿¡å·ä¸é¢„è®­ç»ƒçš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªæ ¡æ­£æ¨¡å—ç›´æ¥ç”Ÿæˆè‡ªç„¶çš„è¿ç»­æ–‡æœ¬ã€‚ä¸ä»¥å¾€çš„è§£ç æ–¹æ³•ç›¸æ¯”ï¼ŒNeuro2Semantic åœ¨æ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä»…æœ‰ 30 åˆ†é’Ÿçš„ç¥ç»æ•°æ®ä¸‹å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºè„‘æœºæ¥å£å’Œç¥ç»è§£ç æŠ€æœ¯çš„å®é™…åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17127', 'title': 'Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language\n  Models through Visual Counterfacts', 'url': 'https://huggingface.co/papers/2505.17127', 'abstract': 'Visual CounterFact and PvP steering vectors help interpret and control the competition between visual input and memorized world knowledge in multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) perform well on tasks such as visual question answering, but it remains unclear whether their reasoning relies more on memorized world knowledge or on the visual information present in the input image. To investigate this, we introduce Visual CounterFact, a new dataset of visually-realistic counterfactuals that put world knowledge priors (e.g, red strawberry) into direct conflict with visual input (e.g, blue strawberry). Using Visual CounterFact, we show that model predictions initially reflect memorized priors, but shift toward visual evidence in mid-to-late layers. This dynamic reveals a competition between the two modalities, with visual input ultimately overriding priors during evaluation. To control this behavior, we propose Pixels Versus Priors (PvP) steering vectors, a mechanism for controlling model outputs toward either world knowledge or visual input through activation-level interventions. On average, PvP successfully shifts 92.5% of color and 74.6% of size predictions from priors to counterfactuals. Together, these findings offer new tools for interpreting and controlling factual behavior in multimodal models.', 'score': 2, 'issue_id': 4105, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ', 'en': 'May 21', 'zh': '5æœˆ21æ—¥'}, 'hash': '1c1f950796c1f608', 'authors': ['Michal Golovanevsky', 'William Rudman', 'Michael Lepori', 'Amir Bar', 'Ritambhara Singh', 'Carsten Eickhoff'], 'affiliations': ['Brown University', 'Tel Aviv University', 'University of TÃ¼bingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.17127.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#reasoning', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Visual CounterFact, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾ Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¼ĞµÑ‰Ğ°ÑÑ‚ÑÑ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ PvP (Pixels Versus Priors), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Balancing Visual Input and Knowledge in MLLMs', 'desc': 'This paper explores how Multimodal Large Language Models (MLLMs) balance visual information and memorized knowledge when making predictions. It introduces a dataset called Visual CounterFact, which presents scenarios where visual cues conflict with known facts, allowing researchers to observe how models prioritize information. The study finds that while models initially rely on memorized knowledge, they increasingly favor visual evidence as processing continues. To manage this competition, the authors propose Pixels Versus Priors (PvP) steering vectors, which can adjust model outputs to emphasize either visual input or prior knowledge effectively.'}, 'zh': {'title': 'æ§åˆ¶è§†è§‰ä¸è®°å¿†çŸ¥è¯†çš„ç«äº‰', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¾“å…¥å’Œè®°å¿†çŸ¥è¯†ä¹‹é—´çš„ç«äº‰ã€‚æˆ‘ä»¬å¼•å…¥äº†Visual CounterFactæ•°æ®é›†ï¼Œé€šè¿‡è§†è§‰åäº‹å®æ¥ç›´æ¥å¯¹æŠ—ä¸–ç•ŒçŸ¥è¯†å’Œè§†è§‰ä¿¡æ¯ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„é¢„æµ‹æœ€åˆä¾èµ–äºè®°å¿†çŸ¥è¯†ï¼Œä½†åœ¨åæœŸå±‚æ¬¡ä¸­é€æ¸è½¬å‘è§†è§‰è¯æ®ã€‚ä¸ºæ§åˆ¶è¿™ç§è¡Œä¸ºï¼Œæˆ‘ä»¬æå‡ºäº†Pixels Versus Priorsï¼ˆPvPï¼‰å¼•å¯¼å‘é‡ï¼Œå¯ä»¥é€šè¿‡æ¿€æ´»çº§åˆ«å¹²é¢„æ¥è°ƒæ•´æ¨¡å‹è¾“å‡ºï¼ŒæˆåŠŸå°†å¤§éƒ¨åˆ†é¢„æµ‹ä»è®°å¿†çŸ¥è¯†è½¬å‘è§†è§‰è¾“å…¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01062', 'title': 'SealQA: Raising the Bar for Reasoning in Search-Augmented Language\n  Models', 'url': 'https://huggingface.co/papers/2506.01062', 'abstract': 'SealQA evaluates search-augmented language models\' performance on fact-seeking questions with conflicting or noisy search results, revealing limitations in reasoning and factual accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.', 'score': 1, 'issue_id': 4104, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '32b5670dde584ad1', 'authors': ['Thinh Pham', 'Nguyen Nguyen', 'Pratibha Zunjare', 'Weiyuan Chen', 'Yu-Min Tseng', 'Tu Vu'], 'affiliations': ['Virginia Tech, Blacksburg, VA 24061'], 'pdf_title_img': 'assets/pdf/title_img/2506.01062.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#long_context', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'SealQA: Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹', 'desc': 'SealQA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°: Seal-0, Seal-Hard Ğ¸ LongSeal, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… SealQA. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'SealQA: Unveiling the Limits of Search-Augmented Language Models', 'desc': 'SealQA is a benchmark designed to evaluate the performance of search-augmented language models on fact-seeking questions, especially when search results are conflicting or noisy. It consists of three versions: Seal-0, which focuses on challenging questions; Seal-Hard, which tests reasoning and factual accuracy; and LongSeal, which assesses long-context reasoning in complex scenarios. The evaluation shows that even advanced language models struggle significantly, with low accuracy rates on Seal-0 and vulnerability to noisy search results. Furthermore, increasing computational resources does not consistently improve performance, highlighting the need for better models in handling complex information retrieval tasks.'}, 'zh': {'title': 'SealQAï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æœç´¢ä¸­çš„è¡¨ç°', 'desc': 'SealQAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¢å¼ºæœç´¢çš„è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å†²çªæˆ–å™ªå£°æœç´¢ç»“æœæ—¶çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨äº‹å®å¯»æ±‚é—®é¢˜ä¸Šã€‚è¯¥åŸºå‡†åˆ†ä¸ºä¸‰ç§ç±»å‹ï¼šSeal-0å’ŒSeal-Hardä¸»è¦è¯„ä¼°æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ï¼Œè€ŒLongSealåˆ™æµ‹è¯•é•¿ä¸Šä¸‹æ–‡å’Œå¤šæ–‡æ¡£æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å‰æ²¿æ¨¡å‹åœ¨æ‰€æœ‰SealQAç±»å‹ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨Seal-0ä¸­ï¼Œå‡†ç¡®ç‡æä½ã€‚å°½ç®¡ä¸€äº›å…ˆè¿›çš„æ¨ç†æ¨¡å‹å­˜åœ¨ï¼Œä½†å®ƒä»¬åœ¨é¢å¯¹å™ªå£°æœç´¢ç»“æœæ—¶ä»ç„¶éå¸¸è„†å¼±ï¼Œä¸”å¢åŠ è®¡ç®—èµ„æºå¹¶æœªæ˜¾è‘—æé«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00979', 'title': 'IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and\n  Video AIGC Detection', 'url': 'https://huggingface.co/papers/2506.00979', 'abstract': 'IVY-FAKE dataset and Ivy Explainable Detector (IVY-XDETECTOR) architecture address the limitations of current AIGC detection by providing a unified, explainable framework for images and videos.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Artificial Intelligence Generated Content (AIGC) in visual domains has resulted in highly realistic synthetic images and videos, driven by sophisticated generative frameworks such as diffusion-based architectures. While these breakthroughs open substantial opportunities, they simultaneously raise critical concerns about content authenticity and integrity. Many current AIGC detection methods operate as black-box binary classifiers, which offer limited interpretability, and no approach supports detecting both images and videos in a unified framework. This dual limitation compromises model transparency, reduces trustworthiness, and hinders practical deployment. To address these challenges, we introduce IVY-FAKE , a novel, unified, and large-scale dataset specifically designed for explainable multimodal AIGC detection. Unlike prior benchmarks, which suffer from fragmented modality coverage and sparse annotations, IVY-FAKE contains over 150,000 richly annotated training samples (images and videos) and 18,700 evaluation examples, each accompanied by detailed natural-language reasoning beyond simple binary labels. Building on this, we propose Ivy Explainable Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture that jointly performs explainable detection for both image and video content. Our unified vision-language model achieves state-of-the-art performance across multiple image and video detection benchmarks, highlighting the significant advancements enabled by our dataset and modeling framework. Our data is publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.', 'score': 1, 'issue_id': 4101, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ½Ñ', 'en': 'June 1', 'zh': '6æœˆ1æ—¥'}, 'hash': '13910274f7d956aa', 'authors': ['Wayne Zhang', 'Changjiang Jiang', 'Zhonghao Zhang', 'Chenyang Si', 'Fengchang Yu', 'Wei Peng'], 'affiliations': ['Wuhan University', 'Ï€3 AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.00979.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#interpretability', '#multimodal', '#synthetic', '#benchmark', '#architecture'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IVY-FAKE Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ IVY-XDETECTOR Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° (AIGC). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 150 000 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ AIGC Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Unifying AIGC Detection with Explainability', 'desc': 'The paper introduces the IVY-FAKE dataset and the Ivy Explainable Detector (IVY-XDETECTOR) to improve the detection of AI-generated content (AIGC) in images and videos. Current detection methods often lack transparency and only classify content without providing explanations, which can undermine trust. IVY-FAKE offers a large-scale dataset with over 150,000 annotated samples, enhancing the interpretability of detection results through detailed reasoning. The IVY-XDETECTOR utilizes a unified vision-language model to achieve state-of-the-art performance in detecting both images and videos, addressing the limitations of existing approaches.'}, 'zh': {'title': 'ç»Ÿä¸€å¯è§£é‡Šçš„ AIGC æ£€æµ‹æ–°æ¡†æ¶', 'desc': 'IVY-FAKE æ•°æ®é›†å’Œ Ivy Explainable Detector (IVY-XDETECTOR) æ¶æ„æ—¨åœ¨è§£å†³å½“å‰ AIGC æ£€æµ‹çš„å±€é™æ€§ï¼Œæä¾›ä¸€ä¸ªç»Ÿä¸€ä¸”å¯è§£é‡Šçš„å›¾åƒå’Œè§†é¢‘æ£€æµ‹æ¡†æ¶ã€‚éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åœ¨è§†è§‰é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆçš„å›¾åƒå’Œè§†é¢‘å˜å¾—è¶Šæ¥è¶ŠçœŸå®ï¼Œè¿™å¼•å‘äº†å¯¹å†…å®¹çœŸå®æ€§çš„æ‹…å¿§ã€‚ç°æœ‰çš„ AIGC æ£€æµ‹æ–¹æ³•é€šå¸¸ä½œä¸ºé»‘ç®±äºŒå…ƒåˆ†ç±»å™¨ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œä¸”æ— æ³•åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹åŒæ—¶æ£€æµ‹å›¾åƒå’Œè§†é¢‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡å¼•å…¥ IVY-FAKE æ•°æ®é›†å’Œ IVY-XDETECTORï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ AIGC æ£€æµ‹çš„æ€§èƒ½å’Œé€æ˜åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00469', 'title': 'Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data', 'url': 'https://huggingface.co/papers/2506.00469', 'abstract': 'Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations.', 'score': 1, 'issue_id': 4093, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '2ebeb941a6f4f7cc', 'authors': ['Shaoxiong Ji', 'Zihao Li', 'Jaakko Paavola', 'Indraneil Paul', 'Hengyu Luo', 'JÃ¶rg Tiedemann'], 'affiliations': ['Technical University of Darmstadt', 'University of Helsinki'], 'pdf_title_img': 'assets/pdf/title_img/2506.00469.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multilingual', '#transfer_learning', '#open_source', '#machine_translation', '#training', '#low_resource'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ”Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Llama3 Ğº 500 ÑĞ·Ñ‹ĞºĞ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ MaLA, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 2500 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ EMMA-500 Llama 3 Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° 7 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ 12 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ° Ğ²ÑĞµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°.'}, 'en': {'title': 'Boosting Multilingual Models with Bilingual Data', 'desc': 'This paper explores how using bilingual translation data can improve the performance of the Llama3 family of models in multilingual settings. It introduces the MaLA bilingual translation corpus, which includes data from over 2,500 language pairs, to facilitate better language adaptation. The authors develop the EMMA-500 suite of models, which are continually pre-trained on a vast amount of diverse data. Their findings show that incorporating bilingual data significantly enhances language transfer, especially for languages with fewer resources.'}, 'zh': {'title': 'åŒè¯­æ•°æ®åŠ©åŠ›å¤šè¯­è¨€æ¨¡å‹æå‡æ€§èƒ½', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤§è§„æ¨¡å¤šè¯­è¨€æŒç»­é¢„è®­ç»ƒä¸­ï¼ŒåŒè¯­ç¿»è¯‘æ•°æ®çš„å…³é”®è®¾è®¡å†³ç­–ã€‚æˆ‘ä»¬æ„å»ºäº†MaLAåŒè¯­ç¿»è¯‘è¯­æ–™åº“ï¼ŒåŒ…å«2500å¤šä¸ªè¯­è¨€å¯¹çš„æ•°æ®ï¼Œä»¥æ”¯æŒLlama3æ¨¡å‹åœ¨500ç§è¯­è¨€ä¸Šçš„é€‚åº”ã€‚é€šè¿‡å¼€å‘EMMA-500 Llama 3æ¨¡å‹å¥—ä»¶ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä½¿ç”¨æˆ–ä¸ä½¿ç”¨åŒè¯­ç¿»è¯‘æ•°æ®çš„æŒç»­é¢„è®­ç»ƒæ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼ŒåŒè¯­æ•°æ®èƒ½å¤Ÿå¢å¼ºè¯­è¨€è¿ç§»å’Œæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºç¨€ç¼ºçš„è¯­è¨€ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.24216', 'title': 'Shuffle PatchMix Augmentation with Confidence-Margin Weighted\n  Pseudo-Labels for Enhanced Source-Free Domain Adaptation', 'url': 'https://huggingface.co/papers/2505.24216', 'abstract': 'A new augmentation technique, Shuffle PatchMix, and a reweighting strategy improve performance in source-free domain adaptation, achieving state-of-the-art results on PACS, VisDA-C, and DomainNet-126 benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work investigates Source-Free Domain Adaptation (SFDA), where a model adapts to a target domain without access to source data. A new augmentation technique, Shuffle PatchMix (SPM), and a novel reweighting strategy are introduced to enhance performance. SPM shuffles and blends image patches to generate diverse and challenging augmentations, while the reweighting strategy prioritizes reliable pseudo-labels to mitigate label noise. These techniques are particularly effective on smaller datasets like PACS, where overfitting and pseudo-label noise pose greater risks. State-of-the-art results are achieved on three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS, improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target and multi-target settings, respectively, while gains of 2.8% and 0.7% are attained on DomainNet-126 and VisDA-C. This combination of advanced augmentation and robust pseudo-label reweighting establishes a new benchmark for SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM', 'score': 1, 'issue_id': 4102, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '4e2243b50d13c1bf', 'authors': ['Prasanna Reddy Pulakurthi', 'Majid Rabbani', 'Jamison Heard', 'Sohail Dianat', 'Celso M. de Melo', 'Raghuveer Rao'], 'affiliations': ['DEVCOM Army Research Laboratory, Adelphi, MD, USA', 'Rochester Institute of Technology, Rochester, NY, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.24216.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#benchmark', '#data', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ Ğ±ĞµĞ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Shuffle PatchMix (SPM) Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ Ğ±ĞµĞ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Source-Free Domain Adaptation, SFDA). SPM Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚ĞºĞ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…: PACS, VisDA-C Ğ¸ DomainNet-126.'}, 'en': {'title': 'Enhancing Source-Free Domain Adaptation with Shuffle PatchMix', 'desc': 'This paper presents a novel approach to Source-Free Domain Adaptation (SFDA) using a technique called Shuffle PatchMix (SPM) and a reweighting strategy. SPM enhances data augmentation by shuffling and blending image patches, creating diverse training samples that help the model generalize better. The reweighting strategy focuses on selecting reliable pseudo-labels, reducing the impact of label noise during training. The proposed methods achieve state-of-the-art performance on several benchmarks, demonstrating significant improvements in accuracy, especially on smaller datasets like PACS.'}, 'zh': {'title': 'æ— æºé¢†åŸŸé€‚åº”çš„æ–°çªç ´ï¼šShuffle PatchMix', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæŠ€æœ¯Shuffle PatchMixï¼ˆSPMï¼‰å’Œé‡åŠ æƒç­–ç•¥ï¼Œä»¥æé«˜æ— æºé¢†åŸŸé€‚åº”ï¼ˆSFDAï¼‰çš„æ€§èƒ½ã€‚SPMé€šè¿‡æ‰“ä¹±å’Œæ··åˆå›¾åƒå—ç”Ÿæˆå¤šæ ·åŒ–çš„å¢å¼ºæ ·æœ¬ï¼Œè€Œé‡åŠ æƒç­–ç•¥åˆ™ä¼˜å…ˆè€ƒè™‘å¯é çš„ä¼ªæ ‡ç­¾ï¼Œä»¥å‡å°‘æ ‡ç­¾å™ªå£°çš„å½±å“ã€‚è¿™äº›æŠ€æœ¯åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œå¦‚PACSï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹è¿‡æ‹Ÿåˆå’Œä¼ªæ ‡ç­¾å™ªå£°çš„é—®é¢˜ã€‚æœ€ç»ˆï¼Œåœ¨PACSã€VisDA-Cå’ŒDomainNet-126ç­‰ä¸‰ä¸ªä¸»è¦åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22865', 'title': 'BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural\n  Speech Synthesis with Flow Matching Models', 'url': 'https://huggingface.co/papers/2505.22865', 'abstract': 'A flow matching based streaming binaural speech synthesis framework called BinauralFlow generates high-quality, indistinguishable binaural audio using a causal U-Net architecture and continuous inference pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Binaural rendering aims to synthesize binaural audio that mimics natural hearing based on a mono audio and the locations of the speaker and listener. Although many methods have been proposed to solve this problem, they struggle with rendering quality and streamable inference. Synthesizing high-quality binaural audio that is indistinguishable from real-world recordings requires precise modeling of binaural cues, room reverb, and ambient sounds. Additionally, real-world applications demand streaming inference. To address these challenges, we propose a flow matching based streaming binaural speech synthesis framework called BinauralFlow. We consider binaural rendering to be a generation problem rather than a regression problem and design a conditional flow matching model to render high-quality audio. Moreover, we design a causal U-Net architecture that estimates the current audio frame solely based on past information to tailor generative models for streaming inference. Finally, we introduce a continuous inference pipeline incorporating streaming STFT/ISTFT operations, a buffer bank, a midpoint solver, and an early skip schedule to improve rendering continuity and speed. Quantitative and qualitative evaluations demonstrate the superiority of our method over SOTA approaches. A perceptual study further reveals that our model is nearly indistinguishable from real-world recordings, with a 42% confusion rate.', 'score': 1, 'issue_id': 4101, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '0d57f1fd2f698379', 'authors': ['Susan Liang', 'Dejan Markovic', 'Israel D. Gebru', 'Steven Krenn', 'Todd Keebler', 'Jacob Sandakly', 'Frank Yu', 'Samuel Hassel', 'Chenliang Xu', 'Alexander Richard'], 'affiliations': ['1', '2'], 'pdf_title_img': 'assets/pdf/title_img/2505.22865.jpg', 'data': {'categories': ['#audio', '#diffusion'], 'emoji': 'ğŸ§', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'BinauralFlow - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ U-Net Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ½ĞµĞ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ½Ğµ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ STFT/ISTFT, Ğ±Ğ°Ğ½Ğº Ğ±ÑƒÑ„ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'BinauralFlow: Real-Time, High-Quality Binaural Audio Synthesis', 'desc': 'The paper presents BinauralFlow, a novel framework for generating high-quality binaural audio that closely resembles natural hearing. It utilizes a causal U-Net architecture and a flow matching approach to enhance the synthesis of binaural cues, room reverb, and ambient sounds. The framework is designed for streaming inference, allowing for real-time audio generation by processing only past audio frames. Evaluations show that BinauralFlow outperforms state-of-the-art methods, achieving audio quality that is nearly indistinguishable from real-world recordings.'}, 'zh': {'title': 'é«˜è´¨é‡åŒè€³éŸ³é¢‘åˆæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…çš„æµå¼åŒè€³è¯­éŸ³åˆæˆæ¡†æ¶ï¼Œç§°ä¸ºBinauralFlowã€‚è¯¥æ¡†æ¶ä½¿ç”¨å› æœU-Netæ¶æ„å’Œè¿ç»­æ¨ç†ç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€ä¸çœŸå®å½•éŸ³å‡ ä¹æ— æ³•åŒºåˆ†çš„åŒè€³éŸ³é¢‘ã€‚æˆ‘ä»¬å°†åŒè€³æ¸²æŸ“è§†ä¸ºç”Ÿæˆé—®é¢˜ï¼Œè®¾è®¡äº†æ¡ä»¶æµåŒ¹é…æ¨¡å‹æ¥æé«˜éŸ³é¢‘æ¸²æŸ“è´¨é‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†è¿ç»­æ¨ç†ç®¡é“ï¼Œä»¥æ”¹å–„æ¸²æŸ“çš„è¿ç»­æ€§å’Œé€Ÿåº¦ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21668', 'title': 'R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised\n  and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.21668', 'abstract': 'R1-Code-Interpreter extends text-only LLMs with improved code generation abilities through supervised fine-tuning and reinforcement learning, enhancing performance on diverse reasoning and planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in reasoning and planning of R1-like models, Large Language Models (LLMs) still struggle with tasks requiring precise computation, symbolic manipulation, optimization, and algorithmic reasoning, in which textual reasoning lacks the rigor of code execution. A key challenge is enabling LLMs to decide when to use textual reasoning versus code generation. While OpenAI trains models to invoke a Code Interpreter as needed, public research lacks guidance on aligning pre-trained LLMs to effectively leverage code and generalize across diverse tasks. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 diverse questions. We fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies, investigating different answer formats, reasoning vs. non-reasoning models, cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs. Unlike prior RL work on narrow domains, we find that Code Interpreter training is significantly harder due to high task diversity and expensive code execution, highlighting the critical role of the SFT stage. Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to 64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with Code Interpreter (70.9\\%), with the emergent self-checking behavior via code generation. Datasets, Codes, and Models are available at https://github.com/yongchao98/R1-Code-Interpreter and https://huggingface.co/yongchao98.', 'score': 1, 'issue_id': 4101, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'a1b1a393cb209a0c', 'authors': ['Yongchao Chen', 'Yueying Liu', 'Junwei Zhou', 'Yilun Hao', 'Jingquan Wang', 'Yang Zhang', 'Chuchu Fan'], 'affiliations': ['Harvard', 'MIT', 'MIT-IBM Watson AI Lab', 'University of Illinois Urbana-Champaign', 'University of Michigan', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.21668.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#data', '#dataset', '#training', '#rl', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ°', 'desc': 'R1-Code-Interpreter - ÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° 144 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 200 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ R1-CI-14B ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° ÑÑ€ĞµĞ´Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 37 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ 44.0% Ğ´Ğ¾ 64.1%, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ GPT-4 (58.6%) Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ğ²ÑˆĞ¸ÑÑŒ Ğº GPT-4 Ñ Code Interpreter (70.9%). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Code Interpreter Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½ĞµĞµ Ğ¸Ğ·-Ğ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Empowering LLMs with Code: R1-Code-Interpreter', 'desc': 'R1-Code-Interpreter enhances text-only Large Language Models (LLMs) by integrating improved code generation capabilities through supervised fine-tuning and reinforcement learning. This model addresses the limitations of LLMs in tasks that require precise computation and algorithmic reasoning, allowing it to autonomously generate code queries during multi-step reasoning. The research involves fine-tuning Qwen-2.5 models on a diverse set of reasoning and planning tasks, demonstrating significant improvements in accuracy compared to previous models. The findings highlight the importance of the supervised fine-tuning stage in effectively training LLMs to leverage code for better performance across various tasks.'}, 'zh': {'title': 'æå‡ä»£ç ç”Ÿæˆèƒ½åŠ›çš„R1-Code-Interpreter', 'desc': 'R1-Code-Interpreter æ˜¯ä¸€ç§æ‰©å±•æ–‡æœ¬æ¨¡å‹çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†å’Œè§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éœ€è¦ç²¾ç¡®è®¡ç®—å’Œç¬¦å·æ“ä½œçš„ä»»åŠ¡ä¸­ä»ç„¶å­˜åœ¨å›°éš¾ã€‚R1-Code-Interpreter é€šè¿‡å¤šè½®çš„è®­ç»ƒï¼Œèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆä»£ç æŸ¥è¯¢ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œè‡ªæˆ‘æ£€æŸ¥èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20285', 'title': 'MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search\n  Capability', 'url': 'https://huggingface.co/papers/2505.20285', 'abstract': 'A novel pre-training framework, MaskSearch, enhances Large Language Models with universal retrieval and reasoning capabilities through a Retrieval Augmented Mask Prediction task, improving their performance in open-domain multi-hop question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Language Models (RALMs) represent a classic paradigm where models enhance generative capabilities using external knowledge retrieved via a specialized module. Recent advancements in Agent techniques enable Large Language Models (LLMs) to autonomously utilize tools for retrieval, planning, and reasoning. While existing training-based methods show promise, their agentic abilities are limited by inherent characteristics of the task-specific data used during training. To further enhance the universal search capability of agents, we propose a novel pre-training framework, MaskSearch. In the pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to leverage search tools to fill masked spans on a large number of pre-training data, thus acquiring universal retrieval and reasoning capabilities for LLMs. After that, the model is trained on downstream tasks to achieve further improvement. We apply both Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) for training. For SFT, we combine agent-based and distillation-based methods to generate training data, starting with a multi-agent system consisting of a planner, rewriter, observer, and followed by a self-evolving teacher model. While for RL, we employ DAPO as the training framework and adopt a hybrid reward system consisting of answer rewards and format rewards. Additionally, we introduce a curriculum learning approach that allows the model to learn progressively from easier to more challenging instances based on the number of masked spans. We evaluate the effectiveness of our framework in the scenario of open-domain multi-hop question answering. Through extensive experiments, we demonstrate that MaskSearch significantly enhances the performance of LLM-based search agents on both in-domain and out-of-domain downstream tasks.', 'score': 1, 'issue_id': 4098, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'fb55a3a486e6e5a9', 'authors': ['Weiqi Wu', 'Xin Guan', 'Shen Huang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jiuxin Cao', 'Hai Zhao', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.20285.jpg', 'data': {'categories': ['#training', '#agents', '#optimization', '#rag', '#reasoning', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'MaskSearch: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MaskSearch, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (Retrieval Augmented Mask Prediction). MaskSearch Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'Empowering LLMs with Universal Retrieval and Reasoning through MaskSearch', 'desc': 'The paper introduces MaskSearch, a new pre-training framework designed to improve Large Language Models (LLMs) by enhancing their retrieval and reasoning abilities. It employs a Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to use search tools to predict masked spans in data, thereby gaining universal capabilities. The training process includes Supervised Fine-tuning (SFT) and Reinforcement Learning (RL), utilizing a multi-agent system and a hybrid reward system to optimize performance. The results show that MaskSearch significantly boosts the effectiveness of LLMs in open-domain multi-hop question answering tasks.'}, 'zh': {'title': 'MaskSearchï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ£€ç´¢ä¸æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºMaskSearchï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥æ£€ç´¢å¢å¼ºçš„æ©ç é¢„æµ‹ä»»åŠ¡ï¼ˆRAMPï¼‰ï¼Œæ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨æœç´¢å·¥å…·å¡«è¡¥å¤§é‡é¢„è®­ç»ƒæ•°æ®ä¸­çš„æ©ç éƒ¨åˆ†ï¼Œä»è€Œè·å¾—é€šç”¨çš„æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¼•å…¥äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥ä»ç®€å•åˆ°å¤æ‚çš„å®ä¾‹ä¸­å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMaskSearchæ˜¾è‘—æå‡äº†åŸºäºLLMçš„æœç´¢ä»£ç†åœ¨å¼€æ”¾é¢†åŸŸå¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18128', 'title': 'Frankentext: Stitching random text fragments into long-form narratives', 'url': 'https://huggingface.co/papers/2505.18128', 'abstract': 'We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes.', 'score': 1, 'issue_id': 4100, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '2cd0e9db501521da', 'authors': ['Chau Minh Pham', 'Jenna Russell', 'Dzung Pham', 'Mohit Iyyer'], 'affiliations': ['University of Maryland, College Park', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2505.18128.jpg', 'data': {'categories': ['#hallucinations', '#multimodal', '#story_generation', '#training'], 'emoji': 'ğŸ§Ÿ\u200dâ™‚ï¸', 'ru': {'title': 'Frankentexts: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Frankentexts, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ñ‹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ¾Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Gemini-2.5-Pro Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞºÑƒÑÑĞ¸Ñ Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Frankentexts: Merging Human Creativity with AI Precision', 'desc': "Frankentexts are a novel form of long narratives created by large language models (LLMs) that must predominantly use human-written text. This approach tests the model's ability to generate coherent stories while adhering to strict guidelines on text copying. The process involves drafting a narrative by merging human passages and revising it to meet a specified ratio of copied content. The results show that while the generated texts are often coherent and relevant, they can still be distinguished from human writing due to inconsistencies in tone and grammar, highlighting challenges in AI text detection."}, 'zh': {'title': 'Frankentextsï¼šäººæœºå…±åˆ›çš„æ–°æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é•¿ç¯‡å™äº‹æ–‡æœ¬ï¼Œç§°ä¸ºFrankentextsï¼Œè¿™äº›æ–‡æœ¬ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æç«¯çº¦æŸä¸‹ç”Ÿæˆï¼Œè¦æ±‚å¤§éƒ¨åˆ†ï¼ˆä¾‹å¦‚90%ï¼‰çš„è¯æ±‡å¿…é¡»é€å­—å¤åˆ¶è‡ªäººç±»å†™ä½œã€‚è¿™é¡¹ä»»åŠ¡å¯¹å¯æ§ç”Ÿæˆæå‡ºäº†æŒ‘æˆ˜ï¼Œè¦æ±‚æ¨¡å‹æ»¡è¶³å†™ä½œæç¤ºï¼Œæ•´åˆä¸åŒçš„æ–‡æœ¬ç‰‡æ®µï¼Œå¹¶ç”Ÿæˆè¿è´¯çš„å™äº‹ã€‚æˆ‘ä»¬é€šè¿‡æŒ‡å¯¼æ¨¡å‹é€‰æ‹©å’Œç»„åˆäººç±»å†™ä½œçš„æ®µè½æ¥ç”Ÿæˆåˆç¨¿ï¼Œç„¶ååœ¨ä¿æŒç”¨æˆ·æŒ‡å®šçš„å¤åˆ¶æ¯”ä¾‹çš„åŒæ—¶ï¼Œè¿­ä»£ä¿®è®¢åˆç¨¿ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGemini-2.5-Proåœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œ81%çš„Frankentextsè¿è´¯ä¸”100%ä¸æç¤ºç›¸å…³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.16122', 'title': 'Plan and Budget: Effective and Efficient Test-Time Scaling on Large\n  Language Model Reasoning', 'url': 'https://huggingface.co/papers/2505.16122', 'abstract': "Plan-and-Budget framework enhances reasoning efficiency in LLMs by allocating token budgets based on estimated sub-question complexity, improving accuracy, reducing token usage, and boosting $E^3$ metric.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in complex reasoning tasks, but their inference remains computationally inefficient. We observe a common failure mode in many prevalent LLMs, overthinking, where models generate verbose and tangential reasoning traces even for simple queries. Recent works have tried to mitigate this by enforcing fixed token budgets, however, this can lead to underthinking, especially on harder problems. Through empirical analysis, we identify that this inefficiency often stems from unclear problem-solving strategies. To formalize this, we develop a theoretical model, BBAM (Bayesian Budget Allocation Model), which models reasoning as a sequence of sub-questions with varying uncertainty, and introduce the E^3 metric to capture the trade-off between correctness and computation efficiency. Building on theoretical results from BBAM, we propose Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex queries into sub-questions and allocates token budgets based on estimated complexity using adaptive scheduling. Plan-and-Budget improves reasoning efficiency across a range of tasks and models, achieving up to +70% accuracy gains, -39% token reduction, and +187.5% improvement in E^3. Notably, it elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close performance gaps without retraining. Our code is available at anonymous.4open.science/r/P-and-B-6513/.", 'score': 1, 'issue_id': 4105, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': 'fbd65f85b45afd41', 'authors': ['Junhong Lin', 'Xinyue Zeng', 'Jie Zhu', 'Song Wang', 'Julian Shun', 'Jun Wu', 'Dawei Zhou'], 'affiliations': ['MIT CSAIL', 'Michigan State University', 'University of Virginia', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2505.16122.jpg', 'data': {'categories': ['#training', '#inference', '#small_models', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Plan-and-Budget Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ­Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ $E^3$. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸĞ»Ğ°Ğ½-Ğ¸-Ğ‘ÑĞ´Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Smart Token Allocation for Efficient Reasoning in LLMs', 'desc': "The paper introduces the Plan-and-Budget framework, which enhances the reasoning efficiency of Large Language Models (LLMs) by intelligently allocating token budgets based on the complexity of sub-questions. It addresses the common issue of overthinking in LLMs, where they generate excessive and irrelevant reasoning for simple queries. By employing the Bayesian Budget Allocation Model (BBAM), the framework decomposes complex queries into manageable parts and adapts token usage to improve both accuracy and computational efficiency. The results show significant improvements, including up to 70% accuracy gains and a 39% reduction in token usage, demonstrating the framework's effectiveness across various tasks and models."}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†æ•ˆç‡ï¼Œæå‡æ¨¡å‹è¡¨ç°çš„Plan-and-Budgetæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPlan-and-Budgetçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡æ ¹æ®å­é—®é¢˜çš„å¤æ‚æ€§åˆ†é…ä»¤ç‰Œé¢„ç®—ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¸å¤šLLMsåœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶ä¼šå‡ºç°è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œè€ŒPlan-and-Budgetèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œä»è€Œä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡å®è¯åˆ†æï¼ŒPlan-and-Budgetåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ç¼©å°æ¨¡å‹æ€§èƒ½å·®è·çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.15772', 'title': 'MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling', 'url': 'https://huggingface.co/papers/2505.15772', 'abstract': 'Acquiring large-scale emotional speech data with strong consistency remains a challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated multimodal pipeline for extracting high-consistency emotional speech from unlabeled video data. Leveraging face detection and tracking algorithms, we developed an automatic emotion analysis system using a multimodal large language model (MLLM). Our results demonstrate that MIKU-PAL can achieve human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss kappa score) while being much cheaper and faster than human annotation. With the high-quality, flexible, and consistent annotation from MIKU-PAL, we can annotate fine-grained speech emotion categories of up to 26 types, validated by human annotators with 83% rationality ratings. Based on our proposed system, we further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2 hours) as a new benchmark for emotional text-to-speech and visual voice cloning.', 'score': 1, 'issue_id': 4095, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ', 'en': 'May 21', 'zh': '5æœˆ21æ—¥'}, 'hash': '6b39ad10d21b05d8', 'authors': ['Yifan Cheng', 'Ruoyi Zhang', 'Jiatong Shi'], 'affiliations': ['Carnegie Mellon University, Pittsburgh, PA, USA', 'Fish Audio, Santa Clara, CA, USA', 'Huazhong University of Science and Technology, Wuhan, Hubei, China', 'Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.15772.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#audio', '#data'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸', 'desc': 'MIKU-PAL - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ†, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. MIKU-PAL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (68,5% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MELD) Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (0,93 Ğ¿Ğ¾ ÑˆĞºĞ°Ğ»Ğµ Ğ¤Ğ»ĞµĞ¹ÑĞ° ĞºĞ°Ğ¿Ğ¿Ğ°), Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MIKU-EmoBench Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 131,2 Ñ‡Ğ°ÑĞ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 26 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸.'}, 'en': {'title': 'Automating Emotional Speech Extraction with MIKU-PAL', 'desc': 'This paper introduces MIKU-PAL, an automated system designed to extract emotional speech data from unlabeled video sources. It utilizes face detection and tracking, combined with a multimodal large language model, to analyze emotions effectively. The system achieves high accuracy and consistency in emotional speech annotation, outperforming traditional human methods in both cost and speed. Additionally, it provides a new dataset, MIKU-EmoBench, which includes a diverse range of emotional speech categories for further research in speech synthesis.'}, 'zh': {'title': 'MIKU-PALï¼šé«˜æ•ˆä¸€è‡´çš„æƒ…æ„Ÿè¯­éŸ³æå–æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMIKU-PALçš„å…¨è‡ªåŠ¨å¤šæ¨¡æ€ç®¡é“ï¼Œç”¨äºä»æœªæ ‡è®°çš„è§†é¢‘æ•°æ®ä¸­æå–é«˜ä¸€è‡´æ€§çš„æƒ…æ„Ÿè¯­éŸ³ã€‚æˆ‘ä»¬åˆ©ç”¨äººè„¸æ£€æµ‹å’Œè·Ÿè¸ªç®—æ³•ï¼Œå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨æƒ…æ„Ÿåˆ†æç³»ç»Ÿï¼Œä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMIKU-PALåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸Šè¾¾åˆ°äº†äººç±»æ°´å¹³çš„å‡†ç¡®ç‡ï¼ˆ68.5%ï¼‰ï¼Œå¹¶ä¸”ä¸€è‡´æ€§æ˜¾è‘—ä¼˜äºäººå·¥æ ‡æ³¨ï¼ˆ0.93 Fleiss kappaåˆ†æ•°ï¼‰ã€‚åŸºäºè¯¥ç³»ç»Ÿï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªç»†ç²’åº¦æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†MIKU-EmoBenchï¼ˆ131.2å°æ—¶ï¼‰ï¼Œä¸ºæƒ…æ„Ÿæ–‡æœ¬åˆ°è¯­éŸ³å’Œè§†è§‰è¯­éŸ³å…‹éš†æä¾›äº†æ–°çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.01666', 'title': 'Synthesis of discrete-continuous quantum circuits with multimodal\n  diffusion models', 'url': 'https://huggingface.co/papers/2506.01666', 'abstract': "A multimodal denoising diffusion model is introduced for generating both the structure and continuous parameters of quantum circuits, offering an efficient alternative to traditional quantum operation compilation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.", 'score': 0, 'issue_id': 4096, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ½Ñ', 'en': 'June 2', 'zh': '6æœˆ2æ—¥'}, 'hash': 'cf55396f01f6e92c', 'authors': ['Florian FÃ¼rrutter', 'Zohim Chandani', 'Ikko Hamamura', 'Hans J. Briegel', 'Gorka MuÃ±oz-Gil'], 'affiliations': ['Institute for Theoretical Physics University of Innsbruck', 'NVIDIA Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2506.01666.jpg', 'data': {'categories': ['#science', '#dataset', '#diffusion', '#multimodal', '#optimization', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ…ĞµĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑÑ…ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸: Ğ¾Ğ´Ğ¸Ğ½ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ĞµĞ¹, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ ÑÑ…ĞµĞ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑÑ…ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing Quantum Circuit Compilation with Diffusion Models', 'desc': "This paper presents a multimodal denoising diffusion model designed to efficiently generate both the structure and continuous parameters of quantum circuits. Unlike traditional methods that rely on lengthy search algorithms and gradient-based optimizations, this model utilizes two independent diffusion processes to handle discrete gate selection and parameter prediction simultaneously. The authors benchmark the model's performance across various qubit counts and circuit complexities, demonstrating its accuracy and efficiency. Additionally, the rapid circuit generation capability allows for the creation of large datasets, which can be used to uncover new insights into quantum circuit synthesis."}, 'zh': {'title': 'é«˜æ•ˆç¼–è¯‘é‡å­ç”µè·¯çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ¨¡æ€å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé‡å­ç”µè·¯çš„ç»“æ„å’Œè¿ç»­å‚æ•°ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ›¿ä»£ä¼ ç»Ÿé‡å­æ“ä½œç¼–è¯‘æ–¹æ³•ã€‚å½“å‰çš„ç¼–è¯‘æ–¹æ³•è™½ç„¶èƒ½é™ä½ç¼–è¯‘è¯¯å·®ï¼Œä½†è¿è¡Œæ—¶é—´é•¿ä¸”éœ€è¦å¤šæ¬¡è°ƒç”¨é‡å­ç¡¬ä»¶æˆ–æ˜‚è´µçš„ç»å…¸æ¨¡æ‹Ÿï¼Œé™åˆ¶äº†å…¶æ‰©å±•æ€§ã€‚æ–°æå‡ºçš„æ¨¡å‹åŒæ—¶ç”Ÿæˆç”µè·¯çš„ç¦»æ•£é—¨é€‰æ‹©å’Œå‚æ•°é¢„æµ‹ï¼Œåˆ©ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„æ‰©æ•£è¿‡ç¨‹è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡å¯¹ä¸åŒå®éªŒçš„åŸºå‡†æµ‹è¯•ï¼Œåˆ†æäº†è¯¥æ–¹æ³•åœ¨ä¸åŒé‡å­æ¯”ç‰¹æ•°é‡ã€ç”µè·¯æ·±åº¦å’Œå‚æ•°åŒ–é—¨æ¯”ä¾‹ä¸‹çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.00523', 'title': 'SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image\n  Distillation', 'url': 'https://huggingface.co/papers/2506.00523', 'abstract': 'Implicit distribution alignment and intra-segment guidance enhance distribution matching distillation for large-scale text-to-image and flow-based models, improving convergence and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed SenseFlow, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at https://github.com/XingtongGe/SenseFlow.', 'score': 0, 'issue_id': 4099, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ', 'en': 'May 31', 'zh': '5æœˆ31æ—¥'}, 'hash': '6f9b962d86942eda', 'authors': ['Xingtong Ge', 'Xin Zhang', 'Tongda Xu', 'Yi Zhang', 'Xinjie Zhang', 'Yan Wang', 'Jun Zhang'], 'affiliations': ['Institute for AI Industry Research, Tsinghua University', 'SenseTime Research', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.00523.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#cv', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ (IDA) Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ñ„ĞµĞ¹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ (ISG) Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸Ğ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğº Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ĞºĞ°Ğº Stable Diffusion 3.5 Ğ¸ FLUX, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing DMD for Better Convergence in Text-to-Image Models', 'desc': "This paper addresses the challenges of applying Distribution Matching Distillation (DMD) to large-scale text-to-image models, particularly focusing on convergence issues. The authors introduce implicit distribution alignment (IDA) to help align the generator's output with the target distribution, improving the training process. Additionally, they propose intra-segment guidance (ISG) to enhance the importance of timesteps derived from the teacher model, further aiding convergence. The resulting model, SenseFlow, demonstrates improved performance in distillation tasks for both diffusion and flow-based models."}, 'zh': {'title': 'æå‡å¤§è§„æ¨¡æ¨¡å‹æ€§èƒ½çš„è’¸é¦æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„åˆ†å¸ƒåŒ¹é…è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ”¶æ•›é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†éšå¼åˆ†å¸ƒå¯¹é½ï¼ˆIDAï¼‰å’Œæ®µå†…å¼•å¯¼ï¼ˆISGï¼‰æ¥ä¼˜åŒ–ç”Ÿæˆå™¨ä¸å‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡è¿™ä¸¤ç§æ–¹æ³•ï¼ŒDMDåœ¨å¤§å‹æ¨¡å‹å¦‚SD 3.5å’ŒFLUXä¸Šå®ç°äº†æ›´å¥½çš„æ”¶æ•›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹SenseFlowåœ¨è’¸é¦è¿‡ç¨‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚ç”¨äºæ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20670', 'title': 'MMSearch-R1: Incentivizing LMMs to Search', 'url': 'https://huggingface.co/papers/2506.20670', 'abstract': 'MMSearch-R1, a reinforcement learning framework, enables large multimodal models to perform efficient, on-demand, multi-turn search in real-world environments, outperforming existing approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search.', 'score': 46, 'issue_id': 4516, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '15412dc74ea5bed3', 'authors': ['Jinming Wu', 'Zihao Deng', 'Wei Li', 'Yiding Liu', 'Bo You', 'Bo Li', 'Zejun Ma', 'Ziwei Liu'], 'affiliations': ['ByteDance', 'S-Lab, NTU'], 'pdf_title_img': 'assets/pdf/title_img/2506.20670.jpg', 'data': {'categories': ['#optimization', '#dataset', '#reasoning', '#games', '#rl', '#rag', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'MMSearch-R1 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ. ĞĞ½ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ñ‚Ğ¾Ğ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ¸Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ, Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ÑƒÑÑÑŒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ¾Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ…, Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ° Ğ·Ğ° Ğ¿Ğ¾Ğ¸ÑĞº. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VQA Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RAG Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RAG, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 30%.'}, 'en': {'title': 'Efficient Multimodal Search with Reinforcement Learning', 'desc': 'MMSearch-R1 is a novel reinforcement learning framework designed to enhance the performance of large multimodal models (LMMs) in real-world search tasks. It addresses the limitations of traditional methods like retrieval-augmented generation (RAG) by allowing LMMs to conduct efficient, on-demand, multi-turn searches using both text and image data. The framework employs an outcome-based reward system that encourages optimal search strategies while minimizing unnecessary search actions. Through extensive testing, MMSearch-R1 demonstrates superior efficiency and effectiveness compared to existing models, significantly reducing the number of search calls needed to achieve high performance.'}, 'zh': {'title': 'é«˜æ•ˆå¤šæ¨¡æ€æœç´¢çš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶', 'desc': 'MMSearch-R1æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç°å®ç¯å¢ƒä¸­çš„æœç´¢æ•ˆç‡ã€‚ä¸ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ä¸åŒï¼ŒMMSearch-R1é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼ï¼Œæ”¯æŒæŒ‰éœ€çš„å¤šè½®æœç´¢ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬æœç´¢å·¥å…·ï¼Œé€šè¿‡åŸºäºç»“æœçš„å¥–åŠ±å’Œæœç´¢æƒ©ç½šæ¥æŒ‡å¯¼æ¨¡å‹çš„æœç´¢å†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMSearch-R1åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŒç­‰è§„æ¨¡çš„RAGåŸºçº¿ï¼Œå¹¶ä¸”åœ¨å‡å°‘æœç´¢è°ƒç”¨çš„åŒæ—¶ï¼Œæ€§èƒ½ä¸æ›´å¤§è§„æ¨¡çš„RAGæ¨¡å‹ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21506', 'title': 'Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge', 'url': 'https://huggingface.co/papers/2506.21506', 'abstract': 'Mind2Web 2 benchmark evaluates agentic search systems with a suite of realistic, long-horizon tasks, introducing an Agent-as-a-Judge framework to assess accuracy and source attribution.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.', 'score': 38, 'issue_id': 4516, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '00a88b4b0bc63d5b', 'authors': ['Boyu Gou', 'Zanming Huang', 'Yuting Ning', 'Yu Gu', 'Michael Lin', 'Weijian Qi', 'Andrei Kopanev', 'Botao Yu', 'Bernal JimÃ©nez GutiÃ©rrez', 'Yiheng Shu', 'Chan Hee Song', 'Jiaman Wu', 'Shijie Chen', 'Hanane Nour Moussa', 'Tianshu Zhang', 'Jian Xie', 'Yifei Li', 'Tianci Xue', 'Zeyi Liao', 'Kai Zhang', 'Boyuan Zheng', 'Zhaowei Cai', 'Viktor Rozgic', 'Morteza Ziyadi', 'Huan Sun', 'Yu Su'], 'affiliations': ['Amazon AGI', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21506.jpg', 'data': {'categories': ['#interpretability', '#optimization', '#agents', '#agi', '#benchmark'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Mind2Web 2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°', 'desc': 'Mind2Web 2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 130 Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Agent-as-a-Judge Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 1000 Ñ‡Ğ°ÑĞ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚Ñ€ÑƒĞ´Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ 9 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° (OpenAI Deep Research) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 50-70% Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Mind2Web 2: Advancing Evaluation for Agentic Search Systems', 'desc': 'The paper presents Mind2Web 2, a benchmark designed to evaluate agentic search systems through a set of 130 realistic, long-horizon tasks that require extensive web browsing and information synthesis. It introduces the Agent-as-a-Judge framework, which uses task-specific judge agents to automatically assess the accuracy and source attribution of answers generated by these systems. This benchmark addresses the limitations of existing evaluation methods that focus on short search tasks and static responses. The findings indicate that the best-performing system, OpenAI Deep Research, achieves significant performance levels compared to human users, highlighting the potential of agentic search technologies.'}, 'zh': {'title': 'Mind2Web 2ï¼šè¯„ä¼°è‡ªä¸»æœç´¢ç³»ç»Ÿçš„æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Mind2Web 2åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è‡ªä¸»æœç´¢ç³»ç»Ÿåœ¨ç°å®é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Agent-as-a-Judgeæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œæ¥æºå½’å±ã€‚è¯¥åŸºå‡†åŒ…å«130ä¸ªé«˜è´¨é‡çš„ä»»åŠ¡ï¼Œè¦æ±‚å®æ—¶æµè§ˆç½‘é¡µå¹¶ç»¼åˆä¿¡æ¯ï¼Œæ„å»ºè¿‡ç¨‹ä¸­è€—è´¹äº†è¶…è¿‡1000å°æ—¶çš„äººåŠ›ã€‚é€šè¿‡å¯¹ä¹ä¸ªå‰æ²¿è‡ªä¸»æœç´¢ç³»ç»Ÿå’Œäººç±»è¡¨ç°çš„å…¨é¢è¯„ä¼°ï¼Œå±•ç¤ºäº†è¿™äº›ç³»ç»Ÿåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20911', 'title': 'FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient\n  Multi-turn Image Editing', 'url': 'https://huggingface.co/papers/2506.20911', 'abstract': 'A neurosymbolic agent combines language models for fast subtask planning with A$^*$ search for detailed toolpaths, creating a cost-efficient multi-turn image editing solution.  \t\t\t\t\tAI-generated summary \t\t\t\t We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as "Detect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.\'\' It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A^* search per subtask to find a cost-efficient toolpath -- a sequence of calls to AI tools. To save the cost of A^* on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract/refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A^* search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent "FaSTA^*\'\': fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A^* search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA^* is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate.', 'score': 37, 'issue_id': 4519, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': 'f2b4bebdfb3a457f', 'authors': ['Advait Gupta', 'Rishie Raj', 'Dang Nguyen', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.20911.jpg', 'data': {'categories': ['#cv', '#agents', '#optimization', '#reasoning'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº A* Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹. ĞĞ³ĞµĞ½Ñ‚ FaSTA* Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ ĞºĞ°Ğº Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ­Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'FaSTA*: Fast and Efficient Multi-Turn Image Editing', 'desc': 'This paper presents a neurosymbolic agent designed for efficient multi-turn image editing tasks. It utilizes large language models (LLMs) for quick planning of high-level subtasks and employs A* search for precise toolpath execution. The agent learns from previous successful toolpaths, allowing it to reuse effective strategies for similar tasks, which reduces computational costs. The proposed method, named FaSTA*, balances fast planning with detailed execution, achieving competitive performance while being more efficient than existing approaches.'}, 'zh': {'title': 'é«˜æ•ˆçš„å¤šè½®å›¾åƒç¼–è¾‘ä»£ç†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ç¥ç»ç¬¦å·ä»£ç†ï¼Œç”¨äºè§£å†³å¤æ‚çš„å¤šè½®å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚è¯¥ä»£ç†ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¿«é€Ÿçš„é«˜å±‚æ¬¡å­ä»»åŠ¡è§„åˆ’ï¼Œä»¥åŠA^*æœç´¢ç®—æ³•è¿›è¡Œç²¾ç¡®çš„å·¥å…·è·¯å¾„è§„åˆ’ã€‚é€šè¿‡å¯¹æˆåŠŸçš„å·¥å…·è·¯å¾„è¿›è¡Œå½’çº³æ¨ç†ï¼Œä»£ç†èƒ½å¤Ÿæå–å’Œé‡ç”¨å¸¸ç”¨çš„å­ç¨‹åºï¼Œä»è€Œåœ¨ç›¸ä¼¼ä»»åŠ¡ä¸­èŠ‚çœè®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFaSTA^*åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æˆåŠŸç‡ä¸Šä¸æœ€å…ˆè¿›çš„åŸºçº¿ä¿æŒç«äº‰åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21539', 'title': 'WorldVLA: Towards Autoregressive Action World Model', 'url': 'https://huggingface.co/papers/2506.21539', 'abstract': "WorldVLA, an autoregressive action world model integrating vision-language-action (VLA) and world models, enhances performance through mutual understanding and generation, improving action prediction and sequence generation with an attention mask strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task.", 'score': 33, 'issue_id': 4517, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': 'a293600a80c39e2d', 'authors': ['Jun Cen', 'Chaohui Yu', 'Hangjie Yuan', 'Yuming Jiang', 'Siteng Huang', 'Jiayan Guo', 'Xin Li', 'Yibing Song', 'Hao Luo', 'Fan Wang', 'Deli Zhao', 'Hao Chen'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21539.jpg', 'data': {'categories': ['#optimization', '#rl', '#multimodal', '#games', '#cv'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'WorldVLA - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. WorldVLA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Action Prediction with WorldVLA: A Unified Vision-Language-Action Model', 'desc': "WorldVLA is a novel autoregressive action world model that combines vision, language, and action understanding into a single framework. It enhances action prediction and sequence generation by integrating a world model that predicts future images based on actions and visual inputs. The model demonstrates mutual enhancement, where the action model improves visual understanding, which in turn aids the world model's image generation. To tackle the issue of error propagation in action sequences, an attention mask strategy is introduced, leading to significant performance gains in generating action sequences."}, 'zh': {'title': 'ä¸–ç•Œæ¨¡å‹ä¸åŠ¨ä½œæ¨¡å‹çš„ç›¸äº’å¢å¼º', 'desc': 'WorldVLAæ˜¯ä¸€ç§è‡ªå›å½’çš„åŠ¨ä½œä¸–ç•Œæ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰å’Œä¸–ç•Œæ¨¡å‹ã€‚å®ƒé€šè¿‡ç›¸äº’ç†è§£å’Œç”Ÿæˆæ¥å¢å¼ºæ€§èƒ½ï¼Œæ”¹å–„åŠ¨ä½œé¢„æµ‹å’Œåºåˆ—ç”Ÿæˆã€‚è¯¥æ¨¡å‹åˆ©ç”¨åŠ¨ä½œå’Œå›¾åƒç†è§£æ¥é¢„æµ‹æœªæ¥å›¾åƒï¼Œä»è€Œå­¦ä¹ ç¯å¢ƒçš„åŸºæœ¬ç‰©ç†ç‰¹æ€§ã€‚æˆ‘ä»¬æå‡ºçš„æ³¨æ„åŠ›æ©ç ç­–ç•¥æœ‰æ•ˆè§£å†³äº†è‡ªå›å½’ç”Ÿæˆä¸­åŠ¨ä½œæ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†åŠ¨ä½œç”Ÿæˆçš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21520', 'title': 'MADrive: Memory-Augmented Driving Scene Modeling', 'url': 'https://huggingface.co/papers/2506.21520', 'abstract': 'MADrive enhances scene reconstruction for autonomous driving by integrating visually similar 3D car assets from an external memory bank to achieve photorealistic synthesis of altered scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of {sim}70K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/', 'score': 33, 'issue_id': 4524, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '850d8a345d876231', 'authors': ['Polina Karpikova', 'Daniil Selikhanovych', 'Kirill Struminsky', 'Ruslan Musaev', 'Maria Golitsyna', 'Dmitry Baranchuk'], 'affiliations': ['HSE University', 'Skoltech', 'Yandex', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.21520.jpg', 'data': {'categories': ['#3d', '#multimodal', '#games', '#synthetic', '#dataset'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'MADrive - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ğ¸Ğ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MAD-Cars, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 70 Ñ‚Ñ‹ÑÑÑ‡ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. MADrive Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ² Ñ†ĞµĞ»ĞµĞ²ÑƒÑ ÑÑ†ĞµĞ½Ñƒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Autonomous Driving with Memory-Augmented Scene Reconstruction', 'desc': 'MADrive is a framework that improves scene reconstruction for autonomous driving by using a memory bank of 3D car models. It allows for the replacement of real vehicles in a scene with visually similar 3D assets, enhancing the realism of altered driving scenarios. The framework utilizes a curated dataset called MAD-Cars, which contains around 70,000 car videos, to retrieve and reconstruct these 3D assets. This approach enables the generation of photorealistic images of driving environments, even when significant changes are made to the scene.'}, 'zh': {'title': 'MADriveï¼šæå‡è‡ªåŠ¨é©¾é©¶åœºæ™¯é‡å»ºçš„çœŸå®æ„Ÿ', 'desc': 'MADrive æ˜¯ä¸€ä¸ªå¢å¼ºåœºæ™¯é‡å»ºçš„æ¡†æ¶ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡ã€‚å®ƒé€šè¿‡ä»å¤–éƒ¨è®°å¿†åº“ä¸­æ•´åˆè§†è§‰ç›¸ä¼¼çš„ 3D è½¦è¾†èµ„äº§ï¼Œæ¥å®ç°å¯¹æ”¹å˜åœºæ™¯çš„çœŸå®æ„Ÿåˆæˆã€‚è¯¥æ¡†æ¶ä½¿ç”¨äº†ä¸€ä¸ªåä¸º MAD-Cars çš„æ•°æ®é›†ï¼ŒåŒ…å«çº¦ 70,000 ä¸ª 360 åº¦çš„æ±½è½¦è§†é¢‘ï¼Œå¹¶é€šè¿‡æ£€ç´¢æ¨¡å—æ‰¾åˆ°æœ€ç›¸ä¼¼çš„è½¦è¾†å®ä¾‹ã€‚æœ€ç»ˆï¼ŒMADrive èƒ½å¤Ÿç”Ÿæˆå¤šè§†è§’çš„è½¦è¾†è¡¨ç°ï¼Œæ”¯æŒæ˜¾è‘—æ”¹å˜çš„åœºæ™¯åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21551', 'title': 'Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test', 'url': 'https://huggingface.co/papers/2506.21551', 'abstract': 'Grokking, or continued test performance improvement after training loss convergence, is observed during pretraining of a large language model, showcasing a memorization-to-generalization process.  \t\t\t\t\tAI-generated summary \t\t\t\t Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, we conduct the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. We compute the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks.   Our study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. We further demystify grokking\'s "emergence of generalization" by investigating LLM internal dynamics. Specifically, we find that training samples\' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample\'s pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. In the study, we develop two novel metrics to quantify pathway distance and the complexity of a single pathway. We show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, we show that more structured pathways reduce model complexity and improve the generalization bound.', 'score': 25, 'issue_id': 4518, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': 'd78fbac896c81bf5', 'authors': ['Ziyue Li', 'Chenrui Fan', 'Tianyi Zhou'], 'affiliations': ['Department of Computer Science University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.21551.jpg', 'data': {'categories': ['#math', '#data', '#optimization', '#reasoning', '#benchmark', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ° (grokking) Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ“Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ÑÑ‰ĞµĞµÑÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»Ğµ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Grokking: From Memorization to Generalization in Large Language Models', 'desc': 'This paper explores the phenomenon of grokking, where a large language model continues to improve its test performance even after the training loss has stabilized. The authors investigate this behavior during the pretraining of a 7 billion parameter model, OLMoE, and find that grokking occurs asynchronously across different data samples. They analyze the internal dynamics of the model, revealing that the pathways through which training samples are processed evolve from random to more structured forms, indicating a shift from memorization to generalization. Additionally, the study introduces new metrics to measure the complexity of these pathways, which can predict improvements in generalization across various tasks without the need for fine-tuning.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–ä¹‹è°œ', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„â€œgrokkingâ€ç°è±¡ï¼Œå³åœ¨è®­ç»ƒæŸå¤±æ”¶æ•›åï¼Œæµ‹è¯•æ€§èƒ½ä»ç„¶æŒç»­æé«˜ã€‚æˆ‘ä»¬é¦–æ¬¡åœ¨ä¸€ä¸ª7Bå‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹OLMoEçš„å•æ¬¡é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸Šè¿›è¡Œç ”ç©¶ï¼ŒéªŒè¯äº†grokkingåœ¨å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„å­˜åœ¨ã€‚ç ”ç©¶å‘ç°ï¼Œè®­ç»ƒæ ·æœ¬çš„è·¯å¾„ä»éšæœºã€ç‰¹å®šå®ä¾‹é€æ¸æ¼”å˜ä¸ºæ›´ç»“æ„åŒ–å’Œå¯å…±äº«çš„å½¢å¼ï¼Œå°½ç®¡æŸå¤±å·²æ”¶æ•›ï¼Œæ ·æœ¬è·¯å¾„çš„å¤æ‚æ€§å´åœ¨é™ä½ã€‚è¿™è¡¨æ˜äº†ä»è®°å¿†åˆ°æ³›åŒ–çš„è½¬å˜ï¼Œä¸ºå»¶è¿Ÿæ³›åŒ–æä¾›äº†æœºåˆ¶è§£é‡Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21547', 'title': 'SAM4D: Segment Anything in Camera and LiDAR Streams', 'url': 'https://huggingface.co/papers/2506.21547', 'abstract': 'SAM4D is a multi-modal and temporal foundation model for segmentation in autonomous driving using Unified Multi-modal Positional Encoding and Motion-aware Cross-modal Memory Attention, with a multi-modal automated data engine generating pseudo-labels.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SAM4D, a multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. Unified Multi-modal Positional Encoding (UMPE) is introduced to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. Additionally, we propose Motion-aware Cross-modal Memory Attention (MCMA), which leverages ego-motion compensation to enhance temporal consistency and long-horizon feature retrieval, ensuring robust segmentation across dynamically changing autonomous driving scenes. To avoid annotation bottlenecks, we develop a multi-modal automated data engine that synergizes VFM-driven video masklets, spatiotemporal 4D reconstruction, and cross-modal masklet fusion. This framework generates camera-LiDAR aligned pseudo-labels at a speed orders of magnitude faster than human annotation while preserving VFM-derived semantic fidelity in point cloud representations. We conduct extensive experiments on the constructed Waymo-4DSeg, which demonstrate the powerful cross-modal segmentation ability and great potential in data annotation of proposed SAM4D.', 'score': 12, 'issue_id': 4516, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '25172262b153bc59', 'authors': ['Jianyun Xu', 'Song Wang', 'Ziqian Ni', 'Chunyong Hu', 'Sheng Yang', 'Jianke Zhu', 'Qiang Li'], 'affiliations': ['Unmanned Vehicle Dept., CaiNiao Inc., Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21547.jpg', 'data': {'categories': ['#optimization', '#dataset', '#games', '#multimodal', '#cv'], 'emoji': 'ğŸš—', 'ru': {'title': 'SAM4D: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹', 'desc': 'SAM4D - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (UMPE) Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ»Ğ¸Ğ´Ğ°Ñ€Ğ° Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ (MCMA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. SAM4D Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Segmentation in Autonomous Driving with SAM4D', 'desc': 'SAM4D is a cutting-edge model that integrates multiple data types, specifically camera and LiDAR, to improve segmentation tasks in autonomous driving. It uses Unified Multi-modal Positional Encoding to align features from both data sources in a shared 3D space, facilitating effective interaction between them. The model also incorporates Motion-aware Cross-modal Memory Attention to maintain consistency over time and retrieve features from long sequences, which is crucial for dynamic driving environments. To streamline the data labeling process, SAM4D employs an automated engine that generates high-quality pseudo-labels quickly, significantly reducing the need for manual annotation.'}, 'zh': {'title': 'SAM4Dï¼šè‡ªåŠ¨é©¾é©¶åˆ†å‰²çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹', 'desc': 'SAM4Dæ˜¯ä¸€ç§å¤šæ¨¡æ€å’Œæ—¶é—´åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶ä¸­çš„åˆ†å‰²ä»»åŠ¡è®¾è®¡ã€‚å®ƒä½¿ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€ä½ç½®ç¼–ç ï¼ˆUMPEï¼‰æ¥å¯¹é½ç›¸æœºå’Œæ¿€å…‰é›·è¾¾çš„ç‰¹å¾ï¼Œä»è€Œå®ç°æ— ç¼çš„è·¨æ¨¡æ€æç¤ºå’Œäº¤äº’ã€‚æ­¤å¤–ï¼Œè¿åŠ¨æ„ŸçŸ¥è·¨æ¨¡æ€è®°å¿†æ³¨æ„åŠ›ï¼ˆMCMAï¼‰åˆ©ç”¨è‡ªæˆ‘è¿åŠ¨è¡¥å¿æ¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§å’Œé•¿æ—¶é—´ç‰¹å¾æ£€ç´¢ï¼Œç¡®ä¿åœ¨åŠ¨æ€å˜åŒ–çš„è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­è¿›è¡Œç¨³å¥çš„åˆ†å‰²ã€‚ä¸ºäº†é¿å…æ ‡æ³¨ç“¶é¢ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡æ€è‡ªåŠ¨æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆä¸ç›¸æœºå’Œæ¿€å…‰é›·è¾¾å¯¹é½çš„ä¼ªæ ‡ç­¾ï¼Œé€Ÿåº¦è¿œè¶…äººå·¥æ ‡æ³¨ï¼ŒåŒæ—¶ä¿æŒç‚¹äº‘è¡¨ç¤ºä¸­çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21103', 'title': 'Learning to Skip the Middle Layers of Transformers', 'url': 'https://huggingface.co/papers/2506.21103', 'abstract': "A novel conditional computation architecture for Transformers dynamically skips middle layers based on input and a gating mechanism, but does not outperform dense baselines in reducing computational cost or improving validation performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Conditional computation is a popular strategy to make Transformers more efficient. Existing methods often target individual modules (e.g., mixture-of-experts layers) or skip layers independently of one another. However, interpretability research has demonstrated that the middle layers of Transformers exhibit greater redundancy, and that early layers aggregate information into token positions. Guided by these insights, we propose a novel architecture that dynamically skips a variable number of layers from the middle outward. In particular, a learned gating mechanism determines whether to bypass a symmetric span of central blocks based on the input, and a gated attention mechanism prevents subsequent tokens from attending to skipped token positions. Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and gate sparsity with an adaptive regularization loss. We had aimed to reduce compute requirements for 'simpler' tokens and potentially foster an emergent multi-level representational hierarchy but, at the scales investigated, our approach does not achieve improvements in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. We release our code at https://github.com/tim-lawson/skip-middle.", 'score': 7, 'issue_id': 4524, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '3c7e7fa3beaf5d4d', 'authors': ['Tim Lawson', 'Laurence Aitchison'], 'affiliations': ['School of Engineering Mathematics and Technology University of Bristol Bristol, UK'], 'pdf_title_img': 'assets/pdf/title_img/2506.21103.jpg', 'data': {'categories': ['#interpretability', '#architecture', '#optimization', '#training'], 'emoji': 'â­ï¸', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ»Ğ¸ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ğµ insights Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ¾ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Dynamic Layer Skipping in Transformers: Innovation Meets Limitation', 'desc': 'This paper introduces a new architecture for Transformers that uses conditional computation to skip certain middle layers based on the input data. A gating mechanism is employed to decide which layers to bypass, aiming to enhance efficiency without compromising performance. Despite the innovative approach, the results show that this method does not outperform traditional dense models in terms of reducing computational costs or improving validation accuracy. The findings suggest that while the architecture is theoretically sound, it may not provide practical benefits at the scales tested.'}, 'zh': {'title': 'åŠ¨æ€è·³å±‚ï¼Œæå‡Transformeræ•ˆç‡çš„æ¢ç´¢', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡ä»¶è®¡ç®—æ¶æ„ï¼Œç”¨äºTransformeræ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥åŠ¨æ€è·³è¿‡ä¸­é—´å±‚ã€‚é€šè¿‡å­¦ä¹ çš„é—¨æ§æœºåˆ¶ï¼Œå†³å®šæ˜¯å¦è·³è¿‡ä¸€æ®µå¯¹ç§°çš„ä¸­å¤®å—ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚å°½ç®¡è¯¥æ–¹æ³•æ—¨åœ¨å‡å°‘è®¡ç®—éœ€æ±‚å¹¶ä¿ƒè¿›å¤šå±‚æ¬¡è¡¨ç¤ºçš„å½¢æˆï¼Œä½†åœ¨å®éªŒä¸­æœªèƒ½åœ¨éªŒè¯æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ä¹‹é—´å–å¾—æ˜¾è‘—æ”¹å–„ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¯†é›†åŸºçº¿åœ¨å‡å°‘è®¡ç®—æˆæœ¬å’Œæé«˜éªŒè¯æ€§èƒ½æ–¹é¢ä»ç„¶è¡¨ç°æ›´å¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20936', 'title': 'PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for\n  Realistic Articulated Object Modeling', 'url': 'https://huggingface.co/papers/2506.20936', 'abstract': 'A physics-based skinning and rigging framework called PhysRig uses volumetric representation and continuum mechanics for more realistic and physically plausible animations.  \t\t\t\t\tAI-generated summary \t\t\t\t Skinning and rigging are fundamental components in animation, articulated object reconstruction, motion transfer, and 4D generation. Existing approaches predominantly rely on Linear Blend Skinning (LBS), due to its simplicity and differentiability. However, LBS introduces artifacts such as volume loss and unnatural deformations, and it fails to model elastic materials like soft tissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty tissues). In this work, we propose PhysRig: a differentiable physics-based skinning and rigging framework that overcomes these limitations by embedding the rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh), which is simulated as a deformable soft-body structure driven by the animated skeleton. Our method leverages continuum mechanics and discretizes the object as particles embedded in an Eulerian background grid to ensure differentiability with respect to both material properties and skeletal motion. Additionally, we introduce material prototypes, significantly reducing the learning space while maintaining high expressiveness. To evaluate our framework, we construct a comprehensive synthetic dataset using meshes from Objaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object categories and motion patterns. Our method consistently outperforms traditional LBS-based approaches, generating more realistic and physically plausible results. Furthermore, we demonstrate the applicability of our framework in the pose transfer task highlighting its versatility for articulated object modeling.', 'score': 7, 'issue_id': 4530, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': 'a4cf66e609fddfae', 'authors': ['Hao Zhang', 'Haolan Xu', 'Chun Feng', 'Varun Jampani', 'Narendra Ahuja'], 'affiliations': ['Stability AI', 'University of Illinois Urbana Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.20936.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#games', '#cv', '#3d'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'PhysRig: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'PhysRig - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºÑƒ ÑĞ¿Ğ»Ğ¾ÑˆĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Linear Blend Skinning (LBS), PhysRig Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¸ Ğ½ĞµĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼ÑĞ³ĞºĞ¸Ğµ Ñ‚ĞºĞ°Ğ½Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ´Ğ°Ñ‚ĞºĞ¸. PhysRig Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºĞµĞ»ĞµÑ‚Ğ°.'}, 'en': {'title': 'PhysRig: Realistic Animation through Physics-Based Skinning', 'desc': 'The paper introduces PhysRig, a novel skinning and rigging framework that utilizes physics-based principles for more realistic animations. Unlike traditional Linear Blend Skinning (LBS), which can cause unnatural deformations, PhysRig employs a volumetric representation and continuum mechanics to simulate soft-body dynamics. This approach allows for better modeling of elastic materials and complex shapes, such as soft tissues and flexible appendages. The framework is evaluated using a synthetic dataset and shows superior performance in generating physically plausible animations compared to existing methods.'}, 'zh': {'title': 'PhysRigï¼šæ›´çœŸå®çš„åŠ¨ç”»çš®è‚¤ä¸ç»‘å®šæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†çš„çš®è‚¤å’Œç»‘å®šæ¡†æ¶ï¼Œç§°ä¸ºPhysRigï¼Œæ—¨åœ¨å®ç°æ›´çœŸå®å’Œç‰©ç†ä¸Šåˆç†çš„åŠ¨ç”»æ•ˆæœã€‚ä¼ ç»Ÿçš„çº¿æ€§æ··åˆçš®è‚¤ï¼ˆLBSï¼‰æ–¹æ³•è™½ç„¶ç®€å•ï¼Œä½†ä¼šå¯¼è‡´ä½“ç§¯æŸå¤±å’Œä¸è‡ªç„¶çš„å˜å½¢ï¼Œæ— æ³•æœ‰æ•ˆæ¨¡æ‹Ÿè½¯ç»„ç»‡å’Œçµæ´»çš„é™„è‚¢ã€‚PhysRigé€šè¿‡å°†åˆšæ€§éª¨æ¶åµŒå…¥ä½“ç§¯è¡¨ç¤ºä¸­ï¼Œå¹¶åˆ©ç”¨è¿ç»­ä»‹è´¨åŠ›å­¦è¿›è¡Œæ¨¡æ‹Ÿï¼Œå…‹æœäº†è¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒPhysRigåœ¨ç”ŸæˆçœŸå®æ„Ÿå’Œç‰©ç†åˆç†æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„LBSæ–¹æ³•ï¼Œä¸”åœ¨å§¿æ€è½¬ç§»ä»»åŠ¡ä¸­å±•ç°äº†è‰¯å¥½çš„é€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21552', 'title': 'Whole-Body Conditioned Egocentric Video Prediction', 'url': 'https://huggingface.co/papers/2506.21552', 'abstract': "A model trained on real-world egocentric video and body pose predicts video from human actions using an auto-regressive conditional diffusion transformer, evaluated with a hierarchical protocol of tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human.", 'score': 6, 'issue_id': 4519, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '88d888f2aa383886', 'authors': ['Yutong Bai', 'Danny Tran', 'Amir Bar', 'Yann LeCun', 'Trevor Darrell', 'Jitendra Malik'], 'affiliations': ['FAIR, Meta', 'New York University', 'UC Berkeley (BAIR)'], 'pdf_title_img': 'assets/pdf/title_img/2506.21552.jpg', 'data': {'categories': ['#diffusion', '#agents', '#games', '#dataset', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PEVA, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Nymeria Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸ Ñ‚ĞµĞ»Ğ°. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ñ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ÑÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Predicting Video from Human Actions in First-Person View', 'desc': "This paper presents a model called PEVA, which predicts ego-centric video based on human actions and body poses. It utilizes an auto-regressive conditional diffusion transformer to learn how human actions influence the environment from a first-person perspective. The model is trained on a large dataset, Nymeria, which includes real-world video and body pose data. A hierarchical evaluation protocol is introduced to assess the model's performance on various tasks, highlighting its ability to understand and simulate complex interactions in real-world scenarios."}, 'zh': {'title': 'ä»äººç±»åŠ¨ä½œé¢„æµ‹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„åˆ›æ–°æ¨¡å‹', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œæ—¨åœ¨ä»äººç±»åŠ¨ä½œé¢„æµ‹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç›¸å¯¹3Dèº«ä½“å§¿æ€ä½œä¸ºæ¡ä»¶ï¼Œç»“åˆè¿åŠ¨å­¦è½¨è¿¹ï¼Œæ¨¡æ‹Ÿäººç±»åŠ¨ä½œå¦‚ä½•å½±å“ç¯å¢ƒã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåä¸ºNymeriaçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’æ¡ä»¶æ‰©æ•£å˜æ¢å™¨ã€‚é€šè¿‡è®¾è®¡åˆ†å±‚è¯„ä¼°åè®®ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå…¨é¢åˆ†ææ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é¢„æµ‹å’Œæ§åˆ¶èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16655', 'title': 'Arch-Router: Aligning LLM Routing with Human Preferences', 'url': 'https://huggingface.co/papers/2506.16655', 'abstract': 'A preference-aligned routing framework using a compact 1.5B model effectively matches queries to user-defined domains and action types, outperforming proprietary models in subjective evaluation criteria.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid proliferation of large language models (LLMs) -- each optimized for different strengths, style, or latency/cost profile -- routing has become an essential technique to operationalize the use of different models. However, existing LLM routing approaches are limited in two key ways: they evaluate performance using benchmarks that often fail to capture human preferences driven by subjective evaluation criteria, and they typically select from a limited pool of models. In this work, we propose a preference-aligned routing framework that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing) -- offering a practical mechanism to encode preferences in routing decisions. Specifically, we introduce Arch-Router, a compact 1.5B model that learns to map queries to domain-action preferences for model routing decisions. Our approach also supports seamlessly adding new models for routing without requiring retraining or architectural modifications. Experiments on conversational datasets demonstrate that our approach achieves state-of-the-art (SOTA) results in matching queries with human preferences, outperforming top proprietary models. Our approach captures subjective evaluation criteria and makes routing decisions more transparent and flexible. Our model is available at: https://huggingface.co/katanemo/Arch-Router-1.5B.', 'score': 6, 'issue_id': 4519, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '09391602d5fce0b2', 'authors': ['Co Tran', 'Salman Paracha', 'Adil Hafeez', 'Shuguang Chen'], 'affiliations': ['Katanemo Labs, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.16655.jpg', 'data': {'categories': ['#training', '#alignment', '#small_models', '#multimodal'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº Ğ˜Ğ˜ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Arch-Router Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 1,5 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Aligning Queries with User Preferences for Optimal Model Routing', 'desc': 'This paper presents a new routing framework called Arch-Router, which uses a compact 1.5 billion parameter model to effectively match user queries with specific domains and action types. Unlike traditional models that rely on fixed benchmarks, this framework aligns with human preferences by incorporating subjective evaluation criteria into its routing decisions. Arch-Router allows for the easy addition of new models without the need for retraining, enhancing flexibility in model selection. Experiments show that this approach outperforms existing proprietary models, achieving state-of-the-art results in aligning queries with user preferences.'}, 'zh': {'title': 'åå¥½å¯¹é½çš„æ™ºèƒ½è·¯ç”±æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åå¥½å¯¹é½çš„è·¯ç”±æ¡†æ¶ï¼Œä½¿ç”¨ä¸€ä¸ªç´§å‡‘çš„1.5Bæ¨¡å‹æœ‰æ•ˆåœ°å°†æŸ¥è¯¢ä¸ç”¨æˆ·å®šä¹‰çš„é¢†åŸŸå’ŒåŠ¨ä½œç±»å‹åŒ¹é…ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æŸ¥è¯¢ä¸ç”¨æˆ·çš„åå¥½ç›¸ç»“åˆï¼Œå…‹æœäº†ç°æœ‰å¤§è¯­è¨€æ¨¡å‹è·¯ç”±æ–¹æ³•åœ¨ä¸»è§‚è¯„ä¼°æ ‡å‡†ä¸Šçš„å±€é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥çš„Arch-Routeræ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å°†æŸ¥è¯¢æ˜ å°„åˆ°é¢†åŸŸ-åŠ¨ä½œåå¥½ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸äººç±»åå¥½çš„åŒ¹é…ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œè¶…è¶Šäº†è®¸å¤šä¸“æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20703', 'title': 'Generative Blocks World: Moving Things Around in Pictures', 'url': 'https://huggingface.co/papers/2506.20703', 'abstract': 'A generative method that edits 3D scenes using convex primitives and regenerates images with enhanced texture consistency and visual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t We describe Generative Blocks World to interact with the scene of a generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by a flow-based method which is conditioned on depth and a texture hint. Our texture hint takes into account the modified 3D primitives, exceeding texture-consistency provided by existing key-value caching techniques. These texture hints (a) allow accurate object and camera moves and (b) largely preserve the identity of objects depicted. Quantitative and qualitative experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization.', 'score': 5, 'issue_id': 4535, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '5f5979ba6e2c9a34', 'authors': ['Vaibhav Vavilala', 'Seemandhar Jain', 'Rahul Vasanth', 'D. A. Forsyth', 'Anand Bhattad'], 'affiliations': ['Toyota Technological Institute at Chicago', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.20703.jpg', 'data': {'categories': ['#3d', '#cv'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ğ¿ÑƒĞºĞ»Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ÑĞ¼Ğ¸ ÑÑ†ĞµĞ½Ñ‹, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞµÑ‘ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€ 3D-Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞŸĞ¾ÑĞ»Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ flow-based Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Edit 3D Scenes with Precision and Visual Fidelity!', 'desc': 'This paper presents a novel generative method called Generative Blocks World, which allows users to edit 3D scenes using simple geometric shapes known as convex primitives. The method enables flexible scene representation, where the same scene can be constructed with varying numbers of these primitives, facilitating both large structural changes and fine detail adjustments. After editing the scene geometry, a flow-based image generation technique is employed, which utilizes depth information and a texture hint to enhance visual quality. The results show that this approach significantly improves texture consistency and visual fidelity compared to existing methods, making it easier to manipulate and generate realistic 3D images.'}, 'zh': {'title': 'é€šè¿‡å‡¸ä½“ç´ ç¼–è¾‘3Dåœºæ™¯ï¼Œæå‡å›¾åƒè´¨é‡ä¸ä¸€è‡´æ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨å‡¸ä½“ç´ ç¼–è¾‘3Dåœºæ™¯ï¼Œå¹¶ç”Ÿæˆå…·æœ‰å¢å¼ºçº¹ç†ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ„Ÿçš„å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†åœºæ™¯è¡¨ç¤ºä¸ºå‡¸3DåŸè¯­çš„ç»„åˆï¼Œå…è®¸ç¼–è¾‘è€…ç§»åŠ¨æ•´ä¸ªç»“æ„æˆ–å°ç»†èŠ‚ã€‚ç¼–è¾‘åœºæ™¯å‡ ä½•åï¼Œä½¿ç”¨åŸºäºæµçš„æ–¹æ³•ç”Ÿæˆå›¾åƒï¼Œè¯¥æ–¹æ³•ä¾èµ–äºæ·±åº¦ä¿¡æ¯å’Œçº¹ç†æç¤ºã€‚æˆ‘ä»¬çš„çº¹ç†æç¤ºè€ƒè™‘äº†ä¿®æ”¹åçš„3DåŸè¯­ï¼Œè¶…è¶Šäº†ç°æœ‰å…³é”®å€¼ç¼“å­˜æŠ€æœ¯æä¾›çš„çº¹ç†ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20430', 'title': 'An Agentic System for Rare Disease Diagnosis with Traceable Reasoning', 'url': 'https://huggingface.co/papers/2506.20430', 'abstract': "DeepRare, a large language model-based system, provides accurate rare disease diagnoses using heterogeneous clinical inputs and outperforms other diagnostic methods across various datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.   DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor.", 'score': 5, 'issue_id': 4517, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': 'f12b8efd117ae9ab', 'authors': ['Weike Zhao', 'Chaoyi Wu', 'Yanjie Fan', 'Xiaoman Zhang', 'Pengcheng Qiu', 'Yuze Sun', 'Xiao Zhou', 'Yanfeng Wang', 'Ya Zhang', 'Yongguo Yu', 'Kun Sun', 'Weidi Xie'], 'affiliations': ['Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'Shanghai Jiao Tong University, Shanghai, China', 'Xinhua Hospital affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.20430.jpg', 'data': {'categories': ['#agents', '#reasoning', '#benchmark', '#science', '#dataset', '#healthcare'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'DeepRare: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'DeepRare - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ…Ğ¾ÑÑ‚Ğ° Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. DeepRare Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 100% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ 1013 Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Rare Disease Diagnosis with DeepRare', 'desc': 'DeepRare is a novel system that utilizes a large language model to diagnose rare diseases by analyzing diverse clinical data. It generates ranked hypotheses for potential diagnoses, providing clear reasoning linked to medical evidence. The system is modular, featuring a long-term memory and specialized agents that integrate over 40 tools and current medical knowledge. DeepRare outperforms traditional diagnostic methods, achieving high accuracy and recall rates across multiple datasets, making it a significant advancement in rare disease diagnosis.'}, 'zh': {'title': 'DeepRareï¼šç²¾å‡†è¯Šæ–­ç½•è§ç–¾ç—…çš„æ™ºèƒ½ç³»ç»Ÿ', 'desc': 'DeepRareæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤šç§ä¸´åºŠè¾“å…¥æä¾›å‡†ç¡®çš„ç½•è§ç–¾ç—…è¯Šæ–­ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç”Ÿæˆæ’åçš„è¯Šæ–­å‡è®¾ï¼Œå¹¶æä¾›é€æ˜çš„æ¨ç†é“¾ï¼Œç¡®ä¿æ¯ä¸€æ­¥åˆ†æéƒ½ä¸å¯éªŒè¯çš„åŒ»å­¦è¯æ®ç›¸è¿æ¥ã€‚DeepRareçš„è®¾è®¡æ¨¡å—åŒ–ä¸”å¯æ‰©å±•ï¼Œé›†æˆäº†è¶…è¿‡40ç§ä¸“ä¸šå·¥å…·å’Œæœ€æ–°çš„åŒ»å­¦çŸ¥è¯†æ¥æºï¼Œç¡®ä¿è·å–æœ€æ–°çš„ä¸´åºŠä¿¡æ¯ã€‚ç»è¿‡è¯„ä¼°ï¼ŒDeepRareåœ¨2919ç§ç–¾ç—…ä¸­è¡¨ç°å‡ºè‰²ï¼Œ1013ç§ç–¾ç—…çš„è¯Šæ–­å‡†ç¡®ç‡è¾¾åˆ°100%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21272', 'title': 'FairyGen: Storied Cartoon Video from a Single Child-Drawn Character', 'url': 'https://huggingface.co/papers/2506.21272', 'abstract': "FairyGen generates story-driven cartoon videos from a single drawing by disentangling character modeling and background styling, employing MLLM for storyboards, style propagation for consistency, and MMDiT-based diffusion models for motion.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FairyGen, an automatic system for generating story-driven cartoon videos from a single child's drawing, while faithfully preserving its unique artistic style. Unlike previous storytelling methods that primarily focus on character consistency and basic motion, FairyGen explicitly disentangles character modeling from stylized background generation and incorporates cinematic shot design to support expressive and coherent storytelling. Given a single character sketch, we first employ an MLLM to generate a structured storyboard with shot-level descriptions that specify environment settings, character actions, and camera perspectives. To ensure visual consistency, we introduce a style propagation adapter that captures the character's visual style and applies it to the background, faithfully retaining the character's full visual identity while synthesizing style-consistent scenes. A shot design module further enhances visual diversity and cinematic quality through frame cropping and multi-view synthesis based on the storyboard. To animate the story, we reconstruct a 3D proxy of the character to derive physically plausible motion sequences, which are then used to fine-tune an MMDiT-based image-to-video diffusion model. We further propose a two-stage motion customization adapter: the first stage learns appearance features from temporally unordered frames, disentangling identity from motion; the second stage models temporal dynamics using a timestep-shift strategy with frozen identity weights. Once trained, FairyGen directly renders diverse and coherent video scenes aligned with the storyboard. Extensive experiments demonstrate that our system produces animations that are stylistically faithful, narratively structured natural motion, highlighting its potential for personalized and engaging story animation. The code will be available at https://github.com/GVCLab/FairyGen", 'score': 4, 'issue_id': 4525, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '912ef8f5a86d8a2c', 'authors': ['Jiayi Zheng', 'Xiaodong Cun'], 'affiliations': ['GVC Lab, Great Bay University, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.21272.jpg', 'data': {'categories': ['#multimodal', '#3d', '#video', '#diffusion', '#story_generation'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞÑ‚ Ğ´ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ° Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ñƒ: FairyGen Ğ¾Ğ¶Ğ¸Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ', 'desc': 'FairyGen - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ´ĞµÑ‚ÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ¸ÑÑƒĞ½ĞºÑƒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ°Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞºĞ°Ğ´Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ğ½Ğ° Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MMDiT Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. FairyGen Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Transforming Drawings into Animated Stories with FairyGen', 'desc': "FairyGen is an innovative system that creates cartoon videos from a single drawing while maintaining the original artistic style. It separates character modeling from background styling, allowing for more coherent storytelling and visual consistency. The system uses a multi-level language model (MLLM) to generate detailed storyboards and employs a style propagation adapter to ensure that the character's style is preserved in the background. Additionally, it utilizes a two-stage motion customization process to create realistic animations, resulting in engaging and personalized video content."}, 'zh': {'title': 'ä»ä¸€å¹…ç”»ç”Ÿæˆæ•…äº‹åŠ¨ç”»çš„é­”æ³•', 'desc': 'FairyGen æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆæ•…äº‹é©±åŠ¨å¡é€šè§†é¢‘çš„ç³»ç»Ÿï¼Œåªéœ€ä¸€å¹…å„¿ç«¥ç”»å³å¯ï¼ŒåŒæ—¶ä¿ç•™å…¶ç‹¬ç‰¹çš„è‰ºæœ¯é£æ ¼ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†è§’è‰²å»ºæ¨¡ä¸èƒŒæ™¯é£æ ¼ç”Ÿæˆåˆ†ç¦»ï¼Œç»“åˆ MLLM ç”Ÿæˆç»“æ„åŒ–æ•…äº‹æ¿ï¼Œç¡®ä¿è§†è§‰ä¸€è‡´æ€§ã€‚å®ƒè¿˜ä½¿ç”¨ MMDiT åŸºäºæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨åºåˆ—ï¼Œä»è€Œå®ç°åŠ¨ç”»æ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼ŒFairyGen èƒ½å¤Ÿç”Ÿæˆé£æ ¼ä¸€è‡´ã€å™äº‹ç»“æ„æ¸…æ™°çš„è‡ªç„¶è¿åŠ¨åŠ¨ç”»ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸ªæ€§åŒ–æ•…äº‹åŠ¨ç”»ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21263', 'title': 'DiLoCoX: A Low-Communication Large-Scale Training Framework for\n  Decentralized Cluster', 'url': 'https://huggingface.co/papers/2506.21263', 'abstract': 'DiLoCoX, a decentralized cluster training framework, enhances the training of large-scale models over slow networks by utilizing pipeline parallelism, dual optimizer policy, and gradient compression, achieving significant speed improvements and effective scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters.', 'score': 3, 'issue_id': 4527, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '5ee2d59ef586a49c', 'authors': ['Ji Qi', 'WenPeng Zhu', 'Li Li', 'Ming Wu', 'YingJun Wu', 'Wu He', 'Xun Gao', 'Jason Zeng', 'Michael Heinrich'], 'affiliations': ['China Mobile(Suzhou) Software Technology, JiangSu', 'Zero Gravity Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.21263.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'DiLoCoX: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DiLoCoX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼, Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². DiLoCoX Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° ÑĞµÑ‚ÑÑ… ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ 1 Ğ“Ğ±Ğ¸Ñ‚/Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 357-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ AllReduce.'}, 'en': {'title': 'Revolutionizing Large-Scale Model Training on Slow Networks', 'desc': 'DiLoCoX is a decentralized training framework designed to improve the training of large-scale models, especially those with over 100 billion parameters, over slow networks. It employs techniques like pipeline parallelism and a dual optimizer policy to enhance communication efficiency and speed. The framework also introduces an adaptive gradient compression scheme, which helps in reducing the amount of data that needs to be communicated during training. Empirical results show that DiLoCoX can achieve a remarkable 357x speedup in distributed training compared to traditional methods, while still ensuring effective model convergence.'}, 'zh': {'title': 'å»ä¸­å¿ƒåŒ–é›†ç¾¤è®­ç»ƒçš„é€Ÿåº¦é©å‘½', 'desc': 'DiLoCoXæ˜¯ä¸€ç§å»ä¸­å¿ƒåŒ–çš„é›†ç¾¤è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹åœ¨æ…¢é€Ÿç½‘ç»œä¸Šçš„è®­ç»ƒæ•ˆç‡ã€‚å®ƒç»“åˆäº†ç®¡é“å¹¶è¡Œã€åŒä¼˜åŒ–å™¨ç­–ç•¥å’Œæ¢¯åº¦å‹ç¼©ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒé€Ÿåº¦å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯éªŒè¯ï¼ŒDiLoCoXèƒ½å¤Ÿåœ¨1Gbpsç½‘ç»œä¸ŠæˆåŠŸé¢„è®­ç»ƒè¶…è¿‡107äº¿å‚æ•°çš„åŸºç¡€æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„AllReduceæ–¹æ³•ç›¸æ¯”ï¼ŒDiLoCoXåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å®ç°äº†357å€çš„åŠ é€Ÿï¼ŒåŒæ—¶æ¨¡å‹æ”¶æ•›æ€§å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17533', 'title': 'DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for\n  Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2506.17533', 'abstract': "A novel reward modeling framework DuaShepherd integrates correctness and potential signals into a unified multi-head architecture to enhance LLMs' mathematical reasoning capabilities and achieve state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we propose DuaShepherd, a novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. A unified, multi-head architecture was explored to train the two reward models in a multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into a compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints.", 'score': 2, 'issue_id': 4533, 'pub_date': '2025-06-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ½Ñ', 'en': 'June 21', 'zh': '6æœˆ21æ—¥'}, 'hash': '4beafb969a7354be', 'authors': ['Yuanhao Wu', 'Juntong Song', 'Hanning Zhang', 'Tong Zhang', 'Cheng Niu'], 'affiliations': ['NewsBreak', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.17533.jpg', 'data': {'categories': ['#training', '#dataset', '#reasoning', '#optimization', '#data', '#benchmark', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ', 'desc': 'DuaShepherd - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² - ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» - Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ğ±Ğ¾Ğ¸Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ñ‚Ğ¸Ğ¿Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ….'}, 'en': {'title': "Enhancing LLMs' Math Skills with DuaShepherd", 'desc': 'The paper introduces DuaShepherd, a new framework for reward modeling that enhances the mathematical reasoning abilities of Large Language Models (LLMs). It combines two types of reward signals: correctness, which focuses on identifying errors in reasoning steps, and potential, which assesses the likelihood of arriving at the correct answer. The authors created an automated system to build a large dataset that incorporates both signals and employed a multi-head architecture to train the models simultaneously. This approach leads to improved performance on various benchmarks, demonstrating that using both signals together yields better results than using either one alone.'}, 'zh': {'title': 'DuaShepherdï¼šæå‡æ•°å­¦æ¨ç†çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶DuaShepherdï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ•´åˆäº†æ­£ç¡®æ€§å’Œæ½œåŠ›ä¸¤ç§äº’è¡¥çš„å¥–åŠ±ä¿¡å·ï¼Œä»¥æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æµç¨‹ï¼Œç”¨äºæ„å»ºåŒ…å«è¿™ä¸¤ç§ä¿¡å·çš„å¤§è§„æ¨¡å¥–åŠ±å»ºæ¨¡æ•°æ®é›†ã€‚é€šè¿‡åœ¨å¤šä»»åŠ¡è®¾ç½®ä¸­è®­ç»ƒè¿™ä¸¤ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.18729', 'title': 'MuseControlLite: Multifunctional Music Generation with Lightweight\n  Conditioners', 'url': 'https://huggingface.co/papers/2506.18729', 'abstract': 'Rotary positional embeddings enhance time-varying control in text-to-music generation models with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose MuseControlLite, a lightweight mechanism designed to fine-tune text-to-music generation models for precise conditioning using various time-varying musical attributes and reference audio signals. The key finding is that positional embeddings, which have been seldom used by text-to-music generation models in the conditioner for text conditions, are critical when the condition of interest is a function of time. Using melody control as an example, our experiments show that simply adding rotary positional embeddings to the decoupled cross-attention layers increases control accuracy from 56.6% to 61.1%, while requiring 6.75 times fewer trainable parameters than state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion Transformer model of Stable Audio Open. We evaluate various forms of musical attribute control, audio inpainting, and audio outpainting, demonstrating improved controllability over MusicGen-Large and Stable Audio Open ControlNet at a significantly lower fine-tuning cost, with only 85M trainble parameters. Source code, model checkpoints, and demo examples are available at: https://musecontrollite.github.io/web/.', 'score': 1, 'issue_id': 4524, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ½Ñ', 'en': 'June 23', 'zh': '6æœˆ23æ—¥'}, 'hash': 'b1d008a79f59af7d', 'authors': ['Fang-Duo Tsai', 'Shih-Lun Wu', 'Weijaw Lee', 'Sheng-Ping Yang', 'Bo-Rui Chen', 'Hao-Chung Cheng', 'Yi-Hsuan Yang'], 'affiliations': ['Massachusetts Institute of Technology, Cambridge, MA, United States', 'National Taiwan University, Taipei, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2506.18729.jpg', 'data': {'categories': ['#optimization', '#audio', '#diffusion', '#training', '#games', '#data'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ›ĞµĞ³ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MuseControlLite - Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ ÑÑ‚Ğ°Ğ»Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ»Ğ¾ÑÑ… Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ 56.6% Ğ´Ğ¾ 61.1%, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 6.75 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². MuseControlLite Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ MusicGen-Large Ğ¸ Stable Audio Open ControlNet Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Enhancing Music Generation with Efficient Positional Control', 'desc': 'This paper introduces MuseControlLite, a streamlined approach for enhancing text-to-music generation models by incorporating rotary positional embeddings. These embeddings are crucial for managing time-varying musical attributes, allowing for more precise control over generated music. The study demonstrates that adding these embeddings to cross-attention layers improves control accuracy significantly while reducing the number of trainable parameters needed. Overall, MuseControlLite offers a cost-effective solution for fine-tuning music generation models, achieving better performance with fewer resources.'}, 'zh': {'title': 'æ—‹è½¬ä½ç½®åµŒå…¥æå‡æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆçš„æ§åˆ¶èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMuseControlLiteçš„è½»é‡çº§æœºåˆ¶ï¼Œæ—¨åœ¨é€šè¿‡æ—¶é—´å˜åŒ–çš„éŸ³ä¹å±æ€§å’Œå‚è€ƒéŸ³é¢‘ä¿¡å·æ¥ç²¾ç»†è°ƒæ•´æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆæ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œæ—‹è½¬ä½ç½®åµŒå…¥åœ¨æ–‡æœ¬æ¡ä»¶çš„è°ƒèŠ‚å™¨ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯å½“æ¡ä»¶ä¸æ—¶é—´ç›¸å…³æ—¶ã€‚é€šè¿‡åœ¨è§£è€¦çš„äº¤å‰æ³¨æ„åŠ›å±‚ä¸­æ·»åŠ æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œæ§åˆ¶ç²¾åº¦ä»56.6%æé«˜åˆ°61.1%ï¼Œä¸”æ‰€éœ€çš„å¯è®­ç»ƒå‚æ•°æ¯”æœ€å…ˆè¿›çš„å¾®è°ƒæœºåˆ¶å°‘6.75å€ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†å¤šç§éŸ³ä¹å±æ€§æ§åˆ¶å½¢å¼ï¼Œå±•ç¤ºäº†åœ¨è¾ƒä½çš„å¾®è°ƒæˆæœ¬ä¸‹ï¼ŒMuseControlLiteåœ¨å¯æ§æ€§æ–¹é¢çš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.15196', 'title': 'HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges', 'url': 'https://huggingface.co/papers/2506.15196', 'abstract': "HeurAgenix, a two-stage hyper-heuristic framework using large language models, evolves and selects heuristics dynamically for combinatorial optimization problems, achieving performance on par with specialized solvers.  \t\t\t\t\tAI-generated summary \t\t\t\t Heuristic algorithms play a vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce HeurAgenix, a two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLM's perception ability. For flexibility, this selector can be either a state-of-the-art LLM or a fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with a dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at https://github.com/microsoft/HeurAgenix.", 'score': 1, 'issue_id': 4523, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'}, 'hash': '464747c8cdf8780d', 'authors': ['Xianliang Yang', 'Ling Zhang', 'Haolong Qian', 'Lei Song', 'Jiang Bian'], 'affiliations': ['Microsoft Research Asia, Beijing, China', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.15196.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization', '#agents', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'HeurAgenix - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. HeurAgenix Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ğ° ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ….'}, 'en': {'title': 'Dynamic Heuristic Evolution and Selection with LLMs', 'desc': "HeurAgenix is a novel two-stage hyper-heuristic framework that utilizes large language models (LLMs) to enhance the solving of combinatorial optimization (CO) problems. The framework first evolves heuristics by comparing initial solutions with better ones, extracting effective strategies for improvement. In the second stage, it dynamically selects the most suitable heuristic for each problem state, leveraging the LLM's ability to perceive and adapt. This approach not only improves performance compared to traditional methods but also rivals specialized solvers, demonstrating the potential of LLMs in optimization tasks."}, 'zh': {'title': 'åŠ¨æ€æ¼”åŒ–ä¸é€‰æ‹©å¯å‘å¼ç®—æ³•çš„åˆ›æ–°æ¡†æ¶', 'desc': 'HeurAgenix æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸¤é˜¶æ®µè¶…å¯å‘å¼æ¡†æ¶ï¼Œæ—¨åœ¨åŠ¨æ€æ¼”åŒ–å’Œé€‰æ‹©å¯å‘å¼ç®—æ³•ä»¥è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡æ¯”è¾ƒç§å­å¯å‘å¼è§£ä¸é«˜è´¨é‡è§£ï¼Œæå–å¯é‡ç”¨çš„æ¼”åŒ–ç­–ç•¥ã€‚ç„¶åï¼Œåœ¨è§£å†³é—®é¢˜æ—¶ï¼ŒHeurAgenix æ ¹æ®å½“å‰çŠ¶æ€åŠ¨æ€é€‰æ‹©æœ€æœ‰å‰æ™¯çš„å¯å‘å¼ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHeurAgenix çš„æ€§èƒ½ä¸ä¸“é—¨çš„æ±‚è§£å™¨ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†ç°æœ‰çš„åŸºäº LLM çš„è¶…å¯å‘å¼ç®—æ³•ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (49)', '#agents (87)', '#agi (23)', '#alignment (48)', '#architecture (110)', '#audio (27)', '#benchmark (272)', '#cv (112)', '#data (103)', '#dataset (234)', '#diffusion (93)', '#ethics (22)', '#games (89)', '#graphs (6)', '#hallucinations (36)', '#healthcare (19)', '#inference (59)', '#interpretability (66)', '#leakage (4)', '#long_context (54)', '#low_resource (22)', '#machine_translation (7)', '#math (26)', '#multilingual (25)', '#multimodal (212)', '#open_source (138)', '#optimization (301)', '#plp (3)', '#rag (22)', '#reasoning (196)', '#rl (98)', '#rlhf (44)', '#robotics (23)', '#science (38)', '#security (28)', '#small_models (25)', '#story_generation (8)', '#survey (19)', '#synthetic (56)', '#training (296)', '#transfer_learning (56)', '#video (84)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-06-30 08:17',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-30 08:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-30 08:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    