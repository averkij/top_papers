
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. April 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Апрель 2025</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-03.html">⬅️ <span id="prev-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-05.html">➡️ <span id="next-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Апрель 2025', 'en': 'April 2025', 'zh': '4月2025年'};
        let feedDateNext = {'ru': '05.2025', 'en': '05/2025', 'zh': '5月2025年'};
        let feedDatePrev = {'ru': '03.2025', 'en': '03/2025', 'zh': '3月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.19693', 'title': 'AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation', 'url': 'https://huggingface.co/papers/2503.19693', 'abstract': 'Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance', 'score': 52, 'issue_id': 2976, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '2c907e98a0aafb46', 'authors': ['Itay Nakash', 'Nitay Calderon', 'Eyal Ben David', 'Elad Hoffer', 'Roi Reichart'], 'affiliations': ['Habana Labs', 'The Faculty of Data and Decisions Science, Technion - IIT'], 'pdf_title_img': 'assets/pdf/title_img/2503.19693.jpg', 'data': {'categories': ['#architecture', '#low_resource', '#training', '#optimization', '#data', '#transfer_learning'], 'emoji': '🗜️', 'ru': {'title': 'Адаптация словаря для повышения эффективности LLM в специализированных областях', 'desc': 'AdaptiVocab - это новый подход к адаптации словаря больших языковых моделей (LLM) для повышения их эффективности в узкоспециализированных областях. Метод заменяет токены в словаре на специфичные для домена n-граммы, что сокращает количество токенов, необходимых для обработки входных данных и генерации выходных. AdaptiVocab инициализирует новые вложения n-токенов с помощью экспоненциально взвешенной комбинации существующих вложений и использует легковесную фазу дообучения. Эксперименты показали, что AdaptiVocab уменьшает использование токенов более чем на 25% без ухудшения производительности.'}, 'en': {'title': 'Enhancing LLM Efficiency with Domain-Specific Vocabulary', 'desc': 'This paper presents AdaptiVocab, a method for improving the efficiency of Large Language Models (LLMs) in specific domains by adapting their vocabulary. Instead of using a general-purpose vocabulary, AdaptiVocab replaces tokens with domain-specific n-gram-based tokens, which reduces the number of tokens needed for processing and generation. The approach involves initializing new embeddings through a combination of existing ones and includes a lightweight fine-tuning process that can be done on a single GPU. The results demonstrate that AdaptiVocab can decrease token usage by over 25% while maintaining the quality of generated outputs and overall task performance.'}, 'zh': {'title': '提高大型语言模型效率的新方法', 'desc': '大型语言模型（LLMs）在通用模型中展现了令人印象深刻的多功能性，但其广泛应用伴随着高昂的计算开销，尤其是在自回归解码中，每一步都需要进行前向传播。针对特定领域的应用，通用能力并非必要，可以通过提高效率来进行交换。我们提出了一种新颖的领域适应方法——AdaptiVocab，通过调整词汇表来降低延迟和计算成本，旨在提高LLM在低资源领域的效率。AdaptiVocab可以应用于任何分词器和架构，通过用特定领域的n-gram代替原有的tokens，减少输入处理和输出生成所需的tokens数量。'}}}, {'id': 'https://huggingface.co/papers/2503.22230', 'title': 'Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback', 'url': 'https://huggingface.co/papers/2503.22230', 'abstract': "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.", 'score': 27, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': 'a994668c51ac51c4', 'authors': ['Wei Shen', 'Guanlin Liu', 'Zheng Wu', 'Ruofei Zhu', 'Qingping Yang', 'Chao Xin', 'Yu Yue', 'Lin Yan'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2503.22230.jpg', 'data': {'categories': ['#rlhf', '#data', '#reasoning', '#alignment', '#training'], 'emoji': '🧠', 'ru': {'title': 'Усовершенствование RLHF: данные и разнообразие ответов как ключ к успеху', 'desc': 'Статья исследует важность конструирования данных в обучении с подкреплением на основе обратной связи от человека (RLHF) для больших языковых моделей. Авторы предлагают гибридную систему вознаграждений, сочетающую верификаторы задач рассуждения (RTV) и генеративную модель вознаграждений (GenRM), для смягчения проблемы обмана вознаграждений. Они также вводят метод Pre-PPO для поддержания разнообразия ответов и повышения эффективности обучения. Результаты показывают, что предложенные методы значительно улучшают производительность RLHF, особенно в математических задачах и задачах кодирования.'}, 'en': {'title': 'Enhancing RLHF: Bridging Data Gaps for Better AI Alignment', 'desc': 'This paper focuses on improving Reinforcement Learning from Human Feedback (RLHF) for large language models by addressing issues in prompt-data construction. It identifies problems like reward hacking and reduced response diversity that hinder RLHF performance. The authors propose a hybrid reward system that combines reasoning task verifiers (RTV) with a generative reward model (GenRM) to counteract these issues. Additionally, they introduce a new prompt-selection method called Pre-PPO and emphasize the importance of prioritizing mathematical and coding tasks during training to enhance overall model performance.'}, 'zh': {'title': '优化人类反馈的强化学习方法', 'desc': '强化学习中的人类反馈（RLHF）对于使大型语言模型与人类偏好对齐至关重要。尽管近期研究集中在算法改进上，但提示数据构建的重要性却被忽视。本文探讨了RLHF性能扩展中的数据驱动瓶颈，特别是奖励黑客和响应多样性下降的问题。我们提出了一种混合奖励系统，结合推理任务验证器（RTV）和生成奖励模型（GenRM），以减轻奖励黑客现象，并提出了一种新颖的提示选择方法Pre-PPO，以保持响应多样性并增强学习效果。'}}}, {'id': 'https://huggingface.co/papers/2503.22675', 'title': 'Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation', 'url': 'https://huggingface.co/papers/2503.22675', 'abstract': "Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose ReaRec, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\\%-50\\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.", 'score': 25, 'issue_id': 2971, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '092ab2fa75277891', 'authors': ['Jiakai Tang', 'Sunhao Dai', 'Teng Shi', 'Jun Xu', 'Xu Chen', 'Wen Chen', 'Wu Jian', 'Yuning Jiang'], 'affiliations': ['Alibaba Group, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.22675.jpg', 'data': {'categories': ['#inference', '#reasoning', '#dataset', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Многошаговое рассуждение для улучшения рекомендаций', 'desc': 'ReaRec - это новый фреймворк для систем рекомендаций, использующий многошаговое рассуждение во время вывода для улучшения представления пользователей. Он применяет авторегрессивную подачу последнего скрытого состояния последовательности в рекомендательную систему, используя специальные эмбеддинги позиций рассуждения. Авторы также предлагают два метода обучения на основе рассуждений: Ensemble Reasoning Learning (ERL) и Progressive Reasoning Learning (PRL). Эксперименты показывают, что ReaRec значительно повышает эффективность различных архитектур последовательных рекомендательных систем.'}, 'en': {'title': 'ReaRec: Elevating Sequential Recommendations with Multi-Step Reasoning', 'desc': 'This paper introduces ReaRec, a novel framework for Sequential Recommendation (SeqRec) that enhances user representation through multi-step reasoning. Traditional methods often rely on a single forward computation, which limits their ability to capture the evolving nature of user preferences and understand less popular items. ReaRec addresses these limitations by using autoregressive techniques and special embeddings to improve the inference process. The proposed framework, along with two learning methods, shows significant performance improvements across various datasets, suggesting a new direction for research in recommendation systems.'}, 'zh': {'title': 'ReaRec：提升顺序推荐的推理能力', 'desc': '顺序推荐（SeqRec）旨在通过捕捉用户历史交互中的顺序模式来预测下一个项目，这在许多现实世界的推荐系统中起着关键作用。现有方法主要采用直接的前向计算范式，最终的隐藏状态作为用户表示，但这种方法在建模用户偏好的复杂演变方面存在局限。为了解决这个问题，我们提出了ReaRec，这是第一个用于推荐系统的推理时计算框架，通过隐式多步推理增强用户表示。我们的实验表明，ReaRec显著提高了多个顺序推荐模型的性能，开辟了推理时计算的新研究方向。'}}}, {'id': 'https://huggingface.co/papers/2503.21614', 'title': 'A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond', 'url': 'https://huggingface.co/papers/2503.21614', 'abstract': 'Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.', 'score': 21, 'issue_id': 2976, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '805b7cd4ec307c34', 'authors': ['Xiaoye Qu', 'Yafu Li', 'Zhaochen Su', 'Weigao Sun', 'Jianhao Yan', 'Dongrui Liu', 'Ganqu Cui', 'Daizong Liu', 'Shuxian Liang', 'Junxian He', 'Peng Li', 'Wei Wei', 'Jing Shao', 'Chaochao Lu', 'Yue Zhang', 'Xian-Sheng Hua', 'Bowen Zhou', 'Yu Cheng'], 'affiliations': ['Huazhong University of Science and Technology', 'Peking University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'Tongji University', 'Tsinghua University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21614.jpg', 'data': {'categories': ['#reasoning', '#inference', '#training', '#survey', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация рассуждений в крупных языковых моделях', 'desc': 'Этот обзор посвящен повышению эффективности рассуждений в крупных моделях рассуждений (LRM). Авторы рассматривают проблему избыточно длинных цепочек рассуждений, которые часто содержат повторяющийся контент и излишний анализ. Обсуждаются методы улучшения эффективности на всех этапах жизненного цикла LRM - от предварительного обучения до вывода. Статья также предлагает перспективные направления для будущих исследований в этой быстро развивающейся области.'}, 'en': {'title': 'Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses the challenges faced by Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI o1, particularly their tendency to generate long and redundant reasoning processes. These inefficiencies can complicate training and real-world applications, especially where efficient use of tokens is crucial. The authors review various strategies aimed at enhancing reasoning efficiency throughout the LRM lifecycle, from pretraining to inference. They also provide a GitHub repository to track advancements in this field, aiming to inspire further research and innovation.'}, 'zh': {'title': '提升推理效率，助力大型模型发展', 'desc': '最近的大型推理模型（LRMs），如DeepSeek-R1和OpenAI o1，通过扩展推理链（CoT）的长度在推理过程中取得了显著的性能提升。然而，它们往往生成过长的推理过程，内容冗余（例如，重复定义）、对简单问题的过度分析，以及对复杂任务的多条推理路径的表面探索。这种低效性在训练、推理和实际应用中引发了重大挑战，尤其是在代理系统中，令牌经济至关重要。本文综述了改善LRMs推理效率的最新努力，识别了常见的低效模式，并探讨了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2503.16081', 'title': 'OThink-MR1: Stimulating multimodal generalized reasoning capabilities\n  via dynamic reinforcement learning', 'url': 'https://huggingface.co/papers/2503.16081', 'abstract': 'Multimodal Large Language Models (MLLMs) have gained significant traction for their ability to process diverse input data types and generate coherent, contextually relevant outputs across various applications. While supervised fine-tuning (SFT) has been the predominant approach to enhance MLLM capabilities in task-specific optimization, it often falls short in fostering crucial generalized reasoning abilities. Although reinforcement learning (RL) holds great promise in overcoming these limitations, it encounters two significant challenges: (1) its generalized capacities in multimodal tasks remain largely unexplored, and (2) its training constraints, including the constant Kullback-Leibler divergence or the clamp strategy, often result in suboptimal bottlenecks. To address these challenges, we propose OThink-MR1, an advanced MLLM equipped with profound comprehension and reasoning capabilities across multimodal tasks. Specifically, we introduce Group Relative Policy Optimization with a dynamic Kullback-Leibler strategy (GRPO-D), which markedly enhances reinforcement learning (RL) performance. For Qwen2-VL-2B-Instruct, GRPO-D achieves a relative improvement of more than 5.72% over SFT and more than 13.59% over GRPO in same-task evaluation on two adapted datasets. Furthermore, GRPO-D demonstrates remarkable cross-task generalization capabilities, with an average relative improvement of more than 61.63% over SFT in cross-task evaluation. These results highlight that the MLLM trained with GRPO-D on one multimodal task can be effectively transferred to another task, underscoring the superior generalized reasoning capabilities of our proposed OThink-MR1 model.', 'score': 18, 'issue_id': 2981, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '5d09e443b3a0dafb', 'authors': ['Zhiyuan Liu', 'Yuting Zhang', 'Feng Liu', 'Changwang Zhang', 'Ying Sun', 'Jun Wang'], 'affiliations': ['OPPO Research Institute Shenzhen, China', 'The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16081.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#transfer_learning', '#multimodal', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Революция в мультимодальном ИИ: OThink-MR1 с GRPO-D превосходит ожидания', 'desc': 'Статья представляет OThink-MR1 - усовершенствованную мультимодальную большую языковую модель (MLLM) с улучшенными способностями понимания и рассуждения. Авторы предлагают новый метод обучения с подкреплением - Group Relative Policy Optimization с динамической стратегией Кульбака-Лейблера (GRPO-D). GRPO-D показывает значительное улучшение производительности по сравнению с обычным обучением с учителем и другими методами RL. Модель демонстрирует впечатляющие способности к обобщению при переносе на новые задачи.'}, 'en': {'title': 'Unlocking Generalized Reasoning in Multimodal Models with GRPO-D', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) that can handle different types of data and produce relevant outputs. It highlights the limitations of traditional supervised fine-tuning (SFT) in developing generalized reasoning skills. The authors introduce a new model called OThink-MR1, which utilizes a novel reinforcement learning approach named Group Relative Policy Optimization with a dynamic Kullback-Leibler strategy (GRPO-D). This method significantly improves performance in both same-task and cross-task evaluations, demonstrating enhanced reasoning capabilities across various multimodal tasks.'}, 'zh': {'title': '提升多模态推理能力的创新方法', 'desc': '多模态大型语言模型（MLLMs）在处理不同类型输入数据和生成连贯、相关的输出方面取得了显著进展。虽然监督微调（SFT）是提升MLLM任务特定优化能力的主要方法，但它在促进通用推理能力方面常常不足。为了解决这一问题，我们提出了OTink-MR1模型，采用了动态Kullback-Leibler策略的群体相对策略优化（GRPO-D），显著提升了强化学习（RL）的性能。实验结果表明，使用GRPO-D训练的MLLM在跨任务评估中表现出色，显示出其优越的通用推理能力。'}}}, {'id': 'https://huggingface.co/papers/2503.22194', 'title': 'ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2503.22194', 'abstract': 'We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.', 'score': 17, 'issue_id': 2971, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '6d93cc886c16f9b0', 'authors': ['Yunhong Min', 'Daehyeon Choi', 'Kyeongmin Yeo', 'Jihyun Lee', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.22194.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#3d'], 'emoji': '🧭', 'ru': {'title': 'Точное управление 3D ориентацией объектов при генерации изображений', 'desc': 'ORIGEN - это первый метод для определения 3D ориентации объектов при генерации изображений из текста без предварительного обучения. Он использует предобученную дискриминативную модель для оценки 3D ориентации и одношаговую генеративную модель текст-изображение. Вместо градиентного спуска применяется семплирование на основе динамики Ланжевена с добавлением случайного шума. Эксперименты показывают превосходство ORIGEN над другими методами по количественным метрикам и оценкам пользователей.'}, 'en': {'title': 'Revolutionizing 3D Orientation in Text-to-Image Generation with ORIGEN', 'desc': 'ORIGEN is a novel method that enables zero-shot 3D orientation grounding in text-to-image generation, allowing for better control over how objects are oriented in three-dimensional space. Unlike previous methods that focused on 2D positioning, ORIGEN utilizes a reward-guided sampling approach that leverages a pretrained model for estimating 3D orientations. This method incorporates Langevin dynamics to enhance image realism while maintaining effective sampling, requiring minimal code changes. Experimental results demonstrate that ORIGEN surpasses existing training-based and test-time guidance techniques in both quantitative metrics and user evaluations.'}, 'zh': {'title': 'ORIGEN：文本到图像生成中的3D方向定位新方法', 'desc': '我们介绍了ORIGEN，这是首个用于文本到图像生成中实现3D方向定位的零样本方法。以往的研究主要集中在2D定位上，缺乏对3D方向的控制。为了解决这个问题，我们提出了一种基于奖励引导的采样方法，利用预训练的判别模型进行3D方向估计，并结合一步文本到图像生成流模型。实验结果表明，ORIGEN在定量指标和用户研究中均优于基于训练和测试时引导的方法。'}}}, {'id': 'https://huggingface.co/papers/2503.21332', 'title': 'ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback', 'url': 'https://huggingface.co/papers/2503.21332', 'abstract': 'Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released.', 'score': 17, 'issue_id': 2977, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '4797a440ca75521b', 'authors': ['Taewon Yun', 'Jihwan Oh', 'Hyangsuk Min', 'Yuho Lee', 'Jihwan Bang', 'Jason Cai', 'Hwanjun Song'], 'affiliations': ['Amazon Web Services, AI Labs', 'Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2503.21332.jpg', 'data': {'categories': ['#dataset', '#long_context', '#training', '#data', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'Улучшение многомерной суммаризации через рефлексивное рассуждение', 'desc': 'В этой статье представлен ReFeed - мощный конвейер для улучшения суммаризации по нескольким параметрам с использованием рефлексивного рассуждения на основе обратной связи. Авторы создали датасет SumFeed-CoT для обучения легковесной модели рефлексивному рассуждению. Эксперименты показали важность одновременной обработки нескольких видов обратной связи и устойчивость ReFeed к шуму. Исследование подчеркивает значимость правильно сформулированной цели и руководства при создании данных для эффективного рассуждения.'}, 'en': {'title': 'Enhancing Summarization with Reflective Reasoning', 'desc': 'This paper presents ReFeed, a novel summarization refinement pipeline that improves the quality of summaries across multiple dimensions by utilizing reflective reasoning on feedback. The authors introduce a new dataset called SumFeed-CoT, which is designed to train a lightweight model capable of handling reflective reasoning effectively. Through experiments, they demonstrate that the performance of the summarization refinement is significantly influenced by the number of dimensions, the exposure to feedback, and the reasoning policy employed. The findings suggest that addressing multiple feedback sources simultaneously is essential for optimizing performance while also being resilient to noisy feedback and variations in feedback order.'}, 'zh': {'title': '反思性推理提升多维总结精炼', 'desc': '本文介绍了一种名为ReFeed的总结精炼管道，旨在通过反思性推理来增强多维度的总结效果。我们发布了一个名为SumFeed-CoT的大规模数据集，专门用于训练轻量级模型，以实现反思性推理。实验结果表明，维度数量、反馈暴露和推理策略对精炼性能有显著影响，强调了反思性推理和同时处理多个反馈的重要性。最后，我们的研究发现，创建具有明确目标和指导的数据是有效推理的基础。'}}}, {'id': 'https://huggingface.co/papers/2503.20785', 'title': 'Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency', 'url': 'https://huggingface.co/papers/2503.20785', 'abstract': 'We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multiview videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation.', 'score': 15, 'issue_id': 2974, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '1a3973da9e51afd7', 'authors': ['Tianqi Liu', 'Zihao Huang', 'Zhaoxi Chen', 'Guangcong Wang', 'Shoukang Hu', 'Liao Shen', 'Huiqiang Sun', 'Zhiguo Cao', 'Wei Li', 'Ziwei Liu'], 'affiliations': ['Great Bay University', 'Huazhong University of Science and Technology', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20785.jpg', 'data': {'categories': ['#3d', '#multimodal', '#video'], 'emoji': '🎬', 'ru': {'title': 'От одного кадра к полноценной 4D-сцене', 'desc': 'Free4D - это новый подход к генерации 4D-сцен из одного изображения без дополнительного обучения. Метод использует предобученные фундаментальные модели для создания согласованного 4D-представления сцены. Free4D сначала анимирует входное изображение с помощью диффузионных моделей, затем применяет адаптивное руководство для пространственно-временной согласованности и модуляционное уточнение для устранения несоответствий. Результатом является 4D-представление, позволяющее рендерить сцену в реальном времени с возможностью управления.'}, 'en': {'title': 'Revolutionizing 4D Scene Generation from a Single Image', 'desc': 'Free4D is a new framework that generates 4D scenes from just one image without needing extensive tuning. Unlike previous methods that either focus on individual objects or require large datasets, Free4D uses pre-trained models to create consistent 4D representations efficiently. It employs image-to-video diffusion models to animate the input image and then refines the generated structure for spatial and temporal consistency. This approach allows for real-time rendering of 4D scenes, making it a significant step forward in scene generation technology.'}, 'zh': {'title': '从单图像生成4D场景的新突破', 'desc': '我们提出了Free4D，这是一个无需调优的框架，可以从单张图像生成4D场景。现有方法通常专注于物体级生成，导致场景级生成不可行，或者依赖于大规模多视角视频数据集进行昂贵的训练，且由于4D场景数据稀缺，泛化能力有限。我们的关键见解是提炼预训练的基础模型，以实现一致的4D场景表示，这提供了效率和泛化能力的优势。通过图像到视频的扩散模型，我们首先对输入图像进行动画处理，然后初始化4D几何结构，最终实现实时、可控的4D场景生成。'}}}, {'id': 'https://huggingface.co/papers/2503.20308', 'title': 'Perceptually Accurate 3D Talking Head Generation: New Definitions,\n  Speech-Mesh Representation, and Evaluation Metrics', 'url': 'https://huggingface.co/papers/2503.20308', 'abstract': 'Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria -- Temporal Synchronization, Lip Readability, and Expressiveness -- are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/.', 'score': 15, 'issue_id': 2979, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '1b5ec45782117fb1', 'authors': ['Lee Chae-Yeon', 'Oh Hyun-Bin', 'Han EunGi', 'Kim Sung-Bin', 'Suekyeong Nam', 'Tae-Hyun Oh'], 'affiliations': ['Dept. of Electrical Engineering, POSTECH', 'Grad. School of AI, POSTECH', 'KRAFTON', 'School of Computing, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.20308.jpg', 'data': {'categories': ['#3d', '#audio', '#training'], 'emoji': '🗣️', 'ru': {'title': 'Новое представление для реалистичной синхронизации речи и движений губ в 3D', 'desc': 'Эта статья представляет новый подход к генерации трехмерных говорящих голов, управляемых речью. Авторы вводят синхронизированное представление речи и трехмерной сетки лица, которое улучшает соответствие движений губ характеристикам речи. Это представление используется как в качестве перцептивной функции потерь при обучении моделей, так и в качестве метрики для оценки качества синхронизации губ. Эксперименты показывают значительное улучшение по трем ключевым критериям: временной синхронизации, читаемости по губам и выразительности.'}, 'en': {'title': 'Enhancing Lip Synchronization in 3D Talking Heads', 'desc': 'This paper addresses the challenges in generating 3D talking heads that accurately synchronize lip movements with speech. The authors propose that achieving Temporal Synchronization, Lip Readability, and Expressiveness is essential for realistic lip movements. They introduce a novel speech-mesh synchronized representation that effectively captures the relationship between speech signals and 3D facial movements. By integrating this representation as a perceptual loss in existing models, they demonstrate significant improvements in lip synchronization quality through experimental validation.'}, 'zh': {'title': '提升3D说话头唇部同步的关键标准', 'desc': '本研究探讨了语音驱动的3D说话头生成中的唇部同步问题。我们提出了三个关键标准：时间同步、唇部可读性和表现力，以实现感知上准确的唇部运动。为此，我们引入了一种语音-网格同步表示，能够捕捉语音信号与3D面部网格之间的复杂对应关系。实验结果表明，使用我们的感知损失训练3D说话头生成模型显著提高了唇部同步的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.21821', 'title': 'PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving', 'url': 'https://huggingface.co/papers/2503.21821', 'abstract': 'We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.', 'score': 15, 'issue_id': 2972, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '5254bbde255c8669', 'authors': ['Kaiyue Feng', 'Yilun Zhao', 'Yixin Liu', 'Tianyu Yang', 'Chen Zhao', 'John Sous', 'Arman Cohan'], 'affiliations': ['New York University', 'Notre Dame University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21821.jpg', 'data': {'categories': ['#reasoning', '#rag', '#benchmark', '#science'], 'emoji': '🔬', 'ru': {'title': 'Новый вызов для ИИ: решение сложных задач по физике', 'desc': 'PHYSICS - это новый набор данных для оценки способностей моделей машинного обучения решать задачи по физике университетского уровня. Он включает 1297 экспертно размеченных задач по шести основным разделам физики. Даже самые продвинутые языковые модели показывают ограниченную точность на этом наборе данных. Авторы провели анализ ошибок и исследовали различные стратегии промптинга и дополнения знаний с помощью RAG для определения путей улучшения моделей.'}, 'en': {'title': 'Benchmarking Physics Problem Solving with PHYSICS', 'desc': 'The paper presents PHYSICS, a benchmark designed to assess university-level physics problem solving capabilities. It includes 1297 expert-annotated problems across six fundamental physics domains, requiring both advanced knowledge and mathematical skills. The authors introduce an automated evaluation system to ensure accurate validation of model performance. Their findings reveal that even the top-performing model, o3-mini, only achieves 59.9% accuracy, indicating significant room for improvement in tackling complex scientific challenges.'}, 'zh': {'title': '物理问题解决的新基准', 'desc': '我们介绍了PHYSICS，这是一个全面的大学物理问题解决基准。它包含1297个专家注释的问题，涵盖经典力学、量子力学、热力学与统计力学、电磁学、原子物理和光学六个核心领域。每个问题都需要高级物理知识和数学推理能力。我们的评估显示，当前最先进的模型o3-mini的准确率仅为59.9%，这突显了解决高水平科学问题的重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.19108', 'title': 'Your ViT is Secretly an Image Segmentation Model', 'url': 'https://huggingface.co/papers/2503.19108', 'abstract': 'Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: https://www.tue-mps.org/eomt/.', 'score': 11, 'issue_id': 2980, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '96e8b32d843c2265', 'authors': ['Tommie Kerssies', 'Niccolò Cavagnero', 'Alexander Hermans', 'Narges Norouzi', 'Giuseppe Averta', 'Bastian Leibe', 'Gijs Dubbelman', 'Daan de Geus'], 'affiliations': ['Eindhoven University of Technology', 'Polytechnic of Turin', 'RWTH Aachen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19108.jpg', 'data': {'categories': ['#cv', '#architecture', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Простота и эффективность: ViT для сегментации изображений без лишних компонентов', 'desc': 'Статья представляет новый подход к сегментации изображений с использованием Vision Transformers (ViT). Авторы предлагают архитектуру Encoder-only Mask Transformer (EoMT), которая адаптирует стандартную модель ViT для задачи сегментации без добавления специфичных компонентов. EoMT демонстрирует точность сегментации, сравнимую с современными методами, но работает значительно быстрее благодаря своей простоте. Исследование показывает, что при достаточно большом масштабе модели и предобучении, ViT способен самостоятельно изучить необходимые индуктивные смещения.'}, 'en': {'title': 'Streamlining Image Segmentation with EoMT: Faster and Simpler ViTs', 'desc': "This paper presents the Encoder-only Mask Transformer (EoMT), a novel approach for image segmentation using Vision Transformers (ViTs). Instead of relying on additional components like convolutional adapters and pixel decoders, EoMT leverages the ViT's ability to learn necessary features directly from data. The authors demonstrate that with large models and extensive pre-training, EoMT achieves competitive segmentation accuracy while being significantly faster than traditional methods. This suggests that focusing on scaling the ViT architecture can yield better performance than adding complex task-specific elements."}, 'zh': {'title': '简化架构，提升分割效率', 'desc': '视觉变换器（ViTs）在各种计算机视觉任务中表现出色。本文提出了一种新的模型，称为仅编码器掩码变换器（EoMT），它利用简单的ViT架构进行图像分割，而无需额外的任务特定组件。研究表明，ViT可以通过大规模模型和充分的预训练来学习任务所需的偏置。EoMT在分割精度和预测速度之间实现了最佳平衡，且速度比传统方法快，最高可达4倍。'}}}, {'id': 'https://huggingface.co/papers/2503.22268', 'title': 'Segment Any Motion in Videos', 'url': 'https://huggingface.co/papers/2503.22268', 'abstract': 'Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.', 'score': 10, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '084606e82bff72ff', 'authors': ['Nan Huang', 'Wenzhao Zheng', 'Chenfeng Xu', 'Kurt Keutzer', 'Shanghang Zhang', 'Angjoo Kanazawa', 'Qianqian Wang'], 'affiliations': ['Peking University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.22268.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': '🎥', 'ru': {'title': 'Улучшенная сегментация движущихся объектов с помощью траекторного внимания и семантических признаков', 'desc': 'Эта статья представляет новый подход к сегментации движущихся объектов в видео. Метод объединяет траекторные признаки движения с семантическими признаками DINO и использует SAM2 для уточнения масок на уровне пикселей. Модель применяет пространственно-временное внимание к траекториям и раздельное кодирование движения и семантики. Результаты тестирования на различных наборах данных показывают превосходную производительность, особенно в сложных сценариях.'}, 'en': {'title': 'Revolutionizing Moving Object Segmentation with Motion-Semantic Integration', 'desc': 'This paper presents a new method for moving object segmentation in videos, which is essential for understanding visual scenes. The authors address the limitations of traditional optical flow techniques that struggle with issues like motion blur and background distractions. Their approach combines long-range motion cues with semantic features from a DINO model and uses an iterative strategy with SAM2 for detailed pixel-level segmentation. The proposed model shows superior performance on various datasets, particularly in complex scenarios involving multiple moving objects.'}, 'zh': {'title': '创新移动物体分割方法，提升视觉理解能力', 'desc': '移动物体分割是理解视觉场景的重要任务，具有广泛的应用。传统方法主要依赖光流来提供运动线索，但在处理部分运动、复杂变形、运动模糊和背景干扰时常常效果不佳。我们提出了一种新方法，结合长距离轨迹运动线索与基于DINO的语义特征，并通过迭代提示策略利用SAM2进行像素级掩膜细化。我们的模型采用时空轨迹注意力和运动-语义解耦嵌入，优先考虑运动，同时整合语义支持，经过广泛测试在多样化数据集上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.22236', 'title': 'Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging', 'url': 'https://huggingface.co/papers/2503.22236', 'abstract': 'With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.', 'score': 9, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '5024619189472d8c', 'authors': ['Chongjie Ye', 'Yushuang Wu', 'Ziteng Lu', 'Jiahao Chang', 'Xiaoyang Guo', 'Jiaqing Zhou', 'Hao Zhao', 'Xiaoguang Han'], 'affiliations': ['ByteDance', 'The Chinese University of Hong Kong, Shenzhen', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22236.jpg', 'data': {'categories': ['#3d'], 'emoji': '🖼️', 'ru': {'title': 'От плоского к объемному: революция в 3D-моделировании', 'desc': 'Hi3DGen - это новый подход к созданию высококачественных 3D-моделей из 2D-изображений с использованием нормалей в качестве промежуточного представления. Метод состоит из трех ключевых компонентов: оценщика нормалей из изображений, генератора геометрии из нормалей на основе латентной диффузии, и пайплайна для синтеза качественных 3D-данных. Hi3DGen превосходит современные методы в точности воспроизведения мелких геометрических деталей. Этот подход открывает новое направление в генерации высококачественной 3D-геометрии из изображений.'}, 'en': {'title': 'Bridging Normals for High-Fidelity 3D Generation', 'desc': 'The paper introduces Hi3DGen, a new framework designed to create high-fidelity 3D models from 2D images. It tackles challenges like domain gaps and ambiguities in RGB images by using a method called normal bridging. Hi3DGen includes an image-to-normal estimator that improves the accuracy of geometric details through noise injection and dual-stream training. Additionally, it employs a normal-to-geometry learning approach and a 3D data synthesis pipeline to enhance the quality of the generated 3D models, showing superior performance compared to existing methods.'}, 'zh': {'title': '高保真3D几何体生成的新方向', 'desc': '随着对从2D图像生成高保真3D模型的需求增加，现有方法在准确再现细致几何细节方面仍面临重大挑战。为了解决这些问题，我们提出了Hi3DGen，一个通过法线桥接生成高保真3D几何体的新框架。Hi3DGen包含三个关键组件：图像到法线估计器、法线到几何体学习方法和3D数据合成管道。我们的实验表明，该框架在生成丰富几何细节方面的有效性和优越性，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.22329', 'title': 'A Refined Analysis of Massive Activations in LLMs', 'url': 'https://huggingface.co/papers/2503.22329', 'abstract': 'Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.', 'score': 7, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '025b5484847cd3d9', 'authors': ['Louis Owen', 'Nilabhra Roy Chowdhury', 'Abhay Kumar', 'Fabian Güra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2503.22329.jpg', 'data': {'categories': ['#optimization', '#architecture', '#long_context', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый взгляд на массивные активации в LLM: не все так однозначно', 'desc': 'Эта статья посвящена исследованию массивных активаций в больших языковых моделях (LLM). Авторы провели анализ различных архитектур LLM, включая модели на основе GLU и без него. Результаты опровергают некоторые предыдущие предположения, показывая, что не все массивные активации вредны, а существующие стратегии смягчения могут быть неэффективны в определенных случаях. Исследователи предлагают новые гибридные стратегии, сочетающие Target Variance Rescaling с Attention KV bias или Dynamic Tanh, для эффективного смягчения массивных активаций при сохранении производительности модели.'}, 'en': {'title': 'Balancing Act: Mitigating Massive Activations in LLMs', 'desc': 'This paper explores the phenomenon of massive activations in large language models (LLMs) and their implications for low-precision training and quantization. The authors analyze a variety of LLM architectures to understand the effects of massive activations, revealing that not all of them negatively impact model performance. They challenge previous assumptions by demonstrating that suppressing massive activations does not necessarily lead to worse outcomes in downstream tasks. Additionally, the paper introduces new hybrid strategies that effectively mitigate massive activations while maintaining model performance, particularly through the combination of Target Variance Rescaling and other techniques.'}, 'zh': {'title': '大激活的挑战与新策略', 'desc': '本文研究了大语言模型（LLMs）中的大激活现象，特别关注其在低精度训练和量化中的重要性。我们分析了多种LLM架构，包括基于GLU和非GLU的模型，发现并非所有大激活都是有害的，抑制它们并不会导致困惑度的爆炸或下游任务性能的崩溃。我们还发现，现有的缓解策略如注意力KV偏置在某些情况下是模型特定的且无效。因此，我们提出了新的混合缓解策略，结合目标方差重标定（TVR）与注意力KV偏置或动态Tanh（DyT），成功平衡了大激活的缓解与下游模型性能的保持。'}}}, {'id': 'https://huggingface.co/papers/2503.17827', 'title': '4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object\n  Understanding', 'url': 'https://huggingface.co/papers/2503.17827', 'abstract': 'Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities. However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects (3D objects with temporal evolution over time). In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks. With 4D-Bench, we evaluate a wide range of open-source and closed-source MLLMs. The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding. 4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human baseline of 91\\%. These findings highlight a substantial gap in 4D object understanding and the need for further advancements in MLLMs.', 'score': 7, 'issue_id': 2978, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 марта', 'en': 'March 22', 'zh': '3月22日'}, 'hash': '4c510d164c81f13e', 'authors': ['Wenxuan Zhu', 'Bing Li', 'Cheng Zheng', 'Jinjie Mai', 'Jun Chen', 'Letian Jiang', 'Abdullah Hamdi', 'Sara Rojas Martinez', 'Chia-Wen Lin', 'Mohamed Elhoseiny', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology', 'National Tsing Hua University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.17827.jpg', 'data': {'categories': ['#benchmark', '#3d', '#games', '#video', '#multimodal', '#open_source'], 'emoji': '🕰️', 'ru': {'title': '4D-Bench: раскрывая ограничения MLLM в понимании пространственно-временных объектов', 'desc': '4D-Bench - это новый эталонный тест для оценки способностей мультимодальных больших языковых моделей (MLLM) в понимании 4D-объектов. Он включает задачи по ответам на вопросы о 4D-объектах и их описанию, предоставляя разнообразные категории объектов и высококачественные аннотации. Результаты показывают, что MLLM хуже понимают временные аспекты по сравнению с визуальными, причем открытые модели значительно отстают от закрытых в понимании временных характеристик. Даже лучшие модели, такие как GPT-4o, демонстрируют низкую точность в задачах с 4D-объектами по сравнению с человеческим уровнем.'}, 'en': {'title': 'Bridging the Gap in 4D Object Understanding for MLLMs', 'desc': 'This paper introduces 4D-Bench, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their understanding of 4D objects, which are 3D objects that change over time. The benchmark includes tasks such as 4D object Question Answering and 4D object captioning, focusing on multi-view spatial-temporal understanding. The evaluation reveals that MLLMs struggle with temporal understanding, showing significant performance gaps compared to their ability to understand object appearance. Overall, the findings indicate a critical need for improvements in MLLMs to enhance their capabilities in 4D object comprehension.'}, 'zh': {'title': '四维物体理解的新基准：4D-Bench', 'desc': '多模态大型语言模型（MLLMs）在二维图像和视频理解方面表现出色，但缺乏评估四维物体理解能力的标准基准。本文提出了4D-Bench，这是第一个用于评估MLLMs在四维物体理解能力的基准，包含四维物体问答和四维物体描述等任务。4D-Bench提供了多样化的四维物体类别和高质量的注释，要求模型具备多视角的时空理解能力。实验结果显示，MLLMs在时间理解方面普遍较弱，尤其是开源模型在外观理解上接近闭源模型，但在时间理解上存在较大差距。'}}}, {'id': 'https://huggingface.co/papers/2503.21732', 'title': 'SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling', 'url': 'https://huggingface.co/papers/2503.21732', 'abstract': 'Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.', 'score': 5, 'issue_id': 2972, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'f17ea311cc796683', 'authors': ['Xianglong He', 'Zi-Xin Zou', 'Chia-Hao Chen', 'Yuan-Chen Guo', 'Ding Liang', 'Chun Yuan', 'Wanli Ouyang', 'Yan-Pei Cao', 'Yangguang Li'], 'affiliations': ['The Chinese University of Hong Kong', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2503.21732.jpg', 'data': {'categories': ['#3d'], 'emoji': '🧊', 'ru': {'title': 'Революция в 3D-моделировании: высокое разрешение и произвольная топология', 'desc': 'SparseFlex - это новый метод представления изоповерхностей, позволяющий выполнять дифференцируемую реконструкцию 3D-моделей с разрешением до 1024^3 непосредственно на основе потерь рендеринга. Он сочетает точность Flexicubes с разреженной воксельной структурой, фокусируясь на областях, прилегающих к поверхности. Введена стратегия обучения на основе секционных вокселей с учетом фрустума, что значительно снижает потребление памяти. SparseFlex также позволяет реконструировать внутренние части сетки, используя только рендеринг для обучения.'}, 'en': {'title': 'SparseFlex: Revolutionizing 3D Mesh Reconstruction with Efficiency and Detail', 'desc': 'This paper presents SparseFlex, a new method for creating detailed 3D meshes with complex shapes and open surfaces. It uses a sparse voxel structure to focus on areas near the surface, allowing for efficient high-resolution mesh reconstruction directly from rendering losses. The method introduces a frustum-aware training strategy that reduces memory usage by activating only necessary voxels during rendering. SparseFlex achieves impressive results, outperforming previous techniques in accuracy and enabling the generation of intricate 3D shapes with varying topologies.'}, 'zh': {'title': 'SparseFlex：高分辨率3D网格重建的新突破', 'desc': '本文介绍了一种名为SparseFlex的新型稀疏结构等值面表示方法，旨在解决高保真3D网格重建中的挑战。SparseFlex能够直接从渲染损失中进行可微分的网格重建，支持高达1024^3的分辨率。通过结合Flexicubes的准确性和稀疏体素结构，该方法有效处理开放表面，并引入了基于视锥的分段体素训练策略，显著降低了内存消耗。实验结果表明，SparseFlex在重建精度上达到了最先进的水平，成功生成了具有任意拓扑的高分辨率、细节丰富的3D形状。'}}}, {'id': 'https://huggingface.co/papers/2503.21751', 'title': 'Reconstructing Humans with a Biomechanically Accurate Skeleton', 'url': 'https://huggingface.co/papers/2503.21751', 'abstract': 'In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/', 'score': 4, 'issue_id': 2979, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'a715b9755e020ce7', 'authors': ['Yan Xia', 'Xiaowei Zhou', 'Etienne Vouga', 'Qixing Huang', 'Georgios Pavlakos'], 'affiliations': ['The University of Texas at Austin', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21751.jpg', 'data': {'categories': ['#benchmark', '#3d', '#architecture', '#dataset'], 'emoji': '🦴', 'ru': {'title': 'Биомеханически точная 3D-реконструкция человека по одному изображению', 'desc': 'В этой статье представлен метод реконструкции 3D-моделей людей по одному изображению с использованием биомеханически точной модели скелета. Авторы обучают трансформер, который принимает изображение на вход и оценивает параметры модели. Из-за нехватки обучающих данных разработан конвейер для создания псевдо-разметки параметров модели по одиночным изображениям. По сравнению с современными методами восстановления 3D-сетки человека, предложенный подход показывает конкурентоспособные результаты на стандартных тестах и значительно превосходит их в сценариях с экстремальными 3D-позами и ракурсами.'}, 'en': {'title': 'Reconstructing Realistic 3D Humans from Single Images', 'desc': 'This paper presents a novel method for creating 3D models of humans from just one image by using a skeleton model that accurately reflects human biomechanics. The authors train a transformer model that processes the input image to predict the parameters of this skeleton. To address the challenge of limited training data, they develop a pipeline that generates pseudo ground truth parameters, which are refined through an iterative training process. Their approach not only matches the performance of existing 3D human reconstruction methods but also excels in challenging scenarios with extreme poses and viewpoints, while ensuring realistic joint movements.'}, 'zh': {'title': '生物力学驱动的3D人类重建新方法', 'desc': '本文提出了一种从单张图像重建3D人类模型的方法，使用生物力学准确的骨骼模型。我们训练了一个变换器，输入图像并估计模型参数。由于训练数据的不足，我们建立了一个管道来生成单张图像的伪真实模型参数，并实施了一个迭代优化伪标签的训练过程。与现有的3D人类网格恢复方法相比，我们的模型在标准基准测试中表现出竞争力，尤其在极端3D姿势和视角下表现更为优越。'}}}, {'id': 'https://huggingface.co/papers/2503.18968', 'title': 'MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow', 'url': 'https://huggingface.co/papers/2503.18968', 'abstract': 'Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.', 'score': 4, 'issue_id': 2973, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '662e73dea1e23d07', 'authors': ['Ziyue Wang', 'Junde Wu', 'Chang Han Low', 'Yueming Jin'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.18968.jpg', 'data': {'categories': ['#hallucinations', '#3d', '#multimodal', '#reasoning', '#interpretability', '#healthcare', '#agents'], 'emoji': '🩺', 'ru': {'title': 'MedAgent-Pro: надежная мультимодальная диагностика с ИИ', 'desc': 'Статья представляет MedAgent-Pro - агентную систему для надежной и объяснимой медицинской диагностики на основе мультимодальных больших языковых моделей (MLLM). Система решает проблемы прямого применения MLLM в медицине, такие как ограниченное восприятие визуальных данных и склонность к галлюцинациям. MedAgent-Pro использует иерархический рабочий процесс с рассуждениями на основе знаний и множеством инструментальных агентов для обработки мультимодальных входных данных. Эксперименты показывают превосходство и эффективность MedAgent-Pro в задачах 2D и 3D медицинской диагностики.'}, 'en': {'title': 'MedAgent-Pro: Reliable AI for Accurate Medical Diagnosis', 'desc': 'This paper introduces MedAgent-Pro, a new AI system designed to improve medical diagnosis by combining multi-modal inputs and evidence-based reasoning. It addresses the limitations of existing Multi-modal Large Language Models (MLLMs), which struggle with visual data and often produce unreliable outputs. MedAgent-Pro uses a hierarchical approach, where knowledge-based reasoning creates diagnostic plans and multiple tool agents analyze various indicators to provide accurate diagnoses. Experiments show that MedAgent-Pro outperforms traditional methods in both 2D and 3D medical tasks, demonstrating its reliability and interpretability in clinical settings.'}, 'zh': {'title': '提升医疗诊断的可靠性与可解释性', 'desc': '本论文提出了一种名为MedAgent-Pro的系统，旨在提高多模态医疗诊断的可靠性和准确性。该系统结合了基于知识的推理和多工具代理，能够处理多种输入并生成可靠的诊断计划。通过对2D和3D医疗诊断任务的全面实验，MedAgent-Pro展示了其优越性和有效性。该系统不仅提供了精确的诊断，还具备良好的可解释性，适合临床应用。'}}}, {'id': 'https://huggingface.co/papers/2503.21851', 'title': 'On Large Multimodal Models as Open-World Image Classifiers', 'url': 'https://huggingface.co/papers/2503.21851', 'abstract': 'Traditional image classification requires a predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt "What is the main object in the image?"). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming a closed-world setting with a predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in a truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, fine-grained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them.', 'score': 3, 'issue_id': 2977, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '1f5e6c94324fef3f', 'authors': ['Alessandro Conti', 'Massimiliano Mancini', 'Enrico Fini', 'Yiming Wang', 'Paolo Rota', 'Elisa Ricci'], 'affiliations': ['Fondazione Bruno Kessler', 'Independent researcher', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.21851.jpg', 'data': {'categories': ['#reasoning', '#alignment', '#benchmark', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Открытый взгляд на классификацию изображений с помощью мультимодальных моделей', 'desc': 'Статья исследует производительность больших мультимодальных моделей (LMM) в задаче классификации изображений в условиях открытого мира. Авторы предлагают новый протокол оценки и метрики для измерения соответствия между предсказанными и истинными классами. Проводится оценка 13 моделей на 10 наборах данных, включающих прототипические, непрототипические и детальные классы. Анализ выявляет трудности, с которыми сталкиваются LMM, особенно в отношении гранулярности и способности различать тонкие детали.'}, 'en': {'title': 'Unlocking Open-World Image Classification with LMMs', 'desc': 'This paper explores the capabilities of Large Multimodal Models (LMMs) in image classification without relying on a fixed set of categories. It introduces a new evaluation framework to assess LMM performance in an open-world context, where images can belong to any category. The study evaluates 13 different models across various benchmarks, revealing the difficulties LMMs encounter, particularly with fine-grained classifications. Additionally, it discusses how improved prompting and reasoning techniques can help mitigate these challenges.'}, 'zh': {'title': '大型多模态模型在开放世界中的图像分类挑战', 'desc': '传统的图像分类需要预定义的语义类别，而大型多模态模型（LMMs）可以通过自然语言直接对图像进行分类。这项研究评估了LMM在开放世界环境中的分类性能，填补了现有研究的空白。我们引入了一种评估协议，定义了多种指标来评估预测类别与真实类别之间的对齐情况。通过对13个模型在10个基准上的评估，揭示了LMM在细粒度分类中的挑战，并提出了定制提示和推理的方法来缓解这些问题。'}}}, {'id': 'https://huggingface.co/papers/2503.21779', 'title': 'X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction', 'url': 'https://huggingface.co/papers/2503.21779', 'abstract': 'Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.', 'score': 2, 'issue_id': 2976, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '628d77c9c1bdcd9e', 'authors': ['Weihao Yu', 'Yuanhao Cai', 'Ruyi Zha', 'Zhiwen Fan', 'Chenxin Li', 'Yixuan Yuan'], 'affiliations': ['Johns Hopkins University', 'The Australian National University', 'The Chinese University of Hong Kong', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.21779.jpg', 'data': {'categories': ['#architecture', '#healthcare', '#cv'], 'emoji': '🫁', 'ru': {'title': 'Непрерывная 4D-КТ реконструкция без внешней синхронизации', 'desc': 'Эта статья представляет X^2-Gaussian - новый метод для непрерывной реконструкции 4D-КТ изображений без использования внешних устройств синхронизации дыхания. Авторы объединяют динамическое рассеивание гауссовых функций с самоконтролируемым обучением респираторных движений. Метод использует пространственно-временную архитектуру кодировщика-декодировщика для моделирования анатомической динамики через предсказание деформаций гауссовых функций во времени. X^2-Gaussian превосходит традиционные методы на 9.93 дБ по метрике PSNR, открывая новые возможности для динамической клинической визуализации.'}, 'en': {'title': 'Revolutionizing 4D CT with Continuous Motion Modeling', 'desc': 'This paper presents X^2-Gaussian, a new method for 4D CT reconstruction that overcomes the limitations of traditional phase-binning techniques. By using a spatiotemporal encoder-decoder architecture, it models dynamic anatomical changes without relying on fixed phases, thus allowing for continuous-time reconstruction. The method incorporates self-supervised learning to understand patient-specific breathing patterns, eliminating the need for external respiratory gating devices. Experimental results show that X^2-Gaussian significantly improves image quality, achieving higher PSNR compared to existing methods.'}, 'zh': {'title': 'X^2-Gaussian：无硬件的高保真4D CT重建新方法', 'desc': '四维计算机断层扫描（4D CT）重建对于捕捉动态解剖变化至关重要，但传统的相位分箱工作流程存在固有的局限性。现有方法将时间分辨率离散化为固定相位，使用呼吸门控设备，导致运动错位并限制临床实用性。本文提出了一种新框架X^2-Gaussian，通过结合动态辐射高斯点云和自监督呼吸运动学习，实现连续时间的4D CT重建。我们的模型通过时空编码器-解码器架构预测时间变化的高斯变形，消除了相位离散化的需求，并引入生理驱动的周期一致性损失，直接从投影中学习患者特定的呼吸周期。'}}}, {'id': 'https://huggingface.co/papers/2503.22625', 'title': 'Challenges and Paths Towards AI for Software Engineering', 'url': 'https://huggingface.co/papers/2503.22625', 'abstract': 'AI for software engineering has made remarkable progress recently, becoming a notable success within generative AI. Despite this, there are still many challenges that need to be addressed before automated software engineering reaches its full potential. It should be possible to reach high levels of automation where humans can focus on the critical decisions of what to build and how to balance difficult tradeoffs while most routine development effort is automated away. Reaching this level of automation will require substantial research and engineering efforts across academia and industry. In this paper, we aim to discuss progress towards this in a threefold manner. First, we provide a structured taxonomy of concrete tasks in AI for software engineering, emphasizing the many other tasks in software engineering beyond code generation and completion. Second, we outline several key bottlenecks that limit current approaches. Finally, we provide an opinionated list of promising research directions toward making progress on these bottlenecks, hoping to inspire future research in this rapidly maturing field.', 'score': 1, 'issue_id': 2993, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': 'b8c25b067c6a7343', 'authors': ['Alex Gu', 'Naman Jain', 'Wen-Ding Li', 'Manish Shetty', 'Yijia Shao', 'Ziyang Li', 'Diyi Yang', 'Kevin Ellis', 'Koushik Sen', 'Armando Solar-Lezama'], 'affiliations': ['Cornell University', 'MIT CSAIL', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.22625.jpg', 'data': {'categories': [], 'emoji': '🤖', 'ru': {'title': 'ИИ в разработке ПО: от генерации кода к полной автоматизации', 'desc': 'Статья посвящена прогрессу и перспективам применения искусственного интеллекта в разработке программного обеспечения. Авторы предлагают структурированную таксономию задач ИИ в сфере разработки ПО, выходящую за рамки генерации и автодополнения кода. В работе выделяются ключевые ограничения существующих подходов и предлагаются перспективные направления исследований для их преодоления. Цель статьи - вдохновить будущие исследования в этой быстро развивающейся области и приблизить высокий уровень автоматизации в разработке ПО.'}, 'en': {'title': 'Unlocking Automation in Software Engineering with AI', 'desc': 'This paper discusses the advancements in AI for software engineering, highlighting its success in generative AI. It identifies the challenges that must be overcome to achieve high levels of automation, allowing humans to focus on critical decision-making. The authors present a structured taxonomy of various tasks in AI for software engineering, going beyond just code generation. They also outline key bottlenecks in current methods and suggest promising research directions to address these issues, aiming to inspire further exploration in this evolving field.'}, 'zh': {'title': '推动软件工程自动化的未来', 'desc': '本文探讨了人工智能在软件工程领域的进展，尤其是在生成式人工智能方面的成功。尽管取得了显著进展，但在实现完全自动化的软件工程之前，仍面临许多挑战。我们提出了一个结构化的分类法，涵盖了软件工程中除了代码生成和补全之外的多种任务，并指出了当前方法的关键瓶颈。最后，我们列出了有前景的研究方向，以期激励未来在这一快速发展的领域中的研究。'}}}, {'id': 'https://huggingface.co/papers/2503.21544', 'title': 'SWI: Speaking with Intent in Large Language Models', 'url': 'https://huggingface.co/papers/2503.21544', 'abstract': "Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMs' reasoning abilities with cognitive notions.", 'score': 1, 'issue_id': 2985, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '1a89039133f8f32f', 'authors': ['Yuwei Yin', 'EunJeong Hwang', 'Giuseppe Carenini'], 'affiliations': ['University of British Columbia', 'Vector Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.21544.jpg', 'data': {'categories': ['#math', '#hallucinations', '#interpretability', '#benchmark', '#dataset', '#training', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Усиление рассуждений ИИ через осознанное намерение', 'desc': "Эта статья представляет концепцию 'Говорения с намерением' (Speaking with Intent, SWI) для больших языковых моделей (LLM). SWI предполагает генерацию явного намерения модели, которое служит основой для последующего анализа и коммуникации. Эксперименты показывают, что SWI превосходит базовые методы и некоторые современные подходы в задачах математических рассуждений. SWI также демонстрирует улучшения в задачах вопросно-ответных систем и суммаризации текста, повышая точность и уменьшая количество галлюцинаций."}, 'en': {'title': 'Enhancing LLMs with Intent-Driven Reasoning', 'desc': "This paper presents the concept of Speaking with Intent (SWI) in large language models (LLMs), which aims to improve their reasoning and communication by explicitly defining the model's intentions. By mimicking human-like thought processes, SWI enhances the quality of generated outputs and guides the model's analysis. Experimental results show that SWI outperforms traditional methods, including baseline generation and popular prompting techniques, in various reasoning tasks and text summarization. The findings suggest that incorporating cognitive frameworks like SWI can significantly boost the performance and reliability of LLMs."}, 'zh': {'title': '有意说话：提升语言模型推理能力的新方法', 'desc': '本文介绍了在大型语言模型（LLMs）中引入的“有意说话”（SWI）概念，强调明确生成的意图作为模型的认知框架，指导后续分析和交流。SWI通过模拟人类思维中的有意图和目的性思考，假设能够增强LLMs的推理能力和生成质量。大量实验表明，SWI在数学推理基准测试中优于基线方法，并且在推理密集型问答和文本摘要任务中也表现出一致的改进。SWI生成的摘要在准确性、简洁性和事实正确性方面优于基线，且人类评估验证了其生成意图的连贯性和有效性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (8)', '#agents (2)', '#agi', '#alignment (2)', '#architecture (5)', '#audio (1)', '#benchmark (5)', '#cv (3)', '#data (3)', '#dataset (4)', '#diffusion', '#ethics', '#games (1)', '#graphs', '#hallucinations (2)', '#healthcare (2)', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (7)', '#open_source (1)', '#optimization (5)', '#plp', '#rag (1)', '#reasoning (9)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (10)', '#transfer_learning (2)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-01 02:45',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-01 02:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-01 02:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    