
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 76 papers. April 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Апрель 2025</span> | <span id="title-articles-count">76 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-03.html">⬅️ <span id="prev-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-05.html">➡️ <span id="next-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Апрель 2025', 'en': 'April 2025', 'zh': '4月2025年'};
        let feedDateNext = {'ru': '05.2025', 'en': '05/2025', 'zh': '5月2025年'};
        let feedDatePrev = {'ru': '03.2025', 'en': '03/2025', 'zh': '3月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.24379', 'title': 'Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.24379', 'abstract': 'To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/', 'score': 43, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'dce65db5da1b8c34', 'authors': ['Shengqiong Wu', 'Weicai Ye', 'Jiahao Wang', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Shuicheng Yan', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['Kuaishou Technology', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24379.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Универсальный контроль генерации видео через интерпретацию любых входных данных', 'desc': 'Any2Caption - это новая система для контролируемой генерации видео с использованием различных входных данных. Ключевая идея заключается в разделении этапов интерпретации условий и синтеза видео. Система использует мультимодальные большие языковые модели для создания подробных структурированных описаний на основе разнообразных входных данных. Авторы также представили большой набор данных Any2CapIns для обучения модели генерации описаний по различным условиям.'}, 'en': {'title': 'Revolutionizing Video Generation with Any2Caption', 'desc': "Any2Caption is a new framework designed to improve how user intentions are understood in video generation. It separates the process of interpreting different conditions from the actual video creation, allowing for more precise control. By using advanced multimodal large language models, it can transform various inputs like text and images into detailed captions that guide video generators more effectively. The introduction of the Any2CapIns dataset, which contains a large number of examples, further enhances the system's ability to generate high-quality videos based on diverse conditions."}, 'zh': {'title': '可控视频生成的新突破：Any2Caption', 'desc': '为了克服当前视频生成领域中准确理解用户意图的瓶颈，我们提出了Any2Caption，这是一个新颖的可控视频生成框架。其核心思想是将各种条件解释步骤与视频合成步骤解耦。通过利用现代多模态大语言模型（MLLMs），Any2Caption能够将文本、图像、视频及特定提示（如区域、运动和相机姿态）转化为密集的结构化字幕，从而为视频生成器提供更好的指导。我们还引入了Any2CapIns，这是一个包含337K实例和407K条件的大规模数据集，用于任何条件到字幕的指令调优。'}}}, {'id': 'https://huggingface.co/papers/2504.00050', 'title': 'JudgeLRM: Large Reasoning Models as a Judge', 'url': 'https://huggingface.co/papers/2504.00050', 'abstract': 'The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.', 'score': 29, 'issue_id': 3022, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '5060d3d364f635eb', 'authors': ['Nuo Chen', 'Zhiyuan Hu', 'Qingyun Zou', 'Jiaying Wu', 'Qian Wang', 'Bryan Hooi', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.00050.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#rl'], 'emoji': '⚖️', 'ru': {'title': 'Революция в обучении ИИ-судей: от простой настройки к глубоким рассуждениям', 'desc': 'Исследование показывает ограничения обычной тонкой настройки языковых моделей для задач оценки, требующих сложных рассуждений. Авторы предлагают новый подход JudgeLRM, использующий обучение с подкреплением для создания моделей-судей. JudgeLRM превосходит существующие методы, включая GPT-4, особенно в задачах, требующих глубоких рассуждений. Модель JudgeLRM-7B демонстрирует улучшение на 2.79% по F1-мере по сравнению с DeepSeek-R1.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Complex Judging Tasks', 'desc': 'This paper explores the effectiveness of Large Language Models (LLMs) as judges in evaluation tasks, particularly in scenarios that require complex reasoning. It highlights that traditional Supervised Fine-Tuning (SFT) methods do not perform well when faced with tasks that demand higher reasoning skills. The authors introduce JudgeLRM, a new approach that utilizes reinforcement learning (RL) to enhance the reasoning capabilities of LLMs by providing judge-specific rewards. The results show that JudgeLRM models significantly outperform both SFT-tuned models and other leading reasoning models, demonstrating superior performance in tasks that require deep reasoning.'}, 'zh': {'title': 'JudgeLRM：深度推理的评判者', 'desc': '本研究探讨了大型语言模型（LLMs）作为评估者的潜力，尤其是在复杂推理任务中的表现。我们发现，现有的监督微调（SFT）方法在处理需要深度推理的样本时效果不佳。为了解决这个问题，我们提出了JudgeLRM，这是一种基于强化学习（RL）训练的评判导向LLM，能够提供更有效的评估。实验结果表明，JudgeLRM模型在评判任务中表现优于传统的SFT模型和最先进的推理模型，尤其在需要深度推理的任务中表现突出。'}}}, {'id': 'https://huggingface.co/papers/2503.23145', 'title': 'CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis', 'url': 'https://huggingface.co/papers/2503.23145', 'abstract': 'Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.', 'score': 28, 'issue_id': 3021, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '945cc4b51522e668', 'authors': ['Anjiang Wei', 'Tarun Suresh', 'Jiannan Cao', 'Naveen Kannan', 'Yuheng Wu', 'Kai Yan', 'Thiago S. F. X. Teixeira', 'Ke Wang', 'Alex Aiken'], 'affiliations': ['Intel', 'MIT', 'Stanford University', 'University of Illinois Urbana-Champaign', 'Visa Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23145.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#plp', '#agents', '#training'], 'emoji': '🧠', 'ru': {'title': 'CodeARC: Новый рубеж в индуктивном синтезе программ', 'desc': 'Статья представляет CodeARC - новую систему оценки для индуктивного синтеза программ с использованием больших языковых моделей. В отличие от существующих статических методов, CodeARC позволяет агентам интерактивно взаимодействовать со скрытой целевой функцией, итеративно улучшая свои решения. Авторы создали масштабный бенчмарк из 1114 функций для оценки способностей моделей к индуктивному рассуждению и синтезу программ. Результаты показывают, что даже лучшие модели достигают лишь 52.7% успеха, что подчеркивает сложность задачи.'}, 'en': {'title': 'CodeARC: A New Frontier in Inductive Program Synthesis Evaluation', 'desc': 'This paper introduces CodeARC, a new evaluation framework for inductive program synthesis, which is the process of creating functions based on input-output examples. Unlike traditional methods that use static examples, CodeARC allows agents to interact with a hidden target function, enabling them to refine their solutions through feedback. The framework includes a large-scale benchmark with 1114 functions, demonstrating the challenges faced by models in this domain. The results show that fine-tuning models like LLaMA-3.1-8B-Instruct can significantly improve performance, emphasizing the need for dynamic evaluation in program synthesis tasks.'}, 'zh': {'title': 'CodeARC：提升程序合成的评估新标准', 'desc': '归纳程序合成，也称为示例编程，是从输入输出示例中合成函数的过程，要求能够推广到未见过的输入。虽然大型语言模型在自然语言指导的编程任务中表现出色，但它们在归纳程序合成方面的能力尚未得到充分探索。现有的评估协议依赖于静态示例集和保留测试，无法在合成函数错误时提供反馈，也未能反映现实世界中的场景。我们提出了CodeARC，一个新的评估框架，允许代理通过查询隐藏的目标函数与之互动，从而合成候选函数并根据反馈迭代改进解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.24376', 'title': 'Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1', 'url': 'https://huggingface.co/papers/2503.24376', 'abstract': "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.", 'score': 24, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'd22966d0969ded43', 'authors': ['Yi Chen', 'Yuying Ge', 'Rui Wang', 'Yixiao Ge', 'Lu Qiu', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.24376.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#rl', '#benchmark', '#optimization', '#video'], 'emoji': '🎥', 'ru': {'title': 'Обучение с подкреплением улучшает понимание видео мультимодальными ИИ-моделями', 'desc': 'Статья представляет новый бенчмарк SEED-Bench-R1 для оценки методов пост-обучения мультимодальных больших языковых моделей (MLLM) в задачах понимания видео. Исследователи сравнивают обучение с подкреплением (RL) и обычное обучение с учителем (SFT) на модели Qwen2-VL-Instruct-7B. Результаты показывают, что RL более эффективно использует данные и лучше работает как на распределении обучающей выборки, так и вне его. Однако анализ выявляет, что RL улучшает визуальное восприятие, но иногда производит менее логически связные цепочки рассуждений.'}, 'en': {'title': 'Unlocking Reasoning in Multimodal Models with SEED-Bench-R1', 'desc': "This paper discusses the advancements in Chain of Thought (COT) generation for Large Language Models (LLMs) and introduces a new benchmark called SEED-Bench-R1 for evaluating Multimodal Large Language Models (MLLMs) in video understanding tasks. The benchmark includes complex real-world videos and planning tasks presented as multiple-choice questions, assessing the models' perception and reasoning abilities. The study compares reinforcement learning (RL) with supervised fine-tuning (SFT) using the Qwen2-VL-Instruct-7B model, showing that RL is more data-efficient and performs better on various tasks. However, the analysis also highlights limitations in reasoning consistency and visual cue recognition, suggesting areas for future research to enhance model performance."}, 'zh': {'title': '强化学习提升多模态模型推理能力', 'desc': '最近，链式思维（COT）生成的进展显著提升了大型语言模型（LLMs）的推理能力，而强化学习（RL）成为一种有效的后训练方法。多模态大型语言模型（MLLMs）继承了这种推理潜力，但在需要感知和逻辑推理的任务中仍然未被充分探索。为了解决这个问题，我们引入了SEED-Bench-R1，这是一个旨在系统评估MLLMs在视频理解中后训练方法的基准，包含复杂的真实视频和多项选择题的日常规划任务。我们的研究表明，RL在数据效率和性能上优于监督微调（SFT），但在逻辑连贯性方面存在一定的局限性。'}}}, {'id': 'https://huggingface.co/papers/2504.00698', 'title': 'Command A: An Enterprise-Ready Large Language Model', 'url': 'https://huggingface.co/papers/2504.00698', 'abstract': 'In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.', 'score': 16, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '8670e6d1cc4f6bee', 'authors': ['Team Cohere', 'Aakanksha', 'Arash Ahmadian', 'Marwan Ahmed', 'Jay Alammar', 'Yazeed Alnumay', 'Sophia Althammer', 'Arkady Arkhangorodsky', 'Viraat Aryabumi', 'Dennis Aumiller', 'Raphaël Avalos', 'Zahara Aviv', 'Sammie Bae', 'Saurabh Baji', 'Alexandre Barbet', 'Max Bartolo', 'Björn Bebensee', 'Neeral Beladia', 'Walter Beller-Morales', 'Alexandre Bérard', 'Andrew Berneshawi', 'Anna Bialas', 'Phil Blunsom', 'Matt Bobkin', 'Adi Bongale', 'Sam Braun', 'Maxime Brunet', 'Samuel Cahyawijaya', 'David Cairuz', 'Jon Ander Campos', 'Cassie Cao', 'Kris Cao', 'Roman Castagné', 'Julián Cendrero', 'Leila Chan Currie', 'Yash Chandak', 'Diane Chang', 'Giannis Chatziveroglou', 'Hongyu Chen', 'Claire Cheng', 'Alexis Chevalier', 'Justin T. Chiu', 'Eugene Cho', 'Eugene Choi', 'Eujeong Choi', 'Tim Chung', 'Volkan Cirik', 'Ana Cismaru', 'Pierre Clavier', 'Henry Conklin', 'Lucas Crawhall-Stein', 'Devon Crouse', 'Andres Felipe Cruz-Salinas', 'Ben Cyrus', "Daniel D'souza", 'Hugo Dalla-Torre', 'John Dang', 'William Darling', 'Omar Darwiche Domingues', 'Saurabh Dash', 'Antoine Debugne', 'Théo Dehaze', 'Shaan Desai', 'Joan Devassy', 'Rishit Dholakia', 'Kyle Duffy', 'Ali Edalati', 'Ace Eldeib', 'Abdullah Elkady', 'Sarah Elsharkawy', 'Irem Ergün', 'Beyza Ermis', 'Marzieh Fadaee', 'Boyu Fan', 'Lucas Fayoux', 'Yannis Flet-Berliac', 'Nick Frosst', 'Matthias Gallé', 'Wojciech Galuba', 'Utsav Garg', 'Matthieu Geist', 'Mohammad Gheshlaghi Azar', 'Seraphina Goldfarb-Tarrant', 'Tomas Goldsack', 'Aidan Gomez', 'Victor Machado Gonzaga', 'Nithya Govindarajan', 'Manoj Govindassamy', 'Nathan Grinsztajn', 'Nikolas Gritsch', 'Patrick Gu', 'Shangmin Guo', 'Kilian Haefeli', 'Rod Hajjar', 'Tim Hawes', 'Jingyi He', 'Sebastian Hofstätter', 'Sungjin Hong', 'Sara Hooker', 'Tom Hosking', 'Stephanie Howe', 'Eric Hu', 'Renjie Huang', 'Hemant Jain', 'Ritika Jain', 'Nick Jakobi', 'Madeline Jenkins', 'JJ Jordan', 'Dhruti Joshi', 'Jason Jung', 'Trushant Kalyanpur', 'Siddhartha Rao Kamalakara', 'Julia Kedrzycki', 'Gokce Keskin', 'Edward Kim', 'Joon Kim', 'Wei-Yin Ko', 'Tom Kocmi', 'Michael Kozakov', 'Wojciech Kryściński', 'Arnav Kumar Jain', 'Komal Kumar Teru', 'Sander Land', 'Michael Lasby', 'Olivia Lasche', 'Justin Lee', 'Patrick Lewis', 'Jeffrey Li', 'Jonathan Li', 'Hangyu Lin', 'Acyr Locatelli', 'Kevin Luong', 'Raymond Ma', 'Lukas Mach', 'Marina Machado', 'Joanne Magbitang', 'Brenda Malacara Lopez', 'Aryan Mann', 'Kelly Marchisio', 'Olivia Markham', 'Alexandre Matton', 'Alex McKinney', 'Dominic McLoughlin', 'Jozef Mokry', 'Adrien Morisot', 'Autumn Moulder', 'Harry Moynehan', 'Maximilian Mozes', 'Vivek Muppalla', 'Lidiya Murakhovska', 'Hemangani Nagarajan', 'Alekhya Nandula', 'Hisham Nasir', 'Shauna Nehra', 'Josh Netto-Rosen', 'Daniel Ohashi', 'James Owers-Bardsley', 'Jason Ozuzu', 'Dennis Padilla', 'Gloria Park', 'Sam Passaglia', 'Jeremy Pekmez', 'Laura Penstone', 'Aleksandra Piktus', 'Case Ploeg', 'Andrew Poulton', 'Youran Qi', 'Shubha Raghvendra', 'Miguel Ramos', 'Ekagra Ranjan', 'Pierre Richemond', 'Cécile Robert-Michon', 'Aurélien Rodriguez', 'Sudip Roy', 'Laura Ruis', 'Louise Rust', 'Anubhav Sachan', 'Alejandro Salamanca', 'Kailash Karthik Saravanakumar', 'Isha Satyakam', 'Alice Schoenauer Sebag', 'Priyanka Sen', 'Sholeh Sepehri', 'Preethi Seshadri', 'Ye Shen', 'Tom Sherborne', 'Sylvie Chang Shi', 'Sanal Shivaprasad', 'Vladyslav Shmyhlo', 'Anirudh Shrinivason', 'Inna Shteinbuk', 'Amir Shukayev', 'Mathieu Simard', 'Ella Snyder', 'Ava Spataru', 'Victoria Spooner', 'Trisha Starostina', 'Florian Strub', 'Yixuan Su', 'Jimin Sun', 'Dwarak Talupuru', 'Eugene Tarassov', 'Elena Tommasone', 'Jennifer Tracey', 'Billy Trend', 'Evren Tumer', 'Ahmet Üstün', 'Bharat Venkitesh', 'David Venuto', 'Pat Verga', 'Maxime Voisin', 'Alex Wang', 'Donglu Wang', 'Shijian Wang', 'Edmond Wen', 'Naomi White', 'Jesse Willman', 'Marysia Winkels', 'Chen Xia', 'Jessica Xie', 'Minjie Xu', 'Bowen Yang', 'Tan Yi-Chern', 'Ivan Zhang', 'Zhenyu Zhao', 'Zhoujie Zhao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.00698.jpg', 'data': {'categories': ['#training', '#low_resource', '#agents', '#open_source', '#rag', '#multilingual', '#architecture', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'Command A: Мощная многоязычная ИИ-модель для бизнеса', 'desc': 'В статье описывается разработка Command A - мощной языковой модели, оптимизированной для корпоративного использования. Модель поддерживает 23 языка и обладает гибридной архитектурой, сочетающей эффективность и высокую производительность. Command A предлагает передовые возможности Retrieval Augmented Generation (RAG) для автоматизации сложных бизнес-процессов. Эти возможности достигаются с помощью децентрализованного обучения, включая алгоритмы самоулучшения и техники объединения моделей.'}, 'en': {'title': 'Empowering Enterprises with Command A: The Future of Language Models', 'desc': 'This paper presents Command A, a large language model designed specifically for enterprise applications. It features a hybrid architecture that optimizes both performance and efficiency, supporting 23 languages to cater to global business needs. The model excels in Retrieval Augmented Generation (RAG), enabling it to automate complex business processes through effective grounding and tool usage. The training process incorporates decentralized methods, including self-refinement and model merging, and the paper also discusses the similar Command R7B model, providing insights into their training and evaluation results.'}, 'zh': {'title': 'Command A：企业应用的强大语言模型', 'desc': '本文介绍了Command A的开发，这是一种专为企业实际应用而设计的大型语言模型。Command A具备多语言能力，支持23种全球商业语言，并采用新颖的混合架构，兼顾效率与高性能。它提供了最佳的检索增强生成（RAG）能力，能够通过工具使用和基础知识支持来自动化复杂的业务流程。我们还展示了与Command A相似的Command R7B模型的结果，并发布了这两个模型的权重以供研究使用。'}}}, {'id': 'https://huggingface.co/papers/2504.01016', 'title': 'GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors', 'url': 'https://huggingface.co/papers/2504.01016', 'abstract': 'Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.', 'score': 15, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '9430b45c3324fb61', 'authors': ['Tian-Xing Xu', 'Xiangjun Gao', 'Wenbo Hu', 'Xiaoyu Li', 'Song-Hai Zhang', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'HKUST', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01016.jpg', 'data': {'categories': ['#video', '#3d', '#architecture', '#diffusion', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'GeometryCrafter: Высокоточная оценка глубины видео для 3D реконструкции', 'desc': 'GeometryCrafter - это новая система для высокоточной оценки глубины видео. Она использует вариационный автоэнкодер для кодирования карт точек и диффузионную модель для моделирования последовательностей карт точек. Система позволяет выполнять точную 3D/4D реконструкцию и оценку параметров камеры. Эксперименты показали, что GeometryCrafter превосходит существующие методы по точности 3D, временной согласованности и способности к обобщению.'}, 'en': {'title': 'GeometryCrafter: Elevating Video Depth Estimation with High-Fidelity Point Maps', 'desc': 'This paper introduces GeometryCrafter, a new framework designed to improve video depth estimation by producing high-fidelity point map sequences that maintain temporal coherence. It addresses the limitations of existing methods in achieving accurate geometric representations, which are crucial for tasks like 3D reconstruction and camera parameter estimation. The framework utilizes a point map Variational Autoencoder (VAE) to effectively encode and decode point maps, independent of the video latent distributions. By training a video diffusion model on these point map sequences, GeometryCrafter demonstrates superior performance in 3D accuracy and generalization across various datasets.'}, 'zh': {'title': 'GeometryCrafter：高保真视频深度估计的新框架', 'desc': '尽管视频深度估计取得了显著进展，但现有方法在几何保真度方面存在固有局限，限制了其在重建和其他度量基础下游任务中的应用。我们提出了GeometryCrafter，这是一种新颖的框架，可以从开放世界视频中恢复具有时间一致性的高保真点图序列，从而实现准确的3D/4D重建和相机参数估计。我们的方法核心是一个点图变分自编码器（VAE），它学习一个与视频潜在分布无关的潜在空间，以有效地进行点图编码和解码。通过利用VAE，我们训练了一个视频扩散模型，以建模基于输入视频的点图序列的分布。'}}}, {'id': 'https://huggingface.co/papers/2504.00810', 'title': 'Z1: Efficient Test-time Scaling with Code', 'url': 'https://huggingface.co/papers/2504.00810', 'abstract': 'Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.', 'score': 15, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'd982593a14ba7da9', 'authors': ['Zhaojian Yu', 'Yinghao Wu', 'Yilun Zhao', 'Arman Cohan', 'Xiao-Ping Zhang'], 'affiliations': ['Tsinghua University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00810.jpg', 'data': {'categories': ['#reasoning', '#rl', '#dataset', '#training', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование рассуждений в языковых моделях', 'desc': 'Статья представляет эффективный метод масштабирования языковых моделей во время тестирования, основанный на обучении траекториям рассуждений, связанным с кодом. Авторы создали датасет Z1-Code-Reasoning-107K, содержащий задачи по программированию с короткими и длинными решениями. Они также предложили технику Shifted Thinking Window для уменьшения избыточных токенов мышления. Модель Z1-7B, обученная на этих данных, демонстрирует способность адаптировать уровень рассуждений к сложности задач и обобщать на более широкие задачи рассуждения.'}, 'en': {'title': 'Efficient Reasoning in Large Language Models', 'desc': 'This paper introduces a new method for improving the efficiency of Large Language Models (LLMs) during problem-solving by reducing unnecessary reasoning tokens. The authors created a dataset called Z1-Code-Reasoning-107K, which includes various coding problems and their solution paths. They also developed a technique called the Shifted Thinking Window, which helps the model focus on relevant information and limits excessive reasoning. The resulting model, Z1-7B, shows strong performance on complex tasks while using significantly fewer reasoning tokens compared to other models.'}, 'zh': {'title': '高效推理，简化思考！', 'desc': '本文提出了一种高效的测试时间扩展方法，旨在通过训练大型语言模型（LLMs）在代码相关的推理轨迹上，减少多余的思考标记，同时保持性能。我们创建了一个名为Z1-Code-Reasoning-107K的数据集，包含简单和复杂的编码问题及其解决轨迹。我们还提出了一种新颖的移位思维窗口，通过去除上下文分隔标签和限制推理标记，来减轻过度思考的负担。经过训练的模型Z1-7B能够根据问题的复杂性调整推理水平，并在不同的推理任务中实现高效的测试时间扩展。'}}}, {'id': 'https://huggingface.co/papers/2504.00595', 'title': 'Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources', 'url': 'https://huggingface.co/papers/2504.00595', 'abstract': 'The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.', 'score': 15, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '3e8c667bd93d754e', 'authors': ['Weizhi Wang', 'Yu Tian', 'Linjie Yang', 'Heng Wang', 'Xifeng Yan'], 'affiliations': ['Nvidia Research', 'Seed Vision Team, ByteDance', 'UC Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2504.00595.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#training', '#data', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Открытая и эффективная мультимодальная ИИ-модель', 'desc': 'Статья представляет Open-Qwen2VL - полностью открытую мультимодальную языковую модель с 2 миллиардами параметров. Модель была эффективно обучена на 29 миллионах пар изображение-текст, используя всего 442 часа GPU A100-40G. Авторы применили динамическое разрешение изображений и мультимодальную упаковку последовательностей для повышения эффективности предобучения. Open-Qwen2VL превосходит частично открытую модель Qwen2-VL-2B по различным мультимодальным бенчмаркам, демонстрируя высокую эффективность обучения.'}, 'en': {'title': 'Unlocking Efficiency in Multimodal LLMs with Open-Qwen2VL', 'desc': 'The paper presents Open-Qwen2VL, a multimodal large language model (LLM) that is fully open-source and pre-trained on 29 million image-text pairs. It addresses challenges in multimodal LLM pre-training by utilizing advanced data filtering techniques and efficient training strategies, achieving significant improvements in training efficiency. The model is trained using a dynamic image resolution approach and multimodal sequence packing, which enhances the overall performance while reducing resource consumption. Open-Qwen2VL outperforms existing models on various benchmarks, showcasing its effectiveness and the benefits of open-source collaboration in machine learning.'}, 'zh': {'title': '高效开源的多模态大语言模型', 'desc': '本文介绍了Open-Qwen2VL，这是一个完全开源的多模态大语言模型，具有20亿参数，使用2900万对图像-文本数据进行高效预训练。我们采用了动态图像分辨率和多模态序列打包技术，显著提高了预训练的效率。通过使用MLLM和CLIP的过滤技术，提升了数据质量和训练效率。最终，Open-Qwen2VL在多个多模态基准测试中超越了部分开源的最先进模型，展示了其卓越的训练效率。'}}}, {'id': 'https://huggingface.co/papers/2504.01019', 'title': 'MixerMDM: Learnable Composition of Human Motion Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01019', 'abstract': 'Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.', 'score': 13, 'issue_id': 3022, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '745108d1df40d1b4', 'authors': ['Pablo Ruiz-Ponce', 'German Barquero', 'Cristina Palmero', 'Sergio Escalera', 'José García-Rodríguez'], 'affiliations': ['Kings College London, UK', 'Universidad de Alicante, Spain', 'Universitat de Barcelona and Computer Vision Center, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2504.01019.jpg', 'data': {'categories': ['#multimodal', '#cv', '#diffusion', '#benchmark', '#dataset'], 'emoji': '🕺', 'ru': {'title': 'Динамическое смешивание диффузионных моделей для улучшенной генерации движений человека', 'desc': 'Статья представляет MixerMDM - первую обучаемую технику композиции моделей для объединения предобученных диффузионных моделей генерации движений человека на основе текстовых описаний. В отличие от предыдущих подходов, MixerMDM обеспечивает динамическую стратегию смешивания, которая обучается состязательным образом для комбинирования процесса шумоподавления каждой модели в зависимости от заданных условий. Используя MixerMDM для объединения диффузионных моделей движения одного и нескольких человек, авторы достигают тонкого контроля над динамикой каждого человека индивидуально, а также над общим взаимодействием. Кроме того, предлагается новая техника оценки, которая измеряет качество взаимодействия и индивидуальных движений.'}, 'en': {'title': 'Dynamic Control of Human Motion Generation with MixerMDM', 'desc': 'This paper addresses the challenge of generating human motion based on textual descriptions by introducing MixerMDM, a novel learnable model composition technique. Unlike previous methods, MixerMDM dynamically combines pre-trained motion diffusion models in an adversarial manner, allowing for better control over the generated motions based on specific conditions. The approach enables fine-grained control over individual and overall interactions in multi-person scenarios. Additionally, the authors propose a new evaluation method to assess the quality of generated motions in relation to their conditions, highlighting the adaptability of MixerMDM during the denoising process.'}, 'zh': {'title': '动态混合，精细控制人类运动生成', 'desc': '本文提出了一种新的模型组合技术，称为MixerMDM，用于结合预训练的文本条件人类运动扩散模型。与以往的方法不同，MixerMDM采用动态混合策略，通过对抗训练学习如何根据生成条件组合去噪过程。该方法能够实现对单人和多人运动的精细控制，提升了每个人的动态表现及整体互动效果。此外，本文还提出了一种新的评估技术，首次量化了生成运动与条件之间的对齐程度，以及MixerMDM在去噪过程中适应混合的能力。'}}}, {'id': 'https://huggingface.co/papers/2504.00906', 'title': 'Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents', 'url': 'https://huggingface.co/papers/2504.00906', 'abstract': 'Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.', 'score': 13, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'e51174b579a417e9', 'authors': ['Saaket Agashe', 'Kyle Wong', 'Vincent Tu', 'Jiachen Yang', 'Ang Li', 'Xin Eric Wang'], 'affiliations': ['Simular Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00906.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Agent S2: Новый уровень автоматизации компьютерных задач с помощью ИИ', 'desc': 'Статья представляет Agent S2 - новую композиционную систему для автоматизации компьютерных задач через графический интерфейс. Система использует множество специализированных и обобщенных моделей, а также новые методы точной локализации элементов интерфейса и иерархического планирования действий. Agent S2 достигает значительных улучшений производительности по сравнению с существующими решениями на нескольких бенчмарках для разных операционных систем. Авторы утверждают, что их подход открывает новые возможности для повышения продуктивности человека при работе с компьютером.'}, 'en': {'title': 'Agent S2: Revolutionizing Task Automation with Smart Planning and Grounding', 'desc': 'This paper presents Agent S2, a new framework designed to improve the performance of computer use agents that automate tasks by interacting with graphical user interfaces (GUIs). The framework addresses key challenges such as accurately identifying GUI elements and planning complex tasks over time. It introduces a Mixture-of-Grounding technique for better GUI localization and Proactive Hierarchical Planning to adapt action plans based on real-time observations. Evaluations show that Agent S2 outperforms existing agents on multiple benchmarks, demonstrating significant improvements in task execution across different operating systems.'}, 'zh': {'title': 'Agent S2：智能代理的新纪元', 'desc': '本文介绍了一种名为Agent S2的新型智能代理框架，旨在通过将认知任务分配给不同的通用模型和专业模型来提高数字任务的自动化效率。我们提出了一种新的混合定位技术，以实现精确的图形用户界面（GUI）元素定位，并引入了主动层次规划，能够根据不断变化的观察动态调整行动计划。评估结果表明，Agent S2在三个主要的计算机使用基准测试中达到了新的最先进性能，显著超越了现有的领先代理。特别是在OSWorld评估中，Agent S2相较于其他代理实现了18.9%和32.7%的相对提升，展现了其在不同操作系统和应用中的良好泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2504.00509', 'title': 'Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?', 'url': 'https://huggingface.co/papers/2504.00509', 'abstract': "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.", 'score': 12, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'c9697e67f23cfa4e', 'authors': ['Kai Yan', 'Yufei Xu', 'Zhengyin Du', 'Xuesong Yao', 'Zheyu Wang', 'Xiaowen Guo', 'Jiecao Chen'], 'affiliations': ['ByteDance Seed', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.00509.jpg', 'data': {'categories': ['#multimodal', '#hallucinations', '#benchmark', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Языковые модели: умные рассуждения или простое воспроизведение?', 'desc': 'Авторы статьи предлагают новый мультимодальный бенчмарк RoR-Bench для выявления склонности языковых моделей (LLM) к простому воспроизведению заученных решений. Исследование показало, что даже передовые LLM демонстрируют серьезные проблемы с рассуждениями при небольших изменениях в условиях задач. Результаты указывают на значительное снижение производительности ведущих моделей (до 60%) на элементарных арифметических и логических задачах при изменении всего одной фразы. Авторы призывают сообщество переосмыслить реальный уровень интеллекта современных LLM.'}, 'en': {'title': 'Reassessing LLM Intelligence: Are They Truly Reasoning?', 'desc': 'This paper introduces RoR-Bench, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs). The authors investigate whether LLMs demonstrate genuine intelligence or merely replicate learned responses from their training data. Their empirical analysis reveals that even advanced LLMs, like OpenAI-o1 and DeepSeek-R1, show significant performance drops—up to 60%—when faced with slight changes in problem phrasing. This raises important questions about the actual reasoning abilities of these models and suggests a need for a deeper understanding of their intelligence.'}, 'zh': {'title': '重新审视LLM的智能水平', 'desc': '近年来，LLM基准测试的难度从小学水平迅速上升到前沿问题，这让研究人员感到我们离超越人类智能只有一步之遥。然而，LLM的推理能力是否真的是人类标准下的真正智能，还是仅仅在训练中见过的解决方案的复述？为了解决这个问题，我们提出了RoR-Bench，一个新颖的多模态基准，用于检测LLM在简单推理问题中是否存在复述行为。我们的实证分析发现，现有的顶尖LLM在条件稍微改变时，表现出极其严重的复述行为，这促使我们重新评估这些模型的真实智能水平。'}}}, {'id': 'https://huggingface.co/papers/2503.24377', 'title': 'Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models', 'url': 'https://huggingface.co/papers/2503.24377', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.', 'score': 11, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'd18ba97a459aea36', 'authors': ['Rui Wang', 'Hongru Wang', 'Boyang Xue', 'Jianhui Pang', 'Shudong Liu', 'Yi Chen', 'Jiahao Qiu', 'Derek Fai Wong', 'Heng Ji', 'Kam-Fai Wong'], 'affiliations': ['Princeton University', 'The Chinese University of Hong Kong', 'The University of Hong Kong', 'University of Illinois Urbana-Champaign', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.24377.jpg', 'data': {'categories': ['#inference', '#survey', '#reasoning', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Экономия рассуждений в больших языковых моделях: балансируя производительность и затраты', 'desc': 'Статья рассматривает проблему баланса между производительностью и вычислительными затратами в больших языковых моделях (LLM) при выполнении задач рассуждения. Авторы анализируют причины неэффективности рассуждений, различные паттерны рассуждений и потенциальные решения для достижения экономии рассуждений. Исследование охватывает как этап после обучения, так и этап вывода во время тестирования LLM. Авторы стремятся предоставить ценные insights и выделить открытые проблемы для продвижения исследований в этой области.'}, 'en': {'title': 'Balancing Performance and Cost in Language Model Reasoning', 'desc': 'This paper discusses the advancements in Large Language Models (LLMs) that allow them to perform complex reasoning tasks more effectively. It highlights the difference between two types of reasoning: System 1, which is fast and efficient but less accurate, and System 2, which is slower and more accurate but computationally expensive. The authors introduce the concept of reasoning economy, which aims to balance the trade-off between performance and computational costs. They analyze the inefficiencies in reasoning, explore different reasoning patterns, and propose solutions to enhance the reasoning economy of LLMs, providing insights for future research.'}, 'zh': {'title': '推理经济：平衡性能与计算成本的关键', 'desc': '近年来，大型语言模型（LLMs）的进步显著提升了其执行复杂推理任务的能力，尤其是在快速直觉思维（系统1）与缓慢深度推理（系统2）之间的转变。虽然系统2推理提高了任务的准确性，但由于其思维缓慢和推理行为低效，往往会带来较高的计算成本。相对而言，系统1推理计算效率高，但可能导致性能不佳。因此，平衡性能与计算成本之间的权衡，形成了推理经济的概念，这是本研究的核心。'}}}, {'id': 'https://huggingface.co/papers/2503.22952', 'title': 'OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts', 'url': 'https://huggingface.co/papers/2503.22952', 'abstract': 'The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.', 'score': 11, 'issue_id': 3024, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '8b78ccf427a5cdc0', 'authors': ['Yuxuan Wang', 'Yueqian Wang', 'Bo Chen', 'Tong Wu', 'Dongyan Zhao', 'Zilong Zheng'], 'affiliations': ['Beijing Institute for General Artificial Intelligence', 'State Key Laboratory of General Artificial Intelligence', 'Wangxuan Institute of Computer Technology, Peking University', 'X-LANCE Lab, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22952.jpg', 'data': {'categories': ['#video', '#games', '#inference', '#multimodal', '#reasoning', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'OmniMMI: Новый стандарт для оценки мультимодальных моделей в потоковом видео', 'desc': 'Статья представляет OmniMMI - комплексный бенчмарк для оценки возможностей мультимодальных языковых моделей в контексте потокового видео. Бенчмарк включает более 1000 видео и 2000 вопросов, охватывающих шесть различных подзадач. Авторы также предлагают новую архитектуру Multi-modal Multiplexing Modeling (M4) для эффективной обработки потокового видео. Этот подход позволяет модели одновременно воспринимать визуальную и аудиоинформацию во время генерации текста.'}, 'en': {'title': 'Enhancing Interaction with OmniLLMs in Streaming Video', 'desc': 'This paper presents OmniMMI, a new benchmark for evaluating Omni language models (OmniLLMs) in the context of streaming video. It addresses the challenges of understanding and reasoning in real-time video interactions, which are often overlooked in current benchmarks. The benchmark includes a large dataset of over 1,121 videos and 2,290 questions, focusing on proactive reasoning across six subtasks. Additionally, the authors introduce a framework called Multi-modal Multiplexing Modeling (M4) that enhances the efficiency of streaming models by allowing them to process audio and visual data simultaneously while generating responses.'}, 'zh': {'title': '提升多模态语言模型的互动能力', 'desc': '本论文介绍了OmniMMI，这是一个专为Omni语言模型在流媒体视频环境中设计的多模态交互基准。该基准包含超过1121个视频和2290个问题，旨在解决现有视频基准中流媒体视频理解和主动推理的两个关键挑战。我们还提出了一种新框架，称为多模态复用建模（M4），旨在实现高效推理的流媒体模型，能够在生成内容的同时进行视觉和听觉处理。通过这些创新，我们希望提升多模态语言模型在实际应用中的互动能力。'}}}, {'id': 'https://huggingface.co/papers/2504.00927', 'title': 'Multi-Token Attention', 'url': 'https://huggingface.co/papers/2504.00927', 'abstract': 'Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other\'s attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector\'s capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method\'s ability to leverage richer information proves particularly beneficial.', 'score': 10, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '2af4c7adceecde31', 'authors': ['Olga Golovneva', 'Tianlu Wang', 'Jason Weston', 'Sainbayar Sukhbaatar'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2504.00927.jpg', 'data': {'categories': ['#long_context', '#architecture', '#benchmark', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Многотокенное внимание: новый шаг в повышении точности языковых моделей', 'desc': 'Статья представляет новый метод внимания для языковых моделей - Multi-Token Attention (MTA). В отличие от стандартного механизма soft attention, MTA позволяет учитывать информацию из нескольких токенов запроса и ключа одновременно. Это достигается с помощью операций свертки над запросами, ключами и головами внимания. MTA демонстрирует улучшенную производительность на ряде популярных бенчмарков, особенно в задачах, требующих поиска информации в длинных контекстах.'}, 'en': {'title': 'Unlocking Richer Context with Multi-Token Attention', 'desc': 'This paper introduces Multi-Token Attention (MTA), a novel attention mechanism designed to improve the performance of large language models (LLMs). Unlike traditional single token attention, which relies on a single query and key token vector, MTA allows for the simultaneous consideration of multiple query and key vectors. By utilizing convolution operations, MTA enhances the interaction between nearby queries and keys, leading to more accurate attention weights. The results show that MTA significantly outperforms standard Transformer models, especially in tasks involving long contexts and information retrieval.'}, 'zh': {'title': '多令牌注意力：提升上下文理解的关键', 'desc': '软注意力机制是大型语言模型（LLMs）中一个重要的组成部分，用于在给定上下文中定位相关部分。然而，传统的单个令牌注意力方法仅依赖于单个查询和键向量的相似性，这限制了信息的使用。为了解决这个问题，我们提出了一种新的注意力方法——多令牌注意力（MTA），它允许LLMs同时基于多个查询和键向量来调整注意力权重。通过对查询、键和头部应用卷积操作，我们的方法能够利用更丰富的信息，从而在长上下文中更准确地定位相关内容。'}}}, {'id': 'https://huggingface.co/papers/2504.01017', 'title': 'Scaling Language-Free Visual Representation Learning', 'url': 'https://huggingface.co/papers/2504.01017', 'abstract': 'Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.', 'score': 9, 'issue_id': 3023, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '9ab970f68b26c2ea', 'authors': ['David Fan', 'Shengbang Tong', 'Jiachen Zhu', 'Koustuv Sinha', 'Zhuang Liu', 'Xinlei Chen', 'Michael Rabbat', 'Nicolas Ballas', 'Yann LeCun', 'Amir Bar', 'Saining Xie'], 'affiliations': ['FAIR, Meta', 'New York University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01017.jpg', 'data': {'categories': ['#cv', '#training', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Визуальное самообучение не уступает языковому контролю при масштабировании', 'desc': 'Исследование сравнивает методы визуального самоконтролируемого обучения (SSL) и контрастивного обучения на языке и изображениях (CLIP) в задачах мультимодального анализа. Авторы обнаружили, что при обучении на одинаковых данных, модели SSL масштабируются лучше и достигают производительности CLIP на различных задачах визуального анализа вопросов и ответов (VQA). Результаты показывают, что чисто визуальное SSL может соответствовать обучению с языковым контролем при увеличении масштаба. Это открывает новые возможности для обучения визуальных представлений без использования языковой разметки.'}, 'en': {'title': 'Visual SSL: Bridging the Gap with Scale and Data', 'desc': 'This paper investigates the performance gap between Visual Self-Supervised Learning (SSL) and Contrastive Language-Image Pretraining (CLIP) in multimodal tasks like Visual Question Answering (VQA). The authors explore whether this gap is due to the absence of language supervision in visual SSL or differences in the training datasets used. By training both types of models on the same MetaCLIP data, they find that visual SSL models can outperform CLIP models when scaled up, achieving comparable performance on various benchmarks. This suggests that visual SSL can effectively compete with language-supervised methods, highlighting its potential for advancing vision-centric representation learning.'}, 'zh': {'title': '视觉自监督学习的潜力与CLIP相媲美', 'desc': '本研究探讨了视觉自监督学习（SSL）与对比语言-图像预训练（CLIP）在多模态任务中的表现差异。我们通过在相同的MetaCLIP数据上训练视觉SSL和CLIP模型，来分析语言监督的缺乏是否是导致视觉SSL落后的原因。结果显示，视觉SSL模型在数据和模型容量方面的扩展性优于CLIP模型，并且在参数达到70亿时性能仍未饱和。我们的发现表明，纯视觉SSL在大规模下可以达到与语言监督视觉预训练相当的性能，为视觉中心的表示学习开辟了新的机会。'}}}, {'id': 'https://huggingface.co/papers/2504.01005', 'title': 'When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning', 'url': 'https://huggingface.co/papers/2504.01005', 'abstract': 'Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.', 'score': 9, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'ee8e4951bf6c7a18', 'authors': ['Nishad Singhi', 'Hritik Bansal', 'Arian Hosseini', 'Aditya Grover', 'Kai-Wei Chang', 'Marcus Rohrbach', 'Anna Rohrbach'], 'affiliations': ['Google DeepMind', 'Mila', 'TU Darmstadt & hessian.AI', 'University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2504.01005.jpg', 'data': {'categories': ['#training', '#math', '#optimization', '#reasoning', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Баланс между генерацией решений и их верификацией в языковых моделях', 'desc': 'Статья исследует стратегии улучшения способностей больших языковых моделей (LLM) к рассуждениям, особенно в задачах математического характера. Авторы сравнивают метод Self-Consistency (SC), генерирующий множество решений и выбирающий наиболее частое, с подходом Generative Reward Models (GenRM), который оценивает решения путем генерации цепочек рассуждений. Исследование показывает, что SC более эффективен по вычислительным ресурсам для большинства практических задач. Авторы также выводят законы масштабирования для парадигмы GenRM, предоставляя практические рекомендации по оптимизации вычислений во время тестирования.'}, 'en': {'title': 'Balancing Solution Generation and Verification for Efficient Reasoning in LLMs', 'desc': 'This paper explores how to improve the reasoning abilities of large language models (LLMs) during problem-solving by adjusting the amount of computation used at test time. It compares two methods: Self-Consistency (SC), which generates multiple answers and picks the most common, and Generative Reward Models (GenRM), which scores answers based on a next-token prediction approach. The study finds that SC is generally more efficient in terms of compute resources compared to GenRM, especially under limited budgets. The authors provide insights on how to effectively balance the generation of solutions and their verification to optimize performance.'}, 'zh': {'title': '优化推理能力：解生成与验证的平衡', 'desc': '本文探讨了在大语言模型（LLMs）中，如何通过扩展测试时计算来提升推理能力，尤其是在数学问题解决任务中。传统的自一致性（Self-Consistency, SC）方法通过生成多个解并采用多数投票选择最常见的答案。最近的生成奖励模型（Generative Reward Models, GenRM）则将验证重构为下一个标记预测任务，从而在推理时引入新的扩展方式。研究表明，在固定的推理预算下，SC在大多数实际情况下比GenRM更具计算效率，提供了在测试时扩展中优化解生成与验证的实用指导。'}}}, {'id': 'https://huggingface.co/papers/2503.23434', 'title': 'Towards Trustworthy GUI Agents: A Survey', 'url': 'https://huggingface.co/papers/2503.23434', 'abstract': 'GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.', 'score': 9, 'issue_id': 3024, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'e19e4d94bcea9cb0', 'authors': ['Yucheng Shi', 'Wenhao Yu', 'Wenlin Yao', 'Wenhu Chen', 'Ninghao Liu'], 'affiliations': ['Amazon', 'Tencent AI Seattle Lab', 'University of Georgia', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23434.jpg', 'data': {'categories': ['#ethics', '#security', '#agents', '#survey', '#training', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Доверие к ИИ: обеспечение надежности графических агентов', 'desc': 'Это обзор исследует надежность графических агентов искусственного интеллекта, взаимодействующих с цифровыми интерфейсами. Рассматриваются пять ключевых аспектов: уязвимости безопасности, надежность в динамических средах, прозрачность и объяснимость, этические вопросы и методологии оценки. Выявлены основные проблемы, такие как уязвимость к состязательным атакам и каскадные сбои при последовательном принятии решений. Подчеркивается необходимость разработки надежных стандартов безопасности и ответственных практик разработки для широкого внедрения графических ИИ-агентов.'}, 'en': {'title': 'Ensuring Trust in Autonomous GUI Agents', 'desc': 'This paper surveys the trustworthiness of GUI agents that use large foundation models to interact with digital interfaces. It highlights five critical dimensions of trustworthiness: security vulnerabilities, reliability in changing environments, transparency, ethical considerations, and evaluation methods. The authors discuss significant challenges such as susceptibility to adversarial attacks and the need for realistic evaluation benchmarks. The paper emphasizes the importance of developing robust safety standards and responsible practices as GUI agents become more prevalent.'}, 'zh': {'title': '构建可信赖的GUI代理，保障安全与隐私', 'desc': '本论文探讨了基于大型基础模型的图形用户界面（GUI）代理的信任性问题。我们分析了五个关键维度，包括安全漏洞、动态环境中的可靠性、透明性与可解释性、伦理考量以及评估方法。研究还指出了主要挑战，如对抗性攻击的脆弱性、序列决策中的级联失败模式，以及缺乏现实的评估基准。这些问题不仅阻碍了GUI代理的实际应用，还需要超越任务成功的全面缓解策略。'}}}, {'id': 'https://huggingface.co/papers/2503.23733', 'title': 'AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization', 'url': 'https://huggingface.co/papers/2503.23733', 'abstract': 'Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.', 'score': 8, 'issue_id': 3017, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'ed45063868071c13', 'authors': ['Yiyang Du', 'Xiaochen Wang', 'Chi Chen', 'Jiabo Ye', 'Yiru Wang', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Zhifang Sui', 'Maosong Sun', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China', 'Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China', 'Institute of Intelligent Computing, Alibaba Group', 'Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China', 'ModelTC Open Source Organization, Beijing, China', 'School of Software Microelectronics, Peking University, Beijing, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.23733.jpg', 'data': {'categories': ['#training', '#architecture', '#transfer_learning', '#optimization', '#multimodal', '#benchmark'], 'emoji': '🔀', 'ru': {'title': 'AdaMMS: Эффективное слияние разнородных мультимодальных языковых моделей', 'desc': 'Статья представляет AdaMMS - новый метод объединения мультимодальных языковых моделей (MLLM) с разнородной архитектурой. Метод включает три этапа: отображение параметров между моделями, их слияние с помощью линейной интерполяции и поиск оптимальных гиперпараметров. AdaMMS решает проблемы, связанные с различиями в архитектуре и асимметрией в пространстве параметров разнородных MLLM. Эксперименты показали превосходство AdaMMS над существующими методами объединения моделей на различных мультимодальных задачах.'}, 'en': {'title': 'Merging Diverse Models with AdaMMS', 'desc': 'This paper introduces AdaMMS, a new method for merging Multimodal Large Language Models (MLLMs) that have different architectures. Traditional merging techniques struggle with these heterogeneous models due to their varying structures and parameter spaces. AdaMMS addresses this by first mapping the models, then merging their weights through linear interpolation, and finally optimizing hyper-parameters using an unsupervised approach. The results show that AdaMMS significantly improves performance on vision-language tasks compared to earlier methods.'}, 'zh': {'title': '异质模型合并的新突破', 'desc': '最近，模型合并方法在结合多个大型语言模型（LLMs）在不同任务上的能力方面表现出强大的优势。以往的模型合并方法主要集中在合并具有相同架构的同质模型，但在处理具有固有异质性的多模态大型语言模型（MLLMs）时面临挑战。我们提出了AdaMMS，这是一种专为异质MLLMs设计的新型模型合并方法，采用映射、合并和搜索三个步骤来解决这些挑战。通过设计模型之间的映射函数、对模型权重进行线性插值以及提出无监督的超参数选择方法，AdaMMS在各种视觉-语言基准测试中超越了以往的模型合并方法。'}}}, {'id': 'https://huggingface.co/papers/2504.00557', 'title': 'Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features', 'url': 'https://huggingface.co/papers/2504.00557', 'abstract': 'Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.', 'score': 7, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'c5628fbf1a189a06', 'authors': ['Jewon Lee', 'Ki-Ung Song', 'Seungmin Yang', 'Donguk Lim', 'Jaeyeon Kim', 'Wooksu Shin', 'Bo-Kyeong Kim', 'Yong Jae Lee', 'Tae-Ho Kim'], 'affiliations': ['Nota Inc.', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.00557.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#cv'], 'emoji': '✂️', 'ru': {'title': 'Эффективное сжатие визуальных данных в мультимодальных ИИ-моделях', 'desc': 'Статья представляет метод снижения вычислительных затрат в крупных визуально-языковых моделях путем сокращения визуальных токенов. Авторы фокусируются на моделях с кросс-вниманием, выявляя проблему большого размера кэша ключ-значение для визуальных токенов. Они предлагают алгоритм избирательного удаления избыточных визуальных признаков, основанный на разреженности карт кросс-внимания. Метод позволяет снизить задержку и использование памяти при сохранении производительности модели на уровне базовых показателей.'}, 'en': {'title': 'Trimmed Llama: Efficient Visual Token Reduction for Faster Inference', 'desc': 'This paper presents a method called Trimmed Llama, which reduces the number of visual tokens in large vision-language models (LVLMs) to lower inference costs. Unlike previous studies that focused on self-attention models, this work specifically targets cross-attention models, which are known for their better performance. The authors highlight that the key-value (KV) cache for image tokens in cross-attention layers is much larger than that for text tokens, creating a significant computational bottleneck. By selectively pruning redundant visual features based on the sparse nature of cross-attention maps, the model achieves a 50% reduction in visual features, leading to decreased latency and memory usage while maintaining performance benchmarks.'}, 'zh': {'title': '视觉特征修剪，提升推理效率', 'desc': '本论文提出了一种视觉标记减少的方法，以降低大型视觉语言模型（LVLMs）在推理时的计算成本。与以往只针对自注意力模型的研究不同，我们的工作专注于基于交叉注意力的模型，这些模型在性能上更为优越。我们发现交叉注意力层中图像标记的键值（KV）缓存大小远大于自注意力层中的文本标记，成为计算瓶颈。通过利用交叉注意力图的稀疏特性，我们选择性地修剪冗余的视觉特征，从而有效减少KV缓存需求，降低推理延迟和内存使用，同时保持基准性能。'}}}, {'id': 'https://huggingface.co/papers/2503.22165', 'title': 'Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.22165', 'abstract': 'Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts.', 'score': 7, 'issue_id': 3034, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '76c9bb027c844f9c', 'authors': ['Zhanke Zhou', 'Zhaocheng Zhu', 'Xuan Li', 'Mikhail Galkin', 'Xiao Feng', 'Sanmi Koyejo', 'Jian Tang', 'Bo Han'], 'affiliations': ['HEC Montreal', 'Intel AI Lab', 'Mila - Quebec AI Institute', 'Stanford University', 'TMLR Group, Hong Kong Baptist University', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.22165.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#reasoning', '#multimodal', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Визуализация мыслительного процесса языковых моделей', 'desc': "Статья представляет новый инструмент визуализации под названием 'landscape of thoughts' для анализа процесса рассуждений больших языковых моделей (LLM). Этот инструмент позволяет отображать пути рассуждений в виде двумерных графиков, используя t-SNE для визуализации векторов признаков, представляющих состояния в процессе рассуждения. Анализ с помощью 'landscape of thoughts' эффективно различает сильные и слабые модели, правильные и неправильные ответы, а также различные задачи рассуждения. Инструмент также выявляет нежелательные паттерны рассуждений, такие как низкая согласованность и высокая неопределенность."}, 'en': {'title': 'Visualizing Reasoning Paths in Language Models', 'desc': "This paper introduces a new visualization tool called 'landscape of thoughts' that helps users understand how large language models (LLMs) reason through problems. It represents reasoning paths as feature vectors, which show how close each reasoning step is to possible answers. By using a technique called t-SNE, the tool creates two-dimensional plots that allow for easy comparison of different models and their reasoning effectiveness. The tool also identifies problematic reasoning patterns and can be adapted to evaluate the accuracy of reasoning paths in various tasks."}, 'zh': {'title': '揭示大型语言模型的推理路径', 'desc': '本文介绍了一种名为“思维景观”的可视化工具，用于分析大型语言模型（LLMs）的推理路径。该工具通过将推理路径中的状态表示为特征向量，量化它们与所有答案选项的距离，并使用t-SNE进行二维可视化。通过对思维景观的定性和定量分析，可以有效区分强弱模型、正确与错误答案，以及不同的推理任务。此外，该工具还能够揭示不理想的推理模式，如低一致性和高不确定性。'}}}, {'id': 'https://huggingface.co/papers/2504.00294', 'title': 'Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead', 'url': 'https://huggingface.co/papers/2504.00294', 'abstract': "Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.", 'score': 6, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'b455a4adb4eae588', 'authors': ['Vidhisha Balachandran', 'Jingya Chen', 'Lingjiao Chen', 'Shivam Garg', 'Neel Joshi', 'Yash Lara', 'John Langford', 'Besmira Nushi', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00294.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#math', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Масштабирование LLM: потенциал и ограничения в сложных задачах', 'desc': 'Статья исследует влияние масштабирования во время вывода на способности крупных языковых моделей (LLM) решать сложные задачи. Авторы сравнивают обычные модели и модели, настроенные на масштабирование, на восьми различных типах задач. Результаты показывают, что преимущества масштабирования варьируются в зависимости от задачи и уменьшаются с ростом сложности проблемы. Исследование также выявляет потенциал для улучшения производительности моделей при использовании совершенных верификаторов или сильной обратной связи.'}, 'en': {'title': 'Scaling Inference for Smarter Problem Solving in LLMs', 'desc': 'This paper explores how inference-time scaling can improve the reasoning abilities of large language models (LLMs) when tackling complex problems. It examines the effectiveness of extending generated scratchpads for various tasks, including math reasoning and navigation, across nine advanced models. The study finds that while scaling can enhance performance, its benefits vary by task and may decrease with increased complexity. Additionally, the research indicates that simply increasing the number of tokens does not guarantee better accuracy, but using strong feedback mechanisms can lead to significant performance improvements.'}, 'zh': {'title': '推理时间扩展：提升模型推理能力的关键', 'desc': '本研究探讨了推理时间扩展对大型语言模型（LLMs）在复杂问题上的推理能力的影响。我们分析了九种最先进模型在八个具有挑战性的任务上的表现，包括数学推理和空间推理等。结果表明，推理时间扩展的优势因任务而异，且在问题复杂性增加时会减弱。尽管使用更多的标记并不总能提高准确性，但在有强反馈的情况下，所有模型都显示出显著的性能提升潜力。'}}}, {'id': 'https://huggingface.co/papers/2504.00869', 'title': 'm1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models', 'url': 'https://huggingface.co/papers/2504.00869', 'abstract': "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.", 'score': 5, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'bd9586b08ce02a05', 'authors': ['Xiaoke Huang', 'Juncheng Wu', 'Hui Liu', 'Xianfeng Tang', 'Yuyin Zhou'], 'affiliations': ['Amazon Research', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2504.00869.jpg', 'data': {'categories': ['#reasoning', '#healthcare', '#science', '#training', '#inference'], 'emoji': '🩺', 'ru': {'title': 'Медицинские знания важнее глубины рассуждений', 'desc': 'В статье исследуется применение техники масштабирования во время тестирования для улучшения медицинских рассуждений в больших языковых моделях. Авторы представляют метод m1, который повышает способность модели к медицинским рассуждениям на этапе вывода. Исследование показывает, что масштабирование во время тестирования улучшает результаты на медицинских задачах, но выявляет, что недостаток медицинских знаний ограничивает дальнейший прогресс. Для достижения лучших результатов необходимо увеличивать объем данных, улучшать их качество и расширять возможности модели.'}, 'en': {'title': 'Enhancing Medical Reasoning with Test-Time Scaling', 'desc': "This paper explores the use of test-time scaling to improve the reasoning abilities of large language models specifically in the medical field. The authors introduce a method called m1, which enhances medical reasoning during inference, showing that smaller models can achieve state-of-the-art results. They find that while increasing the reasoning token budget can help, it may also lead to performance degradation if overused. The study emphasizes that having rich medical knowledge is crucial for effective reasoning, rather than just increasing the model's complexity or depth of reasoning."}, 'zh': {'title': '医学推理的新突破：测试时缩放的力量', 'desc': '本文探讨了测试时缩放技术在医学推理中的应用，提出了一种名为m1的方法，能够有效提升模型在推理时的医学能力。研究表明，测试时缩放在多种医学任务中均能显著提高推理效果，尤其是对于参数少于10B的轻量级微调模型，能够达到新的最佳性能。我们发现，最佳的推理令牌预算约为4K，超出此范围可能导致性能下降。此外，增加数据规模、提高数据质量和扩展模型容量是提升医学知识基础的关键，尤其是在小模型性能饱和的情况下。'}}}, {'id': 'https://huggingface.co/papers/2503.23361', 'title': 'Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base', 'url': 'https://huggingface.co/papers/2503.23361', 'abstract': "Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.", 'score': 4, 'issue_id': 3017, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '9b957c49c958aea3', 'authors': ['Linxin Song', 'Xuwei Ding', 'Jieyu Zhang', 'Taiwei Shi', 'Ryotaro Shimizu', 'Rahul Gupta', 'Yang Liu', 'Jian Kang', 'Jieyu Zhao'], 'affiliations': ['AGI', 'Amazon', 'University of Rochester', 'University of Southern California', 'University of Washington', 'University of Wisconsin-Madison', 'ZOZO Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23361.jpg', 'data': {'categories': ['#training', '#hallucinations', '#optimization', '#graphs', '#benchmark', '#data'], 'emoji': '🔍', 'ru': {'title': 'SEA: Эффективный поиск пробелов в знаниях языковых моделей', 'desc': 'Эта статья представляет новый метод под названием SEA (стохастический подъем ошибок) для эффективного обнаружения пробелов в знаниях крупных языковых моделей (LLM). SEA использует итеративный процесс оптимизации, чтобы находить новые кандидаты на ошибки, основываясь на семантическом сходстве с ранее обнаруженными ошибками. Метод применяет иерархический поиск и построение графа отношений для выявления систематических ошибок. Эмпирические результаты показывают, что SEA значительно превосходит существующие методы по эффективности обнаружения ошибок в LLM.'}, 'en': {'title': 'Uncovering Knowledge Deficiencies in LLMs Efficiently with SEA', 'desc': 'This paper introduces a new method called Stochastic Error Ascent (SEA) to identify knowledge deficiencies in large language models (LLMs) that often produce unreliable outputs. SEA efficiently discovers errors by using a stochastic optimization approach, focusing on high-error candidates based on their similarity to previously identified failures. The framework enhances its search capabilities through hierarchical retrieval and a directed acyclic graph to track error propagation. The results show that SEA significantly outperforms existing methods in uncovering knowledge errors while drastically reducing the cost of error discovery.'}, 'zh': {'title': '发现LLM知识缺陷的新方法', 'desc': '大型语言模型（LLMs）在语言能力上表现出色，但常常无法准确保留事实知识，导致幻觉和不可靠的输出。我们提出了一种名为随机错误上升（SEA）的框架，用于在严格的查询预算下发现闭合权重LLMs中的知识缺陷。SEA通过利用与先前观察到的失败的语义相似性，迭代检索新的高错误候选项，从而将错误发现过程形式化为随机优化过程。实验证明，SEA发现的知识错误数量显著高于现有方法，同时大幅降低了每个错误的成本。'}}}, {'id': 'https://huggingface.co/papers/2504.00072', 'title': 'Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs', 'url': 'https://huggingface.co/papers/2504.00072', 'abstract': "We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page.", 'score': 3, 'issue_id': 3021, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '48f5266ddbfa7bca', 'authors': ['Lucas Ventura', 'Antoine Yang', 'Cordelia Schmid', 'Gül Varol'], 'affiliations': ['Google DeepMind', 'Inria, Ecole normale superieure, CNRS, PSL Research University', 'LIGM, Ecole des Ponts, IP Paris, Univ Gustave Eiffel, CNRS'], 'pdf_title_img': 'assets/pdf/title_img/2504.00072.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#open_source', '#video', '#multimodal'], 'emoji': '📽️', 'ru': {'title': 'Эффективное разделение видео на главы с помощью ИИ', 'desc': "Статья представляет новый подход к автоматическому разделению видео на главы с использованием большой языковой модели (LLM). Метод 'Chapter-Llama' обрабатывает транскрипты речи и описания кадров, выбранных на основе содержания речи. LLM обучена определять временные метки границ глав и генерировать их названия. Этот подход значительно превосходит существующие методы на бенчмарке VidChapters-7M, показывая F1-score 45.3 против 26.7."}, 'en': {'title': 'Efficient Video Chaptering with Chapter-Llama', 'desc': "This paper presents a novel approach to video chaptering, which involves dividing long videos into meaningful segments and generating titles for these segments. The authors introduce the 'Chapter-Llama' framework that utilizes a pretrained large language model (LLM) to process speech transcripts and video captions efficiently. By implementing a speech-guided frame selection strategy, they enhance the model's performance while reducing the need for extensive captioning. The results show significant improvements in chaptering accuracy, achieving a notable F1 score on the VidChapters-7M benchmark, and the authors provide their code and models for further research."}, 'zh': {'title': '高效视频章节划分的新方法', 'desc': "本文探讨了视频章节划分的任务，即将长视频时间线划分为语义单元并生成相应的章节标题。我们提出了'Chapter-Llama'框架，通过高效处理文本领域的问题，实现了对长达一小时视频的强大章节划分性能。该方法利用了预训练的大型语言模型（LLM），输入包括语音转录文本和描述视频帧的字幕，以及它们各自的时间戳。我们还提出了一种基于语音转录内容的轻量级帧选择策略，显著提高了章节划分的效率和准确性。"}}}, {'id': 'https://huggingface.co/papers/2503.24210', 'title': 'DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.24210', 'abstract': 'Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io', 'score': 2, 'issue_id': 3023, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'df1e0752d5790146', 'authors': ['Seungjun Lee', 'Gim Hee Lee'], 'affiliations': ['Department of Computer Science, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24210.jpg', 'data': {'categories': ['#synthetic', '#3d', '#cv', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Четкое 3D из размытого 2D: DiET-GS раскрывает детали', 'desc': 'Статья представляет DiET-GS - новый метод для реконструкции четких 3D-представлений из размытых многоракурсных изображений. Авторы используют событийные камеры и диффузионные модели для улучшения качества синтеза новых ракурсов. Ключевая идея заключается в ограничении 3DGS с помощью двойного интеграла событий, что позволяет достичь точной цветопередачи и хорошо определенных деталей. Результаты экспериментов показывают значительное превосходство DiET-GS над существующими методами.'}, 'en': {'title': 'Enhancing 3D Image Clarity with DiET-GS', 'desc': 'This paper addresses the challenge of creating clear 3D images from blurry multi-view pictures, a common issue in computer vision. The authors introduce DiET-GS, a new framework that combines event-based camera data with a diffusion prior to improve the quality of 3D image synthesis. By using a two-stage training approach, the framework effectively restores accurate colors and fine details in the images. The results show that DiET-GS outperforms existing methods in generating high-quality novel views from both synthetic and real-world datasets.'}, 'zh': {'title': '清晰三维重建的新方法', 'desc': '本论文提出了一种名为DiET-GS的框架，用于从模糊的多视图图像中重建清晰的三维表示。该方法结合了无模糊事件流和扩散先验，通过两阶段的训练策略来提高图像质量。我们引入了一种新的约束方法，利用事件双重积分来确保颜色准确和细节清晰。此外，我们还提出了一种简单的技术，利用扩散先验进一步增强边缘细节。'}}}, {'id': 'https://huggingface.co/papers/2503.23157', 'title': 'Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL', 'url': 'https://huggingface.co/papers/2503.23157', 'abstract': 'Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.', 'score': 2, 'issue_id': 3029, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '083970087ba6180e', 'authors': ['Mohammadreza Pourreza', 'Shayan Talaei', 'Ruoxi Sun', 'Xingchen Wan', 'Hailong Li', 'Azalia Mirhoseini', 'Amin Saberi', 'Sercan "O. Arik'], 'affiliations': ['Google Cloud', 'Google DeepMind', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23157.jpg', 'data': {'categories': ['#rlhf', '#rl', '#optimization', '#training', '#benchmark', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Улучшение Text-to-SQL с помощью обучения с подкреплением и частичных наград', 'desc': 'Статья представляет новый подход к задаче Text-to-SQL, основанный на обучении с подкреплением (RL). Авторы предлагают набор частичных наград, специально разработанных для решения проблемы разреженности наград в RL. Используя групповую относительную оптимизацию политики (GRPO), метод стимулирует большие языковые модели (LLM) развивать навыки рассуждения для генерации SQL-запросов. Результаты показывают, что RL-обучение с предложенными наградами превосходит supervised fine-tuning и даже более крупные проприетарные модели на бенчмарке BIRD.'}, 'en': {'title': 'Enhancing Text-to-SQL with Tailored Reinforcement Learning Rewards', 'desc': 'This paper addresses the complex task of converting natural language into SQL queries, which requires understanding language, database structures, and formulating precise queries. The authors critique existing methods that use fixed reasoning paths, which can hinder performance, and propose a new approach that utilizes tailored partial rewards to improve reinforcement learning outcomes. By implementing group relative policy optimization (GRPO), their method encourages large language models to enhance their reasoning skills, leading to better SQL generation. The results show that their reinforcement learning approach outperforms traditional supervised fine-tuning, achieving higher accuracy on benchmark tests with a smaller model size.'}, 'zh': {'title': '提升文本到SQL的推理能力与准确性', 'desc': '本文探讨了文本到SQL的任务，这是一项涉及自然语言理解和数据库架构理解的复杂任务。现有的方法往往依赖于手工设计的推理路径，限制了其效果。我们提出了一种新的部分奖励机制，专门针对文本到SQL任务，旨在解决强化学习中的奖励稀疏问题。通过使用群体相对策略优化（GRPO），我们的模型在准确性和推理能力上都表现优异，超越了许多现有的模型。'}}}, {'id': 'https://huggingface.co/papers/2503.21860', 'title': 'ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning', 'url': 'https://huggingface.co/papers/2503.21860', 'abstract': 'Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.', 'score': 2, 'issue_id': 3017, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '0d9ea55946287027', 'authors': ['Kailin Li', 'Puhao Li', 'Tengyu Liu', 'Yuyang Li', 'Siyuan Huang'], 'affiliations': ['Department of Automation, Tsinghua University', 'Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.21860.jpg', 'data': {'categories': ['#training', '#dataset', '#robotics', '#transfer_learning', '#optimization', '#agents'], 'emoji': '🤖', 'ru': {'title': 'ManipTrans: эффективная передача человеческих навыков манипуляции роботам', 'desc': 'ManipTrans - это новый метод передачи навыков бимануальной манипуляции от человека к роботизированным рукам в симуляции. Он включает предварительное обучение имитатора траектории движения рук и дообучение специфического остаточного модуля с учетом ограничений взаимодействия. Метод превосходит существующие подходы по точности и эффективности выполнения сложных задач манипуляции. На его основе создан крупномасштабный датасет DexManipNet с 3.3 тыс. эпизодов роботизированной манипуляции для обучения политик управления ловкими руками.'}, 'en': {'title': 'Efficiently Teaching Robots to Manipulate Like Humans', 'desc': 'This paper presents ManipTrans, a two-stage method designed to transfer human bimanual manipulation skills to robotic hands in a simulated environment. The first stage involves pre-training a trajectory imitator that learns to replicate human hand movements, while the second stage fine-tunes a residual module to enhance performance under specific interaction constraints. This approach allows for efficient learning and execution of complex tasks, outperforming existing methods in terms of success rate and efficiency. Additionally, the authors introduce DexManipNet, a comprehensive dataset that includes diverse manipulation tasks, paving the way for improved policy training and real-world applications of dexterous robotic hands.'}, 'zh': {'title': '高效转移人类双手技能的机器人手', 'desc': '本论文介绍了一种名为ManipTrans的新方法，用于将人类双手的技能高效地转移到灵巧的机器人手上。该方法分为两个阶段：首先训练一个通用的轨迹模仿器来模拟手部动作，然后在交互约束下微调特定的残差模块，从而实现复杂双手任务的高效学习和准确执行。实验结果表明，ManipTrans在成功率、保真度和效率上超越了现有的最先进方法。此外，利用ManipTrans，我们创建了一个名为DexManipNet的大规模数据集，包含了3.3K个机器人操作的实例，支持进一步的策略训练和实际应用。'}}}, {'id': 'https://huggingface.co/papers/2503.24219', 'title': 'MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote\n  Sensing', 'url': 'https://huggingface.co/papers/2503.24219', 'abstract': 'We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially supervised OD task. In the first stage, we construct a graph representation of each image, comprising object queries, class embeddings, and proposal locations. Then, our task-aware architecture processes this graph to perform the VG task. The model consists of: (i) a multi-branch network that integrates spatial, visual, and categorical features to generate task-aware proposals, and (ii) an object reasoning network that assigns probabilities across proposals, followed by a soft selection mechanism for final referring object localization. Our model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods while retaining classical OD capabilities. The code will be available in our repository: https://github.com/rd20karim/MB-ORES.', 'score': 1, 'issue_id': 3027, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'c59d3238abf4164f', 'authors': ['Karim Radouane', 'Hanane Azzag', 'Mustapha lebbah'], 'affiliations': ['University Paris-Saclay - DAVID Lab, UVSQ Versailles, France', 'University Sorbonne Paris Nord - LIPN, Villetaneuse, France'], 'pdf_title_img': 'assets/pdf/title_img/2503.24219.jpg', 'data': {'categories': ['#open_source', '#optimization', '#cv', '#architecture', '#graphs', '#dataset'], 'emoji': '🛰️', 'ru': {'title': 'Унифицированный подход к обнаружению и привязке объектов на спутниковых снимках', 'desc': 'Авторы предлагают унифицированный подход, объединяющий обнаружение объектов и визуальную привязку для дистанционного зондирования. Они дообучают детектор объектов с открытым набором классов, используя данные с языковыми описаниями. Модель строит графовое представление изображения и обрабатывает его с помощью специализированной архитектуры. Эксперименты показывают превосходство предложенного метода над современными аналогами на наборах данных OPT-RSVG и DIOR-RSVG.'}, 'en': {'title': 'Integrating Object Detection and Visual Grounding for Enhanced Remote Sensing Analysis', 'desc': 'This paper presents a new framework that combines object detection (OD) and visual grounding (VG) specifically for remote sensing imagery. The authors enhance a traditional open-set object detector by fine-tuning it with referring expression data, treating VG as a partially supervised OD task. They create a graph representation of images that includes object queries and class embeddings, which is then processed by a multi-branch network to generate proposals. The model outperforms existing methods on benchmark datasets while maintaining the capabilities of classical object detection.'}, 'zh': {'title': '统一框架：目标检测与视觉定位的结合', 'desc': '我们提出了一个统一框架，将目标检测（OD）和视觉定位（VG）集成到遥感图像中。为了支持传统的目标检测并为视觉定位任务建立直观的先验，我们使用参考表达数据微调了一个开放集目标检测器，将其框定为部分监督的目标检测任务。在第一阶段，我们构建了每个图像的图形表示，包括目标查询、类别嵌入和提议位置。然后，我们的任务感知架构处理这个图形以执行视觉定位任务，最终在多个数据集上表现出优越的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.23461', 'title': 'TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes', 'url': 'https://huggingface.co/papers/2503.23461', 'abstract': 'This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.', 'score': 59, 'issue_id': 2995, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '00cccb2000a01b76', 'authors': ['Nikai Du', 'Zhennan Chen', 'Zhizhou Chen', 'Shan Gao', 'Xi Chen', 'Zhengkai Jiang', 'Jian Yang', 'Ying Tai'], 'affiliations': ['China Mobile', 'Nanjing University', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.23461.jpg', 'data': {'categories': ['#cv', '#dataset', '#benchmark'], 'emoji': '📝', 'ru': {'title': 'TextCrafter: Прорыв в генерации сложного визуального текста', 'desc': 'Статья представляет новый метод TextCrafter для генерации сложного визуального текста в изображениях. Этот метод использует прогрессивную стратегию для декомпозиции сложного визуального текста на отдельные компоненты и обеспечивает надежное выравнивание между текстовым содержанием и его визуальным носителем. TextCrafter также включает механизм усиления фокуса токенов для повышения заметности визуального текста в процессе генерации. Авторы также представляют новый набор данных CVTG-2K для оценки производительности генеративных моделей в задачах CVTG.'}, 'en': {'title': 'TextCrafter: Mastering Complex Visual Text Generation', 'desc': 'This paper introduces Complex Visual Text Generation (CVTG), which involves creating detailed text that appears in various parts of images. The authors identify common issues in existing models, such as producing unclear or incomplete visual text. To improve this, they present TextCrafter, a new method that breaks down complex text into manageable parts and ensures that the text aligns well with the images. Additionally, TextCrafter enhances the visibility of the text during generation and is evaluated using a new dataset called CVTG-2K, showing superior performance compared to current methods.'}, 'zh': {'title': 'TextCrafter：提升复杂视觉文本生成的利器', 'desc': '本文探讨了复杂视觉文本生成（CVTG）任务，主要关注在视觉图像中生成分布在不同区域的复杂文本内容。在CVTG中，图像生成模型常常会渲染出扭曲、模糊的视觉文本或遗漏某些视觉文本。为了解决这些问题，我们提出了TextCrafter，这是一种新颖的多视觉文本渲染方法。TextCrafter通过逐步策略将复杂视觉文本分解为不同组件，同时确保文本内容与其视觉载体之间的强对齐。'}}}, {'id': 'https://huggingface.co/papers/2503.23307', 'title': 'MoCha: Towards Movie-Grade Talking Character Synthesis', 'url': 'https://huggingface.co/papers/2503.23307', 'abstract': 'Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.', 'score': 44, 'issue_id': 2994, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '6ce9b3642bf3ace3', 'authors': ['Cong Wei', 'Bo Sun', 'Haoyu Ma', 'Ji Hou', 'Felix Juefei-Xu', 'Zecheng He', 'Xiaoliang Dai', 'Luxin Zhang', 'Kunpeng Li', 'Tingbo Hou', 'Animesh Sinha', 'Peter Vajda', 'Wenhu Chen'], 'affiliations': ['GenAI, Meta', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23307.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#video', '#benchmark', '#story_generation'], 'emoji': '🎭', 'ru': {'title': 'MoCha: новый уровень ИИ-генерации кинематографических историй', 'desc': 'Представлена система MoCha для генерации анимированных разговаривающих персонажей на основе речи и текста. Использован механизм внимания для синхронизации речи и видео, а также совместное обучение на данных с речевой и текстовой разметкой. Система позволяет генерировать диалоги нескольких персонажей с учетом контекста. Результаты превосходят существующие подходы по реалистичности и выразительности генерируемых анимаций.'}, 'en': {'title': 'Revolutionizing Character Animation with MoCha', 'desc': "This paper presents MoCha, a novel approach for generating talking character animations from speech and text, focusing on full character portrayal rather than just facial movements. It introduces a speech-video window attention mechanism to ensure that the generated video aligns accurately with the spoken words. To tackle the challenge of limited speech-labeled video datasets, the authors propose a joint training strategy that utilizes both speech and text-labeled data, enhancing the model's ability to generalize across various character actions. Additionally, structured prompt templates with character tags allow for multi-character dialogues, enabling AI-generated characters to engage in coherent conversations, thus advancing the field of automated cinematic storytelling."}, 'zh': {'title': '会说话的角色：AI生成电影叙事的新标准', 'desc': '本论文介绍了一种新的视频生成任务，称为“会说话的角色”，旨在从语音和文本直接生成角色动画。与传统的“说话头”不同，这种方法生成的不仅仅是面部表情，而是完整的角色形象。我们提出了MoCha，这是首个能够生成会说话角色的模型，并引入了一种语音-视频窗口注意机制，以确保视频与语音的精确同步。此外，我们还设计了结构化的提示模板，使得多个角色能够进行基于回合的对话，从而实现更具电影感的情境对话。'}}}, {'id': 'https://huggingface.co/papers/2503.24290', 'title': 'Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model', 'url': 'https://huggingface.co/papers/2503.24290', 'abstract': 'We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.', 'score': 35, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'c3cd649c5eb9d423', 'authors': ['Jingcheng Hu', 'Yinmin Zhang', 'Qi Han', 'Daxin Jiang', 'Xiangyu Zhang', 'Heung-Yeung Shum'], 'affiliations': ['StepFun', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24290.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#training', '#rl', '#open_source', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Простота и эффективность в обучении моделей рассуждения', 'desc': 'Open-Reasoner-Zero - это первая открытая реализация обучения с подкреплением для масштабного рассуждения, фокусирующаяся на масштабируемости, простоте и доступности. Используя минималистичный подход с vanilla PPO и GAE, без KL-регуляризации, удалось улучшить длину ответов и производительность на бенчмарках. Модель превзошла DeepSeek-R1-Zero на тестах AIME2024, MATH500 и GPQA Diamond, требуя при этом в 10 раз меньше шагов обучения. Авторы открыто публикуют код, параметры, данные для обучения и веса моделей разных размеров.'}, 'en': {'title': 'Simplifying Large-Scale Reasoning in RL with Open-Reasoner-Zero', 'desc': 'Open-Reasoner-Zero is an innovative open-source framework designed for large-scale reasoning-oriented reinforcement learning (RL) training. It utilizes a simple approach with vanilla Proximal Policy Optimization (PPO) and Generalized Advantage Estimation (GAE), achieving impressive results without complex regularization techniques. The framework demonstrates that minimalistic strategies can effectively enhance response length and benchmark performance, outperforming previous models like DeepSeek-R1-Zero. By releasing all components of the project, including source code and model weights, Open-Reasoner-Zero promotes accessibility and collaboration in the machine learning community.'}, 'zh': {'title': '开源推理强化学习的高效实现', 'desc': '我们介绍了Open-Reasoner-Zero，这是首个开源的大规模推理导向强化学习训练实现，重点关注可扩展性、简洁性和可访问性。通过大量实验，我们证明了使用简单的PPO算法和基于规则的奖励机制，能够有效提升响应长度和基准性能。我们的实现与DeepSeek-R1-Zero使用相同的基础模型，在AIME2024、MATH500和GPQA Diamond基准上表现优异，同时训练效率显著提高，仅需DeepSeek-R1-Zero管道的十分之一训练步骤。为了支持开源精神，我们发布了源代码、参数设置、训练数据和不同规模的模型权重。'}}}, {'id': 'https://huggingface.co/papers/2503.24235', 'title': 'What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.24235', 'abstract': "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.", 'score': 34, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '21674468fcc8c7d5', 'authors': ['Qiyuan Zhang', 'Fuyuan Lyu', 'Zexu Sun', 'Lei Wang', 'Weixu Zhang', 'Zhihan Guo', 'Yufei Wang', 'Irwin King', 'Xue Liu', 'Chen Ma'], 'affiliations': ['Chinese University of Hong Kong', 'City University of Hong Kong', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Macquarie University', 'McGill University & MILA', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24235.jpg', 'data': {'categories': ['#survey', '#math', '#reasoning', '#training'], 'emoji': '🔍', 'ru': {'title': 'Систематизация методов масштабирования языковых моделей во время тестирования', 'desc': 'Эта статья представляет собой обзор методов масштабирования во время тестирования (TTS) для больших языковых моделей (LLM). Авторы предлагают унифицированную структуру для классификации TTS-подходов по четырем измерениям: что масштабировать, как масштабировать, где масштабировать и насколько хорошо масштабировать. В работе анализируются различные методы, сценарии применения и аспекты оценки TTS. Статья также выделяет основные тенденции развития TTS и предлагает практические рекомендации по его применению.'}, 'en': {'title': 'Unlocking Potential: The Power of Test-Time Scaling in LLMs', 'desc': 'This paper discusses the emerging focus on test-time scaling (TTS) in machine learning, particularly for large language models (LLMs). TTS enhances the problem-solving abilities of LLMs in various tasks, including specialized reasoning and open-ended questions. The authors propose a comprehensive framework that categorizes TTS research into four dimensions: what, how, where, and how well to scale. They also review existing methods and applications, highlight challenges, and suggest future research directions to improve TTS effectiveness.'}, 'zh': {'title': '测试时扩展：激发大型语言模型的潜力', 'desc': '随着对预训练时代计算规模（数据和参数）的热情逐渐减退，测试时扩展（TTS）成为一个重要的研究焦点。最近的研究表明，TTS可以进一步激发大型语言模型（LLMs）的问题解决能力，在数学、编程等专业推理任务以及开放式问答等一般任务中取得显著突破。尽管这一领域的研究迅速增加，但仍迫切需要一项全面的调查，以提供系统的理解。为此，我们提出了一个统一的多维框架，涵盖TTS研究的四个核心维度，并对方法、应用场景和评估方面进行了广泛的回顾。'}}}, {'id': 'https://huggingface.co/papers/2503.24388', 'title': 'RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy', 'url': 'https://huggingface.co/papers/2503.24388', 'abstract': 'Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.', 'score': 22, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '98b80967d4757be2', 'authors': ['Zhonghan Zhao', 'Wenwei Zhang', 'Haian Huang', 'Kuikun Liu', 'Jianfei Gao', 'Gaoang Wang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24388.jpg', 'data': {'categories': ['#training', '#reasoning', '#agents', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Синергия рассуждения и воображения для создания универсальных ИИ-агентов', 'desc': 'Данная статья представляет новый подход к обучению агентов искусственного интеллекта, названный RIG (Reasoning and Imagination in Generalist policy). RIG объединяет способности рассуждения и воображения в единой комплексной модели, что позволяет значительно повысить эффективность обучения и способность к обобщению. В процессе вывода RIG сначала рассуждает о следующем действии, затем генерирует потенциальное действие и предсказывает его результаты, что дает агенту возможность пересмотреть и скорректировать свои действия перед их реальным выполнением. Экспериментальные результаты показывают, что синергия рассуждения и воображения улучшает устойчивость, обобщение и интероперабельность политики генералиста.'}, 'en': {'title': 'Synergizing Reasoning and Imagination for Enhanced Agent Performance', 'desc': 'This paper introduces RIG, a novel end-to-end Generalist policy that combines reasoning and imagination for embodied agents in complex environments. By integrating these two capabilities, RIG enhances learning efficiency and generalization, overcoming limitations of previous models that focused on only one aspect. The authors developed a data pipeline that enriches trajectories from existing agents, allowing for joint learning of reasoning and next image generation. Experimental results demonstrate that RIG significantly improves sample efficiency and robustness, enabling agents to self-correct actions based on imagined outcomes before execution.'}, 'zh': {'title': '推理与想象的协同提升智能体能力', 'desc': '本文提出了一种名为RIG的通用策略，首次将推理和想象能力结合在一个端到端的智能体中。通过构建数据管道，逐步整合和丰富从现有智能体收集的轨迹中的推理和想象内容，RIG实现了更高的学习效率和泛化能力。联合学习推理和下一图像生成，明确建模了推理、行动和环境动态之间的内在关联，使得样本效率提高了17倍以上。实验结果表明，推理与想象的协同作用不仅增强了通用策略的鲁棒性和互操作性，还提升了整体性能。'}}}, {'id': 'https://huggingface.co/papers/2503.24370', 'title': 'Effectively Controlling Reasoning Models through Thinking Intervention', 'url': 'https://huggingface.co/papers/2503.24370', 'abstract': 'Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.', 'score': 13, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '5f218f08538c601f', 'authors': ['Tong Wu', 'Chong Xiang', 'Jiachen T. Wang', 'Prateek Mittal'], 'affiliations': ['NVIDIA', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24370.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#architecture', '#open_source', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Управление мышлением ИИ: новый путь к улучшению языковых моделей', 'desc': "Статья представляет новый подход к управлению языковыми моделями (LLM) под названием 'Thinking Intervention'. Этот метод позволяет вмешиваться в процесс рассуждений модели, стратегически вставляя или изменяя определенные токены мышления. Авторы провели обширные эксперименты на различных задачах, включая следование инструкциям и безопасность. Результаты показывают значительное улучшение производительности по сравнению с базовыми методами промптинга, открывая новые возможности для контроля над рассуждающими языковыми моделями."}, 'en': {'title': 'Enhancing LLM Reasoning with Thinking Intervention', 'desc': 'This paper introduces a new method called Thinking Intervention, which enhances the reasoning capabilities of large language models (LLMs) by allowing explicit control over their internal thought processes. By inserting or modifying specific reasoning tokens, the model can generate more accurate and contextually relevant answers. The authors conducted extensive tests on various tasks, showing that this approach leads to significant improvements in performance, such as higher accuracy in following instructions and better handling of unsafe prompts. Overall, the findings suggest that Thinking Intervention provides a valuable framework for refining how LLMs reason and respond to complex queries.'}, 'zh': {'title': '思维干预：提升大型语言模型推理能力的新方法', 'desc': '本文提出了一种新的思维干预（Thinking Intervention）方法，旨在通过插入或修改特定的思维标记来引导大型语言模型（LLMs）的内部推理过程。这种方法使得模型在复杂问题解决中能够更好地生成中间推理步骤，从而提高最终答案的准确性。我们在多个任务上进行了全面评估，结果显示思维干预显著优于传统的提示方法，尤其在指令遵循和推理层次方面取得了显著的准确率提升。我们的研究为控制推理过程中的大型语言模型开辟了新的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2503.23284', 'title': 'SketchVideo: Sketch-based Video Generation and Editing', 'url': 'https://huggingface.co/papers/2503.23284', 'abstract': "Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.", 'score': 13, 'issue_id': 2996, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'd968c1da27effa84', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#multimodal', '#optimization', '#games', '#video'], 'emoji': '✏️', 'ru': {'title': 'Точное управление видео через скетчи', 'desc': 'Эта статья представляет новый подход к генерации и редактированию видео на основе скетчей. Авторы предлагают эффективную структуру управления с блоками контроля скетчей, которая работает с моделью генерации видео DiT. Для распространения условий скетча на все кадры используется механизм межкадрового внимания. Также разработан модуль вставки видео для согласованного редактирования, а при инференсе применяется латентное слияние для сохранения неотредактированных областей.'}, 'en': {'title': 'Sketch Your Way to Better Video Control!', 'desc': 'This paper presents a novel approach to video generation and editing using sketch-based controls. It addresses the challenges of accurately managing global layout and motion details by introducing a memory-efficient control structure that utilizes sketch control blocks. The method allows users to draw sketches on keyframes, which are then analyzed through an inter-frame attention mechanism to ensure consistency across all video frames. The proposed system, SketchVideo, enhances the ability to generate and edit videos while preserving the integrity of unedited regions, demonstrating superior performance in controllable video tasks.'}, 'zh': {'title': '草图驱动的视频生成与编辑新方法', 'desc': '本论文探讨了基于草图的空间和运动控制在视频生成中的应用，旨在解决文本和图像条件下的布局和几何细节控制问题。我们提出了一种高效的控制结构，利用草图控制块预测跳过的DiT块的残差特征。通过在关键帧上绘制草图，并使用跨帧注意机制分析关键帧与每个视频帧之间的关系，实现了时间上稀疏的草图条件传播。实验结果表明，我们的SketchVideo在可控视频生成和编辑方面表现优越。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.23077', 'title': 'Efficient Inference for Large Reasoning Models: A Survey', 'url': 'https://huggingface.co/papers/2503.23077', 'abstract': "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant fieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.", 'score': 13, 'issue_id': 2995, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': 'f76d7ead3d85b37e', 'authors': ['Yue Liu', 'Jiaying Wu', 'Yufei He', 'Hongcheng Gao', 'Hongyu Chen', 'Baolong Bi', 'Jiaheng Zhang', 'Zhiqi Huang', 'Bryan Hooi'], 'affiliations': ['Beijing Jiaotong University', 'Moonshot', 'National University of Singapore', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.23077.jpg', 'data': {'categories': ['#reasoning', '#survey', '#interpretability', '#optimization', '#inference', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности вывода в моделях крупномасштабного рассуждения', 'desc': 'Данная статья представляет обзор методов эффективного вывода для моделей крупномасштабного рассуждения (LRM). Авторы классифицируют существующие подходы на две категории: явные компактные цепочки рассуждений и неявные латентные цепочки рассуждений. В работе проводится эмпирический анализ эффективности и производительности этих методов. Также обсуждаются открытые проблемы в этой области, включая контролируемое рассуждение, ориентированное на человека, баланс между интерпретируемостью и эффективностью, безопасность и более широкое применение эффективного рассуждения.'}, 'en': {'title': 'Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses Large Reasoning Models (LRMs) that enhance the reasoning capabilities of Large Language Models (LLMs) but face challenges like inefficient token usage and high memory consumption. It reviews various efficient inference methods aimed at reducing these inefficiencies while maintaining reasoning quality. The authors categorize these methods into explicit and implicit Chain-of-Thought approaches, analyzing their strengths and weaknesses. Additionally, the paper identifies ongoing challenges in the field and suggests strategies for improving inference efficiency in LRMs.'}, 'zh': {'title': '提升推理效率，优化大型推理模型', 'desc': '大型推理模型（LRMs）通过学习推理显著提高了大型语言模型（LLMs）的推理能力，能够在复杂任务中表现出色。然而，它们的深思熟虑的推理过程导致了令牌使用、内存消耗和推理时间的低效。因此，本文综述了专门为LRMs设计的高效推理方法，重点在于减轻令牌低效，同时保持推理质量。我们介绍了一种分类法，将最近的方法分为两大类：显式紧凑的思维链（CoT）和隐式潜在的思维链，讨论了它们的优缺点，并分析了现有方法的性能和效率。'}}}, {'id': 'https://huggingface.co/papers/2503.24364', 'title': 'Query and Conquer: Execution-Guided SQL Generation', 'url': 'https://huggingface.co/papers/2503.24364', 'abstract': 'We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.', 'score': 12, 'issue_id': 2997, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '2af26722887e77ac', 'authors': ['Łukasz Borchmann', 'Marek Wydmuch'], 'affiliations': ['Snowflake AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24364.jpg', 'data': {'categories': ['#optimization', '#small_models', '#dataset', '#inference', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Революция в генерации SQL: эффективность через семантическую согласованность', 'desc': 'Предложен новый подход к генерации сложных выходных данных, значительно повышающий точность в задачах преобразования текста в SQL. Метод использует результаты выполнения для выбора наиболее семантически согласованного запроса из нескольких кандидатов. Это позволяет меньшим, экономически эффективным моделям превзойти вычислительно интенсивные методы рассуждений, такие как o1, o3-mini и DeepSeek R1, при этом снижая стоимость вывода до 30 раз. Подход легко интегрируется с существующими моделями, предлагая практичный и масштабируемый путь к современной генерации SQL.'}, 'en': {'title': 'Efficient SQL Generation: Small Models, Big Results!', 'desc': 'This paper introduces a new method for generating SQL queries from natural language, enhancing accuracy in text-to-SQL tasks. The approach utilizes execution results to choose the best query from several options based on semantic consistency. It allows smaller models to outperform larger, more complex models while also cutting down inference costs significantly. This method is designed to work well with current models, making it a practical solution for improving SQL generation.'}, 'zh': {'title': '高效生成SQL，提升文本到SQL的准确性', 'desc': '我们提出了一种新颖的方法，用于生成复杂输出，显著提高文本到SQL任务的准确性。该方法利用执行结果，从多个候选查询中选择最符合语义的一项，使得较小、成本效益高的模型能够超越计算密集型的推理方法，如o1、o3-mini和DeepSeek R1，同时将推理成本降低多达30倍。它与现有模型无缝集成，提供了一条实用且可扩展的通往最先进SQL生成的路径。'}}}, {'id': 'https://huggingface.co/papers/2503.23829', 'title': 'Expanding RL with Verifiable Rewards Across Diverse Domains', 'url': 'https://huggingface.co/papers/2503.23829', 'abstract': "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.", 'score': 12, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '2c875f8335892dfd', 'authors': ['Yi Su', 'Dian Yu', 'Linfeng Song', 'Juntao Li', 'Haitao Mi', 'Zhaopeng Tu', 'Min Zhang', 'Dong Yu'], 'affiliations': ['Soochow University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.23829.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rlhf', '#training', '#rl', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'RLVR: Универсальное обучение с подкреплением для различных областей знаний', 'desc': 'Исследование посвящено расширению применения обучения с подкреплением с проверяемыми вознаграждениями (RLVR) на различные области, такие как медицина, химия, психология и экономика. Авторы обнаружили высокое согласие в бинарных оценках между различными большими языковыми моделями при наличии объективных эталонных ответов. Для улучшения гибкости метода при работе с неструктурированными эталонными ответами было предложено использование мягкого оценивания на основе моделей. Эксперименты показали, что дистиллированная генеративная модель вознаграждения может служить эффективным междоменным верификатором, обеспечивая надежные сигналы вознаграждения для обучения с подкреплением без необходимости в аннотациях для конкретных доменов.'}, 'en': {'title': 'Expanding RLVR: From Coding to Real-World Applications', 'desc': "This paper explores the use of Reinforcement Learning with Verifiable Rewards (RLVR) in diverse fields like medicine and economics, where traditional structured answers may not be available. The authors find that different large language models (LLMs) often agree on binary judgments when objective reference answers are present, suggesting that extensive annotations may not be necessary for training reward models. To enhance RLVR's effectiveness, they introduce model-based soft scoring to better handle unstructured reference answers. Their experiments demonstrate that a distilled generative reward model can effectively verify rewards across domains, leading to improved performance of RL policies compared to existing state-of-the-art models."}, 'zh': {'title': '强化学习的可验证奖励：跨领域应用的新可能', 'desc': '强化学习与可验证奖励（RLVR）在数学推理和编码任务中表现出色，但其在更广泛领域的应用尚未深入探索。我们研究了RLVR在医学、化学、心理学和经济学等多样化领域的扩展。研究发现，当存在客观参考答案时，不同的大型语言模型（LLMs）在二元判断上高度一致，这表明训练特定领域奖励模型不一定需要大规模标注。通过将基于模型的软评分纳入RLVR，我们提高了其灵活性，并通过实验验证了蒸馏生成奖励模型在跨领域验证中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.19901', 'title': 'TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization', 'url': 'https://huggingface.co/papers/2503.19901', 'abstract': 'Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/', 'score': 12, 'issue_id': 3000, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'c5dd40417e2f952a', 'authors': ['Liang Pan', 'Zeshi Yang', 'Zhiyang Dou', 'Wenjia Wang', 'Buzhen Huang', 'Bo Dai', 'Taku Komura', 'Jingbo Wang'], 'affiliations': ['Feeling AI', 'Independent Researcher', 'Shanghai AI Laboratory', 'Southeast University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19901.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#transfer_learning', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Единая трансформерная политика для гибкого синтеза взаимодействий человека с окружением', 'desc': 'TokenHSI - это новый подход к синтезу разнообразных и физически правдоподобных взаимодействий человека с окружающей средой (HSI). Метод использует единую трансформерную политику, способную объединять несколько навыков и гибко адаптироваться к новым сценариям. Ключевая идея заключается в моделировании проприоцепции гуманоида как отдельного общего токена и комбинировании его с различными токенами задач через механизм маскирования. Эксперименты показывают, что этот подход значительно улучшает универсальность, адаптивность и расширяемость в различных задачах HSI.'}, 'en': {'title': 'Unifying Skills for Enhanced Human-Scene Interactions', 'desc': 'This paper introduces TokenHSI, a novel transformer-based policy designed to enhance Human-Scene Interactions (HSI) by integrating multiple skills into a single framework. Unlike traditional methods that rely on separate controllers for each task, TokenHSI utilizes a shared token for humanoid proprioception, allowing for effective knowledge sharing across different interaction tasks. The architecture supports variable length inputs, making it adaptable to new scenarios and capable of coordinating complex actions, such as sitting while carrying an object. Experimental results show that TokenHSI significantly improves the versatility and adaptability of HSI tasks, paving the way for more sophisticated applications in computer animation and embodied AI.'}, 'zh': {'title': '统一多技能的人类场景交互策略', 'desc': '本论文提出了一种名为TokenHSI的统一变换器基础策略，用于合成多样且物理上合理的人类场景交互（HSI）。当前的方法主要集中在为特定交互任务开发独立控制器，这限制了处理复杂任务的能力。TokenHSI通过将人体本体感知建模为共享的独立标记，并结合不同的任务标记，促进了多技能的统一和灵活适应。实验结果表明，该方法在多种HSI任务中显著提高了多样性、适应性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2503.24115', 'title': 'TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection', 'url': 'https://huggingface.co/papers/2503.24115', 'abstract': 'The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.', 'score': 9, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '61845428f5c3d9df', 'authors': ['Zhiming Ma', 'Peidong Wang', 'Minhua Huang', 'Jingpeng Wang', 'Kai Wu', 'Xiangzhao Lv', 'Yachun Pang', 'Yin Yang', 'Wenjie Tang', 'Yuchen Kang'], 'affiliations': ['China Mobile Internet Company Ltd. Guangzhou, Guangdong, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.24115.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#synthetic', '#open_source', '#benchmark', '#data'], 'emoji': '🎭', 'ru': {'title': 'Мультимодальный датасет для борьбы с телефонным мошенничеством', 'desc': 'Статья представляет TeleAntiFraud-28k - первый открытый аудио-текстовый датасет для анализа телекоммуникационного мошенничества. Датасет создан с использованием генерации образцов на основе ASR и TTS, семантического расширения с помощью LLM и многоагентного состязательного синтеза. Он содержит 28,511 пар речь-текст с аннотациями для рассуждений о мошенничестве и разделен на три задачи. Авторы также представляют TeleAntiFraud-Bench для оценки производительности моделей и открытую модель SFT.'}, 'en': {'title': 'Revolutionizing Telecom Fraud Detection with TeleAntiFraud-28k', 'desc': 'This paper introduces TeleAntiFraud-28k, a novel dataset designed to enhance telecom fraud detection by combining audio signals with textual analysis. The dataset is created using three innovative strategies, including privacy-preserved text generation from speech recordings and semantic enhancement through large language models. It consists of over 28,000 annotated speech-text pairs, enabling tasks like scenario classification and fraud detection. Additionally, the authors provide a benchmark for evaluating model performance and a fine-tuned model for practical applications, promoting further research in multimodal anti-fraud techniques.'}, 'zh': {'title': '构建电信欺诈检测的新基石', 'desc': '本论文提出了TeleAntiFraud-28k数据集，这是第一个专为电信欺诈分析设计的开源音频-文本慢思考数据集。该数据集通过三种策略构建，确保了数据的隐私保护和真实场景的一致性。我们还建立了TeleAntiFraud-Bench评估基准，以便系统地测试模型在电信欺诈检测任务上的表现。此项工作为多模态反欺诈研究奠定了基础，同时解决了数据隐私和场景多样性等关键挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.18809', 'title': 'Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code', 'url': 'https://huggingface.co/papers/2503.18809', 'abstract': 'In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit "reasoning" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs.', 'score': 9, 'issue_id': 2994, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '28288adc69a019ac', 'authors': ['Augusto B. Corrêa', 'André G. Pereira', 'Jendrik Seipp'], 'affiliations': ['Federal University of Rio Grande do Sul', 'Linköping University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.18809.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'LLM как генераторы эффективных эвристик для задач планирования', 'desc': 'Исследователи разработали метод использования больших языковых моделей (LLM) для генерации эвристических функций в задачах планирования. LLM создают несколько эвристик в виде Python-кода, которые затем оцениваются на тренировочных задачах. Выбранные эвристики показывают высокую эффективность на новых задачах, превосходя современные методы доменно-независимого планирования. Этот подход позволяет значительно улучшить способности LLM к планированию, даже для задач возрастающей сложности.'}, 'en': {'title': 'Empowering LLMs with Domain-Specific Heuristics for Better Planning', 'desc': 'This paper addresses the limitations of large language models (LLMs) in planning tasks, which often lead to incorrect and non-generalizable plans. The authors propose a novel approach where LLMs generate domain-specific heuristic functions in Python code, which are then evaluated using a greedy best-first search algorithm. Their method demonstrates that LLM-generated heuristics can outperform traditional domain-independent heuristics and compete with advanced learning algorithms in planning. The results indicate that these heuristics are not only efficient but also provide more informative guidance in certain planning domains.'}, 'zh': {'title': '提升大型语言模型的规划能力', 'desc': '近年来，大型语言模型（LLMs）在各种人工智能问题上展现了卓越的能力。然而，即使在详细定义规划任务的情况下，它们在规划方面仍然不可靠。本文展示了如何利用LLMs生成正确的规划，即使对于越来越大的分布外任务。通过生成领域相关的启发式函数并在贪婪优先搜索中评估，LLM生成的启发式函数在解决未见测试任务方面表现优于传统的领域无关启发式方法。'}}}, {'id': 'https://huggingface.co/papers/2503.22673', 'title': 'ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action Models', 'url': 'https://huggingface.co/papers/2503.22673', 'abstract': 'Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to facilitate research in the community.', 'score': 8, 'issue_id': 3011, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '32c8476678c711ba', 'authors': ['Jianguo Zhang', 'Thai Hoang', 'Ming Zhu', 'Zuxin Liu', 'Shiyu Wang', 'Tulika Awalgaonkar', 'Akshara Prabhakar', 'Haolin Chen', 'Weiran Yao', 'Zhiwei Liu', 'Juntao Tan', 'Juan Carlos Niebles', 'Shelby Heinecke', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.22673.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'ActionStudio: Универсальный инструмент для обучения моделей действий ИИ-агентов', 'desc': 'ActionStudio - это легковесный и расширяемый фреймворк для обучения крупных моделей действий. Он унифицирует разнородные траектории агентов через стандартизированный формат и поддерживает различные парадигмы обучения, включая LoRA и полную тонкую настройку. ActionStudio интегрирует инструменты предобработки и верификации, а также демонстрирует высокую производительность на публичных и промышленных бенчмарках. Фреймворк предназначен для упрощения обучения моделей действий для автономных агентов.'}, 'en': {'title': 'Empowering Autonomous Agents with ActionStudio', 'desc': 'This paper introduces ActionStudio, a new framework designed to improve the training of large action models for autonomous agents. It addresses the challenges of diverse environments and complex data by providing a standardized format for agent trajectories. ActionStudio supports various training methods, including LoRA and full fine-tuning, and offers tools for data preprocessing and verification. The framework has been validated on both public and industry benchmarks, showing strong performance and scalability, and is open-sourced for community use.'}, 'zh': {'title': 'ActionStudio：提升大型动作模型训练的利器', 'desc': '本文介绍了ActionStudio，这是一个轻量级且可扩展的数据和训练框架，旨在支持大型动作模型的训练。ActionStudio通过标准化格式统一了不同的代理轨迹，支持多种训练范式，包括LoRA、完全微调和分布式设置。该框架还集成了强大的预处理和验证工具，以提高训练的效率和可靠性。我们在公共和实际行业基准上验证了其有效性，展示了强大的性能和实用的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2503.21694', 'title': 'Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data', 'url': 'https://huggingface.co/papers/2503.21694', 'abstract': 'It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only 2.5% trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at https://github.com/theEricMa/TriplaneTurbo.', 'score': 8, 'issue_id': 3000, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '703f61255714367b', 'authors': ['Zhiyuan Ma', 'Xinyue Liang', 'Rongyuan Wu', 'Xiangyu Zhu', 'Zhen Lei', 'Lei Zhang'], 'affiliations': ['Center for Artificial Intelligence and Robotics, HKISI CAS', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, UCAS', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21694.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#3d', '#diffusion', '#open_source'], 'emoji': '🧊', 'ru': {'title': 'Быстрая генерация 3D-объектов из текста без 3D-разметки', 'desc': 'Статья представляет новый метод обучения генеративных моделей для создания 3D-объектов из текстовых описаний, называемый Progressive Rendering Distillation (PRD). PRD использует предобученные модели диффузии для 2D-изображений и многоракурсные модели, чтобы обойти проблему нехватки качественных 3D-данных для обучения. Метод позволяет адаптировать модель Stable Diffusion для генерации 3D-представлений без необходимости в 3D-разметке. В результате авторы создали модель TriplaneTurbo, способную генерировать качественные 3D-модели за 1.2 секунды.'}, 'en': {'title': 'Fast and High-Quality 3D Mesh Generation from Text Prompts', 'desc': 'This paper introduces a new method called Progressive Rendering Distillation (PRD) to create high-quality 3D meshes from text prompts quickly. PRD allows training without needing 3D ground-truth data by using multi-view diffusion models to distill textures and geometries into 3D outputs. The method enhances the efficiency of the generation process, enabling the model to produce 3D meshes in just 1.2 seconds while maintaining high quality. The resulting model, TriplaneTurbo, shows significant improvements over previous text-to-3D generators in both speed and output quality.'}, 'zh': {'title': '快速生成高质量3D网格的创新方案', 'desc': '本论文提出了一种新的训练方案，称为渐进渲染蒸馏（PRD），旨在从文本提示中快速生成高质量的3D网格。通过蒸馏多视图扩散模型，PRD消除了对3D真实数据的需求，使得训练数据的扩展变得更加容易。该方法利用U-Net逐步去噪，并将去噪后的潜在表示解码为3D输出，从而提高生成质量。最终，PRD训练的TriplaneTurbo生成器在效率和质量上均优于之前的文本到3D生成器，能够在1.2秒内生成高质量的3D网格。'}}}, {'id': 'https://huggingface.co/papers/2503.24391', 'title': 'Easi3R: Estimating Disentangled Motion from DUSt3R Without Training', 'url': 'https://huggingface.co/papers/2503.24391', 'abstract': 'Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major bottleneck for training a highly generalizable 4D model. This constraint has driven conventional 4D methods to fine-tune 3D models on scalable dynamic video data with additional geometric priors such as optical flow and depths. In this work, we take an opposite path and introduce Easi3R, a simple yet efficient training-free method for 4D reconstruction. Our approach applies attention adaptation during inference, eliminating the need for from-scratch pre-training or network fine-tuning. We find that the attention layers in DUSt3R inherently encode rich information about camera and object motion. By carefully disentangling these attention maps, we achieve accurate dynamic region segmentation, camera pose estimation, and 4D dense point map reconstruction. Extensive experiments on real-world dynamic videos demonstrate that our lightweight attention adaptation significantly outperforms previous state-of-the-art methods that are trained or finetuned on extensive dynamic datasets. Our code is publicly available for research purpose at https://easi3r.github.io/', 'score': 3, 'issue_id': 3001, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '48e601b0440aa5d3', 'authors': ['Xingyu Chen', 'Yue Chen', 'Yuliang Xiu', 'Andreas Geiger', 'Anpei Chen'], 'affiliations': ['Max Planck Institute for Intelligent Systems', 'University of Tubingen, Tubingen AI Center', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24391.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#open_source', '#3d', '#video'], 'emoji': '🌟', 'ru': {'title': 'Easi3R: Эффективная 4D-реконструкция без обучения', 'desc': 'Статья представляет метод Easi3R для 4D-реконструкции, который не требует предварительного обучения или дообучения нейронной сети. Используя адаптацию внимания во время вывода, метод применяет предобученную 3D-модель DUSt3R для динамических сцен. Easi3R разделяет карты внимания для точной сегментации динамических областей, оценки положения камеры и реконструкции плотной 4D карты точек. Эксперименты показывают, что Easi3R превосходит современные методы, обученные на больших динамических датасетах.'}, 'en': {'title': 'Easi3R: Efficient 4D Reconstruction Without Pre-training', 'desc': 'This paper presents Easi3R, a novel method for 4D reconstruction that does not require extensive pre-training or fine-tuning of models. Instead, it utilizes attention adaptation during inference to effectively segment dynamic regions and estimate camera poses. The authors leverage the inherent information encoded in the attention layers of the DUSt3R model, allowing for accurate reconstruction of dense point clouds in dynamic scenes. Experimental results show that Easi3R outperforms existing methods that rely on large dynamic datasets, highlighting its efficiency and effectiveness.'}, 'zh': {'title': 'Easi3R：无需训练的高效4D重建方法', 'desc': '本论文介绍了一种名为Easi3R的4D重建方法，该方法无需预训练或微调网络，利用注意力适应技术进行推理。与传统方法依赖于大规模动态视频数据不同，Easi3R通过解耦注意力图，准确实现动态区域分割、相机姿态估计和4D稠密点云重建。实验结果表明，Easi3R在真实动态视频上的表现显著优于以往的最先进方法。该方法的轻量级设计使其在处理动态场景时更加高效。'}}}, {'id': 'https://huggingface.co/papers/2503.23730', 'title': 'KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language', 'url': 'https://huggingface.co/papers/2503.23730', 'abstract': 'The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA', 'score': 3, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '1d3d53298afe6cce', 'authors': ['Yoonshik Kim', 'Jaeyoon Jung'], 'affiliations': ['MAUM AI Inc. / Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.23730.jpg', 'data': {'categories': ['#open_source', '#low_resource', '#benchmark', '#multilingual'], 'emoji': '🇰🇷', 'ru': {'title': 'KOFFVQA: объективная оценка мультимодальных моделей на корейском языке', 'desc': 'Статья представляет новый бенчмарк KOFFVQA для оценки мультимодальных моделей (VLM) на корейском языке. Бенчмарк состоит из 275 вопросов с изображениями и критериями оценки, охватывающими 10 аспектов работы VLM. Авторы предлагают объективный метод оценки ответов моделей, основанный на предопределенных критериях. Эксперименты показывают, что этот подход более надежен, чем существующие методы оценки VLM.'}, 'en': {'title': 'KOFFVQA: Reliable Evaluation for Korean Vision-Language Models', 'desc': 'This paper introduces KOFFVQA, a new benchmark for evaluating Large Vision-Language Models (VLMs) specifically in the Korean language. It addresses the limitations of current evaluation methods that either restrict responses or rely on subjective judge models. KOFFVQA features 275 visual question-answering tasks with clear grading criteria that enhance reliability and objectivity in assessments. The authors demonstrate that their approach provides a more dependable evaluation of VLMs compared to existing methods, making it suitable for both large and small models.'}, 'zh': {'title': 'KOFFVQA：韩语视觉语言模型的可靠评估基准', 'desc': '本文介绍了一种新的视觉语言模型（VLM）评估基准，名为KOFFVQA，专门针对韩语。现有的评估方法往往依赖于预设的回答选项或评判模型，导致评估结果主观且不可靠。KOFFVQA包含275个精心设计的问题，每个问题都配有图像和涵盖VLM性能的10个评估标准。通过客观的评估标准，我们的方法能够更可靠地评估模型，即使是小型开源模型也能有效使用。'}}}, {'id': 'https://huggingface.co/papers/2503.20286', 'title': 'Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization', 'url': 'https://huggingface.co/papers/2503.20286', 'abstract': 'Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.', 'score': 3, 'issue_id': 2994, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'bf1debfaa462fca8', 'authors': ['Zhenyu Liang', 'Hao Li', 'Naiwei Yu', 'Kebin Sun', 'Ran Cheng'], 'affiliations': ['Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Data Science and Artificial Intelligence and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.20286.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#robotics', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Тензоризация EMO: революция в скорости многоцелевой оптимизации', 'desc': 'Статья представляет новый подход к эволюционной многоцелевой оптимизации (EMO) с использованием тензоризации для ускорения алгоритмов на GPU. Авторы применили эту методологию к трем известным алгоритмам EMO: NSGA-III, MOEA/D и HypE. Эксперименты показали ускорение до 1113 раз по сравнению с версиями для CPU при сохранении качества решений. Также был представлен новый бенчмарк многоцелевого управления роботом с использованием физического движка на GPU для оценки эффективности предложенного подхода.'}, 'en': {'title': 'Accelerating EMO with GPU Tensorization for Enhanced Performance', 'desc': 'This paper addresses the limitations of traditional evolutionary multiobjective optimization (EMO) algorithms when faced with complex and large-scale problems. It highlights the lack of hardware acceleration in existing EMO approaches and proposes a novel method to parallelize these algorithms using GPUs through tensorization. By transforming EMO data structures into tensor representations, the authors enable efficient GPU computing, resulting in significant performance improvements. The proposed tensorized EMO algorithms demonstrate remarkable speedups while maintaining solution quality in multiobjective robot control tasks.'}, 'zh': {'title': '张量化提升EMO算法性能，GPU加速显著', 'desc': '进化多目标优化（EMO）在过去二十年取得了显著进展，但随着问题规模和复杂性的增加，传统的EMO算法面临性能限制。本文提出了一种通过张量化方法在GPU上并行化EMO算法的方案，以解决传统算法的并行性和可扩展性不足的问题。通过张量化，EMO算法的数据结构和操作被转化为简洁的张量表示，从而实现了GPU计算的自动利用。实验结果表明，张量化的EMO算法在速度上比基于CPU的算法快了多达1113倍，同时保持了解决方案的质量，并有效处理复杂的多目标机器人控制任务。'}}}, {'id': 'https://huggingface.co/papers/2503.14941', 'title': 'UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation', 'url': 'https://huggingface.co/papers/2503.14941', 'abstract': 'Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves a Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences.', 'score': 3, 'issue_id': 3000, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '0d6cdad4ff3e795e', 'authors': ['Qihui Zhang', 'Munan Ning', 'Zheyuan Liu', 'Yanbo Wang', 'Jiayi Ye', 'Yue Huang', 'Shuo Yang', 'Xiao Chen', 'Yibing Song', 'Li Yuan'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'School of Electrical and Computer Engineering, Peking University', 'Tsinghua University', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2503.14941.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#interpretability', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Автоматическая оценка мультимодальных ИИ без участия человека', 'desc': 'Статья представляет новый подход к оценке мультимодальных больших языковых моделей (MLLM) для задач визуального вопросно-ответного анализа. Авторы предлагают фреймворк Unsupervised Peer review MLLM Evaluation (UPME), который использует только данные изображений для автоматической генерации вопросов и проведения экспертной оценки ответов других моделей. UPME включает систему оценки связи между зрением и языком, фокусируясь на правильности ответа, визуальном понимании и рассуждении, а также корреляции между изображением и текстом. Экспериментальные результаты показывают высокую корреляцию UPME с человеческими оценками на различных наборах данных.'}, 'en': {'title': 'Automating VQA Evaluations with Unsupervised Peer Review', 'desc': 'This paper introduces a new framework called Unsupervised Peer review MLLM Evaluation (UPME) to improve the evaluation of Multimodal Large Language Models (MLLMs) in Visual Question Answering (VQA). The framework reduces the need for human involvement by allowing models to automatically generate questions and assess answers from other models using only image data. To address biases in automated evaluations, a vision-language scoring system is implemented, focusing on response correctness, visual understanding, and image-text correlation. Experimental results show that UPME closely aligns with human evaluations, achieving high correlation scores on benchmark datasets.'}, 'zh': {'title': '无监督同行评审，提升视觉问答评估的效率与公正性', 'desc': '多模态大型语言模型（MLLMs）正在解决视觉问答（VQA）中的挑战，推动了对这些模型进行客观评估的新研究方向。现有的评估方法由于需要大量人力设计问答对，限制了评估的规模和范围。虽然自动化的MLLM评估方法试图减少人力工作量，但往往会引入偏见。为了解决这些问题，我们提出了一种无监督的同行评审MLLM评估框架，利用图像数据自动生成问题，并对其他模型的答案进行评估，从而有效减轻对人力的依赖。'}}}, {'id': 'https://huggingface.co/papers/2503.23022', 'title': 'MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs', 'url': 'https://huggingface.co/papers/2503.23022', 'abstract': 'In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, a novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) a transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) a flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with a predefined number of faces. By utilizing the diffusion model for the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds (35times faster than existing baselines). Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation.', 'score': 2, 'issue_id': 3002, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '4bb15e0559669bbb', 'authors': ['Xianglong He', 'Junyi Chen', 'Di Huang', 'Zexiang Liu', 'Xiaoshui Huang', 'Wanli Ouyang', 'Chun Yuan', 'Yangguang Li'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'The University of Sydney', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2503.23022.jpg', 'data': {'categories': ['#3d', '#optimization', '#diffusion', '#games'], 'emoji': '🧊', 'ru': {'title': 'MeshCraft: быстрое создание 3D-сеток с контролируемой топологией', 'desc': 'MeshCraft - это новая система для эффективной и контролируемой генерации трёхмерных сеток, использующая непрерывную пространственную диффузию для создания дискретных треугольных граней. Она состоит из VAE на основе трансформера для кодирования и декодирования сеток, а также диффузионного трансформера для генерации топологии с заданным числом граней. MeshCraft значительно ускоряет процесс создания высококачественных 3D-сеток по сравнению с авторегрессивными методами, генерируя сетку из 800 граней всего за 3,2 секунды. Система превосходит современные техники в качественных и количественных оценках на наборах данных ShapeNet и Objaverse, а также легко интегрируется с существующими стратегиями условного управления.'}, 'en': {'title': 'MeshCraft: Fast and Controlled 3D Mesh Generation', 'desc': 'This paper presents MeshCraft, a new framework for generating 3D mesh topologies efficiently and with control over the number of faces. Unlike previous methods that use auto-regressive techniques, MeshCraft employs a transformer-based variational autoencoder (VAE) and a flow-based diffusion transformer to create high-quality meshes quickly. The framework allows for the simultaneous generation of entire mesh structures, significantly speeding up the process to just 3.2 seconds for an 800-face mesh. Experimental results show that MeshCraft outperforms existing methods in both quality and speed, making it a valuable tool for 3D artists.'}, 'zh': {'title': 'MeshCraft：高效可控的3D网格生成新方法', 'desc': '在3D内容创作领域，MeshCraft是一种新颖的高效可控网格生成框架。它利用连续空间扩散生成离散三角面，克服了传统自回归方法的生成速度慢和面数不可控的缺陷。MeshCraft由两个核心组件组成：基于变换器的变分自编码器（VAE）和条件化面数的流扩散变换器。通过同时生成整个网格拓扑，MeshCraft在生成高质量3D网格时速度显著提高，能够在3.2秒内生成800面网格，速度比现有方法快35倍。'}}}, {'id': 'https://huggingface.co/papers/2503.22655', 'title': 'Unicorn: Text-Only Data Synthesis for Vision Language Model Training', 'url': 'https://huggingface.co/papers/2503.22655', 'abstract': 'Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git.', 'score': 2, 'issue_id': 3005, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': 'a724544e9c0362b6', 'authors': ['Xiaomin Yu', 'Pengxiang Ding', 'Wenjie Zhang', 'Siteng Huang', 'Songyang Gao', 'Chengwei Qin', 'Kejian Wu', 'Zhaoxin Fan', 'Ziyue Qiao', 'Donglin Wang'], 'affiliations': ['Beihang University', 'Nanyang Technological University', 'Shanghai AI Lab', 'The Great Bay University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22655.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#data', '#reasoning', '#synthetic'], 'emoji': '🦄', 'ru': {'title': 'Синтез мультимодальных данных из текста для обучения ВЯМ', 'desc': 'Статья представляет новый подход к обучению визуально-языковых моделей без использования реальных изображений. Авторы предлагают трехэтапный процесс синтеза мультимодальных данных, включающий генерацию разнообразных подписей к изображениям, создание инструкций для обучения и преобразование текстовых представлений в визуальные. Этот метод позволяет создать наборы данных Unicorn-1.2M для предварительного обучения и Unicorn-471K-Instruction для обучения на основе инструкций. Подход обеспечивает экономичное и масштабируемое решение для обучения визуально-языковых моделей.'}, 'en': {'title': 'Synthesize High-Quality Multimodal Data from Text Alone!', 'desc': 'This paper presents a novel framework for synthesizing multimodal training data for vision-language models (VLMs) using only text. The proposed three-stage process generates high-quality image-text pairs without the need for real images, significantly reducing costs. In the first stage, diverse captions are created from sparse seeds using large language models, followed by the generation of instruction-tuning tasks in the second stage. Finally, the textual representations are transformed into visual representations, resulting in two datasets: Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, enhancing the training of VLMs.'}, 'zh': {'title': '无图像高效合成多模态数据', 'desc': '本论文提出了一种跨集成的三阶段多模态数据合成框架，用于生成高质量的图像-文本对。第一阶段通过大型语言模型扩展稀疏的文本种子，合成了120万条语义多样的高质量文本描述。第二阶段将47万条文本描述转化为多轮指令调优任务，以支持复杂推理。最后，第三阶段将文本表示转换为视觉表示，从而生成多样的合成图像表示，提供了一种无需真实图像的高效训练方案。'}}}, {'id': 'https://huggingface.co/papers/2503.23913', 'title': 'Entropy-Based Adaptive Weighting for Self-Training', 'url': 'https://huggingface.co/papers/2503.23913', 'abstract': 'The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs a mapping function with a tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K, EAST attains a further 1-2% performance boost compared to the vanilla method.', 'score': 1, 'issue_id': 3002, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'b7e8ee7260c71bb7', 'authors': ['Xiaoxuan Wang', 'Yihe Deng', 'Mingyu Derek Ma', 'Wei Wang'], 'affiliations': ['University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.23913.jpg', 'data': {'categories': ['#training', '#reasoning', '#math', '#benchmark', '#optimization'], 'emoji': '🧮', 'ru': {'title': 'Адаптивное взвешивание для улучшения математических способностей языковых моделей', 'desc': 'Эта статья посвящена улучшению способностей больших языковых моделей (LLM) решать математические задачи с помощью самогенерируемых путей рассуждений. Авторы предлагают метод EAST (Entropy-Based Adaptive Weighting for Self-Training), который адаптивно взвешивает данные во время самообучения, отдавая приоритет неопределенным примерам. EAST использует функцию отображения с настраиваемым параметром, контролирующим резкость взвешивания. Эмпирические результаты показывают, что EAST достигает улучшения производительности на 1% на бенчмарке MATH и на 1-2% на GSM8K по сравнению с базовыми методами.'}, 'en': {'title': 'Enhancing Model Reasoning with Adaptive Uncertainty Weighting', 'desc': "This paper explores how large language models can improve their mathematical problem-solving skills by using self-generated reasoning paths. The authors introduce a new method called Entropy-Based Adaptive Weighting for Self-Training (EAST), which focuses on prioritizing uncertain data during the training process. By adjusting the importance of different training examples based on the model's uncertainty, EAST helps the model learn from more challenging and informative cases. The results show that EAST leads to slight performance improvements on benchmark tests compared to traditional self-training methods."}, 'zh': {'title': '自适应加权提升模型推理能力', 'desc': '本文研究了大型语言模型在数学问题解决中的能力，提出了一种自我训练方法，利用自生成的推理路径来提升模型性能。我们提出了一种名为EAST的自适应加权策略，旨在优先考虑不确定性较高的数据进行自我训练。EAST通过可调参数的映射函数来控制加权的锐度，从而引导模型关注更具信息量和挑战性的示例。实验结果表明，EAST在GSM8K和MATH基准测试中均取得了性能提升，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.19906', 'title': 'AvatarArtist: Open-Domain 4D Avatarization', 'url': 'https://huggingface.co/papers/2503.19906', 'abstract': 'This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies..', 'score': 1, 'issue_id': 3012, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '9cc5699cb424a056', 'authors': ['Hongyu Liu', 'Xuan Wang', 'Ziyu Wan', 'Yue Ma', 'Jingye Chen', 'Yanbo Fan', 'Yujun Shen', 'Yibing Song', 'Qifeng Chen'], 'affiliations': ['Ant Group', 'City University of Hong Kong', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2503.19906.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#diffusion', '#3d'], 'emoji': '🎭', 'ru': {'title': 'AvatarArtist: универсальный генератор 4D-аватаров из портретов', 'desc': 'Статья посвящена созданию 4D-аватаров из портретных изображений в произвольном стиле. Авторы используют параметрические триплейны в качестве промежуточного 4D-представления и предлагают подход, сочетающий генеративно-состязательные сети (GAN) и диффузионные модели. GAN используются для связи изображений и триплейнов, а диффузионная модель помогает обрабатывать разнообразные распределения данных. Эта синергия позволяет создать мультидоменный набор данных изображение-триплейн для обучения универсального генератора 4D-аватаров.'}, 'en': {'title': 'Transforming Portraits into Dynamic 4D Avatars!', 'desc': 'This paper presents a method for creating 4D avatars from 2D portrait images in different styles. It utilizes parametric triplanes as a 4D representation and combines generative adversarial networks (GANs) with diffusion models for effective training. The approach leverages the strengths of 4D GANs to connect images and triplanes while addressing challenges with diverse data through a robust 2D diffusion prior. The resulting model, AvatarArtist, demonstrates the ability to generate high-quality 4D avatars across various image domains, with plans to share the code and data for further research.'}, 'zh': {'title': '打造多领域高质量4D头像的创新之路', 'desc': '本研究专注于开放领域的4D头像生成，旨在从任意风格的肖像图像中创建4D头像。我们选择参数三平面作为中间的4D表示，并提出了一种实用的训练范式，结合了生成对抗网络（GAN）和扩散模型的优势。研究表明，4D GAN在无监督情况下能够有效连接图像和三平面，但在处理多样化数据分布时通常面临挑战。通过引入强大的2D扩散先验，帮助GAN在不同领域之间转移其专业知识，从而构建一个多领域的图像-三平面数据集，推动通用4D头像生成器的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.19794', 'title': 'PAVE: Patching and Adapting Video Large Language Models', 'url': 'https://huggingface.co/papers/2503.19794', 'abstract': 'Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as "patches," which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE.', 'score': 1, 'issue_id': 3008, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'fefb309c48b93a29', 'authors': ['Zhuoming Liu', 'Yiquan Li', 'Khoi Duc Nguyen', 'Yiwu Zhong', 'Yin Li'], 'affiliations': ['The Chinese University of Hong Kong', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.19794.jpg', 'data': {'categories': ['#optimization', '#3d', '#transfer_learning', '#multimodal', '#reasoning', '#video', '#training'], 'emoji': '🎬', 'ru': {'title': 'PAVE: Универсальная адаптация видео-языковых моделей к новым задачам', 'desc': "PAVE - это гибкий фреймворк для адаптации предобученных видео-языковых моделей (Video LLMs) к задачам с дополнительными модальностями данных. Он вводит легковесные адаптеры, называемые 'патчами', которые добавляют небольшое количество параметров и операций к базовой модели без изменения ее архитектуры. PAVE эффективно адаптирует предобученную модель к различным задачам, включая аудиовизуальные вопросно-ответные системы, 3D-рассуждения и распознавание многоракурсного видео. Фреймворк значительно улучшает производительность базовой модели, превосходя специализированные модели при минимальном увеличении вычислительных затрат."}, 'en': {'title': 'PAVE: Adapting Video LLMs with Lightweight Patches', 'desc': 'This paper introduces PAVE, a new framework designed to adapt pre-trained video large language models (Video LLMs) for various tasks that require additional data types like audio or 3D information. PAVE utilizes lightweight adapters, called "patches," which integrate seamlessly into existing models without altering their original architecture or pre-trained weights. By doing so, it enhances the model\'s capabilities for tasks such as audio-visual question answering and multi-view video recognition while maintaining a low computational cost. The framework not only improves performance over state-of-the-art models but also supports multi-task learning and shows strong generalization across different Video LLMs.'}, 'zh': {'title': 'PAVE：灵活适应视频大语言模型的新框架', 'desc': '本文提出了一种名为PAVE的灵活框架，用于将预训练的视频大语言模型（Video LLMs）适应于新的任务，特别是涉及音频、3D信息或多视角视频等附加模态的数据。PAVE引入了轻量级的适配器，称为“补丁”，这些补丁在不改变基础模型架构或预训练权重的情况下，增加了少量参数和操作。通过这种方式，PAVE能够有效地将预训练的基础模型适应于多种下游任务，如音视频问答、3D推理和多视角视频识别等。实验结果表明，PAVE在这些任务上显著提升了基础模型的性能，超越了最先进的特定任务模型，同时仅增加约0.1%的计算量和参数。'}}}, {'id': 'https://huggingface.co/papers/2503.18225', 'title': 'Decoupling Angles and Strength in Low-rank Adaptation', 'url': 'https://huggingface.co/papers/2503.18225', 'abstract': 'Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.', 'score': 1, 'issue_id': 3001, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '36bcda1f90686965', 'authors': ['Massimo Bini', 'Leander Girrbach', 'Zeynep Akata'], 'affiliations': ['Helmholtz Munich', 'Technical University of Munich, Munich Center for Machine Learning', 'University of Tubingen, Tubingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2503.18225.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '🎛️', 'ru': {'title': 'DeLoRA: Устойчивая и эффективная тонкая настройка моделей машинного обучения', 'desc': 'DeLoRA - это новый метод тонкой настройки моделей машинного обучения, который нормализует и масштабирует обучаемые матрицы низкого ранга. Он отделяет угловое обучение от силы адаптации, повышая устойчивость без ущерба для производительности. DeLoRA показывает результаты на уровне или выше других методов эффективной настройки параметров (PEFT) в задачах генерации изображений, понимания естественного языка и инструктивной настройки. Метод особенно эффективен при выборе гиперпараметров и длительном обучении.'}, 'en': {'title': 'Decoupling Adaptation for Robust Fine-Tuning', 'desc': 'This paper introduces Decoupled Low-rank Adaptation (DeLoRA), a new method for fine-tuning large pretrained models efficiently. DeLoRA improves upon existing Parameter-Efficient FineTuning (PEFT) methods by normalizing and scaling low-rank matrices, which enhances robustness against hyperparameter variations. Unlike traditional methods like LoRA, which struggle with stability, DeLoRA decouples the learning angle from the adaptation strength, allowing for better performance across various tasks. The results demonstrate that DeLoRA not only matches but often exceeds the performance of other PEFT techniques in applications such as image generation and natural language understanding.'}, 'zh': {'title': '解耦低秩适应：提升微调鲁棒性的新方法', 'desc': '本文提出了一种新的微调方法，称为解耦低秩适应（DeLoRA），旨在提高参数高效微调（PEFT）方法的鲁棒性。DeLoRA通过规范化和缩放可学习的低秩矩阵，增强了模型在不同任务中的适应能力。与传统的微调方法相比，DeLoRA在保持性能的同时，能够更好地应对超参数选择和训练过程中的变化。通过在图像生成、自然语言理解和指令调优等任务上的评估，DeLoRA的表现与其他PEFT方法相当或更优，同时展现出更强的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.22677', 'title': 'DSO: Aligning 3D Generators with Simulation Feedback for Physical\n  Soundness', 'url': 'https://huggingface.co/papers/2503.22677', 'abstract': 'Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.', 'score': 0, 'issue_id': 3014, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '9a7faeb192777179', 'authors': ['Ruining Li', 'Chuanxia Zheng', 'Christian Rupprecht', 'Andrea Vedaldi'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.22677.jpg', 'data': {'categories': ['#3d', '#dataset', '#alignment', '#optimization', '#rlhf', '#training'], 'emoji': '🏗️', 'ru': {'title': 'Стабильные 3D-объекты: генерация с учетом физики без замедления', 'desc': 'Статья представляет новый подход к генерации стабильных 3D-объектов с помощью машинного обучения. Авторы предлагают метод Direct Simulation Optimization (DSO), который использует обратную связь от физического симулятора для улучшения генеративной модели. Они вводят новую целевую функцию Direct Reward Optimization (DRO) для настройки диффузионных моделей. Эксперименты показывают, что обученный генератор быстрее и эффективнее создает стабильные объекты по сравнению с оптимизацией во время вывода.'}, 'en': {'title': 'Generating Stable 3D Objects with Direct Simulation Optimization', 'desc': 'This paper introduces Direct Simulation Optimization (DSO), a new framework for generating stable 3D objects that can support themselves under gravity. Unlike previous methods that relied on slow and unstable differentiable physics simulators, DSO uses feedback from a non-differentiable simulator to enhance the stability of generated objects. The authors create a dataset of 3D objects with stability scores and employ direct preference optimization (DPO) and direct reward optimization (DRO) to fine-tune the generator. The results demonstrate that DSO significantly improves the speed and stability of 3D object generation without needing ground-truth training data.'}, 'zh': {'title': '直接模拟优化：生成稳定3D物体的新方法', 'desc': '本论文提出了一种新的3D物体生成框架，称为直接模拟优化（DSO），旨在生成符合物理约束的稳定3D物体。传统方法依赖于可微分物理模拟器进行几何优化，但速度慢且容易陷入局部最优。DSO框架利用来自非可微分模拟器的反馈，直接提高生成器输出稳定物体的可能性。通过构建带有稳定性评分的数据集，研究者可以使用直接偏好优化（DPO）或直接奖励优化（DRO）来微调生成器，从而实现更快且更稳定的3D物体生成。'}}}, {'id': 'https://huggingface.co/papers/2503.22668', 'title': 'Understanding Co-speech Gestures in-the-wild', 'url': 'https://huggingface.co/papers/2503.22668', 'abstract': "Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-text-speech associations: (i) gesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal speech-text-video-gesture representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs), across all three tasks. Further analysis reveals that speech and text modalities capture distinct gesture-related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal", 'score': 0, 'issue_id': 3007, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '77fc0ab156b46a61', 'authors': ['Sindhu B Hegde', 'K R Prajwal', 'Taein Kwon', 'Andrew Zisserman'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.22668.jpg', 'data': {'categories': ['#multimodal', '#benchmark'], 'emoji': '🤲', 'ru': {'title': 'Новый подход к пониманию жестов в речи: тримодальное обучение для улучшенного распознавания', 'desc': 'Статья представляет новую систему для понимания жестов, сопровождающих речь, в реальных условиях. Авторы предлагают три новые задачи и критерии оценки для проверки способности модели понимать связи между жестами, текстом и речью. Они разработали подход, который обучает тримодальное представление речи-текста-видео-жестов для решения этих задач. Их метод превосходит предыдущие подходы, включая крупные визуально-языковые модели, во всех трех задачах.'}, 'en': {'title': 'Unlocking Gesture Understanding with Tri-Modal Learning', 'desc': 'This paper presents a new framework for understanding co-speech gestures, which are important for non-verbal communication. It introduces three tasks to evaluate how well models can understand the relationships between gestures, text, and speech. The authors propose a tri-modal representation that combines speech, text, video, and gestures, using a unique loss function to improve learning from real-world videos. Their approach outperforms existing methods, including large vision-language models, highlighting the benefits of a shared embedding space for different modalities.'}, 'zh': {'title': '理解共语手势的新框架', 'desc': '本论文介绍了一种新的框架，用于理解自然环境中的共语手势。我们提出了三个新任务和基准，以评估模型理解手势、文本和语音之间关联的能力。通过结合全局短语对比损失和局部手势-词耦合损失，我们展示了如何在弱监督的情况下，从视频中学习强大的手势表示。我们的学习表示在所有三个任务中都超越了之前的方法，包括大型视觉-语言模型。'}}}, {'id': 'https://huggingface.co/papers/2504.00999', 'title': 'MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization', 'url': 'https://huggingface.co/papers/2504.00999', 'abstract': 'Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.', 'score': 56, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'bb6506ffd72aed19', 'authors': ['Siyuan Li', 'Luyuan Zhang', 'Zedong Wang', 'Juanxi Tian', 'Cheng Tan', 'Zicheng Liu', 'Chang Yu', 'Qingsong Xie', 'Haonan Lu', 'Haoqian Wang', 'Zhen Lei'], 'affiliations': ['CAIR, HKISI-CAS', 'MAIS CASIA', 'OPPO AI Center', 'The Hong Kong University of Science and Technology', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00999.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'MergeVQ: Эффективное объединение генерации изображений и обучения представлений', 'desc': 'Статья представляет новый метод MergeVQ, объединяющий маскированное моделирование изображений (MIM) с векторным квантованием (VQ) для улучшения генерации изображений и обучения представлений. MergeVQ использует технику слияния токенов в кодировщике для эффективного квантования и глобального выравнивания, а также кросс-внимание в декодере для восстановления деталей. Для генерации второго этапа предлагается MergeAR, выполняющий сжатие KV-кэша для эффективного предсказания в растровом порядке. Эксперименты на ImageNet показывают, что MergeVQ достигает конкурентоспособных результатов в задачах обучения визуальных представлений и генерации изображений, сохраняя высокую эффективность токенов и скорость вывода.'}, 'en': {'title': 'MergeVQ: Bridging Image Generation and Representation Learning Efficiently', 'desc': 'This paper introduces MergeVQ, a novel approach that enhances Masked Image Modeling (MIM) using Vector Quantization (VQ) techniques. It addresses the challenge of balancing image generation quality with efficient representation learning by integrating token merging into the VQ framework. During the pre-training phase, MergeVQ utilizes a token merge module to separate high-level semantics from the latent space, allowing for improved quantization and detail recovery. The second stage, MergeAR, optimizes the generation process through KV Cache compression, resulting in a model that excels in both visual representation and image generation while ensuring efficiency in token usage and inference speed.'}, 'zh': {'title': 'MergeVQ：提升图像生成与表示学习的统一模型', 'desc': '本文提出了一种新的模型MergeVQ，旨在改善基于向量量化的图像生成和视觉表示学习之间的平衡。通过在编码器中引入令牌合并技术，MergeVQ能够在自注意力块后解耦潜在空间中的语义，从而提高生成质量和表示学习的效率。该模型在预训练阶段通过交叉注意力恢复细节，并在生成阶段使用KV缓存压缩来提高预测效率。实验结果表明，MergeVQ在视觉表示学习和图像生成任务中表现出色，同时保持了良好的令牌效率和推理速度。'}}}, {'id': 'https://huggingface.co/papers/2504.00883', 'title': 'Improved Visual-Spatial Reasoning via R1-Zero-Like Training', 'url': 'https://huggingface.co/papers/2504.00883', 'abstract': 'Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.', 'score': 42, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '43fba84cc49adfad', 'authors': ['Zhenyi Liao', 'Qingsong Xie', 'Yanhao Zhang', 'Zijian Kong', 'Haonan Lu', 'Zhenyu Yang', 'Zhijie Deng'], 'affiliations': ['OPPO AI Center', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00883.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#video', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в визуально-пространственном мышлении мультимодальных ИИ', 'desc': 'Это исследование посвящено улучшению визуально-пространственного мышления мультимодальных больших языковых моделей (MLLM) с помощью обучения, подобного R1-Zero. Авторы обнаружили, что способности к визуально-пространственному мышлению у небольших и средних моделей Qwen2-VL не могут быть активированы с помощью подсказок цепочки рассуждений (CoT). Они применили обучение GRPO с использованием тщательно подобранного набора данных VSI-100k для улучшения визуально-пространственного мышления. В результате их модель vsGRPO-7B, дообученная на основе Qwen2-VL-7B, достигла производительности, сравнимой с лучшей моделью с открытым исходным кодом LLaVA-NeXT-Video-72B.'}, 'en': {'title': 'Enhancing Visual-Spatial Reasoning in MLLMs with GRPO Training', 'desc': 'This paper focuses on enhancing the visual-spatial reasoning abilities of multi-modal large language models (MLLMs), which are crucial for AI agents interacting with the physical world. The authors introduce a novel training method called GRPO, applied to the Qwen2-VL models, to improve their reasoning capabilities using a specially curated dataset named VSI-100k. They discover that traditional Chain of Thought prompts are ineffective for activating these reasoning skills in smaller models. The results show that their fine-tuned models significantly outperform baseline models, demonstrating the effectiveness of their approach in advancing visual-spatial intelligence in MLLMs.'}, 'zh': {'title': '提升多模态模型的视觉空间推理能力', 'desc': '本研究聚焦于提升多模态大型语言模型（MLLMs）的视觉空间推理能力，特别是在视频基础的视觉空间智能（VSI）方面。我们发现小到中型的Qwen2-VL模型无法通过思维链（CoT）提示激活其视觉空间推理能力，因此引入了GRPO训练方法，并使用精心策划的VSI-100k数据集进行改进。经过120个GPU小时的训练，我们的vsGRPO-2B模型在性能上超越了基础模型12.1%，并且vsGRPO-7B模型的表现与最佳开源模型LLaVA-NeXT-Video-72B相当。我们的研究表明，保持KL惩罚在GRPO中是必要的，并且vsGRPO在与监督微调和直接偏好优化基线的比较中表现出明显的优势。'}}}, {'id': 'https://huggingface.co/papers/2504.01014', 'title': 'AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction', 'url': 'https://huggingface.co/papers/2504.01014', 'abstract': 'Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.', 'score': 29, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '98efa783105c3173', 'authors': ['Junhao Cheng', 'Yuying Ge', 'Yixiao Ge', 'Jing Liao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01014.jpg', 'data': {'categories': ['#diffusion', '#games', '#multimodal', '#video'], 'emoji': '🎮', 'ru': {'title': 'AnimeGamer: погружение в интерактивный мир аниме через языковые инструкции', 'desc': 'AnimeGamer - это новый подход к созданию интерактивных игр с персонажами аниме, использующий мультимодальные языковые модели (MLLM). Система генерирует динамические анимационные кадры, отображающие движения персонажей и изменения их состояний, на основе текстовых диалогов. AnimeGamer вводит новые мультимодальные представления с учетом действий, которые декодируются в видеоклипы высокого качества с помощью диффузионной модели. Метод превосходит существующие подходы по различным аспектам игрового опыта, что подтверждается автоматическими метриками и оценками людей.'}, 'en': {'title': 'Transforming Anime into Interactive Gaming with Dynamic AI', 'desc': 'This paper presents AnimeGamer, a novel approach to infinite anime life simulation games that utilizes Multimodal Large Language Models (MLLMs). Unlike previous methods, AnimeGamer incorporates historical visual context to ensure consistent gameplay and generates dynamic animations rather than just static images. By employing action-aware multimodal representations, it can create high-quality video clips that reflect character movements and state changes. The results show that AnimeGamer significantly enhances the gaming experience compared to existing techniques, as validated by both automated and human evaluations.'}, 'zh': {'title': 'AnimeGamer：动态互动的无限动漫游戏体验', 'desc': '最近在图像和视频合成方面的进展为生成游戏带来了新的希望。本文提出了一种名为AnimeGamer的方法，利用多模态大语言模型生成动态动画镜头，以增强游戏的互动性和沉浸感。通过引入动作感知的多模态表示，AnimeGamer能够生成具有上下文一致性和动态变化的游戏状态。实验结果表明，AnimeGamer在游戏体验的各个方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2504.01724', 'title': 'DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance', 'url': 'https://huggingface.co/papers/2504.01724', 'abstract': 'While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.', 'score': 24, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'd59102a274145730', 'authors': ['Yuxuan Luo', 'Zhengkun Rong', 'Lizhen Wang', 'Longhao Zhang', 'Tianshu Hu', 'Yongming Zhu'], 'affiliations': ['Bytedance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2504.01724.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#3d', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'Реалистичная анимация человека с точным контролем мимики и движений', 'desc': 'DreamActor-M1 - это новая модель на основе диффузионного трансформера для анимации человека по изображениям. Она использует гибридные сигналы управления, включающие неявные лицевые представления, 3D-сферы головы и 3D-скелеты тела для точного контроля мимики и движений. Модель применяет прогрессивное обучение на данных разного масштаба для адаптации к различным позам и ракурсам. DreamActor-M1 также интегрирует паттерны движения из последовательных кадров с визуальными ориентирами для обеспечения долгосрочной согласованности анимации.'}, 'en': {'title': 'DreamActor-M1: Revolutionizing Human Animation with Robust Control and Consistency', 'desc': 'The paper presents DreamActor-M1, a novel framework that utilizes a diffusion transformer (DiT) to enhance human animation by addressing key limitations in existing methods. It introduces hybrid control signals that combine facial representations, 3D head spheres, and body skeletons to improve the expressiveness and control of animations. The framework also employs a progressive training strategy to adapt to various body poses and image scales, ensuring versatility in generating animations from portraits to full-body views. Additionally, it integrates motion patterns from sequential frames to maintain long-term temporal coherence, resulting in more robust and visually appealing animations.'}, 'zh': {'title': '突破动画生成的局限性，DreamActor-M1引领新潮流！', 'desc': '本文提出了一种基于扩散变换器的框架DreamActor-M1，旨在解决现有图像基础的人体动画方法在细粒度整体可控性、多尺度适应性和长期时间一致性方面的不足。通过混合控制信号，结合隐式面部表示、3D头部球体和3D身体骨架，实现了对面部表情和身体动作的强大控制，同时保持动画的表现力和身份一致性。为了适应不同的身体姿势和图像尺度，采用了渐进训练策略，使用不同分辨率和尺度的数据进行训练。实验结果表明，该方法在肖像、上半身和全身生成方面优于现有的最先进技术，具有强大的长期一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.20783', 'title': 'Understanding R1-Zero-Like Training: A Critical Perspective', 'url': 'https://huggingface.co/papers/2503.20783', 'abstract': "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.", 'score': 24, 'issue_id': 3044, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'c5971e424bc52a6b', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение способностей рассуждения языковых моделей с помощью обучения с подкреплением', 'desc': 'Исследование показало, что обучение с подкреплением (RL) в больших масштабах может напрямую улучшить способности рассуждения языковых моделей (LLM) без использования обучения с учителем. Авторы проанализировали различные базовые модели и выявили, что некоторые из них уже демонстрируют сильные способности к рассуждению без специальных шаблонов. Был обнаружен оптимизационный сдвиг в методе Group Relative Policy Optimization (GRPO), который искусственно увеличивает длину ответов. Для решения этой проблемы предложен новый метод Dr. GRPO, который улучшает эффективность токенов при сохранении производительности рассуждений.'}, 'en': {'title': 'Enhancing LLM Reasoning with Unbiased RL Optimization', 'desc': 'This paper explores how reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) without needing supervised fine-tuning. It examines the impact of different base models, particularly DeepSeek-V3-Base and Qwen2.5, on RL performance, revealing that pretraining characteristics can lead to inherent biases in reasoning capabilities. The authors also identify an optimization bias in the Group Relative Policy Optimization (GRPO) method, which can inflate response lengths during training. To counter this, they propose Dr. GRPO, a new optimization technique that enhances token efficiency while preserving reasoning accuracy, achieving a notable 43.3% accuracy on the AIME 2024 benchmark with a 7B base model.'}, 'zh': {'title': '强化学习提升推理能力的新突破', 'desc': 'DeepSeek-R1-Zero展示了大规模强化学习（RL）可以直接增强大型语言模型（LLMs）的推理能力，而无需监督微调。本文深入分析了R1-Zero训练的两个核心组成部分：基础模型和强化学习。我们研究了多种基础模型，包括DeepSeek-V3-Base，以了解预训练特性如何影响RL性能。我们的分析发现，DeepSeek-V3-Base已经展现出“恍然大悟”的时刻，而Qwen2.5基础模型即使在没有提示模板的情况下也表现出强大的推理能力，暗示了潜在的预训练偏差。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.01956', 'title': 'VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step', 'url': 'https://huggingface.co/papers/2504.01956', 'abstract': 'Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene', 'score': 22, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '44f1db8ef8cc244a', 'authors': ['Hanyang Wang', 'Fangfu Liu', 'Jiawei Chi', 'Yueqi Duan'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01956.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d'], 'emoji': '🎬', 'ru': {'title': 'VideoScene: Эффективная генерация 3D сцен из видео с помощью дистилляции диффузионных моделей', 'desc': 'Статья представляет VideoScene - метод для эффективной генерации трехмерных сцен из разреженных видов. Авторы предлагают стратегию дистилляции 3D-aware leap flow для преодоления избыточной информации и обучения динамической политики шумоподавления. VideoScene достигает более быстрых и качественных результатов генерации 3D сцен по сравнению с предыдущими моделями диффузии видео. Этот подход открывает новые возможности для приложений преобразования видео в 3D.'}, 'en': {'title': 'Efficient 3D Scene Generation from Sparse Views Using Video Diffusion Models', 'desc': 'This paper addresses the challenge of reconstructing 3D scenes from sparse views, which is often complicated by the lack of visual information. Traditional methods have limitations, particularly when input views have minimal overlap, leading to degraded performance. The authors introduce VideoScene, a novel approach that utilizes video diffusion models to efficiently generate 3D scenes in a single step. By implementing a 3D-aware leap flow distillation strategy and a dynamic denoising policy network, VideoScene significantly improves the speed and quality of 3D scene generation compared to existing methods.'}, 'zh': {'title': '高效生成3D场景的VideoScene', 'desc': '从稀疏视图恢复3D场景是一项具有挑战性的任务，因为它本质上是一个不适定的问题。传统方法通过几何正则化或前馈确定性模型等专门解决方案来缓解这一问题，但在输入视图重叠较少且视觉信息不足时，性能仍然下降。最近的视频生成模型显示出解决这一挑战的潜力，能够生成具有合理3D结构的视频片段。本文提出了VideoScene，通过视频扩散模型提炼生成3D场景，设计了3D感知的跃迁流蒸馏策略，以提高生成效率和效果。'}}}, {'id': 'https://huggingface.co/papers/2503.23368', 'title': 'Towards Physically Plausible Video Generation via VLM Planning', 'url': 'https://huggingface.co/papers/2503.23368', 'abstract': 'Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.', 'score': 21, 'issue_id': 3050, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'f3087a720104ea83', 'authors': ['Xindi Yang', 'Baolu Li', 'Yiming Zhang', 'Zhenfei Yin', 'Lei Bai', 'Liqian Ma', 'Zhiyong Wang', 'Jianfei Cai', 'Tien-Tsin Wong', 'Huchuan Lu', 'Xu Jia'], 'affiliations': ['Dalian University of Technology', 'Monash University', 'Oxford University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Sydney', 'ZMO AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.23368.jpg', 'data': {'categories': ['#reasoning', '#diffusion', '#video', '#architecture', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Физически достоверная генерация видео с помощью ИИ', 'desc': 'Эта статья представляет новый двухэтапный подход к генерации видео с учетом физических законов. На первом этапе используется Визуально-Языковая Модель (VLM) для планирования грубых траекторий движения с учетом физики. На втором этапе эти траектории используются для управления Моделью Диффузии Видео (VDM) при генерации конечного результата. Добавление шума во время вывода позволяет VDM генерировать более детальное движение. Эксперименты показывают, что этот метод превосходит существующие подходы в создании физически правдоподобных видео.'}, 'en': {'title': 'Bringing Physics to Video Generation: A Two-Stage Approach', 'desc': 'This paper introduces a new two-stage framework for generating videos that are more physically realistic using video diffusion models (VDMs). The first stage uses a Vision Language Model (VLM) to create rough motion trajectories that consider real-world physics, ensuring that the generated video maintains consistency between frames. In the second stage, these trajectories guide the VDM in producing detailed video content, with added noise to allow for creative freedom in motion generation. The results show that this approach significantly improves the physical plausibility of the generated videos compared to existing methods.'}, 'zh': {'title': '引入物理知识的视频生成新框架', 'desc': '视频扩散模型（VDMs）在生成高度真实的视频方面取得了显著进展，但它们常常缺乏对物理的理解，导致生成的视频在动态和事件序列上不够合理。为了解决这个问题，我们提出了一种新颖的两阶段图像到视频生成框架，明确地融入了物理知识。在第一阶段，我们使用视觉语言模型（VLM）作为粗略的运动规划器，结合思维链和物理感知推理，预测接近真实物理动态的粗略运动轨迹。第二阶段则利用预测的运动轨迹来指导VDM的视频生成，从而实现更细致的运动表现。'}}}, {'id': 'https://huggingface.co/papers/2504.01848', 'title': "PaperBench: Evaluating AI's Ability to Replicate AI Research", 'url': 'https://huggingface.co/papers/2504.01848', 'abstract': "We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.", 'score': 18, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '60923777325e85cc', 'authors': ['Giulio Starace', 'Oliver Jaffe', 'Dane Sherburn', 'James Aung', 'Jun Shern Chan', 'Leon Maksin', 'Rachel Dias', 'Evan Mays', 'Benjamin Kinsella', 'Wyatt Thompson', 'Johannes Heidecke', 'Amelia Glaese', 'Tejal Patwardhan'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2504.01848.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#survey'], 'emoji': '🧠', 'ru': {'title': 'PaperBench: измеряем способность ИИ воспроизводить передовые исследования', 'desc': 'PaperBench - это новый бенчмарк для оценки способности ИИ-агентов воспроизводить современные исследования в области искусственного интеллекта. Он включает в себя 20 статей из ICML 2024, которые агенты должны воспроизвести с нуля, включая понимание вклада статьи, разработку кодовой базы и успешное выполнение экспериментов. Для объективной оценки разработаны рубрики, которые иерархически разбивают каждую задачу на подзадачи с четкими критериями оценки. Лучший протестированный агент, Claude 3.5 Sonnet (New) с открытым исходным кодом, достиг среднего балла воспроизведения 21.0%.'}, 'en': {'title': "Evaluating AI's Research Replication Skills with PaperBench", 'desc': "This paper presents PaperBench, a benchmark designed to assess AI agents' ability to replicate advanced AI research. The benchmark includes 20 selected papers from ICML 2024, requiring agents to comprehend contributions, create a codebase, and conduct experiments. To ensure objective evaluation, the authors developed detailed rubrics that break down replication tasks into smaller, graded components, totaling 8,316 tasks. The study also introduces an LLM-based judge for automated grading and compares the performance of AI agents against human experts, revealing that current models still lag behind human capabilities."}, 'zh': {'title': 'PaperBench：评估AI复制研究能力的基准', 'desc': '我们介绍了PaperBench，这是一个评估人工智能代理复制最先进AI研究能力的基准。代理需要从头开始复制20篇ICML 2024的亮点和口头论文，包括理解论文贡献、开发代码库和成功执行实验。为了进行客观评估，我们开发了分层的评分标准，将每个复制任务分解为更小的子任务，并设定明确的评分标准。我们的评估还包括使用基于大型语言模型的评审者自动评分，并与顶尖的机器学习博士进行比较，发现目前的模型尚未超越人类基线。'}}}, {'id': 'https://huggingface.co/papers/2504.00824', 'title': 'ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations', 'url': 'https://huggingface.co/papers/2504.00824', 'abstract': "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.", 'score': 18, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'b135f3f003dcaaff', 'authors': ['Yubo Wang', 'Xueguang Ma', 'Ping Nie', 'Huaye Zeng', 'Zhiheng Lyu', 'Yuxuan Zhang', 'Benjamin Schneider', 'Yi Lu', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.00824.jpg', 'data': {'categories': ['#science', '#dataset', '#rag', '#multimodal', '#alignment'], 'emoji': '🎓', 'ru': {'title': 'ScholarCopilot: ИИ-помощник для профессионального академического письма', 'desc': 'ScholarCopilot - это унифицированная система, улучшающая существующие большие языковые модели для генерации профессиональных академических статей с точными и контекстуально релевантными цитатами. Она динамически определяет, когда извлекать научные ссылки, генерируя токен [RET], и использует его представление для поиска релевантных цитат в базе данных. ScholarCopilot совместно оптимизирует задачи генерации и цитирования в рамках единой системы для повышения эффективности. Обученная на 500 тысячах статей из arXiv, модель достигает точности извлечения top-1 в 40.1% на оценочном наборе данных, превосходя базовые модели.'}, 'en': {'title': 'Enhancing Academic Writing with ScholarCopilot', 'desc': 'This paper presents ScholarCopilot, a new framework that improves large language models for generating academic articles with accurate citations. It uses a retrieval token to decide when to fetch scholarly references, enhancing the text generation process with relevant citations. The model is trained on a large dataset of academic papers and shows significant improvements in both citation accuracy and writing quality compared to existing models. Human evaluations further validate its effectiveness in citation recall and overall user experience.'}, 'zh': {'title': 'ScholarCopilot：提升学术写作的智能助手', 'desc': '本研究提出了ScholarCopilot，一个统一框架，旨在提升大型语言模型在生成专业学术文章时的准确性和相关性。该系统通过生成检索标记[RET]，动态决定何时检索学术参考文献，并利用其表示从数据库中查找相关引用。ScholarCopilot在生成和引用任务上进行联合优化，以提高效率。经过在500K篇arXiv论文上的训练，该模型在评估数据集上实现了40.1%的顶级检索准确率，且在学术写作样本的生成质量上超越了参数更多的模型。'}}}, {'id': 'https://huggingface.co/papers/2504.01934', 'title': 'ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement', 'url': 'https://huggingface.co/papers/2504.01934', 'abstract': 'We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.', 'score': 16, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'a50e19f04d94405f', 'authors': ['Runhui Huang', 'Chunwei Wang', 'Junwei Yang', 'Guansong Lu', 'Yunlong Yuan', 'Jianhua Han', 'Lu Hou', 'Wei Zhang', 'Lanqing Hong', 'Hengshuang Zhao', 'Hang Xu'], 'affiliations': ['Huawei Noahs Ark Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01934.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#training', '#games', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ILLUME+: Унифицированная мультимодальная модель нового поколения', 'desc': 'ILLUME+ - это новая мультимодальная языковая модель, использующая двойную визуальную токенизацию и диффузионный декодер. Она улучшает глубокое семантическое понимание и высококачественную генерацию изображений. ILLUME+ вводит унифицированный двойной визуальный токенизатор DualViTok, сохраняющий как мелкие текстуры, так и семантику, согласованную с текстом. Модель демонстрирует конкурентоспособную производительность в задачах мультимодального понимания, генерации и редактирования по сравнению с существующими унифицированными MLLM и специализированными моделями.'}, 'en': {'title': 'ILLUME+: Unifying Understanding, Generation, and Editing in One Model', 'desc': 'ILLUME+ is a novel model that combines dual visual tokenization and a diffusion decoder to enhance deep semantic understanding and high-quality image generation. Unlike previous models that struggled with understanding, generation, and editing simultaneously, ILLUME+ effectively integrates these capabilities through its innovative DualViTok tokenizer. This approach preserves fine textures and aligns semantics, allowing for better image editing and generation. With a continuous-input, discrete-output framework and progressive training, ILLUME+ achieves competitive performance in multimodal tasks, paving the way for future applications.'}, 'zh': {'title': 'ILLUME+: 多模态理解与生成的新突破', 'desc': 'ILLUME+ 是一种新型的多模态学习模型，结合了双重视觉标记和扩散解码器，旨在提升深层语义理解和高保真图像生成能力。与现有的统一模型相比，ILLUME+ 能够更好地处理理解、生成和编辑这三种基本能力。通过引入 DualViTok，ILLUME+ 保留了细致的纹理和文本对齐的语义，同时采用粗到细的图像表示策略，增强了多模态理解和生成的能力。此外，ILLUME+ 在图像生成质量和超分辨率方面表现出色，展现了与现有模型的竞争力，为未来的多模态应用奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2504.01204', 'title': 'Articulated Kinematics Distillation from Video Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01204', 'abstract': 'We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/', 'score': 12, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'c8630e7ca691cef3', 'authors': ['Xuan Li', 'Qianli Ma', 'Tsung-Yi Lin', 'Yongxin Chen', 'Chenfanfu Jiang', 'Ming-Yu Liu', 'Donglai Xiang'], 'affiliations': ['NVIDIA', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2504.01204.jpg', 'data': {'categories': ['#diffusion', '#games', '#3d'], 'emoji': '🦾', 'ru': {'title': 'Революция в анимации персонажей: от скелета к реалистичному движению', 'desc': 'Articulated Kinematics Distillation (AKD) - это новый подход к генерации высококачественной анимации персонажей, объединяющий скелетную анимацию и современные генеративные модели. AKD использует скелетное представление для 3D-моделей, значительно уменьшая количество степеней свободы за счет фокуса на управлении суставами. Метод применяет Score Distillation Sampling с предобученными видео-диффузионными моделями для синтеза сложных движений, сохраняя структурную целостность. AKD показывает превосходные результаты по сравнению с существующими методами генерации текста в 4D анимацию.'}, 'en': {'title': 'Revolutionizing Character Animation with AKD', 'desc': 'Articulated Kinematics Distillation (AKD) is a new framework designed to create realistic character animations by combining skeleton-based animation techniques with advanced generative models. It simplifies the animation process by using a skeleton representation, which reduces the Degrees of Freedom (DoFs) and allows for better control over joint movements. AKD employs Score Distillation Sampling (SDS) with pre-trained video diffusion models to generate complex motions while ensuring that the shapes of the characters remain consistent. This method also integrates well with physics-based simulations, resulting in animations that are not only visually appealing but also physically plausible.'}, 'zh': {'title': '关节运动蒸馏：高保真动画的新方法', 'desc': '本文提出了一种名为关节运动蒸馏（AKD）的框架，用于生成高保真角色动画。AKD通过使用基于骨骼的表示，显著减少了自由度，专注于关节级控制，从而实现高效且一致的运动合成。通过与预训练的视频扩散模型结合的得分蒸馏采样（SDS），AKD能够蒸馏复杂的关节运动，同时保持结构完整性，克服了4D神经变形场在保持形状一致性方面的挑战。实验表明，AKD在3D一致性和运动质量上优于现有的文本到4D生成方法。'}}}, {'id': 'https://huggingface.co/papers/2504.01308', 'title': 'Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks', 'url': 'https://huggingface.co/papers/2504.01308', 'abstract': 'Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.', 'score': 11, 'issue_id': 3040, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '315938d70f25095e', 'authors': ['Jiawei Wang', 'Yushen Zuo', 'Yuanjun Chai', 'Zhendong Liu', 'Yichen Fu', 'Yichun Feng', 'Kin-man Lam'], 'affiliations': ['Nanjing University', 'Stanford University', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China', 'University of Washington', 'University of the Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.01308.jpg', 'data': {'categories': ['#security', '#cv', '#diffusion', '#training', '#dataset', '#optimization', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Защита визуально-языковых моделей от атак с помощью шумовой аугментации', 'desc': 'Исследователи обнаружили уязвимость визуально-языковых моделей (VLM) к атакам с использованием зашумленных или искаженных изображений. Для решения этой проблемы они предложили метод Robust-VLGuard, включающий набор данных для обучения и дообучение с аугментацией шумом. Также был разработан метод DiffPure-VLM, использующий диффузионные модели для преобразования состязательных возмущений в гауссовский шум. Эксперименты показали, что предложенные методы значительно повышают устойчивость VLM к различным типам атак.'}, 'en': {'title': 'Strengthening Vision-Language Models Against Noise Attacks', 'desc': 'This paper addresses the vulnerabilities of Vision-Language Models (VLMs) to jailbreak attacks, particularly when they encounter noisy or corrupted images. The authors highlight that existing security measures during training do not adequately account for noise-augmented visual inputs, leading to significant security gaps. To combat this issue, they introduce Robust-VLGuard, a dataset designed for multimodal safety that includes both aligned and misaligned image-text pairs, along with a noise-augmented fine-tuning process. Additionally, they propose DiffPure-VLM, which uses diffusion models to transform adversarial perturbations into Gaussian-like noise, enhancing the robustness of VLMs against such attacks while maintaining their functionality.'}, 'zh': {'title': '增强视觉语言模型的安全性', 'desc': '视觉语言模型（VLMs）通过结合视觉信息扩展了大型语言模型（LLMs）的能力，但在处理噪声或损坏的图像时仍然容易受到攻击。现有的VLMs在训练过程中采取了安全措施以减轻这些攻击，但对噪声增强视觉输入的脆弱性却未给予足够重视。我们提出了Robust-VLGuard，这是一个多模态安全数据集，结合了对齐和不对齐的图像-文本对，并通过噪声增强的微调来降低攻击成功率，同时保持VLM的功能。实验结果表明，扩散模型的分布转移特性与我们微调的VLMs很好地对齐，显著减轻了不同强度的对抗扰动。'}}}, {'id': 'https://huggingface.co/papers/2405.20216', 'title': 'Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback', 'url': 'https://huggingface.co/papers/2405.20216', 'abstract': 'The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.', 'score': 10, 'issue_id': 3046, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8ffe2bddf2c0ee58', 'authors': ['Sanghyeon Na', 'Yonggyu Kim', 'Hyunjoon Lee'], 'affiliations': ['Kakao'], 'pdf_title_img': 'assets/pdf/title_img/2405.20216.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#optimization', '#training', '#diffusion', '#cv'], 'emoji': '🧑\u200d🎨', 'ru': {'title': 'Революция в генерации изображений людей с помощью DPO', 'desc': 'Эта статья представляет новый подход к генерации изображений людей с использованием метода Direct Preference Optimization (DPO). Авторы предлагают эффективный способ создания специализированного набора данных DPO для обучения моделей без необходимости дорогостоящей обратной связи от людей. Они также вводят модифицированную функцию потерь, которая улучшает процесс обучения DPO, минимизируя артефакты и повышая точность изображений. Результаты показывают значительное улучшение в генерации изображений людей с естественной анатомией, позами и соответствием текстовым запросам.'}, 'en': {'title': 'Revolutionizing Human Image Generation with Direct Preference Optimization', 'desc': 'This paper addresses the challenges of generating high-quality human images from text descriptions using text-to-image (T2I) methods. It highlights the importance of meeting specific criteria such as human pose and anatomy, which are crucial for realistic image synthesis. The authors propose a novel approach that employs Direct Preference Optimization (DPO) to create a specialized dataset for training without requiring expensive human feedback. Additionally, they introduce a modified loss function that reduces artifacts and enhances image quality, leading to significant improvements in generating human images that align well with textual prompts.'}, 'zh': {'title': '优化人类图像生成的新方法', 'desc': '本文探讨了通过文本到图像（T2I）方法生成高质量人类图像的挑战。与一般图像生成不同，人类图像合成需要满足严格的人体姿势、解剖结构和与文本提示对齐的标准。我们提出了一种新颖的方法，利用直接偏好优化（DPO）专门针对人类图像生成，构建高效的DPO数据集以训练模型，减少对昂贵人类反馈的依赖。通过修改损失函数，我们的训练过程能够减少伪影并提高图像的真实感，显著提升人类图像生成的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.23573', 'title': 'DASH: Detection and Assessment of Systematic Hallucinations of VLMs', 'url': 'https://huggingface.co/papers/2503.23573', 'abstract': "Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH.", 'score': 9, 'issue_id': 3049, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'cd9c0f9c9392d470', 'authors': ['Maximilian Augustin', 'Yannic Neuhaus', 'Matthias Hein'], 'affiliations': ['Tubingen AI Center, University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2503.23573.jpg', 'data': {'categories': ['#cv', '#hallucinations', '#benchmark', '#transfer_learning', '#training', '#dataset', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'DASH: Автоматическое обнаружение систематических ошибок в мультимодальных моделях', 'desc': 'Эта статья представляет DASH - новый метод для обнаружения и оценки систематических галлюцинаций в мультимодальных моделях компьютерного зрения и языка. DASH использует оптимизацию на многообразии естественных изображений для поиска случаев, когда модель ошибочно определяет наличие объектов. Авторы применили DASH к нескольким современным моделям и обнаружили тысячи кластеров изображений, вызывающих галлюцинации. Исследование показало, что дообучение модели на найденных проблемных изображениях помогает уменьшить количество галлюцинаций.'}, 'en': {'title': 'DASH: Detecting and Mitigating Object Hallucinations in Vision-Language Models', 'desc': "This paper addresses the issue of object hallucinations in vision-language models (VLMs), where these models incorrectly identify objects in images. The authors introduce DASH, a new automated pipeline that detects and assesses systematic hallucinations in VLMs using large-scale, real-world image datasets. DASH includes a component called DASH-OPT, which generates misleading images to evaluate the VLM's performance on the 'natural image manifold'. The study demonstrates that fine-tuning VLMs with images identified by DASH can significantly reduce the occurrence of object hallucinations."}, 'zh': {'title': '揭示视觉语言模型的系统幻觉', 'desc': '视觉语言模型（VLMs）容易出现物体幻觉，即错误地指示图像中存在某些物体。现有的基准测试使用相对较小的标记数据集来量化幻觉，但这种方法不足以评估在开放世界环境中出现的幻觉。我们提出了DASH（系统幻觉的检测与评估），这是一个自动化的大规模管道，旨在识别VLMs在真实图像中的系统幻觉。通过优化“自然图像流形”，DASH能够生成误导VLM的图像，并识别出大量的真实和语义相似图像的聚类。'}}}, {'id': 'https://huggingface.co/papers/2503.23135', 'title': 'LSNet: See Large, Focus Small', 'url': 'https://huggingface.co/papers/2503.23135', 'abstract': "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.", 'score': 4, 'issue_id': 3040, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': 'd2ac65a2356c89c3', 'authors': ['Ao Wang', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['BNRist, Tsinghua University', 'Department of Automation, Tsinghua University', 'School of Software, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23135.jpg', 'data': {'categories': ['#architecture', '#training', '#cv'], 'emoji': '👁️', 'ru': {'title': "LSNet: Эффективное компьютерное зрение по принципу 'Смотри широко, фокусируйся узко'", 'desc': "Статья представляет новый подход к проектированию легковесных нейронных сетей для компьютерного зрения, вдохновленный человеческой системой зрения. Авторы предлагают стратегию 'Смотри широко, фокусируйся узко' и вводят LS-свертку, сочетающую восприятие с большим ядром и агрегацию с малым ядром. На основе LS-свертки разработано семейство моделей LSNet, которое эффективно обрабатывает визуальную информацию. Эксперименты показывают, что LSNet превосходит существующие легковесные сети по производительности и эффективности в различных задачах компьютерного зрения."}, 'en': {'title': 'See Large, Focus Small: Efficient Vision Networks', 'desc': "This paper addresses the challenges of deploying complex vision networks like Convolutional Neural Networks and Vision Transformers in real-time applications due to their heavy computations. The authors propose a new lightweight network design strategy called 'See Large, Focus Small', inspired by the human vision system's ability to dynamically adjust focus. They introduce LS convolution, which effectively combines large-kernel perception for broad information capture and small-kernel aggregation for precise feature refinement. The resulting LSNet model demonstrates improved performance and efficiency in various vision tasks compared to existing lightweight networks."}, 'zh': {'title': '轻量级视觉网络的新策略：大视野，小聚焦', 'desc': '本文提出了一种新的轻量级视觉网络设计策略，称为“See Large, Focus Small”。该策略结合了大核感知和小核聚合的LS卷积，能够高效捕捉广泛的感知信息并实现精确的特征聚合。通过这种方法，LSNet在多种视觉任务中展现出优越的性能和效率，克服了现有轻量级模型在计算预算有限情况下的局限性。实验结果表明，LSNet在实时应用中表现出色，适合实际部署。'}}}, {'id': 'https://huggingface.co/papers/2504.00406', 'title': 'VerifiAgent: a Unified Verification Agent in Language Model Reasoning', 'url': 'https://huggingface.co/papers/2504.00406', 'abstract': 'Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent', 'score': 2, 'issue_id': 3046, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'd4ef7ad5ea6d5aad', 'authors': ['Jiuzhou Han', 'Wray Buntine', 'Ehsan Shareghi'], 'affiliations': ['College of Engineering and Computer Science, VinUniversity', 'Department of Data Science & AI, Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00406.jpg', 'data': {'categories': ['#interpretability', '#training', '#math', '#agents', '#reasoning', '#inference'], 'emoji': '🔍', 'ru': {'title': 'VerifiAgent: умный верификатор для надежных ответов языковых моделей', 'desc': 'Статья представляет VerifiAgent - унифицированного агента для верификации ответов больших языковых моделей. VerifiAgent использует двухуровневый подход: мета-верификацию для оценки полноты и согласованности ответов, и адаптивную верификацию на основе инструментов для различных типов рассуждений. Экспериментальные результаты показывают превосходство VerifiAgent над базовыми методами верификации во всех задачах рассуждения. Агент также демонстрирует эффективность в масштабировании вывода, достигая лучших результатов с меньшими затратами по сравнению с существующими моделями вознаграждения процесса в области математических рассуждений.'}, 'en': {'title': 'VerifiAgent: Enhancing Reliability in Language Model Reasoning', 'desc': "This paper introduces VerifiAgent, a novel verification system designed to improve the reliability of large language models' responses. It combines two verification levels: meta-verification for checking the completeness and consistency of answers, and tool-based adaptive verification that selects the best verification tools based on the reasoning type. This approach enhances efficiency and robustness across various reasoning tasks, outperforming traditional verification methods. Additionally, VerifiAgent improves reasoning accuracy by utilizing feedback from its verification processes and is more cost-effective in mathematical reasoning applications."}, 'zh': {'title': 'VerifiAgent：智能验证，提升推理准确性', 'desc': '大型语言模型展现了出色的推理能力，但常常产生不可靠或错误的回答。现有的验证方法通常是针对特定模型或领域，计算资源消耗大，且在不同推理任务中缺乏可扩展性。为了解决这些问题，我们提出了VerifiAgent，一个统一的验证代理，集成了两级验证：元验证评估模型回答的完整性和一致性，工具自适应验证则根据推理类型自动选择合适的验证工具。实验结果表明，VerifiAgent在所有推理任务中优于基线验证方法，并能通过利用验证结果的反馈进一步提高推理准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.22879', 'title': 'Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models', 'url': 'https://huggingface.co/papers/2503.22879', 'abstract': 'State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input x, combined with a per-state-group quantization for input-dependent parameters B and C. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3times and 3times speed-ups in the pre-filling and generation stages, respectively, while offering 4times memory reduction with only a 1.6% average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.', 'score': 2, 'issue_id': 3059, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': 'dbc6e89f0857b2e7', 'authors': ['Hung-Yueh Chiang', 'Chi-Chih Chang', 'Natalia Frumkin', 'Kai-Chiang Wu', 'Mohamed S. Abdelfattah', 'Diana Marculescu'], 'affiliations': ['Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin', 'Department of Computer Science, National Yang Ming Chiao Tung University', 'Department of Electrical and Computer Engineering, Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22879.jpg', 'data': {'categories': ['#open_source', '#inference', '#training', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная квантизация моделей пространства состояний для ускорения и экономии памяти', 'desc': 'Статья представляет Quamba2 - метод квантизации для моделей пространства состояний (SSM), позволяющий эффективно использовать различные конфигурации разрядности (W8A8, W4A8, W4A16) для моделей Mamba1 и Mamba2. Авторы предлагают офлайн-подход к 8-битной квантизации входных данных линейной рекурсии, основанный на сортировке и кластеризации. Эксперименты показывают, что Quamba2-8B превосходит современные методы квантизации SSM, обеспечивая ускорение в 1.3 и 3 раза на этапах предварительного заполнения и генерации соответственно. Оценка на наборе данных MMLU демонстрирует обобщаемость и надежность предложенного фреймворка.'}, 'en': {'title': 'Quamba2: Efficient Quantization for State Space Models', 'desc': 'This paper introduces Quamba2, a method for quantizing State Space Models (SSMs) to improve their efficiency on various platforms. By using low bit-width data formats, Quamba2 reduces the model size and enhances performance without significant accuracy loss. The approach involves sorting and clustering inputs for better quantization of parameters, ensuring that the output remains stable despite the changes. Experimental results demonstrate that Quamba2 achieves significant speed-ups and memory reductions compared to existing SSM quantization techniques.'}, 'zh': {'title': '量化状态空间模型，提升性能与效率！', 'desc': '状态空间模型（SSMs）作为一种新兴的替代方案，因其一致的内存使用和高性能而受到关注。然而，在云服务或资源有限的设备上扩展SSMs面临存储和计算能力的挑战。为了解决这个问题，使用低位宽数据格式对SSMs进行量化可以减少模型大小，并利用硬件加速。我们提出的Quamba2能够支持多种位宽配置，优化了SSMs在不同场景下的性能，同时在保持较高准确率的情况下显著提高了计算速度和内存效率。'}}}, {'id': 'https://huggingface.co/papers/2504.01201', 'title': 'Medical large language models are easily distracted', 'url': 'https://huggingface.co/papers/2504.01201', 'abstract': "Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information.", 'score': 1, 'issue_id': 3052, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'cfc7db001feb908d', 'authors': ['Krithik Vishwanath', 'Anton Alyakin', 'Daniel Alexander Alber', 'Jin Vivian Lee', 'Douglas Kondziolka', 'Eric Karl Oermann'], 'affiliations': ['Center for Data Science, New York University, New York, New York, 10016', 'Department of Aerospace Engineering and Engineering Mechanics, The University of Texas at Austin, Austin, Texas, 78712', 'Department of Mathematics, The University of Texas at Austin, Austin, Texas, 78712', 'Department of Neurological Surgery, NYU Langone Medical Center, New York, New York, 10016', 'Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, Missouri, 63110', 'Department of Radiology, NYU Langone Medical Center, New York, New York, 10016'], 'pdf_title_img': 'assets/pdf/title_img/2504.01201.jpg', 'data': {'categories': ['#benchmark', '#rag', '#healthcare', '#reasoning', '#hallucinations'], 'emoji': '🩺', 'ru': {'title': 'Большие языковые модели в медицине: проблема отделения зерен от плевел', 'desc': 'Статья исследует влияние посторонней информации на производительность больших языковых моделей (LLM) в медицинских сценариях. Авторы разработали бенчмарк MedDistractQA, содержащий вопросы в стиле USMLE с добавленными отвлекающими факторами. Результаты показывают, что отвлекающие утверждения могут снизить точность LLM на 17.9%. Стандартные методы улучшения, такие как RAG и дообучение на медицинских данных, не решают эту проблему, что указывает на отсутствие у LLM механизмов различения релевантной и нерелевантной клинической информации.'}, 'en': {'title': 'Enhancing LLMs: Tackling Noise in Medical Contexts', 'desc': "This paper explores the challenges faced by large language models (LLMs) in medical settings, particularly when they encounter irrelevant information during clinical scenarios. The authors created a benchmark called MedDistractQA, which includes USMLE-style questions with distractions that mimic real-world clinical noise. Their research found that such distractions can significantly decrease the accuracy of LLMs, by as much as 17.9%. Additionally, common strategies to improve model performance, like retrieval-augmented generation and medical fine-tuning, did not alleviate the issue and sometimes worsened it, indicating a fundamental limitation in LLMs' ability to filter relevant clinical data."}, 'zh': {'title': '提升大型语言模型在医学中的抗干扰能力', 'desc': '大型语言模型（LLMs）在医学领域具有变革潜力，但现实临床场景中存在的多余信息可能会影响其表现。我们开发了MedDistractQA基准，使用嵌入模拟现实干扰的USMLE风格问题来评估LLMs过滤相关数据的能力。研究发现，干扰性陈述会使LLM的准确性降低多达17.9%。这表明LLMs在区分相关与无关临床信息方面缺乏必要的逻辑机制，给实际应用带来了挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.18817', 'title': 'Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations', 'url': 'https://huggingface.co/papers/2503.18817', 'abstract': 'Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.', 'score': 1, 'issue_id': 3046, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'fa652fd57c6312f8', 'authors': ['Jeonghyeon Kim', 'Sangheum Hwang'], 'affiliations': ['Seoul National University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.18817.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#training', '#transfer_learning'], 'emoji': '🔍', 'ru': {'title': 'Улучшение обнаружения выбросов через мультимодальную тонкую настройку', 'desc': 'Статья посвящена обнаружению выбросов (OoDD) в мультимодальных моделях машинного обучения. Авторы предлагают метод мультимодальной тонкой настройки (MMFT) для улучшения производительности OoDD. Они вводят регуляризацию для усиления кросс-модального выравнивания, сближая семантически похожие изображения и тексты в гиперсферическом пространстве представлений. Экспериментальные результаты на наборах данных ImageNet-1k OoD показывают, что предложенный метод в сочетании с пост-хок подходами OoDD достигает наилучших результатов в обнаружении выбросов.'}, 'en': {'title': 'Enhancing Out-of-Distribution Detection with Multi-Modal Fine-Tuning', 'desc': 'This paper focuses on improving out-of-distribution detection (OoDD) using multi-modal fine-tuning (MMFT) with vision-language models like CLIP. The authors argue that traditional methods often freeze or partially tune pretrained weights, which limits performance on new datasets. They identify a key issue with na"ive fine-tuning methods, which fail to fully utilize the pretrained knowledge due to a modality gap in embeddings. To overcome this, they propose a new training objective that aligns image and text embeddings more effectively, leading to significant improvements in OoDD performance and overall accuracy on benchmark datasets.'}, 'zh': {'title': '多模态微调提升分布外检测性能', 'desc': '本论文探讨了多模态微调（MMFT）在分布外检测（OoDD）中的应用。以往的研究主要集中在单一模态模型上，而我们提出的方法通过增强图像和文本嵌入之间的跨模态对齐，显著提升了OoDD性能。我们分析了传统微调方法的局限性，并提出了一种新的训练目标，以更好地利用预训练的知识。通过在ImageNet-1k OoD基准数据集上的实验，我们的方法在OoDD性能和ID准确率上均达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2502.18924', 'title': 'MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for\n  Zero-Shot Speech Synthesis', 'url': 'https://huggingface.co/papers/2502.18924', 'abstract': 'While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces MegaTTS 3, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at https://sditdemo.github.io/sditdemo/.', 'score': 1, 'issue_id': 3053, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '538aaff0c9fb3421', 'authors': ['Ziyue Jiang', 'Yi Ren', 'Ruiqi Li', 'Shengpeng Ji', 'Boyang Zhang', 'Zhenhui Ye', 'Chen Zhang', 'Bai Jionghao', 'Xiaoda Yang', 'Jialong Zuo', 'Yu Zhang', 'Rui Liu', 'Xiang Yin', 'Zhou Zhao'], 'affiliations': ['ByteDance', 'Inner Mongolia University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18924.jpg', 'data': {'categories': ['#audio'], 'emoji': '🗣️', 'ru': {'title': 'Революция в синтезе речи: естественность и гибкость с MegaTTS 3', 'desc': 'MegaTTS 3 - это система синтеза речи, использующая инновационный алгоритм разреженного выравнивания для управления латентным диффузионным трансформером. Система предоставляет разреженные границы выравнивания для уменьшения сложности без ограничения пространства поиска, что позволяет достичь высокой естественности речи. MegaTTS 3 использует многоусловную стратегию наведения без классификатора для регулировки интенсивности акцента и технику кусочно-выпрямленного потока для ускорения процесса генерации. Эксперименты показывают, что система достигает современного качества синтеза речи без предварительного обучения и поддерживает гибкий контроль над интенсивностью акцента.'}, 'en': {'title': 'Revolutionizing TTS with Sparse Alignment and Flexible Accent Control', 'desc': 'This paper presents MegaTTS 3, a zero-shot text-to-speech (TTS) system that addresses challenges in speech-text alignment. It introduces a sparse alignment algorithm that enhances the robustness and naturalness of generated speech by providing flexible alignment boundaries. The system also incorporates a multi-condition classifier-free guidance strategy for adjusting accent intensity and uses a piecewise rectified flow technique to speed up the generation process. Experiments show that MegaTTS 3 achieves top-tier speech quality while allowing for precise control over accent, generating high-quality audio efficiently.'}, 'zh': {'title': 'MegaTTS 3：高自然性与灵活控制的文本到语音系统', 'desc': '本论文介绍了一种新的文本到语音（TTS）系统，名为MegaTTS 3，旨在解决语音与文本对齐建模的问题。该系统采用创新的稀疏对齐算法，能够在不限制搜索空间的情况下，减少对齐的难度，从而提高语音的自然性。MegaTTS 3还使用无条件分类器引导策略来调整口音强度，并采用分段修正流技术加速生成过程。实验结果表明，MegaTTS 3在零样本TTS语音质量上达到了最先进的水平，并支持对口音强度的灵活控制。'}}}, {'id': 'https://huggingface.co/papers/2503.18950', 'title': 'Target-Aware Video Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18950', 'abstract': "We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.", 'score': 0, 'issue_id': 3054, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '4009a703c8832026', 'authors': ['Taeksoo Kim', 'Hanbyul Joo'], 'affiliations': ['RLWRLD', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18950.jpg', 'data': {'categories': ['#video', '#diffusion', '#multimodal', '#robotics', '#games'], 'emoji': '🎬', 'ru': {'title': 'Генерация видео с целенаправленным взаимодействием актера и объекта', 'desc': 'Статья представляет модель диффузии видео, которая генерирует ролики из входного изображения, где актер взаимодействует с заданной целью, выполняя желаемое действие. Модель использует маску сегментации для определения цели и текстовое описание для задания действия. В отличие от существующих моделей, она не требует подробных структурных или двигательных подсказок, опираясь на обобщающие способности предобученных моделей. Авторы расширяют базовую модель, добавляя маску цели как дополнительный вход и вводя специальный токен для кодирования пространственной информации о цели в текстовом запросе.'}, 'en': {'title': 'Target-Aware Video Generation: Simplifying Human-Object Interaction', 'desc': "This paper introduces a target-aware video diffusion model that generates videos based on an input image of an actor interacting with a specified target, defined by a segmentation mask, while performing a desired action described by a text prompt. Unlike traditional models that depend on detailed structural or motion cues, this model simplifies the process by using just a mask, leveraging the strengths of pretrained models for realistic action generation. The model incorporates a special token to encode the target's spatial information, and it is fine-tuned with a novel cross-attention loss to ensure alignment between the target mask and the generated actions. Experimental results indicate that this approach significantly improves the accuracy of human-object interactions in generated videos, making it useful for applications like video content creation and 3D motion synthesis."}, 'zh': {'title': '目标感知视频生成，轻松实现人-物体互动', 'desc': '我们提出了一种目标感知的视频扩散模型，可以根据输入图像生成视频，其中演员与指定目标互动并执行所需动作。该目标通过分割掩码定义，所需动作通过文本提示描述。与现有的可控图像到视频扩散模型不同，我们的模型仅需简单的掩码来指示目标，利用预训练模型的泛化能力生成合理的动作。这种方法在处理人-物体互动场景时特别有效，能够在机器人等应用中实现高层次的动作规划。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (11)', '#agents (10)', '#agi', '#alignment (4)', '#architecture (17)', '#audio (1)', '#benchmark (31)', '#cv (13)', '#data (4)', '#dataset (20)', '#diffusion (15)', '#ethics (1)', '#games (7)', '#graphs (2)', '#hallucinations (4)', '#healthcare (2)', '#inference (11)', '#interpretability (4)', '#leakage', '#long_context (3)', '#low_resource (2)', '#machine_translation', '#math (5)', '#multilingual (2)', '#multimodal (26)', '#open_source (15)', '#optimization (35)', '#plp (1)', '#rag (3)', '#reasoning (29)', '#rl (9)', '#rlhf (6)', '#robotics (3)', '#science (2)', '#security (2)', '#small_models (1)', '#story_generation (1)', '#survey (5)', '#synthetic (3)', '#training (43)', '#transfer_learning (6)', '#video (14)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-03 23:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-03 23:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-03 23:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    