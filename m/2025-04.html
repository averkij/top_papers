
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. April 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Апрель 2025</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-03.html">⬅️ <span id="prev-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-05.html">➡️ <span id="next-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Апрель 2025', 'en': 'April 2025', 'zh': '4月2025年'};
        let feedDateNext = {'ru': '05.2025', 'en': '05/2025', 'zh': '5月2025年'};
        let feedDatePrev = {'ru': '03.2025', 'en': '03/2025', 'zh': '3月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.23307', 'title': 'MoCha: Towards Movie-Grade Talking Character Synthesis', 'url': 'https://huggingface.co/papers/2503.23307', 'abstract': 'Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.', 'score': 18, 'issue_id': 2994, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '6ce9b3642bf3ace3', 'authors': ['Cong Wei', 'Bo Sun', 'Haoyu Ma', 'Ji Hou', 'Felix Juefei-Xu', 'Zecheng He', 'Xiaoliang Dai', 'Luxin Zhang', 'Kunpeng Li', 'Tingbo Hou', 'Animesh Sinha', 'Peter Vajda', 'Wenhu Chen'], 'affiliations': ['GenAI, Meta', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23307.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#video', '#benchmark', '#story_generation'], 'emoji': '🎭', 'ru': {'title': 'MoCha: новый уровень ИИ-генерации кинематографических историй', 'desc': 'Представлена система MoCha для генерации анимированных разговаривающих персонажей на основе речи и текста. Использован механизм внимания для синхронизации речи и видео, а также совместное обучение на данных с речевой и текстовой разметкой. Система позволяет генерировать диалоги нескольких персонажей с учетом контекста. Результаты превосходят существующие подходы по реалистичности и выразительности генерируемых анимаций.'}, 'en': {'title': 'Revolutionizing Character Animation with MoCha', 'desc': "This paper presents MoCha, a novel approach for generating talking character animations from speech and text, focusing on full character portrayal rather than just facial movements. It introduces a speech-video window attention mechanism to ensure that the generated video aligns accurately with the spoken words. To tackle the challenge of limited speech-labeled video datasets, the authors propose a joint training strategy that utilizes both speech and text-labeled data, enhancing the model's ability to generalize across various character actions. Additionally, structured prompt templates with character tags allow for multi-character dialogues, enabling AI-generated characters to engage in coherent conversations, thus advancing the field of automated cinematic storytelling."}, 'zh': {'title': '会说话的角色：AI生成电影叙事的新标准', 'desc': '本论文介绍了一种新的视频生成任务，称为“会说话的角色”，旨在从语音和文本直接生成角色动画。与传统的“说话头”不同，这种方法生成的不仅仅是面部表情，而是完整的角色形象。我们提出了MoCha，这是首个能够生成会说话角色的模型，并引入了一种语音-视频窗口注意机制，以确保视频与语音的精确同步。此外，我们还设计了结构化的提示模板，使得多个角色能够进行基于回合的对话，从而实现更具电影感的情境对话。'}}}, {'id': 'https://huggingface.co/papers/2503.24235', 'title': 'What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.24235', 'abstract': "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.", 'score': 17, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '21674468fcc8c7d5', 'authors': ['Qiyuan Zhang', 'Fuyuan Lyu', 'Zexu Sun', 'Lei Wang', 'Weixu Zhang', 'Zhihan Guo', 'Yufei Wang', 'Irwin King', 'Xue Liu', 'Chen Ma'], 'affiliations': ['Chinese University of Hong Kong', 'City University of Hong Kong', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Macquarie University', 'McGill University & MILA', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24235.jpg', 'data': {'categories': ['#survey', '#math', '#reasoning', '#training'], 'emoji': '🔍', 'ru': {'title': 'Систематизация методов масштабирования языковых моделей во время тестирования', 'desc': 'Эта статья представляет собой обзор методов масштабирования во время тестирования (TTS) для больших языковых моделей (LLM). Авторы предлагают унифицированную структуру для классификации TTS-подходов по четырем измерениям: что масштабировать, как масштабировать, где масштабировать и насколько хорошо масштабировать. В работе анализируются различные методы, сценарии применения и аспекты оценки TTS. Статья также выделяет основные тенденции развития TTS и предлагает практические рекомендации по его применению.'}, 'en': {'title': 'Unlocking Potential: The Power of Test-Time Scaling in LLMs', 'desc': 'This paper discusses the emerging focus on test-time scaling (TTS) in machine learning, particularly for large language models (LLMs). TTS enhances the problem-solving abilities of LLMs in various tasks, including specialized reasoning and open-ended questions. The authors propose a comprehensive framework that categorizes TTS research into four dimensions: what, how, where, and how well to scale. They also review existing methods and applications, highlight challenges, and suggest future research directions to improve TTS effectiveness.'}, 'zh': {'title': '测试时扩展：激发大型语言模型的潜力', 'desc': '随着对预训练时代计算规模（数据和参数）的热情逐渐减退，测试时扩展（TTS）成为一个重要的研究焦点。最近的研究表明，TTS可以进一步激发大型语言模型（LLMs）的问题解决能力，在数学、编程等专业推理任务以及开放式问答等一般任务中取得显著突破。尽管这一领域的研究迅速增加，但仍迫切需要一项全面的调查，以提供系统的理解。为此，我们提出了一个统一的多维框架，涵盖TTS研究的四个核心维度，并对方法、应用场景和评估方面进行了广泛的回顾。'}}}, {'id': 'https://huggingface.co/papers/2503.24388', 'title': 'RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy', 'url': 'https://huggingface.co/papers/2503.24388', 'abstract': 'Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.', 'score': 16, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '98b80967d4757be2', 'authors': ['Zhonghan Zhao', 'Wenwei Zhang', 'Haian Huang', 'Kuikun Liu', 'Jianfei Gao', 'Gaoang Wang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24388.jpg', 'data': {'categories': ['#training', '#reasoning', '#agents', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Синергия рассуждения и воображения для создания универсальных ИИ-агентов', 'desc': 'Данная статья представляет новый подход к обучению агентов искусственного интеллекта, названный RIG (Reasoning and Imagination in Generalist policy). RIG объединяет способности рассуждения и воображения в единой комплексной модели, что позволяет значительно повысить эффективность обучения и способность к обобщению. В процессе вывода RIG сначала рассуждает о следующем действии, затем генерирует потенциальное действие и предсказывает его результаты, что дает агенту возможность пересмотреть и скорректировать свои действия перед их реальным выполнением. Экспериментальные результаты показывают, что синергия рассуждения и воображения улучшает устойчивость, обобщение и интероперабельность политики генералиста.'}, 'en': {'title': 'Synergizing Reasoning and Imagination for Enhanced Agent Performance', 'desc': 'This paper introduces RIG, a novel end-to-end Generalist policy that combines reasoning and imagination for embodied agents in complex environments. By integrating these two capabilities, RIG enhances learning efficiency and generalization, overcoming limitations of previous models that focused on only one aspect. The authors developed a data pipeline that enriches trajectories from existing agents, allowing for joint learning of reasoning and next image generation. Experimental results demonstrate that RIG significantly improves sample efficiency and robustness, enabling agents to self-correct actions based on imagined outcomes before execution.'}, 'zh': {'title': '推理与想象的协同提升智能体能力', 'desc': '本文提出了一种名为RIG的通用策略，首次将推理和想象能力结合在一个端到端的智能体中。通过构建数据管道，逐步整合和丰富从现有智能体收集的轨迹中的推理和想象内容，RIG实现了更高的学习效率和泛化能力。联合学习推理和下一图像生成，明确建模了推理、行动和环境动态之间的内在关联，使得样本效率提高了17倍以上。实验结果表明，推理与想象的协同作用不仅增强了通用策略的鲁棒性和互操作性，还提升了整体性能。'}}}, {'id': 'https://huggingface.co/papers/2503.23461', 'title': 'TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes', 'url': 'https://huggingface.co/papers/2503.23461', 'abstract': 'This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.', 'score': 14, 'issue_id': 2995, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '00cccb2000a01b76', 'authors': ['Nikai Du', 'Zhennan Chen', 'Zhizhou Chen', 'Shan Gao', 'Xi Chen', 'Zhengkai Jiang', 'Jian Yang', 'Ying Tai'], 'affiliations': ['China Mobile', 'Nanjing University', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.23461.jpg', 'data': {'categories': ['#cv', '#dataset', '#benchmark'], 'emoji': '📝', 'ru': {'title': 'TextCrafter: Прорыв в генерации сложного визуального текста', 'desc': 'Статья представляет новый метод TextCrafter для генерации сложного визуального текста в изображениях. Этот метод использует прогрессивную стратегию для декомпозиции сложного визуального текста на отдельные компоненты и обеспечивает надежное выравнивание между текстовым содержанием и его визуальным носителем. TextCrafter также включает механизм усиления фокуса токенов для повышения заметности визуального текста в процессе генерации. Авторы также представляют новый набор данных CVTG-2K для оценки производительности генеративных моделей в задачах CVTG.'}, 'en': {'title': 'TextCrafter: Mastering Complex Visual Text Generation', 'desc': 'This paper introduces Complex Visual Text Generation (CVTG), which involves creating detailed text that appears in various parts of images. The authors identify common issues in existing models, such as producing unclear or incomplete visual text. To improve this, they present TextCrafter, a new method that breaks down complex text into manageable parts and ensures that the text aligns well with the images. Additionally, TextCrafter enhances the visibility of the text during generation and is evaluated using a new dataset called CVTG-2K, showing superior performance compared to current methods.'}, 'zh': {'title': 'TextCrafter：提升复杂视觉文本生成的利器', 'desc': '本文探讨了复杂视觉文本生成（CVTG）任务，主要关注在视觉图像中生成分布在不同区域的复杂文本内容。在CVTG中，图像生成模型常常会渲染出扭曲、模糊的视觉文本或遗漏某些视觉文本。为了解决这些问题，我们提出了TextCrafter，这是一种新颖的多视觉文本渲染方法。TextCrafter通过逐步策略将复杂视觉文本分解为不同组件，同时确保文本内容与其视觉载体之间的强对齐。'}}}, {'id': 'https://huggingface.co/papers/2503.24370', 'title': 'Effectively Controlling Reasoning Models through Thinking Intervention', 'url': 'https://huggingface.co/papers/2503.24370', 'abstract': 'Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.', 'score': 9, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '5f218f08538c601f', 'authors': ['Tong Wu', 'Chong Xiang', 'Jiachen T. Wang', 'Prateek Mittal'], 'affiliations': ['NVIDIA', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24370.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#architecture', '#open_source', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Управление мышлением ИИ: новый путь к улучшению языковых моделей', 'desc': "Статья представляет новый подход к управлению языковыми моделями (LLM) под названием 'Thinking Intervention'. Этот метод позволяет вмешиваться в процесс рассуждений модели, стратегически вставляя или изменяя определенные токены мышления. Авторы провели обширные эксперименты на различных задачах, включая следование инструкциям и безопасность. Результаты показывают значительное улучшение производительности по сравнению с базовыми методами промптинга, открывая новые возможности для контроля над рассуждающими языковыми моделями."}, 'en': {'title': 'Enhancing LLM Reasoning with Thinking Intervention', 'desc': 'This paper introduces a new method called Thinking Intervention, which enhances the reasoning capabilities of large language models (LLMs) by allowing explicit control over their internal thought processes. By inserting or modifying specific reasoning tokens, the model can generate more accurate and contextually relevant answers. The authors conducted extensive tests on various tasks, showing that this approach leads to significant improvements in performance, such as higher accuracy in following instructions and better handling of unsafe prompts. Overall, the findings suggest that Thinking Intervention provides a valuable framework for refining how LLMs reason and respond to complex queries.'}, 'zh': {'title': '思维干预：提升大型语言模型推理能力的新方法', 'desc': '本文提出了一种新的思维干预（Thinking Intervention）方法，旨在通过插入或修改特定的思维标记来引导大型语言模型（LLMs）的内部推理过程。这种方法使得模型在复杂问题解决中能够更好地生成中间推理步骤，从而提高最终答案的准确性。我们在多个任务上进行了全面评估，结果显示思维干预显著优于传统的提示方法，尤其在指令遵循和推理层次方面取得了显著的准确率提升。我们的研究为控制推理过程中的大型语言模型开辟了新的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2503.24115', 'title': 'TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection', 'url': 'https://huggingface.co/papers/2503.24115', 'abstract': 'The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.', 'score': 7, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '61845428f5c3d9df', 'authors': ['Zhiming Ma', 'Peidong Wang', 'Minhua Huang', 'Jingpeng Wang', 'Kai Wu', 'Xiangzhao Lv', 'Yachun Pang', 'Yin Yang', 'Wenjie Tang', 'Yuchen Kang'], 'affiliations': ['China Mobile Internet Company Ltd. Guangzhou, Guangdong, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.24115.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#synthetic', '#open_source', '#benchmark', '#data'], 'emoji': '🎭', 'ru': {'title': 'Мультимодальный датасет для борьбы с телефонным мошенничеством', 'desc': 'Статья представляет TeleAntiFraud-28k - первый открытый аудио-текстовый датасет для анализа телекоммуникационного мошенничества. Датасет создан с использованием генерации образцов на основе ASR и TTS, семантического расширения с помощью LLM и многоагентного состязательного синтеза. Он содержит 28,511 пар речь-текст с аннотациями для рассуждений о мошенничестве и разделен на три задачи. Авторы также представляют TeleAntiFraud-Bench для оценки производительности моделей и открытую модель SFT.'}, 'en': {'title': 'Revolutionizing Telecom Fraud Detection with TeleAntiFraud-28k', 'desc': 'This paper introduces TeleAntiFraud-28k, a novel dataset designed to enhance telecom fraud detection by combining audio signals with textual analysis. The dataset is created using three innovative strategies, including privacy-preserved text generation from speech recordings and semantic enhancement through large language models. It consists of over 28,000 annotated speech-text pairs, enabling tasks like scenario classification and fraud detection. Additionally, the authors provide a benchmark for evaluating model performance and a fine-tuned model for practical applications, promoting further research in multimodal anti-fraud techniques.'}, 'zh': {'title': '构建电信欺诈检测的新基石', 'desc': '本论文提出了TeleAntiFraud-28k数据集，这是第一个专为电信欺诈分析设计的开源音频-文本慢思考数据集。该数据集通过三种策略构建，确保了数据的隐私保护和真实场景的一致性。我们还建立了TeleAntiFraud-Bench评估基准，以便系统地测试模型在电信欺诈检测任务上的表现。此项工作为多模态反欺诈研究奠定了基础，同时解决了数据隐私和场景多样性等关键挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.18809', 'title': 'Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code', 'url': 'https://huggingface.co/papers/2503.18809', 'abstract': 'In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit "reasoning" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs.', 'score': 6, 'issue_id': 2994, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '28288adc69a019ac', 'authors': ['Augusto B. Corrêa', 'André G. Pereira', 'Jendrik Seipp'], 'affiliations': ['Federal University of Rio Grande do Sul', 'Linköping University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.18809.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'LLM как генераторы эффективных эвристик для задач планирования', 'desc': 'Исследователи разработали метод использования больших языковых моделей (LLM) для генерации эвристических функций в задачах планирования. LLM создают несколько эвристик в виде Python-кода, которые затем оцениваются на тренировочных задачах. Выбранные эвристики показывают высокую эффективность на новых задачах, превосходя современные методы доменно-независимого планирования. Этот подход позволяет значительно улучшить способности LLM к планированию, даже для задач возрастающей сложности.'}, 'en': {'title': 'Empowering LLMs with Domain-Specific Heuristics for Better Planning', 'desc': 'This paper addresses the limitations of large language models (LLMs) in planning tasks, which often lead to incorrect and non-generalizable plans. The authors propose a novel approach where LLMs generate domain-specific heuristic functions in Python code, which are then evaluated using a greedy best-first search algorithm. Their method demonstrates that LLM-generated heuristics can outperform traditional domain-independent heuristics and compete with advanced learning algorithms in planning. The results indicate that these heuristics are not only efficient but also provide more informative guidance in certain planning domains.'}, 'zh': {'title': '提升大型语言模型的规划能力', 'desc': '近年来，大型语言模型（LLMs）在各种人工智能问题上展现了卓越的能力。然而，即使在详细定义规划任务的情况下，它们在规划方面仍然不可靠。本文展示了如何利用LLMs生成正确的规划，即使对于越来越大的分布外任务。通过生成领域相关的启发式函数并在贪婪优先搜索中评估，LLM生成的启发式函数在解决未见测试任务方面表现优于传统的领域无关启发式方法。'}}}, {'id': 'https://huggingface.co/papers/2503.24364', 'title': 'Query and Conquer: Execution-Guided SQL Generation', 'url': 'https://huggingface.co/papers/2503.24364', 'abstract': 'We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.', 'score': 3, 'issue_id': 2997, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '2af26722887e77ac', 'authors': ['Łukasz Borchmann', 'Marek Wydmuch'], 'affiliations': ['Snowflake AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24364.jpg', 'data': {'categories': ['#optimization', '#small_models', '#dataset', '#inference', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Революция в генерации SQL: эффективность через семантическую согласованность', 'desc': 'Предложен новый подход к генерации сложных выходных данных, значительно повышающий точность в задачах преобразования текста в SQL. Метод использует результаты выполнения для выбора наиболее семантически согласованного запроса из нескольких кандидатов. Это позволяет меньшим, экономически эффективным моделям превзойти вычислительно интенсивные методы рассуждений, такие как o1, o3-mini и DeepSeek R1, при этом снижая стоимость вывода до 30 раз. Подход легко интегрируется с существующими моделями, предлагая практичный и масштабируемый путь к современной генерации SQL.'}, 'en': {'title': 'Efficient SQL Generation: Small Models, Big Results!', 'desc': 'This paper introduces a new method for generating SQL queries from natural language, enhancing accuracy in text-to-SQL tasks. The approach utilizes execution results to choose the best query from several options based on semantic consistency. It allows smaller models to outperform larger, more complex models while also cutting down inference costs significantly. This method is designed to work well with current models, making it a practical solution for improving SQL generation.'}, 'zh': {'title': '高效生成SQL，提升文本到SQL的准确性', 'desc': '我们提出了一种新颖的方法，用于生成复杂输出，显著提高文本到SQL任务的准确性。该方法利用执行结果，从多个候选查询中选择最符合语义的一项，使得较小、成本效益高的模型能够超越计算密集型的推理方法，如o1、o3-mini和DeepSeek R1，同时将推理成本降低多达30倍。它与现有模型无缝集成，提供了一条实用且可扩展的通往最先进SQL生成的路径。'}}}, {'id': 'https://huggingface.co/papers/2503.23284', 'title': 'SketchVideo: Sketch-based Video Generation and Editing', 'url': 'https://huggingface.co/papers/2503.23284', 'abstract': "Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.", 'score': 3, 'issue_id': 2996, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'd968c1da27effa84', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#multimodal', '#optimization', '#games', '#video'], 'emoji': '✏️', 'ru': {'title': 'Точное управление видео через скетчи', 'desc': 'Эта статья представляет новый подход к генерации и редактированию видео на основе скетчей. Авторы предлагают эффективную структуру управления с блоками контроля скетчей, которая работает с моделью генерации видео DiT. Для распространения условий скетча на все кадры используется механизм межкадрового внимания. Также разработан модуль вставки видео для согласованного редактирования, а при инференсе применяется латентное слияние для сохранения неотредактированных областей.'}, 'en': {'title': 'Sketch Your Way to Better Video Control!', 'desc': 'This paper presents a novel approach to video generation and editing using sketch-based controls. It addresses the challenges of accurately managing global layout and motion details by introducing a memory-efficient control structure that utilizes sketch control blocks. The method allows users to draw sketches on keyframes, which are then analyzed through an inter-frame attention mechanism to ensure consistency across all video frames. The proposed system, SketchVideo, enhances the ability to generate and edit videos while preserving the integrity of unedited regions, demonstrating superior performance in controllable video tasks.'}, 'zh': {'title': '草图驱动的视频生成与编辑新方法', 'desc': '本论文探讨了基于草图的空间和运动控制在视频生成中的应用，旨在解决文本和图像条件下的布局和几何细节控制问题。我们提出了一种高效的控制结构，利用草图控制块预测跳过的DiT块的残差特征。通过在关键帧上绘制草图，并使用跨帧注意机制分析关键帧与每个视频帧之间的关系，实现了时间上稀疏的草图条件传播。实验结果表明，我们的SketchVideo在可控视频生成和编辑方面表现优越。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.23077', 'title': 'Efficient Inference for Large Reasoning Models: A Survey', 'url': 'https://huggingface.co/papers/2503.23077', 'abstract': "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant fieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.", 'score': 3, 'issue_id': 2995, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': 'f76d7ead3d85b37e', 'authors': ['Yue Liu', 'Jiaying Wu', 'Yufei He', 'Hongcheng Gao', 'Hongyu Chen', 'Baolong Bi', 'Jiaheng Zhang', 'Zhiqi Huang', 'Bryan Hooi'], 'affiliations': ['Beijing Jiaotong University', 'Moonshot', 'National University of Singapore', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.23077.jpg', 'data': {'categories': ['#reasoning', '#survey', '#interpretability', '#optimization', '#inference', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности вывода в моделях крупномасштабного рассуждения', 'desc': 'Данная статья представляет обзор методов эффективного вывода для моделей крупномасштабного рассуждения (LRM). Авторы классифицируют существующие подходы на две категории: явные компактные цепочки рассуждений и неявные латентные цепочки рассуждений. В работе проводится эмпирический анализ эффективности и производительности этих методов. Также обсуждаются открытые проблемы в этой области, включая контролируемое рассуждение, ориентированное на человека, баланс между интерпретируемостью и эффективностью, безопасность и более широкое применение эффективного рассуждения.'}, 'en': {'title': 'Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses Large Reasoning Models (LRMs) that enhance the reasoning capabilities of Large Language Models (LLMs) but face challenges like inefficient token usage and high memory consumption. It reviews various efficient inference methods aimed at reducing these inefficiencies while maintaining reasoning quality. The authors categorize these methods into explicit and implicit Chain-of-Thought approaches, analyzing their strengths and weaknesses. Additionally, the paper identifies ongoing challenges in the field and suggests strategies for improving inference efficiency in LRMs.'}, 'zh': {'title': '提升推理效率，优化大型推理模型', 'desc': '大型推理模型（LRMs）通过学习推理显著提高了大型语言模型（LLMs）的推理能力，能够在复杂任务中表现出色。然而，它们的深思熟虑的推理过程导致了令牌使用、内存消耗和推理时间的低效。因此，本文综述了专门为LRMs设计的高效推理方法，重点在于减轻令牌低效，同时保持推理质量。我们介绍了一种分类法，将最近的方法分为两大类：显式紧凑的思维链（CoT）和隐式潜在的思维链，讨论了它们的优缺点，并分析了现有方法的性能和效率。'}}}, {'id': 'https://huggingface.co/papers/2503.23730', 'title': 'KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language', 'url': 'https://huggingface.co/papers/2503.23730', 'abstract': 'The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA', 'score': 2, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '1d3d53298afe6cce', 'authors': ['Yoonshik Kim', 'Jaeyoon Jung'], 'affiliations': ['MAUM AI Inc. / Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.23730.jpg', 'data': {'categories': ['#open_source', '#low_resource', '#benchmark', '#multilingual'], 'emoji': '🇰🇷', 'ru': {'title': 'KOFFVQA: объективная оценка мультимодальных моделей на корейском языке', 'desc': 'Статья представляет новый бенчмарк KOFFVQA для оценки мультимодальных моделей (VLM) на корейском языке. Бенчмарк состоит из 275 вопросов с изображениями и критериями оценки, охватывающими 10 аспектов работы VLM. Авторы предлагают объективный метод оценки ответов моделей, основанный на предопределенных критериях. Эксперименты показывают, что этот подход более надежен, чем существующие методы оценки VLM.'}, 'en': {'title': 'KOFFVQA: Reliable Evaluation for Korean Vision-Language Models', 'desc': 'This paper introduces KOFFVQA, a new benchmark for evaluating Large Vision-Language Models (VLMs) specifically in the Korean language. It addresses the limitations of current evaluation methods that either restrict responses or rely on subjective judge models. KOFFVQA features 275 visual question-answering tasks with clear grading criteria that enhance reliability and objectivity in assessments. The authors demonstrate that their approach provides a more dependable evaluation of VLMs compared to existing methods, making it suitable for both large and small models.'}, 'zh': {'title': 'KOFFVQA：韩语视觉语言模型的可靠评估基准', 'desc': '本文介绍了一种新的视觉语言模型（VLM）评估基准，名为KOFFVQA，专门针对韩语。现有的评估方法往往依赖于预设的回答选项或评判模型，导致评估结果主观且不可靠。KOFFVQA包含275个精心设计的问题，每个问题都配有图像和涵盖VLM性能的10个评估标准。通过客观的评估标准，我们的方法能够更可靠地评估模型，即使是小型开源模型也能有效使用。'}}}, {'id': 'https://huggingface.co/papers/2503.24290', 'title': 'Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model', 'url': 'https://huggingface.co/papers/2503.24290', 'abstract': 'We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.', 'score': 1, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'c3cd649c5eb9d423', 'authors': ['Jingcheng Hu', 'Yinmin Zhang', 'Qi Han', 'Daxin Jiang', 'Xiangyu Zhang', 'Heung-Yeung Shum'], 'affiliations': ['StepFun', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24290.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#training', '#rl', '#open_source', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Простота и эффективность в обучении моделей рассуждения', 'desc': 'Open-Reasoner-Zero - это первая открытая реализация обучения с подкреплением для масштабного рассуждения, фокусирующаяся на масштабируемости, простоте и доступности. Используя минималистичный подход с vanilla PPO и GAE, без KL-регуляризации, удалось улучшить длину ответов и производительность на бенчмарках. Модель превзошла DeepSeek-R1-Zero на тестах AIME2024, MATH500 и GPQA Diamond, требуя при этом в 10 раз меньше шагов обучения. Авторы открыто публикуют код, параметры, данные для обучения и веса моделей разных размеров.'}, 'en': {'title': 'Simplifying Large-Scale Reasoning in RL with Open-Reasoner-Zero', 'desc': 'Open-Reasoner-Zero is an innovative open-source framework designed for large-scale reasoning-oriented reinforcement learning (RL) training. It utilizes a simple approach with vanilla Proximal Policy Optimization (PPO) and Generalized Advantage Estimation (GAE), achieving impressive results without complex regularization techniques. The framework demonstrates that minimalistic strategies can effectively enhance response length and benchmark performance, outperforming previous models like DeepSeek-R1-Zero. By releasing all components of the project, including source code and model weights, Open-Reasoner-Zero promotes accessibility and collaboration in the machine learning community.'}, 'zh': {'title': '开源推理强化学习的高效实现', 'desc': '我们介绍了Open-Reasoner-Zero，这是首个开源的大规模推理导向强化学习训练实现，重点关注可扩展性、简洁性和可访问性。通过大量实验，我们证明了使用简单的PPO算法和基于规则的奖励机制，能够有效提升响应长度和基准性能。我们的实现与DeepSeek-R1-Zero使用相同的基础模型，在AIME2024、MATH500和GPQA Diamond基准上表现优异，同时训练效率显著提高，仅需DeepSeek-R1-Zero管道的十分之一训练步骤。为了支持开源精神，我们发布了源代码、参数设置、训练数据和不同规模的模型权重。'}}}, {'id': 'https://huggingface.co/papers/2503.20286', 'title': 'Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization', 'url': 'https://huggingface.co/papers/2503.20286', 'abstract': 'Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.', 'score': 1, 'issue_id': 2994, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'bf1debfaa462fca8', 'authors': ['Zhenyu Liang', 'Hao Li', 'Naiwei Yu', 'Kebin Sun', 'Ran Cheng'], 'affiliations': ['Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Data Science and Artificial Intelligence and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.20286.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#robotics', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Тензоризация EMO: революция в скорости многоцелевой оптимизации', 'desc': 'Статья представляет новый подход к эволюционной многоцелевой оптимизации (EMO) с использованием тензоризации для ускорения алгоритмов на GPU. Авторы применили эту методологию к трем известным алгоритмам EMO: NSGA-III, MOEA/D и HypE. Эксперименты показали ускорение до 1113 раз по сравнению с версиями для CPU при сохранении качества решений. Также был представлен новый бенчмарк многоцелевого управления роботом с использованием физического движка на GPU для оценки эффективности предложенного подхода.'}, 'en': {'title': 'Accelerating EMO with GPU Tensorization for Enhanced Performance', 'desc': 'This paper addresses the limitations of traditional evolutionary multiobjective optimization (EMO) algorithms when faced with complex and large-scale problems. It highlights the lack of hardware acceleration in existing EMO approaches and proposes a novel method to parallelize these algorithms using GPUs through tensorization. By transforming EMO data structures into tensor representations, the authors enable efficient GPU computing, resulting in significant performance improvements. The proposed tensorized EMO algorithms demonstrate remarkable speedups while maintaining solution quality in multiobjective robot control tasks.'}, 'zh': {'title': '张量化提升EMO算法性能，GPU加速显著', 'desc': '进化多目标优化（EMO）在过去二十年取得了显著进展，但随着问题规模和复杂性的增加，传统的EMO算法面临性能限制。本文提出了一种通过张量化方法在GPU上并行化EMO算法的方案，以解决传统算法的并行性和可扩展性不足的问题。通过张量化，EMO算法的数据结构和操作被转化为简洁的张量表示，从而实现了GPU计算的自动利用。实验结果表明，张量化的EMO算法在速度上比基于CPU的算法快了多达1113倍，同时保持了解决方案的质量，并有效处理复杂的多目标机器人控制任务。'}}}, {'id': 'https://huggingface.co/papers/2503.23829', 'title': 'Expanding RL with Verifiable Rewards Across Diverse Domains', 'url': 'https://huggingface.co/papers/2503.23829', 'abstract': "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.", 'score': 0, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '2c875f8335892dfd', 'authors': ['Yi Su', 'Dian Yu', 'Linfeng Song', 'Juntao Li', 'Haitao Mi', 'Zhaopeng Tu', 'Min Zhang', 'Dong Yu'], 'affiliations': ['Soochow University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.23829.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rlhf', '#training', '#rl', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'RLVR: Универсальное обучение с подкреплением для различных областей знаний', 'desc': 'Исследование посвящено расширению применения обучения с подкреплением с проверяемыми вознаграждениями (RLVR) на различные области, такие как медицина, химия, психология и экономика. Авторы обнаружили высокое согласие в бинарных оценках между различными большими языковыми моделями при наличии объективных эталонных ответов. Для улучшения гибкости метода при работе с неструктурированными эталонными ответами было предложено использование мягкого оценивания на основе моделей. Эксперименты показали, что дистиллированная генеративная модель вознаграждения может служить эффективным междоменным верификатором, обеспечивая надежные сигналы вознаграждения для обучения с подкреплением без необходимости в аннотациях для конкретных доменов.'}, 'en': {'title': 'Expanding RLVR: From Coding to Real-World Applications', 'desc': "This paper explores the use of Reinforcement Learning with Verifiable Rewards (RLVR) in diverse fields like medicine and economics, where traditional structured answers may not be available. The authors find that different large language models (LLMs) often agree on binary judgments when objective reference answers are present, suggesting that extensive annotations may not be necessary for training reward models. To enhance RLVR's effectiveness, they introduce model-based soft scoring to better handle unstructured reference answers. Their experiments demonstrate that a distilled generative reward model can effectively verify rewards across domains, leading to improved performance of RL policies compared to existing state-of-the-art models."}, 'zh': {'title': '强化学习的可验证奖励：跨领域应用的新可能', 'desc': '强化学习与可验证奖励（RLVR）在数学推理和编码任务中表现出色，但其在更广泛领域的应用尚未深入探索。我们研究了RLVR在医学、化学、心理学和经济学等多样化领域的扩展。研究发现，当存在客观参考答案时，不同的大型语言模型（LLMs）在二元判断上高度一致，这表明训练特定领域奖励模型不一定需要大规模标注。通过将基于模型的软评分纳入RLVR，我们提高了其灵活性，并通过实验验证了蒸馏生成奖励模型在跨领域验证中的有效性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (6)', '#cv (1)', '#data (1)', '#dataset (5)', '#diffusion', '#ethics', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (3)', '#open_source (5)', '#optimization (6)', '#plp', '#rag', '#reasoning (8)', '#rl (4)', '#rlhf (3)', '#robotics (1)', '#science', '#security', '#small_models (1)', '#story_generation (1)', '#survey (2)', '#synthetic (1)', '#training (6)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-01 07:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-01 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-01 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    