
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 112 papers. April 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">ĞĞ¿Ñ€ĞµĞ»ÑŒ 2025</span> | <span id="title-articles-count">112 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-03.html">â¬…ï¸ <span id="prev-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-05.html">â¡ï¸ <span id="next-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'ĞĞ¿Ñ€ĞµĞ»ÑŒ 2025', 'en': 'April 2025', 'zh': '4æœˆ2025å¹´'};
        let feedDateNext = {'ru': '05.2025', 'en': '05/2025', 'zh': '5æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '03.2025', 'en': '03/2025', 'zh': '3æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.24379', 'title': 'Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.24379', 'abstract': 'To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/', 'score': 43, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'dce65db5da1b8c34', 'authors': ['Shengqiong Wu', 'Weicai Ye', 'Jiahao Wang', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Shuicheng Yan', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['Kuaishou Technology', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24379.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ»ÑĞ±Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Any2Caption - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Any2CapIns Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼.'}, 'en': {'title': 'Revolutionizing Video Generation with Any2Caption', 'desc': "Any2Caption is a new framework designed to improve how user intentions are understood in video generation. It separates the process of interpreting different conditions from the actual video creation, allowing for more precise control. By using advanced multimodal large language models, it can transform various inputs like text and images into detailed captions that guide video generators more effectively. The introduction of the Any2CapIns dataset, which contains a large number of examples, further enhances the system's ability to generate high-quality videos based on diverse conditions."}, 'zh': {'title': 'å¯æ§è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šAny2Caption', 'desc': 'ä¸ºäº†å…‹æœå½“å‰è§†é¢‘ç”Ÿæˆé¢†åŸŸä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†Any2Captionï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¯æ§è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£é‡Šæ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤è§£è€¦ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘åŠç‰¹å®šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œç›¸æœºå§¿æ€ï¼‰è½¬åŒ–ä¸ºå¯†é›†çš„ç»“æ„åŒ–å­—å¹•ï¼Œä»è€Œä¸ºè§†é¢‘ç”Ÿæˆå™¨æä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†Any2CapInsï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«337Kå®ä¾‹å’Œ407Kæ¡ä»¶çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºä»»ä½•æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00050', 'title': 'JudgeLRM: Large Reasoning Models as a Judge', 'url': 'https://huggingface.co/papers/2504.00050', 'abstract': 'The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.', 'score': 29, 'issue_id': 3022, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '5060d3d364f635eb', 'authors': ['Nuo Chen', 'Zhiyuan Hu', 'Qingyun Zou', 'Jiaying Wu', 'Qian Wang', 'Bryan Hooi', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.00050.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#rl'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜-ÑÑƒĞ´ĞµĞ¹: Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ JudgeLRM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑÑƒĞ´ĞµĞ¹. JudgeLRM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ JudgeLRM-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 2.79% Ğ¿Ğ¾ F1-Ğ¼ĞµÑ€Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ DeepSeek-R1.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Complex Judging Tasks', 'desc': 'This paper explores the effectiveness of Large Language Models (LLMs) as judges in evaluation tasks, particularly in scenarios that require complex reasoning. It highlights that traditional Supervised Fine-Tuning (SFT) methods do not perform well when faced with tasks that demand higher reasoning skills. The authors introduce JudgeLRM, a new approach that utilizes reinforcement learning (RL) to enhance the reasoning capabilities of LLMs by providing judge-specific rewards. The results show that JudgeLRM models significantly outperform both SFT-tuned models and other leading reasoning models, demonstrating superior performance in tasks that require deep reasoning.'}, 'zh': {'title': 'JudgeLRMï¼šæ·±åº¦æ¨ç†çš„è¯„åˆ¤è€…', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè¯„ä¼°è€…çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨å¤„ç†éœ€è¦æ·±åº¦æ¨ç†çš„æ ·æœ¬æ—¶æ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†JudgeLRMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„è¯„åˆ¤å¯¼å‘LLMï¼Œèƒ½å¤Ÿæä¾›æ›´æœ‰æ•ˆçš„è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJudgeLRMæ¨¡å‹åœ¨è¯„åˆ¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„SFTæ¨¡å‹å’Œæœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼Œå°¤å…¶åœ¨éœ€è¦æ·±åº¦æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23145', 'title': 'CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis', 'url': 'https://huggingface.co/papers/2503.23145', 'abstract': 'Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.', 'score': 28, 'issue_id': 3021, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '945cc4b51522e668', 'authors': ['Anjiang Wei', 'Tarun Suresh', 'Jiannan Cao', 'Naveen Kannan', 'Yuheng Wu', 'Kai Yan', 'Thiago S. F. X. Teixeira', 'Ke Wang', 'Alex Aiken'], 'affiliations': ['Intel', 'MIT', 'Stanford University', 'University of Illinois Urbana-Champaign', 'Visa Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23145.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#plp', '#agents', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CodeARC: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CodeARC - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², CodeARC Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹, Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 1114 Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 52.7% ÑƒÑĞ¿ĞµÑ…Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'CodeARC: A New Frontier in Inductive Program Synthesis Evaluation', 'desc': 'This paper introduces CodeARC, a new evaluation framework for inductive program synthesis, which is the process of creating functions based on input-output examples. Unlike traditional methods that use static examples, CodeARC allows agents to interact with a hidden target function, enabling them to refine their solutions through feedback. The framework includes a large-scale benchmark with 1114 functions, demonstrating the challenges faced by models in this domain. The results show that fine-tuning models like LLaMA-3.1-8B-Instruct can significantly improve performance, emphasizing the need for dynamic evaluation in program synthesis tasks.'}, 'zh': {'title': 'CodeARCï¼šæå‡ç¨‹åºåˆæˆçš„è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'å½’çº³ç¨‹åºåˆæˆï¼Œä¹Ÿç§°ä¸ºç¤ºä¾‹ç¼–ç¨‹ï¼Œæ˜¯ä»è¾“å…¥è¾“å‡ºç¤ºä¾‹ä¸­åˆæˆå‡½æ•°çš„è¿‡ç¨‹ï¼Œè¦æ±‚èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„è¾“å…¥ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼çš„ç¼–ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å½’çº³ç¨‹åºåˆæˆæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„è¯„ä¼°åè®®ä¾èµ–äºé™æ€ç¤ºä¾‹é›†å’Œä¿ç•™æµ‹è¯•ï¼Œæ— æ³•åœ¨åˆæˆå‡½æ•°é”™è¯¯æ—¶æä¾›åé¦ˆï¼Œä¹Ÿæœªèƒ½åæ˜ ç°å®ä¸–ç•Œä¸­çš„åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†CodeARCï¼Œä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå…è®¸ä»£ç†é€šè¿‡æŸ¥è¯¢éšè—çš„ç›®æ ‡å‡½æ•°ä¸ä¹‹äº’åŠ¨ï¼Œä»è€Œåˆæˆå€™é€‰å‡½æ•°å¹¶æ ¹æ®åé¦ˆè¿­ä»£æ”¹è¿›è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24376', 'title': 'Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1', 'url': 'https://huggingface.co/papers/2503.24376', 'abstract': "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.", 'score': 24, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'd22966d0969ded43', 'authors': ['Yi Chen', 'Yuying Ge', 'Rui Wang', 'Yixiao Ge', 'Lu Qiu', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.24376.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#rl', '#benchmark', '#optimization', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SEED-Bench-R1 Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2-VL-Instruct-7B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ½Ğµ ĞµĞ³Ğ¾. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ½Ğ¾ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµĞ½ĞµĞµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Reasoning in Multimodal Models with SEED-Bench-R1', 'desc': "This paper discusses the advancements in Chain of Thought (COT) generation for Large Language Models (LLMs) and introduces a new benchmark called SEED-Bench-R1 for evaluating Multimodal Large Language Models (MLLMs) in video understanding tasks. The benchmark includes complex real-world videos and planning tasks presented as multiple-choice questions, assessing the models' perception and reasoning abilities. The study compares reinforcement learning (RL) with supervised fine-tuning (SFT) using the Qwen2-VL-Instruct-7B model, showing that RL is more data-efficient and performs better on various tasks. However, the analysis also highlights limitations in reasoning consistency and visual cue recognition, suggesting areas for future research to enhance model performance."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'æœ€è¿‘ï¼Œé“¾å¼æ€ç»´ï¼ˆCOTï¼‰ç”Ÿæˆçš„è¿›å±•æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆä¸ºä¸€ç§æœ‰æ•ˆçš„åè®­ç»ƒæ–¹æ³•ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»§æ‰¿äº†è¿™ç§æ¨ç†æ½œåŠ›ï¼Œä½†åœ¨éœ€è¦æ„ŸçŸ¥å’Œé€»è¾‘æ¨ç†çš„ä»»åŠ¡ä¸­ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SEED-Bench-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°MLLMsåœ¨è§†é¢‘ç†è§£ä¸­åè®­ç»ƒæ–¹æ³•çš„åŸºå‡†ï¼ŒåŒ…å«å¤æ‚çš„çœŸå®è§†é¢‘å’Œå¤šé¡¹é€‰æ‹©é¢˜çš„æ—¥å¸¸è§„åˆ’ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒRLåœ¨æ•°æ®æ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†åœ¨é€»è¾‘è¿è´¯æ€§æ–¹é¢å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00698', 'title': 'Command A: An Enterprise-Ready Large Language Model', 'url': 'https://huggingface.co/papers/2504.00698', 'abstract': 'In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.', 'score': 16, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '8670e6d1cc4f6bee', 'authors': ['Team Cohere', 'Aakanksha', 'Arash Ahmadian', 'Marwan Ahmed', 'Jay Alammar', 'Yazeed Alnumay', 'Sophia Althammer', 'Arkady Arkhangorodsky', 'Viraat Aryabumi', 'Dennis Aumiller', 'RaphaÃ«l Avalos', 'Zahara Aviv', 'Sammie Bae', 'Saurabh Baji', 'Alexandre Barbet', 'Max Bartolo', 'BjÃ¶rn Bebensee', 'Neeral Beladia', 'Walter Beller-Morales', 'Alexandre BÃ©rard', 'Andrew Berneshawi', 'Anna Bialas', 'Phil Blunsom', 'Matt Bobkin', 'Adi Bongale', 'Sam Braun', 'Maxime Brunet', 'Samuel Cahyawijaya', 'David Cairuz', 'Jon Ander Campos', 'Cassie Cao', 'Kris Cao', 'Roman CastagnÃ©', 'JuliÃ¡n Cendrero', 'Leila Chan Currie', 'Yash Chandak', 'Diane Chang', 'Giannis Chatziveroglou', 'Hongyu Chen', 'Claire Cheng', 'Alexis Chevalier', 'Justin T. Chiu', 'Eugene Cho', 'Eugene Choi', 'Eujeong Choi', 'Tim Chung', 'Volkan Cirik', 'Ana Cismaru', 'Pierre Clavier', 'Henry Conklin', 'Lucas Crawhall-Stein', 'Devon Crouse', 'Andres Felipe Cruz-Salinas', 'Ben Cyrus', "Daniel D'souza", 'Hugo Dalla-Torre', 'John Dang', 'William Darling', 'Omar Darwiche Domingues', 'Saurabh Dash', 'Antoine Debugne', 'ThÃ©o Dehaze', 'Shaan Desai', 'Joan Devassy', 'Rishit Dholakia', 'Kyle Duffy', 'Ali Edalati', 'Ace Eldeib', 'Abdullah Elkady', 'Sarah Elsharkawy', 'Irem ErgÃ¼n', 'Beyza Ermis', 'Marzieh Fadaee', 'Boyu Fan', 'Lucas Fayoux', 'Yannis Flet-Berliac', 'Nick Frosst', 'Matthias GallÃ©', 'Wojciech Galuba', 'Utsav Garg', 'Matthieu Geist', 'Mohammad Gheshlaghi Azar', 'Seraphina Goldfarb-Tarrant', 'Tomas Goldsack', 'Aidan Gomez', 'Victor Machado Gonzaga', 'Nithya Govindarajan', 'Manoj Govindassamy', 'Nathan Grinsztajn', 'Nikolas Gritsch', 'Patrick Gu', 'Shangmin Guo', 'Kilian Haefeli', 'Rod Hajjar', 'Tim Hawes', 'Jingyi He', 'Sebastian HofstÃ¤tter', 'Sungjin Hong', 'Sara Hooker', 'Tom Hosking', 'Stephanie Howe', 'Eric Hu', 'Renjie Huang', 'Hemant Jain', 'Ritika Jain', 'Nick Jakobi', 'Madeline Jenkins', 'JJ Jordan', 'Dhruti Joshi', 'Jason Jung', 'Trushant Kalyanpur', 'Siddhartha Rao Kamalakara', 'Julia Kedrzycki', 'Gokce Keskin', 'Edward Kim', 'Joon Kim', 'Wei-Yin Ko', 'Tom Kocmi', 'Michael Kozakov', 'Wojciech KryÅ›ciÅ„ski', 'Arnav Kumar Jain', 'Komal Kumar Teru', 'Sander Land', 'Michael Lasby', 'Olivia Lasche', 'Justin Lee', 'Patrick Lewis', 'Jeffrey Li', 'Jonathan Li', 'Hangyu Lin', 'Acyr Locatelli', 'Kevin Luong', 'Raymond Ma', 'Lukas Mach', 'Marina Machado', 'Joanne Magbitang', 'Brenda Malacara Lopez', 'Aryan Mann', 'Kelly Marchisio', 'Olivia Markham', 'Alexandre Matton', 'Alex McKinney', 'Dominic McLoughlin', 'Jozef Mokry', 'Adrien Morisot', 'Autumn Moulder', 'Harry Moynehan', 'Maximilian Mozes', 'Vivek Muppalla', 'Lidiya Murakhovska', 'Hemangani Nagarajan', 'Alekhya Nandula', 'Hisham Nasir', 'Shauna Nehra', 'Josh Netto-Rosen', 'Daniel Ohashi', 'James Owers-Bardsley', 'Jason Ozuzu', 'Dennis Padilla', 'Gloria Park', 'Sam Passaglia', 'Jeremy Pekmez', 'Laura Penstone', 'Aleksandra Piktus', 'Case Ploeg', 'Andrew Poulton', 'Youran Qi', 'Shubha Raghvendra', 'Miguel Ramos', 'Ekagra Ranjan', 'Pierre Richemond', 'CÃ©cile Robert-Michon', 'AurÃ©lien Rodriguez', 'Sudip Roy', 'Laura Ruis', 'Louise Rust', 'Anubhav Sachan', 'Alejandro Salamanca', 'Kailash Karthik Saravanakumar', 'Isha Satyakam', 'Alice Schoenauer Sebag', 'Priyanka Sen', 'Sholeh Sepehri', 'Preethi Seshadri', 'Ye Shen', 'Tom Sherborne', 'Sylvie Chang Shi', 'Sanal Shivaprasad', 'Vladyslav Shmyhlo', 'Anirudh Shrinivason', 'Inna Shteinbuk', 'Amir Shukayev', 'Mathieu Simard', 'Ella Snyder', 'Ava Spataru', 'Victoria Spooner', 'Trisha Starostina', 'Florian Strub', 'Yixuan Su', 'Jimin Sun', 'Dwarak Talupuru', 'Eugene Tarassov', 'Elena Tommasone', 'Jennifer Tracey', 'Billy Trend', 'Evren Tumer', 'Ahmet ÃœstÃ¼n', 'Bharat Venkitesh', 'David Venuto', 'Pat Verga', 'Maxime Voisin', 'Alex Wang', 'Donglu Wang', 'Shijian Wang', 'Edmond Wen', 'Naomi White', 'Jesse Willman', 'Marysia Winkels', 'Chen Xia', 'Jessica Xie', 'Minjie Xu', 'Bowen Yang', 'Tan Yi-Chern', 'Ivan Zhang', 'Zhenyu Zhao', 'Zhoujie Zhao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.00698.jpg', 'data': {'categories': ['#training', '#low_resource', '#agents', '#open_source', '#rag', '#multilingual', '#architecture', '#benchmark'], 'emoji': 'ğŸš€', 'ru': {'title': 'Command A: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Command A - Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 23 ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Command A Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Retrieval Augmented Generation (RAG) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². Ğ­Ñ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Empowering Enterprises with Command A: The Future of Language Models', 'desc': 'This paper presents Command A, a large language model designed specifically for enterprise applications. It features a hybrid architecture that optimizes both performance and efficiency, supporting 23 languages to cater to global business needs. The model excels in Retrieval Augmented Generation (RAG), enabling it to automate complex business processes through effective grounding and tool usage. The training process incorporates decentralized methods, including self-refinement and model merging, and the paper also discusses the similar Command R7B model, providing insights into their training and evaluation results.'}, 'zh': {'title': 'Command Aï¼šä¼ä¸šåº”ç”¨çš„å¼ºå¤§è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Command Açš„å¼€å‘ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºä¼ä¸šå®é™…åº”ç”¨è€Œè®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚Command Aå…·å¤‡å¤šè¯­è¨€èƒ½åŠ›ï¼Œæ”¯æŒ23ç§å…¨çƒå•†ä¸šè¯­è¨€ï¼Œå¹¶é‡‡ç”¨æ–°é¢–çš„æ··åˆæ¶æ„ï¼Œå…¼é¡¾æ•ˆç‡ä¸é«˜æ€§èƒ½ã€‚å®ƒæä¾›äº†æœ€ä½³çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡å·¥å…·ä½¿ç”¨å’ŒåŸºç¡€çŸ¥è¯†æ”¯æŒæ¥è‡ªåŠ¨åŒ–å¤æ‚çš„ä¸šåŠ¡æµç¨‹ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸Command Aç›¸ä¼¼çš„Command R7Bæ¨¡å‹çš„ç»“æœï¼Œå¹¶å‘å¸ƒäº†è¿™ä¸¤ä¸ªæ¨¡å‹çš„æƒé‡ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01016', 'title': 'GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors', 'url': 'https://huggingface.co/papers/2504.01016', 'abstract': 'Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.', 'score': 15, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '9430b45c3324fb61', 'authors': ['Tian-Xing Xu', 'Xiangjun Gao', 'Wenbo Hu', 'Xiaoyu Li', 'Song-Hai Zhang', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'HKUST', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01016.jpg', 'data': {'categories': ['#video', '#3d', '#architecture', '#diffusion', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'GeometryCrafter: Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'GeometryCrafter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ 3D/4D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GeometryCrafter Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 3D, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'GeometryCrafter: Elevating Video Depth Estimation with High-Fidelity Point Maps', 'desc': 'This paper introduces GeometryCrafter, a new framework designed to improve video depth estimation by producing high-fidelity point map sequences that maintain temporal coherence. It addresses the limitations of existing methods in achieving accurate geometric representations, which are crucial for tasks like 3D reconstruction and camera parameter estimation. The framework utilizes a point map Variational Autoencoder (VAE) to effectively encode and decode point maps, independent of the video latent distributions. By training a video diffusion model on these point map sequences, GeometryCrafter demonstrates superior performance in 3D accuracy and generalization across various datasets.'}, 'zh': {'title': 'GeometryCrafterï¼šé«˜ä¿çœŸè§†é¢‘æ·±åº¦ä¼°è®¡çš„æ–°æ¡†æ¶', 'desc': 'å°½ç®¡è§†é¢‘æ·±åº¦ä¼°è®¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å‡ ä½•ä¿çœŸåº¦æ–¹é¢å­˜åœ¨å›ºæœ‰å±€é™ï¼Œé™åˆ¶äº†å…¶åœ¨é‡å»ºå’Œå…¶ä»–åº¦é‡åŸºç¡€ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†GeometryCrafterï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å¼€æ”¾ä¸–ç•Œè§†é¢‘ä¸­æ¢å¤å…·æœ‰æ—¶é—´ä¸€è‡´æ€§çš„é«˜ä¿çœŸç‚¹å›¾åºåˆ—ï¼Œä»è€Œå®ç°å‡†ç¡®çš„3D/4Dé‡å»ºå’Œç›¸æœºå‚æ•°ä¼°è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¸å¿ƒæ˜¯ä¸€ä¸ªç‚¹å›¾å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå®ƒå­¦ä¹ ä¸€ä¸ªä¸è§†é¢‘æ½œåœ¨åˆ†å¸ƒæ— å…³çš„æ½œåœ¨ç©ºé—´ï¼Œä»¥æœ‰æ•ˆåœ°è¿›è¡Œç‚¹å›¾ç¼–ç å’Œè§£ç ã€‚é€šè¿‡åˆ©ç”¨VAEï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥å»ºæ¨¡åŸºäºè¾“å…¥è§†é¢‘çš„ç‚¹å›¾åºåˆ—çš„åˆ†å¸ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00810', 'title': 'Z1: Efficient Test-time Scaling with Code', 'url': 'https://huggingface.co/papers/2504.00810', 'abstract': 'Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.', 'score': 15, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'd982593a14ba7da9', 'authors': ['Zhaojian Yu', 'Yinghao Wu', 'Yilun Zhao', 'Arman Cohan', 'Xiao-Ping Zhang'], 'affiliations': ['Tsinghua University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00810.jpg', 'data': {'categories': ['#reasoning', '#rl', '#dataset', '#training', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Z1-Code-Reasoning-107K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Shifted Thinking Window Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Z1-7B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Efficient Reasoning in Large Language Models', 'desc': 'This paper introduces a new method for improving the efficiency of Large Language Models (LLMs) during problem-solving by reducing unnecessary reasoning tokens. The authors created a dataset called Z1-Code-Reasoning-107K, which includes various coding problems and their solution paths. They also developed a technique called the Shifted Thinking Window, which helps the model focus on relevant information and limits excessive reasoning. The resulting model, Z1-7B, shows strong performance on complex tasks while using significantly fewer reasoning tokens compared to other models.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œç®€åŒ–æ€è€ƒï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç›¸å…³çš„æ¨ç†è½¨è¿¹ä¸Šï¼Œå‡å°‘å¤šä½™çš„æ€è€ƒæ ‡è®°ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºZ1-Code-Reasoning-107Kçš„æ•°æ®é›†ï¼ŒåŒ…å«ç®€å•å’Œå¤æ‚çš„ç¼–ç é—®é¢˜åŠå…¶è§£å†³è½¨è¿¹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç§»ä½æ€ç»´çª—å£ï¼Œé€šè¿‡å»é™¤ä¸Šä¸‹æ–‡åˆ†éš”æ ‡ç­¾å’Œé™åˆ¶æ¨ç†æ ‡è®°ï¼Œæ¥å‡è½»è¿‡åº¦æ€è€ƒçš„è´Ÿæ‹…ã€‚ç»è¿‡è®­ç»ƒçš„æ¨¡å‹Z1-7Bèƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„å¤æ‚æ€§è°ƒæ•´æ¨ç†æ°´å¹³ï¼Œå¹¶åœ¨ä¸åŒçš„æ¨ç†ä»»åŠ¡ä¸­å®ç°é«˜æ•ˆçš„æµ‹è¯•æ—¶é—´æ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00595', 'title': 'Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources', 'url': 'https://huggingface.co/papers/2504.00595', 'abstract': 'The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.', 'score': 15, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '3e8c667bd93d754e', 'authors': ['Weizhi Wang', 'Yu Tian', 'Linjie Yang', 'Heng Wang', 'Xifeng Yan'], 'affiliations': ['Nvidia Research', 'Seed Vision Team, ByteDance', 'UC Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2504.00595.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#training', '#data', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Open-Qwen2VL - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 29 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 442 Ñ‡Ğ°ÑĞ° GPU A100-40G. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºÑƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Open-Qwen2VL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2-VL-2B Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Efficiency in Multimodal LLMs with Open-Qwen2VL', 'desc': 'The paper presents Open-Qwen2VL, a multimodal large language model (LLM) that is fully open-source and pre-trained on 29 million image-text pairs. It addresses challenges in multimodal LLM pre-training by utilizing advanced data filtering techniques and efficient training strategies, achieving significant improvements in training efficiency. The model is trained using a dynamic image resolution approach and multimodal sequence packing, which enhances the overall performance while reducing resource consumption. Open-Qwen2VL outperforms existing models on various benchmarks, showcasing its effectiveness and the benefits of open-source collaboration in machine learning.'}, 'zh': {'title': 'é«˜æ•ˆå¼€æºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Open-Qwen2VLï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰20äº¿å‚æ•°ï¼Œä½¿ç”¨2900ä¸‡å¯¹å›¾åƒ-æ–‡æœ¬æ•°æ®è¿›è¡Œé«˜æ•ˆé¢„è®­ç»ƒã€‚æˆ‘ä»¬é‡‡ç”¨äº†åŠ¨æ€å›¾åƒåˆ†è¾¨ç‡å’Œå¤šæ¨¡æ€åºåˆ—æ‰“åŒ…æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†é¢„è®­ç»ƒçš„æ•ˆç‡ã€‚é€šè¿‡ä½¿ç”¨MLLMå’ŒCLIPçš„è¿‡æ»¤æŠ€æœ¯ï¼Œæå‡äº†æ•°æ®è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚æœ€ç»ˆï¼ŒOpen-Qwen2VLåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†éƒ¨åˆ†å¼€æºçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„è®­ç»ƒæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01019', 'title': 'MixerMDM: Learnable Composition of Human Motion Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01019', 'abstract': 'Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.', 'score': 13, 'issue_id': 3022, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '745108d1df40d1b4', 'authors': ['Pablo Ruiz-Ponce', 'German Barquero', 'Cristina Palmero', 'Sergio Escalera', 'JosÃ© GarcÃ­a-RodrÃ­guez'], 'affiliations': ['Kings College London, UK', 'Universidad de Alicante, Spain', 'Universitat de Barcelona and Computer Vision Center, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2504.01019.jpg', 'data': {'categories': ['#multimodal', '#cv', '#diffusion', '#benchmark', '#dataset'], 'emoji': 'ğŸ•º', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MixerMDM - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², MixerMDM Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ MixerMDM Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ´ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Dynamic Control of Human Motion Generation with MixerMDM', 'desc': 'This paper addresses the challenge of generating human motion based on textual descriptions by introducing MixerMDM, a novel learnable model composition technique. Unlike previous methods, MixerMDM dynamically combines pre-trained motion diffusion models in an adversarial manner, allowing for better control over the generated motions based on specific conditions. The approach enables fine-grained control over individual and overall interactions in multi-person scenarios. Additionally, the authors propose a new evaluation method to assess the quality of generated motions in relation to their conditions, highlighting the adaptability of MixerMDM during the denoising process.'}, 'zh': {'title': 'åŠ¨æ€æ··åˆï¼Œç²¾ç»†æ§åˆ¶äººç±»è¿åŠ¨ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ç»„åˆæŠ€æœ¯ï¼Œç§°ä¸ºMixerMDMï¼Œç”¨äºç»“åˆé¢„è®­ç»ƒçš„æ–‡æœ¬æ¡ä»¶äººç±»è¿åŠ¨æ‰©æ•£æ¨¡å‹ã€‚ä¸ä»¥å¾€çš„æ–¹æ³•ä¸åŒï¼ŒMixerMDMé‡‡ç”¨åŠ¨æ€æ··åˆç­–ç•¥ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒå­¦ä¹ å¦‚ä½•æ ¹æ®ç”Ÿæˆæ¡ä»¶ç»„åˆå»å™ªè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å¯¹å•äººå’Œå¤šäººè¿åŠ¨çš„ç²¾ç»†æ§åˆ¶ï¼Œæå‡äº†æ¯ä¸ªäººçš„åŠ¨æ€è¡¨ç°åŠæ•´ä½“äº’åŠ¨æ•ˆæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŠ€æœ¯ï¼Œé¦–æ¬¡é‡åŒ–äº†ç”Ÿæˆè¿åŠ¨ä¸æ¡ä»¶ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼Œä»¥åŠMixerMDMåœ¨å»å™ªè¿‡ç¨‹ä¸­é€‚åº”æ··åˆçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00906', 'title': 'Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents', 'url': 'https://huggingface.co/papers/2504.00906', 'abstract': 'Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.', 'score': 13, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'e51174b579a417e9', 'authors': ['Saaket Agashe', 'Kyle Wong', 'Vincent Tu', 'Jiachen Yang', 'Ang Li', 'Xin Eric Wang'], 'affiliations': ['Simular Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00906.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Agent S2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agent S2 - Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Agent S2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼.'}, 'en': {'title': 'Agent S2: Revolutionizing Task Automation with Smart Planning and Grounding', 'desc': 'This paper presents Agent S2, a new framework designed to improve the performance of computer use agents that automate tasks by interacting with graphical user interfaces (GUIs). The framework addresses key challenges such as accurately identifying GUI elements and planning complex tasks over time. It introduces a Mixture-of-Grounding technique for better GUI localization and Proactive Hierarchical Planning to adapt action plans based on real-time observations. Evaluations show that Agent S2 outperforms existing agents on multiple benchmarks, demonstrating significant improvements in task execution across different operating systems.'}, 'zh': {'title': 'Agent S2ï¼šæ™ºèƒ½ä»£ç†çš„æ–°çºªå…ƒ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAgent S2çš„æ–°å‹æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†è®¤çŸ¥ä»»åŠ¡åˆ†é…ç»™ä¸åŒçš„é€šç”¨æ¨¡å‹å’Œä¸“ä¸šæ¨¡å‹æ¥æé«˜æ•°å­—ä»»åŠ¡çš„è‡ªåŠ¨åŒ–æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆå®šä½æŠ€æœ¯ï¼Œä»¥å®ç°ç²¾ç¡®çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å…ƒç´ å®šä½ï¼Œå¹¶å¼•å…¥äº†ä¸»åŠ¨å±‚æ¬¡è§„åˆ’ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸æ–­å˜åŒ–çš„è§‚å¯ŸåŠ¨æ€è°ƒæ•´è¡ŒåŠ¨è®¡åˆ’ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAgent S2åœ¨ä¸‰ä¸ªä¸»è¦çš„è®¡ç®—æœºä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„é¢†å…ˆä»£ç†ã€‚ç‰¹åˆ«æ˜¯åœ¨OSWorldè¯„ä¼°ä¸­ï¼ŒAgent S2ç›¸è¾ƒäºå…¶ä»–ä»£ç†å®ç°äº†18.9%å’Œ32.7%çš„ç›¸å¯¹æå‡ï¼Œå±•ç°äº†å…¶åœ¨ä¸åŒæ“ä½œç³»ç»Ÿå’Œåº”ç”¨ä¸­çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00509', 'title': 'Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?', 'url': 'https://huggingface.co/papers/2504.00509', 'abstract': "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.", 'score': 12, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'c9697e67f23cfa4e', 'authors': ['Kai Yan', 'Yufei Xu', 'Zhengyin Du', 'Xuesong Yao', 'Zheyu Wang', 'Xiaowen Guo', 'Jiecao Chen'], 'affiliations': ['ByteDance Seed', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.00509.jpg', 'data': {'categories': ['#multimodal', '#hallucinations', '#benchmark', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑƒĞ¼Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ?', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RoR-Bench Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ·Ğ°ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ´Ğ¾ 60%) Ğ½Ğ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ñ€Ğ°Ğ·Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'Reassessing LLM Intelligence: Are They Truly Reasoning?', 'desc': 'This paper introduces RoR-Bench, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs). The authors investigate whether LLMs demonstrate genuine intelligence or merely replicate learned responses from their training data. Their empirical analysis reveals that even advanced LLMs, like OpenAI-o1 and DeepSeek-R1, show significant performance dropsâ€”up to 60%â€”when faced with slight changes in problem phrasing. This raises important questions about the actual reasoning abilities of these models and suggests a need for a deeper understanding of their intelligence.'}, 'zh': {'title': 'é‡æ–°å®¡è§†LLMçš„æ™ºèƒ½æ°´å¹³', 'desc': 'è¿‘å¹´æ¥ï¼ŒLLMåŸºå‡†æµ‹è¯•çš„éš¾åº¦ä»å°å­¦æ°´å¹³è¿…é€Ÿä¸Šå‡åˆ°å‰æ²¿é—®é¢˜ï¼Œè¿™è®©ç ”ç©¶äººå‘˜æ„Ÿåˆ°æˆ‘ä»¬ç¦»è¶…è¶Šäººç±»æ™ºèƒ½åªæœ‰ä¸€æ­¥ä¹‹é¥ã€‚ç„¶è€Œï¼ŒLLMçš„æ¨ç†èƒ½åŠ›æ˜¯å¦çœŸçš„æ˜¯äººç±»æ ‡å‡†ä¸‹çš„çœŸæ­£æ™ºèƒ½ï¼Œè¿˜æ˜¯ä»…ä»…åœ¨è®­ç»ƒä¸­è§è¿‡çš„è§£å†³æ–¹æ¡ˆçš„å¤è¿°ï¼Ÿä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RoR-Benchï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œç”¨äºæ£€æµ‹LLMåœ¨ç®€å•æ¨ç†é—®é¢˜ä¸­æ˜¯å¦å­˜åœ¨å¤è¿°è¡Œä¸ºã€‚æˆ‘ä»¬çš„å®è¯åˆ†æå‘ç°ï¼Œç°æœ‰çš„é¡¶å°–LLMåœ¨æ¡ä»¶ç¨å¾®æ”¹å˜æ—¶ï¼Œè¡¨ç°å‡ºæå…¶ä¸¥é‡çš„å¤è¿°è¡Œä¸ºï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬é‡æ–°è¯„ä¼°è¿™äº›æ¨¡å‹çš„çœŸå®æ™ºèƒ½æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24377', 'title': 'Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models', 'url': 'https://huggingface.co/papers/2503.24377', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.', 'score': 11, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'd18ba97a459aea36', 'authors': ['Rui Wang', 'Hongru Wang', 'Boyang Xue', 'Jianhui Pang', 'Shudong Liu', 'Yi Chen', 'Jiahao Qiu', 'Derek Fai Wong', 'Heng Ji', 'Kam-Fai Wong'], 'affiliations': ['Princeton University', 'The Chinese University of Hong Kong', 'The University of Hong Kong', 'University of Illinois Urbana-Champaign', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.24377.jpg', 'data': {'categories': ['#inference', '#survey', '#reasoning', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ¿ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ÑÑ‚Ğ°Ğ¿ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ñ€ĞµĞ¼ÑÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Balancing Performance and Cost in Language Model Reasoning', 'desc': 'This paper discusses the advancements in Large Language Models (LLMs) that allow them to perform complex reasoning tasks more effectively. It highlights the difference between two types of reasoning: System 1, which is fast and efficient but less accurate, and System 2, which is slower and more accurate but computationally expensive. The authors introduce the concept of reasoning economy, which aims to balance the trade-off between performance and computational costs. They analyze the inefficiencies in reasoning, explore different reasoning patterns, and propose solutions to enhance the reasoning economy of LLMs, providing insights for future research.'}, 'zh': {'title': 'æ¨ç†ç»æµï¼šå¹³è¡¡æ€§èƒ½ä¸è®¡ç®—æˆæœ¬çš„å…³é”®', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å…¶æ‰§è¡Œå¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¿«é€Ÿç›´è§‰æ€ç»´ï¼ˆç³»ç»Ÿ1ï¼‰ä¸ç¼“æ…¢æ·±åº¦æ¨ç†ï¼ˆç³»ç»Ÿ2ï¼‰ä¹‹é—´çš„è½¬å˜ã€‚è™½ç„¶ç³»ç»Ÿ2æ¨ç†æé«˜äº†ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œä½†ç”±äºå…¶æ€ç»´ç¼“æ…¢å’Œæ¨ç†è¡Œä¸ºä½æ•ˆï¼Œå¾€å¾€ä¼šå¸¦æ¥è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚ç›¸å¯¹è€Œè¨€ï¼Œç³»ç»Ÿ1æ¨ç†è®¡ç®—æ•ˆç‡é«˜ï¼Œä½†å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚å› æ­¤ï¼Œå¹³è¡¡æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œå½¢æˆäº†æ¨ç†ç»æµçš„æ¦‚å¿µï¼Œè¿™æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22952', 'title': 'OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts', 'url': 'https://huggingface.co/papers/2503.22952', 'abstract': 'The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.', 'score': 11, 'issue_id': 3024, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '8b78ccf427a5cdc0', 'authors': ['Yuxuan Wang', 'Yueqian Wang', 'Bo Chen', 'Tong Wu', 'Dongyan Zhao', 'Zilong Zheng'], 'affiliations': ['Beijing Institute for General Artificial Intelligence', 'State Key Laboratory of General Artificial Intelligence', 'Wangxuan Institute of Computer Technology, Peking University', 'X-LANCE Lab, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22952.jpg', 'data': {'categories': ['#video', '#games', '#inference', '#multimodal', '#reasoning', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'OmniMMI: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniMMI - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1000 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 2000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞµÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Multi-modal Multiplexing Modeling (M4) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Enhancing Interaction with OmniLLMs in Streaming Video', 'desc': 'This paper presents OmniMMI, a new benchmark for evaluating Omni language models (OmniLLMs) in the context of streaming video. It addresses the challenges of understanding and reasoning in real-time video interactions, which are often overlooked in current benchmarks. The benchmark includes a large dataset of over 1,121 videos and 2,290 questions, focusing on proactive reasoning across six subtasks. Additionally, the authors introduce a framework called Multi-modal Multiplexing Modeling (M4) that enhances the efficiency of streaming models by allowing them to process audio and visual data simultaneously while generating responses.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„äº’åŠ¨èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†OmniMMIï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºOmniè¯­è¨€æ¨¡å‹åœ¨æµåª’ä½“è§†é¢‘ç¯å¢ƒä¸­è®¾è®¡çš„å¤šæ¨¡æ€äº¤äº’åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡1121ä¸ªè§†é¢‘å’Œ2290ä¸ªé—®é¢˜ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘åŸºå‡†ä¸­æµåª’ä½“è§†é¢‘ç†è§£å’Œä¸»åŠ¨æ¨ç†çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç§°ä¸ºå¤šæ¨¡æ€å¤ç”¨å»ºæ¨¡ï¼ˆM4ï¼‰ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆæ¨ç†çš„æµåª’ä½“æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆå†…å®¹çš„åŒæ—¶è¿›è¡Œè§†è§‰å’Œå¬è§‰å¤„ç†ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬å¸Œæœ›æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„äº’åŠ¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00927', 'title': 'Multi-Token Attention', 'url': 'https://huggingface.co/papers/2504.00927', 'abstract': 'Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other\'s attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector\'s capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method\'s ability to leverage richer information proves particularly beneficial.', 'score': 10, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '2af4c7adceecde31', 'authors': ['Olga Golovneva', 'Tianlu Wang', 'Jason Weston', 'Sainbayar Sukhbaatar'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2504.00927.jpg', 'data': {'categories': ['#long_context', '#architecture', '#benchmark', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Multi-Token Attention (MTA). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° soft attention, MTA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ ĞºĞ»ÑÑ‡Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. MTA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unlocking Richer Context with Multi-Token Attention', 'desc': 'This paper introduces Multi-Token Attention (MTA), a novel attention mechanism designed to improve the performance of large language models (LLMs). Unlike traditional single token attention, which relies on a single query and key token vector, MTA allows for the simultaneous consideration of multiple query and key vectors. By utilizing convolution operations, MTA enhances the interaction between nearby queries and keys, leading to more accurate attention weights. The results show that MTA significantly outperforms standard Transformer models, especially in tasks involving long contexts and information retrieval.'}, 'zh': {'title': 'å¤šä»¤ç‰Œæ³¨æ„åŠ›ï¼šæå‡ä¸Šä¸‹æ–‡ç†è§£çš„å…³é”®', 'desc': 'è½¯æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ä¸€ä¸ªé‡è¦çš„ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºåœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­å®šä½ç›¸å…³éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å•ä¸ªä»¤ç‰Œæ³¨æ„åŠ›æ–¹æ³•ä»…ä¾èµ–äºå•ä¸ªæŸ¥è¯¢å’Œé”®å‘é‡çš„ç›¸ä¼¼æ€§ï¼Œè¿™é™åˆ¶äº†ä¿¡æ¯çš„ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æ–¹æ³•â€”â€”å¤šä»¤ç‰Œæ³¨æ„åŠ›ï¼ˆMTAï¼‰ï¼Œå®ƒå…è®¸LLMsåŒæ—¶åŸºäºå¤šä¸ªæŸ¥è¯¢å’Œé”®å‘é‡æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ã€‚é€šè¿‡å¯¹æŸ¥è¯¢ã€é”®å’Œå¤´éƒ¨åº”ç”¨å·ç§¯æ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œä»è€Œåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ›´å‡†ç¡®åœ°å®šä½ç›¸å…³å†…å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01017', 'title': 'Scaling Language-Free Visual Representation Learning', 'url': 'https://huggingface.co/papers/2504.01017', 'abstract': 'Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.', 'score': 9, 'issue_id': 3023, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '9ab970f68b26c2ea', 'authors': ['David Fan', 'Shengbang Tong', 'Jiachen Zhu', 'Koustuv Sinha', 'Zhuang Liu', 'Xinlei Chen', 'Michael Rabbat', 'Nicolas Ballas', 'Yann LeCun', 'Amir Bar', 'Saining Xie'], 'affiliations': ['FAIR, Meta', 'New York University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01017.jpg', 'data': {'categories': ['#cv', '#training', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (SSL) Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… (CLIP) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SSL Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ CLIP Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² (VQA). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ SSL Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸.'}, 'en': {'title': 'Visual SSL: Bridging the Gap with Scale and Data', 'desc': 'This paper investigates the performance gap between Visual Self-Supervised Learning (SSL) and Contrastive Language-Image Pretraining (CLIP) in multimodal tasks like Visual Question Answering (VQA). The authors explore whether this gap is due to the absence of language supervision in visual SSL or differences in the training datasets used. By training both types of models on the same MetaCLIP data, they find that visual SSL models can outperform CLIP models when scaled up, achieving comparable performance on various benchmarks. This suggests that visual SSL can effectively compete with language-supervised methods, highlighting its potential for advancing vision-centric representation learning.'}, 'zh': {'title': 'è§†è§‰è‡ªç›‘ç£å­¦ä¹ çš„æ½œåŠ›ä¸CLIPç›¸åª²ç¾', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä¸å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç›¸åŒçš„MetaCLIPæ•°æ®ä¸Šè®­ç»ƒè§†è§‰SSLå’ŒCLIPæ¨¡å‹ï¼Œæ¥åˆ†æè¯­è¨€ç›‘ç£çš„ç¼ºä¹æ˜¯å¦æ˜¯å¯¼è‡´è§†è§‰SSLè½åçš„åŸå› ã€‚ç»“æœæ˜¾ç¤ºï¼Œè§†è§‰SSLæ¨¡å‹åœ¨æ•°æ®å’Œæ¨¡å‹å®¹é‡æ–¹é¢çš„æ‰©å±•æ€§ä¼˜äºCLIPæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å‚æ•°è¾¾åˆ°70äº¿æ—¶æ€§èƒ½ä»æœªé¥±å’Œã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œçº¯è§†è§‰SSLåœ¨å¤§è§„æ¨¡ä¸‹å¯ä»¥è¾¾åˆ°ä¸è¯­è¨€ç›‘ç£è§†è§‰é¢„è®­ç»ƒç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºè§†è§‰ä¸­å¿ƒçš„è¡¨ç¤ºå­¦ä¹ å¼€è¾Ÿäº†æ–°çš„æœºä¼šã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01005', 'title': 'When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning', 'url': 'https://huggingface.co/papers/2504.01005', 'abstract': 'Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.', 'score': 9, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'ee8e4951bf6c7a18', 'authors': ['Nishad Singhi', 'Hritik Bansal', 'Arian Hosseini', 'Aditya Grover', 'Kai-Wei Chang', 'Marcus Rohrbach', 'Anna Rohrbach'], 'affiliations': ['Google DeepMind', 'Mila', 'TU Darmstadt & hessian.AI', 'University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2504.01005.jpg', 'data': {'categories': ['#training', '#math', '#optimization', '#reasoning', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Consistency (SC), Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ğ¾Ğµ, Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Generative Reward Models (GenRM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ SC Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ GenRM, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Balancing Solution Generation and Verification for Efficient Reasoning in LLMs', 'desc': 'This paper explores how to improve the reasoning abilities of large language models (LLMs) during problem-solving by adjusting the amount of computation used at test time. It compares two methods: Self-Consistency (SC), which generates multiple answers and picks the most common, and Generative Reward Models (GenRM), which scores answers based on a next-token prediction approach. The study finds that SC is generally more efficient in terms of compute resources compared to GenRM, especially under limited budgets. The authors provide insights on how to effectively balance the generation of solutions and their verification to optimize performance.'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†èƒ½åŠ›ï¼šè§£ç”Ÿæˆä¸éªŒè¯çš„å¹³è¡¡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œå¦‚ä½•é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—æ¥æå‡æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­ã€‚ä¼ ç»Ÿçš„è‡ªä¸€è‡´æ€§ï¼ˆSelf-Consistency, SCï¼‰æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šä¸ªè§£å¹¶é‡‡ç”¨å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€å¸¸è§çš„ç­”æ¡ˆã€‚æœ€è¿‘çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenerative Reward Models, GenRMï¼‰åˆ™å°†éªŒè¯é‡æ„ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä»»åŠ¡ï¼Œä»è€Œåœ¨æ¨ç†æ—¶å¼•å…¥æ–°çš„æ‰©å±•æ–¹å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å›ºå®šçš„æ¨ç†é¢„ç®—ä¸‹ï¼ŒSCåœ¨å¤§å¤šæ•°å®é™…æƒ…å†µä¸‹æ¯”GenRMæ›´å…·è®¡ç®—æ•ˆç‡ï¼Œæä¾›äº†åœ¨æµ‹è¯•æ—¶æ‰©å±•ä¸­ä¼˜åŒ–è§£ç”Ÿæˆä¸éªŒè¯çš„å®ç”¨æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23434', 'title': 'Towards Trustworthy GUI Agents: A Survey', 'url': 'https://huggingface.co/papers/2503.23434', 'abstract': 'GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.', 'score': 9, 'issue_id': 3024, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'e19e4d94bcea9cb0', 'authors': ['Yucheng Shi', 'Wenhao Yu', 'Wenlin Yao', 'Wenhu Chen', 'Ninghao Liu'], 'affiliations': ['Amazon', 'Tencent AI Seattle Lab', 'University of Georgia', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23434.jpg', 'data': {'categories': ['#ethics', '#security', '#agents', '#survey', '#training', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”Ğ¾Ğ²ĞµÑ€Ğ¸Ğµ Ğº Ğ˜Ğ˜: Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²: ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¸ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğµ ÑĞ±Ğ¾Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Ensuring Trust in Autonomous GUI Agents', 'desc': 'This paper surveys the trustworthiness of GUI agents that use large foundation models to interact with digital interfaces. It highlights five critical dimensions of trustworthiness: security vulnerabilities, reliability in changing environments, transparency, ethical considerations, and evaluation methods. The authors discuss significant challenges such as susceptibility to adversarial attacks and the need for realistic evaluation benchmarks. The paper emphasizes the importance of developing robust safety standards and responsible practices as GUI agents become more prevalent.'}, 'zh': {'title': 'æ„å»ºå¯ä¿¡èµ–çš„GUIä»£ç†ï¼Œä¿éšœå®‰å…¨ä¸éšç§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºå¤§å‹åŸºç¡€æ¨¡å‹çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„ä¿¡ä»»æ€§é—®é¢˜ã€‚æˆ‘ä»¬åˆ†æäº†äº”ä¸ªå…³é”®ç»´åº¦ï¼ŒåŒ…æ‹¬å®‰å…¨æ¼æ´ã€åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§ã€é€æ˜æ€§ä¸å¯è§£é‡Šæ€§ã€ä¼¦ç†è€ƒé‡ä»¥åŠè¯„ä¼°æ–¹æ³•ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºäº†ä¸»è¦æŒ‘æˆ˜ï¼Œå¦‚å¯¹æŠ—æ€§æ”»å‡»çš„è„†å¼±æ€§ã€åºåˆ—å†³ç­–ä¸­çš„çº§è”å¤±è´¥æ¨¡å¼ï¼Œä»¥åŠç¼ºä¹ç°å®çš„è¯„ä¼°åŸºå‡†ã€‚è¿™äº›é—®é¢˜ä¸ä»…é˜»ç¢äº†GUIä»£ç†çš„å®é™…åº”ç”¨ï¼Œè¿˜éœ€è¦è¶…è¶Šä»»åŠ¡æˆåŠŸçš„å…¨é¢ç¼“è§£ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23733', 'title': 'AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization', 'url': 'https://huggingface.co/papers/2503.23733', 'abstract': 'Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.', 'score': 8, 'issue_id': 3017, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'ed45063868071c13', 'authors': ['Yiyang Du', 'Xiaochen Wang', 'Chi Chen', 'Jiabo Ye', 'Yiru Wang', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Zhifang Sui', 'Maosong Sun', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China', 'Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China', 'Institute of Intelligent Computing, Alibaba Group', 'Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China', 'ModelTC Open Source Organization, Beijing, China', 'School of Software Microelectronics, Peking University, Beijing, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.23733.jpg', 'data': {'categories': ['#training', '#architecture', '#transfer_learning', '#optimization', '#multimodal', '#benchmark'], 'emoji': 'ğŸ”€', 'ru': {'title': 'AdaMMS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AdaMMS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸Ñ… ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². AdaMMS Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… MLLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ AdaMMS Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Merging Diverse Models with AdaMMS', 'desc': 'This paper introduces AdaMMS, a new method for merging Multimodal Large Language Models (MLLMs) that have different architectures. Traditional merging techniques struggle with these heterogeneous models due to their varying structures and parameter spaces. AdaMMS addresses this by first mapping the models, then merging their weights through linear interpolation, and finally optimizing hyper-parameters using an unsupervised approach. The results show that AdaMMS significantly improves performance on vision-language tasks compared to earlier methods.'}, 'zh': {'title': 'å¼‚è´¨æ¨¡å‹åˆå¹¶çš„æ–°çªç ´', 'desc': 'æœ€è¿‘ï¼Œæ¨¡å‹åˆå¹¶æ–¹æ³•åœ¨ç»“åˆå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ä¼˜åŠ¿ã€‚ä»¥å¾€çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åˆå¹¶å…·æœ‰ç›¸åŒæ¶æ„çš„åŒè´¨æ¨¡å‹ï¼Œä½†åœ¨å¤„ç†å…·æœ‰å›ºæœ‰å¼‚è´¨æ€§çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†AdaMMSï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¼‚è´¨MLLMsè®¾è®¡çš„æ–°å‹æ¨¡å‹åˆå¹¶æ–¹æ³•ï¼Œé‡‡ç”¨æ˜ å°„ã€åˆå¹¶å’Œæœç´¢ä¸‰ä¸ªæ­¥éª¤æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é€šè¿‡è®¾è®¡æ¨¡å‹ä¹‹é—´çš„æ˜ å°„å‡½æ•°ã€å¯¹æ¨¡å‹æƒé‡è¿›è¡Œçº¿æ€§æ’å€¼ä»¥åŠæå‡ºæ— ç›‘ç£çš„è¶…å‚æ•°é€‰æ‹©æ–¹æ³•ï¼ŒAdaMMSåœ¨å„ç§è§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä»¥å¾€çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00557', 'title': 'Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features', 'url': 'https://huggingface.co/papers/2504.00557', 'abstract': 'Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.', 'score': 7, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'c5628fbf1a189a06', 'authors': ['Jewon Lee', 'Ki-Ung Song', 'Seungmin Yang', 'Donguk Lim', 'Jaeyeon Kim', 'Wooksu Shin', 'Bo-Kyeong Kim', 'Yong Jae Lee', 'Tae-Ho Kim'], 'affiliations': ['Nota Inc.', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.00557.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#cv'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ñ€Ñ‚ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Trimmed Llama: Efficient Visual Token Reduction for Faster Inference', 'desc': 'This paper presents a method called Trimmed Llama, which reduces the number of visual tokens in large vision-language models (LVLMs) to lower inference costs. Unlike previous studies that focused on self-attention models, this work specifically targets cross-attention models, which are known for their better performance. The authors highlight that the key-value (KV) cache for image tokens in cross-attention layers is much larger than that for text tokens, creating a significant computational bottleneck. By selectively pruning redundant visual features based on the sparse nature of cross-attention maps, the model achieves a 50% reduction in visual features, leading to decreased latency and memory usage while maintaining performance benchmarks.'}, 'zh': {'title': 'è§†è§‰ç‰¹å¾ä¿®å‰ªï¼Œæå‡æ¨ç†æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è§†è§‰æ ‡è®°å‡å°‘çš„æ–¹æ³•ï¼Œä»¥é™ä½å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ¨ç†æ—¶çš„è®¡ç®—æˆæœ¬ã€‚ä¸ä»¥å¾€åªé’ˆå¯¹è‡ªæ³¨æ„åŠ›æ¨¡å‹çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸“æ³¨äºåŸºäºäº¤å‰æ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ›´ä¸ºä¼˜è¶Šã€‚æˆ‘ä»¬å‘ç°äº¤å‰æ³¨æ„åŠ›å±‚ä¸­å›¾åƒæ ‡è®°çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°è¿œå¤§äºè‡ªæ³¨æ„åŠ›å±‚ä¸­çš„æ–‡æœ¬æ ‡è®°ï¼Œæˆä¸ºè®¡ç®—ç“¶é¢ˆã€‚é€šè¿‡åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å›¾çš„ç¨€ç–ç‰¹æ€§ï¼Œæˆ‘ä»¬é€‰æ‹©æ€§åœ°ä¿®å‰ªå†—ä½™çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘KVç¼“å­˜éœ€æ±‚ï¼Œé™ä½æ¨ç†å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒåŸºå‡†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22165', 'title': 'Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.22165', 'abstract': 'Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts.', 'score': 7, 'issue_id': 3034, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '76c9bb027c844f9c', 'authors': ['Zhanke Zhou', 'Zhaocheng Zhu', 'Xuan Li', 'Mikhail Galkin', 'Xiao Feng', 'Sanmi Koyejo', 'Jian Tang', 'Bo Han'], 'affiliations': ['HEC Montreal', 'Intel AI Lab', 'Mila - Quebec AI Institute', 'Stanford University', 'TMLR Group, Hong Kong Baptist University', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.22165.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#reasoning', '#multimodal', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'landscape of thoughts' Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ t-SNE Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 'landscape of thoughts' ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ¸Ğ·ĞºĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ."}, 'en': {'title': 'Visualizing Reasoning Paths in Language Models', 'desc': "This paper introduces a new visualization tool called 'landscape of thoughts' that helps users understand how large language models (LLMs) reason through problems. It represents reasoning paths as feature vectors, which show how close each reasoning step is to possible answers. By using a technique called t-SNE, the tool creates two-dimensional plots that allow for easy comparison of different models and their reasoning effectiveness. The tool also identifies problematic reasoning patterns and can be adapted to evaluate the accuracy of reasoning paths in various tasks."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è·¯å¾„', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œæ€ç»´æ™¯è§‚â€çš„å¯è§†åŒ–å·¥å…·ï¼Œç”¨äºåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†è·¯å¾„ã€‚è¯¥å·¥å…·é€šè¿‡å°†æ¨ç†è·¯å¾„ä¸­çš„çŠ¶æ€è¡¨ç¤ºä¸ºç‰¹å¾å‘é‡ï¼Œé‡åŒ–å®ƒä»¬ä¸æ‰€æœ‰ç­”æ¡ˆé€‰é¡¹çš„è·ç¦»ï¼Œå¹¶ä½¿ç”¨t-SNEè¿›è¡ŒäºŒç»´å¯è§†åŒ–ã€‚é€šè¿‡å¯¹æ€ç»´æ™¯è§‚çš„å®šæ€§å’Œå®šé‡åˆ†æï¼Œå¯ä»¥æœ‰æ•ˆåŒºåˆ†å¼ºå¼±æ¨¡å‹ã€æ­£ç¡®ä¸é”™è¯¯ç­”æ¡ˆï¼Œä»¥åŠä¸åŒçš„æ¨ç†ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥å·¥å…·è¿˜èƒ½å¤Ÿæ­ç¤ºä¸ç†æƒ³çš„æ¨ç†æ¨¡å¼ï¼Œå¦‚ä½ä¸€è‡´æ€§å’Œé«˜ä¸ç¡®å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00294', 'title': 'Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead', 'url': 'https://huggingface.co/papers/2504.00294', 'abstract': "Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.", 'score': 6, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'b455a4adb4eae588', 'authors': ['Vidhisha Balachandran', 'Jingya Chen', 'Lingjiao Chen', 'Shivam Garg', 'Neel Joshi', 'Yash Lara', 'John Langford', 'Besmira Nushi', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00294.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#math', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM: Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ÑÑ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸.'}, 'en': {'title': 'Scaling Inference for Smarter Problem Solving in LLMs', 'desc': 'This paper explores how inference-time scaling can improve the reasoning abilities of large language models (LLMs) when tackling complex problems. It examines the effectiveness of extending generated scratchpads for various tasks, including math reasoning and navigation, across nine advanced models. The study finds that while scaling can enhance performance, its benefits vary by task and may decrease with increased complexity. Additionally, the research indicates that simply increasing the number of tokens does not guarantee better accuracy, but using strong feedback mechanisms can lead to significant performance improvements.'}, 'zh': {'title': 'æ¨ç†æ—¶é—´æ‰©å±•ï¼šæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ¨ç†æ—¶é—´æ‰©å±•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬åˆ†æäº†ä¹ç§æœ€å…ˆè¿›æ¨¡å‹åœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ•°å­¦æ¨ç†å’Œç©ºé—´æ¨ç†ç­‰ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨ç†æ—¶é—´æ‰©å±•çš„ä¼˜åŠ¿å› ä»»åŠ¡è€Œå¼‚ï¼Œä¸”åœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶ä¼šå‡å¼±ã€‚å°½ç®¡ä½¿ç”¨æ›´å¤šçš„æ ‡è®°å¹¶ä¸æ€»èƒ½æé«˜å‡†ç¡®æ€§ï¼Œä½†åœ¨æœ‰å¼ºåé¦ˆçš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00869', 'title': 'm1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models', 'url': 'https://huggingface.co/papers/2504.00869', 'abstract': "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.", 'score': 5, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'bd9586b08ce02a05', 'authors': ['Xiaoke Huang', 'Juncheng Wu', 'Hui Liu', 'Xianfeng Tang', 'Yuyin Zhou'], 'affiliations': ['Amazon Research', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2504.00869.jpg', 'data': {'categories': ['#reasoning', '#healthcare', '#science', '#training', '#inference'], 'emoji': 'ğŸ©º', 'ru': {'title': 'ĞœĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ m1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Medical Reasoning with Test-Time Scaling', 'desc': "This paper explores the use of test-time scaling to improve the reasoning abilities of large language models specifically in the medical field. The authors introduce a method called m1, which enhances medical reasoning during inference, showing that smaller models can achieve state-of-the-art results. They find that while increasing the reasoning token budget can help, it may also lead to performance degradation if overused. The study emphasizes that having rich medical knowledge is crucial for effective reasoning, rather than just increasing the model's complexity or depth of reasoning."}, 'zh': {'title': 'åŒ»å­¦æ¨ç†çš„æ–°çªç ´ï¼šæµ‹è¯•æ—¶ç¼©æ”¾çš„åŠ›é‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯åœ¨åŒ»å­¦æ¨ç†ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åä¸ºm1çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨æ¨ç†æ—¶çš„åŒ»å­¦èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾åœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸­å‡èƒ½æ˜¾è‘—æé«˜æ¨ç†æ•ˆæœï¼Œå°¤å…¶æ˜¯å¯¹äºå‚æ•°å°‘äº10Bçš„è½»é‡çº§å¾®è°ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿè¾¾åˆ°æ–°çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œæœ€ä½³çš„æ¨ç†ä»¤ç‰Œé¢„ç®—çº¦ä¸º4Kï¼Œè¶…å‡ºæ­¤èŒƒå›´å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œå¢åŠ æ•°æ®è§„æ¨¡ã€æé«˜æ•°æ®è´¨é‡å’Œæ‰©å±•æ¨¡å‹å®¹é‡æ˜¯æå‡åŒ»å­¦çŸ¥è¯†åŸºç¡€çš„å…³é”®ï¼Œå°¤å…¶æ˜¯åœ¨å°æ¨¡å‹æ€§èƒ½é¥±å’Œçš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23361', 'title': 'Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base', 'url': 'https://huggingface.co/papers/2503.23361', 'abstract': "Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.", 'score': 4, 'issue_id': 3017, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': '9b957c49c958aea3', 'authors': ['Linxin Song', 'Xuwei Ding', 'Jieyu Zhang', 'Taiwei Shi', 'Ryotaro Shimizu', 'Rahul Gupta', 'Yang Liu', 'Jian Kang', 'Jieyu Zhao'], 'affiliations': ['AGI', 'Amazon', 'University of Rochester', 'University of Southern California', 'University of Washington', 'University of Wisconsin-Madison', 'ZOZO Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23361.jpg', 'data': {'categories': ['#training', '#hallucinations', '#optimization', '#graphs', '#benchmark', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'SEA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SEA (ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´ÑŠĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). SEA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SEA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² LLM.'}, 'en': {'title': 'Uncovering Knowledge Deficiencies in LLMs Efficiently with SEA', 'desc': 'This paper introduces a new method called Stochastic Error Ascent (SEA) to identify knowledge deficiencies in large language models (LLMs) that often produce unreliable outputs. SEA efficiently discovers errors by using a stochastic optimization approach, focusing on high-error candidates based on their similarity to previously identified failures. The framework enhances its search capabilities through hierarchical retrieval and a directed acyclic graph to track error propagation. The results show that SEA significantly outperforms existing methods in uncovering knowledge errors while drastically reducing the cost of error discovery.'}, 'zh': {'title': 'å‘ç°LLMçŸ¥è¯†ç¼ºé™·çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸æ— æ³•å‡†ç¡®ä¿ç•™äº‹å®çŸ¥è¯†ï¼Œå¯¼è‡´å¹»è§‰å’Œä¸å¯é çš„è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºéšæœºé”™è¯¯ä¸Šå‡ï¼ˆSEAï¼‰çš„æ¡†æ¶ï¼Œç”¨äºåœ¨ä¸¥æ ¼çš„æŸ¥è¯¢é¢„ç®—ä¸‹å‘ç°é—­åˆæƒé‡LLMsä¸­çš„çŸ¥è¯†ç¼ºé™·ã€‚SEAé€šè¿‡åˆ©ç”¨ä¸å…ˆå‰è§‚å¯Ÿåˆ°çš„å¤±è´¥çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè¿­ä»£æ£€ç´¢æ–°çš„é«˜é”™è¯¯å€™é€‰é¡¹ï¼Œä»è€Œå°†é”™è¯¯å‘ç°è¿‡ç¨‹å½¢å¼åŒ–ä¸ºéšæœºä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒSEAå‘ç°çš„çŸ¥è¯†é”™è¯¯æ•°é‡æ˜¾è‘—é«˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†æ¯ä¸ªé”™è¯¯çš„æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00072', 'title': 'Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs', 'url': 'https://huggingface.co/papers/2504.00072', 'abstract': "We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page.", 'score': 3, 'issue_id': 3021, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '48f5266ddbfa7bca', 'authors': ['Lucas Ventura', 'Antoine Yang', 'Cordelia Schmid', 'GÃ¼l Varol'], 'affiliations': ['Google DeepMind', 'Inria, Ecole normale superieure, CNRS, PSL Research University', 'LIGM, Ecole des Ponts, IP Paris, Univ Gustave Eiffel, CNRS'], 'pdf_title_img': 'assets/pdf/title_img/2504.00072.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#open_source', '#video', '#multimodal'], 'emoji': 'ğŸ“½ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ 'Chapter-Llama' Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸. LLM Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ³Ğ»Ğ°Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VidChapters-7M, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ F1-score 45.3 Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 26.7."}, 'en': {'title': 'Efficient Video Chaptering with Chapter-Llama', 'desc': "This paper presents a novel approach to video chaptering, which involves dividing long videos into meaningful segments and generating titles for these segments. The authors introduce the 'Chapter-Llama' framework that utilizes a pretrained large language model (LLM) to process speech transcripts and video captions efficiently. By implementing a speech-guided frame selection strategy, they enhance the model's performance while reducing the need for extensive captioning. The results show significant improvements in chaptering accuracy, achieving a notable F1 score on the VidChapters-7M benchmark, and the authors provide their code and models for further research."}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç« èŠ‚åˆ’åˆ†çš„æ–°æ–¹æ³•', 'desc': "æœ¬æ–‡æ¢è®¨äº†è§†é¢‘ç« èŠ‚åˆ’åˆ†çš„ä»»åŠ¡ï¼Œå³å°†é•¿è§†é¢‘æ—¶é—´çº¿åˆ’åˆ†ä¸ºè¯­ä¹‰å•å…ƒå¹¶ç”Ÿæˆç›¸åº”çš„ç« èŠ‚æ ‡é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†'Chapter-Llama'æ¡†æ¶ï¼Œé€šè¿‡é«˜æ•ˆå¤„ç†æ–‡æœ¬é¢†åŸŸçš„é—®é¢˜ï¼Œå®ç°äº†å¯¹é•¿è¾¾ä¸€å°æ—¶è§†é¢‘çš„å¼ºå¤§ç« èŠ‚åˆ’åˆ†æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¾“å…¥åŒ…æ‹¬è¯­éŸ³è½¬å½•æ–‡æœ¬å’Œæè¿°è§†é¢‘å¸§çš„å­—å¹•ï¼Œä»¥åŠå®ƒä»¬å„è‡ªçš„æ—¶é—´æˆ³ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºè¯­éŸ³è½¬å½•å†…å®¹çš„è½»é‡çº§å¸§é€‰æ‹©ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†ç« èŠ‚åˆ’åˆ†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚"}}}, {'id': 'https://huggingface.co/papers/2503.24210', 'title': 'DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.24210', 'abstract': 'Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io', 'score': 2, 'issue_id': 3023, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'df1e0752d5790146', 'authors': ['Seungjun Lee', 'Gim Hee Lee'], 'affiliations': ['Department of Computer Science, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24210.jpg', 'data': {'categories': ['#synthetic', '#3d', '#cv', '#diffusion'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ§ĞµÑ‚ĞºĞ¾Ğµ 3D Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ 2D: DiET-GS Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiET-GS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ‚ĞºĞ¸Ñ… 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ 3DGS Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ğ»Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiET-GS Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing 3D Image Clarity with DiET-GS', 'desc': 'This paper addresses the challenge of creating clear 3D images from blurry multi-view pictures, a common issue in computer vision. The authors introduce DiET-GS, a new framework that combines event-based camera data with a diffusion prior to improve the quality of 3D image synthesis. By using a two-stage training approach, the framework effectively restores accurate colors and fine details in the images. The results show that DiET-GS outperforms existing methods in generating high-quality novel views from both synthetic and real-world datasets.'}, 'zh': {'title': 'æ¸…æ™°ä¸‰ç»´é‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDiET-GSçš„æ¡†æ¶ï¼Œç”¨äºä»æ¨¡ç³Šçš„å¤šè§†å›¾å›¾åƒä¸­é‡å»ºæ¸…æ™°çš„ä¸‰ç»´è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ— æ¨¡ç³Šäº‹ä»¶æµå’Œæ‰©æ•£å…ˆéªŒï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥æ¥æé«˜å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„çº¦æŸæ–¹æ³•ï¼Œåˆ©ç”¨äº‹ä»¶åŒé‡ç§¯åˆ†æ¥ç¡®ä¿é¢œè‰²å‡†ç¡®å’Œç»†èŠ‚æ¸…æ™°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€å•çš„æŠ€æœ¯ï¼Œåˆ©ç”¨æ‰©æ•£å…ˆéªŒè¿›ä¸€æ­¥å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23157', 'title': 'Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL', 'url': 'https://huggingface.co/papers/2503.23157', 'abstract': 'Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.', 'score': 2, 'issue_id': 3029, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '083970087ba6180e', 'authors': ['Mohammadreza Pourreza', 'Shayan Talaei', 'Ruoxi Sun', 'Xingchen Wan', 'Hailong Li', 'Azalia Mirhoseini', 'Amin Saberi', 'Sercan "O. Arik'], 'affiliations': ['Google Cloud', 'Google DeepMind', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23157.jpg', 'data': {'categories': ['#rlhf', '#rl', '#optimization', '#training', '#benchmark', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Text-to-SQL Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Text-to-SQL, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² RL. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO), Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ supervised fine-tuning Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BIRD.'}, 'en': {'title': 'Enhancing Text-to-SQL with Tailored Reinforcement Learning Rewards', 'desc': 'This paper addresses the complex task of converting natural language into SQL queries, which requires understanding language, database structures, and formulating precise queries. The authors critique existing methods that use fixed reasoning paths, which can hinder performance, and propose a new approach that utilizes tailored partial rewards to improve reinforcement learning outcomes. By implementing group relative policy optimization (GRPO), their method encourages large language models to enhance their reasoning skills, leading to better SQL generation. The results show that their reinforcement learning approach outperforms traditional supervised fine-tuning, achieving higher accuracy on benchmark tests with a smaller model size.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°SQLçš„æ¨ç†èƒ½åŠ›ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°SQLçš„ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€é¡¹æ¶‰åŠè‡ªç„¶è¯­è¨€ç†è§£å’Œæ•°æ®åº“æ¶æ„ç†è§£çš„å¤æ‚ä»»åŠ¡ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æ¨ç†è·¯å¾„ï¼Œé™åˆ¶äº†å…¶æ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„éƒ¨åˆ†å¥–åŠ±æœºåˆ¶ï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬åˆ°SQLä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ä¸Šéƒ½è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21860', 'title': 'ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning', 'url': 'https://huggingface.co/papers/2503.21860', 'abstract': 'Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.', 'score': 2, 'issue_id': 3017, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '0d9ea55946287027', 'authors': ['Kailin Li', 'Puhao Li', 'Tengyu Liu', 'Yuyang Li', 'Siyuan Huang'], 'affiliations': ['Department of Automation, Tsinghua University', 'Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.21860.jpg', 'data': {'categories': ['#training', '#dataset', '#robotics', '#transfer_learning', '#optimization', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ManipTrans: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼', 'desc': 'ManipTrans - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€ÑƒĞºĞ°Ğ¼ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ÑƒĞº Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ° ĞµĞ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DexManipNet Ñ 3.3 Ñ‚Ñ‹Ñ. ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¼Ğ¸ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficiently Teaching Robots to Manipulate Like Humans', 'desc': 'This paper presents ManipTrans, a two-stage method designed to transfer human bimanual manipulation skills to robotic hands in a simulated environment. The first stage involves pre-training a trajectory imitator that learns to replicate human hand movements, while the second stage fine-tunes a residual module to enhance performance under specific interaction constraints. This approach allows for efficient learning and execution of complex tasks, outperforming existing methods in terms of success rate and efficiency. Additionally, the authors introduce DexManipNet, a comprehensive dataset that includes diverse manipulation tasks, paving the way for improved policy training and real-world applications of dexterous robotic hands.'}, 'zh': {'title': 'é«˜æ•ˆè½¬ç§»äººç±»åŒæ‰‹æŠ€èƒ½çš„æœºå™¨äººæ‰‹', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºManipTransçš„æ–°æ–¹æ³•ï¼Œç”¨äºå°†äººç±»åŒæ‰‹çš„æŠ€èƒ½é«˜æ•ˆåœ°è½¬ç§»åˆ°çµå·§çš„æœºå™¨äººæ‰‹ä¸Šã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè®­ç»ƒä¸€ä¸ªé€šç”¨çš„è½¨è¿¹æ¨¡ä»¿å™¨æ¥æ¨¡æ‹Ÿæ‰‹éƒ¨åŠ¨ä½œï¼Œç„¶ååœ¨äº¤äº’çº¦æŸä¸‹å¾®è°ƒç‰¹å®šçš„æ®‹å·®æ¨¡å—ï¼Œä»è€Œå®ç°å¤æ‚åŒæ‰‹ä»»åŠ¡çš„é«˜æ•ˆå­¦ä¹ å’Œå‡†ç¡®æ‰§è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒManipTransåœ¨æˆåŠŸç‡ã€ä¿çœŸåº¦å’Œæ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ManipTransï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºDexManipNetçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«äº†3.3Kä¸ªæœºå™¨äººæ“ä½œçš„å®ä¾‹ï¼Œæ”¯æŒè¿›ä¸€æ­¥çš„ç­–ç•¥è®­ç»ƒå’Œå®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24219', 'title': 'MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote\n  Sensing', 'url': 'https://huggingface.co/papers/2503.24219', 'abstract': 'We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially supervised OD task. In the first stage, we construct a graph representation of each image, comprising object queries, class embeddings, and proposal locations. Then, our task-aware architecture processes this graph to perform the VG task. The model consists of: (i) a multi-branch network that integrates spatial, visual, and categorical features to generate task-aware proposals, and (ii) an object reasoning network that assigns probabilities across proposals, followed by a soft selection mechanism for final referring object localization. Our model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods while retaining classical OD capabilities. The code will be available in our repository: https://github.com/rd20karim/MB-ORES.', 'score': 1, 'issue_id': 3027, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'c59d3238abf4164f', 'authors': ['Karim Radouane', 'Hanane Azzag', 'Mustapha lebbah'], 'affiliations': ['University Paris-Saclay - DAVID Lab, UVSQ Versailles, France', 'University Sorbonne Paris Nord - LIPN, Villetaneuse, France'], 'pdf_title_img': 'assets/pdf/title_img/2503.24219.jpg', 'data': {'categories': ['#open_source', '#optimization', '#cv', '#architecture', '#graphs', '#dataset'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… OPT-RSVG Ğ¸ DIOR-RSVG.'}, 'en': {'title': 'Integrating Object Detection and Visual Grounding for Enhanced Remote Sensing Analysis', 'desc': 'This paper presents a new framework that combines object detection (OD) and visual grounding (VG) specifically for remote sensing imagery. The authors enhance a traditional open-set object detector by fine-tuning it with referring expression data, treating VG as a partially supervised OD task. They create a graph representation of images that includes object queries and class embeddings, which is then processed by a multi-branch network to generate proposals. The model outperforms existing methods on benchmark datasets while maintaining the capabilities of classical object detection.'}, 'zh': {'title': 'ç»Ÿä¸€æ¡†æ¶ï¼šç›®æ ‡æ£€æµ‹ä¸è§†è§‰å®šä½çš„ç»“åˆ', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†ç›®æ ‡æ£€æµ‹ï¼ˆODï¼‰å’Œè§†è§‰å®šä½ï¼ˆVGï¼‰é›†æˆåˆ°é¥æ„Ÿå›¾åƒä¸­ã€‚ä¸ºäº†æ”¯æŒä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹å¹¶ä¸ºè§†è§‰å®šä½ä»»åŠ¡å»ºç«‹ç›´è§‚çš„å…ˆéªŒï¼Œæˆ‘ä»¬ä½¿ç”¨å‚è€ƒè¡¨è¾¾æ•°æ®å¾®è°ƒäº†ä¸€ä¸ªå¼€æ”¾é›†ç›®æ ‡æ£€æµ‹å™¨ï¼Œå°†å…¶æ¡†å®šä¸ºéƒ¨åˆ†ç›‘ç£çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬æ„å»ºäº†æ¯ä¸ªå›¾åƒçš„å›¾å½¢è¡¨ç¤ºï¼ŒåŒ…æ‹¬ç›®æ ‡æŸ¥è¯¢ã€ç±»åˆ«åµŒå…¥å’Œæè®®ä½ç½®ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ„ŸçŸ¥æ¶æ„å¤„ç†è¿™ä¸ªå›¾å½¢ä»¥æ‰§è¡Œè§†è§‰å®šä½ä»»åŠ¡ï¼Œæœ€ç»ˆåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02605', 'title': 'Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving', 'url': 'https://huggingface.co/papers/2504.02605', 'abstract': 'The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.', 'score': 28, 'issue_id': 3095, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'bcf7d7c20685c914', 'authors': ['Daoguang Zan', 'Zhirong Huang', 'Wei Liu', 'Hanwu Chen', 'Linhao Zhang', 'Shulin Xin', 'Lu Chen', 'Qi Liu', 'Xiaojian Zhong', 'Aoyan Li', 'Siyao Liu', 'Yongsheng Xiao', 'Liangqiang Chen', 'Yuyu Zhang', 'Jing Su', 'Tianyu Liu', 'Rui Long', 'Kai Shen', 'Liang Xiang'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2504.02605.jpg', 'data': {'categories': ['#agi', '#benchmark', '#rl', '#open_source', '#multilingual', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Multi-SWE-bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1632 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€ĞµÑ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²: Agentless, SWE-agent Ğ¸ OpenHands. Ğ—Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Multi-SWE-RL Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'Empowering Issue Resolution with Multi-SWE-bench for Diverse Languages', 'desc': 'This paper introduces Multi-SWE-bench, a multilingual benchmark designed to evaluate Large Language Models (LLMs) in issue resolving across various programming languages, including Java, TypeScript, and C++. The benchmark consists of 1,632 high-quality instances, meticulously annotated by experts to ensure reliability in assessing model performance. The authors also present an analysis of state-of-the-art models using different evaluation methods and launch the Multi-SWE-RL community to foster the development of reinforcement learning datasets for issue-resolving tasks. By open-sourcing their data production pipeline and tutorials, they aim to encourage community contributions and advance research in this area, ultimately pushing towards the goal of Artificial General Intelligence (AGI).'}, 'zh': {'title': 'å¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†ï¼Œæ¨åŠ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§å¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†ï¼Œç§°ä¸ºMulti-SWE-benchï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿä¸­çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ¶µç›–äº†Javaã€TypeScriptã€JavaScriptã€Goã€Rustã€Cå’ŒC++ç­‰ä¸ƒç§ç¼–ç¨‹è¯­è¨€ï¼Œå…±åŒ…å«1,632ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†Multi-SWE-RLå¼€æºç¤¾åŒºï¼Œæ—¨åœ¨ä¸ºé—®é¢˜è§£å†³ä»»åŠ¡æ„å»ºå¤§è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å‘å¸ƒäº†4,723ä¸ªç»“æ„è‰¯å¥½çš„å®ä¾‹ã€‚é€šè¿‡å¼€æ”¾æ•°æ®ç”Ÿäº§æµç¨‹å’Œè¯¦ç»†æ•™ç¨‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ¿€åŠ±å¼€æºç¤¾åŒºæŒç»­è´¡çŒ®ï¼Œæ¨åŠ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03553', 'title': 'Agentic Knowledgeable Self-awareness', 'url': 'https://huggingface.co/papers/2504.03553', 'abstract': 'Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent\'s self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.', 'score': 15, 'issue_id': 3096, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '4a06cb6959ea30d3', 'authors': ['Shuofei Qiao', 'Zhisong Qiu', 'Baochang Ren', 'Xiaobin Wang', 'Xiangyuan Ru', 'Ningyu Zhang', 'Xiang Chen', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen'], 'affiliations': ['Alibaba Group', 'Nanjing University of Aeronautics and Astronautics', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03553.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#agents', '#agi'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ KnowSelf. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ ÑĞ¸Ñ‚ÑƒĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. KnowSelf Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering LLMs with Self-Aware Decision Making', 'desc': "This paper introduces a new approach called agentic knowledgeable self-awareness for Large Language Models (LLMs) in planning tasks. Unlike traditional methods that flood models with external information, this approach emphasizes the importance of situational awareness, allowing agents to assess their environment and use knowledge more effectively. The proposed method, KnowSelf, utilizes a heuristic to identify key moments in an agent's learning process, enabling it to adapt its strategies based on the situation. Experimental results show that KnowSelf significantly improves performance on various tasks while minimizing reliance on external knowledge."}, 'zh': {'title': 'è‡ªä¸»è°ƒèŠ‚çŸ¥è¯†ä½¿ç”¨çš„æ™ºèƒ½ä»£ç†', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»£ç†è§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ä»£ç†è§„åˆ’æ–¹æ³•é‡‡ç”¨äº†"æ´ªæ°´çŒæº‰"çš„æ–¹å¼ï¼Œéšæ„æ³¨å…¥é»„é‡‘è½¨è¿¹ã€å¤–éƒ¨åé¦ˆå’Œé¢†åŸŸçŸ¥è¯†ï¼Œè¿™ç§åšæ³•å¿½è§†äº†äººç±»åœ¨å†³ç­–è¿‡ç¨‹ä¸­åŠ¨æ€è¯„ä¼°æƒ…å¢ƒéœ€æ±‚çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä»£ç†çŸ¥è¯†è‡ªæˆ‘æ„è¯†çš„æ¦‚å¿µï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½¿åŸºäºLLMçš„ä»£ç†èƒ½å¤Ÿè‡ªä¸»è°ƒèŠ‚çŸ¥è¯†çš„ä½¿ç”¨ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¯å‘å¼æƒ…å¢ƒåˆ¤æ–­æ ‡å‡†ï¼Œé€šè¿‡æ ‡è®°ä»£ç†è‡ªæˆ‘æ¢ç´¢è½¨è¿¹ä¸Šçš„ç‰¹æ®Šæ ‡è®°ï¼Œæ”¶é›†è®­ç»ƒæ•°æ®ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„è§„åˆ’æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02807', 'title': 'MegaMath: Pushing the Limits of Open Math Corpora', 'url': 'https://huggingface.co/papers/2504.02807', 'abstract': 'Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.', 'score': 13, 'issue_id': 3102, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'a6dd17864afc6dca', 'authors': ['Fan Zhou', 'Zengzhi Wang', 'Nikhil Ranjan', 'Zhoujun Cheng', 'Liping Tang', 'Guowei He', 'Zhengzhong Liu', 'Eric P. Xing'], 'affiliations': ['MBZUAI', 'MegaMath'], 'pdf_title_img': 'assets/pdf/title_img/2504.02807.jpg', 'data': {'categories': ['#open_source', '#dataset', '#reasoning', '#synthetic', '#data'], 'emoji': 'ğŸ§®', 'ru': {'title': 'MegaMath: Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MegaMath - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ´ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 371 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼. MegaMath Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'MegaMath: Elevating LLMs with a Massive Math Dataset', 'desc': "This paper introduces MegaMath, a comprehensive dataset designed to enhance the mathematical reasoning capabilities of large language models (LLMs). The dataset is created by extracting and optimizing mathematical documents from the web, ensuring high quality through filtering and deduplication. Additionally, it incorporates high-quality math-related code from existing code corpora, further enriching the dataset's diversity. By synthesizing various forms of data, MegaMath provides a substantial resource of 371 billion tokens, making it the largest and highest quality open dataset for math pre-training available."}, 'zh': {'title': 'MegaMathï¼šæ•°å­¦æ¨ç†çš„å¼€æ”¾æ•°æ®é›†', 'desc': 'æ•°å­¦æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„åŸºçŸ³ï¼Œä¹Ÿæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é«˜çº§èƒ½åŠ›çš„é‡è¦åŸºå‡†ã€‚ç„¶è€Œï¼Œç›®å‰ç ”ç©¶ç•Œç¼ºä¹ä¸€ä¸ªå¼€æ”¾çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°å­¦æ•°æ®é›†ï¼Œä»¥æ»¡è¶³æ•°å­¦ä¸­å¿ƒçš„LLMé¢„è®­ç»ƒéœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†MegaMathï¼Œè¿™æ˜¯ä¸€ä¸ªä»å¤šç§æ•°å­¦ç›¸å…³æ¥æºç²¾å¿ƒç­–åˆ’çš„å¼€æ”¾æ•°æ®é›†ï¼ŒåŒ…å«3710äº¿ä¸ªæ ‡è®°ï¼Œå…·æœ‰ç°æœ‰å¼€æ”¾æ•°å­¦é¢„è®­ç»ƒæ•°æ®é›†ä¸­æœ€å¤§çš„æ•°é‡å’Œæœ€ä½³è´¨é‡ã€‚è¯¥æ•°æ®é›†é€šè¿‡é‡æ–°æå–ç½‘ç»œæ•°æ®ã€å›æ”¶æ•°å­¦ç›¸å…³ä»£ç æ•°æ®å’Œæ¢ç´¢åˆæˆæ•°æ®ç­‰ç­–ç•¥ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’Œé«˜è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03561', 'title': 'SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement', 'url': 'https://huggingface.co/papers/2504.03561', 'abstract': 'In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.', 'score': 11, 'issue_id': 3096, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '469f9f28c32e1a1a', 'authors': ['Runnan Fang', 'Xiaobin Wang', 'Yuan Liang', 'Shuofei Qiao', 'Jialong Wu', 'Zekun Xi', 'Ningyu Zhang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen'], 'affiliations': ['Alibaba Group', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03561.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#agents', '#rl'], 'emoji': 'ğŸŒ', 'ru': {'title': 'SynWorld: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…', 'desc': 'SynWorld - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ (MCTS) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SynWorld ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ² Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸Ğ»Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Agents to Explore with SynWorld', 'desc': "This paper introduces SynWorld, a framework designed to help agents improve their capabilities in unfamiliar environments. It allows agents to create and evaluate different scenarios by using multi-step actions and Monte Carlo Tree Search (MCTS) for exploration. By synthesizing possible actions, agents can better understand how to navigate and optimize their workflows. The results show that SynWorld effectively enhances agents' action knowledge in new settings, making it a valuable tool for autonomous exploration."}, 'zh': {'title': 'SynWorldï¼šèµ‹èƒ½ä»£ç†æ¢ç´¢æ–°ç¯å¢ƒçš„æ¡†æ¶', 'desc': 'åœ¨ä»£ç†ä¸ç¯å¢ƒçš„äº’åŠ¨ä¸­ï¼Œä»£ç†é€šè¿‡è§„åˆ’å’Œæ‰§è¡ŒåŠ¨ä½œæ¥æ‰©å±•å…¶èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨æ–°ç¯å¢ƒä¸­æˆ–éœ€è¦åœ¨éå¸¸è§„åŠ¨ä½œç©ºé—´ä¸­å¯¼èˆªæ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SynWorldæ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿåˆæˆå¯èƒ½çš„åœºæ™¯ï¼Œå¹¶åœ¨åŠ¨ä½œç©ºé—´å†…è¿›è¡Œå¤šæ­¥åŠ¨ä½œè°ƒç”¨ï¼ŒåŒæ—¶æ‰§è¡Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¢ç´¢ï¼Œä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–å…¶åœ¨å½“å‰ç¯å¢ƒä¸­çš„åŠ¨ä½œçŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynWorldæ˜¯å­¦ä¹ æ–°ç¯å¢ƒä¸­åŠ¨ä½œçŸ¥è¯†çš„æœ‰æ•ˆä¸”é€šç”¨çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03641', 'title': 'MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models', 'url': 'https://huggingface.co/papers/2504.03641', 'abstract': 'Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies." 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.', 'score': 8, 'issue_id': 3095, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '45da77ffd9c21caf', 'authors': ['Wulin Xie', 'Yi-Fan Zhang', 'Chaoyou Fu', 'Yang Shi', 'Bingyan Nie', 'Hongkai Chen', 'Zhang Zhang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['CASIA', 'M-M-E Project', 'NJU', 'PKU', 'Vivo'], 'pdf_title_img': 'assets/pdf/title_img/2504.03641.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (U-MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 12 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… U-MLLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… U-MLLM, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¾ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Enhancing Evaluation for Unified Multimodal Language Models', 'desc': 'This paper addresses the challenges in evaluating Unified Multimodal Language Models (U-MLLMs) due to inconsistent benchmarks and the lack of assessments for mixed-modality tasks. It introduces a comprehensive evaluation framework that includes standardized traditional task evaluations across multiple datasets and novel tasks that test multimodal reasoning capabilities. The framework assesses 12 leading U-MLLMs, revealing significant performance gaps and underscoring the necessity for improved models that can effectively manage mixed-modality tasks. The findings aim to enhance the evaluation process and guide future developments in U-MLLMs.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¿…è¦æ€§', 'desc': 'ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆU-MLLMï¼‰åŸºå‡†åœ¨è¯„ä¼°æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºä¹æ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡åŸºå‡†å’Œæ··åˆæ¨¡æ€ç”Ÿæˆçš„åŸºå‡†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°è¯„ä¼°U-MLLMã€‚è¯¥åŸºå‡†åŒ…æ‹¬æ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°å’Œäº”ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œå¦‚å›¾åƒç¼–è¾‘å’Œå¸¸è¯†é—®ç­”ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ç°æœ‰U-MLLMåœ¨æ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†å¼€å‘æ›´å¼ºå¤§æ¨¡å‹çš„å¿…è¦æ€§ï¼Œä»¥æœ‰æ•ˆå¤„ç†æ··åˆæ¨¡æ€ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02949', 'title': 'VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.02949', 'abstract': 'In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.', 'score': 8, 'issue_id': 3095, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '71423989b2bed2d8', 'authors': ['Xianwei Zhuang', 'Yuxin Xie', 'Yufan Deng', 'Dongchao Yang', 'Liming Liang', 'Jinghan Ru', 'Yuguo Yin', 'Yuexian Zou'], 'affiliations': ['School of Electronic and Computer Engineering, Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.02949.jpg', 'data': {'categories': ['#training', '#multimodal', '#rlhf', '#open_source', '#cv', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'VARGPT-v1.1 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ VARGPT. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Direct Preference Optimization (DPO). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 8,3 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2 Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹. VARGPT-v1.1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Unifying Visual Understanding and Generation with VARGPT-v1.1', 'desc': 'VARGPT-v1.1 is a cutting-edge visual autoregressive model that enhances its predecessor by integrating advanced training techniques and a larger dataset. It employs a unique combination of visual instruction tuning and reinforcement learning to improve its performance in understanding and generating images. The model also features an upgraded backbone and higher image resolution, allowing for better quality outputs and new image editing capabilities. Overall, VARGPT-v1.1 demonstrates the effectiveness of unified models in handling multimodal tasks, showcasing significant advancements in both comprehension and generation.'}, 'zh': {'title': 'ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹çš„çªç ´æ€§è¿›å±•', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†VARGPT-v1.1ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ŒåŸºäºæˆ‘ä»¬ä¹‹å‰çš„VARGPTæ¡†æ¶ã€‚è¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰ç†è§£çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œå›¾åƒåˆæˆçš„ä¸‹ä¸€ä¸ªå°ºåº¦ç”Ÿæˆçš„åŒé‡èŒƒå¼ã€‚VARGPT-v1.1é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†è¿­ä»£è§†è§‰æŒ‡ä»¤è°ƒä¼˜å’Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡è¿™äº›æ”¹è¿›ï¼ŒVARGPT-v1.1åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨ç†è§£å’Œç”ŸæˆæŒ‡æ ‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03601', 'title': 'APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay', 'url': 'https://huggingface.co/papers/2504.03601', 'abstract': 'Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io', 'score': 6, 'issue_id': 3097, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '05921cbfa42a13b4', 'authors': ['Akshara Prabhakar', 'Zuxin Liu', 'Weiran Yao', 'Jianguo Zhang', 'Ming Zhu', 'Shiyu Wang', 'Zhiwei Liu', 'Tulika Awalgaonkar', 'Haolin Chen', 'Thai Hoang', 'Juan Carlos Niebles', 'Shelby Heinecke', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.03601.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#agents', '#open_source', '#data', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'APIGen-MT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ²ÑŒÑĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ xLAM-2-fc-r, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº GPT-4 Ğ¸ Claude 3.5. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Generating High-Quality Data for AI Agents with APIGen-MT', 'desc': 'The paper presents APIGen-MT, a framework designed to generate high-quality multi-turn interaction data for training AI agents. It consists of two phases: first, creating detailed task blueprints with accurate actions using a committee of large language model (LLM) reviewers and feedback loops. In the second phase, these blueprints are turned into full interaction sequences through simulated human-agent interactions. The resulting models, particularly the xLAM-2-fc-r series, show superior performance on benchmark tests, especially in multi-turn scenarios, and the authors provide open access to the generated data and models to support further research.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆå¤šè½®äº¤äº’æ•°æ®çš„AIä»£ç†è®­ç»ƒæ¡†æ¶', 'desc': 'ä¸ºäº†è®­ç»ƒæœ‰æ•ˆçš„AIä»£ç†è¿›è¡Œå¤šè½®äº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†APIGen-MTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¯éªŒè¯å’Œå¤šæ ·åŒ–çš„å¤šè½®ä»£ç†æ•°æ®ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œé¦–å…ˆé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„å®¡å§”å‘˜ä¼šå’Œè¿­ä»£åé¦ˆç”Ÿæˆè¯¦ç»†çš„ä»»åŠ¡è“å›¾ï¼Œå¹¶æä¾›çœŸå®çš„è¡ŒåŠ¨ã€‚æ¥ç€ï¼Œè¿™äº›è“å›¾è¢«è½¬åŒ–ä¸ºå®Œæ•´çš„äº¤äº’è½¨è¿¹ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººæœºäº’åŠ¨å®ç°ã€‚æˆ‘ä»¬çš„xLAM-2-fc-rç³»åˆ—æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè½®è®¾ç½®ä¸­ï¼Œå°æ¨¡å‹çš„è¡¨ç°è¶…è¿‡äº†å¤§æ¨¡å‹ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03536', 'title': 'HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration', 'url': 'https://huggingface.co/papers/2504.03536', 'abstract': 'Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce HumanDreamer-X, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, HumanFixer is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.', 'score': 6, 'issue_id': 3097, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '9a6ad8e0086d88eb', 'authors': ['Boyuan Wang', 'Runqi Ouyang', 'Xiaofeng Wang', 'Zheng Zhu', 'Guosheng Zhao', 'Chaojun Ni', 'Guan Huang', 'Lihong Liu', 'Xingang Wang'], 'affiliations': ['GigaAI', 'Institute of Automation, Chinese Academy of Sciences', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03536.jpg', 'data': {'categories': ['#synthetic', '#cv', '#3d'], 'emoji': 'ğŸ§‘\u200dğŸ¦°', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾', 'desc': 'HumanDreamer-X - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Gaussian Splatting Ğ´Ğ»Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ HumanFixer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Human Reconstruction with HumanDreamer-X', 'desc': 'This paper presents HumanDreamer-X, a new framework for single-image human reconstruction that combines multi-view generation and 3D reconstruction into one process. The framework addresses common issues like geometric inconsistencies and blurred limbs by using 3D Gaussian Splatting for better initial geometry and appearance. Additionally, it includes a component called HumanFixer, which enhances the photorealism of the 3D models. The authors also introduce an attention modulation strategy to improve detail consistency across different views, resulting in significant improvements in image quality metrics.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šè§†å›¾ç”Ÿæˆä¸é‡å»ºï¼Œæå‡äººç±»æ¨¡å‹è´¨é‡', 'desc': 'å•å›¾åƒäººç±»é‡å»ºå¯¹æ•°å­—äººç±»å»ºæ¨¡åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†ä»ç„¶æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç›®å‰çš„æ–¹æ³•ä¾èµ–ç”Ÿæˆæ¨¡å‹åˆæˆå¤šè§†å›¾å›¾åƒä»¥è¿›è¡Œåç»­çš„3Dé‡å»ºå’ŒåŠ¨ç”»ã€‚ç„¶è€Œï¼Œä»å•ä¸ªäººä½“å›¾åƒç›´æ¥ç”Ÿæˆå¤šä¸ªè§†å›¾ä¼šå¯¼è‡´å‡ ä½•ä¸ä¸€è‡´ï¼Œé‡å»ºæ¨¡å‹ä¸­å‡ºç°è‚¢ä½“ç¢ç‰‡æˆ–æ¨¡ç³Šçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†HumanDreamer-Xï¼Œä¸€ä¸ªå°†å¤šè§†å›¾äººç±»ç”Ÿæˆå’Œé‡å»ºæ•´åˆä¸ºç»Ÿä¸€æµç¨‹çš„æ–°æ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†é‡å»º3Dæ¨¡å‹çš„å‡ ä½•ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ„Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24067', 'title': 'TransMamba: Flexibly Switching between Transformer and Mamba', 'url': 'https://huggingface.co/papers/2503.24067', 'abstract': 'Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, a novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling.', 'score': 6, 'issue_id': 3100, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'c397bc55eaf9dd26', 'authors': ['Yixing Li', 'Ruobing Xie', 'Zhen Yang', 'Xingwu Sun', 'Shuaipeng Li', 'Weidong Han', 'Zhanhui Kang', 'Yu Cheng', 'Chengzhong Xu', 'Di Wang', 'Jie Jiang'], 'affiliations': ['Tencent Hunyuan', 'The Chinese University of Hong Kong', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.24067.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#long_context'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Transformer Ğ¸ Mamba Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'TransMamba - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Transformer Ğ¸ Mamba Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ»Ğ¾ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ĞµÑ€ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ Ñ SSM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ TransMamba Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'TransMamba: Bridging Transformers and State Space Models for Efficient Sequence Processing', 'desc': 'This paper introduces TransMamba, a new framework that combines the strengths of Transformers and Mamba, a state space model, to improve efficiency in processing long sequences. By using shared parameter matrices, TransMamba can switch between attention mechanisms and state space models based on the length of the input tokens. The Memory converter is designed to ensure smooth transitions between these two methods, allowing for effective information flow. Experimental results show that TransMamba outperforms existing models in both training efficiency and performance, making it a promising solution for future sequence modeling tasks.'}, 'zh': {'title': 'TransMambaï¼šé«˜æ•ˆçš„åºåˆ—å»ºæ¨¡æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶TransMambaï¼Œæ—¨åœ¨ç»“åˆTransformerå’ŒMambaæ¨¡å‹ï¼Œä»¥æé«˜é•¿åºåˆ—å¤„ç†çš„æ•ˆç‡ã€‚é€šè¿‡å…±äº«å‚æ•°çŸ©é˜µï¼ŒTransMambaèƒ½å¤Ÿåœ¨ä¸åŒçš„tokené•¿åº¦å’Œå±‚æ¬¡ä¹‹é—´åŠ¨æ€åˆ‡æ¢æ³¨æ„åŠ›æœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ã€‚æˆ‘ä»¬è®¾è®¡äº†è®°å¿†è½¬æ¢å™¨ï¼Œå°†æ³¨æ„åŠ›è¾“å‡ºè½¬æ¢ä¸ºSSMå…¼å®¹çš„çŠ¶æ€ï¼Œç¡®ä¿ä¿¡æ¯åœ¨è½¬æ¢ç‚¹çš„æ— ç¼æµåŠ¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ·±å…¥æ¢è®¨äº†TransPointè°ƒåº¦ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03011', 'title': 'Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization', 'url': 'https://huggingface.co/papers/2504.03011', 'abstract': 'This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods.', 'score': 5, 'issue_id': 3096, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '94d3411c37993837', 'authors': ['Junying Wang', 'Jingyuan Liu', 'Xin Sun', 'Krishna Kumar Singh', 'Zhixin Shu', 'He Zhang', 'Jimei Yang', 'Nanxuan Zhao', 'Tuanfeng Y. Wang', 'Simon S. Chen', 'Ulrich Neumann', 'Jae Shin Yoon'], 'affiliations': ['Adobe Research', 'Runway', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2504.03011.jpg', 'data': {'categories': ['#inference', '#video', '#cv', '#diffusion'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Comprehensive Relighting - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‡Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ñ‚ĞµĞ»Ğ° Ğ² Ğ»ÑĞ±Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ»Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ğ½Ğ°. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¿ĞµÑ€ĞµĞ»Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Lighting Control in Images and Videos', 'desc': 'This paper presents Comprehensive Relighting, a novel method that allows for flexible control and harmonization of lighting in images or videos featuring humans. The challenge lies in the limited datasets available, which typically restrict existing models to specific scenarios like faces or static poses. To overcome this, the authors utilize a pre-trained diffusion model to create a unified approach that addresses both human relighting and background harmonization. Additionally, they introduce an unsupervised temporal lighting model that ensures consistent lighting across frames, enhancing the overall quality and realism of the relit images.'}, 'zh': {'title': 'å…¨é¢é‡å…‰ç…§ï¼šäººç±»å›¾åƒå…‰ç…§çš„å…¨èƒ½è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†å…¨é¢é‡å…‰ç…§ï¼ˆComprehensive Relightingï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿæ§åˆ¶å’Œåè°ƒæ¥è‡ªä»»æ„åœºæ™¯ä¸­äººç±»å›¾åƒæˆ–è§†é¢‘çš„å…‰ç…§çš„å…¨èƒ½æ–¹æ³•ã€‚æ„å»ºè¿™æ ·ä¸€ä¸ªé€šç”¨æ¨¡å‹éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¼ºä¹æ•°æ®é›†ï¼Œé™åˆ¶äº†ç°æœ‰åŸºäºå›¾åƒçš„é‡å…‰ç…§æ¨¡å‹åªèƒ½åº”ç”¨äºç‰¹å®šåœºæ™¯ï¼ˆä¾‹å¦‚ï¼Œé¢éƒ¨æˆ–é™æ€äººç±»ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é‡æ–°åˆ©ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºé€šç”¨å›¾åƒå…ˆéªŒï¼Œå¹¶åœ¨ç²—åˆ°ç»†çš„æ¡†æ¶ä¸­è”åˆå»ºæ¨¡äººç±»é‡å…‰ç…§å’ŒèƒŒæ™¯åè°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…¨é¢é‡å…‰ç…§åœ¨é€šç”¨æ€§å’Œå…‰ç…§æ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºäºå›¾åƒçš„äººç±»é‡å…‰ç…§å’Œåè°ƒæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02402', 'title': 'EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling', 'url': 'https://huggingface.co/papers/2504.02402', 'abstract': 'When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.', 'score': 4, 'issue_id': 3099, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'ca80ca19171ef86b', 'authors': ['Hao Yin', 'Shi Guo', 'Xu Jia', 'Xudong XU', 'Lu Zhang', 'Si Liu', 'Dong Wang', 'Huchuan Lu', 'Tianfan Xue'], 'affiliations': ['Beihang University', 'Dalian University of Technology', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.02402.jpg', 'data': {'categories': ['#training', '#dataset', '#data', '#cv'], 'emoji': 'ğŸ”Š', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±ĞµÑĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµÑĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mamba Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Sound Recovery with Event Cameras', 'desc': 'This paper presents a new method for recovering sound from visual changes caused by sound-induced vibrations. It addresses limitations in previous techniques by utilizing event camera technology, which excels at capturing high-frequency signals. The authors developed a training pipeline to create a large dataset and designed a neural network that effectively captures both spatial and temporal information from the event data. Their approach includes a specialized imaging system to enhance signal detection, leading to improved sound recovery performance in experiments.'}, 'zh': {'title': 'åˆ©ç”¨äº‹ä»¶æµå®ç°é«˜æ•ˆå£°éŸ³æ¢å¤', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„éæ¥è§¦å£°éŸ³æ¢å¤æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨äº‹ä»¶æµä¸­çš„æ—¶ç©ºä¿¡æ¯ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡æ–°çš„ä»¿çœŸç®¡é“ç”Ÿæˆäº†ä¸€ä¸ªå¤§å‹è®­ç»ƒé›†ï¼Œç„¶åè®¾è®¡äº†ä¸€ä¸ªç½‘ç»œï¼Œåˆ©ç”¨äº‹ä»¶çš„ç¨€ç–æ€§æ•æ‰ç©ºé—´ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨Mambaæ¨¡å‹æ¥å¤„ç†é•¿æœŸçš„æ—¶é—´ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç©ºé—´èšåˆæ¨¡å—ï¼Œä»¥èšåˆæ¥è‡ªä¸åŒä½ç½®çš„ä¿¡æ¯ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜ä¿¡å·è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®ä¸Šéƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03600', 'title': 'MedSAM2: Segment Anything in 3D Medical Images and Videos', 'url': 'https://huggingface.co/papers/2504.03600', 'abstract': 'Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.', 'score': 3, 'issue_id': 3103, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': 'c1ef5354c6e2cdcb', 'authors': ['Jun Ma', 'Zongxin Yang', 'Sumin Kim', 'Bihui Chen', 'Mohammed Baharoon', 'Adibvafa Fallahpour', 'Reza Asakereh', 'Hongwei Lyu', 'Bo Wang'], 'affiliations': ['AI Collaborative Centre, University Health Network', 'AI Hub, University Health Network', 'Department of Biomedical Informatics, Harvard Medical School, Harvard University, Boston, USA', 'Department of Computer Science, University of Toronto', 'Department of Laboratory Medicine and Pathobiology and Department of Computer Science, University of Toronto', 'Peter Munk Cardiac Centre, University Health Network', 'University of Toronto, Toronto, Canada', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2504.03600.jpg', 'data': {'categories': ['#data', '#healthcare', '#3d', '#dataset', '#training', '#cv'], 'emoji': '\U0001fa7b', 'ru': {'title': 'MedSAM2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'MedSAM2 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° fine-tuning Segment Anything Model 2. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 455 000 Ğ¿Ğ°Ñ€ 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ 76 000 ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ñƒ ÑĞ¿ĞµĞºÑ‚Ñ€Ñƒ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ², Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ human-in-the-loop Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 85%. MedSAM2 Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ñ ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'MedSAM2: Revolutionizing 3D Medical Segmentation', 'desc': 'This paper introduces MedSAM2, a new model designed for segmenting 3D medical images and videos, which is essential for precision medicine. It builds on the Segment Anything Model 2 and has been fine-tuned using a vast dataset of over 455,000 3D image-mask pairs and 76,000 video frames. MedSAM2 outperforms existing models in segmenting various organs and lesions, while also significantly reducing manual annotation costs by over 85% through a human-in-the-loop approach. Additionally, it is user-friendly and can be deployed on both local and cloud platforms, making it accessible for research and healthcare applications.'}, 'zh': {'title': 'MedSAM2ï¼šé«˜æ•ˆçš„3DåŒ»å­¦å›¾åƒåˆ†å‰²å·¥å…·', 'desc': 'MedSAM2æ˜¯ä¸€ç§ç”¨äº3DåŒ»å­¦å›¾åƒå’Œè§†é¢‘åˆ†å‰²çš„å¯æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡455,000ä¸ª3Då›¾åƒ-æ©è†œå¯¹å’Œ76,000å¸§çš„å¤§å‹åŒ»å­¦æ•°æ®é›†ä¸Šå¾®è°ƒSegment Anything Model 2è€Œå¼€å‘ã€‚MedSAM2åœ¨å¤šä¸ªå™¨å®˜ã€ç—…å˜å’Œæˆåƒæ¨¡å¼ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ï¼Œå¹¶é€šè¿‡äººæœºåä½œçš„æµç¨‹åˆ›å»ºäº†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿›è¡Œäº†ä¸€é¡¹å¹¿æ³›çš„ç”¨æˆ·ç ”ç©¶ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå°†äººå·¥æˆæœ¬é™ä½è¶…è¿‡85%ï¼Œå¹¶ä¸”å·²é›†æˆåˆ°å¹¿æ³›ä½¿ç”¨çš„å¹³å°ä¸­ï¼Œä¾¿äºæœ¬åœ°å’Œäº‘ç«¯éƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24310', 'title': 'BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models', 'url': 'https://huggingface.co/papers/2503.24310', 'abstract': 'In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.', 'score': 2, 'issue_id': 3097, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '890bba46601fef07', 'authors': ['Alok Abhishek', 'Lisa Erickson', 'Tushar Bandopadhyay'], 'affiliations': ['Boston, USA', 'San Francisco, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.24310.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#ethics'], 'emoji': 'âš–ï¸', 'ru': {'title': 'BEATS: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ BEATS - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸, ÑÑ‚Ğ¸ĞºĞ¸, ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ BEATS Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ¿Ğ¾ 29 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ 37,65% Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ»Ğ¸ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸. BEATS Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM, Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ğ¾ ĞµĞµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'BEATS: A Framework for Fair and Ethical AI Evaluation', 'desc': 'This paper presents BEATS, a new framework designed to evaluate Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). It introduces a comprehensive bias benchmark that assesses LLM performance using 29 different metrics, covering various biases and ethical considerations. The framework aims to quantitatively measure how LLM outputs may reinforce societal prejudices and systemic inequities. The findings reveal that a significant portion of outputs from leading models exhibit bias, underscoring the need for responsible AI practices and the potential for BEATS to guide improvements in AI ethics.'}, 'zh': {'title': 'BEATSæ¡†æ¶ï¼šæ¨åŠ¨è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½è¯„ä¼°', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†BEATSæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åè§ã€ä¼¦ç†ã€å…¬å¹³æ€§å’Œäº‹å®æ€§ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåè§åŸºå‡†ï¼Œæ¶µç›–29ä¸ªä¸åŒçš„æŒ‡æ ‡ï¼Œè¯„ä¼°LLMsåœ¨å¤šæ ·æ€§ã€è®¤çŸ¥å’Œç¤¾ä¼šåè§ç­‰æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡è¿™äº›æŒ‡æ ‡ï¼Œå¯ä»¥å®šé‡è¯„ä¼°LLMç”Ÿæˆçš„å“åº”åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¯èƒ½å»¶ç»­ç¤¾ä¼šåè§ï¼Œå¼ºåŒ–æˆ–æ‰©å¤§ç³»ç»Ÿæ€§ä¸å¹³ç­‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡BEATSæ¡†æ¶ï¼Œä¿ƒè¿›æ›´å…·ç¤¾ä¼šè´£ä»»æ„Ÿå’Œä¼¦ç†å¯¹é½çš„äººå·¥æ™ºèƒ½æ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23461', 'title': 'TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes', 'url': 'https://huggingface.co/papers/2503.23461', 'abstract': 'This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.', 'score': 59, 'issue_id': 2995, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': '00cccb2000a01b76', 'authors': ['Nikai Du', 'Zhennan Chen', 'Zhizhou Chen', 'Shan Gao', 'Xi Chen', 'Zhengkai Jiang', 'Jian Yang', 'Ying Tai'], 'affiliations': ['China Mobile', 'Nanjing University', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.23461.jpg', 'data': {'categories': ['#cv', '#dataset', '#benchmark'], 'emoji': 'ğŸ“', 'ru': {'title': 'TextCrafter: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ TextCrafter Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ĞµĞ¼. TextCrafter Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ¾ĞºÑƒÑĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CVTG-2K Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… CVTG.'}, 'en': {'title': 'TextCrafter: Mastering Complex Visual Text Generation', 'desc': 'This paper introduces Complex Visual Text Generation (CVTG), which involves creating detailed text that appears in various parts of images. The authors identify common issues in existing models, such as producing unclear or incomplete visual text. To improve this, they present TextCrafter, a new method that breaks down complex text into manageable parts and ensures that the text aligns well with the images. Additionally, TextCrafter enhances the visibility of the text during generation and is evaluated using a new dataset called CVTG-2K, showing superior performance compared to current methods.'}, 'zh': {'title': 'TextCrafterï¼šæå‡å¤æ‚è§†è§‰æ–‡æœ¬ç”Ÿæˆçš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤æ‚è§†è§‰æ–‡æœ¬ç”Ÿæˆï¼ˆCVTGï¼‰ä»»åŠ¡ï¼Œä¸»è¦å…³æ³¨åœ¨è§†è§‰å›¾åƒä¸­ç”Ÿæˆåˆ†å¸ƒåœ¨ä¸åŒåŒºåŸŸçš„å¤æ‚æ–‡æœ¬å†…å®¹ã€‚åœ¨CVTGä¸­ï¼Œå›¾åƒç”Ÿæˆæ¨¡å‹å¸¸å¸¸ä¼šæ¸²æŸ“å‡ºæ‰­æ›²ã€æ¨¡ç³Šçš„è§†è§‰æ–‡æœ¬æˆ–é—æ¼æŸäº›è§†è§‰æ–‡æœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TextCrafterï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¤šè§†è§‰æ–‡æœ¬æ¸²æŸ“æ–¹æ³•ã€‚TextCrafteré€šè¿‡é€æ­¥ç­–ç•¥å°†å¤æ‚è§†è§‰æ–‡æœ¬åˆ†è§£ä¸ºä¸åŒç»„ä»¶ï¼ŒåŒæ—¶ç¡®ä¿æ–‡æœ¬å†…å®¹ä¸å…¶è§†è§‰è½½ä½“ä¹‹é—´çš„å¼ºå¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23307', 'title': 'MoCha: Towards Movie-Grade Talking Character Synthesis', 'url': 'https://huggingface.co/papers/2503.23307', 'abstract': 'Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.', 'score': 44, 'issue_id': 2994, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': '6ce9b3642bf3ace3', 'authors': ['Cong Wei', 'Bo Sun', 'Haoyu Ma', 'Ji Hou', 'Felix Juefei-Xu', 'Zecheng He', 'Xiaoliang Dai', 'Luxin Zhang', 'Kunpeng Li', 'Tingbo Hou', 'Animesh Sinha', 'Peter Vajda', 'Wenhu Chen'], 'affiliations': ['GenAI, Meta', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23307.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#video', '#benchmark', '#story_generation'], 'emoji': 'ğŸ­', 'ru': {'title': 'MoCha: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MoCha Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ°Ñ€Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Character Animation with MoCha', 'desc': "This paper presents MoCha, a novel approach for generating talking character animations from speech and text, focusing on full character portrayal rather than just facial movements. It introduces a speech-video window attention mechanism to ensure that the generated video aligns accurately with the spoken words. To tackle the challenge of limited speech-labeled video datasets, the authors propose a joint training strategy that utilizes both speech and text-labeled data, enhancing the model's ability to generalize across various character actions. Additionally, structured prompt templates with character tags allow for multi-character dialogues, enabling AI-generated characters to engage in coherent conversations, thus advancing the field of automated cinematic storytelling."}, 'zh': {'title': 'ä¼šè¯´è¯çš„è§’è‰²ï¼šAIç”Ÿæˆç”µå½±å™äº‹çš„æ–°æ ‡å‡†', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼Œç§°ä¸ºâ€œä¼šè¯´è¯çš„è§’è‰²â€ï¼Œæ—¨åœ¨ä»è¯­éŸ³å’Œæ–‡æœ¬ç›´æ¥ç”Ÿæˆè§’è‰²åŠ¨ç”»ã€‚ä¸ä¼ ç»Ÿçš„â€œè¯´è¯å¤´â€ä¸åŒï¼Œè¿™ç§æ–¹æ³•ç”Ÿæˆçš„ä¸ä»…ä»…æ˜¯é¢éƒ¨è¡¨æƒ…ï¼Œè€Œæ˜¯å®Œæ•´çš„è§’è‰²å½¢è±¡ã€‚æˆ‘ä»¬æå‡ºäº†MoChaï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿç”Ÿæˆä¼šè¯´è¯è§’è‰²çš„æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§è¯­éŸ³-è§†é¢‘çª—å£æ³¨æ„æœºåˆ¶ï¼Œä»¥ç¡®ä¿è§†é¢‘ä¸è¯­éŸ³çš„ç²¾ç¡®åŒæ­¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ç»“æ„åŒ–çš„æç¤ºæ¨¡æ¿ï¼Œä½¿å¾—å¤šä¸ªè§’è‰²èƒ½å¤Ÿè¿›è¡ŒåŸºäºå›åˆçš„å¯¹è¯ï¼Œä»è€Œå®ç°æ›´å…·ç”µå½±æ„Ÿçš„æƒ…å¢ƒå¯¹è¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24290', 'title': 'Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model', 'url': 'https://huggingface.co/papers/2503.24290', 'abstract': 'We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.', 'score': 35, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'c3cd649c5eb9d423', 'authors': ['Jingcheng Hu', 'Yinmin Zhang', 'Qi Han', 'Daxin Jiang', 'Xiangyu Zhang', 'Heung-Yeung Shum'], 'affiliations': ['StepFun', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24290.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#training', '#rl', '#open_source', '#benchmark', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Open-Reasoner-Zero - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ vanilla PPO Ğ¸ GAE, Ğ±ĞµĞ· KL-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° DeepSeek-R1-Zero Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… AIME2024, MATH500 Ğ¸ GPQA Diamond, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ² 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ ĞºĞ¾Ğ´, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Simplifying Large-Scale Reasoning in RL with Open-Reasoner-Zero', 'desc': 'Open-Reasoner-Zero is an innovative open-source framework designed for large-scale reasoning-oriented reinforcement learning (RL) training. It utilizes a simple approach with vanilla Proximal Policy Optimization (PPO) and Generalized Advantage Estimation (GAE), achieving impressive results without complex regularization techniques. The framework demonstrates that minimalistic strategies can effectively enhance response length and benchmark performance, outperforming previous models like DeepSeek-R1-Zero. By releasing all components of the project, including source code and model weights, Open-Reasoner-Zero promotes accessibility and collaboration in the machine learning community.'}, 'zh': {'title': 'å¼€æºæ¨ç†å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆå®ç°', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Open-Reasoner-Zeroï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æºçš„å¤§è§„æ¨¡æ¨ç†å¯¼å‘å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®ç°ï¼Œé‡ç‚¹å…³æ³¨å¯æ‰©å±•æ€§ã€ç®€æ´æ€§å’Œå¯è®¿é—®æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨ç®€å•çš„PPOç®—æ³•å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å“åº”é•¿åº¦å’ŒåŸºå‡†æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®ç°ä¸DeepSeek-R1-Zeroä½¿ç”¨ç›¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨AIME2024ã€MATH500å’ŒGPQA DiamondåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶è®­ç»ƒæ•ˆç‡æ˜¾è‘—æé«˜ï¼Œä»…éœ€DeepSeek-R1-Zeroç®¡é“çš„ååˆ†ä¹‹ä¸€è®­ç»ƒæ­¥éª¤ã€‚ä¸ºäº†æ”¯æŒå¼€æºç²¾ç¥ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æºä»£ç ã€å‚æ•°è®¾ç½®ã€è®­ç»ƒæ•°æ®å’Œä¸åŒè§„æ¨¡çš„æ¨¡å‹æƒé‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24235', 'title': 'What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.24235', 'abstract': "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.", 'score': 34, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '21674468fcc8c7d5', 'authors': ['Qiyuan Zhang', 'Fuyuan Lyu', 'Zexu Sun', 'Lei Wang', 'Weixu Zhang', 'Zhihan Guo', 'Yufei Wang', 'Irwin King', 'Xue Liu', 'Chen Ma'], 'affiliations': ['Chinese University of Hong Kong', 'City University of Hong Kong', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Macquarie University', 'McGill University & MILA', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24235.jpg', 'data': {'categories': ['#survey', '#math', '#reasoning', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (TTS) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ TTS-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, Ğ³Ğ´Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ TTS. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ TTS Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Potential: The Power of Test-Time Scaling in LLMs', 'desc': 'This paper discusses the emerging focus on test-time scaling (TTS) in machine learning, particularly for large language models (LLMs). TTS enhances the problem-solving abilities of LLMs in various tasks, including specialized reasoning and open-ended questions. The authors propose a comprehensive framework that categorizes TTS research into four dimensions: what, how, where, and how well to scale. They also review existing methods and applications, highlight challenges, and suggest future research directions to improve TTS effectiveness.'}, 'zh': {'title': 'æµ‹è¯•æ—¶æ‰©å±•ï¼šæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›', 'desc': 'éšç€å¯¹é¢„è®­ç»ƒæ—¶ä»£è®¡ç®—è§„æ¨¡ï¼ˆæ•°æ®å’Œå‚æ•°ï¼‰çš„çƒ­æƒ…é€æ¸å‡é€€ï¼Œæµ‹è¯•æ—¶æ‰©å±•ï¼ˆTTSï¼‰æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶ç„¦ç‚¹ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒTTSå¯ä»¥è¿›ä¸€æ­¥æ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œåœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰ä¸“ä¸šæ¨ç†ä»»åŠ¡ä»¥åŠå¼€æ”¾å¼é—®ç­”ç­‰ä¸€èˆ¬ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çªç ´ã€‚å°½ç®¡è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶è¿…é€Ÿå¢åŠ ï¼Œä½†ä»è¿«åˆ‡éœ€è¦ä¸€é¡¹å…¨é¢çš„è°ƒæŸ¥ï¼Œä»¥æä¾›ç³»ç»Ÿçš„ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šç»´æ¡†æ¶ï¼Œæ¶µç›–TTSç ”ç©¶çš„å››ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå¹¶å¯¹æ–¹æ³•ã€åº”ç”¨åœºæ™¯å’Œè¯„ä¼°æ–¹é¢è¿›è¡Œäº†å¹¿æ³›çš„å›é¡¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24388', 'title': 'RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy', 'url': 'https://huggingface.co/papers/2503.24388', 'abstract': 'Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.', 'score': 22, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '98b80967d4757be2', 'authors': ['Zhonghan Zhao', 'Wenwei Zhang', 'Haian Huang', 'Kuikun Liu', 'Jianfei Gao', 'Gaoang Wang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24388.jpg', 'data': {'categories': ['#training', '#reasoning', '#agents', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ RIG (Reasoning and Imagination in Generalist policy). RIG Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° RIG ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¾Ğ¿ĞµÑ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ°.'}, 'en': {'title': 'Synergizing Reasoning and Imagination for Enhanced Agent Performance', 'desc': 'This paper introduces RIG, a novel end-to-end Generalist policy that combines reasoning and imagination for embodied agents in complex environments. By integrating these two capabilities, RIG enhances learning efficiency and generalization, overcoming limitations of previous models that focused on only one aspect. The authors developed a data pipeline that enriches trajectories from existing agents, allowing for joint learning of reasoning and next image generation. Experimental results demonstrate that RIG significantly improves sample efficiency and robustness, enabling agents to self-correct actions based on imagined outcomes before execution.'}, 'zh': {'title': 'æ¨ç†ä¸æƒ³è±¡çš„ååŒæå‡æ™ºèƒ½ä½“èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRIGçš„é€šç”¨ç­–ç•¥ï¼Œé¦–æ¬¡å°†æ¨ç†å’Œæƒ³è±¡èƒ½åŠ›ç»“åˆåœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“ä¸­ã€‚é€šè¿‡æ„å»ºæ•°æ®ç®¡é“ï¼Œé€æ­¥æ•´åˆå’Œä¸°å¯Œä»ç°æœ‰æ™ºèƒ½ä½“æ”¶é›†çš„è½¨è¿¹ä¸­çš„æ¨ç†å’Œæƒ³è±¡å†…å®¹ï¼ŒRIGå®ç°äº†æ›´é«˜çš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚è”åˆå­¦ä¹ æ¨ç†å’Œä¸‹ä¸€å›¾åƒç”Ÿæˆï¼Œæ˜ç¡®å»ºæ¨¡äº†æ¨ç†ã€è¡ŒåŠ¨å’Œç¯å¢ƒåŠ¨æ€ä¹‹é—´çš„å†…åœ¨å…³è”ï¼Œä½¿å¾—æ ·æœ¬æ•ˆç‡æé«˜äº†17å€ä»¥ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†ä¸æƒ³è±¡çš„ååŒä½œç”¨ä¸ä»…å¢å¼ºäº†é€šç”¨ç­–ç•¥çš„é²æ£’æ€§å’Œäº’æ“ä½œæ€§ï¼Œè¿˜æå‡äº†æ•´ä½“æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24370', 'title': 'Effectively Controlling Reasoning Models through Thinking Intervention', 'url': 'https://huggingface.co/papers/2503.24370', 'abstract': 'Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.', 'score': 13, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '5f218f08538c601f', 'authors': ['Tong Wu', 'Chong Xiang', 'Jiachen T. Wang', 'Prateek Mittal'], 'affiliations': ['NVIDIA', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24370.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#architecture', '#open_source', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Thinking Intervention'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."}, 'en': {'title': 'Enhancing LLM Reasoning with Thinking Intervention', 'desc': 'This paper introduces a new method called Thinking Intervention, which enhances the reasoning capabilities of large language models (LLMs) by allowing explicit control over their internal thought processes. By inserting or modifying specific reasoning tokens, the model can generate more accurate and contextually relevant answers. The authors conducted extensive tests on various tasks, showing that this approach leads to significant improvements in performance, such as higher accuracy in following instructions and better handling of unsafe prompts. Overall, the findings suggest that Thinking Intervention provides a valuable framework for refining how LLMs reason and respond to complex queries.'}, 'zh': {'title': 'æ€ç»´å¹²é¢„ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ€ç»´å¹²é¢„ï¼ˆThinking Interventionï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ’å…¥æˆ–ä¿®æ”¹ç‰¹å®šçš„æ€ç»´æ ‡è®°æ¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­èƒ½å¤Ÿæ›´å¥½åœ°ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ€ç»´å¹²é¢„æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æç¤ºæ–¹æ³•ï¼Œå°¤å…¶åœ¨æŒ‡ä»¤éµå¾ªå’Œæ¨ç†å±‚æ¬¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ§åˆ¶æ¨ç†è¿‡ç¨‹ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23284', 'title': 'SketchVideo: Sketch-based Video Generation and Editing', 'url': 'https://huggingface.co/papers/2503.23284', 'abstract': "Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.", 'score': 13, 'issue_id': 2996, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'd968c1da27effa84', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#multimodal', '#optimization', '#games', '#video'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞºĞµÑ‚Ñ‡Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞµÑ‚Ñ‡ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞºĞµÑ‚Ñ‡ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ DiT. Ğ”Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑĞºĞµÑ‚Ñ‡Ğ° Ğ½Ğ° Ğ²ÑĞµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Sketch Your Way to Better Video Control!', 'desc': 'This paper presents a novel approach to video generation and editing using sketch-based controls. It addresses the challenges of accurately managing global layout and motion details by introducing a memory-efficient control structure that utilizes sketch control blocks. The method allows users to draw sketches on keyframes, which are then analyzed through an inter-frame attention mechanism to ensure consistency across all video frames. The proposed system, SketchVideo, enhances the ability to generate and edit videos while preserving the integrity of unedited regions, demonstrating superior performance in controllable video tasks.'}, 'zh': {'title': 'è‰å›¾é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºè‰å›¾çš„ç©ºé—´å’Œè¿åŠ¨æ§åˆ¶åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬å’Œå›¾åƒæ¡ä»¶ä¸‹çš„å¸ƒå±€å’Œå‡ ä½•ç»†èŠ‚æ§åˆ¶é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ§åˆ¶ç»“æ„ï¼Œåˆ©ç”¨è‰å›¾æ§åˆ¶å—é¢„æµ‹è·³è¿‡çš„DiTå—çš„æ®‹å·®ç‰¹å¾ã€‚é€šè¿‡åœ¨å…³é”®å¸§ä¸Šç»˜åˆ¶è‰å›¾ï¼Œå¹¶ä½¿ç”¨è·¨å¸§æ³¨æ„æœºåˆ¶åˆ†æå…³é”®å¸§ä¸æ¯ä¸ªè§†é¢‘å¸§ä¹‹é—´çš„å…³ç³»ï¼Œå®ç°äº†æ—¶é—´ä¸Šç¨€ç–çš„è‰å›¾æ¡ä»¶ä¼ æ’­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SketchVideoåœ¨å¯æ§è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.23077', 'title': 'Efficient Inference for Large Reasoning Models: A Survey', 'url': 'https://huggingface.co/papers/2503.23077', 'abstract': "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant fieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.", 'score': 13, 'issue_id': 2995, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': 'f76d7ead3d85b37e', 'authors': ['Yue Liu', 'Jiaying Wu', 'Yufei He', 'Hongcheng Gao', 'Hongyu Chen', 'Baolong Bi', 'Jiaheng Zhang', 'Zhiqi Huang', 'Bryan Hooi'], 'affiliations': ['Beijing Jiaotong University', 'Moonshot', 'National University of Singapore', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.23077.jpg', 'data': {'categories': ['#reasoning', '#survey', '#interpretability', '#optimization', '#inference', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: ÑĞ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses Large Reasoning Models (LRMs) that enhance the reasoning capabilities of Large Language Models (LLMs) but face challenges like inefficient token usage and high memory consumption. It reviews various efficient inference methods aimed at reducing these inefficiencies while maintaining reasoning quality. The authors categorize these methods into explicit and implicit Chain-of-Thought approaches, analyzing their strengths and weaknesses. Additionally, the paper identifies ongoing challenges in the field and suggests strategies for improving inference efficiency in LRMs.'}, 'zh': {'title': 'æå‡æ¨ç†æ•ˆç‡ï¼Œä¼˜åŒ–å¤§å‹æ¨ç†æ¨¡å‹', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡å­¦ä¹ æ¨ç†æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹å¯¼è‡´äº†ä»¤ç‰Œä½¿ç”¨ã€å†…å­˜æ¶ˆè€—å’Œæ¨ç†æ—¶é—´çš„ä½æ•ˆã€‚å› æ­¤ï¼Œæœ¬æ–‡ç»¼è¿°äº†ä¸“é—¨ä¸ºLRMsè®¾è®¡çš„é«˜æ•ˆæ¨ç†æ–¹æ³•ï¼Œé‡ç‚¹åœ¨äºå‡è½»ä»¤ç‰Œä½æ•ˆï¼ŒåŒæ—¶ä¿æŒæ¨ç†è´¨é‡ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ†ç±»æ³•ï¼Œå°†æœ€è¿‘çš„æ–¹æ³•åˆ†ä¸ºä¸¤å¤§ç±»ï¼šæ˜¾å¼ç´§å‡‘çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œéšå¼æ½œåœ¨çš„æ€ç»´é“¾ï¼Œè®¨è®ºäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶åˆ†æäº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24364', 'title': 'Query and Conquer: Execution-Guided SQL Generation', 'url': 'https://huggingface.co/papers/2503.24364', 'abstract': 'We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.', 'score': 12, 'issue_id': 2997, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '2af26722887e77ac', 'authors': ['Åukasz Borchmann', 'Marek Wydmuch'], 'affiliations': ['Snowflake AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24364.jpg', 'data': {'categories': ['#optimization', '#small_models', '#dataset', '#inference', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² SQL. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼, ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº o1, o3-mini Ğ¸ DeepSeek R1, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 30 Ñ€Ğ°Ğ·. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL.'}, 'en': {'title': 'Efficient SQL Generation: Small Models, Big Results!', 'desc': 'This paper introduces a new method for generating SQL queries from natural language, enhancing accuracy in text-to-SQL tasks. The approach utilizes execution results to choose the best query from several options based on semantic consistency. It allows smaller models to outperform larger, more complex models while also cutting down inference costs significantly. This method is designed to work well with current models, making it a practical solution for improving SQL generation.'}, 'zh': {'title': 'é«˜æ•ˆç”ŸæˆSQLï¼Œæå‡æ–‡æœ¬åˆ°SQLçš„å‡†ç¡®æ€§', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¤æ‚è¾“å‡ºï¼Œæ˜¾è‘—æé«˜æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰§è¡Œç»“æœï¼Œä»å¤šä¸ªå€™é€‰æŸ¥è¯¢ä¸­é€‰æ‹©æœ€ç¬¦åˆè¯­ä¹‰çš„ä¸€é¡¹ï¼Œä½¿å¾—è¾ƒå°ã€æˆæœ¬æ•ˆç›Šé«˜çš„æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šè®¡ç®—å¯†é›†å‹çš„æ¨ç†æ–¹æ³•ï¼Œå¦‚o1ã€o3-miniå’ŒDeepSeek R1ï¼ŒåŒæ—¶å°†æ¨ç†æˆæœ¬é™ä½å¤šè¾¾30å€ã€‚å®ƒä¸ç°æœ‰æ¨¡å‹æ— ç¼é›†æˆï¼Œæä¾›äº†ä¸€æ¡å®ç”¨ä¸”å¯æ‰©å±•çš„é€šå¾€æœ€å…ˆè¿›SQLç”Ÿæˆçš„è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23829', 'title': 'Expanding RL with Verifiable Rewards Across Diverse Domains', 'url': 'https://huggingface.co/papers/2503.23829', 'abstract': "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.", 'score': 12, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '2c875f8335892dfd', 'authors': ['Yi Su', 'Dian Yu', 'Linfeng Song', 'Juntao Li', 'Haitao Mi', 'Zhaopeng Tu', 'Min Zhang', 'Dong Yu'], 'affiliations': ['Soochow University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.23829.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rlhf', '#training', '#rl', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RLVR: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ°, Ñ…Ğ¸Ğ¼Ğ¸Ñ, Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ğ² Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑĞ³ĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµĞ¶Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Expanding RLVR: From Coding to Real-World Applications', 'desc': "This paper explores the use of Reinforcement Learning with Verifiable Rewards (RLVR) in diverse fields like medicine and economics, where traditional structured answers may not be available. The authors find that different large language models (LLMs) often agree on binary judgments when objective reference answers are present, suggesting that extensive annotations may not be necessary for training reward models. To enhance RLVR's effectiveness, they introduce model-based soft scoring to better handle unstructured reference answers. Their experiments demonstrate that a distilled generative reward model can effectively verify rewards across domains, leading to improved performance of RL policies compared to existing state-of-the-art models."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ çš„å¯éªŒè¯å¥–åŠ±ï¼šè·¨é¢†åŸŸåº”ç”¨çš„æ–°å¯èƒ½', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦æ¨ç†å’Œç¼–ç ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨å°šæœªæ·±å…¥æ¢ç´¢ã€‚æˆ‘ä»¬ç ”ç©¶äº†RLVRåœ¨åŒ»å­¦ã€åŒ–å­¦ã€å¿ƒç†å­¦å’Œç»æµå­¦ç­‰å¤šæ ·åŒ–é¢†åŸŸçš„æ‰©å±•ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å­˜åœ¨å®¢è§‚å‚è€ƒç­”æ¡ˆæ—¶ï¼Œä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äºŒå…ƒåˆ¤æ–­ä¸Šé«˜åº¦ä¸€è‡´ï¼Œè¿™è¡¨æ˜è®­ç»ƒç‰¹å®šé¢†åŸŸå¥–åŠ±æ¨¡å‹ä¸ä¸€å®šéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨ã€‚é€šè¿‡å°†åŸºäºæ¨¡å‹çš„è½¯è¯„åˆ†çº³å…¥RLVRï¼Œæˆ‘ä»¬æé«˜äº†å…¶çµæ´»æ€§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è’¸é¦ç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨è·¨é¢†åŸŸéªŒè¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19901', 'title': 'TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization', 'url': 'https://huggingface.co/papers/2503.19901', 'abstract': 'Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/', 'score': 12, 'issue_id': 3000, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'c5dd40417e2f952a', 'authors': ['Liang Pan', 'Zeshi Yang', 'Zhiyang Dou', 'Wenjia Wang', 'Buzhen Huang', 'Bo Dai', 'Taku Komura', 'Jingbo Wang'], 'affiliations': ['Feeling AI', 'Independent Researcher', 'Shanghai AI Laboratory', 'Southeast University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19901.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#transfer_learning', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'TokenHSI - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹ (HSI). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸Ğ¾Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ° ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞµĞ³Ğ¾ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… HSI.'}, 'en': {'title': 'Unifying Skills for Enhanced Human-Scene Interactions', 'desc': 'This paper introduces TokenHSI, a novel transformer-based policy designed to enhance Human-Scene Interactions (HSI) by integrating multiple skills into a single framework. Unlike traditional methods that rely on separate controllers for each task, TokenHSI utilizes a shared token for humanoid proprioception, allowing for effective knowledge sharing across different interaction tasks. The architecture supports variable length inputs, making it adaptable to new scenarios and capable of coordinating complex actions, such as sitting while carrying an object. Experimental results show that TokenHSI significantly improves the versatility and adaptability of HSI tasks, paving the way for more sophisticated applications in computer animation and embodied AI.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæŠ€èƒ½çš„äººç±»åœºæ™¯äº¤äº’ç­–ç•¥', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTokenHSIçš„ç»Ÿä¸€å˜æ¢å™¨åŸºç¡€ç­–ç•¥ï¼Œç”¨äºåˆæˆå¤šæ ·ä¸”ç‰©ç†ä¸Šåˆç†çš„äººç±»åœºæ™¯äº¤äº’ï¼ˆHSIï¼‰ã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¸ºç‰¹å®šäº¤äº’ä»»åŠ¡å¼€å‘ç‹¬ç«‹æ§åˆ¶å™¨ï¼Œè¿™é™åˆ¶äº†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚TokenHSIé€šè¿‡å°†äººä½“æœ¬ä½“æ„ŸçŸ¥å»ºæ¨¡ä¸ºå…±äº«çš„ç‹¬ç«‹æ ‡è®°ï¼Œå¹¶ç»“åˆä¸åŒçš„ä»»åŠ¡æ ‡è®°ï¼Œä¿ƒè¿›äº†å¤šæŠ€èƒ½çš„ç»Ÿä¸€å’Œçµæ´»é€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§HSIä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†å¤šæ ·æ€§ã€é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24115', 'title': 'TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection', 'url': 'https://huggingface.co/papers/2503.24115', 'abstract': 'The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.', 'score': 9, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '61845428f5c3d9df', 'authors': ['Zhiming Ma', 'Peidong Wang', 'Minhua Huang', 'Jingpeng Wang', 'Kai Wu', 'Xiangzhao Lv', 'Yachun Pang', 'Yin Yang', 'Wenjie Tang', 'Yuchen Kang'], 'affiliations': ['China Mobile Internet Company Ltd. Guangzhou, Guangdong, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.24115.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#synthetic', '#open_source', '#benchmark', '#data'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TeleAntiFraud-28k - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚ĞµĞ»ĞµĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ASR Ğ¸ TTS, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 28,511 Ğ¿Ğ°Ñ€ Ñ€ĞµÑ‡ÑŒ-Ñ‚ĞµĞºÑÑ‚ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½ Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ TeleAntiFraud-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SFT.'}, 'en': {'title': 'Revolutionizing Telecom Fraud Detection with TeleAntiFraud-28k', 'desc': 'This paper introduces TeleAntiFraud-28k, a novel dataset designed to enhance telecom fraud detection by combining audio signals with textual analysis. The dataset is created using three innovative strategies, including privacy-preserved text generation from speech recordings and semantic enhancement through large language models. It consists of over 28,000 annotated speech-text pairs, enabling tasks like scenario classification and fraud detection. Additionally, the authors provide a benchmark for evaluating model performance and a fine-tuned model for practical applications, promoting further research in multimodal anti-fraud techniques.'}, 'zh': {'title': 'æ„å»ºç”µä¿¡æ¬ºè¯ˆæ£€æµ‹çš„æ–°åŸºçŸ³', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†TeleAntiFraud-28kæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºç”µä¿¡æ¬ºè¯ˆåˆ†æè®¾è®¡çš„å¼€æºéŸ³é¢‘-æ–‡æœ¬æ…¢æ€è€ƒæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸‰ç§ç­–ç•¥æ„å»ºï¼Œç¡®ä¿äº†æ•°æ®çš„éšç§ä¿æŠ¤å’ŒçœŸå®åœºæ™¯çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†TeleAntiFraud-Benchè¯„ä¼°åŸºå‡†ï¼Œä»¥ä¾¿ç³»ç»Ÿåœ°æµ‹è¯•æ¨¡å‹åœ¨ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æ­¤é¡¹å·¥ä½œä¸ºå¤šæ¨¡æ€åæ¬ºè¯ˆç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼ŒåŒæ—¶è§£å†³äº†æ•°æ®éšç§å’Œåœºæ™¯å¤šæ ·æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18809', 'title': 'Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code', 'url': 'https://huggingface.co/papers/2503.18809', 'abstract': 'In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit "reasoning" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs.', 'score': 9, 'issue_id': 2994, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '28288adc69a019ac', 'authors': ['Augusto B. CorrÃªa', 'AndrÃ© G. Pereira', 'Jendrik Seipp'], 'affiliations': ['Federal University of Rio Grande do Sul', 'LinkÃ¶ping University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.18809.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. LLM ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ² Ğ²Ğ¸Ğ´Ğµ Python-ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Empowering LLMs with Domain-Specific Heuristics for Better Planning', 'desc': 'This paper addresses the limitations of large language models (LLMs) in planning tasks, which often lead to incorrect and non-generalizable plans. The authors propose a novel approach where LLMs generate domain-specific heuristic functions in Python code, which are then evaluated using a greedy best-first search algorithm. Their method demonstrates that LLM-generated heuristics can outperform traditional domain-independent heuristics and compete with advanced learning algorithms in planning. The results indicate that these heuristics are not only efficient but also provide more informative guidance in certain planning domains.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è§„åˆ’èƒ½åŠ›', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§äººå·¥æ™ºèƒ½é—®é¢˜ä¸Šå±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨è¯¦ç»†å®šä¹‰è§„åˆ’ä»»åŠ¡çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬åœ¨è§„åˆ’æ–¹é¢ä»ç„¶ä¸å¯é ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨LLMsç”Ÿæˆæ­£ç¡®çš„è§„åˆ’ï¼Œå³ä½¿å¯¹äºè¶Šæ¥è¶Šå¤§çš„åˆ†å¸ƒå¤–ä»»åŠ¡ã€‚é€šè¿‡ç”Ÿæˆé¢†åŸŸç›¸å…³çš„å¯å‘å¼å‡½æ•°å¹¶åœ¨è´ªå©ªä¼˜å…ˆæœç´¢ä¸­è¯„ä¼°ï¼ŒLLMç”Ÿæˆçš„å¯å‘å¼å‡½æ•°åœ¨è§£å†³æœªè§æµ‹è¯•ä»»åŠ¡æ–¹é¢è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„é¢†åŸŸæ— å…³å¯å‘å¼æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22673', 'title': 'ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action Models', 'url': 'https://huggingface.co/papers/2503.22673', 'abstract': 'Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to facilitate research in the community.', 'score': 8, 'issue_id': 3011, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '32c8476678c711ba', 'authors': ['Jianguo Zhang', 'Thai Hoang', 'Ming Zhu', 'Zuxin Liu', 'Shiyu Wang', 'Tulika Awalgaonkar', 'Akshara Prabhakar', 'Haolin Chen', 'Weiran Yao', 'Zhiwei Liu', 'Juntao Tan', 'Juan Carlos Niebles', 'Shelby Heinecke', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.22673.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#agents', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ActionStudio: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ActionStudio - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LoRA Ğ¸ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ. ActionStudio Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Autonomous Agents with ActionStudio', 'desc': 'This paper introduces ActionStudio, a new framework designed to improve the training of large action models for autonomous agents. It addresses the challenges of diverse environments and complex data by providing a standardized format for agent trajectories. ActionStudio supports various training methods, including LoRA and full fine-tuning, and offers tools for data preprocessing and verification. The framework has been validated on both public and industry benchmarks, showing strong performance and scalability, and is open-sourced for community use.'}, 'zh': {'title': 'ActionStudioï¼šæå‡å¤§å‹åŠ¨ä½œæ¨¡å‹è®­ç»ƒçš„åˆ©å™¨', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ActionStudioï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”å¯æ‰©å±•çš„æ•°æ®å’Œè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå¤§å‹åŠ¨ä½œæ¨¡å‹çš„è®­ç»ƒã€‚ActionStudioé€šè¿‡æ ‡å‡†åŒ–æ ¼å¼ç»Ÿä¸€äº†ä¸åŒçš„ä»£ç†è½¨è¿¹ï¼Œæ”¯æŒå¤šç§è®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬LoRAã€å®Œå…¨å¾®è°ƒå’Œåˆ†å¸ƒå¼è®¾ç½®ã€‚è¯¥æ¡†æ¶è¿˜é›†æˆäº†å¼ºå¤§çš„é¢„å¤„ç†å’ŒéªŒè¯å·¥å…·ï¼Œä»¥æé«˜è®­ç»ƒçš„æ•ˆç‡å’Œå¯é æ€§ã€‚æˆ‘ä»¬åœ¨å…¬å…±å’Œå®é™…è¡Œä¸šåŸºå‡†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½å’Œå®ç”¨çš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21694', 'title': 'Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data', 'url': 'https://huggingface.co/papers/2503.21694', 'abstract': 'It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only 2.5% trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at https://github.com/theEricMa/TriplaneTurbo.', 'score': 8, 'issue_id': 3000, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '703f61255714367b', 'authors': ['Zhiyuan Ma', 'Xinyue Liang', 'Rongyuan Wu', 'Xiangyu Zhu', 'Zhen Lei', 'Lei Zhang'], 'affiliations': ['Center for Artificial Intelligence and Robotics, HKISI CAS', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, UCAS', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21694.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#3d', '#diffusion', '#open_source'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Progressive Rendering Distillation (PRD). PRD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Stable Diffusion Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TriplaneTurbo, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ° 1.2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹.'}, 'en': {'title': 'Fast and High-Quality 3D Mesh Generation from Text Prompts', 'desc': 'This paper introduces a new method called Progressive Rendering Distillation (PRD) to create high-quality 3D meshes from text prompts quickly. PRD allows training without needing 3D ground-truth data by using multi-view diffusion models to distill textures and geometries into 3D outputs. The method enhances the efficiency of the generation process, enabling the model to produce 3D meshes in just 1.2 seconds while maintaining high quality. The resulting model, TriplaneTurbo, shows significant improvements over previous text-to-3D generators in both speed and output quality.'}, 'zh': {'title': 'å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡3Dç½‘æ ¼çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ¡ˆï¼Œç§°ä¸ºæ¸è¿›æ¸²æŸ“è’¸é¦ï¼ˆPRDï¼‰ï¼Œæ—¨åœ¨ä»æ–‡æœ¬æç¤ºä¸­å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼ã€‚é€šè¿‡è’¸é¦å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ŒPRDæ¶ˆé™¤äº†å¯¹3DçœŸå®æ•°æ®çš„éœ€æ±‚ï¼Œä½¿å¾—è®­ç»ƒæ•°æ®çš„æ‰©å±•å˜å¾—æ›´åŠ å®¹æ˜“ã€‚è¯¥æ–¹æ³•åˆ©ç”¨U-Neté€æ­¥å»å™ªï¼Œå¹¶å°†å»å™ªåçš„æ½œåœ¨è¡¨ç¤ºè§£ç ä¸º3Dè¾“å‡ºï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚æœ€ç»ˆï¼ŒPRDè®­ç»ƒçš„TriplaneTurboç”Ÿæˆå™¨åœ¨æ•ˆç‡å’Œè´¨é‡ä¸Šå‡ä¼˜äºä¹‹å‰çš„æ–‡æœ¬åˆ°3Dç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿåœ¨1.2ç§’å†…ç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24391', 'title': 'Easi3R: Estimating Disentangled Motion from DUSt3R Without Training', 'url': 'https://huggingface.co/papers/2503.24391', 'abstract': 'Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major bottleneck for training a highly generalizable 4D model. This constraint has driven conventional 4D methods to fine-tune 3D models on scalable dynamic video data with additional geometric priors such as optical flow and depths. In this work, we take an opposite path and introduce Easi3R, a simple yet efficient training-free method for 4D reconstruction. Our approach applies attention adaptation during inference, eliminating the need for from-scratch pre-training or network fine-tuning. We find that the attention layers in DUSt3R inherently encode rich information about camera and object motion. By carefully disentangling these attention maps, we achieve accurate dynamic region segmentation, camera pose estimation, and 4D dense point map reconstruction. Extensive experiments on real-world dynamic videos demonstrate that our lightweight attention adaptation significantly outperforms previous state-of-the-art methods that are trained or finetuned on extensive dynamic datasets. Our code is publicly available for research purpose at https://easi3r.github.io/', 'score': 3, 'issue_id': 3001, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '48e601b0440aa5d3', 'authors': ['Xingyu Chen', 'Yue Chen', 'Yuliang Xiu', 'Andreas Geiger', 'Anpei Chen'], 'affiliations': ['Max Planck Institute for Intelligent Systems', 'University of Tubingen, Tubingen AI Center', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24391.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#open_source', '#3d', '#video'], 'emoji': 'ğŸŒŸ', 'ru': {'title': 'Easi3R: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Easi3R Ğ´Ğ»Ñ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DUSt3R Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. Easi3R Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ 4D ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Easi3R Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ….'}, 'en': {'title': 'Easi3R: Efficient 4D Reconstruction Without Pre-training', 'desc': 'This paper presents Easi3R, a novel method for 4D reconstruction that does not require extensive pre-training or fine-tuning of models. Instead, it utilizes attention adaptation during inference to effectively segment dynamic regions and estimate camera poses. The authors leverage the inherent information encoded in the attention layers of the DUSt3R model, allowing for accurate reconstruction of dense point clouds in dynamic scenes. Experimental results show that Easi3R outperforms existing methods that rely on large dynamic datasets, highlighting its efficiency and effectiveness.'}, 'zh': {'title': 'Easi3Rï¼šæ— éœ€è®­ç»ƒçš„é«˜æ•ˆ4Dé‡å»ºæ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºEasi3Rçš„4Dé‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€é¢„è®­ç»ƒæˆ–å¾®è°ƒç½‘ç»œï¼Œåˆ©ç”¨æ³¨æ„åŠ›é€‚åº”æŠ€æœ¯è¿›è¡Œæ¨ç†ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡åŠ¨æ€è§†é¢‘æ•°æ®ä¸åŒï¼ŒEasi3Ré€šè¿‡è§£è€¦æ³¨æ„åŠ›å›¾ï¼Œå‡†ç¡®å®ç°åŠ¨æ€åŒºåŸŸåˆ†å‰²ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œ4Dç¨ å¯†ç‚¹äº‘é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEasi3Råœ¨çœŸå®åŠ¨æ€è§†é¢‘ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä»¥å¾€çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„è½»é‡çº§è®¾è®¡ä½¿å…¶åœ¨å¤„ç†åŠ¨æ€åœºæ™¯æ—¶æ›´åŠ é«˜æ•ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23730', 'title': 'KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language', 'url': 'https://huggingface.co/papers/2503.23730', 'abstract': 'The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA', 'score': 3, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '1d3d53298afe6cce', 'authors': ['Yoonshik Kim', 'Jaeyoon Jung'], 'affiliations': ['MAUM AI Inc. / Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.23730.jpg', 'data': {'categories': ['#open_source', '#low_resource', '#benchmark', '#multilingual'], 'emoji': 'ğŸ‡°ğŸ‡·', 'ru': {'title': 'KOFFVQA: Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº KOFFVQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 275 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ 10 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ VLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶ĞµĞ½, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ VLM.'}, 'en': {'title': 'KOFFVQA: Reliable Evaluation for Korean Vision-Language Models', 'desc': 'This paper introduces KOFFVQA, a new benchmark for evaluating Large Vision-Language Models (VLMs) specifically in the Korean language. It addresses the limitations of current evaluation methods that either restrict responses or rely on subjective judge models. KOFFVQA features 275 visual question-answering tasks with clear grading criteria that enhance reliability and objectivity in assessments. The authors demonstrate that their approach provides a more dependable evaluation of VLMs compared to existing methods, making it suitable for both large and small models.'}, 'zh': {'title': 'KOFFVQAï¼šéŸ©è¯­è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é è¯„ä¼°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¯„ä¼°åŸºå‡†ï¼Œåä¸ºKOFFVQAï¼Œä¸“é—¨é’ˆå¯¹éŸ©è¯­ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€ä¾èµ–äºé¢„è®¾çš„å›ç­”é€‰é¡¹æˆ–è¯„åˆ¤æ¨¡å‹ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸»è§‚ä¸”ä¸å¯é ã€‚KOFFVQAåŒ…å«275ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…æœ‰å›¾åƒå’Œæ¶µç›–VLMæ€§èƒ½çš„10ä¸ªè¯„ä¼°æ ‡å‡†ã€‚é€šè¿‡å®¢è§‚çš„è¯„ä¼°æ ‡å‡†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ›´å¯é åœ°è¯„ä¼°æ¨¡å‹ï¼Œå³ä½¿æ˜¯å°å‹å¼€æºæ¨¡å‹ä¹Ÿèƒ½æœ‰æ•ˆä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20286', 'title': 'Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization', 'url': 'https://huggingface.co/papers/2503.20286', 'abstract': 'Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.', 'score': 3, 'issue_id': 2994, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'bf1debfaa462fca8', 'authors': ['Zhenyu Liang', 'Hao Li', 'Naiwei Yu', 'Kebin Sun', 'Ran Cheng'], 'affiliations': ['Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Data Science and Artificial Intelligence and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.20286.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#robotics', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ¢ĞµĞ½Ğ·Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ EMO: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (EMO) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ½Ğ° GPU. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğº Ñ‚Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼ EMO: NSGA-III, MOEA/D Ğ¸ HypE. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 1113 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ CPU Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ° Ğ½Ğ° GPU Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Accelerating EMO with GPU Tensorization for Enhanced Performance', 'desc': 'This paper addresses the limitations of traditional evolutionary multiobjective optimization (EMO) algorithms when faced with complex and large-scale problems. It highlights the lack of hardware acceleration in existing EMO approaches and proposes a novel method to parallelize these algorithms using GPUs through tensorization. By transforming EMO data structures into tensor representations, the authors enable efficient GPU computing, resulting in significant performance improvements. The proposed tensorized EMO algorithms demonstrate remarkable speedups while maintaining solution quality in multiobjective robot control tasks.'}, 'zh': {'title': 'å¼ é‡åŒ–æå‡EMOç®—æ³•æ€§èƒ½ï¼ŒGPUåŠ é€Ÿæ˜¾è‘—', 'desc': 'è¿›åŒ–å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆEMOï¼‰åœ¨è¿‡å»äºŒåå¹´å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†éšç€é—®é¢˜è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œä¼ ç»Ÿçš„EMOç®—æ³•é¢ä¸´æ€§èƒ½é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¼ é‡åŒ–æ–¹æ³•åœ¨GPUä¸Šå¹¶è¡ŒåŒ–EMOç®—æ³•çš„æ–¹æ¡ˆï¼Œä»¥è§£å†³ä¼ ç»Ÿç®—æ³•çš„å¹¶è¡Œæ€§å’Œå¯æ‰©å±•æ€§ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¼ é‡åŒ–ï¼ŒEMOç®—æ³•çš„æ•°æ®ç»“æ„å’Œæ“ä½œè¢«è½¬åŒ–ä¸ºç®€æ´çš„å¼ é‡è¡¨ç¤ºï¼Œä»è€Œå®ç°äº†GPUè®¡ç®—çš„è‡ªåŠ¨åˆ©ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼ é‡åŒ–çš„EMOç®—æ³•åœ¨é€Ÿåº¦ä¸Šæ¯”åŸºäºCPUçš„ç®—æ³•å¿«äº†å¤šè¾¾1113å€ï¼ŒåŒæ—¶ä¿æŒäº†è§£å†³æ–¹æ¡ˆçš„è´¨é‡ï¼Œå¹¶æœ‰æ•ˆå¤„ç†å¤æ‚çš„å¤šç›®æ ‡æœºå™¨äººæ§åˆ¶ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14941', 'title': 'UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation', 'url': 'https://huggingface.co/papers/2503.14941', 'abstract': 'Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves a Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences.', 'score': 3, 'issue_id': 3000, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '0d6cdad4ff3e795e', 'authors': ['Qihui Zhang', 'Munan Ning', 'Zheyuan Liu', 'Yanbo Wang', 'Jiayi Ye', 'Yue Huang', 'Shuo Yang', 'Xiao Chen', 'Yibing Song', 'Li Yuan'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'School of Electrical and Computer Engineering, Peking University', 'Tsinghua University', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2503.14941.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#interpretability', '#alignment'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Unsupervised Peer review MLLM Evaluation (UPME), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. UPME Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ UPME Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Automating VQA Evaluations with Unsupervised Peer Review', 'desc': 'This paper introduces a new framework called Unsupervised Peer review MLLM Evaluation (UPME) to improve the evaluation of Multimodal Large Language Models (MLLMs) in Visual Question Answering (VQA). The framework reduces the need for human involvement by allowing models to automatically generate questions and assess answers from other models using only image data. To address biases in automated evaluations, a vision-language scoring system is implemented, focusing on response correctness, visual understanding, and image-text correlation. Experimental results show that UPME closely aligns with human evaluations, achieving high correlation scores on benchmark datasets.'}, 'zh': {'title': 'æ— ç›‘ç£åŒè¡Œè¯„å®¡ï¼Œæå‡è§†è§‰é—®ç­”è¯„ä¼°çš„æ•ˆç‡ä¸å…¬æ­£æ€§', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åœ¨è§£å†³è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œæ¨åŠ¨äº†å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå®¢è§‚è¯„ä¼°çš„æ–°ç ”ç©¶æ–¹å‘ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ç”±äºéœ€è¦å¤§é‡äººåŠ›è®¾è®¡é—®ç­”å¯¹ï¼Œé™åˆ¶äº†è¯„ä¼°çš„è§„æ¨¡å’ŒèŒƒå›´ã€‚è™½ç„¶è‡ªåŠ¨åŒ–çš„MLLMè¯„ä¼°æ–¹æ³•è¯•å›¾å‡å°‘äººåŠ›å·¥ä½œé‡ï¼Œä½†å¾€å¾€ä¼šå¼•å…¥åè§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„åŒè¡Œè¯„å®¡MLLMè¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨å›¾åƒæ•°æ®è‡ªåŠ¨ç”Ÿæˆé—®é¢˜ï¼Œå¹¶å¯¹å…¶ä»–æ¨¡å‹çš„ç­”æ¡ˆè¿›è¡Œè¯„ä¼°ï¼Œä»è€Œæœ‰æ•ˆå‡è½»å¯¹äººåŠ›çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23022', 'title': 'MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs', 'url': 'https://huggingface.co/papers/2503.23022', 'abstract': 'In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, a novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) a transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) a flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with a predefined number of faces. By utilizing the diffusion model for the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds (35times faster than existing baselines). Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation.', 'score': 2, 'issue_id': 3002, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '4bb15e0559669bbb', 'authors': ['Xianglong He', 'Junyi Chen', 'Di Huang', 'Zexiang Liu', 'Xiaoshui Huang', 'Wanli Ouyang', 'Chun Yuan', 'Yangguang Li'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'The University of Sydney', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2503.23022.jpg', 'data': {'categories': ['#3d', '#optimization', '#diffusion', '#games'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'MeshCraft: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ 3D-ÑĞµÑ‚Ğ¾Ğº Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹', 'desc': 'MeshCraft - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½ĞµĞ¹. ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· VAE Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµÑ‚Ğ¾Ğº, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ³Ñ€Ğ°Ğ½ĞµĞ¹. MeshCraft Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-ÑĞµÑ‚Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑĞµÑ‚ĞºÑƒ Ğ¸Ğ· 800 Ğ³Ñ€Ğ°Ğ½ĞµĞ¹ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 3,2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ShapeNet Ğ¸ Objaverse, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'MeshCraft: Fast and Controlled 3D Mesh Generation', 'desc': 'This paper presents MeshCraft, a new framework for generating 3D mesh topologies efficiently and with control over the number of faces. Unlike previous methods that use auto-regressive techniques, MeshCraft employs a transformer-based variational autoencoder (VAE) and a flow-based diffusion transformer to create high-quality meshes quickly. The framework allows for the simultaneous generation of entire mesh structures, significantly speeding up the process to just 3.2 seconds for an 800-face mesh. Experimental results show that MeshCraft outperforms existing methods in both quality and speed, making it a valuable tool for 3D artists.'}, 'zh': {'title': 'MeshCraftï¼šé«˜æ•ˆå¯æ§çš„3Dç½‘æ ¼ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'åœ¨3Då†…å®¹åˆ›ä½œé¢†åŸŸï¼ŒMeshCraftæ˜¯ä¸€ç§æ–°é¢–çš„é«˜æ•ˆå¯æ§ç½‘æ ¼ç”Ÿæˆæ¡†æ¶ã€‚å®ƒåˆ©ç”¨è¿ç»­ç©ºé—´æ‰©æ•£ç”Ÿæˆç¦»æ•£ä¸‰è§’é¢ï¼Œå…‹æœäº†ä¼ ç»Ÿè‡ªå›å½’æ–¹æ³•çš„ç”Ÿæˆé€Ÿåº¦æ…¢å’Œé¢æ•°ä¸å¯æ§çš„ç¼ºé™·ã€‚MeshCraftç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼šåŸºäºå˜æ¢å™¨çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œæ¡ä»¶åŒ–é¢æ•°çš„æµæ‰©æ•£å˜æ¢å™¨ã€‚é€šè¿‡åŒæ—¶ç”Ÿæˆæ•´ä¸ªç½‘æ ¼æ‹“æ‰‘ï¼ŒMeshCraftåœ¨ç”Ÿæˆé«˜è´¨é‡3Dç½‘æ ¼æ—¶é€Ÿåº¦æ˜¾è‘—æé«˜ï¼Œèƒ½å¤Ÿåœ¨3.2ç§’å†…ç”Ÿæˆ800é¢ç½‘æ ¼ï¼Œé€Ÿåº¦æ¯”ç°æœ‰æ–¹æ³•å¿«35å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22655', 'title': 'Unicorn: Text-Only Data Synthesis for Vision Language Model Training', 'url': 'https://huggingface.co/papers/2503.22655', 'abstract': 'Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git.', 'score': 2, 'issue_id': 3005, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': 'a724544e9c0362b6', 'authors': ['Xiaomin Yu', 'Pengxiang Ding', 'Wenjie Zhang', 'Siteng Huang', 'Songyang Gao', 'Chengwei Qin', 'Kejian Wu', 'Zhaoxin Fan', 'Ziyue Qiao', 'Donglin Wang'], 'affiliations': ['Beihang University', 'Nanyang Technological University', 'Shanghai AI Lab', 'The Great Bay University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22655.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#data', '#reasoning', '#synthetic'], 'emoji': 'ğŸ¦„', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ’Ğ¯Ğœ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Unicorn-1.2M Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Unicorn-471K-Instruction Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Synthesize High-Quality Multimodal Data from Text Alone!', 'desc': 'This paper presents a novel framework for synthesizing multimodal training data for vision-language models (VLMs) using only text. The proposed three-stage process generates high-quality image-text pairs without the need for real images, significantly reducing costs. In the first stage, diverse captions are created from sparse seeds using large language models, followed by the generation of instruction-tuning tasks in the second stage. Finally, the textual representations are transformed into visual representations, resulting in two datasets: Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, enhancing the training of VLMs.'}, 'zh': {'title': 'æ— å›¾åƒé«˜æ•ˆåˆæˆå¤šæ¨¡æ€æ•°æ®', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è·¨é›†æˆçš„ä¸‰é˜¶æ®µå¤šæ¨¡æ€æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬å¯¹ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹æ‰©å±•ç¨€ç–çš„æ–‡æœ¬ç§å­ï¼Œåˆæˆäº†120ä¸‡æ¡è¯­ä¹‰å¤šæ ·çš„é«˜è´¨é‡æ–‡æœ¬æè¿°ã€‚ç¬¬äºŒé˜¶æ®µå°†47ä¸‡æ¡æ–‡æœ¬æè¿°è½¬åŒ–ä¸ºå¤šè½®æŒ‡ä»¤è°ƒä¼˜ä»»åŠ¡ï¼Œä»¥æ”¯æŒå¤æ‚æ¨ç†ã€‚æœ€åï¼Œç¬¬ä¸‰é˜¶æ®µå°†æ–‡æœ¬è¡¨ç¤ºè½¬æ¢ä¸ºè§†è§‰è¡¨ç¤ºï¼Œä»è€Œç”Ÿæˆå¤šæ ·çš„åˆæˆå›¾åƒè¡¨ç¤ºï¼Œæä¾›äº†ä¸€ç§æ— éœ€çœŸå®å›¾åƒçš„é«˜æ•ˆè®­ç»ƒæ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23913', 'title': 'Entropy-Based Adaptive Weighting for Self-Training', 'url': 'https://huggingface.co/papers/2503.23913', 'abstract': 'The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs a mapping function with a tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K, EAST attains a further 1-2% performance boost compared to the vanilla method.', 'score': 1, 'issue_id': 3002, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'b7e8ee7260c71bb7', 'authors': ['Xiaoxuan Wang', 'Yihe Deng', 'Mingyu Derek Ma', 'Wei Wang'], 'affiliations': ['University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.23913.jpg', 'data': {'categories': ['#training', '#reasoning', '#math', '#benchmark', '#optimization'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ EAST (Entropy-Based Adaptive Weighting for Self-Training), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼. EAST Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ Ñ€ĞµĞ·ĞºĞ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EAST Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 1% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MATH Ğ¸ Ğ½Ğ° 1-2% Ğ½Ğ° GSM8K Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Model Reasoning with Adaptive Uncertainty Weighting', 'desc': "This paper explores how large language models can improve their mathematical problem-solving skills by using self-generated reasoning paths. The authors introduce a new method called Entropy-Based Adaptive Weighting for Self-Training (EAST), which focuses on prioritizing uncertain data during the training process. By adjusting the importance of different training examples based on the model's uncertainty, EAST helps the model learn from more challenging and informative cases. The results show that EAST leads to slight performance improvements on benchmark tests compared to traditional self-training methods."}, 'zh': {'title': 'è‡ªé€‚åº”åŠ æƒæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§è‡ªæˆ‘è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨è‡ªç”Ÿæˆçš„æ¨ç†è·¯å¾„æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºEASTçš„è‡ªé€‚åº”åŠ æƒç­–ç•¥ï¼Œæ—¨åœ¨ä¼˜å…ˆè€ƒè™‘ä¸ç¡®å®šæ€§è¾ƒé«˜çš„æ•°æ®è¿›è¡Œè‡ªæˆ‘è®­ç»ƒã€‚EASTé€šè¿‡å¯è°ƒå‚æ•°çš„æ˜ å°„å‡½æ•°æ¥æ§åˆ¶åŠ æƒçš„é”åº¦ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å…³æ³¨æ›´å…·ä¿¡æ¯é‡å’ŒæŒ‘æˆ˜æ€§çš„ç¤ºä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEASTåœ¨GSM8Kå’ŒMATHåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19906', 'title': 'AvatarArtist: Open-Domain 4D Avatarization', 'url': 'https://huggingface.co/papers/2503.19906', 'abstract': 'This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies..', 'score': 1, 'issue_id': 3012, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '9cc5699cb424a056', 'authors': ['Hongyu Liu', 'Xuan Wang', 'Ziyu Wan', 'Yue Ma', 'Jingye Chen', 'Yanbo Fan', 'Yujun Shen', 'Yibing Song', 'Qifeng Chen'], 'affiliations': ['Ant Group', 'City University of Hong Kong', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2503.19906.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#diffusion', '#3d'], 'emoji': 'ğŸ­', 'ru': {'title': 'AvatarArtist: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ 4D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 4D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµĞ¹Ğ½Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾-ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ (GAN) Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. GAN Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµĞ¹Ğ½Ğ¾Ğ², Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ° ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµĞ¹Ğ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° 4D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Transforming Portraits into Dynamic 4D Avatars!', 'desc': 'This paper presents a method for creating 4D avatars from 2D portrait images in different styles. It utilizes parametric triplanes as a 4D representation and combines generative adversarial networks (GANs) with diffusion models for effective training. The approach leverages the strengths of 4D GANs to connect images and triplanes while addressing challenges with diverse data through a robust 2D diffusion prior. The resulting model, AvatarArtist, demonstrates the ability to generate high-quality 4D avatars across various image domains, with plans to share the code and data for further research.'}, 'zh': {'title': 'æ‰“é€ å¤šé¢†åŸŸé«˜è´¨é‡4Då¤´åƒçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬ç ”ç©¶ä¸“æ³¨äºå¼€æ”¾é¢†åŸŸçš„4Då¤´åƒç”Ÿæˆï¼Œæ—¨åœ¨ä»ä»»æ„é£æ ¼çš„è‚–åƒå›¾åƒä¸­åˆ›å»º4Då¤´åƒã€‚æˆ‘ä»¬é€‰æ‹©å‚æ•°ä¸‰å¹³é¢ä½œä¸ºä¸­é—´çš„4Dè¡¨ç¤ºï¼Œå¹¶æå‡ºäº†ä¸€ç§å®ç”¨çš„è®­ç»ƒèŒƒå¼ï¼Œç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚ç ”ç©¶è¡¨æ˜ï¼Œ4D GANåœ¨æ— ç›‘ç£æƒ…å†µä¸‹èƒ½å¤Ÿæœ‰æ•ˆè¿æ¥å›¾åƒå’Œä¸‰å¹³é¢ï¼Œä½†åœ¨å¤„ç†å¤šæ ·åŒ–æ•°æ®åˆ†å¸ƒæ—¶é€šå¸¸é¢ä¸´æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥å¼ºå¤§çš„2Dæ‰©æ•£å…ˆéªŒï¼Œå¸®åŠ©GANåœ¨ä¸åŒé¢†åŸŸä¹‹é—´è½¬ç§»å…¶ä¸“ä¸šçŸ¥è¯†ï¼Œä»è€Œæ„å»ºä¸€ä¸ªå¤šé¢†åŸŸçš„å›¾åƒ-ä¸‰å¹³é¢æ•°æ®é›†ï¼Œæ¨åŠ¨é€šç”¨4Då¤´åƒç”Ÿæˆå™¨çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19794', 'title': 'PAVE: Patching and Adapting Video Large Language Models', 'url': 'https://huggingface.co/papers/2503.19794', 'abstract': 'Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as "patches," which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE.', 'score': 1, 'issue_id': 3008, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'fefb309c48b93a29', 'authors': ['Zhuoming Liu', 'Yiquan Li', 'Khoi Duc Nguyen', 'Yiwu Zhong', 'Yin Li'], 'affiliations': ['The Chinese University of Hong Kong', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.19794.jpg', 'data': {'categories': ['#optimization', '#3d', '#transfer_learning', '#multimodal', '#reasoning', '#video', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'PAVE: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼', 'desc': "PAVE - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Video LLMs) Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ 'Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞµĞµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. PAVE ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, 3D-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."}, 'en': {'title': 'PAVE: Adapting Video LLMs with Lightweight Patches', 'desc': 'This paper introduces PAVE, a new framework designed to adapt pre-trained video large language models (Video LLMs) for various tasks that require additional data types like audio or 3D information. PAVE utilizes lightweight adapters, called "patches," which integrate seamlessly into existing models without altering their original architecture or pre-trained weights. By doing so, it enhances the model\'s capabilities for tasks such as audio-visual question answering and multi-view video recognition while maintaining a low computational cost. The framework not only improves performance over state-of-the-art models but also supports multi-task learning and shows strong generalization across different Video LLMs.'}, 'zh': {'title': 'PAVEï¼šçµæ´»é€‚åº”è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPAVEçš„çµæ´»æ¡†æ¶ï¼Œç”¨äºå°†é¢„è®­ç»ƒçš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰é€‚åº”äºæ–°çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠéŸ³é¢‘ã€3Dä¿¡æ¯æˆ–å¤šè§†è§’è§†é¢‘ç­‰é™„åŠ æ¨¡æ€çš„æ•°æ®ã€‚PAVEå¼•å…¥äº†è½»é‡çº§çš„é€‚é…å™¨ï¼Œç§°ä¸ºâ€œè¡¥ä¸â€ï¼Œè¿™äº›è¡¥ä¸åœ¨ä¸æ”¹å˜åŸºç¡€æ¨¡å‹æ¶æ„æˆ–é¢„è®­ç»ƒæƒé‡çš„æƒ…å†µä¸‹ï¼Œå¢åŠ äº†å°‘é‡å‚æ•°å’Œæ“ä½œã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒPAVEèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹é€‚åº”äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚éŸ³è§†é¢‘é—®ç­”ã€3Dæ¨ç†å’Œå¤šè§†è§’è§†é¢‘è¯†åˆ«ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPAVEåœ¨è¿™äº›ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼ŒåŒæ—¶ä»…å¢åŠ çº¦0.1%çš„è®¡ç®—é‡å’Œå‚æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18225', 'title': 'Decoupling Angles and Strength in Low-rank Adaptation', 'url': 'https://huggingface.co/papers/2503.18225', 'abstract': 'Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.', 'score': 1, 'issue_id': 3001, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': '36bcda1f90686965', 'authors': ['Massimo Bini', 'Leander Girrbach', 'Zeynep Akata'], 'affiliations': ['Helmholtz Munich', 'Technical University of Munich, Munich Center for Machine Learning', 'University of Tubingen, Tubingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2503.18225.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'DeLoRA: Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'DeLoRA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°. ĞĞ½ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ ÑƒĞ³Ğ»Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ¸Ğ»Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. DeLoRA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑˆĞµ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (PEFT) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Decoupling Adaptation for Robust Fine-Tuning', 'desc': 'This paper introduces Decoupled Low-rank Adaptation (DeLoRA), a new method for fine-tuning large pretrained models efficiently. DeLoRA improves upon existing Parameter-Efficient FineTuning (PEFT) methods by normalizing and scaling low-rank matrices, which enhances robustness against hyperparameter variations. Unlike traditional methods like LoRA, which struggle with stability, DeLoRA decouples the learning angle from the adaptation strength, allowing for better performance across various tasks. The results demonstrate that DeLoRA not only matches but often exceeds the performance of other PEFT techniques in applications such as image generation and natural language understanding.'}, 'zh': {'title': 'è§£è€¦ä½ç§©é€‚åº”ï¼šæå‡å¾®è°ƒé²æ£’æ€§çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºè§£è€¦ä½ç§©é€‚åº”ï¼ˆDeLoRAï¼‰ï¼Œæ—¨åœ¨æé«˜å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•çš„é²æ£’æ€§ã€‚DeLoRAé€šè¿‡è§„èŒƒåŒ–å’Œç¼©æ”¾å¯å­¦ä¹ çš„ä½ç§©çŸ©é˜µï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒDeLoRAåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹è¶…å‚æ•°é€‰æ‹©å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–ã€‚é€šè¿‡åœ¨å›¾åƒç”Ÿæˆã€è‡ªç„¶è¯­è¨€ç†è§£å’ŒæŒ‡ä»¤è°ƒä¼˜ç­‰ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼ŒDeLoRAçš„è¡¨ç°ä¸å…¶ä»–PEFTæ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶å±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22677', 'title': 'DSO: Aligning 3D Generators with Simulation Feedback for Physical\n  Soundness', 'url': 'https://huggingface.co/papers/2503.22677', 'abstract': 'Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.', 'score': 0, 'issue_id': 3014, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '9a7faeb192777179', 'authors': ['Ruining Li', 'Chuanxia Zheng', 'Christian Rupprecht', 'Andrea Vedaldi'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.22677.jpg', 'data': {'categories': ['#3d', '#dataset', '#alignment', '#optimization', '#rlhf', '#training'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ±ĞµĞ· Ğ·Ğ°Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Direct Simulation Optimization (DSO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Direct Reward Optimization (DRO) Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Generating Stable 3D Objects with Direct Simulation Optimization', 'desc': 'This paper introduces Direct Simulation Optimization (DSO), a new framework for generating stable 3D objects that can support themselves under gravity. Unlike previous methods that relied on slow and unstable differentiable physics simulators, DSO uses feedback from a non-differentiable simulator to enhance the stability of generated objects. The authors create a dataset of 3D objects with stability scores and employ direct preference optimization (DPO) and direct reward optimization (DRO) to fine-tune the generator. The results demonstrate that DSO significantly improves the speed and stability of 3D object generation without needing ground-truth training data.'}, 'zh': {'title': 'ç›´æ¥æ¨¡æ‹Ÿä¼˜åŒ–ï¼šç”Ÿæˆç¨³å®š3Dç‰©ä½“çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„3Dç‰©ä½“ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºç›´æ¥æ¨¡æ‹Ÿä¼˜åŒ–ï¼ˆDSOï¼‰ï¼Œæ—¨åœ¨ç”Ÿæˆç¬¦åˆç‰©ç†çº¦æŸçš„ç¨³å®š3Dç‰©ä½“ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¯å¾®åˆ†ç‰©ç†æ¨¡æ‹Ÿå™¨è¿›è¡Œå‡ ä½•ä¼˜åŒ–ï¼Œä½†é€Ÿåº¦æ…¢ä¸”å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚DSOæ¡†æ¶åˆ©ç”¨æ¥è‡ªéå¯å¾®åˆ†æ¨¡æ‹Ÿå™¨çš„åé¦ˆï¼Œç›´æ¥æé«˜ç”Ÿæˆå™¨è¾“å‡ºç¨³å®šç‰©ä½“çš„å¯èƒ½æ€§ã€‚é€šè¿‡æ„å»ºå¸¦æœ‰ç¨³å®šæ€§è¯„åˆ†çš„æ•°æ®é›†ï¼Œç ”ç©¶è€…å¯ä»¥ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æˆ–ç›´æ¥å¥–åŠ±ä¼˜åŒ–ï¼ˆDROï¼‰æ¥å¾®è°ƒç”Ÿæˆå™¨ï¼Œä»è€Œå®ç°æ›´å¿«ä¸”æ›´ç¨³å®šçš„3Dç‰©ä½“ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22668', 'title': 'Understanding Co-speech Gestures in-the-wild', 'url': 'https://huggingface.co/papers/2503.22668', 'abstract': "Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-text-speech associations: (i) gesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal speech-text-video-gesture representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs), across all three tasks. Further analysis reveals that speech and text modalities capture distinct gesture-related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal", 'score': 0, 'issue_id': 3007, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '77fc0ab156b46a61', 'authors': ['Sindhu B Hegde', 'K R Prajwal', 'Taein Kwon', 'Andrew Zisserman'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.22668.jpg', 'data': {'categories': ['#multimodal', '#benchmark'], 'emoji': 'ğŸ¤²', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¶ĞµÑÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµÑ‡Ğ¸: Ñ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¶ĞµÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµÑ‡ÑŒ, Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¶ĞµÑÑ‚Ğ°Ğ¼Ğ¸, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ñ€ĞµÑ‡ÑŒÑ. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸-Ñ‚ĞµĞºÑÑ‚Ğ°-Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¶ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚Ñ€ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Unlocking Gesture Understanding with Tri-Modal Learning', 'desc': 'This paper presents a new framework for understanding co-speech gestures, which are important for non-verbal communication. It introduces three tasks to evaluate how well models can understand the relationships between gestures, text, and speech. The authors propose a tri-modal representation that combines speech, text, video, and gestures, using a unique loss function to improve learning from real-world videos. Their approach outperforms existing methods, including large vision-language models, highlighting the benefits of a shared embedding space for different modalities.'}, 'zh': {'title': 'ç†è§£å…±è¯­æ‰‹åŠ¿çš„æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç”¨äºç†è§£è‡ªç„¶ç¯å¢ƒä¸­çš„å…±è¯­æ‰‹åŠ¿ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªæ–°ä»»åŠ¡å’ŒåŸºå‡†ï¼Œä»¥è¯„ä¼°æ¨¡å‹ç†è§£æ‰‹åŠ¿ã€æ–‡æœ¬å’Œè¯­éŸ³ä¹‹é—´å…³è”çš„èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆå…¨å±€çŸ­è¯­å¯¹æ¯”æŸå¤±å’Œå±€éƒ¨æ‰‹åŠ¿-è¯è€¦åˆæŸå¤±ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨å¼±ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œä»è§†é¢‘ä¸­å­¦ä¹ å¼ºå¤§çš„æ‰‹åŠ¿è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å­¦ä¹ è¡¨ç¤ºåœ¨æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ä¸­éƒ½è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00999', 'title': 'MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization', 'url': 'https://huggingface.co/papers/2504.00999', 'abstract': 'Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.', 'score': 56, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'bb6506ffd72aed19', 'authors': ['Siyuan Li', 'Luyuan Zhang', 'Zedong Wang', 'Juanxi Tian', 'Cheng Tan', 'Zicheng Liu', 'Chang Yu', 'Qingsong Xie', 'Haonan Lu', 'Haoqian Wang', 'Zhen Lei'], 'affiliations': ['CAIR, HKISI-CAS', 'MAIS CASIA', 'OPPO AI Center', 'The Hong Kong University of Science and Technology', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00999.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'MergeVQ: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MergeVQ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (MIM) Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (VQ) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. MergeVQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ MergeAR, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MergeVQ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'MergeVQ: Bridging Image Generation and Representation Learning Efficiently', 'desc': 'This paper introduces MergeVQ, a novel approach that enhances Masked Image Modeling (MIM) using Vector Quantization (VQ) techniques. It addresses the challenge of balancing image generation quality with efficient representation learning by integrating token merging into the VQ framework. During the pre-training phase, MergeVQ utilizes a token merge module to separate high-level semantics from the latent space, allowing for improved quantization and detail recovery. The second stage, MergeAR, optimizes the generation process through KV Cache compression, resulting in a model that excels in both visual representation and image generation while ensuring efficiency in token usage and inference speed.'}, 'zh': {'title': 'MergeVQï¼šæå‡å›¾åƒç”Ÿæˆä¸è¡¨ç¤ºå­¦ä¹ çš„ç»Ÿä¸€æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹MergeVQï¼Œæ—¨åœ¨æ”¹å–„åŸºäºå‘é‡é‡åŒ–çš„å›¾åƒç”Ÿæˆå’Œè§†è§‰è¡¨ç¤ºå­¦ä¹ ä¹‹é—´çš„å¹³è¡¡ã€‚é€šè¿‡åœ¨ç¼–ç å™¨ä¸­å¼•å…¥ä»¤ç‰Œåˆå¹¶æŠ€æœ¯ï¼ŒMergeVQèƒ½å¤Ÿåœ¨è‡ªæ³¨æ„åŠ›å—åè§£è€¦æ½œåœ¨ç©ºé—´ä¸­çš„è¯­ä¹‰ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¢å¤ç»†èŠ‚ï¼Œå¹¶åœ¨ç”Ÿæˆé˜¶æ®µä½¿ç”¨KVç¼“å­˜å‹ç¼©æ¥æé«˜é¢„æµ‹æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMergeVQåœ¨è§†è§‰è¡¨ç¤ºå­¦ä¹ å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00883', 'title': 'Improved Visual-Spatial Reasoning via R1-Zero-Like Training', 'url': 'https://huggingface.co/papers/2504.00883', 'abstract': 'Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.', 'score': 42, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '43fba84cc49adfad', 'authors': ['Zhenyi Liao', 'Qingsong Xie', 'Yanhao Zhang', 'Zijian Kong', 'Haonan Lu', 'Zhenyu Yang', 'Zhijie Deng'], 'affiliations': ['OPPO AI Center', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00883.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#video', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ R1-Zero. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2-VL Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ GRPO Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VSI-100k Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ vsGRPO-7B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2-VL-7B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ LLaVA-NeXT-Video-72B.'}, 'en': {'title': 'Enhancing Visual-Spatial Reasoning in MLLMs with GRPO Training', 'desc': 'This paper focuses on enhancing the visual-spatial reasoning abilities of multi-modal large language models (MLLMs), which are crucial for AI agents interacting with the physical world. The authors introduce a novel training method called GRPO, applied to the Qwen2-VL models, to improve their reasoning capabilities using a specially curated dataset named VSI-100k. They discover that traditional Chain of Thought prompts are ineffective for activating these reasoning skills in smaller models. The results show that their fine-tuned models significantly outperform baseline models, demonstrating the effectiveness of their approach in advancing visual-spatial intelligence in MLLMs.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶èšç„¦äºæå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘åŸºç¡€çš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ˆVSIï¼‰æ–¹é¢ã€‚æˆ‘ä»¬å‘ç°å°åˆ°ä¸­å‹çš„Qwen2-VLæ¨¡å‹æ— æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¿€æ´»å…¶è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå› æ­¤å¼•å…¥äº†GRPOè®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„VSI-100kæ•°æ®é›†è¿›è¡Œæ”¹è¿›ã€‚ç»è¿‡120ä¸ªGPUå°æ—¶çš„è®­ç»ƒï¼Œæˆ‘ä»¬çš„vsGRPO-2Bæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†åŸºç¡€æ¨¡å‹12.1%ï¼Œå¹¶ä¸”vsGRPO-7Bæ¨¡å‹çš„è¡¨ç°ä¸æœ€ä½³å¼€æºæ¨¡å‹LLaVA-NeXT-Video-72Bç›¸å½“ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä¿æŒKLæƒ©ç½šåœ¨GRPOä¸­æ˜¯å¿…è¦çš„ï¼Œå¹¶ä¸”vsGRPOåœ¨ä¸ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–åŸºçº¿çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01014', 'title': 'AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction', 'url': 'https://huggingface.co/papers/2504.01014', 'abstract': 'Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.', 'score': 29, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '98efa783105c3173', 'authors': ['Junhao Cheng', 'Yuying Ge', 'Yixiao Ge', 'Jing Liao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01014.jpg', 'data': {'categories': ['#diffusion', '#games', '#multimodal', '#video'], 'emoji': 'ğŸ®', 'ru': {'title': 'AnimeGamer: Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ°Ğ½Ğ¸Ğ¼Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'AnimeGamer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². AnimeGamer Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Transforming Anime into Interactive Gaming with Dynamic AI', 'desc': 'This paper presents AnimeGamer, a novel approach to infinite anime life simulation games that utilizes Multimodal Large Language Models (MLLMs). Unlike previous methods, AnimeGamer incorporates historical visual context to ensure consistent gameplay and generates dynamic animations rather than just static images. By employing action-aware multimodal representations, it can create high-quality video clips that reflect character movements and state changes. The results show that AnimeGamer significantly enhances the gaming experience compared to existing techniques, as validated by both automated and human evaluations.'}, 'zh': {'title': 'AnimeGamerï¼šåŠ¨æ€äº’åŠ¨çš„æ— é™åŠ¨æ¼«æ¸¸æˆä½“éªŒ', 'desc': 'æœ€è¿‘åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆæ–¹é¢çš„è¿›å±•ä¸ºç”Ÿæˆæ¸¸æˆå¸¦æ¥äº†æ–°çš„å¸Œæœ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAnimeGamerçš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”ŸæˆåŠ¨æ€åŠ¨ç”»é•œå¤´ï¼Œä»¥å¢å¼ºæ¸¸æˆçš„äº’åŠ¨æ€§å’Œæ²‰æµ¸æ„Ÿã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œæ„ŸçŸ¥çš„å¤šæ¨¡æ€è¡¨ç¤ºï¼ŒAnimeGamerèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’ŒåŠ¨æ€å˜åŒ–çš„æ¸¸æˆçŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAnimeGameråœ¨æ¸¸æˆä½“éªŒçš„å„ä¸ªæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01724', 'title': 'DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance', 'url': 'https://huggingface.co/papers/2504.01724', 'abstract': 'While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.', 'score': 24, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'd59102a274145730', 'authors': ['Yuxuan Luo', 'Zhengkun Rong', 'Lizhen Wang', 'Longhao Zhang', 'Tianshu Hu', 'Yongming Zhu'], 'affiliations': ['Bytedance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2504.01724.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#3d', '#diffusion'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'DreamActor-M1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, 3D-ÑÑ„ĞµÑ€Ñ‹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ 3D-ÑĞºĞµĞ»ĞµÑ‚Ñ‹ Ñ‚ĞµĞ»Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ°Ğ¼ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼. DreamActor-M1 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'DreamActor-M1: Revolutionizing Human Animation with Robust Control and Consistency', 'desc': 'The paper presents DreamActor-M1, a novel framework that utilizes a diffusion transformer (DiT) to enhance human animation by addressing key limitations in existing methods. It introduces hybrid control signals that combine facial representations, 3D head spheres, and body skeletons to improve the expressiveness and control of animations. The framework also employs a progressive training strategy to adapt to various body poses and image scales, ensuring versatility in generating animations from portraits to full-body views. Additionally, it integrates motion patterns from sequential frames to maintain long-term temporal coherence, resulting in more robust and visually appealing animations.'}, 'zh': {'title': 'çªç ´åŠ¨ç”»ç”Ÿæˆçš„å±€é™æ€§ï¼ŒDreamActor-M1å¼•é¢†æ–°æ½®æµï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡†æ¶DreamActor-M1ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåŸºç¡€çš„äººä½“åŠ¨ç”»æ–¹æ³•åœ¨ç»†ç²’åº¦æ•´ä½“å¯æ§æ€§ã€å¤šå°ºåº¦é€‚åº”æ€§å’Œé•¿æœŸæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡æ··åˆæ§åˆ¶ä¿¡å·ï¼Œç»“åˆéšå¼é¢éƒ¨è¡¨ç¤ºã€3Då¤´éƒ¨çƒä½“å’Œ3Dèº«ä½“éª¨æ¶ï¼Œå®ç°äº†å¯¹é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“åŠ¨ä½œçš„å¼ºå¤§æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒåŠ¨ç”»çš„è¡¨ç°åŠ›å’Œèº«ä»½ä¸€è‡´æ€§ã€‚ä¸ºäº†é€‚åº”ä¸åŒçš„èº«ä½“å§¿åŠ¿å’Œå›¾åƒå°ºåº¦ï¼Œé‡‡ç”¨äº†æ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡å’Œå°ºåº¦çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‚–åƒã€ä¸ŠåŠèº«å’Œå…¨èº«ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå…·æœ‰å¼ºå¤§çš„é•¿æœŸä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20783', 'title': 'Understanding R1-Zero-Like Training: A Critical Perspective', 'url': 'https://huggingface.co/papers/2503.20783', 'abstract': "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.", 'score': 24, 'issue_id': 3044, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'c5971e424bc52a6b', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… ÑƒĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ². Ğ‘Ñ‹Ğ» Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Group Relative Policy Optimization (GRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dr. GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing LLM Reasoning with Unbiased RL Optimization', 'desc': 'This paper explores how reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) without needing supervised fine-tuning. It examines the impact of different base models, particularly DeepSeek-V3-Base and Qwen2.5, on RL performance, revealing that pretraining characteristics can lead to inherent biases in reasoning capabilities. The authors also identify an optimization bias in the Group Relative Policy Optimization (GRPO) method, which can inflate response lengths during training. To counter this, they propose Dr. GRPO, a new optimization technique that enhances token efficiency while preserving reasoning accuracy, achieving a notable 43.3% accuracy on the AIME 2024 benchmark with a 7B base model.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›çš„æ–°çªç ´', 'desc': 'DeepSeek-R1-Zeroå±•ç¤ºäº†å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚æœ¬æ–‡æ·±å…¥åˆ†æäº†R1-Zeroè®­ç»ƒçš„ä¸¤ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼šåŸºç¡€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬ç ”ç©¶äº†å¤šç§åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬DeepSeek-V3-Baseï¼Œä»¥äº†è§£é¢„è®­ç»ƒç‰¹æ€§å¦‚ä½•å½±å“RLæ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æå‘ç°ï¼ŒDeepSeek-V3-Baseå·²ç»å±•ç°å‡ºâ€œæç„¶å¤§æ‚Ÿâ€çš„æ—¶åˆ»ï¼Œè€ŒQwen2.5åŸºç¡€æ¨¡å‹å³ä½¿åœ¨æ²¡æœ‰æç¤ºæ¨¡æ¿çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œæš—ç¤ºäº†æ½œåœ¨çš„é¢„è®­ç»ƒåå·®ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.01956', 'title': 'VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step', 'url': 'https://huggingface.co/papers/2504.01956', 'abstract': 'Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene', 'score': 22, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '44f1db8ef8cc244a', 'authors': ['Hanyang Wang', 'Fangfu Liu', 'Jiawei Chi', 'Yueqi Duan'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01956.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VideoScene: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoScene - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ 3D-aware leap flow Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. VideoScene Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D ÑÑ†ĞµĞ½ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 3D.'}, 'en': {'title': 'Efficient 3D Scene Generation from Sparse Views Using Video Diffusion Models', 'desc': 'This paper addresses the challenge of reconstructing 3D scenes from sparse views, which is often complicated by the lack of visual information. Traditional methods have limitations, particularly when input views have minimal overlap, leading to degraded performance. The authors introduce VideoScene, a novel approach that utilizes video diffusion models to efficiently generate 3D scenes in a single step. By implementing a 3D-aware leap flow distillation strategy and a dynamic denoising policy network, VideoScene significantly improves the speed and quality of 3D scene generation compared to existing methods.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆ3Dåœºæ™¯çš„VideoScene', 'desc': 'ä»ç¨€ç–è§†å›¾æ¢å¤3Dåœºæ™¯æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡å‡ ä½•æ­£åˆ™åŒ–æˆ–å‰é¦ˆç¡®å®šæ€§æ¨¡å‹ç­‰ä¸“é—¨è§£å†³æ–¹æ¡ˆæ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†åœ¨è¾“å…¥è§†å›¾é‡å è¾ƒå°‘ä¸”è§†è§‰ä¿¡æ¯ä¸è¶³æ—¶ï¼Œæ€§èƒ½ä»ç„¶ä¸‹é™ã€‚æœ€è¿‘çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¾ç¤ºå‡ºè§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰åˆç†3Dç»“æ„çš„è§†é¢‘ç‰‡æ®µã€‚æœ¬æ–‡æå‡ºäº†VideoSceneï¼Œé€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹æç‚¼ç”Ÿæˆ3Dåœºæ™¯ï¼Œè®¾è®¡äº†3Dæ„ŸçŸ¥çš„è·ƒè¿æµè’¸é¦ç­–ç•¥ï¼Œä»¥æé«˜ç”Ÿæˆæ•ˆç‡å’Œæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23368', 'title': 'Towards Physically Plausible Video Generation via VLM Planning', 'url': 'https://huggingface.co/papers/2503.23368', 'abstract': 'Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.', 'score': 21, 'issue_id': 3050, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'f3087a720104ea83', 'authors': ['Xindi Yang', 'Baolu Li', 'Yiming Zhang', 'Zhenfei Yin', 'Lei Bai', 'Liqian Ma', 'Zhiyong Wang', 'Jianfei Cai', 'Tien-Tsin Wong', 'Huchuan Lu', 'Xu Jia'], 'affiliations': ['Dalian University of Technology', 'Monash University', 'Oxford University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Sydney', 'ZMO AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.23368.jpg', 'data': {'categories': ['#reasoning', '#diffusion', '#video', '#architecture', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ ĞœĞ¾Ğ´ĞµĞ»ÑŒ (VLM) Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞœĞ¾Ğ´ĞµĞ»ÑŒÑ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ’Ğ¸Ğ´ĞµĞ¾ (VDM) Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ VDM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Bringing Physics to Video Generation: A Two-Stage Approach', 'desc': 'This paper introduces a new two-stage framework for generating videos that are more physically realistic using video diffusion models (VDMs). The first stage uses a Vision Language Model (VLM) to create rough motion trajectories that consider real-world physics, ensuring that the generated video maintains consistency between frames. In the second stage, these trajectories guide the VDM in producing detailed video content, with added noise to allow for creative freedom in motion generation. The results show that this approach significantly improves the physical plausibility of the generated videos compared to existing methods.'}, 'zh': {'title': 'å¼•å…¥ç‰©ç†çŸ¥è¯†çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰åœ¨ç”Ÿæˆé«˜åº¦çœŸå®çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¸¸å¸¸ç¼ºä¹å¯¹ç‰©ç†çš„ç†è§£ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘åœ¨åŠ¨æ€å’Œäº‹ä»¶åºåˆ—ä¸Šä¸å¤Ÿåˆç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ˜ç¡®åœ°èå…¥äº†ç‰©ç†çŸ¥è¯†ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºç²—ç•¥çš„è¿åŠ¨è§„åˆ’å™¨ï¼Œç»“åˆæ€ç»´é“¾å’Œç‰©ç†æ„ŸçŸ¥æ¨ç†ï¼Œé¢„æµ‹æ¥è¿‘çœŸå®ç‰©ç†åŠ¨æ€çš„ç²—ç•¥è¿åŠ¨è½¨è¿¹ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹æ¥æŒ‡å¯¼VDMçš„è§†é¢‘ç”Ÿæˆï¼Œä»è€Œå®ç°æ›´ç»†è‡´çš„è¿åŠ¨è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01848', 'title': "PaperBench: Evaluating AI's Ability to Replicate AI Research", 'url': 'https://huggingface.co/papers/2504.01848', 'abstract': "We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.", 'score': 18, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '60923777325e85cc', 'authors': ['Giulio Starace', 'Oliver Jaffe', 'Dane Sherburn', 'James Aung', 'Jun Shern Chan', 'Leon Maksin', 'Rachel Dias', 'Evan Mays', 'Benjamin Kinsella', 'Wyatt Thompson', 'Johannes Heidecke', 'Amelia Glaese', 'Tejal Patwardhan'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2504.01848.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#survey'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'PaperBench: Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'PaperBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 20 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· ICML 2024, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ Ñ Ğ½ÑƒĞ»Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ñ‡ĞµÑ‚ĞºĞ¸Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Claude 3.5 Sonnet (New) Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ»Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ 21.0%.'}, 'en': {'title': "Evaluating AI's Research Replication Skills with PaperBench", 'desc': "This paper presents PaperBench, a benchmark designed to assess AI agents' ability to replicate advanced AI research. The benchmark includes 20 selected papers from ICML 2024, requiring agents to comprehend contributions, create a codebase, and conduct experiments. To ensure objective evaluation, the authors developed detailed rubrics that break down replication tasks into smaller, graded components, totaling 8,316 tasks. The study also introduces an LLM-based judge for automated grading and compares the performance of AI agents against human experts, revealing that current models still lag behind human capabilities."}, 'zh': {'title': 'PaperBenchï¼šè¯„ä¼°AIå¤åˆ¶ç ”ç©¶èƒ½åŠ›çš„åŸºå‡†', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†PaperBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†å¤åˆ¶æœ€å…ˆè¿›AIç ”ç©¶èƒ½åŠ›çš„åŸºå‡†ã€‚ä»£ç†éœ€è¦ä»å¤´å¼€å§‹å¤åˆ¶20ç¯‡ICML 2024çš„äº®ç‚¹å’Œå£å¤´è®ºæ–‡ï¼ŒåŒ…æ‹¬ç†è§£è®ºæ–‡è´¡çŒ®ã€å¼€å‘ä»£ç åº“å’ŒæˆåŠŸæ‰§è¡Œå®éªŒã€‚ä¸ºäº†è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†åˆ†å±‚çš„è¯„åˆ†æ ‡å‡†ï¼Œå°†æ¯ä¸ªå¤åˆ¶ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶è®¾å®šæ˜ç¡®çš„è¯„åˆ†æ ‡å‡†ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¿˜åŒ…æ‹¬ä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„å®¡è€…è‡ªåŠ¨è¯„åˆ†ï¼Œå¹¶ä¸é¡¶å°–çš„æœºå™¨å­¦ä¹ åšå£«è¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°ç›®å‰çš„æ¨¡å‹å°šæœªè¶…è¶Šäººç±»åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00824', 'title': 'ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations', 'url': 'https://huggingface.co/papers/2504.00824', 'abstract': "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.", 'score': 18, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'b135f3f003dcaaff', 'authors': ['Yubo Wang', 'Xueguang Ma', 'Ping Nie', 'Huaye Zeng', 'Zhiheng Lyu', 'Yuxuan Zhang', 'Benjamin Schneider', 'Yi Lu', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.00824.jpg', 'data': {'categories': ['#science', '#dataset', '#rag', '#multimodal', '#alignment'], 'emoji': 'ğŸ“', 'ru': {'title': 'ScholarCopilot: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°', 'desc': 'ScholarCopilot - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½ [RET], Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ScholarCopilot ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 500 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· arXiv, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ top-1 Ğ² 40.1% Ğ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Academic Writing with ScholarCopilot', 'desc': 'This paper presents ScholarCopilot, a new framework that improves large language models for generating academic articles with accurate citations. It uses a retrieval token to decide when to fetch scholarly references, enhancing the text generation process with relevant citations. The model is trained on a large dataset of academic papers and shows significant improvements in both citation accuracy and writing quality compared to existing models. Human evaluations further validate its effectiveness in citation recall and overall user experience.'}, 'zh': {'title': 'ScholarCopilotï¼šæå‡å­¦æœ¯å†™ä½œçš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ScholarCopilotï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä¸“ä¸šå­¦æœ¯æ–‡ç« æ—¶çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç”Ÿæˆæ£€ç´¢æ ‡è®°[RET]ï¼ŒåŠ¨æ€å†³å®šä½•æ—¶æ£€ç´¢å­¦æœ¯å‚è€ƒæ–‡çŒ®ï¼Œå¹¶åˆ©ç”¨å…¶è¡¨ç¤ºä»æ•°æ®åº“ä¸­æŸ¥æ‰¾ç›¸å…³å¼•ç”¨ã€‚ScholarCopilotåœ¨ç”Ÿæˆå’Œå¼•ç”¨ä»»åŠ¡ä¸Šè¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥æé«˜æ•ˆç‡ã€‚ç»è¿‡åœ¨500Kç¯‡arXivè®ºæ–‡ä¸Šçš„è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨è¯„ä¼°æ•°æ®é›†ä¸Šå®ç°äº†40.1%çš„é¡¶çº§æ£€ç´¢å‡†ç¡®ç‡ï¼Œä¸”åœ¨å­¦æœ¯å†™ä½œæ ·æœ¬çš„ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†å‚æ•°æ›´å¤šçš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01934', 'title': 'ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement', 'url': 'https://huggingface.co/papers/2504.01934', 'abstract': 'We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.', 'score': 16, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'a50e19f04d94405f', 'authors': ['Runhui Huang', 'Chunwei Wang', 'Junwei Yang', 'Guansong Lu', 'Yunlong Yuan', 'Jianhua Han', 'Lu Hou', 'Wei Zhang', 'Lanqing Hong', 'Hengshuang Zhao', 'Hang Xu'], 'affiliations': ['Huawei Noahs Ark Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01934.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#training', '#games', '#multimodal', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ILLUME+: Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'ILLUME+ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ILLUME+ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ DualViTok, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ MLLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'ILLUME+: Unifying Understanding, Generation, and Editing in One Model', 'desc': 'ILLUME+ is a novel model that combines dual visual tokenization and a diffusion decoder to enhance deep semantic understanding and high-quality image generation. Unlike previous models that struggled with understanding, generation, and editing simultaneously, ILLUME+ effectively integrates these capabilities through its innovative DualViTok tokenizer. This approach preserves fine textures and aligns semantics, allowing for better image editing and generation. With a continuous-input, discrete-output framework and progressive training, ILLUME+ achieves competitive performance in multimodal tasks, paving the way for future applications.'}, 'zh': {'title': 'ILLUME+: å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'ILLUME+ æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å­¦ä¹ æ¨¡å‹ï¼Œç»“åˆäº†åŒé‡è§†è§‰æ ‡è®°å’Œæ‰©æ•£è§£ç å™¨ï¼Œæ—¨åœ¨æå‡æ·±å±‚è¯­ä¹‰ç†è§£å’Œé«˜ä¿çœŸå›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ä¸ç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹ç›¸æ¯”ï¼ŒILLUME+ èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘è¿™ä¸‰ç§åŸºæœ¬èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ DualViTokï¼ŒILLUME+ ä¿ç•™äº†ç»†è‡´çš„çº¹ç†å’Œæ–‡æœ¬å¯¹é½çš„è¯­ä¹‰ï¼ŒåŒæ—¶é‡‡ç”¨ç²—åˆ°ç»†çš„å›¾åƒè¡¨ç¤ºç­–ç•¥ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒILLUME+ åœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œè¶…åˆ†è¾¨ç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†ä¸ç°æœ‰æ¨¡å‹çš„ç«äº‰åŠ›ï¼Œä¸ºæœªæ¥çš„å¤šæ¨¡æ€åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01204', 'title': 'Articulated Kinematics Distillation from Video Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01204', 'abstract': 'We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/', 'score': 12, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'c8630e7ca691cef3', 'authors': ['Xuan Li', 'Qianli Ma', 'Tsung-Yi Lin', 'Yongxin Chen', 'Chenfanfu Jiang', 'Ming-Yu Liu', 'Donglai Xiang'], 'affiliations': ['NVIDIA', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2504.01204.jpg', 'data': {'categories': ['#diffusion', '#games', '#3d'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹: Ğ¾Ñ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Articulated Kinematics Distillation (AKD) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½ÑƒÑ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. AKD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚ĞµĞ¿ĞµĞ½ĞµĞ¹ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ñ„Ğ¾ĞºÑƒÑĞ° Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Score Distillation Sampling Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ. AKD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 4D Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Character Animation with AKD', 'desc': 'Articulated Kinematics Distillation (AKD) is a new framework designed to create realistic character animations by combining skeleton-based animation techniques with advanced generative models. It simplifies the animation process by using a skeleton representation, which reduces the Degrees of Freedom (DoFs) and allows for better control over joint movements. AKD employs Score Distillation Sampling (SDS) with pre-trained video diffusion models to generate complex motions while ensuring that the shapes of the characters remain consistent. This method also integrates well with physics-based simulations, resulting in animations that are not only visually appealing but also physically plausible.'}, 'zh': {'title': 'å…³èŠ‚è¿åŠ¨è’¸é¦ï¼šé«˜ä¿çœŸåŠ¨ç”»çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå…³èŠ‚è¿åŠ¨è’¸é¦ï¼ˆAKDï¼‰çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸè§’è‰²åŠ¨ç”»ã€‚AKDé€šè¿‡ä½¿ç”¨åŸºäºéª¨éª¼çš„è¡¨ç¤ºï¼Œæ˜¾è‘—å‡å°‘äº†è‡ªç”±åº¦ï¼Œä¸“æ³¨äºå…³èŠ‚çº§æ§åˆ¶ï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”ä¸€è‡´çš„è¿åŠ¨åˆæˆã€‚é€šè¿‡ä¸é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç»“åˆçš„å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰ï¼ŒAKDèƒ½å¤Ÿè’¸é¦å¤æ‚çš„å…³èŠ‚è¿åŠ¨ï¼ŒåŒæ—¶ä¿æŒç»“æ„å®Œæ•´æ€§ï¼Œå…‹æœäº†4Dç¥ç»å˜å½¢åœºåœ¨ä¿æŒå½¢çŠ¶ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼ŒAKDåœ¨3Dä¸€è‡´æ€§å’Œè¿åŠ¨è´¨é‡ä¸Šä¼˜äºç°æœ‰çš„æ–‡æœ¬åˆ°4Dç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01308', 'title': 'Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks', 'url': 'https://huggingface.co/papers/2504.01308', 'abstract': 'Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.', 'score': 11, 'issue_id': 3040, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '315938d70f25095e', 'authors': ['Jiawei Wang', 'Yushen Zuo', 'Yuanjun Chai', 'Zhendong Liu', 'Yichen Fu', 'Yichun Feng', 'Kin-man Lam'], 'affiliations': ['Nanjing University', 'Stanford University', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China', 'University of Washington', 'University of the Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.01308.jpg', 'data': {'categories': ['#security', '#cv', '#diffusion', '#training', '#dataset', '#optimization', '#multimodal'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğ¹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Robust-VLGuard, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ ÑˆÑƒĞ¼Ğ¾Ğ¼. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DiffPure-VLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑˆÑƒĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ VLM Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ°Ñ‚Ğ°Ğº.'}, 'en': {'title': 'Strengthening Vision-Language Models Against Noise Attacks', 'desc': 'This paper addresses the vulnerabilities of Vision-Language Models (VLMs) to jailbreak attacks, particularly when they encounter noisy or corrupted images. The authors highlight that existing security measures during training do not adequately account for noise-augmented visual inputs, leading to significant security gaps. To combat this issue, they introduce Robust-VLGuard, a dataset designed for multimodal safety that includes both aligned and misaligned image-text pairs, along with a noise-augmented fine-tuning process. Additionally, they propose DiffPure-VLM, which uses diffusion models to transform adversarial perturbations into Gaussian-like noise, enhancing the robustness of VLMs against such attacks while maintaining their functionality.'}, 'zh': {'title': 'å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å™ªå£°æˆ–æŸåçš„å›¾åƒæ—¶ä»ç„¶å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç°æœ‰çš„VLMsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡å–äº†å®‰å…¨æªæ–½ä»¥å‡è½»è¿™äº›æ”»å‡»ï¼Œä½†å¯¹å™ªå£°å¢å¼ºè§†è§‰è¾“å…¥çš„è„†å¼±æ€§å´æœªç»™äºˆè¶³å¤Ÿé‡è§†ã€‚æˆ‘ä»¬æå‡ºäº†Robust-VLGuardï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å®‰å…¨æ•°æ®é›†ï¼Œç»“åˆäº†å¯¹é½å’Œä¸å¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œå¹¶é€šè¿‡å™ªå£°å¢å¼ºçš„å¾®è°ƒæ¥é™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒVLMçš„åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒè½¬ç§»ç‰¹æ€§ä¸æˆ‘ä»¬å¾®è°ƒçš„VLMså¾ˆå¥½åœ°å¯¹é½ï¼Œæ˜¾è‘—å‡è½»äº†ä¸åŒå¼ºåº¦çš„å¯¹æŠ—æ‰°åŠ¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2405.20216', 'title': 'Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback', 'url': 'https://huggingface.co/papers/2405.20216', 'abstract': 'The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.', 'score': 10, 'issue_id': 3046, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '8ffe2bddf2c0ee58', 'authors': ['Sanghyeon Na', 'Yonggyu Kim', 'Hyunjoon Lee'], 'affiliations': ['Kakao'], 'pdf_title_img': 'assets/pdf/title_img/2405.20216.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#optimization', '#training', '#diffusion', '#cv'], 'emoji': 'ğŸ§‘\u200dğŸ¨', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DPO', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ DPO, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼.'}, 'en': {'title': 'Revolutionizing Human Image Generation with Direct Preference Optimization', 'desc': 'This paper addresses the challenges of generating high-quality human images from text descriptions using text-to-image (T2I) methods. It highlights the importance of meeting specific criteria such as human pose and anatomy, which are crucial for realistic image synthesis. The authors propose a novel approach that employs Direct Preference Optimization (DPO) to create a specialized dataset for training without requiring expensive human feedback. Additionally, they introduce a modified loss function that reduces artifacts and enhances image quality, leading to significant improvements in generating human images that align well with textual prompts.'}, 'zh': {'title': 'ä¼˜åŒ–äººç±»å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é€šè¿‡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡äººç±»å›¾åƒçš„æŒ‘æˆ˜ã€‚ä¸ä¸€èˆ¬å›¾åƒç”Ÿæˆä¸åŒï¼Œäººç±»å›¾åƒåˆæˆéœ€è¦æ»¡è¶³ä¸¥æ ¼çš„äººä½“å§¿åŠ¿ã€è§£å‰–ç»“æ„å’Œä¸æ–‡æœ¬æç¤ºå¯¹é½çš„æ ‡å‡†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸“é—¨é’ˆå¯¹äººç±»å›¾åƒç”Ÿæˆï¼Œæ„å»ºé«˜æ•ˆçš„DPOæ•°æ®é›†ä»¥è®­ç»ƒæ¨¡å‹ï¼Œå‡å°‘å¯¹æ˜‚è´µäººç±»åé¦ˆçš„ä¾èµ–ã€‚é€šè¿‡ä¿®æ”¹æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹èƒ½å¤Ÿå‡å°‘ä¼ªå½±å¹¶æé«˜å›¾åƒçš„çœŸå®æ„Ÿï¼Œæ˜¾è‘—æå‡äººç±»å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23573', 'title': 'DASH: Detection and Assessment of Systematic Hallucinations of VLMs', 'url': 'https://huggingface.co/papers/2503.23573', 'abstract': "Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH.", 'score': 9, 'issue_id': 3049, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'cd9c0f9c9392d470', 'authors': ['Maximilian Augustin', 'Yannic Neuhaus', 'Matthias Hein'], 'affiliations': ['Tubingen AI Center, University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2503.23573.jpg', 'data': {'categories': ['#cv', '#hallucinations', '#benchmark', '#transfer_learning', '#training', '#dataset', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'DASH: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DASH - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. DASH Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ DASH Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'DASH: Detecting and Mitigating Object Hallucinations in Vision-Language Models', 'desc': "This paper addresses the issue of object hallucinations in vision-language models (VLMs), where these models incorrectly identify objects in images. The authors introduce DASH, a new automated pipeline that detects and assesses systematic hallucinations in VLMs using large-scale, real-world image datasets. DASH includes a component called DASH-OPT, which generates misleading images to evaluate the VLM's performance on the 'natural image manifold'. The study demonstrates that fine-tuning VLMs with images identified by DASH can significantly reduce the occurrence of object hallucinations."}, 'zh': {'title': 'æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿå¹»è§‰', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å®¹æ˜“å‡ºç°ç‰©ä½“å¹»è§‰ï¼Œå³é”™è¯¯åœ°æŒ‡ç¤ºå›¾åƒä¸­å­˜åœ¨æŸäº›ç‰©ä½“ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„æ ‡è®°æ•°æ®é›†æ¥é‡åŒ–å¹»è§‰ï¼Œä½†è¿™ç§æ–¹æ³•ä¸è¶³ä»¥è¯„ä¼°åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å‡ºç°çš„å¹»è§‰ã€‚æˆ‘ä»¬æå‡ºäº†DASHï¼ˆç³»ç»Ÿå¹»è§‰çš„æ£€æµ‹ä¸è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å¤§è§„æ¨¡ç®¡é“ï¼Œæ—¨åœ¨è¯†åˆ«VLMsåœ¨çœŸå®å›¾åƒä¸­çš„ç³»ç»Ÿå¹»è§‰ã€‚é€šè¿‡ä¼˜åŒ–â€œè‡ªç„¶å›¾åƒæµå½¢â€ï¼ŒDASHèƒ½å¤Ÿç”Ÿæˆè¯¯å¯¼VLMçš„å›¾åƒï¼Œå¹¶è¯†åˆ«å‡ºå¤§é‡çš„çœŸå®å’Œè¯­ä¹‰ç›¸ä¼¼å›¾åƒçš„èšç±»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23135', 'title': 'LSNet: See Large, Focus Small', 'url': 'https://huggingface.co/papers/2503.23135', 'abstract': "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.", 'score': 4, 'issue_id': 3040, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': 'd2ac65a2356c89c3', 'authors': ['Ao Wang', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['BNRist, Tsinghua University', 'Department of Automation, Tsinghua University', 'School of Software, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23135.jpg', 'data': {'categories': ['#architecture', '#training', '#cv'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': "LSNet: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾'", 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾' Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ LS-ÑĞ²ĞµÑ€Ñ‚ĞºÑƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LS-ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LSNet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LSNet Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'See Large, Focus Small: Efficient Vision Networks', 'desc': "This paper addresses the challenges of deploying complex vision networks like Convolutional Neural Networks and Vision Transformers in real-time applications due to their heavy computations. The authors propose a new lightweight network design strategy called 'See Large, Focus Small', inspired by the human vision system's ability to dynamically adjust focus. They introduce LS convolution, which effectively combines large-kernel perception for broad information capture and small-kernel aggregation for precise feature refinement. The resulting LSNet model demonstrates improved performance and efficiency in various vision tasks compared to existing lightweight networks."}, 'zh': {'title': 'è½»é‡çº§è§†è§‰ç½‘ç»œçš„æ–°ç­–ç•¥ï¼šå¤§è§†é‡ï¼Œå°èšç„¦', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§è§†è§‰ç½‘ç»œè®¾è®¡ç­–ç•¥ï¼Œç§°ä¸ºâ€œSee Large, Focus Smallâ€ã€‚è¯¥ç­–ç•¥ç»“åˆäº†å¤§æ ¸æ„ŸçŸ¥å’Œå°æ ¸èšåˆçš„LSå·ç§¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•æ‰å¹¿æ³›çš„æ„ŸçŸ¥ä¿¡æ¯å¹¶å®ç°ç²¾ç¡®çš„ç‰¹å¾èšåˆã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒLSNetåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå…‹æœäº†ç°æœ‰è½»é‡çº§æ¨¡å‹åœ¨è®¡ç®—é¢„ç®—æœ‰é™æƒ…å†µä¸‹çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSNetåœ¨å®æ—¶åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚åˆå®é™…éƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00406', 'title': 'VerifiAgent: a Unified Verification Agent in Language Model Reasoning', 'url': 'https://huggingface.co/papers/2504.00406', 'abstract': 'Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent', 'score': 2, 'issue_id': 3046, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'd4ef7ad5ea6d5aad', 'authors': ['Jiuzhou Han', 'Wray Buntine', 'Ehsan Shareghi'], 'affiliations': ['College of Engineering and Computer Science, VinUniversity', 'Department of Data Science & AI, Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00406.jpg', 'data': {'categories': ['#interpretability', '#training', '#math', '#agents', '#reasoning', '#inference'], 'emoji': 'ğŸ”', 'ru': {'title': 'VerifiAgent: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VerifiAgent - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. VerifiAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¼ĞµÑ‚Ğ°-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ VerifiAgent Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'VerifiAgent: Enhancing Reliability in Language Model Reasoning', 'desc': "This paper introduces VerifiAgent, a novel verification system designed to improve the reliability of large language models' responses. It combines two verification levels: meta-verification for checking the completeness and consistency of answers, and tool-based adaptive verification that selects the best verification tools based on the reasoning type. This approach enhances efficiency and robustness across various reasoning tasks, outperforming traditional verification methods. Additionally, VerifiAgent improves reasoning accuracy by utilizing feedback from its verification processes and is more cost-effective in mathematical reasoning applications."}, 'zh': {'title': 'VerifiAgentï¼šæ™ºèƒ½éªŒè¯ï¼Œæå‡æ¨ç†å‡†ç¡®æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¸¸å¸¸äº§ç”Ÿä¸å¯é æˆ–é”™è¯¯çš„å›ç­”ã€‚ç°æœ‰çš„éªŒè¯æ–¹æ³•é€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šæ¨¡å‹æˆ–é¢†åŸŸï¼Œè®¡ç®—èµ„æºæ¶ˆè€—å¤§ï¼Œä¸”åœ¨ä¸åŒæ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VerifiAgentï¼Œä¸€ä¸ªç»Ÿä¸€çš„éªŒè¯ä»£ç†ï¼Œé›†æˆäº†ä¸¤çº§éªŒè¯ï¼šå…ƒéªŒè¯è¯„ä¼°æ¨¡å‹å›ç­”çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼Œå·¥å…·è‡ªé€‚åº”éªŒè¯åˆ™æ ¹æ®æ¨ç†ç±»å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„éªŒè¯å·¥å…·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVerifiAgentåœ¨æ‰€æœ‰æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºåŸºçº¿éªŒè¯æ–¹æ³•ï¼Œå¹¶èƒ½é€šè¿‡åˆ©ç”¨éªŒè¯ç»“æœçš„åé¦ˆè¿›ä¸€æ­¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22879', 'title': 'Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models', 'url': 'https://huggingface.co/papers/2503.22879', 'abstract': 'State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input x, combined with a per-state-group quantization for input-dependent parameters B and C. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3times and 3times speed-ups in the pre-filling and generation stages, respectively, while offering 4times memory reduction with only a 1.6% average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.', 'score': 2, 'issue_id': 3059, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': 'dbc6e89f0857b2e7', 'authors': ['Hung-Yueh Chiang', 'Chi-Chih Chang', 'Natalia Frumkin', 'Kai-Chiang Wu', 'Mohamed S. Abdelfattah', 'Diana Marculescu'], 'affiliations': ['Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin', 'Department of Computer Science, National Yang Ming Chiao Tung University', 'Department of Electrical and Computer Engineering, Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22879.jpg', 'data': {'categories': ['#open_source', '#inference', '#training', '#architecture', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Quamba2 - Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ (W8A8, W4A8, W4A16) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mamba1 Ğ¸ Mamba2. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº 8-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Quamba2-8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ SSM, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 1.3 Ğ¸ 3 Ñ€Ğ°Ğ·Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMLU Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°.'}, 'en': {'title': 'Quamba2: Efficient Quantization for State Space Models', 'desc': 'This paper introduces Quamba2, a method for quantizing State Space Models (SSMs) to improve their efficiency on various platforms. By using low bit-width data formats, Quamba2 reduces the model size and enhances performance without significant accuracy loss. The approach involves sorting and clustering inputs for better quantization of parameters, ensuring that the output remains stable despite the changes. Experimental results demonstrate that Quamba2 achieves significant speed-ups and memory reductions compared to existing SSM quantization techniques.'}, 'zh': {'title': 'é‡åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡ï¼', 'desc': 'çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºä¸€ç§æ–°å…´çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› å…¶ä¸€è‡´çš„å†…å­˜ä½¿ç”¨å’Œé«˜æ€§èƒ½è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨äº‘æœåŠ¡æˆ–èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šæ‰©å±•SSMsé¢ä¸´å­˜å‚¨å’Œè®¡ç®—èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½¿ç”¨ä½ä½å®½æ•°æ®æ ¼å¼å¯¹SSMsè¿›è¡Œé‡åŒ–å¯ä»¥å‡å°‘æ¨¡å‹å¤§å°ï¼Œå¹¶åˆ©ç”¨ç¡¬ä»¶åŠ é€Ÿã€‚æˆ‘ä»¬æå‡ºçš„Quamba2èƒ½å¤Ÿæ”¯æŒå¤šç§ä½å®½é…ç½®ï¼Œä¼˜åŒ–äº†SSMsåœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä¿æŒè¾ƒé«˜å‡†ç¡®ç‡çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†è®¡ç®—é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01201', 'title': 'Medical large language models are easily distracted', 'url': 'https://huggingface.co/papers/2504.01201', 'abstract': "Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information.", 'score': 1, 'issue_id': 3052, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'cfc7db001feb908d', 'authors': ['Krithik Vishwanath', 'Anton Alyakin', 'Daniel Alexander Alber', 'Jin Vivian Lee', 'Douglas Kondziolka', 'Eric Karl Oermann'], 'affiliations': ['Center for Data Science, New York University, New York, New York, 10016', 'Department of Aerospace Engineering and Engineering Mechanics, The University of Texas at Austin, Austin, Texas, 78712', 'Department of Mathematics, The University of Texas at Austin, Austin, Texas, 78712', 'Department of Neurological Surgery, NYU Langone Medical Center, New York, New York, 10016', 'Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, Missouri, 63110', 'Department of Radiology, NYU Langone Medical Center, New York, New York, 10016'], 'pdf_title_img': 'assets/pdf/title_img/2504.01201.jpg', 'data': {'categories': ['#benchmark', '#rag', '#healthcare', '#reasoning', '#hallucinations'], 'emoji': 'ğŸ©º', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ·ĞµÑ€ĞµĞ½ Ğ¾Ñ‚ Ğ¿Ğ»ĞµĞ²ĞµĞ»', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MedDistractQA, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ USMLE Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ½Ğ° 17.9%. Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº RAG Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğµ Ñ€ĞµÑˆĞ°ÑÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñƒ LLM Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing LLMs: Tackling Noise in Medical Contexts', 'desc': "This paper explores the challenges faced by large language models (LLMs) in medical settings, particularly when they encounter irrelevant information during clinical scenarios. The authors created a benchmark called MedDistractQA, which includes USMLE-style questions with distractions that mimic real-world clinical noise. Their research found that such distractions can significantly decrease the accuracy of LLMs, by as much as 17.9%. Additionally, common strategies to improve model performance, like retrieval-augmented generation and medical fine-tuning, did not alleviate the issue and sometimes worsened it, indicating a fundamental limitation in LLMs' ability to filter relevant clinical data."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦ä¸­çš„æŠ—å¹²æ‰°èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸå…·æœ‰å˜é©æ½œåŠ›ï¼Œä½†ç°å®ä¸´åºŠåœºæ™¯ä¸­å­˜åœ¨çš„å¤šä½™ä¿¡æ¯å¯èƒ½ä¼šå½±å“å…¶è¡¨ç°ã€‚æˆ‘ä»¬å¼€å‘äº†MedDistractQAåŸºå‡†ï¼Œä½¿ç”¨åµŒå…¥æ¨¡æ‹Ÿç°å®å¹²æ‰°çš„USMLEé£æ ¼é—®é¢˜æ¥è¯„ä¼°LLMsè¿‡æ»¤ç›¸å…³æ•°æ®çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå¹²æ‰°æ€§é™ˆè¿°ä¼šä½¿LLMçš„å‡†ç¡®æ€§é™ä½å¤šè¾¾17.9%ã€‚è¿™è¡¨æ˜LLMsåœ¨åŒºåˆ†ç›¸å…³ä¸æ— å…³ä¸´åºŠä¿¡æ¯æ–¹é¢ç¼ºä¹å¿…è¦çš„é€»è¾‘æœºåˆ¶ï¼Œç»™å®é™…åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18817', 'title': 'Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations', 'url': 'https://huggingface.co/papers/2503.18817', 'abstract': 'Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.', 'score': 1, 'issue_id': 3046, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'fa652fd57c6312f8', 'authors': ['Jeonghyeon Kim', 'Sangheum Hwang'], 'affiliations': ['Seoul National University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.18817.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#training', '#transfer_learning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² (OoDD) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (MMFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ OoDD. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ±Ğ»Ğ¸Ğ¶Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet-1k OoD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾ÑÑ‚-Ñ…Ğ¾Ğº Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ OoDD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Out-of-Distribution Detection with Multi-Modal Fine-Tuning', 'desc': 'This paper focuses on improving out-of-distribution detection (OoDD) using multi-modal fine-tuning (MMFT) with vision-language models like CLIP. The authors argue that traditional methods often freeze or partially tune pretrained weights, which limits performance on new datasets. They identify a key issue with na"ive fine-tuning methods, which fail to fully utilize the pretrained knowledge due to a modality gap in embeddings. To overcome this, they propose a new training objective that aligns image and text embeddings more effectively, leading to significant improvements in OoDD performance and overall accuracy on benchmark datasets.'}, 'zh': {'title': 'å¤šæ¨¡æ€å¾®è°ƒæå‡åˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¾®è°ƒï¼ˆMMFTï¼‰åœ¨åˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆOoDDï¼‰ä¸­çš„åº”ç”¨ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€æ¨¡æ€æ¨¡å‹ä¸Šï¼Œè€Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•é€šè¿‡å¢å¼ºå›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†OoDDæ€§èƒ½ã€‚æˆ‘ä»¬åˆ†æäº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒçš„çŸ¥è¯†ã€‚é€šè¿‡åœ¨ImageNet-1k OoDåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨OoDDæ€§èƒ½å’ŒIDå‡†ç¡®ç‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18924', 'title': 'MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for\n  Zero-Shot Speech Synthesis', 'url': 'https://huggingface.co/papers/2502.18924', 'abstract': 'While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces MegaTTS 3, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at https://sditdemo.github.io/sditdemo/.', 'score': 1, 'issue_id': 3053, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '538aaff0c9fb3421', 'authors': ['Ziyue Jiang', 'Yi Ren', 'Ruiqi Li', 'Shengpeng Ji', 'Boyang Zhang', 'Zhenhui Ye', 'Chen Zhang', 'Bai Jionghao', 'Xiaoda Yang', 'Jialong Zuo', 'Yu Zhang', 'Rui Liu', 'Xiang Yin', 'Zhou Zhao'], 'affiliations': ['ByteDance', 'Inner Mongolia University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18924.jpg', 'data': {'categories': ['#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸: ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ñ MegaTTS 3', 'desc': 'MegaTTS 3 - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑ‡Ğ¸. MegaTTS 3 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ĞºÑƒÑĞ¾Ñ‡Ğ½Ğ¾-Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Revolutionizing TTS with Sparse Alignment and Flexible Accent Control', 'desc': 'This paper presents MegaTTS 3, a zero-shot text-to-speech (TTS) system that addresses challenges in speech-text alignment. It introduces a sparse alignment algorithm that enhances the robustness and naturalness of generated speech by providing flexible alignment boundaries. The system also incorporates a multi-condition classifier-free guidance strategy for adjusting accent intensity and uses a piecewise rectified flow technique to speed up the generation process. Experiments show that MegaTTS 3 achieves top-tier speech quality while allowing for precise control over accent, generating high-quality audio efficiently.'}, 'zh': {'title': 'MegaTTS 3ï¼šé«˜è‡ªç„¶æ€§ä¸çµæ´»æ§åˆ¶çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œåä¸ºMegaTTS 3ï¼Œæ—¨åœ¨è§£å†³è¯­éŸ³ä¸æ–‡æœ¬å¯¹é½å»ºæ¨¡çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åˆ›æ–°çš„ç¨€ç–å¯¹é½ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸é™åˆ¶æœç´¢ç©ºé—´çš„æƒ…å†µä¸‹ï¼Œå‡å°‘å¯¹é½çš„éš¾åº¦ï¼Œä»è€Œæé«˜è¯­éŸ³çš„è‡ªç„¶æ€§ã€‚MegaTTS 3è¿˜ä½¿ç”¨æ— æ¡ä»¶åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥æ¥è°ƒæ•´å£éŸ³å¼ºåº¦ï¼Œå¹¶é‡‡ç”¨åˆ†æ®µä¿®æ­£æµæŠ€æœ¯åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMegaTTS 3åœ¨é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶æ”¯æŒå¯¹å£éŸ³å¼ºåº¦çš„çµæ´»æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18950', 'title': 'Target-Aware Video Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18950', 'abstract': "We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.", 'score': 0, 'issue_id': 3054, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '4009a703c8832026', 'authors': ['Taeksoo Kim', 'Hanbyul Joo'], 'affiliations': ['RLWRLD', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18950.jpg', 'data': {'categories': ['#video', '#diffusion', '#multimodal', '#robotics', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ°ĞºÑ‚ĞµÑ€Ğ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ°ĞºÑ‚ĞµÑ€ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ÑŒÑ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºÑƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ½Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ°ÑĞºÑƒ Ñ†ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ…Ğ¾Ğ´ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ†ĞµĞ»Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ.'}, 'en': {'title': 'Target-Aware Video Generation: Simplifying Human-Object Interaction', 'desc': "This paper introduces a target-aware video diffusion model that generates videos based on an input image of an actor interacting with a specified target, defined by a segmentation mask, while performing a desired action described by a text prompt. Unlike traditional models that depend on detailed structural or motion cues, this model simplifies the process by using just a mask, leveraging the strengths of pretrained models for realistic action generation. The model incorporates a special token to encode the target's spatial information, and it is fine-tuned with a novel cross-attention loss to ensure alignment between the target mask and the generated actions. Experimental results indicate that this approach significantly improves the accuracy of human-object interactions in generated videos, making it useful for applications like video content creation and 3D motion synthesis."}, 'zh': {'title': 'ç›®æ ‡æ„ŸçŸ¥è§†é¢‘ç”Ÿæˆï¼Œè½»æ¾å®ç°äºº-ç‰©ä½“äº’åŠ¨', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›®æ ‡æ„ŸçŸ¥çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®è¾“å…¥å›¾åƒç”Ÿæˆè§†é¢‘ï¼Œå…¶ä¸­æ¼”å‘˜ä¸æŒ‡å®šç›®æ ‡äº’åŠ¨å¹¶æ‰§è¡Œæ‰€éœ€åŠ¨ä½œã€‚è¯¥ç›®æ ‡é€šè¿‡åˆ†å‰²æ©ç å®šä¹‰ï¼Œæ‰€éœ€åŠ¨ä½œé€šè¿‡æ–‡æœ¬æç¤ºæè¿°ã€‚ä¸ç°æœ‰çš„å¯æ§å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…éœ€ç®€å•çš„æ©ç æ¥æŒ‡ç¤ºç›®æ ‡ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ç”Ÿæˆåˆç†çš„åŠ¨ä½œã€‚è¿™ç§æ–¹æ³•åœ¨å¤„ç†äºº-ç‰©ä½“äº’åŠ¨åœºæ™¯æ—¶ç‰¹åˆ«æœ‰æ•ˆï¼Œèƒ½å¤Ÿåœ¨æœºå™¨äººç­‰åº”ç”¨ä¸­å®ç°é«˜å±‚æ¬¡çš„åŠ¨ä½œè§„åˆ’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01990', 'title': 'Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems', 'url': 'https://huggingface.co/papers/2504.01990', 'abstract': 'The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.', 'score': 145, 'issue_id': 3069, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'f72a29b6411b97b1', 'authors': ['Bang Liu', 'Xinfeng Li', 'Jiayi Zhang', 'Jinlin Wang', 'Tanjin He', 'Sirui Hong', 'Hongzhang Liu', 'Shaokun Zhang', 'Kaitao Song', 'Kunlun Zhu', 'Yuheng Cheng', 'Suyuchen Wang', 'Xiaoqiang Wang', 'Yuyu Luo', 'Haibo Jin', 'Peiyan Zhang', 'Ollie Liu', 'Jiaqi Chen', 'Huan Zhang', 'Zhaoyang Yu', 'Haochen Shi', 'Boyan Li', 'Dekun Wu', 'Fengwei Teng', 'Xiaojun Jia', 'Jiawei Xu', 'Jinyu Xiang', 'Yizhang Lin', 'Tianming Liu', 'Tongliang Liu', 'Yu Su', 'Huan Sun', 'Glen Berseth', 'Jianyun Nie', 'Ian Foster', 'Logan Ward', 'Qingyun Wu', 'Yu Gu', 'Mingchen Zhuge', 'Xiangru Tang', 'Haohan Wang', 'Jiaxuan You', 'Chi Wang', 'Jian Pei', 'Qiang Yang', 'Xiaoliang Qi', 'Chenglin Wu'], 'affiliations': ['Argonne National Laboratory', 'Canada CIFAR AI Chair', 'Duke University', 'Google DeepMind', 'King Abdullah University of Science and Technology', 'MetaGPT', 'Microsoft Research Asia', 'Mila - Quebec AI Institute', 'Nanyang Technological University', 'Penn State University', 'Stanford University', 'The Hong Kong Polytechnic University', 'The Hong Kong University of Science and Technology', 'The Ohio State University', 'University of Georgia', 'University of Illinois at Urbana-Champaign', 'University of Southern California', 'University of Sydney', 'UniversitÃ© de MontrÃ©al', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01990.jpg', 'data': {'categories': ['#architecture', '#survey', '#security', '#ethics', '#agi', '#agents', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ: Ğ¾Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ¼, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸, Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Building Intelligent Agents: From Brain-Inspired Design to Safe AI Deployment', 'desc': 'This paper explores the development of large language models (LLMs) and their role in creating advanced intelligent agents that can reason, perceive, and act in various environments. It presents a modular architecture inspired by the human brain, detailing how cognitive, perceptual, and operational modules correspond to brain functions like memory and emotion. The paper also discusses how these agents can improve themselves through adaptive learning and optimization techniques, including AutoML. Finally, it emphasizes the importance of ensuring that AI systems are safe, ethical, and reliable for real-world applications.'}, 'zh': {'title': 'æ™ºèƒ½ä½“çš„æœªæ¥ï¼šä»å¤§è„‘å¯å‘åˆ°å®‰å…¨åº”ç”¨', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å˜é©æ€§å½±å“ï¼Œå¼ºè°ƒäº†æ™ºèƒ½ä½“çš„è®¾è®¡ã€è¯„ä¼°å’ŒæŒç»­æ”¹è¿›æ‰€é¢ä¸´çš„å¤æ‚æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†æ™ºèƒ½ä½“æ¡†æ¶ç½®äºæ¨¡å—åŒ–çš„ã€å—å¤§è„‘å¯å‘çš„æ¶æ„ä¸­ï¼Œç»“åˆäº†è®¤çŸ¥ç§‘å­¦ã€ç¥ç»ç§‘å­¦å’Œè®¡ç®—ç ”ç©¶çš„åŸåˆ™ã€‚æ–‡ç« åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼Œé¦–å…ˆåˆ†ææ™ºèƒ½ä½“çš„æ¨¡å—åŒ–åŸºç¡€ï¼Œæ˜ å°„å…¶è®¤çŸ¥ã€æ„ŸçŸ¥å’Œæ“ä½œæ¨¡å—ä¸äººç±»å¤§è„‘åŠŸèƒ½çš„ç›¸ä¼¼æ€§ã€‚æ¥ç€è®¨è®ºè‡ªæˆ‘å¢å¼ºå’Œé€‚åº”æ€§è¿›åŒ–æœºåˆ¶ï¼Œæœ€åå¼ºè°ƒæ„å»ºå®‰å…¨ã€å¯é å’Œæœ‰ç›Šçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02507', 'title': 'ZClip: Adaptive Spike Mitigation for LLM Pre-Training', 'url': 'https://huggingface.co/papers/2504.02507', 'abstract': 'Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.', 'score': 69, 'issue_id': 3065, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '290019150fe5b4c9', 'authors': ['Abhay Kumar', 'Louis Owen', 'Nilabhra Roy Chowdhury', 'Fabian GÃ¼ra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2504.02507.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ZClip: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ZClip Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ZClip Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ¾Ñ€Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ z-Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¾Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ZClip Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğµ Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ½Ğµ Ğ¼ĞµÑˆĞ°Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ….'}, 'en': {'title': 'ZClip: Smart Gradient Clipping for Stable LLM Training', 'desc': 'This paper addresses the challenges of training large language models (LLMs) by introducing ZClip, an adaptive gradient clipping algorithm. Traditional methods often fail to manage gradient instability and loss spikes effectively, leading to inefficient training and the need for manual adjustments. ZClip improves upon these methods by dynamically adjusting the clipping threshold based on the statistical behavior of gradient norms, allowing for a more responsive training process. By using z-score-based anomaly detection, ZClip prevents harmful loss spikes while maintaining the overall convergence of the model.'}, 'zh': {'title': 'è‡ªé€‚åº”æ¢¯åº¦è£å‰ªï¼Œæå‡è®­ç»ƒç¨³å®šæ€§', 'desc': 'åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¸¸å¸¸ä¼šé‡åˆ°æ¢¯åº¦ä¸ç¨³å®šå’ŒæŸå¤±å³°å€¼ç­‰é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´ç¾éš¾æ€§çš„å‘æ•£ã€‚ä¼ ç»Ÿçš„æ¢¯åº¦è£å‰ªæŠ€æœ¯æ— æ³•æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºå›ºå®šçš„é˜ˆå€¼æˆ–å¯å‘å¼æ–¹æ³•ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºZClipçš„è‡ªé€‚åº”æ¢¯åº¦è£å‰ªç®—æ³•ï¼Œå®ƒæ ¹æ®æ¢¯åº¦èŒƒæ•°çš„ç»Ÿè®¡ç‰¹æ€§åŠ¨æ€è°ƒæ•´è£å‰ªé˜ˆå€¼ã€‚ZClipé€šè¿‡åŸºäºz-scoreçš„å¼‚å¸¸æ£€æµ‹æ¥è¯†åˆ«å’Œå‡è½»å¤§æ¢¯åº¦å³°å€¼ï¼Œä»è€Œé˜²æ­¢æŸå¤±å³°å€¼ï¼ŒåŒæ—¶ä¸å¹²æ‰°æ”¶æ•›è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02826', 'title': 'Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing', 'url': 'https://huggingface.co/papers/2504.02826', 'abstract': 'Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.', 'score': 60, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'dbb1c07cd5a01838', 'authors': ['Xiangyu Zhao', 'Peiyuan Zhang', 'Kexian Tang', 'Hao Li', 'Zicheng Zhang', 'Guangtao Zhai', 'Junchi Yan', 'Hua Yang', 'Xue Yang', 'Haodong Duan'], 'affiliations': ['Shanghai Jiao Tong University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02826.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RISEBench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'RISEBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼. Ğ¢ĞµÑÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ LLM-ÑÑƒĞ´ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'RISEBench: Advancing Reasoning in Visual Editing', 'desc': 'This paper introduces RISEBench, a new benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) in Large Multi-modality Models (LMMs). It identifies challenges in visual editing, such as following complex instructions and maintaining appearance consistency. RISEBench categorizes reasoning into four types: Temporal, Causal, Spatial, and Logical, and provides a framework for assessing these reasoning types through both human and model evaluations. The findings indicate that even advanced models like GPT-4o-Native struggle with logical reasoning, suggesting a need for further research in this area.'}, 'zh': {'title': 'æ¨ç†é©±åŠ¨çš„è§†è§‰ç¼–è¾‘æ–°åŸºå‡†', 'desc': 'å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éµå¾ªå¤æ‚æŒ‡ä»¤ã€ä¿æŒå¤–è§‚ä¸€è‡´æ€§å’Œæ”¯æŒçµæ´»è¾“å…¥æ ¼å¼æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RISEBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨ç†é©±åŠ¨è§†è§‰ç¼–è¾‘ï¼ˆRISEï¼‰çš„åŸºå‡†ã€‚RISEBenchä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ï¼šæ—¶é—´æ¨ç†ã€å› æœæ¨ç†ã€ç©ºé—´æ¨ç†å’Œé€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡GPT-4o-Nativeåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä½†åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ï¼Œæ˜¾ç¤ºå‡ºè¿™ä¸€é¢†åŸŸä»éœ€æ·±å…¥æ¢ç´¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02782', 'title': 'GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation', 'url': 'https://huggingface.co/papers/2504.02782', 'abstract': "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.", 'score': 48, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '5346697bd326eed4', 'authors': ['Zhiyuan Yan', 'Junyan Ye', 'Weijia Li', 'Zilong Huang', 'Shenghai Yuan', 'Xiangyang He', 'Kaiqing Lin', 'Jun He', 'Conghui He', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Rabbitpre AI', 'Shanghai AI Laboratory', 'Shenzhen University', 'Sun Yat-sen University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2504.02782.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#architecture', '#diffusion', '#interpretability', '#optimization', '#hallucinations'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'GPT-4o: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº (GPT-ImgEval) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¾Ñ‚ OpenAI, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GPT-4o Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° GPT-4o Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ GPT-4o, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµĞµ Ñ Gemini 2.0 Flash Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unleashing the Power of GPT-4o in Image Generation and Editing', 'desc': "This paper evaluates the performance of OpenAI's GPT-4o model in image generation and editing using a new benchmark called GPT-ImgEval. The evaluation focuses on three key areas: the quality of generated images, the model's ability to edit images, and its understanding of semantic context. Results show that GPT-4o outperforms existing models in both image generation and editing, while also demonstrating strong reasoning capabilities. The paper also explores the model's architecture and limitations, providing insights for future research in image generation."}, 'zh': {'title': 'GPT-4oï¼šå›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†OpenAIçš„GPT-4oæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„æœ€æ–°çªç ´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºGPT-ImgEvalçš„è¯„ä¼°åŸºå‡†ï¼Œå®šé‡å’Œå®šæ€§åœ°åˆ†æäº†GPT-4oåœ¨ç”Ÿæˆè´¨é‡ã€ç¼–è¾‘èƒ½åŠ›å’ŒçŸ¥è¯†æ¨ç†ç­‰ä¸‰ä¸ªå…³é”®ç»´åº¦çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGPT-4oåœ¨å›¾åƒç”Ÿæˆæ§åˆ¶å’Œè¾“å‡ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å“è¶Šçš„çŸ¥è¯†æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†GPT-4oçš„æ¶æ„ï¼Œå¹¶è¯†åˆ«äº†å…¶åœ¨å›¾åƒç”Ÿæˆä¸­å¸¸è§çš„åˆæˆä¼ªå½±å’Œå±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23377', 'title': 'JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical\n  Spatio-Temporal Prior Synchronization', 'url': 'https://huggingface.co/papers/2503.23377', 'abstract': 'This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce a fine-grained spatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatio-temporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose a new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise a robust metric for evaluating the synchronization between generated audio-video pairs in real-world complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting a new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/.', 'score': 41, 'issue_id': 3076, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': '5437e5bc1ca6fe1e', 'authors': ['Kai Liu', 'Wei Li', 'Lai Chen', 'Shengqiong Wu', 'Yanhao Zheng', 'Jiayi Ji', 'Fan Zhou', 'Rongxin Jiang', 'Jiebo Luo', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['National University of Singapore', 'University of Rochester', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23377.jpg', 'data': {'categories': ['#audio', '#benchmark', '#multimodal', '#diffusion', '#dataset', '#open_source', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ JavisDiT - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ HiST-Sypo Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº JavisBench Ğ¸Ğ· 10,140 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ JavisDiT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'JavisDiT: Synchronizing Audio and Video Generation with Precision', 'desc': "This paper presents JavisDiT, a new model for generating audio and video together, called Joint Audio-Video Diffusion Transformer (JAVG). It uses a special architecture called Diffusion Transformer (DiT) to create high-quality content based on user prompts. To keep the audio and video in sync, the model includes a mechanism that aligns their timing and spatial features through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. Additionally, the authors introduce a benchmark dataset, JavisBench, to evaluate the model's performance in generating synchronized audio-video pairs, showing that JavisDiT outperforms existing methods in both quality and synchronization."}, 'zh': {'title': 'JavisDiTï¼šéŸ³è§†é¢‘ç”Ÿæˆçš„æ–°æ ‡å‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„è”åˆéŸ³è§†é¢‘æ‰©æ•£å˜æ¢å™¨JavisDiTï¼Œæ—¨åœ¨å®ç°åŒæ­¥çš„éŸ³è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¨¡å‹åŸºäºå¼ºå¤§çš„æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œèƒ½å¤Ÿä»å¼€æ”¾å¼ç”¨æˆ·æç¤ºä¸­åŒæ—¶ç”Ÿæˆé«˜è´¨é‡çš„éŸ³é¢‘å’Œè§†é¢‘å†…å®¹ã€‚ä¸ºäº†ç¡®ä¿æœ€ä½³åŒæ­¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»†ç²’åº¦çš„æ—¶ç©ºå¯¹é½æœºåˆ¶ï¼Œé€šè¿‡å±‚æ¬¡åŒ–æ—¶ç©ºåŒæ­¥å…ˆéªŒä¼°è®¡å™¨ï¼ˆHiST-Sypoï¼‰æå–å…¨å±€å’Œç»†ç²’åº¦çš„æ—¶ç©ºå…ˆéªŒï¼ŒæŒ‡å¯¼è§†è§‰å’Œå¬è§‰ç»„ä»¶ä¹‹é—´çš„åŒæ­¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†JavisBenchï¼ŒåŒ…å«10,140ä¸ªé«˜è´¨é‡çš„æ–‡æœ¬æ ‡æ³¨éŸ³è§†é¢‘ï¼Œæ¶µç›–å¤šæ ·çš„åœºæ™¯å’Œå¤æ‚çš„ç°å®ä¸–ç•Œåœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00939', 'title': 'WikiVideo: Article Generation from Multiple Videos', 'url': 'https://huggingface.co/papers/2504.00939', 'abstract': "We present the challenging task of automatically creating a high-level Wikipedia-style article that aggregates information from multiple diverse videos about real-world events, such as natural disasters or political elections. Videos are intuitive sources for retrieval-augmented generation (RAG), but most contemporary RAG workflows focus heavily on text and existing methods for video-based summarization focus on low-level scene understanding rather than high-level event semantics. To close this gap, we introduce WikiVideo, a benchmark consisting of expert-written articles and densely annotated videos that provide evidence for articles' claims, facilitating the integration of video into RAG pipelines and enabling the creation of in-depth content that is grounded in multimodal sources. We further propose Collaborative Article Generation (CAG), a novel interactive method for article creation from multiple videos. CAG leverages an iterative interaction between an r1-style reasoning model and a VideoLLM to draw higher level inferences about the target event than is possible with VideoLLMs alone, which fixate on low-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in both oracle retrieval and RAG settings and find that CAG consistently outperforms alternative methods, while suggesting intriguing avenues for future work.", 'score': 30, 'issue_id': 3076, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'd04bc5c674ac39fc', 'authors': ['Alexander Martin', 'Reno Kriz', 'William Gantt Walden', 'Kate Sanders', 'Hannah Recknor', 'Eugene Yang', 'Francis Ferraro', 'Benjamin Van Durme'], 'affiliations': ['Human Language Technology Center of Excellence', 'Johns Hopkins University', 'University of Maryland Baltimore County'], 'pdf_title_img': 'assets/pdf/title_img/2504.00939.jpg', 'data': {'categories': ['#survey', '#benchmark', '#rag', '#reasoning', '#multimodal', '#video'], 'emoji': 'ğŸ“½ï¸', 'ru': {'title': 'Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ WikiVideo, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Collaborative Article Generation (CAG), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ VideoLLM Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Transforming Videos into Knowledge: The Future of Article Generation', 'desc': 'This paper addresses the challenge of generating comprehensive Wikipedia-style articles from various videos about real-world events. It highlights the limitations of current retrieval-augmented generation (RAG) methods that primarily focus on text and low-level video analysis. The authors introduce WikiVideo, a benchmark that combines expert-written articles with annotated videos to enhance the integration of video content into RAG workflows. They also propose Collaborative Article Generation (CAG), an innovative approach that uses an interactive model to derive higher-level insights from videos, outperforming existing methods in generating detailed and contextually rich articles.'}, 'zh': {'title': 'è§†é¢‘é©±åŠ¨çš„é«˜æ°´å¹³æ–‡ç« ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå³è‡ªåŠ¨åˆ›å»ºé«˜æ°´å¹³çš„ç»´åŸºç™¾ç§‘é£æ ¼æ–‡ç« ï¼Œæ±‡æ€»å…³äºç°å®äº‹ä»¶ï¼ˆå¦‚è‡ªç„¶ç¾å®³æˆ–æ”¿æ²»é€‰ä¸¾ï¼‰çš„å¤šç§è§†é¢‘ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†WikiVideoåŸºå‡†ï¼ŒåŒ…å«ä¸“å®¶æ’°å†™çš„æ–‡ç« å’Œå¯†é›†æ³¨é‡Šçš„è§†é¢‘ï¼Œä»¥æ”¯æŒæ–‡ç« çš„è®ºç‚¹ï¼Œä»è€Œä¿ƒè¿›è§†é¢‘åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹ä¸­çš„æ•´åˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åä½œæ–‡ç« ç”Ÿæˆï¼ˆCAGï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¸r1é£æ ¼æ¨ç†æ¨¡å‹å’ŒVideoLLMçš„è¿­ä»£äº’åŠ¨ï¼Œèƒ½å¤Ÿå¯¹ç›®æ ‡äº‹ä»¶è¿›è¡Œæ›´é«˜å±‚æ¬¡çš„æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAGåœ¨å„ç±»è®¾ç½®ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†æœªæ¥ç ”ç©¶çš„æœ‰è¶£æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02587', 'title': 'Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme', 'url': 'https://huggingface.co/papers/2504.02587', 'abstract': 'Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.', 'score': 27, 'issue_id': 3063, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '58300c3a6e30995f', 'authors': ['Yan Ma', 'Steffi Chern', 'Xuyang Shen', 'Yiran Zhong', 'Pengfei Liu'], 'affiliations': ['Fudan University', 'Generative Artificial Intelligence Lab (GAIR)', 'Minimax', 'SII', 'Shanghai Jiao Tong University (SJTU)'], 'pdf_title_img': 'assets/pdf/title_img/2504.02587.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ RL Ğ² VLM, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Vision-Language Models!', 'desc': 'This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks.'}, 'zh': {'title': 'å»ºç«‹å¯é‡å¤çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå¹¶æ­£åœ¨ç§¯ææ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLåº”ç”¨å¾€å¾€ä¾èµ–äºå¤æ‚çš„æ¡†æ¶ï¼Œé™åˆ¶äº†å¯é‡å¤æ€§å’Œå¯è®¿é—®æ€§ï¼ŒåŒæ—¶ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œä½¿å¾—ç»“æœæ¯”è¾ƒå’Œè®­ç»ƒåŠ¨æ€è§£é‡Šå˜å¾—å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€æ˜çš„ã€ä»é›¶å¼€å§‹çš„RLæ¡†æ¶ï¼Œæä¾›äº†ä¸€ä¸ªç»è¿‡å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†éªŒè¯çš„æœ€å°åŠŸèƒ½å››æ­¥æµç¨‹ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥è¯„ä¼°è®­ç»ƒåŠ¨æ€å’Œåæ€è¡Œä¸ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02495', 'title': 'Inference-Time Scaling for Generalist Reward Modeling', 'url': 'https://huggingface.co/papers/2504.02495', 'abstract': 'Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.', 'score': 25, 'issue_id': 3071, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '07408fa4b72ccb6c', 'authors': ['Zijun Liu', 'Peiyi Wang', 'Runxin Xu', 'Shirong Ma', 'Chong Ruan', 'Peng Li', 'Yang Liu', 'Yu Wu'], 'affiliations': ['DeepSeek-AI', 'Dept. of Computer Sci. & Tech., Tsinghua University', 'Institute for AI Industry Research (AIR), Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02495.jpg', 'data': {'categories': ['#training', '#benchmark', '#rl', '#rlhf', '#reasoning', '#open_source', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RM) Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Principled Critique Tuning (SPCT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ°-RM Ğ´Ğ»Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SPCT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (GRM), Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… RM.'}, 'en': {'title': 'Enhancing Language Models with Scalable Reward Learning', 'desc': 'This paper explores advancements in reinforcement learning (RL) for enhancing large language models (LLMs) by focusing on reward modeling (RM) for general queries. The authors introduce a novel approach called Self-Principled Critique Tuning (SPCT) that improves the generation of reward signals, enabling better inference-time scalability. They also propose a pointwise generative reward modeling (GRM) technique that allows flexibility in handling different input types. Empirical results demonstrate that SPCT significantly enhances the quality and scalability of GRMs, outperforming existing methods while addressing challenges in generalist reward systems.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åè®­ç»ƒä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ”¹è¿›å¥–åŠ±å»ºæ¨¡ï¼ˆRMï¼‰æ¥æé«˜ä¸€èˆ¬æŸ¥è¯¢çš„æ¨ç†æ—¶é—´å¯æ‰©å±•æ€§ï¼Œå¹¶æå‡ºäº†è‡ªæˆ‘åŸåˆ™æ‰¹è¯„è°ƒä¼˜ï¼ˆSPCTï¼‰æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›GRMä¸­çš„å¯æ‰©å±•å¥–åŠ±ç”Ÿæˆè¡Œä¸ºã€‚æˆ‘ä»¬é‡‡ç”¨ç‚¹å¯¹ç‚¹ç”Ÿæˆå¥–åŠ±å»ºæ¨¡ï¼ˆGRMï¼‰ï¼Œä»¥é€‚åº”ä¸åŒè¾“å…¥ç±»å‹å¹¶å®ç°æ¨ç†æ—¶é—´çš„å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPCTæ˜¾è‘—æé«˜äº†GRMçš„è´¨é‡å’Œå¯æ‰©å±•æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•å’Œæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02398', 'title': 'Scaling Analysis of Interleaved Speech-Text Language Models', 'url': 'https://huggingface.co/papers/2504.02398', 'abstract': 'Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.', 'score': 24, 'issue_id': 3067, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'c03d1b64b7e6e276', 'authors': ['Gallil Maimon', 'Michael Hassid', 'Amit Roth', 'Yossi Adi'], 'affiliations': ['Department of Computer Science and Engineering, Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2504.02398.jpg', 'data': {'categories': ['#dataset', '#data', '#audio', '#open_source', '#synthetic', '#transfer_learning', '#training'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (SLM), Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾, Ñ‡ĞµĞ¼ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ»ĞµĞ¹Ğ²Ğ½Ñ‹Ñ… SLM, Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµÑÑÑ‚ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ² Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ñ‚Ğ°ĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ğ½Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Interleaved SLMs: Efficient Scaling for Speech Models!', 'desc': 'This paper investigates the efficiency of interleaved Speech Language Models (SLMs) compared to traditional textless SLMs. It finds that interleaved SLMs, which leverage pre-trained Text Language Models (TextLMs), require less compute and data while achieving competitive performance on speech tasks. The authors conduct a scaling analysis that reveals distinct scaling dynamics, suggesting a need for more compute allocation towards model size rather than training data. Additionally, the study highlights the importance of synthetic data and various TextLM families in enhancing the performance of SLMs.'}, 'zh': {'title': 'äº¤é”™SLMï¼šæ›´é«˜æ•ˆçš„æ‰©å±•ä¹‹è·¯', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†äº¤é”™è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ‰©å±•æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œäº¤é”™SLMåœ¨è®¡ç®—èµ„æºçš„ä½¿ç”¨ä¸Šæ¯”æ— æ–‡æœ¬SLMæ›´ä¸ºé«˜æ•ˆã€‚æˆ‘ä»¬å‘ç°ï¼Œäº¤é”™SLMçš„æ‰©å±•åŠ¨æ€ä¸æ— æ–‡æœ¬SLMæ˜¾è‘—ä¸åŒï¼Œå»ºè®®åœ¨å¢åŠ æ¨¡å‹è§„æ¨¡æ—¶åº”æ›´å¤šåœ°åˆ†é…è®¡ç®—é¢„ç®—ã€‚æœ€ç»ˆï¼Œç»è¿‡æ‰©å±•çš„æ¨¡å‹åœ¨è¯­éŸ³è¯­ä¹‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºä¸é¢†å…ˆæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨çš„è®¡ç®—å’Œæ•°æ®é‡æ›´å°‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02436', 'title': 'SkyReels-A2: Compose Anything in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2504.02436', 'abstract': 'This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.', 'score': 22, 'issue_id': 3063, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '86b46513a72dbd76', 'authors': ['Zhengcong Fei', 'Debang Li', 'Di Qiu', 'Jiahua Wang', 'Yikun Dou', 'Rui Wang', 'Jingtao Xu', 'Mingyuan Fan', 'Guibin Chen', 'Yang Li', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.02436.jpg', 'data': {'categories': ['#inference', '#multimodal', '#video', '#benchmark', '#open_source'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'SkyReels-A2 - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. SkyReels-A2 ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (E2V).'}, 'en': {'title': 'SkyReels-A2: Mastering Video Generation with Element Control', 'desc': 'This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation.'}, 'zh': {'title': 'SkyReels-A2ï¼šå¯æ§è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†SkyReels-A2ï¼Œä¸€ä¸ªå¯æ§çš„è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºå°†ä»»æ„è§†è§‰å…ƒç´ ï¼ˆå¦‚è§’è‰²ã€ç‰©ä½“ã€èƒŒæ™¯ï¼‰ç»„åˆæˆåˆæˆè§†é¢‘ï¼ŒåŒæ—¶ä¿æŒä¸æ¯ä¸ªå…ƒç´ çš„å‚è€ƒå›¾åƒçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†è¿™ä¸€ä»»åŠ¡ç§°ä¸ºå…ƒç´ åˆ°è§†é¢‘ï¼ˆE2Vï¼‰ï¼Œå…¶ä¸»è¦æŒ‘æˆ˜åœ¨äºä¿æŒæ¯ä¸ªå‚è€ƒå…ƒç´ çš„çœŸå®æ€§ï¼Œç¡®ä¿åœºæ™¯çš„è¿è´¯æ€§ï¼Œä»¥åŠå®ç°è‡ªç„¶çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®ç®¡é“ï¼Œä»¥æ„å»ºæç¤º-å‚è€ƒ-è§†é¢‘ä¸‰å…ƒç»„ç”¨äºæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è§†é¢‘ï¼Œå¹¶å®ç°ç²¾ç¡®çš„å…ƒç´ æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02542', 'title': 'Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation', 'url': 'https://huggingface.co/papers/2504.02542', 'abstract': 'Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.', 'score': 19, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'fa93ea3aeacd0dbc', 'authors': ['Fa-Ting Hong', 'Zunnan Xu', 'Zixiang Zhou', 'Jun Zhou', 'Xiu Li', 'Qin Lin', 'Qinglin Lu', 'Dan Xu'], 'affiliations': ['HKUST', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02542.jpg', 'data': {'categories': ['#video', '#multimodal', '#diffusion'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼', 'desc': 'ACTalker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ mamba Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²ĞµÑ‚Ğ²ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ACTalker Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'ACTalker: Multi-Signal Control for Natural Talking Head Synthesis', 'desc': 'This paper presents ACTalker, a novel framework for generating talking head videos that can be controlled by multiple signals simultaneously. Unlike traditional methods that rely on a single control modality, ACTalker employs a parallel mamba structure with multiple branches, each dedicated to a specific facial region. A gate mechanism allows for flexible control, ensuring that different driving signals can manipulate facial features without interference. The introduction of a mask-drop strategy further enhances this capability, enabling independent control of facial regions and resulting in more natural and coordinated video outputs.'}, 'zh': {'title': 'å¤šä¿¡å·æ§åˆ¶çš„å¯¹è¯å¤´åƒç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºACTalkerçš„ç«¯åˆ°ç«¯è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆè™šæ‹Ÿå¤´åƒçš„å¯¹è¯è§†é¢‘ã€‚è¯¥æ–¹æ³•æ”¯æŒå¤šä¿¡å·å’Œå•ä¿¡å·æ§åˆ¶ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡è®¾è®¡å¹¶è¡Œçš„mambaç»“æ„ï¼Œå…è®¸ä¸åŒçš„é©±åŠ¨ä¿¡å·æ§åˆ¶é¢éƒ¨çš„ç‰¹å®šåŒºåŸŸï¼Œå¹¶ä½¿ç”¨é—¨æ§æœºåˆ¶å®ç°çµæ´»æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACTalkerèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶çš„é¢éƒ¨è§†é¢‘ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ— å†²çªåœ°æ•´åˆå¤šç§é©±åŠ¨ä¿¡å·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00502', 'title': 'ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers', 'url': 'https://huggingface.co/papers/2504.00502', 'abstract': "Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV", 'score': 17, 'issue_id': 3067, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '1b236225c1d92fd7', 'authors': ['Qianhao Yuan', 'Qingyu Zhang', 'Yanjiang Liu', 'Jiawei Chen', 'Yaojie Lu', 'Hongyu Lin', 'Jia Zheng', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.00502.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal', '#inference'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ MLLM: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ° Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ÑĞ»Ğ¾ĞµĞ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Layer Contribution (LC). ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ MLLM Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ShortV, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ½Ğ¸Ñ…. ShortV Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 60% ÑĞ»Ğ¾ĞµĞ² MLLM, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimizing MLLMs: Freeze the Unnecessary Layers!', 'desc': 'This paper addresses the high computational costs associated with Multimodal Large Language Models (MLLMs) by analyzing layer-wise redundancy. It introduces a new metric called Layer Contribution (LC) to measure how much each layer affects the processing of visual and text tokens. The findings indicate that many layers contribute little to the processing of visual tokens, allowing for optimization. The authors propose a method called ShortV, which identifies and freezes these ineffective layers, resulting in significant reductions in computational costs while preserving model performance.'}, 'zh': {'title': 'ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹ï¼Œé™ä½è®¡ç®—æˆæœ¬', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºå…¶åºå¤§çš„è§„æ¨¡å’Œå¤§é‡çš„è§†è§‰æ ‡è®°ï¼Œé¢ä¸´ç€é«˜è®¡ç®—æˆæœ¬çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åº¦é‡æ ‡å‡†â€”â€”å±‚è´¡çŒ®ï¼ˆLayer Contributionï¼ŒLCï¼‰ï¼Œç”¨äºé‡åŒ–æ¨¡å‹ä¸­å„å±‚å¯¹è§†è§‰å’Œæ–‡æœ¬æ ‡è®°çš„å½±å“ã€‚é€šè¿‡è®¡ç®—å»é™¤æŸå±‚å˜æ¢åæ¨¡å‹è¾“å‡ºçš„å·®å¼‚ï¼ŒLCèƒ½å¤Ÿè¯„ä¼°è¯¥å±‚çš„è´¡çŒ®ã€‚å®éªŒè¡¨æ˜ï¼Œè®¸å¤šå±‚åœ¨å¤„ç†è§†è§‰æ ‡è®°æ—¶çš„è´¡çŒ®å¾ˆå°ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ShortVæ–¹æ³•ï¼Œèƒ½å¤Ÿè¯†åˆ«å¹¶å†»ç»“è¿™äº›æ— æ•ˆå±‚ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02154', 'title': 'FreSca: Unveiling the Scaling Space in Diffusion Models', 'url': 'https://huggingface.co/papers/2504.02154', 'abstract': "Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a ``scaling space'' whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets.", 'score': 12, 'issue_id': 3074, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '1c4d8a95379fa9a1', 'authors': ['Chao Huang', 'Susan Liang', 'Yunlong Tang', 'Li Ma', 'Yapeng Tian', 'Chenliang Xu'], 'affiliations': ['Netflix Eyeline Studios', 'The University of Texas at Dallas', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.02154.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#optimization', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¤ÑƒÑ€ÑŒĞµ-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾- Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ FreSca, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ğ¼ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. FreSca ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹.'}, 'en': {'title': 'Unlocking Image Control with Frequency-Based Scaling', 'desc': "This paper explores the potential of diffusion models in image tasks, focusing on how noise predictions can be manipulated for better control. It introduces a concept called 'scaling space' that allows for fine-tuned semantic editing by analyzing the differences in noise predictions. The authors present FreSca, a novel method that applies guidance scaling to different frequency components of noise in the Fourier domain. This approach not only improves image editing techniques but also enhances performance in image understanding tasks like depth estimation across various datasets."}, 'zh': {'title': 'æ¢ç´¢æ‰©æ•£æ¨¡å‹çš„ç¼©æ”¾ç©ºé—´', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä»»åŠ¡ä¸­æä¾›äº†å‡ºè‰²çš„å¯æ§æ€§ï¼Œä¸»è¦é€šè¿‡å™ªå£°é¢„æµ‹æ¥ç¼–ç ç‰¹å®šä»»åŠ¡çš„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æ— åˆ†ç±»å™¨å¼•å¯¼å®ç°å¯è°ƒç¼©æ”¾ã€‚æœ¬æ–‡æ¢è®¨äº†è¿™ç§ç¼©æ”¾æœºåˆ¶æ‰€å®šä¹‰çš„â€œç¼©æ”¾ç©ºé—´â€ï¼Œå¹¶é‡ç‚¹ç ”ç©¶äº†åŸºäºåæ¼”çš„ç¼–è¾‘æ–¹æ³•ï¼Œå…¶ä¸­æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°é¢„æµ‹ä¹‹é—´çš„å·®å¼‚æºå¸¦äº†é‡è¦çš„è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡å¯¹å™ªå£°é¢„æµ‹çš„å‚…é‡Œå¶åˆ†æï¼Œå‘ç°å…¶ä½é¢‘å’Œé«˜é¢‘æˆåˆ†åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä»¥ä¸åŒæ–¹å¼æ¼”å˜ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†FreScaæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‚…é‡Œå¶åŸŸä¸­ç‹¬ç«‹åœ°å¯¹ä¸åŒé¢‘å¸¦åº”ç”¨å¼•å¯¼ç¼©æ”¾ï¼Œä»è€Œæ˜¾è‘—æå‡ç°æœ‰çš„å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02119', 'title': 'Efficient Model Selection for Time Series Forecasting via LLMs', 'url': 'https://huggingface.co/papers/2504.02119', 'abstract': 'Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.', 'score': 11, 'issue_id': 3063, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '7c31e20ce0a7813b', 'authors': ['Wang Wei', 'Tiankai Yang', 'Hongjie Chen', 'Ryan A. Rossi', 'Yue Zhao', 'Franck Dernoncourt', 'Hoda Eldardiry'], 'affiliations': ['Adobe Research San Jose, CA, USA', 'Adobe Research Seattle, WA, USA', 'Department of Computer Science University of South California Los Angeles, CA, USA', 'Department of Computer Science Virginia Tech Blacksburg, VA, USA', 'Dolby Labs Atlanta, GA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.02119.jpg', 'data': {'categories': ['#dataset', '#training', '#reasoning', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'LLM ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ LLaMA, GPT Ğ¸ Gemini Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» LLM Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Model Selection with Large Language Models', 'desc': 'This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain.'}, 'zh': {'title': 'åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ—¶é—´åºåˆ—é¢„æµ‹çš„æ¨¡å‹é€‰æ‹©', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ¨¡å‹é€‰æ‹©é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›çš„æ€§èƒ½è¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè½»é‡çº§æ›¿ä»£æ–¹æ¡ˆçš„æ–¹æ³•ï¼Œé¿å…äº†æ„å»ºæ˜‚è´µçš„æ€§èƒ½çŸ©é˜µã€‚é€šè¿‡ä¸LLaMAã€GPTå’ŒGeminiçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„å…ƒå­¦ä¹ æŠ€æœ¯å’Œå¯å‘å¼åŸºçº¿ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†LLMsåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­é«˜æ•ˆæ¨¡å‹é€‰æ‹©çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22444', 'title': 'Scaling Laws in Scientific Discovery with AI and Robot Scientists', 'url': 'https://huggingface.co/papers/2503.22444', 'abstract': "Scientific discovery is poised for rapid advancement through advanced robotics and artificial intelligence. Current scientific practices face substantial limitations as manual experimentation remains time-consuming and resource-intensive, while multidisciplinary research demands knowledge integration beyond individual researchers' expertise boundaries. Here, we envision an autonomous generalist scientist (AGS) concept combines agentic AI and embodied robotics to automate the entire research lifecycle. This system could dynamically interact with both physical and virtual environments while facilitating the integration of knowledge across diverse scientific disciplines. By deploying these technologies throughout every research stage -- spanning literature review, hypothesis generation, experimentation, and manuscript writing -- and incorporating internal reflection alongside external feedback, this system aims to significantly reduce the time and resources needed for scientific discovery. Building on the evolution from virtual AI scientists to versatile generalist AI-based robot scientists, AGS promises groundbreaking potential. As these autonomous systems become increasingly integrated into the research process, we hypothesize that scientific discovery might adhere to new scaling laws, potentially shaped by the number and capabilities of these autonomous systems, offering novel perspectives on how knowledge is generated and evolves. The adaptability of embodied robots to extreme environments, paired with the flywheel effect of accumulating scientific knowledge, holds the promise of continually pushing beyond both physical and intellectual frontiers.", 'score': 11, 'issue_id': 3069, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': 'c2d75e49d08273c1', 'authors': ['Pengsong Zhang', 'Heng Zhang', 'Huazhe Xu', 'Renjun Xu', 'Zhenting Wang', 'Cong Wang', 'Animesh Garg', 'Zhibin Li', 'Arash Ajoudani', 'Xinyu Liu'], 'affiliations': ['Georgia Tech', 'Harvard University', 'Istituto Italiano di Tecnologia', 'Rutgers University', 'Tsinghua University', 'Universita di Genova', 'University College of London', 'University of Toronto', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22444.jpg', 'data': {'categories': ['#agents', '#robotics', '#agi', '#science'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ ÑƒÑ‡Ñ‘Ğ½Ñ‹Ğ¹-ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ñ‘Ğ½Ğ¾Ğ³Ğ¾-ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»Ğ° (AGS), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° AGS Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½. ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ AGS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸ĞµĞ¼ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½ÑÑ‚ÑŒÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğ¼ Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing Science with Autonomous Generalist Scientists', 'desc': 'This paper introduces the concept of an Autonomous Generalist Scientist (AGS) that combines artificial intelligence and robotics to automate the entire scientific research process. The AGS can interact with both physical and virtual environments, facilitating knowledge integration across various scientific fields. By automating tasks such as literature review, hypothesis generation, experimentation, and manuscript writing, the AGS aims to significantly reduce the time and resources required for scientific discovery. The authors suggest that as these systems become more integrated into research, they could change how knowledge is generated and evolve, potentially leading to new scaling laws in scientific discovery.'}, 'zh': {'title': 'è‡ªä¸»ç§‘å­¦å®¶ï¼šåŠ é€Ÿç§‘å­¦å‘ç°çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªä¸»é€šç”¨ç§‘å­¦å®¶ï¼ˆAGSï¼‰çš„æ¦‚å¿µï¼Œç»“åˆäº†æ™ºèƒ½ä»£ç†AIå’Œå…·èº«æœºå™¨äººï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–æ•´ä¸ªç ”ç©¶ç”Ÿå‘½å‘¨æœŸã€‚è¯¥ç³»ç»Ÿèƒ½å¤ŸåŠ¨æ€ä¸ç‰©ç†å’Œè™šæ‹Ÿç¯å¢ƒäº’åŠ¨ï¼Œå¹¶ä¿ƒè¿›ä¸åŒç§‘å­¦å­¦ç§‘ä¹‹é—´çš„çŸ¥è¯†æ•´åˆã€‚é€šè¿‡åœ¨æ–‡çŒ®å›é¡¾ã€å‡è®¾ç”Ÿæˆã€å®éªŒå’Œè®ºæ–‡å†™ä½œç­‰ç ”ç©¶é˜¶æ®µåº”ç”¨è¿™äº›æŠ€æœ¯ï¼ŒAGSå¸Œæœ›æ˜¾è‘—å‡å°‘ç§‘å­¦å‘ç°æ‰€éœ€çš„æ—¶é—´å’Œèµ„æºã€‚éšç€è¿™äº›è‡ªä¸»ç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°èå…¥ç ”ç©¶è¿‡ç¨‹ï¼Œç§‘å­¦å‘ç°å¯èƒ½ä¼šéµå¾ªæ–°çš„è§„æ¨¡æ³•åˆ™ï¼Œæä¾›å…³äºçŸ¥è¯†ç”Ÿæˆå’Œæ¼”å˜çš„æ–°è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01871', 'title': 'Interpreting Emergent Planning in Model-Free Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.01871', 'abstract': "We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL", 'score': 10, 'issue_id': 3069, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'bf12d8cbe28bb942', 'authors': ['Thomas Bush', 'Stephen Chung', 'Usman Anwar', 'AdriÃ  Garriga-Alonso', 'David Krueger'], 'affiliations': ['FAR AI', 'Mila, University of Montreal', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2504.01871.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#benchmark', '#rl', '#games', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¸Ğ³Ñ€Ğµ Sokoban. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ DRC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ° Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Unveiling Planning in Model-Free Reinforcement Learning Agents', 'desc': 'This paper provides evidence that model-free reinforcement learning agents can learn to plan by using concept-based interpretability. The authors focus on a model-free agent called DRC, which learns to create internal plans that predict the outcomes of its actions in the Sokoban environment. They explore how the agent identifies relevant concepts, forms plans, and how these plans affect its behavior through interventions. The study reveals that as the agent develops planning capabilities, it also shows improved performance with additional computational resources, resembling advanced search algorithms.'}, 'zh': {'title': 'æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„è§„åˆ’èƒ½åŠ›', 'desc': 'æœ¬æ–‡é¦–æ¬¡æä¾›äº†æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä»£ç†èƒ½å¤Ÿå­¦ä¹ è§„åˆ’çš„æœºåˆ¶æ€§è¯æ®ã€‚æˆ‘ä»¬é€šè¿‡åœ¨Sokobanè¿™ä¸€å¸¸ç”¨åŸºå‡†ä¸Šåº”ç”¨åŸºäºæ¦‚å¿µçš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œå±•ç¤ºäº†DRCä»£ç†å¦‚ä½•åˆ©ç”¨å­¦ä¹ åˆ°çš„æ¦‚å¿µè¡¨ç¤ºæ¥å†…éƒ¨åˆ¶å®šè®¡åˆ’ã€‚å…·ä½“è€Œè¨€ï¼Œä»£ç†èƒ½å¤Ÿé¢„æµ‹è¡ŒåŠ¨å¯¹ç¯å¢ƒçš„é•¿æœŸå½±å“ï¼Œå¹¶å½±å“è¡ŒåŠ¨é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»£ç†çš„è®¡åˆ’å½¢æˆä¸å…¶è¡Œä¸ºä¹‹é—´å­˜åœ¨å› æœå…³ç³»ï¼Œå¹¶ä¸”è¿™ç§è®¡åˆ’çš„å‡ºç°ä¸ä»£ç†åœ¨æµ‹è¯•æ—¶åˆ©ç”¨é¢å¤–è®¡ç®—èƒ½åŠ›çš„èƒ½åŠ›ç›¸å»åˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23162', 'title': 'NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact\n  3D Representations', 'url': 'https://huggingface.co/papers/2503.23162', 'abstract': '3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and a complex encoding and quantization strategy. In this paper, we aim to develop a simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into a compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 45-times average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields.', 'score': 9, 'issue_id': 3073, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '33d8d348d9e9fccf', 'authors': ['Zhenyu Tang', 'Chaoran Feng', 'Xinhua Cheng', 'Wangbo Yu', 'Junwu Zhang', 'Yuan Liu', 'Xiaoxiao Long', 'Wenping Wang', 'Li Yuan'], 'affiliations': ['Hong Kong University of Science and Technology', 'Peking University', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23162.jpg', 'data': {'categories': ['#optimization', '#inference', '#dataset', '#3d'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NeuralGS - Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² (3DGS) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ñ‹ (MLP) Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ MLP Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 45-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'NeuralGS: Compact 3D Gaussian Compression with Neural Fields', 'desc': 'This paper introduces NeuralGS, a novel method for compressing 3D Gaussian Splatting (3DGS) representations into a more compact form without relying on complex voxel structures. By leveraging neural fields and Multi-Layer Perceptron (MLP) networks, NeuralGS can effectively encode the attributes of 3D Gaussians while significantly reducing storage requirements. The approach involves clustering Gaussians and fitting them with small MLPs based on their importance scores, leading to a remarkable 45-times reduction in model size. Experimental results show that NeuralGS maintains high visual quality and achieves compression performance comparable to existing methods that use Scaffold-GS.'}, 'zh': {'title': 'ç”¨ç¥ç»åœºå‹ç¼©3Dé«˜æ–¯ç‚¹äº‘çš„åˆ›æ–°æ–¹æ³•', 'desc': '3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰åœ¨è´¨é‡å’Œæ¸²æŸ“é€Ÿåº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶å­˜å‚¨å’Œä¼ è¾“æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNeuralGSçš„ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»åœºè¡¨ç¤ºæ¥å‹ç¼©åŸå§‹3DGSï¼Œé¿å…äº†å¤æ‚çš„ä½“ç´ ç»“æ„å’Œé‡åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬åˆ©ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ç¥ç»ç½‘ç»œå¯¹3Dé«˜æ–¯çš„å±æ€§è¿›è¡Œç¼–ç ï¼Œä»…éœ€å°‘é‡å­˜å‚¨ç©ºé—´ï¼Œå³ä½¿åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­ä¹Ÿèƒ½ä¿æŒè‰¯å¥½çš„è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨¡å‹å¤§å°ä¸Šå¹³å‡å‡å°‘äº†45å€ï¼Œå‹ç¼©æ€§èƒ½ä¸ç°æœ‰çš„ä½“ç´ åŸºç¡€å‹ç¼©æ–¹æ³•ç›¸å½“ï¼Œå±•ç¤ºäº†ç›´æ¥ä½¿ç”¨ç¥ç»åœºå‹ç¼©3DGSçš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02821', 'title': 'Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2504.02821', 'abstract': 'Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.', 'score': 8, 'issue_id': 3069, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'd4568f020b5f2eca', 'authors': ['Mateusz Pach', 'Shyamgopal Karthik', 'Quentin Bouniot', 'Serge Belongie', 'Zeynep Akata'], 'affiliations': ['Helmholtz Munich', 'Munich Center of Machine Learning', 'Munich Data Science Institute', 'Technical University of Munich', 'University of Copenhagen', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2504.02821.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#architecture', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SAE) Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº (VLM). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SAE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² VLM Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ·Ñ€ĞµĞ½Ğ¸Ñ CLIP Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SAE. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ SAE Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ VLM.'}, 'en': {'title': 'Enhancing Vision-Language Models with Sparse Autoencoders', 'desc': 'This paper explores the use of Sparse Autoencoders (SAEs) to improve the interpretability and steerability of Vision-Language Models (VLMs) like CLIP. The authors present a framework to assess how well these models represent single concepts, known as monosemanticity. Their experiments show that SAEs can enhance the clarity of individual neurons in VLMs and align these representations with established expert categories. Importantly, they demonstrate that SAEs can influence the output of multimodal language models without changing the original model architecture.'}, 'zh': {'title': 'ç¨€ç–è‡ªç¼–ç å™¨æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸æ“æ§æ€§', 'desc': 'ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰æœ€è¿‘è¢«è¯æ˜å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ã€‚æœ¬æ–‡å°†SAEsçš„åº”ç”¨æ‰©å±•åˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥è¯„ä¼°è§†è§‰è¡¨ç¤ºçš„å•ä¹‰æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨VLMsä¸Šè®­ç»ƒçš„SAEsæ˜¾è‘—å¢å¼ºäº†å•ä¸ªç¥ç»å…ƒçš„å•ä¹‰æ€§ï¼Œå¹¶å±•ç¤ºäº†ä¸ä¸“å®¶å®šä¹‰ç»“æ„ï¼ˆå¦‚iNaturaliståˆ†ç±»æ³•ï¼‰è‰¯å¥½å¯¹é½çš„å±‚æ¬¡è¡¨ç¤ºã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†å°†SAEsåº”ç”¨äºCLIPè§†è§‰ç¼–ç å™¨ï¼Œå¯ä»¥ç›´æ¥æ“æ§å¤šæ¨¡æ€LLMsï¼ˆå¦‚LLaVAï¼‰çš„è¾“å‡ºï¼Œè€Œæ— éœ€å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00891', 'title': 'GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning', 'url': 'https://huggingface.co/papers/2504.00891', 'abstract': 'Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.', 'score': 8, 'issue_id': 3066, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'b22a54f43f9d7a89', 'authors': ['Jian Zhao', 'Runze Liu', 'Kaiyan Zhang', 'Zhimu Zhou', 'Junqi Gao', 'Dong Li', 'Jiafei Lyu', 'Zhouyi Qian', 'Biqing Qi', 'Xiu Li', 'Bowen Zhou'], 'affiliations': ['BUPT', 'Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00891.jpg', 'data': {'categories': ['#training', '#optimization', '#math', '#reasoning', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'GenPRM: ĞĞ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenPRM - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GenPRM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PRM Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 23 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'GenPRM: Elevating LLMs with Generative Process Reward Models', 'desc': 'This paper presents GenPRM, a generative process reward model designed to improve the performance of large language models (LLMs) by addressing key challenges faced by existing process reward models (PRMs). GenPRM utilizes Chain-of-Thought (CoT) reasoning and incorporates code verification to enhance the quality of its judgments at each reasoning step. The authors introduce a novel method called Relative Progress Estimation (RPE) to generate high-quality supervision labels and rationale data, leading to significant performance improvements on various reasoning tasks. Experimental results demonstrate that GenPRM outperforms previous PRMs and shows strong capabilities as a critic model for refining policy models, establishing a new approach for process supervision in LLMs.'}, 'zh': {'title': 'ç”Ÿæˆæ€§è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼šæå‡LLMsçš„æ–°èŒƒå¼', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºéªŒè¯å™¨å¯ä»¥æå‡LLMsçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„PRMsé¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæœ‰é™çš„è¿‡ç¨‹ç›‘ç£å’Œæ³›åŒ–èƒ½åŠ›ã€ä¾èµ–äºæ ‡é‡å€¼é¢„æµ‹è€Œæœªåˆ©ç”¨LLMsçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥åŠæ— æ³•æ‰©å±•PRMsçš„æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†GenPRMï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆæ€§è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡ä»£ç éªŒè¯è¿›è¡Œæ˜ç¡®çš„æ€ç»´é“¾æ¨ç†ï¼Œç„¶åå¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œåˆ¤æ–­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenPRMåœ¨å¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„PRMsï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºæ”¿ç­–æ¨¡å‹ç²¾ç‚¼çš„æ‰¹è¯„æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23542', 'title': 'Whisper-LM: Improving ASR Models with Language Models for Low-Resource\n  Languages', 'url': 'https://huggingface.co/papers/2503.23542', 'abstract': 'Automatic speech recognition systems have undoubtedly advanced with the integration of multilingual and multitask models such as Whisper, which have shown a promising ability to understand and process speech across a wide range of languages. Despite their robustness, these models often fall short in handling the linguistic distinctions of minority languages. This study addresses this gap by integrating traditional and novel language models with fine-tuned Whisper models to raise their performance in less commonly studied languages. Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. We obtained improvements up to 51\\% for in-distribution datasets and up to 34\\% for out-of-distribution sentences using statistical language models, while large language models provided moderate but consistently robust improvement across diverse linguistic contexts. The findings reveal that, while the integration reliably benefits all model sizes, the extent of improvement varies, highlighting the importance of optimized language model parameters. Finally, we emphasize the importance of selecting appropriate evaluation parameters when reporting the results using transformer-based ASR models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge. For further implementation details of this study, the technical documentation and source code are available at http://www.github.com/hitz-zentroa/whisper-lm.', 'score': 8, 'issue_id': 3072, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'c8ef94a1b05fc918', 'authors': ['Xabier de Zuazo', 'Eva Navas', 'Ibon Saratxaga', 'Inma HernÃ¡ez Rioja'], 'affiliations': ['HiTZ - University of the Basque Country - UPV/EHU, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2503.23542.jpg', 'data': {'categories': ['#training', '#inference', '#multilingual', '#dataset', '#low_resource', '#open_source'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Whisper. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Whisper Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸.'}, 'en': {'title': 'Enhancing ASR for Minority Languages with Whisper and Language Models', 'desc': 'This paper discusses advancements in automatic speech recognition (ASR) systems, particularly focusing on multilingual and multitask models like Whisper. It highlights the challenges these models face with minority languages and proposes a solution by combining traditional and novel language models with fine-tuned Whisper models. The study shows significant improvements in word error rates for low-resource languages through rigorous fine-tuning and evaluation. The results indicate that optimized language model parameters can enhance ASR performance across various linguistic contexts, paving the way for more inclusive ASR technologies.'}, 'zh': {'title': 'æå‡å°‘æ•°è¯­è¨€è¯­éŸ³è¯†åˆ«çš„åŒ…å®¹æ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨å°‘æ•°è¯­è¨€ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯é€šè¿‡ç»“åˆä¼ ç»Ÿå’Œæ–°å‹è¯­è¨€æ¨¡å‹ä¸å¾®è°ƒçš„Whisperæ¨¡å‹ã€‚å°½ç®¡Whisperåœ¨å¤šè¯­è¨€å¤„ç†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å°‘æ•°è¯­è¨€æ—¶ä»å­˜åœ¨ä¸è¶³ã€‚æˆ‘ä»¬é€šè¿‡ä¸¥æ ¼çš„å¾®è°ƒå’Œå¤šæ•°æ®é›†è¯„ä¼°ï¼Œæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºåŒ®ä¹çš„æƒ…å†µä¸‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–è¯­è¨€æ¨¡å‹å‚æ•°å¯¹æå‡æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œä»è€Œæ¨åŠ¨äº†æ›´å…·åŒ…å®¹æ€§çš„è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02012', 'title': 'Instruction-Guided Autoregressive Neural Network Parameter Generation', 'url': 'https://huggingface.co/papers/2504.02012', 'abstract': "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.", 'score': 6, 'issue_id': 3065, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'cbdd586ccd2b682d', 'authors': ['Soro Bedionita', 'Bruno Andreis', 'Song Chong', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai, South Korea', 'KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.02012.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#optimization', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'IGPG: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€', 'desc': 'IGPG - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° VQ-VAE Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ÑĞµÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸. IGPG Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸Ğ· ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IGPG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€.'}, 'en': {'title': 'IGPG: Unifying Neural Network Parameter Generation for Enhanced Adaptability', 'desc': 'This paper introduces IGPG (Instruction Guided Parameter Generation), a new framework for generating neural network parameters based on task descriptions and architecture specifications. Unlike previous methods, IGPG uses an autoregressive approach that ensures coherence between layers and adapts efficiently to different models and datasets. By employing a VQ-VAE and generating weights at the token level, IGPG captures complex parameter distributions from various pretrained models. The results show that IGPG outperforms existing methods in scalability and efficiency, making it a valuable tool for model adaptation and fine-tuning.'}, 'zh': {'title': 'IGPGï¼šçµæ´»çš„ç¥ç»ç½‘ç»œå‚æ•°ç”Ÿæˆå·¥å…·', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºIGPGï¼ˆæŒ‡ä»¤å¼•å¯¼å‚æ•°ç”Ÿæˆï¼‰çš„è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆç¥ç»ç½‘ç»œå‚æ•°ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡æè¿°å’Œæ¶æ„è§„èŒƒã€‚IGPGé€šè¿‡ç»“åˆVQ-VAEå’Œè‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡å’Œæ¶æ„ä¸­ç»Ÿä¸€å‚æ•°åˆæˆï¼Œç¡®ä¿å±‚é—´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆç¥ç»ç½‘ç»œæƒé‡æ—¶ï¼Œé‡‡ç”¨äº†åŸºäºtokençš„ç”Ÿæˆæ–¹å¼ï¼Œæœ‰æ•ˆæ•æ‰æ¥è‡ªå¤šç§é¢„è®­ç»ƒæ¨¡å‹çš„å¤æ‚å‚æ•°åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIGPGåœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡æ¶æ„çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡æ–¹é¢ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01955', 'title': 'Scene-Centric Unsupervised Panoptic Segmentation', 'url': 'https://huggingface.co/papers/2504.01955', 'abstract': 'Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ.', 'score': 4, 'issue_id': 3079, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '24a963913e1c2220', 'authors': ['Oliver Hahn', 'Christoph Reich', 'Nikita Araslanov', 'Daniel Cremers', 'Christian Rupprecht', 'Stefan Roth'], 'affiliations': ['ELIZA', 'MCML', 'TU Darmstadt', 'TU Munich', 'University of Oxford', 'hessian.AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.01955.jpg', 'data': {'categories': ['#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚ĞºĞ°Ñ… Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Unsupervised Panoptic Segmentation with Scene-Centric Learning', 'desc': 'This paper introduces a new method for unsupervised panoptic segmentation, which is the task of dividing an image into meaningful areas and separate object instances without using labeled data. The authors propose a novel approach that focuses on scene-centric images, eliminating the reliance on object-centric training data. They develop a technique to generate high-resolution pseudo labels by integrating visual features, depth information, and motion cues. Their method demonstrates a significant improvement in panoptic segmentation quality, achieving a 9.4% increase in performance on the Cityscapes dataset compared to previous state-of-the-art methods.'}, 'zh': {'title': 'æ— ç›‘ç£å…¨æ™¯åˆ†å‰²çš„æ–°çªç ´', 'desc': 'æ— ç›‘ç£å…¨æ™¯åˆ†å‰²æ—¨åœ¨å°†å›¾åƒåˆ’åˆ†ä¸ºå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„åŒºåŸŸå’Œç‹¬ç‰¹çš„ç‰©ä½“å®ä¾‹ï¼Œè€Œæ— éœ€ä¾èµ–äººå·¥æ ‡æ³¨çš„æ•°æ®ã€‚ä¸ä¹‹å‰çš„æ— ç›‘ç£å…¨æ™¯åœºæ™¯ç†è§£å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†å¯¹ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ•°æ®çš„éœ€æ±‚ï¼Œä»è€Œå®ç°å¯¹å¤æ‚åœºæ™¯çš„æ— ç›‘ç£ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— ç›‘ç£å…¨æ™¯æ–¹æ³•ï¼Œç›´æ¥åœ¨åœºæ™¯ä¸­å¿ƒå›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œç»“åˆè§†è§‰è¡¨ç¤ºã€æ·±åº¦å’Œè¿åŠ¨çº¿ç´¢ï¼Œè·å¾—é«˜åˆ†è¾¨ç‡çš„å…¨æ™¯ä¼ªæ ‡ç­¾ã€‚é€šè¿‡ä¼ªæ ‡ç­¾è®­ç»ƒå’Œå…¨æ™¯è‡ªæˆ‘è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— éœ€ä»»ä½•äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®é¢„æµ‹å¤æ‚åœºæ™¯çš„å…¨æ™¯åˆ†å‰²ï¼Œæ˜¾è‘—æé«˜äº†å…¨æ™¯è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01943', 'title': 'OpenCodeReasoning: Advancing Data Distillation for Competitive Coding', 'url': 'https://huggingface.co/papers/2504.01943', 'abstract': 'Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community.', 'score': 4, 'issue_id': 3083, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '6581bbd20e92ba34', 'authors': ['Wasi Uddin Ahmad', 'Sean Narenthiran', 'Somshubra Majumdar', 'Aleksander Ficek', 'Siddhartha Jain', 'Jocelyn Huang', 'Vahid Noroozi', 'Boris Ginsburg'], 'affiliations': ['NVIDIA Santa Clara, CA 15213, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.01943.jpg', 'data': {'categories': ['#optimization', '#open_source', '#benchmark', '#reasoning', '#dataset', '#data', '#training'], 'emoji': 'ğŸ’»', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT), Ğ¾Ğ½Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LiveCodeBench Ğ¸ CodeContests Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ±Ñ‹Ğ»Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¾ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ°ĞºÑ†ĞµĞ½Ñ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Unlocking Coding Potential with Enhanced Supervised Fine-Tuning', 'desc': 'This paper discusses the development of a new supervised fine-tuning (SFT) dataset aimed at enhancing the coding capabilities of reasoning-based large language models (LLMs). The authors demonstrate that their distilled models, trained solely on this SFT dataset, achieve superior performance on coding benchmarks compared to those trained with reinforcement learning. They analyze the effects of data sources, code execution filtering, and the diversity of instructions and solutions on model performance. The findings suggest that prioritizing instruction diversity can lead to better outcomes, and the authors plan to share their datasets and models with the research community.'}, 'zh': {'title': 'æå‡ç¼–ç èƒ½åŠ›çš„æ¨ç†æ¨¡å‹æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åŸºäºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¼–ç ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¼˜è´¨çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œä»¥æå‡ä¸åŒè§„æ¨¡æ¨¡å‹çš„ç¼–ç èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨LiveCodeBenchå’ŒCodeContestsä¸Šåˆ†åˆ«è¾¾åˆ°äº†61.8%å’Œ24.6%çš„æˆç»©ï¼Œè¶…è¶Šäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜åˆ†æäº†æ•°æ®æ¥æºã€ä»£ç æ‰§è¡Œè¿‡æ»¤çš„å½±å“ä»¥åŠæŒ‡ä»¤å’Œè§£å†³æ–¹æ¡ˆå¤šæ ·æ€§çš„é‡è¦æ€§ï¼Œå¹¶è®¡åˆ’å°†è¿™äº›æ•°æ®é›†å’Œæ¨¡å‹å¼€æºã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (14)', '#agents (16)', '#agi (4)', '#alignment (4)', '#architecture (22)', '#audio (3)', '#benchmark (43)', '#cv (24)', '#data (10)', '#dataset (33)', '#diffusion (20)', '#ethics (3)', '#games (8)', '#graphs (2)', '#hallucinations (5)', '#healthcare (3)', '#inference (16)', '#interpretability (7)', '#leakage', '#long_context (4)', '#low_resource (3)', '#machine_translation', '#math (6)', '#multilingual (4)', '#multimodal (35)', '#open_source (27)', '#optimization (51)', '#plp (1)', '#rag (4)', '#reasoning (39)', '#rl (14)', '#rlhf (9)', '#robotics (4)', '#science (3)', '#security (3)', '#small_models (1)', '#story_generation (1)', '#survey (8)', '#synthetic (7)', '#training (59)', '#transfer_learning (9)', '#video (19)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-04-07 13:23',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-07 13:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-07 13:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    