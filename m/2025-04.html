
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. April 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">ĞĞ¿Ñ€ĞµĞ»ÑŒ 2025</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-03.html">â¬…ï¸ <span id="prev-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-05.html">â¡ï¸ <span id="next-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'ĞĞ¿Ñ€ĞµĞ»ÑŒ 2025', 'en': 'April 2025', 'zh': '4æœˆ2025å¹´'};
        let feedDateNext = {'ru': '05.2025', 'en': '05/2025', 'zh': '5æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '03.2025', 'en': '03/2025', 'zh': '3æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.19693', 'title': 'AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation', 'url': 'https://huggingface.co/papers/2503.19693', 'abstract': 'Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance', 'score': 52, 'issue_id': 2976, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '2c907e98a0aafb46', 'authors': ['Itay Nakash', 'Nitay Calderon', 'Eyal Ben David', 'Elad Hoffer', 'Roi Reichart'], 'affiliations': ['Habana Labs', 'The Faculty of Data and Decisions Science, Technion - IIT'], 'pdf_title_img': 'assets/pdf/title_img/2503.19693.jpg', 'data': {'categories': ['#architecture', '#low_resource', '#training', '#optimization', '#data', '#transfer_learning'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…', 'desc': 'AdaptiVocab - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğµ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° n-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ñ‡Ñ‚Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ…. AdaptiVocab Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ n-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ñ„Ğ°Ğ·Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AdaptiVocab ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 25% Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing LLM Efficiency with Domain-Specific Vocabulary', 'desc': 'This paper presents AdaptiVocab, a method for improving the efficiency of Large Language Models (LLMs) in specific domains by adapting their vocabulary. Instead of using a general-purpose vocabulary, AdaptiVocab replaces tokens with domain-specific n-gram-based tokens, which reduces the number of tokens needed for processing and generation. The approach involves initializing new embeddings through a combination of existing ones and includes a lightweight fine-tuning process that can be done on a single GPU. The results demonstrate that AdaptiVocab can decrease token usage by over 25% while maintaining the quality of generated outputs and overall task performance.'}, 'zh': {'title': 'æé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨æ¨¡å‹ä¸­å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å¤šåŠŸèƒ½æ€§ï¼Œä½†å…¶å¹¿æ³›åº”ç”¨ä¼´éšç€é«˜æ˜‚çš„è®¡ç®—å¼€é”€ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªå›å½’è§£ç ä¸­ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦è¿›è¡Œå‰å‘ä¼ æ’­ã€‚é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ï¼Œé€šç”¨èƒ½åŠ›å¹¶éå¿…è¦ï¼Œå¯ä»¥é€šè¿‡æé«˜æ•ˆç‡æ¥è¿›è¡Œäº¤æ¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é¢†åŸŸé€‚åº”æ–¹æ³•â€”â€”AdaptiVocabï¼Œé€šè¿‡è°ƒæ•´è¯æ±‡è¡¨æ¥é™ä½å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬ï¼Œæ—¨åœ¨æé«˜LLMåœ¨ä½èµ„æºé¢†åŸŸçš„æ•ˆç‡ã€‚AdaptiVocabå¯ä»¥åº”ç”¨äºä»»ä½•åˆ†è¯å™¨å’Œæ¶æ„ï¼Œé€šè¿‡ç”¨ç‰¹å®šé¢†åŸŸçš„n-gramä»£æ›¿åŸæœ‰çš„tokensï¼Œå‡å°‘è¾“å…¥å¤„ç†å’Œè¾“å‡ºç”Ÿæˆæ‰€éœ€çš„tokensæ•°é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22230', 'title': 'Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback', 'url': 'https://huggingface.co/papers/2503.22230', 'abstract': "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.", 'score': 27, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': 'a994668c51ac51c4', 'authors': ['Wei Shen', 'Guanlin Liu', 'Zheng Wu', 'Ruofei Zhu', 'Qingping Yang', 'Chao Xin', 'Yu Yue', 'Lin Yan'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2503.22230.jpg', 'data': {'categories': ['#rlhf', '#data', '#reasoning', '#alignment', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RLHF: Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (RTV) Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (GenRM), Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Pre-PPO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ RLHF, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing RLHF: Bridging Data Gaps for Better AI Alignment', 'desc': 'This paper focuses on improving Reinforcement Learning from Human Feedback (RLHF) for large language models by addressing issues in prompt-data construction. It identifies problems like reward hacking and reduced response diversity that hinder RLHF performance. The authors propose a hybrid reward system that combines reasoning task verifiers (RTV) with a generative reward model (GenRM) to counteract these issues. Additionally, they introduce a new prompt-selection method called Pre-PPO and emphasize the importance of prioritizing mathematical and coding tasks during training to enhance overall model performance.'}, 'zh': {'title': 'ä¼˜åŒ–äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸­çš„äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹äºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½è‡³å…³é‡è¦ã€‚å°½ç®¡è¿‘æœŸç ”ç©¶é›†ä¸­åœ¨ç®—æ³•æ”¹è¿›ä¸Šï¼Œä½†æç¤ºæ•°æ®æ„å»ºçš„é‡è¦æ€§å´è¢«å¿½è§†ã€‚æœ¬æ–‡æ¢è®¨äº†RLHFæ€§èƒ½æ‰©å±•ä¸­çš„æ•°æ®é©±åŠ¨ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯å¥–åŠ±é»‘å®¢å’Œå“åº”å¤šæ ·æ€§ä¸‹é™çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆå¥–åŠ±ç³»ç»Ÿï¼Œç»“åˆæ¨ç†ä»»åŠ¡éªŒè¯å™¨ï¼ˆRTVï¼‰å’Œç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰ï¼Œä»¥å‡è½»å¥–åŠ±é»‘å®¢ç°è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æç¤ºé€‰æ‹©æ–¹æ³•Pre-PPOï¼Œä»¥ä¿æŒå“åº”å¤šæ ·æ€§å¹¶å¢å¼ºå­¦ä¹ æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22675', 'title': 'Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation', 'url': 'https://huggingface.co/papers/2503.22675', 'abstract': "Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose ReaRec, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\\%-50\\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.", 'score': 25, 'issue_id': 2971, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '092ab2fa75277891', 'authors': ['Jiakai Tang', 'Sunhao Dai', 'Teng Shi', 'Jun Xu', 'Xu Chen', 'Wen Chen', 'Wu Jian', 'Yuning Jiang'], 'affiliations': ['Alibaba Group, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.22675.jpg', 'data': {'categories': ['#inference', '#reasoning', '#dataset', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'ReaRec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ensemble Reasoning Learning (ERL) Ğ¸ Progressive Reasoning Learning (PRL). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReaRec Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'ReaRec: Elevating Sequential Recommendations with Multi-Step Reasoning', 'desc': 'This paper introduces ReaRec, a novel framework for Sequential Recommendation (SeqRec) that enhances user representation through multi-step reasoning. Traditional methods often rely on a single forward computation, which limits their ability to capture the evolving nature of user preferences and understand less popular items. ReaRec addresses these limitations by using autoregressive techniques and special embeddings to improve the inference process. The proposed framework, along with two learning methods, shows significant performance improvements across various datasets, suggesting a new direction for research in recommendation systems.'}, 'zh': {'title': 'ReaRecï¼šæå‡é¡ºåºæ¨èçš„æ¨ç†èƒ½åŠ›', 'desc': 'é¡ºåºæ¨èï¼ˆSeqRecï¼‰æ—¨åœ¨é€šè¿‡æ•æ‰ç”¨æˆ·å†å²äº¤äº’ä¸­çš„é¡ºåºæ¨¡å¼æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªé¡¹ç›®ï¼Œè¿™åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„æ¨èç³»ç»Ÿä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡ç”¨ç›´æ¥çš„å‰å‘è®¡ç®—èŒƒå¼ï¼Œæœ€ç»ˆçš„éšè—çŠ¶æ€ä½œä¸ºç”¨æˆ·è¡¨ç¤ºï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å»ºæ¨¡ç”¨æˆ·åå¥½çš„å¤æ‚æ¼”å˜æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReaRecï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæ¨èç³»ç»Ÿçš„æ¨ç†æ—¶è®¡ç®—æ¡†æ¶ï¼Œé€šè¿‡éšå¼å¤šæ­¥æ¨ç†å¢å¼ºç”¨æˆ·è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReaRecæ˜¾è‘—æé«˜äº†å¤šä¸ªé¡ºåºæ¨èæ¨¡å‹çš„æ€§èƒ½ï¼Œå¼€è¾Ÿäº†æ¨ç†æ—¶è®¡ç®—çš„æ–°ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21614', 'title': 'A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond', 'url': 'https://huggingface.co/papers/2503.21614', 'abstract': 'Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.', 'score': 21, 'issue_id': 2976, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '805b7cd4ec307c34', 'authors': ['Xiaoye Qu', 'Yafu Li', 'Zhaochen Su', 'Weigao Sun', 'Jianhao Yan', 'Dongrui Liu', 'Ganqu Cui', 'Daizong Liu', 'Shuxian Liang', 'Junxian He', 'Peng Li', 'Wei Wei', 'Jing Shao', 'Chaochao Lu', 'Yue Zhang', 'Xian-Sheng Hua', 'Bowen Zhou', 'Yu Cheng'], 'affiliations': ['Huazhong University of Science and Technology', 'Peking University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'Tongji University', 'Tsinghua University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21614.jpg', 'data': {'categories': ['#reasoning', '#inference', '#training', '#survey', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ğ¹ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¸ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° LRM - Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses the challenges faced by Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI o1, particularly their tendency to generate long and redundant reasoning processes. These inefficiencies can complicate training and real-world applications, especially where efficient use of tokens is crucial. The authors review various strategies aimed at enhancing reasoning efficiency throughout the LRM lifecycle, from pretraining to inference. They also provide a GitHub repository to track advancements in this field, aiming to inspire further research and innovation.'}, 'zh': {'title': 'æå‡æ¨ç†æ•ˆç‡ï¼ŒåŠ©åŠ›å¤§å‹æ¨¡å‹å‘å±•', 'desc': 'æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI o1ï¼Œé€šè¿‡æ‰©å±•æ¨ç†é“¾ï¼ˆCoTï¼‰çš„é•¿åº¦åœ¨æ¨ç†è¿‡ç¨‹ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€ç”Ÿæˆè¿‡é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå†…å®¹å†—ä½™ï¼ˆä¾‹å¦‚ï¼Œé‡å¤å®šä¹‰ï¼‰ã€å¯¹ç®€å•é—®é¢˜çš„è¿‡åº¦åˆ†æï¼Œä»¥åŠå¯¹å¤æ‚ä»»åŠ¡çš„å¤šæ¡æ¨ç†è·¯å¾„çš„è¡¨é¢æ¢ç´¢ã€‚è¿™ç§ä½æ•ˆæ€§åœ¨è®­ç»ƒã€æ¨ç†å’Œå®é™…åº”ç”¨ä¸­å¼•å‘äº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä»£ç†ç³»ç»Ÿä¸­ï¼Œä»¤ç‰Œç»æµè‡³å…³é‡è¦ã€‚æœ¬æ–‡ç»¼è¿°äº†æ”¹å–„LRMsæ¨ç†æ•ˆç‡çš„æœ€æ–°åŠªåŠ›ï¼Œè¯†åˆ«äº†å¸¸è§çš„ä½æ•ˆæ¨¡å¼ï¼Œå¹¶æ¢è®¨äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16081', 'title': 'OThink-MR1: Stimulating multimodal generalized reasoning capabilities\n  via dynamic reinforcement learning', 'url': 'https://huggingface.co/papers/2503.16081', 'abstract': 'Multimodal Large Language Models (MLLMs) have gained significant traction for their ability to process diverse input data types and generate coherent, contextually relevant outputs across various applications. While supervised fine-tuning (SFT) has been the predominant approach to enhance MLLM capabilities in task-specific optimization, it often falls short in fostering crucial generalized reasoning abilities. Although reinforcement learning (RL) holds great promise in overcoming these limitations, it encounters two significant challenges: (1) its generalized capacities in multimodal tasks remain largely unexplored, and (2) its training constraints, including the constant Kullback-Leibler divergence or the clamp strategy, often result in suboptimal bottlenecks. To address these challenges, we propose OThink-MR1, an advanced MLLM equipped with profound comprehension and reasoning capabilities across multimodal tasks. Specifically, we introduce Group Relative Policy Optimization with a dynamic Kullback-Leibler strategy (GRPO-D), which markedly enhances reinforcement learning (RL) performance. For Qwen2-VL-2B-Instruct, GRPO-D achieves a relative improvement of more than 5.72% over SFT and more than 13.59% over GRPO in same-task evaluation on two adapted datasets. Furthermore, GRPO-D demonstrates remarkable cross-task generalization capabilities, with an average relative improvement of more than 61.63% over SFT in cross-task evaluation. These results highlight that the MLLM trained with GRPO-D on one multimodal task can be effectively transferred to another task, underscoring the superior generalized reasoning capabilities of our proposed OThink-MR1 model.', 'score': 18, 'issue_id': 2981, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '5d09e443b3a0dafb', 'authors': ['Zhiyuan Liu', 'Yuting Zhang', 'Feng Liu', 'Changwang Zhang', 'Ying Sun', 'Jun Wang'], 'affiliations': ['OPPO Research Institute Shenzhen, China', 'The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16081.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#transfer_learning', '#multimodal', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜: OThink-MR1 Ñ GRPO-D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OThink-MR1 - ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ - Group Relative Policy Optimization Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ ĞšÑƒĞ»ÑŒĞ±Ğ°ĞºĞ°-Ğ›ĞµĞ¹Ğ±Ğ»ĞµÑ€Ğ° (GRPO-D). GRPO-D Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RL. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Unlocking Generalized Reasoning in Multimodal Models with GRPO-D', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) that can handle different types of data and produce relevant outputs. It highlights the limitations of traditional supervised fine-tuning (SFT) in developing generalized reasoning skills. The authors introduce a new model called OThink-MR1, which utilizes a novel reinforcement learning approach named Group Relative Policy Optimization with a dynamic Kullback-Leibler strategy (GRPO-D). This method significantly improves performance in both same-task and cross-task evaluations, demonstrating enhanced reasoning capabilities across various multimodal tasks.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†ä¸åŒç±»å‹è¾“å…¥æ•°æ®å’Œç”Ÿæˆè¿è´¯ã€ç›¸å…³çš„è¾“å‡ºæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è™½ç„¶ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜¯æå‡MLLMä»»åŠ¡ç‰¹å®šä¼˜åŒ–èƒ½åŠ›çš„ä¸»è¦æ–¹æ³•ï¼Œä½†å®ƒåœ¨ä¿ƒè¿›é€šç”¨æ¨ç†èƒ½åŠ›æ–¹é¢å¸¸å¸¸ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OTink-MR1æ¨¡å‹ï¼Œé‡‡ç”¨äº†åŠ¨æ€Kullback-Leiblerç­–ç•¥çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPO-Dï¼‰ï¼Œæ˜¾è‘—æå‡äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨GRPO-Dè®­ç»ƒçš„MLLMåœ¨è·¨ä»»åŠ¡è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22194', 'title': 'ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2503.22194', 'abstract': 'We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.', 'score': 17, 'issue_id': 2971, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '6d93cc886c16f9b0', 'authors': ['Yunhong Min', 'Daehyeon Choi', 'Kyeongmin Yeo', 'Jihyun Lee', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.22194.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#3d'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ORIGEN - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ 3D Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿ÑƒÑĞºĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ›Ğ°Ğ½Ğ¶ĞµĞ²ĞµĞ½Ğ° Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ORIGEN Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing 3D Orientation in Text-to-Image Generation with ORIGEN', 'desc': 'ORIGEN is a novel method that enables zero-shot 3D orientation grounding in text-to-image generation, allowing for better control over how objects are oriented in three-dimensional space. Unlike previous methods that focused on 2D positioning, ORIGEN utilizes a reward-guided sampling approach that leverages a pretrained model for estimating 3D orientations. This method incorporates Langevin dynamics to enhance image realism while maintaining effective sampling, requiring minimal code changes. Experimental results demonstrate that ORIGEN surpasses existing training-based and test-time guidance techniques in both quantitative metrics and user evaluations.'}, 'zh': {'title': 'ORIGENï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„3Dæ–¹å‘å®šä½æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ORIGENï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å®ç°3Dæ–¹å‘å®šä½çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨2Då®šä½ä¸Šï¼Œç¼ºä¹å¯¹3Dæ–¹å‘çš„æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¥–åŠ±å¼•å¯¼çš„é‡‡æ ·æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„åˆ¤åˆ«æ¨¡å‹è¿›è¡Œ3Dæ–¹å‘ä¼°è®¡ï¼Œå¹¶ç»“åˆä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæµæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORIGENåœ¨å®šé‡æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶ä¸­å‡ä¼˜äºåŸºäºè®­ç»ƒå’Œæµ‹è¯•æ—¶å¼•å¯¼çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21332', 'title': 'ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback', 'url': 'https://huggingface.co/papers/2503.21332', 'abstract': 'Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released.', 'score': 17, 'issue_id': 2977, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '4797a440ca75521b', 'authors': ['Taewon Yun', 'Jihwan Oh', 'Hyangsuk Min', 'Yuho Lee', 'Jihwan Bang', 'Jason Cai', 'Hwanjun Song'], 'affiliations': ['Amazon Web Services, AI Labs', 'Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2503.21332.jpg', 'data': {'categories': ['#dataset', '#long_context', '#training', '#data', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ReFeed - Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SumFeed-CoT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ReFeed Ğº ÑˆÑƒĞ¼Ñƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Summarization with Reflective Reasoning', 'desc': 'This paper presents ReFeed, a novel summarization refinement pipeline that improves the quality of summaries across multiple dimensions by utilizing reflective reasoning on feedback. The authors introduce a new dataset called SumFeed-CoT, which is designed to train a lightweight model capable of handling reflective reasoning effectively. Through experiments, they demonstrate that the performance of the summarization refinement is significantly influenced by the number of dimensions, the exposure to feedback, and the reasoning policy employed. The findings suggest that addressing multiple feedback sources simultaneously is essential for optimizing performance while also being resilient to noisy feedback and variations in feedback order.'}, 'zh': {'title': 'åæ€æ€§æ¨ç†æå‡å¤šç»´æ€»ç»“ç²¾ç‚¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºReFeedçš„æ€»ç»“ç²¾ç‚¼ç®¡é“ï¼Œæ—¨åœ¨é€šè¿‡åæ€æ€§æ¨ç†æ¥å¢å¼ºå¤šç»´åº¦çš„æ€»ç»“æ•ˆæœã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªåä¸ºSumFeed-CoTçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè®­ç»ƒè½»é‡çº§æ¨¡å‹ï¼Œä»¥å®ç°åæ€æ€§æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»´åº¦æ•°é‡ã€åé¦ˆæš´éœ²å’Œæ¨ç†ç­–ç•¥å¯¹ç²¾ç‚¼æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œå¼ºè°ƒäº†åæ€æ€§æ¨ç†å’ŒåŒæ—¶å¤„ç†å¤šä¸ªåé¦ˆçš„é‡è¦æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œåˆ›å»ºå…·æœ‰æ˜ç¡®ç›®æ ‡å’ŒæŒ‡å¯¼çš„æ•°æ®æ˜¯æœ‰æ•ˆæ¨ç†çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20785', 'title': 'Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency', 'url': 'https://huggingface.co/papers/2503.20785', 'abstract': 'We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multiview videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation.', 'score': 15, 'issue_id': 2974, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '1a3973da9e51afd7', 'authors': ['Tianqi Liu', 'Zihao Huang', 'Zhaoxi Chen', 'Guangcong Wang', 'Shoukang Hu', 'Liao Shen', 'Huiqiang Sun', 'Zhiguo Cao', 'Wei Li', 'Ziwei Liu'], 'affiliations': ['Great Bay University', 'Huazhong University of Science and Technology', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20785.jpg', 'data': {'categories': ['#3d', '#multimodal', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ¾Ğ¹ 4D-ÑÑ†ĞµĞ½Ğµ', 'desc': 'Free4D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. Free4D ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ ÑÑ†ĞµĞ½Ñƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 4D Scene Generation from a Single Image', 'desc': 'Free4D is a new framework that generates 4D scenes from just one image without needing extensive tuning. Unlike previous methods that either focus on individual objects or require large datasets, Free4D uses pre-trained models to create consistent 4D representations efficiently. It employs image-to-video diffusion models to animate the input image and then refines the generated structure for spatial and temporal consistency. This approach allows for real-time rendering of 4D scenes, making it a significant step forward in scene generation technology.'}, 'zh': {'title': 'ä»å•å›¾åƒç”Ÿæˆ4Dåœºæ™¯çš„æ–°çªç ´', 'desc': 'æˆ‘ä»¬æå‡ºäº†Free4Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è°ƒä¼˜çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒç”Ÿæˆ4Dåœºæ™¯ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¸“æ³¨äºç‰©ä½“çº§ç”Ÿæˆï¼Œå¯¼è‡´åœºæ™¯çº§ç”Ÿæˆä¸å¯è¡Œï¼Œæˆ–è€…ä¾èµ–äºå¤§è§„æ¨¡å¤šè§†è§’è§†é¢‘æ•°æ®é›†è¿›è¡Œæ˜‚è´µçš„è®­ç»ƒï¼Œä¸”ç”±äº4Dåœºæ™¯æ•°æ®ç¨€ç¼ºï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯æç‚¼é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œä»¥å®ç°ä¸€è‡´çš„4Dåœºæ™¯è¡¨ç¤ºï¼Œè¿™æä¾›äº†æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›çš„ä¼˜åŠ¿ã€‚é€šè¿‡å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥å›¾åƒè¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œç„¶ååˆå§‹åŒ–4Då‡ ä½•ç»“æ„ï¼Œæœ€ç»ˆå®ç°å®æ—¶ã€å¯æ§çš„4Dåœºæ™¯ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20308', 'title': 'Perceptually Accurate 3D Talking Head Generation: New Definitions,\n  Speech-Mesh Representation, and Evaluation Metrics', 'url': 'https://huggingface.co/papers/2503.20308', 'abstract': 'Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria -- Temporal Synchronization, Lip Readability, and Expressiveness -- are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/.', 'score': 15, 'issue_id': 2979, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '1b5ec45782117fb1', 'authors': ['Lee Chae-Yeon', 'Oh Hyun-Bin', 'Han EunGi', 'Kim Sung-Bin', 'Suekyeong Nam', 'Tae-Hyun Oh'], 'affiliations': ['Dept. of Electrical Engineering, POSTECH', 'Grad. School of AI, POSTECH', 'KRAFTON', 'School of Computing, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.20308.jpg', 'data': {'categories': ['#3d', '#audio', '#training'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ÑƒĞ± Ğ² 3D', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ², ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ñ€ĞµÑ‡ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞ¸ Ğ»Ğ¸Ñ†Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ÑƒĞ± Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼ Ñ€ĞµÑ‡Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ±. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼: Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ³ÑƒĞ±Ğ°Ğ¼ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Lip Synchronization in 3D Talking Heads', 'desc': 'This paper addresses the challenges in generating 3D talking heads that accurately synchronize lip movements with speech. The authors propose that achieving Temporal Synchronization, Lip Readability, and Expressiveness is essential for realistic lip movements. They introduce a novel speech-mesh synchronized representation that effectively captures the relationship between speech signals and 3D facial movements. By integrating this representation as a perceptual loss in existing models, they demonstrate significant improvements in lip synchronization quality through experimental validation.'}, 'zh': {'title': 'æå‡3Dè¯´è¯å¤´å”‡éƒ¨åŒæ­¥çš„å…³é”®æ ‡å‡†', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è¯­éŸ³é©±åŠ¨çš„3Dè¯´è¯å¤´ç”Ÿæˆä¸­çš„å”‡éƒ¨åŒæ­¥é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®æ ‡å‡†ï¼šæ—¶é—´åŒæ­¥ã€å”‡éƒ¨å¯è¯»æ€§å’Œè¡¨ç°åŠ›ï¼Œä»¥å®ç°æ„ŸçŸ¥ä¸Šå‡†ç¡®çš„å”‡éƒ¨è¿åŠ¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯­éŸ³-ç½‘æ ¼åŒæ­¥è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ•æ‰è¯­éŸ³ä¿¡å·ä¸3Dé¢éƒ¨ç½‘æ ¼ä¹‹é—´çš„å¤æ‚å¯¹åº”å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ„ŸçŸ¥æŸå¤±è®­ç»ƒ3Dè¯´è¯å¤´ç”Ÿæˆæ¨¡å‹æ˜¾è‘—æé«˜äº†å”‡éƒ¨åŒæ­¥çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21821', 'title': 'PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving', 'url': 'https://huggingface.co/papers/2503.21821', 'abstract': 'We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.', 'score': 15, 'issue_id': 2972, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '5254bbde255c8669', 'authors': ['Kaiyue Feng', 'Yilun Zhao', 'Yixin Liu', 'Tianyu Yang', 'Chen Zhao', 'John Sous', 'Arman Cohan'], 'affiliations': ['New York University', 'Notre Dame University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21821.jpg', 'data': {'categories': ['#reasoning', '#rag', '#benchmark', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜: Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ', 'desc': 'PHYSICS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1297 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ°Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. Ğ”Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RAG Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Benchmarking Physics Problem Solving with PHYSICS', 'desc': 'The paper presents PHYSICS, a benchmark designed to assess university-level physics problem solving capabilities. It includes 1297 expert-annotated problems across six fundamental physics domains, requiring both advanced knowledge and mathematical skills. The authors introduce an automated evaluation system to ensure accurate validation of model performance. Their findings reveal that even the top-performing model, o3-mini, only achieves 59.9% accuracy, indicating significant room for improvement in tackling complex scientific challenges.'}, 'zh': {'title': 'ç‰©ç†é—®é¢˜è§£å†³çš„æ–°åŸºå‡†', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†PHYSICSï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤§å­¦ç‰©ç†é—®é¢˜è§£å†³åŸºå‡†ã€‚å®ƒåŒ…å«1297ä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ï¼Œæ¶µç›–ç»å…¸åŠ›å­¦ã€é‡å­åŠ›å­¦ã€çƒ­åŠ›å­¦ä¸ç»Ÿè®¡åŠ›å­¦ã€ç”µç£å­¦ã€åŸå­ç‰©ç†å’Œå…‰å­¦å…­ä¸ªæ ¸å¿ƒé¢†åŸŸã€‚æ¯ä¸ªé—®é¢˜éƒ½éœ€è¦é«˜çº§ç‰©ç†çŸ¥è¯†å’Œæ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹o3-miniçš„å‡†ç¡®ç‡ä»…ä¸º59.9%ï¼Œè¿™çªæ˜¾äº†è§£å†³é«˜æ°´å¹³ç§‘å­¦é—®é¢˜çš„é‡å¤§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19108', 'title': 'Your ViT is Secretly an Image Segmentation Model', 'url': 'https://huggingface.co/papers/2503.19108', 'abstract': 'Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: https://www.tue-mps.org/eomt/.', 'score': 11, 'issue_id': 2980, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '96e8b32d843c2265', 'authors': ['Tommie Kerssies', 'NiccolÃ² Cavagnero', 'Alexander Hermans', 'Narges Norouzi', 'Giuseppe Averta', 'Bastian Leibe', 'Gijs Dubbelman', 'Daan de Geus'], 'affiliations': ['Eindhoven University of Technology', 'Polytechnic of Turin', 'RWTH Aachen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19108.jpg', 'data': {'categories': ['#cv', '#architecture', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ: ViT Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Vision Transformers (ViT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Encoder-only Mask Transformer (EoMT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ViT Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². EoMT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ²Ğ¾ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ViT ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Streamlining Image Segmentation with EoMT: Faster and Simpler ViTs', 'desc': "This paper presents the Encoder-only Mask Transformer (EoMT), a novel approach for image segmentation using Vision Transformers (ViTs). Instead of relying on additional components like convolutional adapters and pixel decoders, EoMT leverages the ViT's ability to learn necessary features directly from data. The authors demonstrate that with large models and extensive pre-training, EoMT achieves competitive segmentation accuracy while being significantly faster than traditional methods. This suggests that focusing on scaling the ViT architecture can yield better performance than adding complex task-specific elements."}, 'zh': {'title': 'ç®€åŒ–æ¶æ„ï¼Œæå‡åˆ†å‰²æ•ˆç‡', 'desc': 'è§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œç§°ä¸ºä»…ç¼–ç å™¨æ©ç å˜æ¢å™¨ï¼ˆEoMTï¼‰ï¼Œå®ƒåˆ©ç”¨ç®€å•çš„ViTæ¶æ„è¿›è¡Œå›¾åƒåˆ†å‰²ï¼Œè€Œæ— éœ€é¢å¤–çš„ä»»åŠ¡ç‰¹å®šç»„ä»¶ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒViTå¯ä»¥é€šè¿‡å¤§è§„æ¨¡æ¨¡å‹å’Œå……åˆ†çš„é¢„è®­ç»ƒæ¥å­¦ä¹ ä»»åŠ¡æ‰€éœ€çš„åç½®ã€‚EoMTåœ¨åˆ†å‰²ç²¾åº¦å’Œé¢„æµ‹é€Ÿåº¦ä¹‹é—´å®ç°äº†æœ€ä½³å¹³è¡¡ï¼Œä¸”é€Ÿåº¦æ¯”ä¼ ç»Ÿæ–¹æ³•å¿«ï¼Œæœ€é«˜å¯è¾¾4å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22268', 'title': 'Segment Any Motion in Videos', 'url': 'https://huggingface.co/papers/2503.22268', 'abstract': 'Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.', 'score': 10, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '084606e82bff72ff', 'authors': ['Nan Huang', 'Wenzhao Zheng', 'Chenfeng Xu', 'Kurt Keutzer', 'Shanghang Zhang', 'Angjoo Kanazawa', 'Qianqian Wang'], 'affiliations': ['Peking University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.22268.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ DINO Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ SAM2 Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing Moving Object Segmentation with Motion-Semantic Integration', 'desc': 'This paper presents a new method for moving object segmentation in videos, which is essential for understanding visual scenes. The authors address the limitations of traditional optical flow techniques that struggle with issues like motion blur and background distractions. Their approach combines long-range motion cues with semantic features from a DINO model and uses an iterative strategy with SAM2 for detailed pixel-level segmentation. The proposed model shows superior performance on various datasets, particularly in complex scenarios involving multiple moving objects.'}, 'zh': {'title': 'åˆ›æ–°ç§»åŠ¨ç‰©ä½“åˆ†å‰²æ–¹æ³•ï¼Œæå‡è§†è§‰ç†è§£èƒ½åŠ›', 'desc': 'ç§»åŠ¨ç‰©ä½“åˆ†å‰²æ˜¯ç†è§£è§†è§‰åœºæ™¯çš„é‡è¦ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–å…‰æµæ¥æä¾›è¿åŠ¨çº¿ç´¢ï¼Œä½†åœ¨å¤„ç†éƒ¨åˆ†è¿åŠ¨ã€å¤æ‚å˜å½¢ã€è¿åŠ¨æ¨¡ç³Šå’ŒèƒŒæ™¯å¹²æ‰°æ—¶å¸¸å¸¸æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç»“åˆé•¿è·ç¦»è½¨è¿¹è¿åŠ¨çº¿ç´¢ä¸åŸºäºDINOçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡è¿­ä»£æç¤ºç­–ç•¥åˆ©ç”¨SAM2è¿›è¡Œåƒç´ çº§æ©è†œç»†åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨æ—¶ç©ºè½¨è¿¹æ³¨æ„åŠ›å’Œè¿åŠ¨-è¯­ä¹‰è§£è€¦åµŒå…¥ï¼Œä¼˜å…ˆè€ƒè™‘è¿åŠ¨ï¼ŒåŒæ—¶æ•´åˆè¯­ä¹‰æ”¯æŒï¼Œç»è¿‡å¹¿æ³›æµ‹è¯•åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22236', 'title': 'Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging', 'url': 'https://huggingface.co/papers/2503.22236', 'abstract': 'With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.', 'score': 9, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '5024619189472d8c', 'authors': ['Chongjie Ye', 'Yushuang Wu', 'Ziteng Lu', 'Jiahao Chang', 'Xiaoyang Guo', 'Jiaqing Zhou', 'Hao Zhao', 'Xiaoguang Han'], 'affiliations': ['ByteDance', 'The Chinese University of Hong Kong, Shenzhen', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22236.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ»Ğ¾ÑĞºĞ¾Ğ³Ğ¾ Ğº Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğ¼Ñƒ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Hi3DGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Hi3DGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging Normals for High-Fidelity 3D Generation', 'desc': 'The paper introduces Hi3DGen, a new framework designed to create high-fidelity 3D models from 2D images. It tackles challenges like domain gaps and ambiguities in RGB images by using a method called normal bridging. Hi3DGen includes an image-to-normal estimator that improves the accuracy of geometric details through noise injection and dual-stream training. Additionally, it employs a normal-to-geometry learning approach and a 3D data synthesis pipeline to enhance the quality of the generated 3D models, showing superior performance compared to existing methods.'}, 'zh': {'title': 'é«˜ä¿çœŸ3Då‡ ä½•ä½“ç”Ÿæˆçš„æ–°æ–¹å‘', 'desc': 'éšç€å¯¹ä»2Då›¾åƒç”Ÿæˆé«˜ä¿çœŸ3Dæ¨¡å‹çš„éœ€æ±‚å¢åŠ ï¼Œç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®å†ç°ç»†è‡´å‡ ä½•ç»†èŠ‚æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Hi3DGenï¼Œä¸€ä¸ªé€šè¿‡æ³•çº¿æ¡¥æ¥ç”Ÿæˆé«˜ä¿çœŸ3Då‡ ä½•ä½“çš„æ–°æ¡†æ¶ã€‚Hi3DGenåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå›¾åƒåˆ°æ³•çº¿ä¼°è®¡å™¨ã€æ³•çº¿åˆ°å‡ ä½•ä½“å­¦ä¹ æ–¹æ³•å’Œ3Dæ•°æ®åˆæˆç®¡é“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆä¸°å¯Œå‡ ä½•ç»†èŠ‚æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22329', 'title': 'A Refined Analysis of Massive Activations in LLMs', 'url': 'https://huggingface.co/papers/2503.22329', 'abstract': 'Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.', 'score': 7, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '025b5484847cd3d9', 'authors': ['Louis Owen', 'Nilabhra Roy Chowdhury', 'Abhay Kumar', 'Fabian GÃ¼ra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2503.22329.jpg', 'data': {'categories': ['#optimization', '#architecture', '#long_context', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² LLM: Ğ½Ğµ Ğ²ÑĞµ Ñ‚Ğ°Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GLU Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ³Ğ°ÑÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ²ÑĞµ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹, Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Target Variance Rescaling Ñ Attention KV bias Ğ¸Ğ»Ğ¸ Dynamic Tanh, Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Balancing Act: Mitigating Massive Activations in LLMs', 'desc': 'This paper explores the phenomenon of massive activations in large language models (LLMs) and their implications for low-precision training and quantization. The authors analyze a variety of LLM architectures to understand the effects of massive activations, revealing that not all of them negatively impact model performance. They challenge previous assumptions by demonstrating that suppressing massive activations does not necessarily lead to worse outcomes in downstream tasks. Additionally, the paper introduces new hybrid strategies that effectively mitigate massive activations while maintaining model performance, particularly through the combination of Target Variance Rescaling and other techniques.'}, 'zh': {'title': 'å¤§æ¿€æ´»çš„æŒ‘æˆ˜ä¸æ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¤§æ¿€æ´»ç°è±¡ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨ä½ç²¾åº¦è®­ç»ƒå’Œé‡åŒ–ä¸­çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åˆ†æäº†å¤šç§LLMæ¶æ„ï¼ŒåŒ…æ‹¬åŸºäºGLUå’ŒéGLUçš„æ¨¡å‹ï¼Œå‘ç°å¹¶éæ‰€æœ‰å¤§æ¿€æ´»éƒ½æ˜¯æœ‰å®³çš„ï¼ŒæŠ‘åˆ¶å®ƒä»¬å¹¶ä¸ä¼šå¯¼è‡´å›°æƒ‘åº¦çš„çˆ†ç‚¸æˆ–ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å´©æºƒã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç°æœ‰çš„ç¼“è§£ç­–ç•¥å¦‚æ³¨æ„åŠ›KVåç½®åœ¨æŸäº›æƒ…å†µä¸‹æ˜¯æ¨¡å‹ç‰¹å®šçš„ä¸”æ— æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„æ··åˆç¼“è§£ç­–ç•¥ï¼Œç»“åˆç›®æ ‡æ–¹å·®é‡æ ‡å®šï¼ˆTVRï¼‰ä¸æ³¨æ„åŠ›KVåç½®æˆ–åŠ¨æ€Tanhï¼ˆDyTï¼‰ï¼ŒæˆåŠŸå¹³è¡¡äº†å¤§æ¿€æ´»çš„ç¼“è§£ä¸ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½çš„ä¿æŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17827', 'title': '4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object\n  Understanding', 'url': 'https://huggingface.co/papers/2503.17827', 'abstract': 'Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities. However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects (3D objects with temporal evolution over time). In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks. With 4D-Bench, we evaluate a wide range of open-source and closed-source MLLMs. The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding. 4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human baseline of 91\\%. These findings highlight a substantial gap in 4D object understanding and the need for further advancements in MLLMs.', 'score': 7, 'issue_id': 2978, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 22', 'zh': '3æœˆ22æ—¥'}, 'hash': '4c510d164c81f13e', 'authors': ['Wenxuan Zhu', 'Bing Li', 'Cheng Zheng', 'Jinjie Mai', 'Jun Chen', 'Letian Jiang', 'Abdullah Hamdi', 'Sara Rojas Martinez', 'Chia-Wen Lin', 'Mohamed Elhoseiny', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology', 'National Tsing Hua University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.17827.jpg', 'data': {'categories': ['#benchmark', '#3d', '#games', '#video', '#multimodal', '#open_source'], 'emoji': 'ğŸ•°ï¸', 'ru': {'title': '4D-Bench: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ MLLM Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': '4D-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ 4D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ 4D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¸ Ğ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLM Ñ…ÑƒĞ¶Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. Ğ”Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4o, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ 4D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼.'}, 'en': {'title': 'Bridging the Gap in 4D Object Understanding for MLLMs', 'desc': 'This paper introduces 4D-Bench, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their understanding of 4D objects, which are 3D objects that change over time. The benchmark includes tasks such as 4D object Question Answering and 4D object captioning, focusing on multi-view spatial-temporal understanding. The evaluation reveals that MLLMs struggle with temporal understanding, showing significant performance gaps compared to their ability to understand object appearance. Overall, the findings indicate a critical need for improvements in MLLMs to enhance their capabilities in 4D object comprehension.'}, 'zh': {'title': 'å››ç»´ç‰©ä½“ç†è§£çš„æ–°åŸºå‡†ï¼š4D-Bench', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨äºŒç»´å›¾åƒå’Œè§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹è¯„ä¼°å››ç»´ç‰©ä½“ç†è§£èƒ½åŠ›çš„æ ‡å‡†åŸºå‡†ã€‚æœ¬æ–‡æå‡ºäº†4D-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsåœ¨å››ç»´ç‰©ä½“ç†è§£èƒ½åŠ›çš„åŸºå‡†ï¼ŒåŒ…å«å››ç»´ç‰©ä½“é—®ç­”å’Œå››ç»´ç‰©ä½“æè¿°ç­‰ä»»åŠ¡ã€‚4D-Benchæä¾›äº†å¤šæ ·åŒ–çš„å››ç»´ç‰©ä½“ç±»åˆ«å’Œé«˜è´¨é‡çš„æ³¨é‡Šï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡å¤šè§†è§’çš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLLMsåœ¨æ—¶é—´ç†è§£æ–¹é¢æ™®éè¾ƒå¼±ï¼Œå°¤å…¶æ˜¯å¼€æºæ¨¡å‹åœ¨å¤–è§‚ç†è§£ä¸Šæ¥è¿‘é—­æºæ¨¡å‹ï¼Œä½†åœ¨æ—¶é—´ç†è§£ä¸Šå­˜åœ¨è¾ƒå¤§å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21732', 'title': 'SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling', 'url': 'https://huggingface.co/papers/2503.21732', 'abstract': 'Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.', 'score': 5, 'issue_id': 2972, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': 'f17ea311cc796683', 'authors': ['Xianglong He', 'Zi-Xin Zou', 'Chia-Hao Chen', 'Yuan-Chen Guo', 'Ding Liang', 'Chun Yuan', 'Wanli Ouyang', 'Yan-Pei Cao', 'Yangguang Li'], 'affiliations': ['The Chinese University of Hong Kong', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2503.21732.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ', 'desc': 'SparseFlex - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 1024^3 Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Flexicubes Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¿Ñ€Ğ¸Ğ»ĞµĞ³Ğ°ÑÑ‰Ğ¸Ñ… Ğº Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ñ€ÑƒÑÑ‚ÑƒĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. SparseFlex Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ ÑĞµÑ‚ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SparseFlex: Revolutionizing 3D Mesh Reconstruction with Efficiency and Detail', 'desc': 'This paper presents SparseFlex, a new method for creating detailed 3D meshes with complex shapes and open surfaces. It uses a sparse voxel structure to focus on areas near the surface, allowing for efficient high-resolution mesh reconstruction directly from rendering losses. The method introduces a frustum-aware training strategy that reduces memory usage by activating only necessary voxels during rendering. SparseFlex achieves impressive results, outperforming previous techniques in accuracy and enabling the generation of intricate 3D shapes with varying topologies.'}, 'zh': {'title': 'SparseFlexï¼šé«˜åˆ†è¾¨ç‡3Dç½‘æ ¼é‡å»ºçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSparseFlexçš„æ–°å‹ç¨€ç–ç»“æ„ç­‰å€¼é¢è¡¨ç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é«˜ä¿çœŸ3Dç½‘æ ¼é‡å»ºä¸­çš„æŒ‘æˆ˜ã€‚SparseFlexèƒ½å¤Ÿç›´æ¥ä»æ¸²æŸ“æŸå¤±ä¸­è¿›è¡Œå¯å¾®åˆ†çš„ç½‘æ ¼é‡å»ºï¼Œæ”¯æŒé«˜è¾¾1024^3çš„åˆ†è¾¨ç‡ã€‚é€šè¿‡ç»“åˆFlexicubesçš„å‡†ç¡®æ€§å’Œç¨€ç–ä½“ç´ ç»“æ„ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå¤„ç†å¼€æ”¾è¡¨é¢ï¼Œå¹¶å¼•å…¥äº†åŸºäºè§†é”¥çš„åˆ†æ®µä½“ç´ è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSparseFlexåœ¨é‡å»ºç²¾åº¦ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒæˆåŠŸç”Ÿæˆäº†å…·æœ‰ä»»æ„æ‹“æ‰‘çš„é«˜åˆ†è¾¨ç‡ã€ç»†èŠ‚ä¸°å¯Œçš„3Då½¢çŠ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21751', 'title': 'Reconstructing Humans with a Biomechanically Accurate Skeleton', 'url': 'https://huggingface.co/papers/2503.21751', 'abstract': 'In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/', 'score': 4, 'issue_id': 2979, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': 'a715b9755e020ce7', 'authors': ['Yan Xia', 'Xiaowei Zhou', 'Etienne Vouga', 'Qixing Huang', 'Georgios Pavlakos'], 'affiliations': ['The University of Texas at Austin', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21751.jpg', 'data': {'categories': ['#benchmark', '#3d', '#architecture', '#dataset'], 'emoji': 'ğŸ¦´', 'ru': {'title': 'Ğ‘Ğ¸Ğ¾Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¸Ğ¾Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜Ğ·-Ğ·Ğ° Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-ÑĞµÑ‚ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ñ… Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ 3D-Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Reconstructing Realistic 3D Humans from Single Images', 'desc': 'This paper presents a novel method for creating 3D models of humans from just one image by using a skeleton model that accurately reflects human biomechanics. The authors train a transformer model that processes the input image to predict the parameters of this skeleton. To address the challenge of limited training data, they develop a pipeline that generates pseudo ground truth parameters, which are refined through an iterative training process. Their approach not only matches the performance of existing 3D human reconstruction methods but also excels in challenging scenarios with extreme poses and viewpoints, while ensuring realistic joint movements.'}, 'zh': {'title': 'ç”Ÿç‰©åŠ›å­¦é©±åŠ¨çš„3Däººç±»é‡å»ºæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒé‡å»º3Däººç±»æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿ç”¨ç”Ÿç‰©åŠ›å­¦å‡†ç¡®çš„éª¨éª¼æ¨¡å‹ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå˜æ¢å™¨ï¼Œè¾“å…¥å›¾åƒå¹¶ä¼°è®¡æ¨¡å‹å‚æ•°ã€‚ç”±äºè®­ç»ƒæ•°æ®çš„ä¸è¶³ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç®¡é“æ¥ç”Ÿæˆå•å¼ å›¾åƒçš„ä¼ªçœŸå®æ¨¡å‹å‚æ•°ï¼Œå¹¶å®æ–½äº†ä¸€ä¸ªè¿­ä»£ä¼˜åŒ–ä¼ªæ ‡ç­¾çš„è®­ç»ƒè¿‡ç¨‹ã€‚ä¸ç°æœ‰çš„3Däººç±»ç½‘æ ¼æ¢å¤æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå°¤å…¶åœ¨æç«¯3Då§¿åŠ¿å’Œè§†è§’ä¸‹è¡¨ç°æ›´ä¸ºä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18968', 'title': 'MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow', 'url': 'https://huggingface.co/papers/2503.18968', 'abstract': 'Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.', 'score': 4, 'issue_id': 2973, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '662e73dea1e23d07', 'authors': ['Ziyue Wang', 'Junde Wu', 'Chang Han Low', 'Yueming Jin'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.18968.jpg', 'data': {'categories': ['#hallucinations', '#3d', '#multimodal', '#reasoning', '#interpretability', '#healthcare', '#agents'], 'emoji': 'ğŸ©º', 'ru': {'title': 'MedAgent-Pro: Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MedAgent-Pro - Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ MLLM Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. MedAgent-Pro Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ MedAgent-Pro Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 2D Ğ¸ 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸.'}, 'en': {'title': 'MedAgent-Pro: Reliable AI for Accurate Medical Diagnosis', 'desc': 'This paper introduces MedAgent-Pro, a new AI system designed to improve medical diagnosis by combining multi-modal inputs and evidence-based reasoning. It addresses the limitations of existing Multi-modal Large Language Models (MLLMs), which struggle with visual data and often produce unreliable outputs. MedAgent-Pro uses a hierarchical approach, where knowledge-based reasoning creates diagnostic plans and multiple tool agents analyze various indicators to provide accurate diagnoses. Experiments show that MedAgent-Pro outperforms traditional methods in both 2D and 3D medical tasks, demonstrating its reliability and interpretability in clinical settings.'}, 'zh': {'title': 'æå‡åŒ»ç–—è¯Šæ–­çš„å¯é æ€§ä¸å¯è§£é‡Šæ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMedAgent-Proçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€åŒ»ç–—è¯Šæ–­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†åŸºäºçŸ¥è¯†çš„æ¨ç†å’Œå¤šå·¥å…·ä»£ç†ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è¾“å…¥å¹¶ç”Ÿæˆå¯é çš„è¯Šæ–­è®¡åˆ’ã€‚é€šè¿‡å¯¹2Då’Œ3DåŒ»ç–—è¯Šæ–­ä»»åŠ¡çš„å…¨é¢å®éªŒï¼ŒMedAgent-Proå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚è¯¥ç³»ç»Ÿä¸ä»…æä¾›äº†ç²¾ç¡®çš„è¯Šæ–­ï¼Œè¿˜å…·å¤‡è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œé€‚åˆä¸´åºŠåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21851', 'title': 'On Large Multimodal Models as Open-World Image Classifiers', 'url': 'https://huggingface.co/papers/2503.21851', 'abstract': 'Traditional image classification requires a predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt "What is the main object in the image?"). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming a closed-world setting with a predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in a truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, fine-grained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them.', 'score': 3, 'issue_id': 2977, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '1f5e6c94324fef3f', 'authors': ['Alessandro Conti', 'Massimiliano Mancini', 'Enrico Fini', 'Yiming Wang', 'Paolo Rota', 'Elisa Ricci'], 'affiliations': ['Fondazione Bruno Kessler', 'Independent researcher', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.21851.jpg', 'data': {'categories': ['#reasoning', '#alignment', '#benchmark', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ°Ğ¼Ğ¸. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° 13 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 10 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ½ĞµĞ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ LMM, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸.'}, 'en': {'title': 'Unlocking Open-World Image Classification with LMMs', 'desc': 'This paper explores the capabilities of Large Multimodal Models (LMMs) in image classification without relying on a fixed set of categories. It introduces a new evaluation framework to assess LMM performance in an open-world context, where images can belong to any category. The study evaluates 13 different models across various benchmarks, revealing the difficulties LMMs encounter, particularly with fine-grained classifications. Additionally, it discusses how improved prompting and reasoning techniques can help mitigate these challenges.'}, 'zh': {'title': 'å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œä¸­çš„å›¾åƒåˆ†ç±»æŒ‘æˆ˜', 'desc': 'ä¼ ç»Ÿçš„å›¾åƒåˆ†ç±»éœ€è¦é¢„å®šä¹‰çš„è¯­ä¹‰ç±»åˆ«ï¼Œè€Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ç›´æ¥å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚è¿™é¡¹ç ”ç©¶è¯„ä¼°äº†LMMåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„åˆ†ç±»æ€§èƒ½ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯„ä¼°åè®®ï¼Œå®šä¹‰äº†å¤šç§æŒ‡æ ‡æ¥è¯„ä¼°é¢„æµ‹ç±»åˆ«ä¸çœŸå®ç±»åˆ«ä¹‹é—´çš„å¯¹é½æƒ…å†µã€‚é€šè¿‡å¯¹13ä¸ªæ¨¡å‹åœ¨10ä¸ªåŸºå‡†ä¸Šçš„è¯„ä¼°ï¼Œæ­ç¤ºäº†LMMåœ¨ç»†ç²’åº¦åˆ†ç±»ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†å®šåˆ¶æç¤ºå’Œæ¨ç†çš„æ–¹æ³•æ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21779', 'title': 'X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction', 'url': 'https://huggingface.co/papers/2503.21779', 'abstract': 'Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.', 'score': 2, 'issue_id': 2976, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '628d77c9c1bdcd9e', 'authors': ['Weihao Yu', 'Yuanhao Cai', 'Ruyi Zha', 'Zhiwen Fan', 'Chenxin Li', 'Yixuan Yuan'], 'affiliations': ['Johns Hopkins University', 'The Australian National University', 'The Chinese University of Hong Kong', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.21779.jpg', 'data': {'categories': ['#architecture', '#healthcare', '#cv'], 'emoji': 'ğŸ«', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ 4D-ĞšĞ¢ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ X^2-Gaussian - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D-ĞšĞ¢ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑĞµĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€ĞµÑĞ¿Ğ¸Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. X^2-Gaussian Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 9.93 Ğ´Ğ‘ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ PSNR, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing 4D CT with Continuous Motion Modeling', 'desc': 'This paper presents X^2-Gaussian, a new method for 4D CT reconstruction that overcomes the limitations of traditional phase-binning techniques. By using a spatiotemporal encoder-decoder architecture, it models dynamic anatomical changes without relying on fixed phases, thus allowing for continuous-time reconstruction. The method incorporates self-supervised learning to understand patient-specific breathing patterns, eliminating the need for external respiratory gating devices. Experimental results show that X^2-Gaussian significantly improves image quality, achieving higher PSNR compared to existing methods.'}, 'zh': {'title': 'X^2-Gaussianï¼šæ— ç¡¬ä»¶çš„é«˜ä¿çœŸ4D CTé‡å»ºæ–°æ–¹æ³•', 'desc': 'å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4D CTï¼‰é‡å»ºå¯¹äºæ•æ‰åŠ¨æ€è§£å‰–å˜åŒ–è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿçš„ç›¸ä½åˆ†ç®±å·¥ä½œæµç¨‹å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•å°†æ—¶é—´åˆ†è¾¨ç‡ç¦»æ•£åŒ–ä¸ºå›ºå®šç›¸ä½ï¼Œä½¿ç”¨å‘¼å¸é—¨æ§è®¾å¤‡ï¼Œå¯¼è‡´è¿åŠ¨é”™ä½å¹¶é™åˆ¶ä¸´åºŠå®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶X^2-Gaussianï¼Œé€šè¿‡ç»“åˆåŠ¨æ€è¾å°„é«˜æ–¯ç‚¹äº‘å’Œè‡ªç›‘ç£å‘¼å¸è¿åŠ¨å­¦ä¹ ï¼Œå®ç°è¿ç»­æ—¶é—´çš„4D CTé‡å»ºã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æ—¶ç©ºç¼–ç å™¨-è§£ç å™¨æ¶æ„é¢„æµ‹æ—¶é—´å˜åŒ–çš„é«˜æ–¯å˜å½¢ï¼Œæ¶ˆé™¤äº†ç›¸ä½ç¦»æ•£åŒ–çš„éœ€æ±‚ï¼Œå¹¶å¼•å…¥ç”Ÿç†é©±åŠ¨çš„å‘¨æœŸä¸€è‡´æ€§æŸå¤±ï¼Œç›´æ¥ä»æŠ•å½±ä¸­å­¦ä¹ æ‚£è€…ç‰¹å®šçš„å‘¼å¸å‘¨æœŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22625', 'title': 'Challenges and Paths Towards AI for Software Engineering', 'url': 'https://huggingface.co/papers/2503.22625', 'abstract': 'AI for software engineering has made remarkable progress recently, becoming a notable success within generative AI. Despite this, there are still many challenges that need to be addressed before automated software engineering reaches its full potential. It should be possible to reach high levels of automation where humans can focus on the critical decisions of what to build and how to balance difficult tradeoffs while most routine development effort is automated away. Reaching this level of automation will require substantial research and engineering efforts across academia and industry. In this paper, we aim to discuss progress towards this in a threefold manner. First, we provide a structured taxonomy of concrete tasks in AI for software engineering, emphasizing the many other tasks in software engineering beyond code generation and completion. Second, we outline several key bottlenecks that limit current approaches. Finally, we provide an opinionated list of promising research directions toward making progress on these bottlenecks, hoping to inspire future research in this rapidly maturing field.', 'score': 1, 'issue_id': 2993, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': 'b8c25b067c6a7343', 'authors': ['Alex Gu', 'Naman Jain', 'Wen-Ding Li', 'Manish Shetty', 'Yijia Shao', 'Ziyang Li', 'Diyi Yang', 'Kevin Ellis', 'Koushik Sen', 'Armando Solar-Lezama'], 'affiliations': ['Cornell University', 'MIT CSAIL', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.22625.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ˜Ğ˜ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ: Ğ¾Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑÑƒ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ˜Ğ˜ Ğ² ÑÑ„ĞµÑ€Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ. Ğ¦ĞµĞ»ÑŒ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ - Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ.'}, 'en': {'title': 'Unlocking Automation in Software Engineering with AI', 'desc': 'This paper discusses the advancements in AI for software engineering, highlighting its success in generative AI. It identifies the challenges that must be overcome to achieve high levels of automation, allowing humans to focus on critical decision-making. The authors present a structured taxonomy of various tasks in AI for software engineering, going beyond just code generation. They also outline key bottlenecks in current methods and suggest promising research directions to address these issues, aiming to inspire further exploration in this evolving field.'}, 'zh': {'title': 'æ¨åŠ¨è½¯ä»¶å·¥ç¨‹è‡ªåŠ¨åŒ–çš„æœªæ¥', 'desc': 'æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–¹é¢çš„æˆåŠŸã€‚å°½ç®¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å®ç°å®Œå…¨è‡ªåŠ¨åŒ–çš„è½¯ä»¶å·¥ç¨‹ä¹‹å‰ï¼Œä»é¢ä¸´è®¸å¤šæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»æ³•ï¼Œæ¶µç›–äº†è½¯ä»¶å·¥ç¨‹ä¸­é™¤äº†ä»£ç ç”Ÿæˆå’Œè¡¥å…¨ä¹‹å¤–çš„å¤šç§ä»»åŠ¡ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰æ–¹æ³•çš„å…³é”®ç“¶é¢ˆã€‚æœ€åï¼Œæˆ‘ä»¬åˆ—å‡ºäº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æœŸæ¿€åŠ±æœªæ¥åœ¨è¿™ä¸€å¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21544', 'title': 'SWI: Speaking with Intent in Large Language Models', 'url': 'https://huggingface.co/papers/2503.21544', 'abstract': "Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMs' reasoning abilities with cognitive notions.", 'score': 1, 'issue_id': 2985, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '1a89039133f8f32f', 'authors': ['Yuwei Yin', 'EunJeong Hwang', 'Giuseppe Carenini'], 'affiliations': ['University of British Columbia', 'Vector Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.21544.jpg', 'data': {'categories': ['#math', '#hallucinations', '#interpretability', '#benchmark', '#dataset', '#training', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ', 'desc': "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ“Ğ¾Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ĞµĞ¼' (Speaking with Intent, SWI) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). SWI Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SWI Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. SWI Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹."}, 'en': {'title': 'Enhancing LLMs with Intent-Driven Reasoning', 'desc': "This paper presents the concept of Speaking with Intent (SWI) in large language models (LLMs), which aims to improve their reasoning and communication by explicitly defining the model's intentions. By mimicking human-like thought processes, SWI enhances the quality of generated outputs and guides the model's analysis. Experimental results show that SWI outperforms traditional methods, including baseline generation and popular prompting techniques, in various reasoning tasks and text summarization. The findings suggest that incorporating cognitive frameworks like SWI can significantly boost the performance and reliability of LLMs."}, 'zh': {'title': 'æœ‰æ„è¯´è¯ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å¼•å…¥çš„â€œæœ‰æ„è¯´è¯â€ï¼ˆSWIï¼‰æ¦‚å¿µï¼Œå¼ºè°ƒæ˜ç¡®ç”Ÿæˆçš„æ„å›¾ä½œä¸ºæ¨¡å‹çš„è®¤çŸ¥æ¡†æ¶ï¼ŒæŒ‡å¯¼åç»­åˆ†æå’Œäº¤æµã€‚SWIé€šè¿‡æ¨¡æ‹Ÿäººç±»æ€ç»´ä¸­çš„æœ‰æ„å›¾å’Œç›®çš„æ€§æ€è€ƒï¼Œå‡è®¾èƒ½å¤Ÿå¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSWIåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ¨ç†å¯†é›†å‹é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºä¸€è‡´çš„æ”¹è¿›ã€‚SWIç”Ÿæˆçš„æ‘˜è¦åœ¨å‡†ç¡®æ€§ã€ç®€æ´æ€§å’Œäº‹å®æ­£ç¡®æ€§æ–¹é¢ä¼˜äºåŸºçº¿ï¼Œä¸”äººç±»è¯„ä¼°éªŒè¯äº†å…¶ç”Ÿæˆæ„å›¾çš„è¿è´¯æ€§å’Œæœ‰æ•ˆæ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (8)', '#agents (2)', '#agi', '#alignment (2)', '#architecture (5)', '#audio (1)', '#benchmark (5)', '#cv (3)', '#data (3)', '#dataset (4)', '#diffusion', '#ethics', '#games (1)', '#graphs', '#hallucinations (2)', '#healthcare (2)', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (7)', '#open_source (1)', '#optimization (5)', '#plp', '#rag (1)', '#reasoning (9)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (10)', '#transfer_learning (2)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-04-01 02:45',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-01 02:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-01 02:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    