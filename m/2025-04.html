
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 315 papers. April 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">ĞĞ¿Ñ€ĞµĞ»ÑŒ 2025</span> | <span id="title-articles-count">315 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-03.html">â¬…ï¸ <span id="prev-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-05.html">â¡ï¸ <span id="next-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'ĞĞ¿Ñ€ĞµĞ»ÑŒ 2025', 'en': 'April 2025', 'zh': '4æœˆ2025å¹´'};
        let feedDateNext = {'ru': '05.2025', 'en': '05/2025', 'zh': '5æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '03.2025', 'en': '03/2025', 'zh': '3æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.06263', 'title': 'OmniSVG: A Unified Scalable Vector Graphics Generation Model', 'url': 'https://huggingface.co/papers/2504.06263', 'abstract': 'Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.', 'score': 85, 'issue_id': 3138, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': '3b3365aa60717b2a', 'authors': ['Yiying Yang', 'Wei Cheng', 'Sijin Chen', 'Xianfang Zeng', 'Jiaxu Zhang', 'Liao Wang', 'Gang Yu', 'Xingjun Ma', 'Yu-Gang Jiang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.06263.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'OmniSVG: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'OmniSVG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ SVG Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ SVG Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMSVG-2M Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… SVG-Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OmniSVG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° SVG.'}, 'en': {'title': 'OmniSVG: Revolutionizing SVG Generation with Vision-Language Models', 'desc': "This paper presents OmniSVG, a novel framework for generating high-quality Scalable Vector Graphics (SVG) using pre-trained Vision-Language Models (VLMs). It addresses the limitations of existing methods by producing structured outputs efficiently, avoiding the high computational costs and oversimplification seen in previous approaches. OmniSVG achieves this by converting SVG commands and coordinates into discrete tokens, allowing for a clear separation of structural logic from geometric details. Additionally, the introduction of the MMSVG-2M dataset, containing two million annotated SVG assets, supports the framework's training and evaluation, showcasing its superiority over current SVG generation techniques."}, 'zh': {'title': 'OmniSVGï¼šé«˜æ•ˆç”Ÿæˆå¤æ‚SVGçš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºOmniSVGçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡å’Œå¤æ‚çš„å¯ç¼©æ”¾çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œé€šè¿‡å°†SVGå‘½ä»¤å’Œåæ ‡å‚æ•°åŒ–ä¸ºç¦»æ•£æ ‡è®°ï¼Œå®ç°äº†é«˜æ•ˆçš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€SVGç”Ÿæˆã€‚OmniSVGå°†ç»“æ„é€»è¾‘ä¸ä½çº§å‡ ä½•è§£è€¦ï¼Œä»è€Œåœ¨ä¿æŒå¤æ‚SVGç»“æ„è¡¨ç°åŠ›çš„åŒæ—¶ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†MMSVG-2Mæ•°æ®é›†ï¼ŒåŒ…å«ä¸¤ç™¾ä¸‡ä¸ªä¸°å¯Œæ³¨é‡Šçš„SVGèµ„äº§ï¼Œä»¥æ¨åŠ¨SVGåˆæˆçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06261', 'title': 'Hogwild! Inference: Parallel LLM Generation via Concurrent Attention', 'url': 'https://huggingface.co/papers/2504.06261', 'abstract': 'Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other\'s partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other\'s generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.', 'score': 68, 'issue_id': 3141, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': '354599744af26b06', 'authors': ['Gleb Rodionov', 'Roman Garipov', 'Alina Shutova', 'George Yakushev', 'Vage Egiazarian', 'Anton Sinitsin', 'Denis Kuznedelev', 'Dan Alistarh'], 'affiliations': ['HSE University', 'IST Austria', 'Yandex'], 'pdf_title_img': 'assets/pdf/title_img/2504.06261.jpg', 'data': {'categories': ['#inference', '#reasoning', '#long_context', '#optimization', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‰ĞµĞµÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Hogwild! Inference, Ğ³Ğ´Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ LLM Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ĞºÑÑˆ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ, ĞºĞ°Ğº Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°Ñ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸Ğ¼ĞµÑ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Rotary Position Embeddings Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Collaborative Parallelism for Enhanced LLM Efficiency', 'desc': "This paper explores a new method for improving the efficiency of Large Language Models (LLMs) during complex tasks by enabling them to work in parallel. The authors introduce a system called Hogwild! Inference, where multiple LLM instances share an attention cache, allowing them to see each other's progress and collaborate effectively. This approach leverages Rotary Position Embeddings (RoPE) to enhance performance without the need for extra fine-tuning. The findings suggest that LLMs can autonomously develop collaboration strategies, leading to faster and more efficient problem-solving."}, 'zh': {'title': 'å¹¶è¡Œåˆä½œï¼Œæå‡LLMæ¨ç†æ•ˆç‡', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡é«˜çº§æ¨ç†ã€é•¿ç¯‡å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨æ¥å¤„ç†è¶Šæ¥è¶Šå¤æ‚çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›ä»»åŠ¡ï¼Œç ”ç©¶è¡¨æ˜LLMså¯ä»¥é€šè¿‡å®ç°æ˜ç¡®çš„åˆä½œæ¡†æ¶æ¥å¹¶è¡Œæ“ä½œï¼Œä¾‹å¦‚æŠ•ç¥¨æœºåˆ¶æˆ–ç‹¬ç«‹å­ä»»åŠ¡çš„åˆ›å»ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®¾è®¡æ–¹æ³•ï¼šè®©LLMâ€œå·¥ä½œè€…â€å¹¶è¡Œè¿è¡Œï¼Œé€šè¿‡åŒæ—¶æ›´æ–°çš„æ³¨æ„åŠ›ç¼“å­˜è¿›è¡ŒåŒæ­¥ï¼Œå¹¶å†³å®šæœ€ä½³çš„åˆä½œæ–¹å¼ã€‚æˆ‘ä»¬å®ç°äº†Hogwild!æ¨ç†ï¼Œè¿™æ˜¯ä¸€ç§å¹¶è¡ŒLLMæ¨ç†å¼•æ“ï¼Œå¤šä¸ªç›¸åŒçš„LLMå®ä¾‹åœ¨å…±äº«çš„æ³¨æ„åŠ›ç¼“å­˜ä¸­å¹¶è¡Œè¿è¡Œï¼Œèƒ½å¤Ÿâ€œå³æ—¶â€è®¿é—®å½¼æ­¤ç”Ÿæˆçš„æ ‡è®°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05599', 'title': 'Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought', 'url': 'https://huggingface.co/papers/2504.05599', 'abstract': 'We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retraining of either the foundational language model or the vision encoder. To strengthen visual-text alignment, we propose a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), significantly enhancing cross-modal integration efficiency. Additionally, we introduce an adaptive-length Chain-of-Thought distillation approach for reasoning data generation. This approach dynamically optimizes reasoning chain lengths, thereby enhancing inference efficiency and preventing excessive reasoning overthinking. Empirical evaluations demonstrate that Skywork R1V, with only 38B parameters, delivers competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. Meanwhile, it maintains robust textual reasoning performance, evidenced by impressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model weights have been publicly released to promote openness and reproducibility.', 'score': 60, 'issue_id': 3142, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': 'b963d5098e669229', 'authors': ['Yi Peng', 'Chris', 'Xiaokun Wang', 'Yichen Wei', 'Jiangbo Pei', 'Weijie Qiu', 'Ai Jian', 'Yunzhuo Hao', 'Jiachun Pan', 'Tianyidan Xie', 'Li Ge', 'Rongxian Zhuang', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.05599.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#transfer_learning', '#benchmark', '#inference', '#architecture', '#multimodal', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Skywork R1V - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµÑ€Ğ¸Ğ¸ R1 Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Skywork R1V Ñ 38 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Seamless Multimodal Reasoning with Skywork R1V', 'desc': 'Skywork R1V is a new multimodal reasoning model that enhances the capabilities of existing R1-series large language models by integrating visual data. It uses a lightweight visual projector to adapt to visual inputs without needing to retrain the foundational language model or the vision encoder. The model employs a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning and Group Relative Policy Optimization to improve the alignment between text and visual information. Additionally, it features an adaptive-length Chain-of-Thought distillation method that optimizes reasoning processes, leading to efficient inference and strong performance on various benchmarks.'}, 'zh': {'title': 'Skywork R1Vï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹', 'desc': 'Skywork R1Væ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œå®ƒé€šè¿‡é«˜æ•ˆçš„å¤šæ¨¡æ€è½¬ç§»æ–¹æ³•æ‰©å±•äº†R1ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ°è§†è§‰æ¨¡æ€ã€‚è¯¥æ¨¡å‹åˆ©ç”¨è½»é‡çº§è§†è§‰æŠ•å½±å™¨ï¼Œå®ç°äº†æ— é¡»é‡æ–°è®­ç»ƒåŸºç¡€è¯­è¨€æ¨¡å‹æˆ–è§†è§‰ç¼–ç å™¨çš„æ— ç¼å¤šæ¨¡æ€é€‚åº”ã€‚ä¸ºäº†å¢å¼ºè§†è§‰ä¸æ–‡æœ¬çš„å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆä¼˜åŒ–ç­–ç•¥ï¼Œç»“åˆäº†è¿­ä»£ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæ˜¾è‘—æé«˜äº†è·¨æ¨¡æ€é›†æˆçš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”é•¿åº¦çš„æ€ç»´é“¾è’¸é¦æ–¹æ³•ï¼ŒåŠ¨æ€ä¼˜åŒ–æ¨ç†é“¾çš„é•¿åº¦ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ï¼Œé¿å…è¿‡åº¦æ€è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05979', 'title': 'An Empirical Study of GPT-4o Image Generation Capabilities', 'url': 'https://huggingface.co/papers/2504.05979', 'abstract': "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.", 'score': 46, 'issue_id': 3137, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': 'f1195a87ec5b86f1', 'authors': ['Sixiang Chen', 'Jinbin Bai', 'Zhuoran Zhao', 'Tian Ye', 'Qingyu Shi', 'Donghao Zhou', 'Wenhao Chai', 'Xin Lin', 'Jianzong Wu', 'Chao Tang', 'Shilin Xu', 'Tao Zhang', 'Haobo Yuan', 'Yikang Zhou', 'Wei Chow', 'Linfeng Li', 'Xiangtai Li', 'Lei Zhu', 'Lu Qi'], 'affiliations': ['National University of Singapore', 'Peking University', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (GZ)', 'University of Washington', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05979.jpg', 'data': {'categories': ['#diffusion', '#cv', '#architecture', '#multimodal', '#open_source', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'GPT-4o: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ GPT-4o. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ GPT-4o Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 20 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-3D Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-X. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ GPT-4o Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unifying Image and Text Generation with GPT-4o', 'desc': "This paper explores the advancements in image generation, focusing on the capabilities of the GPT-4o model. It conducts a thorough evaluation of GPT-4o's performance in various generative tasks, including text-to-image and image-to-3D generation. The study benchmarks GPT-4o against other leading models, revealing its strengths and weaknesses in multimodal generation. The findings suggest future directions for improving unified generative architectures, particularly in terms of design and data utilization."}, 'zh': {'title': 'æ¢ç´¢ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„æœªæ¥æ–¹å‘', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å›¾åƒç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯GPT-4oæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œå¹¶ä¸é¢†å…ˆçš„å¼€æºå’Œå•†ä¸šæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°3Då’Œå›¾åƒåˆ°Xç”Ÿæˆç­‰å››ä¸ªä¸»è¦ç±»åˆ«ã€‚åˆ†æç»“æœæ­ç¤ºäº†GPT-4oåœ¨ä¸åŒè®¾ç½®ä¸‹çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å°†å…¶ç½®äºç”Ÿæˆå»ºæ¨¡çš„æ›´å¹¿æ³›æ¼”å˜ä¸­ã€‚é€šè¿‡è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæœªæ¥ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„æœ‰å¸Œæœ›çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†æ¶æ„è®¾è®¡å’Œæ•°æ®æ‰©å±•çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05535', 'title': 'COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for\n  Alignment with Human Values', 'url': 'https://huggingface.co/papers/2504.05535', 'abstract': 'Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and response labeling significantly constrains the scalability of human preference datasets. To address these challenges, we design an LLM-based Chinese preference dataset annotation pipeline with no human intervention. Specifically, we crawled and carefully filtered 92k high-quality Chinese queries and employed 15 mainstream LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce COIG-P (Chinese Open Instruction Generalist - Preference), a high-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese preference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel, and Role. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously constructed a Chinese Reward Benchmark (CRBench). Evaluation results based on AlignBench liu2024alignbenchbenchmarkingchinesealignment show that that COIG-P significantly outperforms other Chinese preference datasets, and it brings significant performance improvements ranging from 2% to 12% for the Qwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results on CRBench demonstrate that our CRM has a strong and robust scoring ability. We apply it to filter chosen-rejected response pairs in a test split of COIG-P, and our experiments show that it is comparable to GPT-4o in identifying low-quality samples while maintaining efficiency and cost-effectiveness. Our codes and data are released in https://github.com/multimodal-art-projection/COIG-P.', 'score': 33, 'issue_id': 3145, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '420fafe139f00d54', 'authors': ['M-A-P Team', 'Siwei Wu', 'Jincheng Ren', 'Xinrun Du', 'Shuyue Guo', 'Xingwei Qu', 'Yiming Liang', 'Jie Liu', 'Yunwen Li', 'Tianyu Zheng', 'Boyu Feng', 'Huaqing Yuan', 'Zenith Wang', 'Jiaheng Liu', 'Wenhao Huang', 'Chenglin Cai', 'Haoran Que', 'Jian Yang', 'Yuelin Bai', 'Zekun Moore Wang', 'Zhouliang Yu', 'Qunshu Lin', 'Ding Pan', 'Yuchen Jiang', 'Tiannan Wang', 'Wangchunshu Zhou', 'Shenzhi Wang', 'Xingyuan Bu', 'Minghao Liu', 'Guoyin Wang', 'Ge Zhang', 'Chenghua Lin'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.05535.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#data', '#rlhf', '#open_source'], 'emoji': 'ğŸ‡¨ğŸ‡³', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ COIG-P - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM, ÑĞ¾Ğ±Ñ€Ğ°Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² 6 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ COIG-P Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (CRM) Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CRBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ COIG-P Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 2-12%.'}, 'en': {'title': 'Revolutionizing Chinese Preference Datasets with LLMs', 'desc': 'This paper presents a novel approach to creating a large-scale Chinese preference dataset, COIG-P, which addresses the limitations of existing datasets. By utilizing 15 mainstream large language models (LLMs) to generate and score response pairs, the authors eliminate the need for human annotators, enhancing scalability. The dataset includes 1,009k preference pairs across six diverse domains, significantly improving performance metrics for various models. Additionally, a Chinese Reward Model (CRM) is developed to efficiently score responses, demonstrating strong performance in identifying low-quality samples compared to existing benchmarks.'}, 'zh': {'title': 'æ„å»ºé«˜è´¨é‡ä¸­æ–‡åå¥½æ•°æ®é›†çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ— äººå·¥å¹²é¢„çš„ä¸­æ–‡åå¥½æ•°æ®é›†æ³¨é‡Šç®¡é“ï¼Œä»¥è§£å†³ç°æœ‰ä¸­æ–‡åå¥½æ•°æ®é›†è§„æ¨¡å°ã€é¢†åŸŸç‹­çª„å’Œç¼ºä¹ä¸¥æ ¼éªŒè¯çš„é—®é¢˜ã€‚æˆ‘ä»¬æ”¶é›†å¹¶ç­›é€‰äº†92,000ä¸ªé«˜è´¨é‡ä¸­æ–‡æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨15ä¸ªä¸»æµå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå’Œè¯„åˆ†é€‰æ‹©-æ‹’ç»çš„å“åº”å¯¹ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†COIG-Pæ•°æ®é›†ï¼ŒåŒ…å«1,009,000ä¸ªä¸­æ–‡åå¥½å¯¹ï¼Œè¦†ç›–èŠå¤©ã€ä»£ç ã€æ•°å­¦ã€é€»è¾‘ã€å°è¯´å’Œè§’è‰²ç­‰å…­ä¸ªå¤šæ ·åŒ–é¢†åŸŸã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ª8Bè§„æ¨¡çš„ä¸­æ–‡å¥–åŠ±æ¨¡å‹ï¼ˆCRMï¼‰ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸­æ–‡å¥–åŠ±åŸºå‡†ï¼ˆCRBenchï¼‰ï¼Œå¹¶åœ¨è¯„ä¼°ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02160', 'title': 'Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation', 'url': 'https://huggingface.co/papers/2504.02160', 'abstract': 'Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.', 'score': 27, 'issue_id': 3139, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '511e3ea71050e14e', 'authors': ['Shaojin Wu', 'Mengqi Huang', 'Wenxu Wu', 'Yufeng Cheng', 'Fei Ding', 'Qian He'], 'affiliations': ['Intelligent Creation Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2504.02160.jpg', 'data': {'categories': ['#dataset', '#data', '#synthetic', '#multimodal', '#diffusion', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UNO Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Multi-Subject Image Generation with Consistent Data Synthesis', 'desc': 'This paper addresses the challenges of generating images with multiple subjects by proposing a new data synthesis pipeline. The authors utilize diffusion transformers to create high-consistency paired data for both single and multi-subject scenarios. They introduce a novel model called UNO, which incorporates cross-modal alignment and rotary position embedding to enhance the generation process. Experimental results demonstrate that their approach maintains high consistency and controllability in image generation tasks.'}, 'zh': {'title': 'é«˜ä¸€è‡´æ€§å¤šä¸»é¢˜ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å›¾åƒç”Ÿæˆä¸­ï¼Œå¦‚ä½•è§£å†³ä»¥ä¸»é¢˜ä¸ºé©±åŠ¨çš„ç”Ÿæˆé¢ä¸´çš„æ•°æ®å¯æ‰©å±•æ€§å’Œä¸»é¢˜æ‰©å±•æ€§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜ä¸€è‡´æ€§çš„æ•°æ®åˆæˆç®¡é“ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨çš„å†…åœ¨ç”Ÿæˆèƒ½åŠ›ï¼Œç”Ÿæˆé«˜ä¸€è‡´æ€§çš„å¤šä¸»é¢˜é…å¯¹æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†UNOæ¨¡å‹ï¼Œç»“åˆäº†æ¸è¿›çš„è·¨æ¨¡æ€å¯¹é½å’Œé€šç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œèƒ½å¤Ÿåœ¨å•ä¸»é¢˜å’Œå¤šä¸»é¢˜ç”Ÿæˆä¸­ä¿æŒé«˜ä¸€è‡´æ€§å’Œå¯æ§æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œæ§åˆ¶èƒ½åŠ›ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02810', 'title': 'Generative Evaluation of Complex Reasoning in Large Language Models', 'url': 'https://huggingface.co/papers/2504.02810', 'abstract': "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.", 'score': 10, 'issue_id': 3138, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '5a9b5f817a1c09b6', 'authors': ['Haowei Lin', 'Xiangyu Wang', 'Ruilin Yan', 'Baizhou Huang', 'Haotian Ye', 'Jianhua Zhu', 'Zihao Wang', 'James Zou', 'Jianzhu Ma', 'Yitao Liang'], 'affiliations': ['Computer Science Department, Stanford University, California, United States', 'Department of Electronic Engineering, Tsinghua University, Beijing, China', 'Institute for AI Industry Research, Tsinghua University, Beijing, China', 'Institute for Artificial Intelligence, Peking University, Beijing, China', 'Wangxuan institute of computer technology, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.02810.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'KUMO: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ KUMO - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. KUMO Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ LLM Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ, Ğ° Ğ½Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 23 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° 5000 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¸ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'KUMO: Unveiling True Reasoning in LLMs', 'desc': "This paper introduces KUMO, a new framework for evaluating the reasoning abilities of large language models (LLMs). KUMO generates diverse reasoning tasks that require models to demonstrate true understanding rather than simple recall from their training data. By combining LLMs with symbolic engines, it creates adjustable tasks that challenge models across various domains. The evaluation shows that many LLMs can outperform university students on easier tasks and achieve comparable performance on more complex reasoning challenges, highlighting KUMO's effectiveness as a reliable assessment tool."}, 'zh': {'title': 'KUMOï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°å·¥å…·', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦çœŸæ­£å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ˜¯ä»…ä»…ä»å…¶åºå¤§çš„è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†KUMOï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMsæ¨ç†èƒ½åŠ›çš„ç”Ÿæˆè¯„ä¼°æ¡†æ¶ã€‚KUMOç»“åˆäº†LLMså’Œç¬¦å·å¼•æ“ï¼ŒåŠ¨æ€ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†ä»»åŠ¡ï¼Œä¿ƒè¿›æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹23ä¸ªæœ€å…ˆè¿›çš„LLMsè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè®¸å¤šæ¨¡å‹åœ¨ç®€å•æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¤§å­¦ç”Ÿçš„è¡¨ç°ï¼Œè€Œåœ¨å¤æ‚æ¨ç†æŒ‘æˆ˜ä¸­ä¹Ÿè¾¾åˆ°äº†å¤§å­¦æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06148', 'title': 'V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric\n  Capabilities in Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2504.06148', 'abstract': 'Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.', 'score': 9, 'issue_id': 3144, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': '7d7e4a2197e26482', 'authors': ['Xiangxi Zheng', 'Linjie Li', 'Zhengyuan Yang', 'Ping Yu', 'Alex Jinpeng Wang', 'Rui Yan', 'Yuan Yao', 'Lijuan Wang'], 'affiliations': ['Central South University, China', 'Microsoft Corporation, USA', 'Nanjing University, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.06148.jpg', 'data': {'categories': ['#benchmark', '#games', '#multimodal', '#reasoning', '#agents'], 'emoji': 'ğŸ®', 'ru': {'title': 'V-MAGE: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ V-MAGE. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 30 ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸, ĞºĞ°Ğº Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ñ‚Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… MLLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Visual Reasoning in MLLMs with V-MAGE', 'desc': "This paper presents a new evaluation framework called Visual-centric Multiple Abilities Game Evaluation (V-MAGE) for assessing the visual reasoning skills of Multimodal Large Language Models (MLLMs). Unlike traditional benchmarks, V-MAGE includes dynamic, game-based tasks that require models to demonstrate core visual skills and higher-level reasoning abilities. The framework consists of five games with over 30 levels, focusing on tasks like positioning and visual memory. The evaluation reveals significant performance gaps between MLLMs and humans, highlighting the models' perceptual errors and suggesting areas for improvement in their reasoning strategies."}, 'zh': {'title': 'è§†è§‰æ¨ç†èƒ½åŠ›çš„æ–°è¯„ä¼°æ¡†æ¶', 'desc': 'æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œéšç€è¯„ä¼°ä»é™æ€æ•°æ®é›†è½¬å‘å¼€æ”¾ä¸–ç•Œçš„åŠ¨æ€ç¯å¢ƒï¼Œç°æœ‰çš„åŸºäºæ¸¸æˆçš„åŸºå‡†æµ‹è¯•æ˜¾å¾—ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹è§†è§‰ä¸­å¿ƒçš„ä»»åŠ¡ï¼Œæ— æ³•è¯„ä¼°çœŸå®ä¸–ç•Œå†³ç­–æ‰€éœ€çš„å¤šæ ·æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰ä¸­å¿ƒå¤šèƒ½åŠ›æ¸¸æˆè¯„ä¼°ï¼ˆV-MAGEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMsè§†è§‰æ¨ç†èƒ½åŠ›çš„æ¸¸æˆè¯„ä¼°æ¡†æ¶ã€‚V-MAGEåŒ…å«äº”ä¸ªå¤šæ ·åŒ–çš„æ¸¸æˆå’Œ30å¤šä¸ªæ‰‹å·¥è®¾è®¡çš„å…³å¡ï¼Œæµ‹è¯•æ¨¡å‹åœ¨å®šä½ã€è½¨è¿¹è·Ÿè¸ªã€æ—¶æœºæŠŠæ¡å’Œè§†è§‰è®°å¿†ç­‰æ ¸å¿ƒè§†è§‰æŠ€èƒ½ä¸Šçš„è¡¨ç°ï¼Œä»¥åŠé•¿æœŸè§„åˆ’å’Œæ·±æ€ç†Ÿè™‘ç­‰æ›´é«˜å±‚æ¬¡çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05594', 'title': 'Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model', 'url': 'https://huggingface.co/papers/2504.05594', 'abstract': 'Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.', 'score': 9, 'issue_id': 3138, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': '7da2f86ad5e0bfc2', 'authors': ['Qi Mao', 'Lan Chen', 'Yuchao Gu', 'Mike Zheng Shou', 'Ming-Hsuan Yang'], 'affiliations': ['Show Lab, National University of Singapore', 'State Key Laboratory of Media Convergence and Communication, Communication University of China', 'University of California at Merced', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05594.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#multimodal'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UnifyEdit - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Achieving Perfect Balance in Text-Based Image Editing with UnifyEdit', 'desc': 'This paper presents UnifyEdit, a novel method for text-based image editing that aims to balance fidelity and editability. Traditional approaches often struggle with over- or under-editing due to conflicting constraints in attention mechanisms. UnifyEdit introduces a unified framework that employs self-attention and cross-attention constraints to maintain structural fidelity and enhance text alignment, respectively. An adaptive time-step scheduler is also proposed to dynamically manage the influence of these constraints, ensuring optimal performance in various editing tasks.'}, 'zh': {'title': 'å¹³è¡¡ä¿çœŸåº¦ä¸å¯ç¼–è¾‘æ€§çš„åˆ›æ–°æ–¹æ³•', 'desc': 'åœ¨åŸºäºæ–‡æœ¬çš„å›¾åƒç¼–è¾‘ä¸­ï¼Œå¹³è¡¡ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§éå¸¸é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ³¨æ„åŠ›æœºåˆ¶æ¥ä¿æŒç»“æ„ï¼Œä½†ç¼ºä¹ç»Ÿä¸€çš„æœºåˆ¶æ¥å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºäº†UnifyEditï¼Œè¿™æ˜¯ä¸€ç§æ— è°ƒä¼˜çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£æ½œåœ¨ä¼˜åŒ–å®ç°ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§çš„å¹³è¡¡ã€‚æˆ‘ä»¬å¼€å‘äº†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›çº¦æŸï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”æ—¶é—´æ­¥è°ƒåº¦å™¨ï¼Œä»¥åŠ¨æ€è°ƒæ•´è¿™äº›çº¦æŸçš„å½±å“ï¼Œä»è€Œä¼˜åŒ–ç¼–è¾‘æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00043', 'title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation', 'url': 'https://huggingface.co/papers/2504.00043', 'abstract': 'Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.', 'score': 7, 'issue_id': 3137, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': '2b2bfdd590c5394d', 'authors': ['Jixuan Leng', 'Chengsong Huang', 'Langlin Huang', 'Bill Yuchen Lin', 'William W. Cohen', 'Haohan Wang', 'Jiaxin Huang'], 'affiliations': ['CMU', 'UIUC', 'UW', 'WUSTL'], 'pdf_title_img': 'assets/pdf/title_img/2504.00043.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞšÑ€Ğ¾ÑÑĞ²Ğ¾Ñ€Ğ´Ñ‹ ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'CrossWordBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€Ğ¾ÑÑĞ²Ğ¾Ñ€Ğ´Ñ‹ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¿ĞµÑ€ĞµÑĞµĞºĞ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ ÑĞµÑ‚ĞºĞ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ LLM Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ñ‚Ğ°ĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ LVLM Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ğ° ÑĞµÑ‚ĞºĞ¸.'}, 'en': {'title': 'CrossWordBench: Evaluating Reasoning in LLMs and LVLMs with Crossword Puzzles', 'desc': 'This paper presents CrossWordBench, a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) using crossword puzzles. The benchmark focuses on the interaction between text-based clues and visual grid structures, requiring models to adhere to both semantic and intersectional constraints. The study shows that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints, while LVLMs face challenges linked to their grid-parsing accuracy. Overall, the findings highlight the limitations of current models in reasoning tasks and suggest a novel approach for multimodal evaluation.'}, 'zh': {'title': 'è·¨æ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†ï¼šCrossWordBench', 'desc': 'ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ¨ç†è¯„ä¼°æ¡†æ¶ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æ¨ç†æˆ–è§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›ä¸Šï¼Œç¼ºä¹æ–‡æœ¬ä¸è§†è§‰ä¹‹é—´çš„åŠ¨æ€äº’åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CrossWordBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¡«å­—æ¸¸æˆè¯„ä¼°LLMså’ŒLVLMsæ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œè¦æ±‚åœ¨æ–‡æœ¬çº¿ç´¢å’Œè§†è§‰ç½‘æ ¼ç»“æ„çš„è¯­ä¹‰çº¦æŸä¸‹è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚CrossWordBenchåˆ©ç”¨å¯æ§çš„æ‹¼å›¾ç”Ÿæˆæ¡†æ¶ï¼Œç”Ÿæˆå¤šç§æ ¼å¼çš„æ‹¼å›¾ï¼Œå¹¶æä¾›ä»ç›´æ¥è§£è°œåˆ°äº’åŠ¨æ¨¡å¼çš„ä¸åŒè¯„ä¼°ç­–ç•¥ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„LLMsåœ¨åˆ©ç”¨äº¤å‰å­—æ¯çº¦æŸæ–¹é¢æ˜¾è‘—ä¼˜äºéæ¨ç†æ¨¡å‹ï¼Œè€ŒLVLMsåœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå…¶è§£è°œè¡¨ç°ä¸ç½‘æ ¼è§£æå‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20533', 'title': 'Accelerate Parallelizable Reasoning via Parallel Decoding within One\n  Sequence', 'url': 'https://huggingface.co/papers/2503.20533', 'abstract': 'Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.', 'score': 7, 'issue_id': 3144, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '3b237389882b1344', 'authors': ['Yijiong Yu'], 'affiliations': ['OpenCSG', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20533.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#math'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° ÑˆĞ°Ğ³ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ²Ğ´Ğ²Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Accelerating Reasoning with Parallel Token Decoding', 'desc': 'This paper presents a novel approach to enhance the efficiency of reasoning models in machine learning, particularly for complex tasks like mathematical reasoning. The authors focus on reducing the computational cost associated with generating lengthy reasoning sequences by utilizing parallel processing techniques. By implementing a specialized attention mask, they enable the model to decode multiple tokens simultaneously, which significantly speeds up the reasoning process without increasing memory usage. Experimental results indicate that this method can achieve more than 100% improvement in decoding speed while preserving the quality of the answers.'}, 'zh': {'title': 'åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œæå‡æ•ˆç‡ä¸è´¨é‡', 'desc': 'æœ€è¿‘çš„æ¨ç†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦æ¨ç†ï¼‰ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ï¼Œä¸»è¦å¾—ç›Šäºè¯¦ç»†å’Œå…¨é¢çš„æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿™äº›å†—é•¿çš„æ¨ç†åºåˆ—åœ¨è®¡ç®—ä¸Šæ˜¯æ˜‚è´µä¸”è€—æ—¶çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æŸäº›ä»»åŠ¡çš„å›ºæœ‰å¹¶è¡Œæ€§æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå½“å­˜åœ¨å¤šä¸ªå¹¶è¡Œæ¨ç†åˆ†æ”¯æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸“é—¨çš„æ³¨æ„åŠ›æ©ç åœ¨æ¯ä¸€æ­¥è§£ç å¤šä¸ªæ ‡è®°ï¼Œä»è€Œåœ¨å•ä¸ªåºåˆ—ä¸­å¤„ç†å®ƒä»¬ï¼Œé¿å…äº†é¢å¤–çš„å†…å­˜ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06232', 'title': 'HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned\n  Guidance', 'url': 'https://huggingface.co/papers/2504.06232', 'abstract': "Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's superiority in achieving superior high-resolution image quality over current state-of-the-art methods.", 'score': 6, 'issue_id': 3145, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': '7def3ddddf039685', 'authors': ['Jiazi Bu', 'Pengyang Ling', 'Yujie Zhou', 'Pan Zhang', 'Tong Wu', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.06232.jpg', 'data': {'categories': ['#diffusion', '#training', '#optimization', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'HiFlow: Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'HiFlow - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ capture Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. HiFlow Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ HiFlow Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking High-Resolution Image Synthesis with HiFlow', 'desc': 'This paper introduces HiFlow, a novel framework designed to enhance the capabilities of text-to-image (T2I) diffusion and flow models for generating high-resolution images. HiFlow operates without the need for additional training and is compatible with existing pre-trained flow models. It utilizes a virtual reference flow to align low-resolution and high-resolution data, ensuring consistency in low-frequency details, structural integrity, and fine details. The results show that HiFlow significantly improves the quality of high-resolution images compared to current leading methods, demonstrating its effectiveness and adaptability across various T2I models.'}, 'zh': {'title': 'HiFlowï¼šæå‡é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHiFlowçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­çš„æŒ‘æˆ˜ã€‚HiFlowä¸éœ€è¦è®­ç»ƒï¼Œå¹¶ä¸”ä¸æ¨¡å‹æ— å…³ï¼Œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒçš„æµæ¨¡å‹ã€‚å®ƒé€šè¿‡å»ºç«‹è™šæ‹Ÿå‚è€ƒæµï¼Œæ•æ‰ä½åˆ†è¾¨ç‡æµä¿¡æ¯çš„ç‰¹å¾ï¼Œä»è€ŒæŒ‡å¯¼é«˜åˆ†è¾¨ç‡ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiFlowåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒè´¨é‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05897', 'title': 'HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference', 'url': 'https://huggingface.co/papers/2504.05897', 'abstract': 'The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33times in the prefill stage and 1.70times in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE.', 'score': 6, 'issue_id': 3151, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': 'a4c4d6d7e202a236', 'authors': ['Shuzhang Zhong', 'Yanfan Sun', 'Ling Liang', 'Runsheng Wang', 'Ru Huang', 'Meng Li'], 'affiliations': ['Beijing Advanced Innovation Center for Integrated Circuits, Beijing, China', 'Institute for Artificial Intelligence, Peking University, Beijing, China', 'Institute of Electronic Design Automation, Peking University, Wuxi, China', 'School of Computer Science and Engineering, Beihang University, Beijing, China', 'School of Integrated Circuits, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.05897.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#inference', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° CPU Ğ¸ GPU', 'desc': 'HybriMoE - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture of Experts (MoE) Ğ½Ğ° CPU Ğ¸ GPU. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ CPU Ğ¸ GPU. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑĞ»Ğ¾ĞµĞ², Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Optimizing MoE Inference with HybriMoE: Speed and Efficiency Unleashed!', 'desc': 'The paper presents HybriMoE, a hybrid CPU-GPU inference framework designed to optimize the performance of Mixture of Experts (MoE) models. It addresses the challenges of unstable expert activation patterns and complex scheduling due to varying expert sizes and workloads. HybriMoE introduces innovative strategies such as dynamic intra-layer scheduling, impact-driven inter-layer prefetching, and score-based caching to enhance resource utilization. Experimental results show that HybriMoE significantly speeds up the inference process, achieving notable improvements over existing frameworks.'}, 'zh': {'title': 'HybriMoEï¼šæå‡æ··åˆæ¨ç†æ•ˆç‡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„åœ¨æé«˜æ¨¡å‹å®¹é‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†å…¶å¤§è§„æ¨¡æ¨¡å‹ä»ç„¶å¯¹å†…å­˜æå‡ºäº†é«˜è¦æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†HybriMoEï¼Œä¸€ä¸ªæ··åˆCPU-GPUæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æ–°é¢–çš„è°ƒåº¦å’Œç¼“å­˜ç®¡ç†ç³»ç»Ÿæ¥æé«˜èµ„æºåˆ©ç”¨ç‡ã€‚HybriMoEå¼•å…¥äº†åŠ¨æ€çš„å±‚å†…è°ƒåº¦ç­–ç•¥ã€å½±å“é©±åŠ¨çš„å±‚é—´é¢„å–ç®—æ³•å’ŒåŸºäºè¯„åˆ†çš„ç¼“å­˜ç®—æ³•ï¼Œä»¥å‡è½»ä¸“å®¶æ¿€æ´»çš„ä¸ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHybriMoEåœ¨é¢„å¡«å……é˜¶æ®µå’Œè§£ç é˜¶æ®µçš„é€Ÿåº¦åˆ†åˆ«æé«˜äº†1.33å€å’Œ1.70å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05520', 'title': 'Efficient Reinforcement Finetuning via Adaptive Curriculum Learning', 'url': 'https://huggingface.co/papers/2504.05520', 'abstract': "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.", 'score': 5, 'issue_id': 3151, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '474ad1b587ab6240', 'authors': ['Taiwei Shi', 'Yiyang Wu', 'Linxin Song', 'Tianyi Zhou', 'Jieyu Zhao'], 'affiliations': ['University of Maryland, College Park', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2504.05520.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl', '#math', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'AdaRFT: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ AdaRFT (ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. AdaRFT Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸, Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AdaRFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2 Ñ€Ğ°Ğ· Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Adaptive Learning for Efficient Reinforcement Finetuning', 'desc': "This paper presents AdaRFT, a new method that enhances Reinforcement Finetuning (RFT) for large language models (LLMs) by using adaptive curriculum learning. AdaRFT adjusts the difficulty of training tasks based on the model's performance, ensuring that it learns from problems that are appropriately challenging. This approach improves training efficiency by reducing unnecessary computations on tasks that are too easy or too hard. Experiments show that AdaRFT can cut training steps in half while significantly boosting the model's accuracy on complex math problems."}, 'zh': {'title': 'è‡ªé€‚åº”è¯¾ç¨‹å¼ºåŒ–å¾®è°ƒï¼šæå‡æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°äº†å·¨å¤§æ½œåŠ›ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡æ ·æœ¬å’Œè®¡ç®—èµ„æºã€‚æˆ‘ä»¬æå‡ºäº†AdaRFTï¼ˆè‡ªé€‚åº”è¯¾ç¨‹å¼ºåŒ–å¾®è°ƒï¼‰ï¼Œé€šè¿‡è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ æ˜¾è‘—æé«˜äº†RFTçš„æ•ˆç‡å’Œæœ€ç»ˆå‡†ç¡®æ€§ã€‚AdaRFTæ ¹æ®æ¨¡å‹æœ€è¿‘çš„å¥–åŠ±ä¿¡å·åŠ¨æ€è°ƒæ•´è®­ç»ƒé—®é¢˜çš„éš¾åº¦ï¼Œç¡®ä¿æ¨¡å‹å§‹ç»ˆåœ¨å…·æœ‰æŒ‘æˆ˜æ€§ä½†å¯è§£å†³çš„ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™ç§è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥åŠ é€Ÿäº†å­¦ä¹ ï¼Œé¿å…äº†åœ¨è¿‡äºç®€å•æˆ–è¿‡äºå›°éš¾çš„é—®é¢˜ä¸Šæµªè´¹è®¡ç®—èµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06122', 'title': 'Leanabell-Prover: Posttraining Scaling in Formal Reasoning', 'url': 'https://huggingface.co/papers/2504.06122', 'abstract': 'Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.', 'score': 3, 'issue_id': 3147, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': 'a180ec0989dfd0c8', 'authors': ['Jingyuan Zhang', 'Qi Wang', 'Xingguang Ji', 'Yahui Liu', 'Yang Yue', 'Fuzheng Zhang', 'Di Zhang', 'Guorui Zhou', 'Kun Gai'], 'affiliations': ['Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2504.06122.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#training', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼: Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ (ĞĞ”Ğ¢) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ¿Ğ°Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ-Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ‚Ğ¾Ñ€Ğ° Lean 4. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¸Ğ¼ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞĞ”Ğ¢, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ².'}, 'en': {'title': 'Revolutionizing Automated Theorem Proving with Human-like Reasoning', 'desc': 'This paper explores the enhancement of automated theorem proving (ATP) using large language models (LLMs) and Lean 4 code. The authors propose a continual training approach that combines a hybrid dataset of statement-proof pairs with cognitive behavior data to mimic human reasoning. They also implement reinforcement learning, utilizing feedback from the Lean 4 compiler to refine the models further. As a result, they report significant improvements in existing ATP systems, achieving a notable 59.8% pass rate on the MiniF2F benchmark.'}, 'zh': {'title': 'æå‡è‡ªåŠ¨å®šç†è¯æ˜çš„æ™ºèƒ½æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†é€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆATPï¼‰çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯ä¸Lean 4ä»£ç çš„å½¢å¼æ¨ç†ç›¸å…³çš„è¿›å±•ã€‚æˆ‘ä»¬é‡‡ç”¨æ··åˆæ•°æ®é›†å¯¹ç°æœ‰ATPæ¨¡å‹è¿›è¡ŒæŒç»­è®­ç»ƒï¼Œæ•°æ®é›†ä¸­åŒ…å«å¤§é‡çš„å‘½é¢˜-è¯æ˜å¯¹ï¼Œå¹¶åŠ å…¥æ¨¡æ‹Ÿäººç±»æ¨ç†å’Œå‡è®¾ä¿®æ­£çš„è®¤çŸ¥è¡Œä¸ºæ•°æ®ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ©ç”¨Lean 4ç¼–è¯‘å™¨è¿”å›çš„ç»“æœå¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬æˆåŠŸæå‡äº†ç°æœ‰çš„å½¢å¼è¯æ˜å™¨ï¼Œå¦‚DeepSeek-Prover-v1.5å’ŒGoedel-Proverï¼Œåœ¨æ•´ä½“è¯æ˜ç”Ÿæˆé¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03755', 'title': 'ProtoGCD: Unified and Unbiased Prototype Learning for Generalized\n  Category Discovery', 'url': 'https://huggingface.co/papers/2504.03755', 'abstract': 'Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https://github.com/mashijie1028/ProtoGCD.', 'score': 1, 'issue_id': 3151, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'bdbc423fafc55199', 'authors': ['Shijie Ma', 'Fei Zhu', 'Xu-Yao Zhang', 'Cheng-Lin Liu'], 'affiliations': ['Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation, Chinese Academy of Sciences, Hong Kong 999077, P.R. China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing 100190, P.R. China'], 'pdf_title_img': 'assets/pdf/title_img/2504.03755.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#data', '#benchmark', '#optimization', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ (GCD) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ProtoGCD, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ€Ñ‹Ğµ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ ĞµĞ´Ğ¸Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ€Ğ¼Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ProtoGCD Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑÑ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unified Learning for Discovering New Categories in Machine Learning', 'desc': 'This paper addresses the challenge of Generalized Category Discovery (GCD), where models must identify new categories using existing labeled data. The authors introduce ProtoGCD, a framework that uses joint prototypes to represent both old and new classes, allowing for a more balanced learning process. They implement a dual-level adaptive pseudo-labeling mechanism to reduce bias and improve representation learning. Additionally, ProtoGCD includes a method for estimating new class counts and detecting unseen outliers, demonstrating superior performance on various datasets.'}, 'zh': {'title': 'ç»Ÿä¸€å»ºæ¨¡ï¼Œå‘ç°æ–°ç±»åˆ«çš„åŠ›é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºProtoGCDï¼Œç”¨äºè§£å†³å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆGCDï¼‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆåŸå‹å­¦ä¹ æ¥åŒæ—¶å»ºæ¨¡æ—§ç±»åˆ«å’Œæ–°ç±»åˆ«ï¼Œå…‹æœäº†ä»¥å¾€æ–¹æ³•ä¸­å­˜åœ¨çš„åå·®å’Œä¸å¹³è¡¡é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†åŒå±‚è‡ªé€‚åº”ä¼ªæ ‡ç­¾æœºåˆ¶ï¼Œä»¥å‡å°‘ç¡®è®¤åå·®ï¼Œå¹¶é€šè¿‡æ­£åˆ™åŒ–é¡¹æ¥ä¼˜åŒ–è¡¨ç¤ºå­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProtoGCDåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05299', 'title': 'SmolVLM: Redefining small and efficient multimodal models', 'url': 'https://huggingface.co/papers/2504.05299', 'abstract': 'Large Vision-Language Models (VLMs) deliver exceptional performance but require significant computational resources, limiting their deployment on mobile and edge devices. Smaller VLMs typically mirror design choices of larger models, such as extensive image tokenization, leading to inefficient GPU memory usage and constrained practicality for on-device applications.   We introduce SmolVLM, a series of compact multimodal models specifically engineered for resource-efficient inference. We systematically explore architectural configurations, tokenization strategies, and data curation optimized for low computational overhead. Through this, we identify key design choices that yield substantial performance gains on image and video tasks with minimal memory footprints.   Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during inference and outperforms the 300-times larger Idefics-80B model, despite an 18-month development gap. Our largest model, at 2.2B parameters, rivals state-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend beyond static images, demonstrating robust video comprehension capabilities.   Our results emphasize that strategic architectural optimizations, aggressive yet efficient tokenization, and carefully curated training data significantly enhance multimodal performance, facilitating practical, energy-efficient deployments at significantly smaller scales.', 'score': 84, 'issue_id': 3120, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '325b8841a555743d', 'authors': ['AndrÃ©s Marafioti', 'Orr Zohar', 'Miquel FarrÃ©', 'Merve Noyan', 'Elie Bakouch', 'Pedro Cuenca', 'Cyril Zakka', 'Loubna Ben Allal', 'Anton Lozhkov', 'Nouamane Tazi', 'Vaibhav Srivastav', 'Joshua Lochner', 'Hugo Larcher', 'Mathieu Morlon', 'Lewis Tunstall', 'Leandro von Werra', 'Thomas Wolf'], 'affiliations': ['Hugging Face', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05299.jpg', 'data': {'categories': ['#data', '#multimodal', '#video', '#optimization', '#low_resource', '#architecture', '#inference', '#small_models'], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: SmolVLM revolutionizes ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ VLM', 'desc': 'SmolVLM - ÑÑ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ¡Ğ°Ğ¼Ğ°Ñ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, SmolVLM-256M, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ 1 Ğ“Ğ‘ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² 300 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Idefics-80B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ….'}, 'en': {'title': 'SmolVLM: Compact Models for Efficient Vision-Language Tasks', 'desc': 'This paper presents SmolVLM, a series of compact vision-language models designed to operate efficiently on mobile and edge devices. Unlike larger models that require extensive computational resources, SmolVLM employs optimized architectural configurations and tokenization strategies to minimize GPU memory usage. The smallest model, SmolVLM-256M, achieves superior performance on image and video tasks while using less than 1GB of GPU memory, outperforming much larger models. The findings highlight the importance of strategic design choices in enhancing multimodal capabilities while ensuring practical deployment in resource-constrained environments.'}, 'zh': {'title': 'SmolVLMï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¡¨ç°ä¼˜å¼‚ï¼Œä½†éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚è¾ƒå°çš„VLMé€šå¸¸æ¨¡ä»¿å¤§å‹æ¨¡å‹çš„è®¾è®¡é€‰æ‹©ï¼Œå¯¼è‡´GPUå†…å­˜ä½¿ç”¨æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†SmolVLMï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¸“ä¸ºèµ„æºé«˜æ•ˆæ¨ç†è€Œè®¾è®¡çš„ç´§å‡‘å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä¼˜åŒ–æ¶æ„é…ç½®ã€æ ‡è®°ç­–ç•¥å’Œæ•°æ®æ•´ç†ï¼Œå¯ä»¥åœ¨ä¿æŒè¾ƒå°å†…å­˜å ç”¨çš„åŒæ—¶æ˜¾è‘—æå‡å›¾åƒå’Œè§†é¢‘ä»»åŠ¡çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05298', 'title': 'One-Minute Video Generation with Test-Time Training', 'url': 'https://huggingface.co/papers/2504.05298', 'abstract': 'Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit', 'score': 54, 'issue_id': 3116, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': 'c8edb1a98923d77d', 'authors': ['Karan Dalal', 'Daniel Koceja', 'Gashon Hussein', 'Jiarui Xu', 'Yue Zhao', 'Youjin Song', 'Shihao Han', 'Ka Chun Cheung', 'Jan Kautz', 'Carlos Guestrin', 'Tatsunori Hashimoto', 'Sanmi Koyejo', 'Yejin Choi', 'Yu Sun', 'Xiaolong Wang'], 'affiliations': ['NVIDIA', 'Stanford University', 'UC Berkeley', 'UCSD', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2504.05298.jpg', 'data': {'categories': ['#training', '#video', '#story_generation', '#long_context', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'TTT ÑĞ»Ğ¾Ğ¸: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸', 'desc': "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾ĞµĞ² Test-Time Training (TTT) Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. TTT ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Mamba. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² 'Ğ¢Ğ¾Ğ¼ Ğ¸ Ğ”Ğ¶ĞµÑ€Ñ€Ğ¸', Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ TTT ÑĞ»Ğ¾ĞµĞ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑĞºĞ°Ğ´Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼. Ğ¥Ğ¾Ñ‚Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ, Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹, Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ Ğ¸Ğ·-Ğ·Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."}, 'en': {'title': 'Enhancing Video Generation with Test-Time Training Layers', 'desc': 'This paper addresses the challenge of generating one-minute videos from text using Transformers, which struggle with long contexts due to inefficient self-attention layers. The authors introduce Test-Time Training (TTT) layers, which enhance the expressiveness of hidden states by allowing them to be neural networks. By integrating TTT layers into a pre-trained Transformer, the model significantly improves video coherence and storytelling ability compared to existing methods like Mamba and Gated DeltaNet. The results show a notable increase in human evaluation scores, although the authors acknowledge the presence of artifacts and the need for further efficiency improvements.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆçš„è¡¨è¾¾èƒ½åŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨ç”Ÿæˆä¸€åˆ†é’Ÿè§†é¢‘æ—¶ï¼Œå˜æ¢å™¨æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯è‡ªæ³¨æ„åŠ›å±‚åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„ä½æ•ˆã€‚æˆ‘ä»¬æå‡ºäº†æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰å±‚ï¼Œè¿™äº›å±‚çš„éšè—çŠ¶æ€å¯ä»¥æ˜¯ç¥ç»ç½‘ç»œï¼Œä»è€Œæé«˜äº†è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡å°†TTTå±‚æ·»åŠ åˆ°é¢„è®­ç»ƒçš„å˜æ¢å™¨ä¸­ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»æ–‡æœ¬æ•…äº‹æ¿ç”Ÿæˆæ›´è¿è´¯çš„ä¸€åˆ†é’Ÿè§†é¢‘ã€‚å°½ç®¡ç»“æœæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨ä¸€äº›ä¼ªå½±ï¼Œè¡¨æ˜é¢„è®­ç»ƒçš„5Bæ¨¡å‹èƒ½åŠ›æœ‰é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.04022', 'title': 'Rethinking Reflection in Pre-Training', 'url': 'https://huggingface.co/papers/2504.04022', 'abstract': "A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks.", 'score': 47, 'issue_id': 3128, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 5', 'zh': '4æœˆ5æ—¥'}, 'hash': 'c3bb14b88112a3ea', 'authors': ['Essential AI', ':', 'Darsh J Shah', 'Peter Rushton', 'Somanshu Singla', 'Mohit Parmar', 'Kurt Smith', 'Yash Vanjani', 'Ashish Vaswani', 'Adarsh Chaluvaraju', 'Andrew Hojel', 'Andrew Ma', 'Anil Thomas', 'Anthony Polloreno', 'Ashish Tanwer', 'Burhan Drak Sibai', 'Divya S Mansingka', 'Divya Shivaprasad', 'Ishaan Shah', 'Karl Stratos', 'Khoi Nguyen', 'Michael Callahan', 'Michael Pust', 'Mrinal Iyer', 'Philip Monk', 'Platon Mazarakis', 'Ritvik Kapila', 'Saurabh Srivastava', 'Tim Romanski'], 'affiliations': ['DeepSeek-AI', 'Essential AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.04022.jpg', 'data': {'categories': ['#interpretability', '#training', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ ÑƒĞ¶Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞ»Ğ¸, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¹Ñ‚Ğ¸ Ğº Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ, Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ² Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ² ÑÑ‚Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ€Ğ°Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OLMo2-7B, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 4 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ.'}, 'en': {'title': 'Self-Reflection: A Key to Early Problem Solving in Language Models', 'desc': 'This paper explores how language models can reflect on their own reasoning, which helps them solve complex problems. The authors reveal that this self-reflective ability starts to develop during the pre-training phase, rather than only during reinforcement learning. They introduce intentional errors in reasoning tasks to see if the model can identify and correct these mistakes. Their findings show that the self-correcting capability improves as the model undergoes more pre-training, with the OLMo2-7B model demonstrating this ability effectively after being trained on a large dataset.'}, 'zh': {'title': 'è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘åæ€èƒ½åŠ›ï¼šæ—©æœŸæ˜¾ç°ä¸æŒç»­æå‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶è‡ªæˆ‘åæ€èƒ½åŠ›çš„é‡è¦æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§èƒ½åŠ›ä¸ä»…åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µå‘å±•ï¼Œå®é™…ä¸Šåœ¨æ¨¡å‹çš„é¢„è®­ç»ƒé˜¶æ®µå°±å¼€å§‹æ˜¾ç°ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æ€ç»´é“¾ä¸­å¼•å…¥æ•…æ„é”™è¯¯ï¼Œæµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿè¯†åˆ«å¹¶çº æ­£è¿™äº›é”™è¯¯ï¼Œä»è€Œå¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ç»“æœæ˜¾ç¤ºï¼ŒOLMo2-7Bæ¨¡å‹åœ¨é¢„è®­ç»ƒçš„æ—©æœŸé˜¶æ®µå°±å±•ç°å‡ºè‡ªæˆ‘çº æ­£èƒ½åŠ›ï¼Œå¹¶éšç€æ—¶é—´çš„æ¨ç§»ä¸æ–­æé«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05305', 'title': 'URECA: Unique Region Caption Anything', 'url': 'https://huggingface.co/papers/2504.05305', 'abstract': 'Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.', 'score': 26, 'issue_id': 3115, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '6eec948e6319fc99', 'authors': ['Sangbeom Lim', 'Junwan Kim', 'Heeji Yoon', 'Jaewoo Jung', 'Seungryong Kim'], 'affiliations': ['KAIST AI', 'Korea University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05305.jpg', 'data': {'categories': ['#data', '#games', '#cv', '#interpretability', '#dataset', '#benchmark', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… URECA Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ URECA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ URECA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹.'}, 'en': {'title': 'Enhancing Region-Level Captioning with URECA Dataset and Model', 'desc': 'This paper presents a new approach to region-level captioning, which generates detailed descriptions for specific parts of images. The authors introduce the URECA dataset, designed to improve the uniqueness of captions by including a variety of objects and backgrounds. They propose a novel captioning model, URECA, that uses advanced techniques like dynamic mask modeling to maintain spatial properties and enhance the quality of generated captions. The results demonstrate that URECA outperforms existing methods, providing more accurate and diverse descriptions across different image regions.'}, 'zh': {'title': 'å¤šç²’åº¦åŒºåŸŸæè¿°çš„æ–°çªç ´', 'desc': 'åŒºåŸŸçº§æè¿°æ—¨åœ¨ä¸ºç‰¹å®šå›¾åƒåŒºåŸŸç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼Œå¹¶çªå‡ºå…¶ç‹¬ç‰¹ç‰¹å¾ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤šç²’åº¦ç”Ÿæˆç‹¬ç‰¹æè¿°æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†URECAæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šç²’åº¦åŒºåŸŸæè¿°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç¡®ä¿åŒºåŸŸä¸æè¿°ä¹‹é—´çš„ç‹¬ç‰¹å’Œä¸€è‡´çš„æ˜ å°„ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†URECAæ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼–ç å¤šç²’åº¦åŒºåŸŸï¼Œç”Ÿæˆç»†è‡´ä¸”è¯­ä¹‰ä¸°å¯Œçš„æè¿°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.04718', 'title': 'T1: Tool-integrated Self-verification for Test-time Compute Scaling in\n  Small Language Models', 'url': 'https://huggingface.co/papers/2504.04718', 'abstract': 'Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs.', 'score': 26, 'issue_id': 3120, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '51f832049b5599a6', 'authors': ['Minki Kang', 'Jongwon Jeong', 'Jaewoong Cho'], 'affiliations': ['KAIST', 'KRAFTON'], 'pdf_title_img': 'assets/pdf/title_img/2504.04718.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#small_models', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ ÑƒĞ¼Ğ½ĞµĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (sLMs) Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ sLMs Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Tool-integrated self-verification (T1), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ»ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ T1 Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3.2 1B Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Llama-3.1 8B Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Empowering Small Models with Tool Integration for Better Self-Verification', 'desc': 'This paper explores how small language models (sLMs) can improve their performance through test-time compute scaling, particularly focusing on their ability to self-verify outputs. The authors find that sLMs face challenges in verification tasks that require memorization, such as numerical calculations and fact-checking, even when they learn from larger models. To overcome this, they introduce Tool-integrated self-verification (T1), which allows sLMs to use external tools for tasks that require heavy memorization. Their experiments show that T1 significantly enhances the performance of sLMs, enabling them to outperform larger models in various knowledge-intensive tasks.'}, 'zh': {'title': 'å·¥å…·é›†æˆæå‡å°å‹è¯­è¨€æ¨¡å‹è‡ªæˆ‘éªŒè¯èƒ½åŠ›', 'desc': 'æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæµ‹è¯•æ—¶è®¡ç®—æ‰©å±•å¯ä»¥æœ‰æ•ˆæé«˜å°å‹è¯­è¨€æ¨¡å‹ï¼ˆsLMsï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨äºä½¿ç”¨æ›´å¤§æ¨¡å‹ä½œä¸ºéªŒè¯è€…çš„æµ‹è¯•æ—¶è®¡ç®—æ‰©å±•ï¼Œè€Œå¯¹sLMsçš„è‡ªæˆ‘éªŒè¯ç ”ç©¶è¾ƒå°‘ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿é€šè¿‡çŸ¥è¯†è’¸é¦ä»æ›´å¤§çš„éªŒè¯è€…é‚£é‡Œè·å¾—çŸ¥è¯†ï¼ŒsLMsåœ¨éœ€è¦è®°å¿†çš„éªŒè¯ä»»åŠ¡ï¼ˆå¦‚æ•°å­—è®¡ç®—å’Œäº‹å®æ ¸æŸ¥ï¼‰ä¸­ä»ç„¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å·¥å…·é›†æˆè‡ªæˆ‘éªŒè¯ï¼ˆT1ï¼‰ï¼Œå°†é‡è®°å¿†çš„éªŒè¯æ­¥éª¤å§”æ‰˜ç»™å¤–éƒ¨å·¥å…·ï¼Œå¦‚ä»£ç è§£é‡Šå™¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02828', 'title': 'Concept Lancet: Image Editing with Compositional Representation\n  Transplant', 'url': 'https://huggingface.co/papers/2504.02828', 'abstract': 'Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.', 'score': 15, 'issue_id': 3117, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '1c289ffc8ceda51e', 'authors': ['Jinqi Luo', 'Tianjiao Ding', 'Kwan Ho Ryan Chan', 'Hancheng Min', 'Chris Callison-Burch', 'RenÃ© Vidal'], 'affiliations': ['University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2504.02828.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#cv', '#inference'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºĞ°Ğ»ÑŒĞ¿ĞµĞ»Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Concept Lancet (CoLan). CoLan Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ»Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ĞºĞ°Ğº Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CoLan-150K Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ.'}, 'en': {'title': 'Precision Editing with Concept Lancet', 'desc': 'This paper introduces Concept Lancet (CoLan), a novel framework for improving image editing using diffusion models. It addresses the challenge of determining the right strength of edits needed for different images, which can vary significantly. CoLan utilizes a sparse linear combination of visual concept representations to accurately assess and manipulate the presence of these concepts in images. The framework is supported by a comprehensive dataset, CoLan-150K, which enhances the editing process by providing diverse visual descriptions and scenarios.'}, 'zh': {'title': 'ç²¾å‡†ç¼–è¾‘ï¼Œæ¦‚å¿µç§»æ¤ï¼', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¢«å¹¿æ³›åº”ç”¨ã€‚ç°æœ‰çš„ç¼–è¾‘æ–¹æ³•é€šå¸¸é€šè¿‡åœ¨æ–‡æœ¬åµŒå…¥æˆ–è¯„åˆ†ç©ºé—´ä¸­è®¾è®¡ç¼–è¾‘æ–¹å‘æ¥æ“æ§è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•é¢ä¸´ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šè¿‡é«˜çš„ç¼–è¾‘å¼ºåº¦ä¼šæŸå®³è§†è§‰ä¸€è‡´æ€§ï¼Œè€Œè¿‡ä½çš„ç¼–è¾‘å¼ºåº¦åˆ™æ— æ³•å®Œæˆç¼–è¾‘ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Concept Lancetï¼ˆCoLanï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶-shotçš„å³æ’å³ç”¨æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ‰©æ•£åŸºç¡€çš„å›¾åƒç¼–è¾‘ä¸­è¿›è¡ŒåŸåˆ™æ€§çš„è¡¨ç¤ºæ“æ§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.04823', 'title': 'Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models', 'url': 'https://huggingface.co/papers/2504.04823', 'abstract': 'Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.', 'score': 14, 'issue_id': 3117, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '92bd3deed21195f2', 'authors': ['Ruikang Liu', 'Yuxuan Sun', 'Manyi Zhang', 'Haoli Bai', 'Xianzhi Yu', 'Tiezheng Yu', 'Chun Yuan', 'Lu Hou'], 'affiliations': ['Huawei Noahs Ark Lab', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.04823.jpg', 'data': {'categories': ['#open_source', '#optimization', '#inference', '#reasoning', '#math', '#training', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ 1,5B Ğ´Ğ¾ 70B. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ², KV-ĞºÑÑˆĞ° Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑˆĞ¸Ñ€Ğ¸Ğ½Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ±ĞµĞ·Ğ»Ğ¾ÑÑÑ‚Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ W8A8 Ğ¸Ğ»Ğ¸ W4A16, Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Optimizing Reasoning Models with Quantization', 'desc': 'This paper investigates the effects of quantization on reasoning language models, which are known for their complex task performance but high inference costs. The authors systematically evaluate various quantization techniques on models like DeepSeek-R1-Distilled Qwen and LLaMA, focusing on different parameter sizes and quantization methods. They find that while lossless quantization is possible with certain configurations, lower bit-widths can lead to significant accuracy drops. Additionally, the study highlights that model size, origin, and task difficulty are crucial factors influencing performance, and suggests that adjusting model sizes or reasoning steps can improve outcomes.'}, 'zh': {'title': 'é‡åŒ–æ¨ç†æ¨¡å‹çš„ç³»ç»Ÿç ”ç©¶', 'desc': 'æœ€è¿‘ï¼Œæ¨ç†è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é“¾å¼æ¨ç†è¿‡ç¨‹å¢åŠ äº†æ¨ç†å¼€é”€ã€‚è™½ç„¶é‡åŒ–æŠ€æœ¯å·²è¢«å¹¿æ³›åº”ç”¨äºé™ä½å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æˆæœ¬ï¼Œä½†å…¶å¯¹æ¨ç†æ¨¡å‹çš„å½±å“ä»æœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æˆ‘ä»¬é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†é‡åŒ–æ¨ç†æ¨¡å‹ï¼Œè¯„ä¼°äº†å¤šä¸ªå¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨ä¸åŒçš„ä½å®½ä¸‹è¿›è¡Œæƒé‡ã€KVç¼“å­˜å’Œæ¿€æ´»é‡åŒ–çš„å®éªŒã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å¯ä»¥å®ç°æ— æŸé‡åŒ–ï¼Œä½†è¾ƒä½çš„ä½å®½ä¼šæ˜¾è‘—å½±å“å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ¨¡å‹å¤§å°ã€æ¥æºå’Œä»»åŠ¡éš¾åº¦æ˜¯æ€§èƒ½çš„å…³é”®å› ç´ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05288', 'title': 'LiveVQA: Live Visual Knowledge Seeking', 'url': 'https://huggingface.co/papers/2504.05288', 'abstract': 'We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen-2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research.', 'score': 12, 'issue_id': 3117, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '8302679426ac3ec4', 'authors': ['Mingyang Fu', 'Yuyang Peng', 'Benlin Liu', 'Yao Wan', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.05288.jpg', 'data': {'categories': ['#cv', '#reasoning', '#survey', '#dataset', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'LiveVQA: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ', 'desc': 'LiveVQA - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² (VQA). ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 3602 Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ñ 6 Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ 14 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞÑ†ĞµĞ½ĞºĞ° 15 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ² Ñ†ĞµĞ»Ğ¾Ğ¼, Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Visual Question Answering with LiveVQA', 'desc': 'LiveVQA is a new dataset designed to enhance visual question answering (VQA) by providing up-to-date visual knowledge sourced from the Internet. It includes 3,602 questions that require reasoning over images and text, covering various news topics. Our tests on 15 advanced machine learning language models (MLLMs) show that models with better visual reasoning skills excel at answering complex questions. However, even the best models struggle with visual questions that need the latest information, indicating a need for further research in this area.'}, 'zh': {'title': 'æœ€æ–°è§†è§‰çŸ¥è¯†çš„é—®ç­”æŒ‘æˆ˜', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†LiveVQAï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨æ”¶é›†çš„æœ€æ–°è§†è§‰çŸ¥è¯†æ•°æ®é›†ï¼ŒåŒ…å«åˆæˆçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰é—®é¢˜ã€‚LiveVQAåŒ…å«æ¥è‡ª6ä¸ªæ–°é—»ç½‘ç«™çš„3,602ä¸ªå•è·³å’Œå¤šè·³è§†è§‰é—®é¢˜ï¼Œæ¶µç›–14ä¸ªæ–°é—»ç±»åˆ«ï¼Œå…·æœ‰é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬ä¸€è‡´æ€§å’ŒçœŸå®ä¿¡æ¯ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œ15ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oã€Gemma-3å’ŒQwen-2.5-VLç³»åˆ—ï¼‰ä¸­ï¼Œæ€§èƒ½æ›´å¼ºçš„æ¨¡å‹åœ¨æ•´ä½“è¡¨ç°ä¸Šæ›´å¥½ï¼Œå°¤å…¶åœ¨å¤æ‚çš„å¤šè·³é—®é¢˜ä¸Šï¼Œå…ˆè¿›çš„è§†è§‰æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚å°½ç®¡åœ¨æ–‡æœ¬é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ä½¿ç”¨æœç´¢å¼•æ“ç­‰å·¥å…·çš„æ¨¡å‹åœ¨å¤„ç†éœ€è¦æœ€æ–°è§†è§‰çŸ¥è¯†çš„è§†è§‰é—®é¢˜æ—¶ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œè¿™çªæ˜¾äº†æœªæ¥ç ”ç©¶çš„é‡è¦é¢†åŸŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05118', 'title': 'VAPO: Efficient and Reliable Reinforcement Learning for Advanced\n  Reasoning Tasks', 'url': 'https://huggingface.co/papers/2504.05118', 'abstract': 'We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of 60.4. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks.', 'score': 11, 'issue_id': 3123, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '33841bfe919ea1d5', 'authors': ['YuYue', 'Yufeng Yuan', 'Qiying Yu', 'Xiaochen Zuo', 'Ruofei Zhu', 'Wenyuan Xu', 'Jiaze Chen', 'Chengyi Wang', 'TianTian Fan', 'Zhengyin Du', 'Xiangpeng Wei', 'Gaohong Liu', 'Juncai Liu', 'Lingjun Liu', 'Haibin Lin', 'Zhiqi Lin', 'Bole Ma', 'Chi Zhang', 'Mofan Zhang', 'Wang Zhang', 'Hang Zhu', 'Ru Zhang', 'Xin Liu', 'Mingxuan Wang', 'Yonghui Wu', 'Lin Yan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.05118.jpg', 'data': {'categories': ['#long_context', '#rl', '#benchmark', '#optimization', '#reasoning', '#training', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VAPO: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'VAPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen 32B, VAPO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ² 60.4 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ AIME 2024. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 10 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 5000 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. VAPO Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ¼Ñ‹ÑĞ»ĞµĞ¹.'}, 'en': {'title': 'VAPO: Revolutionizing Reasoning with Value-Based Reinforcement Learning', 'desc': 'VAPO is a new framework designed for reasoning models that uses value-based reinforcement learning. It achieves a remarkable score of 60.4 on the AIME 2024 dataset, surpassing previous models by over 10 points. The training of VAPO is both stable and efficient, completing in just 5,000 steps without any crashes during multiple runs. This research addresses key issues in value-based methods, such as model bias and reward sparsity, providing solutions that improve long chain-of-thought reasoning.'}, 'zh': {'title': 'VAPOï¼šæå‡æ¨ç†æ¨¡å‹çš„ä»·å€¼åŸºç¡€æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVAPOçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºåŸºäºä»·å€¼çš„æ¨ç†æ¨¡å‹æä¾›æ”¯æŒã€‚VAPOåœ¨AIME 2024æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†60.4çš„æœ€æ–°æˆç»©ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„DeepSeek-R1-Zero-Qwen-32Bå’ŒDAPOæ¨¡å‹ã€‚è¯¥æ¡†æ¶çš„è®­ç»ƒè¿‡ç¨‹ç¨³å®šé«˜æ•ˆï¼Œä»…éœ€5000æ­¥å³å¯è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¤šæ¬¡ç‹¬ç«‹è¿è¡Œä¸­æ²¡æœ‰å‘ç”Ÿè®­ç»ƒå´©æºƒï¼Œæ˜¾ç¤ºå‡ºå…¶å¯é æ€§ã€‚VAPOé€šè¿‡ç³»ç»Ÿè®¾è®¡æœ‰æ•ˆè§£å†³äº†åŸºäºä»·å€¼çš„æ–¹æ³•é¢ä¸´çš„ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œæå‡äº†é•¿é“¾æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05304', 'title': 'Gaussian Mixture Flow Matching Models', 'url': 'https://huggingface.co/papers/2504.05304', 'abstract': 'Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an L_2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256times256.', 'score': 6, 'issue_id': 3115, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': 'b0223808c61a3545', 'authors': ['Hansheng Chen', 'Kai Zhang', 'Hao Tan', 'Zexiang Xu', 'Fujun Luan', 'Leonidas Guibas', 'Gordon Wetzstein', 'Sai Bi'], 'affiliations': ['Adobe Research, CA 95110, USA', 'Hillbot', 'Stanford University, CA 94305, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.05304.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#inference'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'GMFlow: Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¼ĞµÑÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gaussian mixture flow matching (GMFlow) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. GMFlow Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸ GM-SDE/ODE Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµÑÑ‹Ñ‰ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'GMFlow: Enhancing Image Generation with Dynamic Gaussian Mixtures', 'desc': 'This paper introduces a new model called Gaussian Mixture Flow Matching (GMFlow) that improves upon traditional diffusion models and flow matching models. Instead of just predicting a single Gaussian mean, GMFlow predicts parameters for a dynamic Gaussian mixture, allowing it to better capture complex distributions in the data. The model addresses issues like discretization error and color saturation in generated images by using a novel probabilistic guidance scheme. Experimental results show that GMFlow achieves higher image generation quality with fewer sampling steps compared to existing methods.'}, 'zh': {'title': 'é«˜æ–¯æ··åˆæµåŒ¹é…ï¼šæå‡å›¾åƒç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹é€šè¿‡é«˜æ–¯åˆ†å¸ƒæ¥è¿‘ä¼¼å»å™ªåˆ†å¸ƒå¹¶é¢„æµ‹å…¶å‡å€¼ï¼Œè€ŒæµåŒ¹é…æ¨¡å‹åˆ™å°†é«˜æ–¯å‡å€¼é‡æ–°å‚æ•°åŒ–ä¸ºæµé€Ÿã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å°‘æ­¥é‡‡æ ·æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸»è¦æ˜¯ç”±äºç¦»æ•£åŒ–è¯¯å·®ï¼Œå¹¶ä¸”åœ¨æ— åˆ†ç±»å™¨å¼•å¯¼ä¸‹å®¹æ˜“äº§ç”Ÿè¿‡é¥±å’Œçš„é¢œè‰²ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ–¯æ··åˆæµåŒ¹é…ï¼ˆGMFlowï¼‰æ¨¡å‹ï¼šGMFlowé¢„æµ‹åŠ¨æ€é«˜æ–¯æ··åˆå‚æ•°ï¼Œä»¥æ•æ‰å¤šæ¨¡æ€æµé€Ÿåˆ†å¸ƒï¼Œå¹¶é€šè¿‡KLæ•£åº¦æŸå¤±è¿›è¡Œå­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒGMFlowåœ¨ç”Ÿæˆè´¨é‡ä¸Šå§‹ç»ˆä¼˜äºæµåŒ¹é…åŸºçº¿ï¼Œåœ¨ImageNet 256x256ä¸Šä»…ç”¨6ä¸ªé‡‡æ ·æ­¥éª¤å°±è¾¾åˆ°äº†0.942çš„ç²¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.04715', 'title': 'Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs', 'url': 'https://huggingface.co/papers/2504.04715', 'abstract': 'The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit', 'score': 6, 'issue_id': 3117, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '28e9dfa4b4a0421a', 'authors': ['Will Cai', 'Tianneng Shi', 'Xuandong Zhao', 'Dawn Song'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.04715.jpg', 'data': {'categories': ['#security', '#inference', '#benchmark', '#ethics'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹: Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² API ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğº API Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ‚Ğ°Ğ¹Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑˆĞµĞ²Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº. Ğ’ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (TEE) Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Ensuring Trust in Large Language Models: Detecting Substitutions in Black-Box APIs', 'desc': 'This paper addresses the issue of trust in Large Language Models (LLMs) accessed through APIs, where users may unknowingly receive lower-quality models instead of the advertised ones. It formalizes the challenge of detecting these model substitutions, which is complicated by the black-box nature of LLMs that limits user interactions to simple input-output queries. The authors evaluate various existing verification techniques, revealing their limitations, particularly against sophisticated attacks that can evade detection. They propose hardware-based solutions like Trusted Execution Environments (TEEs) as a potential way to ensure model integrity, while also considering the trade-offs involved.'}, 'zh': {'title': 'ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹çš„é€æ˜æ€§ä¸ä¿¡ä»»', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é»‘ç®±APIä¸­ä½¿ç”¨æ‰€å¸¦æ¥çš„ä¿¡ä»»æŒ‘æˆ˜ã€‚ç”¨æˆ·æ”¯ä»˜æœåŠ¡è´¹ç”¨æ—¶ï¼Œä¾èµ–äºæ¨¡å‹çš„èƒ½åŠ›ï¼ˆå¦‚è§„æ¨¡å’Œæ€§èƒ½ï¼‰ï¼Œä½†æä¾›è€…å¯èƒ½ä¼šå·å·ç”¨æ›´ä¾¿å®œã€è´¨é‡æ›´ä½çš„æ›¿ä»£æ¨¡å‹æ¥é™ä½æˆæœ¬ã€‚è¿™ç§ç¼ºä¹é€æ˜åº¦çš„é—®é¢˜å½±å“äº†å…¬å¹³æ€§å’Œä¿¡ä»»åº¦ï¼Œå¹¶ä½¿å¾—å¯é çš„åŸºå‡†æµ‹è¯•å˜å¾—å¤æ‚ã€‚æˆ‘ä»¬ç³»ç»Ÿè¯„ä¼°äº†ç°æœ‰çš„éªŒè¯æŠ€æœ¯ï¼Œå¹¶æå‡ºäº†åŸºäºç¡¬ä»¶çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥æé«˜æ¨¡å‹çš„å®Œæ•´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03151', 'title': 'Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning\n  (v1)', 'url': 'https://huggingface.co/papers/2504.03151', 'abstract': 'Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.', 'score': 5, 'issue_id': 3130, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': 'c209fd185b1e9776', 'authors': ['Jing Bi', 'Susan Liang', 'Xiaofei Zhou', 'Pinxin Liu', 'Junjia Guo', 'Yunlong Tang', 'Luchuan Song', 'Chao Huang', 'Guangyu Sun', 'Jinxi He', 'Jiarui Wu', 'Shu Yang', 'Daoan Zhang', 'Chen Chen', 'Lianggong Bruce Wen', 'Zhang Liu', 'Jiebo Luo', 'Chenliang Xu'], 'affiliations': ['Corning Inc.', 'University of Central Florida', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.03151.jpg', 'data': {'categories': ['#training', '#optimization', '#inference', '#reasoning', '#interpretability', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ LLM', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¸ Ğ¸Ñ… Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Reasoning in Multimodal Large Language Models', 'desc': "This paper discusses the importance of reasoning in human intelligence and how it relates to large language models (LLMs). It highlights the advancements in LLMs' reasoning capabilities, particularly in arithmetic and commonsense tasks, but points out the challenges in applying these abilities to multimodal contexts that involve both visual and textual data. The authors address the complexities of multimodal reasoning, such as managing conflicting information, and propose methods for improving model performance through post-training optimization. Overall, the paper serves as a comprehensive overview of reasoning techniques in LLMs and outlines future research directions to enhance multimodal reasoning."}, 'zh': {'title': 'æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€æ‰©å±•æŒ‘æˆ˜', 'desc': 'æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å„ç§ä»»åŠ¡ä¸­è¿›è¡Œç»“æ„åŒ–çš„é—®é¢˜è§£å†³ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å®ƒä»¬åœ¨ç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›èƒ½åŠ›æœ‰æ•ˆæ‰©å±•åˆ°å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ¨¡å‹éœ€è¦æ•´åˆè§†è§‰å’Œæ–‡æœ¬è¾“å…¥ã€‚æœ¬æ–‡æä¾›äº†å¯¹æ–‡æœ¬å’Œå¤šæ¨¡æ€LLMsä¸­æ¨ç†æŠ€æœ¯çš„ç®€æ˜æ¦‚è¿°ï¼Œæ˜ç¡®äº†æ ¸å¿ƒæ¨ç†æŒ‘æˆ˜å’Œæœºä¼šï¼Œå¹¶å¼ºè°ƒäº†åè®­ç»ƒä¼˜åŒ–å’Œæµ‹è¯•æ—¶æ¨ç†çš„å®ç”¨æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02882', 'title': 'DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models', 'url': 'https://huggingface.co/papers/2504.02882', 'abstract': "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling.", 'score': 4, 'issue_id': 3120, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'c42de57890092432', 'authors': ['Sunghee Jung', 'Donghun Lee', 'Shinbok Lee', 'Gaeun Seo', 'Daniel Lee', 'Byeongil Ko', 'Junrae Cho', 'Kihyun Kim', 'Eunggyun Kim', 'Myeongcheol Shin'], 'affiliations': ['Kakao Corp. Seongnam-si, Gyeonggi-do, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2504.02882.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#alignment'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'DiaTool-DPO: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ±ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'DiaTool-DPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾-Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (TA-LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ TA-LLM ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ 5 ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸ 3 Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ¼. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ DiaTool-DPO Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ GPT-4, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ, Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Dialogue with DiaTool-DPO for TA-LLMs', 'desc': 'This paper introduces DiaTool-DPO, a new method to improve Tool-Augmented Large Language Models (TA-LLMs) in handling incomplete and out-of-scope queries. It treats TA-LLM interactions as a Markov Decision Process, identifying five dialogue states and categorizing user queries into three types based on their transitions. The method involves creating paired datasets of correct and incorrect dialogue flows and using a specialized loss function for better dialogue control. The results show that DiaTool-DPO significantly enhances performance, approaching that of GPT-4o, while reducing the need for expert input and human labeling.'}, 'zh': {'title': 'æå‡å¯¹è¯èƒ½åŠ›çš„æ–°æ–¹æ³•ï¼šDiaTool-DPO', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•DiaTool-DPOï¼Œæ—¨åœ¨æå‡å·¥å…·å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆTA-LLMï¼‰çš„å¯¹è¯èƒ½åŠ›ã€‚æˆ‘ä»¬å°†TA-LLMçš„äº¤äº’å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶æ ¹æ®å¯¹è¯çŠ¶æ€çš„è½¬ç§»è½¨è¿¹å°†ç”¨æˆ·æŸ¥è¯¢åˆ†ä¸ºä¸‰ç±»ã€‚é€šè¿‡è‡ªåŠ¨æ„å»ºæ­£ç¡®å’Œé”™è¯¯å¯¹è¯æµçš„é…å¯¹è½¨è¿¹æ•°æ®é›†ï¼Œå¹¶å¼•å…¥ä¸“é—¨çš„ç›®æ ‡æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºDiaTool-DPOåœ¨ä¿¡æ¯è·å–å’Œå·¥å…·è°ƒç”¨æ‹’ç»æ–¹é¢çš„è¡¨ç°æ¥è¿‘GPT-4oã€‚è¯¥æ–¹æ³•ä¸ºå¼€å‘èƒ½å¤Ÿå¤„ç†å¤šæ ·åŒ–ç°å®åœºæ™¯çš„TA-LLMå¼€è¾Ÿäº†æ–°å¯èƒ½ï¼Œæ— éœ€é¢å¤–çš„ä¸“å®¶æ¼”ç¤ºæˆ–äººå·¥æ ‡æ³¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03964', 'title': 'Clinical ModernBERT: An efficient and long context encoder for\n  biomedical text', 'url': 'https://huggingface.co/papers/2504.03964', 'abstract': 'We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.', 'score': 3, 'issue_id': 3119, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '6602bd6699f6f653', 'authors': ['Simon A. Lee', 'Anthony Wu', 'Jeffrey N. Chiang'], 'affiliations': ['Department of Computational Medicine & Neurosurgery UCLA', 'Department of Computational Medicine UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2504.03964.jpg', 'data': {'categories': ['#science', '#long_context', '#benchmark', '#architecture', '#healthcare', '#dataset'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: Clinical ModernBERT', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Clinical ModernBERT - Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğµ, ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¼ĞµÑ‚ĞºĞ°Ñ… Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ ModernBERT Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ 8192 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Clinical ModernBERT ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… NLP-Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ².'}, 'en': {'title': 'Empowering Clinical NLP with Advanced Transformer Technology', 'desc': 'Clinical ModernBERT is a specialized transformer model designed for the biomedical and clinical fields. It is pretrained on a vast array of data, including biomedical literature and clinical notes, to enhance its understanding of medical language. The model incorporates advanced features like rotary positional embeddings and Flash Attention, allowing it to handle longer text inputs effectively. Its performance is validated through rigorous testing on various clinical natural language processing benchmarks, demonstrating its ability to generate meaningful representations for complex medical tasks.'}, 'zh': {'title': 'ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„å¼ºå¤§æ–‡æœ¬ç¼–ç å™¨', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Clinical ModernBERTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå˜æ¢å™¨çš„ç¼–ç å™¨ï¼Œç»è¿‡å¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ã€ä¸´åºŠç¬”è®°å’ŒåŒ»å­¦æœ¬ä½“çš„é¢„è®­ç»ƒã€‚è¯¥æ¨¡å‹ç»“åˆäº†PubMedæ‘˜è¦ã€MIMIC IVä¸´åºŠæ•°æ®å’ŒåŒ»å­¦ä»£ç åŠå…¶æ–‡æœ¬æè¿°ï¼Œé‡‡ç”¨äº†ç°ä»£BERTçš„æ¶æ„å‡çº§ï¼Œå¦‚æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰å’Œé—ªå­˜æ³¨æ„åŠ›ï¼ˆFlash Attentionï¼‰ã€‚Clinical ModernBERTåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºã€‚æˆ‘ä»¬é€šè¿‡åˆ†æå…¶é¢„è®­ç»ƒæƒé‡å’Œåœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†ä¸Šçš„å®è¯è¯„ä¼°æ¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03193', 'title': 'Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation', 'url': 'https://huggingface.co/papers/2504.03193', 'abstract': 'Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser.', 'score': 3, 'issue_id': 3120, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '6eca2c0b1acce1f3', 'authors': ['Xin Zhang', 'Robby T. Tan'], 'affiliations': ['ASUS Intelligent Cloud Services', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.03193.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#architecture', '#cv', '#benchmark', '#synthetic'], 'emoji': 'ğŸ”', 'ru': {'title': 'MFuser: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ VFM Ğ¸ VLM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² (DGSS), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ MFuser. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (VFM) Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Mamba. MFuser Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ MVFuser Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ MTEnhancer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MFuser Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ DGSS Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Harnessing the Power of Vision Models for Better Segmentation', 'desc': 'This paper introduces MFuser, a new framework that combines Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) for Domain Generalized Semantic Segmentation (DGSS). VFMs are good at capturing detailed visual features, while VLMs excel in aligning text with images, but they have limitations when used separately. MFuser uses a co-adapter and a hybrid attention mechanism to effectively merge these models, allowing for better feature extraction and text alignment without heavy computational costs. The results show that MFuser outperforms existing DGSS methods, achieving high accuracy on various benchmarks.'}, 'zh': {'title': 'èåˆè§†è§‰ä¸è¯­è¨€ï¼Œæå‡è¯­ä¹‰åˆ†å‰²èƒ½åŠ›', 'desc': 'è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨é¢†åŸŸæ³›åŒ–è¯­ä¹‰åˆ†å‰²ï¼ˆDGSSï¼‰ä¸­å› å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚ç°æœ‰çš„DGSSæ–¹æ³•é€šå¸¸åªä¾èµ–äºVFMæˆ–VLMï¼Œå¿½è§†äº†å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬æå‡ºäº†MFuserï¼Œä¸€ä¸ªæ–°é¢–çš„èåˆæ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆç»“åˆVFMå’ŒVLMçš„ä¼˜ç‚¹ï¼ŒåŒæ—¶ä¿æŒåºåˆ—é•¿åº¦çš„çº¿æ€§å¯æ‰©å±•æ€§ã€‚é€šè¿‡è”åˆå¾®è°ƒå’Œæ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼ŒMFuseråœ¨ç‰¹å¾å±€éƒ¨æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„DGSSæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02812', 'title': 'BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation', 'url': 'https://huggingface.co/papers/2504.02812', 'abstract': 'We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/', 'score': 3, 'issue_id': 3118, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '692278d7307c7950', 'authors': ['Van Nguyen Nguyen', 'Stephen Tyree', 'Andrew Guo', 'Mederic Fourmy', 'Anas Gouda', 'Taeyeop Lee', 'Sungphill Moon', 'Hyeontae Son', 'Lukas Ranftl', 'Jonathan Tremblay', 'Eric Brachmann', 'Bertram Drost', 'Vincent Lepetit', 'Carsten Rother', 'Stan Birchfield', 'Jiri Matas', 'Yann Labbe', 'Martin Sundermeyer', 'Tomas Hodan'], 'affiliations': ['CTU Prague', 'ENPC', 'Google', 'Heidelberg University', 'KAIST', 'MVTec', 'Meta', 'NAVER LABS', 'NVIDIA', 'Niantic', 'TU Dortmund', 'TU Munich', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2504.02812.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 6D Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ BOP Challenge 2024 Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ 6D Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… BOP-H3, Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞµĞ½ÑĞ¾Ñ€Ğ¾Ğ² Ğ¸ AR/VR Ğ³Ğ°Ñ€Ğ½Ğ¸Ñ‚ÑƒÑ€. Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ 2024 Ğ³Ğ¾Ğ´Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ 2023 Ğ³Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Advancing 6D Object Pose Estimation in Real-World Scenarios', 'desc': 'The BOP Challenge 2024 focuses on advancing 6D object pose estimation by transitioning from controlled lab environments to real-world applications. This year, new model-free tasks were introduced, requiring methods to learn from reference videos without 3D models. The challenge also featured a practical 6D object detection task where object identities were not provided, alongside the release of the BOP-H3 datasets that simulate real-world conditions. Notably, the top-performing methods demonstrated significant improvements in accuracy and speed compared to previous years, highlighting the ongoing evolution in this field of machine learning.'}, 'zh': {'title': 'BOPæŒ‘æˆ˜èµ›ï¼šä»å®éªŒå®¤åˆ°çœŸå®ä¸–ç•Œçš„6Dç‰©ä½“å§¿æ€ä¼°è®¡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†2024å¹´BOPæŒ‘æˆ˜èµ›çš„è¯„ä¼°æ–¹æ³•ã€æ•°æ®é›†å’Œç»“æœï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ•æ‰6Dç‰©ä½“å§¿æ€ä¼°è®¡æœ€æ–°æŠ€æœ¯çš„å…¬å¼€ç«èµ›ã€‚2024å¹´çš„ç›®æ ‡æ˜¯å°†BOPä»å®éªŒå®¤ç¯å¢ƒè½¬å‘çœŸå®ä¸–ç•Œåœºæ™¯ï¼Œæ¨å‡ºäº†æ–°çš„æ— æ¨¡å‹ä»»åŠ¡ï¼Œè¦æ±‚æ–¹æ³•ä»…é€šè¿‡å‚è€ƒè§†é¢‘è¿›è¡Œç‰©ä½“è¯†åˆ«ã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªæ›´å®ç”¨çš„6Dç‰©ä½“æ£€æµ‹ä»»åŠ¡ï¼Œæµ‹è¯•å›¾åƒä¸­ç‰©ä½“çš„èº«ä»½ä¸å†ä½œä¸ºè¾“å…¥æä¾›ã€‚æ­¤å¤–ï¼ŒBOP-H3æ•°æ®é›†ä½¿ç”¨é«˜åˆ†è¾¨ç‡ä¼ æ„Ÿå™¨å’ŒAR/VRå¤´æ˜¾å½•åˆ¶ï¼Œæ”¯æŒæ¨¡å‹åŸºç¡€å’Œæ— æ¨¡å‹ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03770', 'title': 'JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model', 'url': 'https://huggingface.co/papers/2504.03770', 'abstract': 'Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.', 'score': 2, 'issue_id': 3117, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '01c2f4c752f3e71d', 'authors': ['Yi Nian', 'Shenzhe Zhu', 'Yuehan Qin', 'Li Li', 'Ziyi Wang', 'Chaowei Xiao', 'Yue Zhao'], 'affiliations': ['University of Maryland', 'University of Southern California', 'University of Toronto', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.03770.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#security', '#dataset', '#benchmark'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° MLLM Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº 'jailbreak' Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼", 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ JAILDAM Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº Ñ‚Ğ¸Ğ¿Ğ° 'jailbreak' Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). JAILDAM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ²Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ½Ğ° Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ JAILDAM Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ."}, 'en': {'title': 'JAILDAM: Enhancing Safety in MLLMs Against Jailbreak Attacks', 'desc': 'This paper discusses the challenges of detecting jailbreak attacks in multimodal large language models (MLLMs), which can generate harmful content. Jailbreak attacks manipulate models to bypass safety features, making detection crucial for responsible use. The authors present JAILDAM, a novel framework that adapts during testing to identify these attacks without needing extensive harmful datasets. By using a memory-based approach, JAILDAM enhances detection efficiency and accuracy against various jailbreak strategies.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹å®‰å…¨æ€§çš„å…³é”®', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¹Ÿå­˜åœ¨ç”Ÿæˆæœ‰å®³å†…å®¹çš„é‡å¤§é£é™©ï¼Œå°¤å…¶æ˜¯é€šè¿‡è¶Šç‹±æ”»å‡»ã€‚è¶Šç‹±æ”»å‡»æ˜¯æŒ‡æ•…æ„æ“æ§ä»¥ç»•è¿‡æ¨¡å‹çš„å®‰å…¨æœºåˆ¶ï¼Œå¯¼è‡´ç”Ÿæˆä¸å½“æˆ–ä¸å®‰å…¨çš„å†…å®¹ã€‚æ£€æµ‹æ­¤ç±»æ”»å‡»å¯¹äºç¡®ä¿MLLMsçš„è´Ÿè´£ä»»éƒ¨ç½²è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºJAILDAMçš„æµ‹è¯•æ—¶è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æ›´æ–°ä¸å®‰å…¨çŸ¥è¯†æ¥æé«˜å¯¹æœªè§è¶Šç‹±ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03947', 'title': 'Distillation and Refinement of Reasoning in Small Language Models for\n  Document Re-ranking', 'url': 'https://huggingface.co/papers/2504.03947', 'abstract': 'We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems.', 'score': 1, 'issue_id': 3134, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '32b0bcdc9035f276', 'authors': ['Chris Samarinas', 'Hamed Zamani'], 'affiliations': ['University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2504.03947.jpg', 'data': {'categories': ['#interpretability', '#optimization', '#rl', '#benchmark', '#small_models', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ LLM Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BRIGHT, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 20 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ° Ğ½Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Smaller Models, Smarter Reasoning!', 'desc': 'This paper introduces a new method for training smaller language models to improve document ranking through a combination of knowledge distillation and reinforcement learning. Instead of relying on costly human annotations, the approach uses web data and a teacher language model to create high-quality training examples that include relevance explanations. By treating document ranking as a reinforcement learning task, the authors successfully train a compact 3B parameter model that performs exceptionally well on the BRIGHT benchmark, even outperforming much larger models. The findings suggest that generating explanations during the inference process enhances reasoning capabilities in smaller models, making the method both scalable and interpretable for information retrieval applications.'}, 'zh': {'title': 'å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹ä»¥è¿›è¡Œæ¨ç†å¯†é›†å‹æ–‡æ¡£æ’åï¼Œç»“åˆäº†çŸ¥è¯†è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚ä¸ç°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººç±»æ ‡æ³¨æˆ–å¤§å‹é»‘ç®±è¯­è¨€æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç½‘ç»œæ•°æ®å’Œæ•™å¸ˆè¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒç¤ºä¾‹åŠç›¸å…³è§£é‡Šã€‚é€šè¿‡å°†æ–‡æ¡£æ’åè§†ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå¹¶æ¿€åŠ±æ˜ç¡®çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç´§å‡‘çš„3Bå‚æ•°è¯­è¨€æ¨¡å‹ï¼Œåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸‰ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°æ•°é‡è¿œä½äºå…¶ä»–æ–¹æ³•ï¼Œè¶…è¶Šäº†é‚£äº›å‚æ•°è¶…è¿‡20å€çš„å¤§å‹æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.04155', 'title': 'GlotEval: A Test Suite for Massively Multilingual Evaluation of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2504.04155', 'abstract': "Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.", 'score': 0, 'issue_id': 3124, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 5', 'zh': '4æœˆ5æ—¥'}, 'hash': '8105e367a29eaa88', 'authors': ['Hengyu Luo', 'Zihao Li', 'Joseph Attieh', 'Sawal Devkota', 'Ona de Gibert', 'Shaoxiong Ji', 'Peiqin Lin', 'Bhavani Sai Praneeth Varma Mantina', 'Ananda Sreenidhi', 'RaÃºl VÃ¡zquez', 'Mengjie Wang', 'Samea Yusofi', 'JÃ¶rg Tiedemann'], 'affiliations': ['Technical University of Darmstadt, Germany', 'University of Helsinki, Finland', 'University of Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.04155.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#benchmark', '#machine_translation', '#open_source'], 'emoji': 'ğŸŒ', 'ru': {'title': 'GlotEval: ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ°Ğ½Ğ³Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ğ·Ğ¼Ğ°', 'desc': 'GlotEval - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ¾Ñ‚Ğ½Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ². GlotEval Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ½Ğµ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'GlotEval: Bridging the Evaluation Gap for Multilingual LLMs', 'desc': 'This paper presents GlotEval, a new framework aimed at evaluating large language models (LLMs) across multiple languages, particularly focusing on low-resource languages. Current evaluation methods are biased towards English and a few high-resource languages, which limits our understanding of LLM performance in diverse linguistic settings. GlotEval supports various tasks such as machine translation and text classification, allowing for comprehensive benchmarking across dozens of languages. By providing language-specific templates and non-English-centric evaluations, GlotEval helps identify the strengths and weaknesses of LLMs in multilingual contexts.'}, 'zh': {'title': 'å¤šè¯­è¨€è¯„ä¼°çš„æ–°æ¡†æ¶ï¼šGlotEval', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…¨çƒèŒƒå›´å†…è¿…é€Ÿå‘å±•ï¼Œå„åœ°åŒºè¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨è¿™äº›æ¨¡å‹è¿›è¡Œæœ¬å›½è¯­è¨€çš„åº”ç”¨ã€‚è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºåŒ®ä¹çš„è¯­è¨€ä¸­ï¼Œå·²æˆä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„ä¸€å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­å’Œå°‘æ•°é«˜èµ„æºè¯­è¨€ä¸Šï¼Œå¿½è§†äº†LLMsåœ¨å¤šè¯­è¨€å’Œä½èµ„æºåœºæ™¯ä¸­çš„å®é™…è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GlotEvalï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œæ—¨åœ¨è¿›è¡Œå¤§è§„æ¨¡å¤šè¯­è¨€è¯„ä¼°ï¼Œæ”¯æŒä¸ƒé¡¹å…³é”®ä»»åŠ¡ï¼Œå¸®åŠ©å‡†ç¡®è¯Šæ–­æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ç¯å¢ƒä¸­çš„ä¼˜ç¼ºç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.04152', 'title': 'Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting\n  LLMs Across Languages and Resources', 'url': 'https://huggingface.co/papers/2504.04152', 'abstract': 'Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies.', 'score': 0, 'issue_id': 3124, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 5', 'zh': '4æœˆ5æ—¥'}, 'hash': '3c22dafb05a10aab', 'authors': ['Zihao Li', 'Shaoxiong Ji', 'Hengyu Luo', 'JÃ¶rg Tiedemann'], 'affiliations': ['Technical University of Darmstadt', 'University of Helsinki'], 'pdf_title_img': 'assets/pdf/title_img/2504.04152.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#low_resource', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ LLM: ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (CPT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ CPT ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ½Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ CPT Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging the Gap: Enhancing Low-Resource Languages with Continual Pretraining', 'desc': 'This paper investigates how Large Language Models (LLMs) perform differently across various languages, especially highlighting the advantages for high-resource languages and the challenges faced by low-resource ones. It explores the effectiveness of Continual Pretraining (CPT) using different data strategies, including monolingual, bilingual, and code-augmented data. The study evaluates 36 configurations across multiple languages and reveals that bilingual CPT can enhance classification but may lead to language mixing, while code data improves accuracy for low-resource languages at the cost of generation quality. The findings also challenge previous assumptions about language classifications, showing that altruistic languages can harm related ones, and stagnant languages can adapt under certain conditions, highlighting the complexity of multilingual learning.'}, 'zh': {'title': 'è§£å†³è¯­è¨€ä¸å¹³è¡¡çš„æŒç»­é¢„è®­ç»ƒç­–ç•¥', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒè¯­è¨€ä¸Šçš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œä¸»è¦ä½¿é«˜èµ„æºè¯­è¨€å—ç›Šï¼Œè€Œè¾¹ç¼˜åŒ–äº†ä½èµ„æºè¯­è¨€ã€‚æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰è¢«è®¤ä¸ºæ˜¯è§£å†³è¿™ä¸€ä¸å¹³è¡¡çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å•è¯­ã€åŒè¯­å’Œä»£ç å¢å¼ºæ•°æ®ç­–ç•¥çš„ç›¸å¯¹æœ‰æ•ˆæ€§å°šä¸æ˜ç¡®ã€‚ç ”ç©¶è¯„ä¼°äº†36ç§CPTé…ç½®ï¼Œæ¶µç›–ä¸‰ç§å¤šè¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œæ¶‰åŠ30å¤šç§è¯­è¨€ï¼Œæ­ç¤ºäº†å¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„å¤æ‚æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŒè¯­CPTæé«˜äº†å¤šè¯­è¨€åˆ†ç±»ï¼Œä½†åœ¨ç”Ÿæˆæ—¶å¸¸å¯¼è‡´è¯­è¨€æ··åˆé—®é¢˜ï¼Œè€Œç¼–ç¨‹ä»£ç æ•°æ®çš„åŠ å…¥åˆ™æé«˜äº†ä½èµ„æºè¯­è¨€çš„åˆ†ç±»å‡†ç¡®æ€§ï¼Œä½†ç•¥å¾®é™ä½äº†ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03790', 'title': "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models", 'url': 'https://huggingface.co/papers/2504.03790', 'abstract': 'Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.', 'score': 0, 'issue_id': 3128, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': 'bbc79821089111ea', 'authors': ['GonÃ§alo Faria', 'Noah A. Smith'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.03790.jpg', 'data': {'categories': ['#alignment', '#math', '#training', '#optimization', '#benchmark', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'QAlign: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ QAlign. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ°Ğ¼. QAlign Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ†ĞµĞ¿ĞµĞ¹ ĞœĞ°Ñ€ĞºĞ¾Ğ²Ğ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ QAlign Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'QAlign: Optimizing Language Model Outputs at Test Time', 'desc': 'This paper presents QAlign, a novel approach for enhancing language model performance during test time by optimizing the alignment of outputs without requiring model retraining. Traditional methods using reward models often suffer from quality degradation as computational resources increase, due to reliance on imperfect reward proxies. QAlign leverages recent advancements in Markov chain Monte Carlo techniques to sample from the optimal distribution for each prompt, leading to better-aligned text generation. The method shows significant improvements over existing test-time strategies on various benchmarks, demonstrating its effectiveness in maximizing the utility of pre-trained language models.'}, 'zh': {'title': 'QAlignï¼šæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶é—´å¯¹é½æ–¹æ³•QAlignï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹å¾®è°ƒä¸å¯è¡Œçš„æƒ…å†µä¸‹ã€‚QAligné€šè¿‡åœ¨æµ‹è¯•æ—¶é—´è®¡ç®—ä¸­é‡‡æ ·æœ€ä¼˜å¯¹é½åˆ†å¸ƒï¼Œå…‹æœäº†ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨è®¡ç®—è§„æ¨¡æ‰©å¤§æ—¶è´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›æŠ€æœ¯ç”Ÿæˆæ–‡æœ¬ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹åŸºç¡€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆæ›´å¥½å¯¹é½çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQAlignåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„æµ‹è¯•æ—¶é—´è®¡ç®—æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10481', 'title': 'xVerify: Efficient Answer Verifier for Reasoning Model Evaluations', 'url': 'https://huggingface.co/papers/2504.10481', 'abstract': 'With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify.', 'score': 62, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '72678fc1ff453072', 'authors': ['Ding Chen', 'Qingchen Yu', 'Pengyuan Wang', 'Wentao Zhang', 'Bo Tang', 'Feiyu Xiong', 'Xinchi Li', 'Minchuan Yang', 'Zhiyu Li'], 'affiliations': ['Center for Data Science, Peking University', 'MemTensor (Shanghai) Technology Co., Ltd.', 'Research Institute of China Telecom, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.10481.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#dataset', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'xVerify: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ xVerify - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. xVerify ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ xVerify Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VAR, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ xVerify Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ F1-Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹ÑˆĞµ 95% Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'xVerify: Elevating Reasoning Model Evaluation with Precision', 'desc': 'This paper introduces xVerify, a novel answer verification tool designed to evaluate reasoning models that utilize slow thinking strategies. Traditional evaluation methods struggle with complex outputs from large language models (LLMs), particularly in assessing the equivalence of answers and extracting final responses. xVerify addresses these challenges by leveraging a specially constructed VAR dataset, which includes diverse question-answer pairs generated by various LLMs. The results show that xVerify models achieve high accuracy and F1 scores, demonstrating their effectiveness in evaluating reasoning models compared to existing methods.'}, 'zh': {'title': 'xVerifyï¼šæ¨ç†æ¨¡å‹è¯„ä¼°çš„æ–°æ ‡å‡†', 'desc': 'éšç€OpenAIå‘å¸ƒo1æ¨¡å‹ï¼Œé‡‡ç”¨æ…¢æ€ç»´ç­–ç•¥çš„æ¨ç†æ¨¡å‹é€æ¸å‡ºç°ã€‚è¿™äº›æ¨¡å‹ç”Ÿæˆçš„å“åº”é€šå¸¸åŒ…å«å¤æ‚çš„æ¨ç†ã€ä¸­é—´æ­¥éª¤å’Œè‡ªæˆ‘åæ€ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ¤æ–­LLMè¾“å‡ºæ˜¯å¦çœŸæ­£ç­‰åŒäºå‚è€ƒç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†xVerifyï¼Œä¸€ä¸ªé«˜æ•ˆçš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œç”¨äºæ¨ç†æ¨¡å‹çš„è¯„ä¼°ã€‚xVerifyåœ¨ç­‰ä»·åˆ¤æ–­æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ¤æ–­æ¨ç†æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆç­‰ä»·ï¼Œå¹¶åœ¨å¤šä¸ªå®¢è§‚é—®é¢˜ç±»å‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08672', 'title': 'Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning', 'url': 'https://huggingface.co/papers/2504.08672', 'abstract': 'Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https://github.com/xufangzhi/Genius.', 'score': 41, 'issue_id': 3264, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '1de57b79eb1b4e84', 'authors': ['Fangzhi Xu', 'Hang Yan', 'Chang Ma', 'Haiteng Zhao', 'Qiushi Sun', 'Kanzhi Cheng', 'Junxian He', 'Jun Liu', 'Zhiyong Wu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Peking University', 'Shanghai AI Lab', 'The University of Hong Kong', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.08672.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Genius Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°. Genius Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸.'}, 'en': {'title': 'Genius: Unsupervised Self-Training for Enhanced LLM Reasoning', 'desc': 'This paper presents Genius, a novel unsupervised self-training framework designed to enhance the reasoning skills of large language models (LLMs) without relying on external supervisory signals. Genius optimizes LLMs by seeking the best response sequences through a stepwise approach, utilizing a foresight re-sampling strategy to simulate and evaluate potential future outcomes. To address the challenges of noise and uncertainty in unsupervised settings, the authors introduce an advantage-calibrated optimization (ACO) loss function that improves the robustness of the optimization process. Overall, Genius aims to advance LLM reasoning capabilities efficiently, leveraging the abundance of general queries available.'}, 'zh': {'title': 'æ— ç›‘ç£è‡ªæˆ‘æå‡LLMæ¨ç†èƒ½åŠ›çš„é©å‘½æ€§è¿›å±•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGeniusçš„æ— ç›‘ç£è‡ªæˆ‘è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¾èµ–å¤–éƒ¨ç›‘ç£ä¿¡å·ã€‚Geniusé€šè¿‡é€æ­¥å¯»æ‰¾æœ€ä½³å“åº”åºåˆ—æ¥ä¼˜åŒ–LLMï¼Œå¹¶å¼•å…¥äº†ä¸€ç§é€æ­¥å‰ç»é‡é‡‡æ ·ç­–ç•¥ï¼Œä»¥æ¨¡æ‹Ÿæœªæ¥ç»“æœå¹¶è¯„ä¼°æ­¥éª¤ä»·å€¼ã€‚ä¸ºäº†åº”å¯¹æ— ç›‘ç£è®¾ç½®ä¸­å›ºæœ‰çš„å™ªå£°å’Œä¸ç¡®å®šæ€§ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§ä¼˜åŠ¿æ ¡å‡†ä¼˜åŒ–ï¼ˆACOï¼‰æŸå¤±å‡½æ•°ï¼Œä»¥å‡è½»ä¼°è®¡ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡ç»“åˆè¿™äº›æŠ€æœ¯ï¼ŒGeniusä¸ºæ— ç›‘ç£æ¡ä»¶ä¸‹çš„LLMæ¨ç†è‡ªæˆ‘æå‡æä¾›äº†ä¸€ä¸ªå…ˆè¿›çš„åˆæ­¥æ­¥éª¤ï¼Œæ¨åŠ¨äº†æ¨ç†æ‰©å±•æ³•åˆ™çš„å˜é©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10337', 'title': 'Heimdall: test-time scaling on the generative verification', 'url': 'https://huggingface.co/papers/2504.10337', 'abstract': 'An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.', 'score': 28, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '6da5db970a101d21', 'authors': ['Wenlei Shi', 'Xing Jin'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2504.10337.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#long_context', '#optimization', '#training', '#rl', '#math'], 'emoji': 'ğŸ”', 'ru': {'title': 'Heimdall: Ğ˜Ğ˜-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Heimdall - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought). Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ° Ñ 62.5% Ğ´Ğ¾ 94.5% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑÑĞ¸Ğ¼Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Heimdall Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ°Ğ¶Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ñ….'}, 'en': {'title': 'Heimdall: Elevating AI Verification with Chain-of-Thought Reasoning', 'desc': 'This paper introduces Heimdall, a long Chain-of-Thought (CoT) verification model designed to enhance the accuracy of solution verification in AI systems. By employing pure reinforcement learning, Heimdall significantly improves verification accuracy on competitive math problems from 62.5% to 94.5%, and with repeated sampling, it reaches 97.5%. The paper also presents Pessimistic Verification, which optimizes solution selection by minimizing uncertainty, leading to improved accuracy in problem-solving tasks. Additionally, Heimdall is part of an automatic knowledge discovery system that identifies flaws in datasets, revealing that a substantial portion of the data is incorrect, which is consistent with findings from previous studies.'}, 'zh': {'title': 'æå‡AIçŸ¥è¯†éªŒè¯èƒ½åŠ›çš„Heimdallæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHeimdallçš„é•¿é“¾æ€ç»´éªŒè¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨æé«˜è§£å†³ç«äº‰æ€§æ•°å­¦é—®é¢˜çš„è§£ç­”å‡†ç¡®æ€§ã€‚é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ ï¼ŒHeimdallçš„éªŒè¯å‡†ç¡®ç‡ä»62.5%æå‡è‡³94.5%ï¼Œå¹¶é€šè¿‡é‡å¤é‡‡æ ·è¿›ä¸€æ­¥æé«˜è‡³97.5%ã€‚æ­¤å¤–ï¼ŒHeimdallè¿˜å±•ç¤ºäº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ£€æµ‹å‡ºè®­ç»ƒä¸­æœªåŒ…å«çš„å¤æ‚æ•°å­¦è¯æ˜ä¸­çš„å¤§å¤šæ•°é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†æ‚²è§‚éªŒè¯æ–¹æ³•ï¼Œä»¥æ‰©å±•Heimdallçš„åŠŸèƒ½ï¼Œæ˜¾è‘—æé«˜äº†è§£å†³æ–¹æ¡ˆçš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11346', 'title': 'Seedream 3.0 Technical Report', 'url': 'https://huggingface.co/papers/2504.11346', 'abstract': 'We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.', 'score': 25, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '3a4b797a3a9516d2', 'authors': ['Yu Gao', 'Lixue Gong', 'Qiushan Guo', 'Xiaoxia Hou', 'Zhichao Lai', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Chao Liao', 'Liyang Liu', 'Wei Liu', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Rui Wang', 'Xuanda Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Xin Xia', 'Xuefeng Xiao', 'Zhonghua Zhai', 'Xinyu Zhang', 'Qi Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Jianchao Yang', 'Weilin Huang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.11346.jpg', 'data': {'categories': ['#alignment', '#data', '#optimization', '#dataset', '#multimodal', '#architecture', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Seedream 3.0 Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½ĞºÑƒ', 'desc': 'Seedream 3.0 - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM. Seedream 3.0 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ² 4-8 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Revolutionizing Bilingual Image Generation with Seedream 3.0', 'desc': 'Seedream 3.0 is an advanced bilingual image generation model designed for Chinese and English. It improves upon its predecessor by enhancing prompt alignment, typography generation, and overall image quality through a series of technical upgrades. Key innovations include a larger dataset, mixed-resolution training, and a novel acceleration method that speeds up processing while preserving image fidelity. The model excels in generating high-resolution images and accurately rendering complex Chinese characters, making it valuable for professional typography applications.'}, 'zh': {'title': 'Seedream 3.0ï¼šé«˜æ•ˆçš„ä¸­è‹±æ–‡å›¾åƒç”Ÿæˆæ–°çºªå…ƒ', 'desc': 'Seedream 3.0 æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„ä¸­è‹±æ–‡åŒè¯­å›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ã€‚å®ƒé€šè¿‡å¤šé¡¹æŠ€æœ¯æ”¹è¿›ï¼Œè§£å†³äº† Seedream 2.0 ä¸­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚æç¤ºçš„å¯¹é½ã€ç»†è‡´çš„æ’ç‰ˆç”Ÿæˆå’Œå›¾åƒåˆ†è¾¨ç‡é™åˆ¶ã€‚è¯¥æ¨¡å‹åœ¨æ•°æ®æ„å»ºå’Œæ¨¡å‹éƒ¨ç½²çš„æ•´ä¸ªæµç¨‹ä¸­è¿›è¡Œäº†æ”¹è¿›ï¼Œé‡‡ç”¨äº†ç¼ºé™·æ„ŸçŸ¥è®­ç»ƒå’ŒåŒè½´åä½œæ•°æ®é‡‡æ ·ç­‰æ–¹æ³•ã€‚Seedream 3.0 è¿˜å¼•å…¥äº†æ–°çš„åŠ é€ŸèŒƒå¼ï¼Œå®ç°äº†å›¾åƒè´¨é‡ä¸ç”Ÿæˆé€Ÿåº¦çš„æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ±‰å­—çš„æ–‡æœ¬æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10766', 'title': 'How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients', 'url': 'https://huggingface.co/papers/2504.10766', 'abstract': "As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.", 'score': 25, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'b95bec819bad5307', 'authors': ['Ming Li', 'Yanhong Li', 'Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['University of Chicago', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.10766.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#data', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº IFD, InsTag, Difficulty Ğ¸ Reward, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ², Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ ÑĞ´ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ½Ğ³Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¸Ğ¼ĞµÑÑ‚ ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ.'}, 'en': {'title': 'Unlocking the Secrets of Data Quality in LLM Fine-Tuning', 'desc': 'This paper investigates how the quality of data influences the fine-tuning of large language models (LLMs) during post-training, particularly for complex reasoning tasks. It employs spectral analysis of layer-wise gradients to understand the effects of low and high-quality instruction and reasoning data. The study finds that traditional metrics for data evaluation can be unified through the spectral properties derived from the singular value decomposition (SVD) of gradients. Notably, it shows that higher-quality data leads to lower nuclear norms and higher effective ranks, with effective rank being a more reliable measure for capturing quality differences, especially in reasoning tasks.'}, 'zh': {'title': 'æ•°æ®è´¨é‡ä¸è®­ç»ƒç¨³å®šæ€§çš„ç»Ÿä¸€è§†è§’', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åè®­ç»ƒé˜¶æ®µä¸­ï¼Œä¸åŒæ•°æ®å¯¹å¾®è°ƒåŠ¨æ€çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä½è´¨é‡å’Œé«˜è´¨é‡æŒ‡ä»¤åŠæ¨ç†æ•°æ®çš„å±‚çº§æ¢¯åº¦è¿›è¡Œè°±åˆ†æï¼Œå‘ç°å¸¸ç”¨çš„æ•°æ®è¯„ä¼°æŒ‡æ ‡å¯ä»¥é€šè¿‡æ¢¯åº¦çš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰è°±ç‰¹æ€§æ¥è§£é‡Šå’Œç»Ÿä¸€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜è´¨é‡æ•°æ®é€šå¸¸ä¸è¾ƒä½çš„æ ¸èŒƒæ•°å’Œè¾ƒé«˜çš„æœ‰æ•ˆç§©ç›¸å…³ï¼Œä¸”æœ‰æ•ˆç§©åœ¨æ•æ‰ç»†å¾®è´¨é‡å·®å¼‚æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§å’Œåˆ†è¾¨ç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜è¡¨æ˜ï¼ŒåŒä¸€å®¶æ—çš„æ¨¡å‹åœ¨æ¢¯åº¦æ¨¡å¼ä¸Šç›¸ä¼¼ï¼Œè€Œä¸åŒæ¨¡å‹å®¶æ—ä¹‹é—´åˆ™å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10465', 'title': 'Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding', 'url': 'https://huggingface.co/papers/2504.10465', 'abstract': "Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA.", 'score': 22, 'issue_id': 3260, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '6c90d31f3f941694', 'authors': ['Tao Zhang', 'Xiangtai Li', 'Zilong Huang', 'Yanwei Li', 'Weixian Lei', 'Xueqing Deng', 'Shihao Chen', 'Shunping Ji', 'Jiashi Feng'], 'affiliations': ['Bytedance Seed', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2504.10465.jpg', 'data': {'categories': ['#survey', '#optimization', '#benchmark', '#multimodal', '#training', '#architecture', '#games'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Pixel-SAIL - ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰ĞµĞ¹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PerBench Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Simplifying Multimodal Learning with Pixel-SAIL', 'desc': "This paper introduces Pixel-SAIL, a simplified Multimodal Large Language Model (MLLM) designed for pixel-level understanding tasks without relying on additional components like vision encoders. The authors propose three key innovations: a learnable upsampling module for refining visual features, a visual prompt injection strategy for better integration of visual and text inputs, and a vision expert distillation method to enhance feature extraction. By focusing on a single transformer architecture, Pixel-SAIL aims to reduce system complexity while maintaining high performance. The paper also presents a new benchmark, PerBench, to evaluate the model's effectiveness across various pixel understanding tasks."}, 'zh': {'title': 'ç®€åŒ–çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼šPixel-SAIL', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç»†ç²’åº¦åƒç´ çº§ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¤§å¤šæ•°å·¥ä½œä¾èµ–äºé¢å¤–çš„ç»„ä»¶ï¼Œå¦‚è§†è§‰ç¼–ç å™¨å’Œåˆ†å‰²ä¸“å®¶ï¼Œå¯¼è‡´ç³»ç»Ÿå¤æ‚æ€§é«˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€åŒ–çš„MLLMï¼Œåä¸ºPixel-SAILï¼Œæ—¨åœ¨ä¸å¼•å…¥é¢å¤–ç»„ä»¶çš„æƒ…å†µä¸‹è¿›è¡Œåƒç´ çº§ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡è®¾è®¡å¯å­¦ä¹ çš„ä¸Šé‡‡æ ·æ¨¡å—ã€è§†è§‰æç¤ºæ³¨å…¥ç­–ç•¥å’Œè§†è§‰ä¸“å®¶è’¸é¦ç­–ç•¥ï¼Œæå‡äº†å•ä¸€å˜æ¢å™¨çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPixel-SAILåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”å…·æœ‰æ›´ç®€å•çš„å¤„ç†æµç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11442', 'title': 'TextArena', 'url': 'https://huggingface.co/papers/2504.11442', 'abstract': 'TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.', 'score': 20, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '7c9ae6533757828a', 'authors': ['Leon Guertler', 'Bobby Cheng', 'Simon Yu', 'Bo Liu', 'Leshem Choshen', 'Cheston Tan'], 'affiliations': ['Centre for Frontier AI Research (CFAR), A*STAR Institute of High Performance Computing, A*STAR', 'MIT, MIT-IBM Watson AI Lab', 'National University of Singapore', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11442.jpg', 'data': {'categories': ['#agents', '#games', '#benchmark', '#open_source'], 'emoji': 'ğŸ®', 'ru': {'title': 'TextArena: ĞÑ€ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'TextArena - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 57 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¸Ğ³Ñ€Ñ‹ Ñ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ TrueSkill Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. TextArena Ğ²Ğ¾ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ², Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ° Ğ¸ Ğ¾Ğ±Ğ¼Ğ°Ğ½. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Train LLMs in Competitive Text Games!', 'desc': 'TextArena is a platform designed for training and evaluating Large Language Models (LLMs) through competitive text-based games. It features over 57 unique environments that support various gameplay modes, allowing models to interact with both human players and other models. This framework addresses the lack of benchmarks for assessing social skills like negotiation and deception, which are crucial for agentic behavior. TextArena is built for research and community engagement, making it easy to add new games and adapt the system for testing and training purposes.'}, 'zh': {'title': 'TextArenaï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æŠ€èƒ½è®­ç»ƒå¹³å°', 'desc': 'TextArenaæ˜¯ä¸€ä¸ªå¼€æºçš„ç«äº‰æ€§æ–‡æœ¬æ¸¸æˆé›†åˆï¼Œæ—¨åœ¨è®­ç»ƒå’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç†è¡Œä¸ºã€‚å®ƒåŒ…å«57ä¸ªä»¥ä¸Šç‹¬ç‰¹çš„ç¯å¢ƒï¼Œæ”¯æŒå•äººã€åŒäººå’Œå¤šäººè®¾ç½®ï¼Œå¹¶é€šè¿‡åœ¨çº¿æ¸¸æˆç³»ç»Ÿè½»æ¾è¯„ä¼°æ¨¡å‹èƒ½åŠ›ã€‚ä¼ ç»ŸåŸºå‡†æµ‹è¯•é€šå¸¸æ— æ³•è¯„ä¼°åŠ¨æ€ç¤¾äº¤æŠ€èƒ½ï¼Œå¦‚è°ˆåˆ¤ã€å¿ƒæ™ºç†è®ºå’Œæ¬ºéª—ï¼Œè€ŒTextArenaæ­£å¥½å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚è¯¥å¹³å°å¼ºè°ƒæ˜“äºæ·»åŠ æ–°æ¸¸æˆã€é€‚åº”æ¡†æ¶ã€æµ‹è¯•æ¨¡å‹å’Œè®­ç»ƒæ¨¡å‹ï¼Œé€‚åˆç ”ç©¶å’Œç¤¾åŒºä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10462', 'title': 'The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer', 'url': 'https://huggingface.co/papers/2504.10462', 'abstract': "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL.", 'score': 12, 'issue_id': 3260, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '1f70a22447fd1fc5', 'authors': ['Weixian Lei', 'Jiacong Wang', 'Haochen Wang', 'Xiangtai Li', 'Jun Hao Liew', 'Jiashi Feng', 'Zilong Huang'], 'affiliations': ['Bytedance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2504.10462.jpg', 'data': {'categories': ['#multimodal', '#agi', '#architecture', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SAIL: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'SAIL - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, SAIL Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ vision transformer, Ğ° Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. SAIL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SAIL: A Unified Approach to Multimodal Learning', 'desc': "This paper presents SAIL, a unified multimodal large language model that combines image and text processing in one architecture without needing a separate vision encoder. SAIL uses mix-attention mechanisms and multimodal positional encodings to effectively handle both visual and textual data. The study shows that SAIL can scale well with increased training data and model size, achieving performance similar to existing modular models. Additionally, SAIL's design leads to unique patterns in how information flows between modalities, while also excelling in visual tasks like semantic segmentation."}, 'zh': {'title': 'SAILï¼šç®€çº¦æ¶æ„ä¸‹çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†SAILï¼Œè¿™æ˜¯ä¸€ç§å•ä¸€å˜æ¢å™¨ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒåœ¨ä¸€ä¸ªæ¶æ„ä¸­æ•´åˆäº†åŸå§‹åƒç´ ç¼–ç å’Œè¯­è¨€è§£ç ã€‚ä¸ç°æœ‰çš„æ¨¡å—åŒ–MLLMä¸åŒï¼ŒSAILä¸éœ€è¦å•ç‹¬çš„è§†è§‰ç¼–ç å™¨ï¼Œå‘ˆç°å‡ºæ›´ç®€çº¦çš„æ¶æ„è®¾è®¡ã€‚SAILé‡‡ç”¨æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œå¤šæ¨¡æ€ä½ç½®ç¼–ç ï¼Œä»¥æ›´å¥½åœ°é€‚åº”è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„ç‹¬ç‰¹ç‰¹å¾ã€‚é€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®å’Œæ¨¡å‹è§„æ¨¡ï¼ŒSAILåœ¨æ€§èƒ½ä¸Šä¸æ¨¡å—åŒ–MLLMç›¸å½“ï¼ŒåŒæ—¶åœ¨è§†è§‰è¡¨ç¤ºèƒ½åŠ›ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10903', 'title': 'Efficient Reasoning Models: A Survey', 'url': 'https://huggingface.co/papers/2504.10903', 'abstract': 'Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this "slow-thinking" paradigm, with numerous tokens generated in sequence, inevitably introduces substantial computational overhead. To this end, it highlights an urgent need for effective acceleration. This survey aims to provide a comprehensive overview of recent advances in efficient reasoning. It categorizes existing works into three key directions: (1) shorter - compressing lengthy CoTs into concise yet effective reasoning chains; (2) smaller - developing compact language models with strong reasoning capabilities through techniques such as knowledge distillation, other model compression techniques, and reinforcement learning; and (3) faster - designing efficient decoding strategies to accelerate inference. A curated collection of papers discussed in this survey is available in our GitHub repository.', 'score': 10, 'issue_id': 3269, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '8a7af29eb70394e2', 'authors': ['Sicheng Feng', 'Gongfan Fang', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['Nankai University, Tianjin, China', 'National University of Singapore, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.10903.jpg', 'data': {'categories': ['#training', '#inference', '#reasoning', '#survey', '#optimization', '#data', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ĞºĞ¾Ñ€Ğ¾Ñ‡Ğµ, Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Accelerating Reasoning: Shorter, Smaller, and Faster!', 'desc': 'This paper discusses the advancements in reasoning models that generate Chain-of-Thoughts (CoTs) to solve complex tasks. However, the traditional approach can be slow and computationally expensive due to the lengthy sequences of tokens. The authors propose three strategies to enhance efficiency: creating shorter CoTs, developing smaller models with strong reasoning abilities, and implementing faster decoding methods. The survey also includes a collection of relevant research papers to support these advancements.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†çš„æœªæ¥ï¼šåŠ é€Ÿæ€ç»´é“¾', 'desc': 'æ¨ç†æ¨¡å‹åœ¨è§£å†³å¤æ‚å’Œé€»è¾‘å¯†é›†å‹ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡ç”Ÿæˆæ‰©å±•çš„æ€ç»´é“¾ï¼ˆCoTsï¼‰æ¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ç„¶è€Œï¼Œè¿™ç§â€œæ…¢æ€è€ƒâ€èŒƒå¼çš„å‡ºç°ï¼Œå¯¼è‡´äº†å¤§é‡åºåˆ—ç”Ÿæˆçš„è®¡ç®—å¼€é”€ã€‚ä¸ºæ­¤ï¼Œè¿«åˆ‡éœ€è¦æœ‰æ•ˆçš„åŠ é€Ÿæ–¹æ³•ã€‚æœ¬æ–‡ç»¼è¿°äº†é«˜æ•ˆæ¨ç†çš„æœ€æ–°è¿›å±•ï¼Œå¹¶å°†ç°æœ‰å·¥ä½œåˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ–¹å‘ï¼šå‹ç¼©é•¿æ€ç»´é“¾ã€å¼€å‘ç´§å‡‘çš„è¯­è¨€æ¨¡å‹ä»¥åŠè®¾è®¡é«˜æ•ˆçš„è§£ç ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10559', 'title': 'Efficient Process Reward Model Training via Active Learning', 'url': 'https://huggingface.co/papers/2504.10559', 'abstract': "Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.", 'score': 10, 'issue_id': 3259, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'f718e5da41cde633', 'authors': ['Keyu Duan', 'Zichen Liu', 'Xin Mao', 'Tianyu Pang', 'Changyu Chen', 'Qiguang Chen', 'Michael Qizhe Shieh', 'Longxu Dou'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'Singapore Management University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10559.jpg', 'data': {'categories': ['#data', '#reasoning', '#optimization', '#benchmark', '#math', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ActPRM - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² (PRM). ActPRM Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ PRM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ActPRM ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° 50% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Efficient Learning with Uncertainty: ActPRM for Enhanced Model Training', 'desc': 'This paper introduces ActPRM, an active learning method designed to enhance Process Reward Models (PRMs) for training large language models (LLMs). By focusing on the most uncertain samples, ActPRM significantly cuts down the costs associated with data labeling while maintaining or improving model performance. The approach involves using the PRM to estimate uncertainty and selectively retaining only the most ambiguous data for labeling by a more complex reasoning model. The results show that ActPRM not only reduces annotation requirements by 50% but also achieves state-of-the-art performance on benchmark tasks.'}, 'zh': {'title': 'ä¸»åŠ¨å­¦ä¹ æå‡æ¨¡å‹è®­ç»ƒæ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ActPRMï¼Œç”¨äºæé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼ŒActPRMæ˜¾è‘—é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ¥ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œä»…ä¿ç•™é«˜åº¦ä¸ç¡®å®šçš„æ•°æ®è¿›è¡Œæ ‡æ³¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒActPRMåœ¨å‡å°‘50%æ ‡æ³¨çš„åŒæ—¶ï¼Œæ€§èƒ½ä¸ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ç›¸å½“ç”šè‡³æ›´å¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11427', 'title': 'NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors', 'url': 'https://huggingface.co/papers/2504.11427', 'abstract': 'Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing methods with temporal components, we present NormalCrafter to leverage the inherent temporal priors of video diffusion models. To secure high-fidelity normal estimation across sequences, we propose Semantic Feature Regularization (SFR), which aligns diffusion features with semantic cues, encouraging the model to concentrate on the intrinsic semantics of the scene. Moreover, we introduce a two-stage training protocol that leverages both latent and pixel space learning to preserve spatial accuracy while maintaining long temporal context. Extensive evaluations demonstrate the efficacy of our method, showcasing a superior performance in generating temporally consistent normal sequences with intricate details from diverse videos.', 'score': 8, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'f1c298d14d78b468', 'authors': ['Yanrui Bin', 'Wenbo Hu', 'Haoyuan Wang', 'Xinya Chen', 'Bing Wang'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong', 'Huazhong University of Science and Technology', 'Spatial Intelligence Group, The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11427.jpg', 'data': {'categories': ['#cv', '#long_context', '#diffusion', '#video', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'NormalCrafter: Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NormalCrafter - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (SFR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹.'}, 'en': {'title': 'NormalCrafter: Enhancing Video Normal Estimation with Semantic Insights', 'desc': "This paper introduces NormalCrafter, a novel approach for estimating surface normals in video sequences. It addresses the challenge of maintaining temporal coherence, which is often overlooked in traditional static image methods. The authors propose Semantic Feature Regularization (SFR) to enhance the model's focus on the scene's semantics, improving the quality of normal estimation. Additionally, a two-stage training protocol is implemented to balance spatial accuracy and long-term temporal context, resulting in high-fidelity normal sequences across various videos."}, 'zh': {'title': 'è§†é¢‘æ³•çº¿ä¼°è®¡çš„æ–°æ–¹æ³•ï¼šNormalCrafter', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•NormalCrafterï¼Œç”¨äºè§†é¢‘ä¸­çš„æ³•çº¿ä¼°è®¡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ—¶é—´å…ˆéªŒï¼Œç¡®ä¿æ³•çº¿ä¼°è®¡åœ¨æ—¶é—´ä¸Šçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰ç‰¹å¾æ­£åˆ™åŒ–ï¼ˆSFRï¼‰ï¼Œé€šè¿‡å¯¹é½æ‰©æ•£ç‰¹å¾å’Œè¯­ä¹‰çº¿ç´¢ï¼Œå¸®åŠ©æ¨¡å‹å…³æ³¨åœºæ™¯çš„å†…åœ¨è¯­ä¹‰ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒåè®®ï¼Œæˆ‘ä»¬åœ¨ä¿æŒç©ºé—´ç²¾åº¦çš„åŒæ—¶ï¼Œå¢å¼ºäº†å¯¹é•¿æ—¶é—´ä¸Šä¸‹æ–‡çš„å­¦ä¹ ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç”Ÿæˆç»†èŠ‚ä¸°å¯Œä¸”æ—¶é—´ä¸€è‡´çš„æ³•çº¿åºåˆ—æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11001', 'title': 'ReZero: Enhancing LLM search ability by trying one-more-time', 'url': 'https://huggingface.co/papers/2504.11001', 'abstract': 'Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM) performance on knowledge-intensive tasks but depends heavily on initial search query quality. Current methods, often using Reinforcement Learning (RL), typically focus on query formulation or reasoning over results, without explicitly encouraging persistence after a failed search. We introduce ReZero (Retry-Zero), a novel RL framework that directly rewards the act of retrying a search query following an initial unsuccessful attempt. This incentivizes the LLM to explore alternative queries rather than prematurely halting. ReZero demonstrates significant improvement, achieving 46.88% accuracy compared to a 25% baseline. By rewarding persistence, ReZero enhances LLM robustness in complex information-seeking scenarios where initial queries may prove insufficient.', 'score': 7, 'issue_id': 3263, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '9e93b5032c2a9d9c', 'authors': ['Alan Dao', 'Thinh Le'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.11001.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#rag'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ°ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¾ĞºÑƒĞ¿Ğ°ĞµÑ‚ÑÑ: ReZero Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ReZero Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ÑĞ»Ğµ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ. ReZero Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ 25% Ğ´Ğ¾ 46.88% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Persistence Pays Off: Enhancing LLMs with ReZero', 'desc': 'This paper presents ReZero, a new reinforcement learning framework designed to improve the performance of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs). Unlike traditional methods that focus on refining search queries or analyzing results, ReZero encourages LLMs to persist and retry after an unsuccessful search. By rewarding the act of retrying, it promotes exploration of alternative queries, leading to better outcomes. The results show a significant accuracy increase, demonstrating that persistence can enhance LLM effectiveness in challenging knowledge-intensive tasks.'}, 'zh': {'title': 'é‡è¯•æœç´¢ï¼Œæå‡æ¨¡å‹é²æ£’æ€§', 'desc': 'Retrieval-Augmented Generationï¼ˆRAGï¼‰é€šè¿‡å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä½†å…¶æ•ˆæœä¾èµ–äºåˆå§‹æœç´¢æŸ¥è¯¢çš„è´¨é‡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä¸»è¦å…³æ³¨æŸ¥è¯¢çš„åˆ¶å®šæˆ–ç»“æœçš„æ¨ç†ï¼Œè€Œæ²¡æœ‰æ˜ç¡®é¼“åŠ±åœ¨æœç´¢å¤±è´¥åç»§ç»­å°è¯•ã€‚æˆ‘ä»¬æå‡ºäº†ReZeroï¼ˆRetry-Zeroï¼‰ï¼Œä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥å¥–åŠ±åœ¨åˆæ¬¡æœç´¢å¤±è´¥åé‡è¯•æŸ¥è¯¢çš„è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†LLMçš„é²æ£’æ€§ï¼Œåœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å¥½çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10342', 'title': 'VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge', 'url': 'https://huggingface.co/papers/2504.10342', 'abstract': 'Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with "thinking" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.', 'score': 7, 'issue_id': 3269, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '86516767518e49b1', 'authors': ['Yueqi Song', 'Tianyue Ou', 'Yibo Kong', 'Zecheng Li', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10342.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VisualPuzzles: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VisualPuzzles Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², VisualPuzzles Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'VisualPuzzles: Evaluating Pure Reasoning in Multimodal AI', 'desc': 'The paper introduces VisualPuzzles, a new benchmark designed to assess visual reasoning without relying heavily on specialized knowledge. It includes a variety of reasoning types such as algorithmic, analogical, deductive, inductive, and spatial reasoning. The authors demonstrate that existing multimodal large language models struggle with VisualPuzzles, highlighting that success in knowledge-heavy tasks does not guarantee performance in reasoning-focused scenarios. This benchmark aims to provide a clearer evaluation of reasoning abilities, emphasizing the importance of reasoning over mere factual recall.'}, 'zh': {'title': 'VisualPuzzlesï¼šè¯„ä¼°æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†', 'desc': 'å½“å‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•å¸¸å¸¸å°†æ¨ç†ä¸ç‰¹å®šé¢†åŸŸçŸ¥è¯†æ··æ·†ï¼Œè¿™ä½¿å¾—åœ¨éä¸“ä¸šç¯å¢ƒä¸­è¯„ä¼°ä¸€èˆ¬æ¨ç†èƒ½åŠ›å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisualPuzzlesï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºè§†è§‰æ¨ç†çš„åŸºå‡†ï¼Œæ•…æ„å‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†çš„ä¾èµ–ã€‚VisualPuzzlesåŒ…å«äº”ç±»å¤šæ ·åŒ–çš„é—®é¢˜ï¼šç®—æ³•æ¨ç†ã€ç±»æ¯”æ¨ç†ã€æ¼”ç»æ¨ç†ã€å½’çº³æ¨ç†å’Œç©ºé—´æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒVisualPuzzleséœ€è¦çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶æ¨ç†çš„å¤æ‚æ€§æ›´é«˜ï¼Œä»è€Œæ›´å¥½åœ°è¯„ä¼°çœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10277', 'title': 'RealHarm: A Collection of Real-World Language Model Application Failures', 'url': 'https://huggingface.co/papers/2504.10277', 'abstract': "Language model deployments in consumer-facing applications introduce numerous risks. While existing research on harms and hazards of such applications follows top-down approaches derived from regulatory frameworks and theoretical analyses, empirical evidence of real-world failure modes remains underexplored. In this work, we introduce RealHarm, a dataset of annotated problematic interactions with AI agents built from a systematic review of publicly reported incidents. Analyzing harms, causes, and hazards specifically from the deployer's perspective, we find that reputational damage constitutes the predominant organizational harm, while misinformation emerges as the most common hazard category. We empirically evaluate state-of-the-art guardrails and content moderation systems to probe whether such systems would have prevented the incidents, revealing a significant gap in the protection of AI applications.", 'score': 7, 'issue_id': 3274, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '996eec7ad28b00d4', 'authors': ['Pierre Le Jeune', 'Jiaen Liu', 'Luca Rossi', 'Matteo Dora'], 'affiliations': ['Giskard AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.10277.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#ethics', '#data', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'RealHarm: Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ RealHarm, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ†Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ¸ÑĞºĞ°Ñ… Ğ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑÑ… Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹, Ğ²Ğ½ĞµĞ´Ñ€ÑÑÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑƒÑ‰ĞµÑ€Ğ± ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ñ€ĞµĞ´Ğ¾Ğ¼, Ğ° Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ - Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ Ğ˜Ğ˜-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Understanding Real-World Risks of AI Deployments', 'desc': 'This paper addresses the risks associated with deploying machine learning models in consumer applications, focusing on real-world failures. It introduces RealHarm, a dataset that captures problematic interactions with AI agents based on a review of reported incidents. The study highlights that reputational damage is the main harm organizations face, while misinformation is the most frequent hazard. Additionally, it evaluates existing safety measures and finds that current guardrails and content moderation systems are insufficient to prevent these incidents.'}, 'zh': {'title': 'æ­ç¤ºAIåº”ç”¨ä¸­çš„æ½œåœ¨é£é™©ä¸å±å®³', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ¶ˆè´¹è€…åº”ç”¨ä¸­è¯­è¨€æ¨¡å‹éƒ¨ç½²æ‰€å¸¦æ¥çš„é£é™©ã€‚æˆ‘ä»¬å¼•å…¥äº†RealHarmæ•°æ®é›†ï¼Œè®°å½•äº†ä¸AIä»£ç†çš„æœ‰é—®é¢˜äº’åŠ¨ï¼ŒåŸºäºå¯¹å…¬å¼€æŠ¥å‘Šäº‹ä»¶çš„ç³»ç»Ÿæ€§å®¡æŸ¥ã€‚ç ”ç©¶å‘ç°ï¼Œå£°èª‰æŸå®³æ˜¯ä¸»è¦çš„ç»„ç»‡æ€§å±å®³ï¼Œè€Œé”™è¯¯ä¿¡æ¯åˆ™æ˜¯æœ€å¸¸è§çš„å±é™©ç±»åˆ«ã€‚é€šè¿‡è¯„ä¼°ç°æœ‰çš„é˜²æŠ¤æªæ–½å’Œå†…å®¹å®¡æ ¸ç³»ç»Ÿï¼Œæˆ‘ä»¬å‘ç°è¿™äº›ç³»ç»Ÿåœ¨ä¿æŠ¤AIåº”ç”¨æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10188', 'title': 'Efficient Generative Model Training via Embedded Representation Warmup', 'url': 'https://huggingface.co/papers/2504.10188', 'abstract': "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40times acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW.", 'score': 7, 'issue_id': 3261, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '280d2a5386c25fa2', 'authors': ['Deyuan Liu', 'Peng Sun', 'Xufeng Li', 'Tao Lin'], 'affiliations': ['Nanjing University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10188.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#architecture', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Embedded Representation Warmup (ERW). ERW Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ERW ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 40 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Accelerating Diffusion Models with Embedded Representation Warmup', 'desc': 'This paper addresses the limitations of diffusion models in generating high-dimensional data efficiently. It identifies that the slow training process is due to the underutilization of high-quality representations in the early layers of the model. The authors propose a method called Embedded Representation Warmup (ERW), which initializes these layers with pretrained representations to enhance learning speed and quality. Their results show that ERW significantly accelerates training convergence and improves representation quality, achieving a 40 times faster training speed compared to existing methods.'}, 'zh': {'title': 'åŠ é€Ÿæ‰©æ•£æ¨¡å‹è®­ç»ƒçš„åµŒå…¥è¡¨ç¤ºé¢„çƒ­', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ç»´æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è®­ç»ƒæ•ˆç‡å’Œè¡¨ç¤ºè´¨é‡ä¸Šä¸å¦‚è‡ªç›‘ç£æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªå……åˆ†åˆ©ç”¨é«˜è´¨é‡ã€è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºï¼Œæ˜¾è‘—å‡ç¼“äº†æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åµŒå…¥è¡¨ç¤ºé¢„çƒ­ï¼ˆERWï¼‰æ¡†æ¶ï¼Œé€šè¿‡åœ¨åˆå§‹é˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒçš„é«˜è´¨é‡è¡¨ç¤ºæ¥åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹çš„æ—©æœŸå±‚ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›å¹¶æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼ŒERWçš„æœ‰æ•ˆæ€§ä¾èµ–äºå…¶åœ¨ç‰¹å®šç¥ç»ç½‘ç»œå±‚çš„ç²¾ç¡®é›†æˆï¼Œè¿™äº›å±‚ä¸»è¦å¤„ç†å’Œè½¬æ¢ç‰¹å¾è¡¨ç¤ºä»¥ä¾¿åç»­ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11456', 'title': 'DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning', 'url': 'https://huggingface.co/papers/2504.11456', 'abstract': 'The capacity for complex mathematical reasoning is a key benchmark for artificial intelligence. While reinforcement learning (RL) applied to LLMs shows promise, progress is significantly hindered by the lack of large-scale training data that is sufficiently challenging, possesses verifiable answer formats suitable for RL, and is free from contamination with evaluation benchmarks. To address these limitations, we introduce DeepMath-103K, a new, large-scale dataset comprising approximately 103K mathematical problems, specifically designed to train advanced reasoning models via RL. DeepMath-103K is curated through a rigorous pipeline involving source analysis, stringent decontamination against numerous benchmarks, and filtering for high difficulty (primarily Levels 5-9), significantly exceeding existing open resources in challenge. Each problem includes a verifiable final answer, enabling rule-based RL, and three distinct R1-generated solutions suitable for diverse training paradigms like supervised fine-tuning or distillation. Spanning a wide range of mathematical topics, DeepMath-103K promotes the development of generalizable reasoning. We demonstrate that models trained on DeepMath-103K achieve significant improvements on challenging mathematical benchmarks, validating its effectiveness. We release DeepMath-103K publicly to facilitate community progress in building more capable AI reasoning systems: https://github.com/zwhe99/DeepMath.', 'score': 6, 'issue_id': 3266, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '3af8b373358fa97c', 'authors': ['Zhiwei He', 'Tian Liang', 'Jiahao Xu', 'Qiuzhi Liu', 'Xingyu Chen', 'Yue Wang', 'Linfeng Song', 'Dian Yu', 'Zhenwen Liang', 'Wenxuan Wang', 'Zhuosheng Zhang', 'Rui Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2504.11456.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#math', '#dataset', '#data', '#open_source', '#rl'], 'emoji': 'ğŸ§®', 'ru': {'title': 'DeepMath-103K: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'DeepMath-103K - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 103 Ñ‚Ñ‹ÑÑÑ‡ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ¼ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ R1, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° DeepMath-103K, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Empowering AI with DeepMath-103K: A Leap in Mathematical Reasoning', 'desc': 'This paper presents DeepMath-103K, a new dataset designed to enhance the training of AI models in complex mathematical reasoning. It addresses the challenges faced by reinforcement learning (RL) in large language models (LLMs) due to the lack of high-quality, difficult training data. The dataset includes around 103,000 carefully curated mathematical problems with verifiable answers, allowing for effective rule-based RL training. By providing diverse solutions and covering a wide range of topics, DeepMath-103K aims to improve the generalization capabilities of AI reasoning systems.'}, 'zh': {'title': 'DeepMath-103Kï¼šæ¨åŠ¨AIæ•°å­¦æ¨ç†çš„çªç ´æ€§æ•°æ®é›†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†DeepMath-103Kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«çº¦103,000ä¸ªæ•°å­¦é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒé«˜çº§æ¨ç†æ¨¡å‹ã€‚è¯¥æ•°æ®é›†ç»è¿‡ä¸¥æ ¼çš„ç­›é€‰å’Œå»æ±¡æŸ“å¤„ç†ï¼Œç¡®ä¿é—®é¢˜å…·æœ‰é«˜éš¾åº¦ï¼Œå¹¶ä¸”æ¯ä¸ªé—®é¢˜éƒ½æä¾›å¯éªŒè¯çš„æœ€ç»ˆç­”æ¡ˆï¼Œé€‚åˆè§„åˆ™åŸºç¡€çš„RLã€‚DeepMath-103Kæ¶µç›–å¹¿æ³›çš„æ•°å­¦ä¸»é¢˜ï¼Œä¿ƒè¿›äº†å¯æ¨å¹¿æ¨ç†çš„å‘å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºDeepMath-103Kè®­ç»ƒçš„æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11393', 'title': 'DataDecide: How to Predict Best Pretraining Data with Small Experiments', 'url': 'https://huggingface.co/papers/2504.11393', 'abstract': 'Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.', 'score': 6, 'issue_id': 3275, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'c08bb6187070e5e2', 'authors': ['Ian Magnusson', 'Nguyen Tai', 'Ben Bogin', 'David Heineman', 'Jena D. Hwang', 'Luca Soldaini', 'Akshita Bhagia', 'Jiacheng Liu', 'Dirk Groeneveld', 'Oyvind Tafjord', 'Noah A. Smith', 'Pang Wei Koh', 'Jesse Dodge'], 'affiliations': ['Allen Institute for AI', 'University of Pennsylvania', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.11393.jpg', 'data': {'categories': ['#small_models', '#data', '#benchmark', '#optimization', '#training', '#dataset', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ DataDecide - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (150M Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° (1B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²). Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ >80% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 0.01% Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Optimize Dataset Selection for Large Language Models with DataDecide', 'desc': 'This paper addresses the challenge of efficiently selecting datasets for pretraining large language models by utilizing smaller-scale experiments. It introduces DataDecide, a comprehensive suite that allows researchers to evaluate the impact of different datasets and model sizes on performance. The authors conduct extensive experiments across various corpora and find that performance rankings at smaller model sizes can effectively predict outcomes at larger scales. Additionally, they demonstrate that using continuous likelihood metrics can significantly enhance the predictability of benchmarks with minimal computational resources.'}, 'zh': {'title': 'å°è§„æ¨¡å®éªŒåŠ©åŠ›å¤§æ¨¡å‹æ•°æ®é€‰æ‹©', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å°è§„æ¨¡å®éªŒæ¥é€‰æ‹©æ•°æ®é›†ï¼Œä»¥é™ä½å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæˆæœ¬ã€‚æˆ‘ä»¬å‘å¸ƒäº†DataDecideï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ¨¡å‹ã€æ•°æ®å’Œè¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨å¸®åŠ©ç ”ç©¶è€…æ¢ç´¢æœ€ä½³æ•°æ®é›†é€‰æ‹©ã€‚é€šè¿‡åœ¨25ä¸ªä¸åŒæ¥æºçš„è¯­æ–™åº“ä¸Šè¿›è¡Œæ§åˆ¶é¢„è®­ç»ƒå®éªŒï¼Œæˆ‘ä»¬å‘ç°å°è§„æ¨¡æ¨¡å‹çš„æ’åå¯ä»¥æœ‰æ•ˆé¢„æµ‹å¤§è§„æ¨¡æ¨¡å‹çš„è¡¨ç°ã€‚ä½¿ç”¨è¿ç»­ä¼¼ç„¶åº¦æŒ‡æ ‡ä½œä¸ºå°è§„æ¨¡å®éªŒçš„ä»£ç†ï¼Œå¯ä»¥åœ¨ç›®æ ‡è§„æ¨¡ä¸‹å®ç°è¶…è¿‡80%çš„å¯é¢„æµ‹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11343', 'title': 'A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce', 'url': 'https://huggingface.co/papers/2504.11343', 'abstract': "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.", 'score': 6, 'issue_id': 3260, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '72714d765a5a497f', 'authors': ['Wei Xiong', 'Jiarui Yao', 'Yuhui Xu', 'Bo Pang', 'Lei Wang', 'Doyen Sahoo', 'Junnan Li', 'Nan Jiang', 'Tong Zhang', 'Caiming Xiong', 'Hanze Dong'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.11343.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² RAFT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº GRPO. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Reinforce-Rej, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ RAFT ĞºĞ°Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°Ñ….'}, 'en': {'title': 'Simplifying Reinforcement Learning for Better Language Model Training', 'desc': "This paper explores the effectiveness of the GRPO method in reinforcement learning for fine-tuning large language models on reasoning tasks. The authors discover that a simpler method, RAFT, which only uses positively rewarded samples, performs comparably to GRPO and PPO. They find that GRPO's strength lies in its ability to discard prompts with completely incorrect responses, rather than its reward normalization technique. To enhance performance, they introduce Reinforce-Rej, which filters out both incorrect and correct samples, improving efficiency and stability in training."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ çš„æ–°è§†è§’ï¼šæ‹’ç»é‡‡æ ·çš„åŠ›é‡', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†GRPOç®—æ³•ï¼Œå‘ç°ä¸€ä¸ªç®€å•çš„æ‹’ç»é‡‡æ ·åŸºçº¿RAFTåœ¨è®­ç»ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³ä¸GRPOå’ŒPPOç›¸å½“ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGRPOçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºä¸¢å¼ƒå®Œå…¨é”™è¯¯çš„æç¤ºï¼Œè€Œä¸æ˜¯å…¶å¥–åŠ±å½’ä¸€åŒ–ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Reinforce-Rejï¼Œè¿™æ˜¯ä¸€ç§è¿‡æ»¤å®Œå…¨é”™è¯¯å’Œå®Œå…¨æ­£ç¡®æ ·æœ¬çš„ç­–ç•¥æ¢¯åº¦æ‰©å±•ï¼Œèƒ½å¤Ÿæé«˜KLæ•ˆç‡å’Œç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11455', 'title': 'SimpleAR: Pushing the Frontier of Autoregressive Visual Generation\n  through Pretraining, SFT, and RL', 'url': 'https://huggingface.co/papers/2504.11455', 'abstract': 'This work presents SimpleAR, a vanilla autoregressive visual generation framework without complex architecure modifications. Through careful exploration of training and inference optimization, we demonstrate that: 1) with only 0.5B parameters, our model can generate 1024x1024 resolution images with high fidelity, and achieve competitive results on challenging text-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) training could lead to significant improvements on generation aesthectics and prompt alignment; and 3) when optimized with inference acceleraton techniques like vLLM, the time for SimpleAR to generate an 1024x1024 image could be reduced to around 14 seconds. By sharing these findings and open-sourcing the code, we hope to reveal the potential of autoregressive visual generation and encourage more participation in this research field. Code is available at https://github.com/wdrink/SimpleAR.', 'score': 4, 'issue_id': 3269, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'cec2567a7d0af48a', 'authors': ['Junke Wang', 'Zhi Tian', 'Xun Wang', 'Xinyu Zhang', 'Weilin Huang', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['ByteDance', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11455.jpg', 'data': {'categories': ['#training', '#inference', '#open_source', '#benchmark', '#optimization', '#small_models', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'SimpleAR - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (0.5 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1024x1024 Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğº ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (SFT), Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºÑƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº vLLM, Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ 1024x1024 ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¾ 14 ÑĞµĞºÑƒĞ½Ğ´.'}, 'en': {'title': 'Unlocking High-Quality Image Generation with SimpleAR', 'desc': 'This paper introduces SimpleAR, a straightforward autoregressive framework for generating high-quality images without the need for complex architecture changes. The model, with only 0.5 billion parameters, can produce 1024x1024 resolution images and performs competitively on text-to-image benchmarks. It highlights the effectiveness of supervised fine-tuning and Group Relative Policy Optimization in enhancing image aesthetics and prompt alignment. Additionally, by employing inference acceleration techniques, the generation time for images is significantly reduced, showcasing the efficiency of the SimpleAR model.'}, 'zh': {'title': 'SimpleARï¼šé«˜æ•ˆè‡ªå›å½’è§†è§‰ç”Ÿæˆçš„æ½œåŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSimpleARçš„è‡ªå›å½’è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ²¡æœ‰å¤æ‚çš„æ¶æ„ä¿®æ”¹ã€‚æˆ‘ä»¬é€šè¿‡å¯¹è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–çš„æ·±å…¥æ¢ç´¢ï¼Œå±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨ä»…æœ‰5äº¿å‚æ•°çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„1024x1024åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸­å–å¾—ç«äº‰æ€§ç»“æœã€‚æˆ‘ä»¬å‘ç°ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒéƒ½èƒ½æ˜¾è‘—æé«˜ç”Ÿæˆå›¾åƒçš„ç¾å­¦å’Œæç¤ºå¯¹é½æ•ˆæœã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ¨ç†åŠ é€ŸæŠ€æœ¯å¦‚vLLMåï¼ŒSimpleARç”Ÿæˆ1024x1024å›¾åƒçš„æ—¶é—´å¯ç¼©çŸ­è‡³çº¦14ç§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11447', 'title': 'Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion', 'url': 'https://huggingface.co/papers/2504.11447', 'abstract': "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO.", 'score': 4, 'issue_id': 3258, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'cda8d39111df28d8', 'authors': ['An Zhaol', 'Shengyuan Zhang', 'Ling Yang', 'Zejian Li', 'Jiale Wu', 'Haoran Xu', 'AnYang Wei', 'Perry Pengyun GU Lingyun Sun'], 'affiliations': ['Peking University', 'Zhejiang Green Zhixing Technology co., ltd', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11447.jpg', 'data': {'categories': ['#open_source', '#3d', '#rlhf', '#training', '#diffusion', '#optimization'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½ LiDAR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Distillation-DPO', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Distillation-DPO Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½ LiDAR Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (DPO), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ†ĞµĞ½ LiDAR Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Distillation-DPO Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ¼ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Accelerating 3D LiDAR Scene Completion with Distillation-DPO', 'desc': 'This paper introduces Distillation-DPO, a new framework that enhances 3D LiDAR scene completion using diffusion models. It combines score distillation with direct policy optimization (DPO) to improve performance while speeding up the sampling process. The method generates paired scene completions with varying initial noises and uses LiDAR evaluation metrics to create winning and losing pairs for optimization. The results show that Distillation-DPO significantly outperforms existing models in both quality and speed, marking a novel approach in preference-aligned distillation for LiDAR applications.'}, 'zh': {'title': 'æå‡LiDARåœºæ™¯è¡¥å…¨é€Ÿåº¦ä¸è´¨é‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£è’¸é¦æ¡†æ¶ï¼Œç§°ä¸ºDistillation-DPOï¼Œç”¨äº3D LiDARåœºæ™¯è¡¥å…¨ã€‚è¯¥æ–¹æ³•é€šè¿‡åå¥½å¯¹é½æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ï¼Œé¦–å…ˆç”Ÿæˆä¸åŒåˆå§‹å™ªå£°çš„é…å¯¹è¡¥å…¨åœºæ™¯ã€‚ç„¶åï¼Œåˆ©ç”¨LiDARåœºæ™¯è¯„ä¼°æŒ‡æ ‡æ„å»ºèƒœè´Ÿæ ·æœ¬å¯¹ï¼Œä»¥æ­¤æ¥ä¼˜åŒ–æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDistillation-DPOåœ¨åœºæ™¯è¡¥å…¨è´¨é‡å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œè¡¥å…¨é€Ÿåº¦æé«˜äº†5å€ä»¥ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11326', 'title': 'PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild', 'url': 'https://huggingface.co/papers/2504.11326', 'abstract': 'This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/.', 'score': 4, 'issue_id': 3260, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'abbfe1ceb2f9688a', 'authors': ['Henghui Ding', 'Chang Liu', 'Nikhila Ravi', 'Shuting He', 'Yunchao Wei', 'Song Bai', 'Philip Torr', 'Kehuan Song', 'Xinglin Xie', 'Kexin Zhang', 'Licheng Jiao', 'Lingling Li', 'Shuyuan Yang', 'Xuqiang Cao', 'Linnan Zhao', 'Jiaxuan Zhao', 'Fang Liu', 'Mengjiao Wang', 'Junpei Zhang', 'Xu Liu', 'Yuting Yang', 'Mengru Ma', 'Hao Fang', 'Runmin Cong', 'Xiankai Lu', 'Zhiyang Che', 'Wei Zhan', 'Tianming Liang', 'Haichao Jiang', 'Wei-Shi Zheng', 'Jian-Fang Hu', 'Haobo Yuan', 'Xiangtai Li', 'Tao Zhang', 'Lu Qi', 'Ming-Hsuan Yang'], 'affiliations': ['the Institute of Big Data, Fudan University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.11326.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ 4-Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞ° PVUW Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… CVPR 2025. ĞšĞ¾Ğ½ĞºÑƒÑ€Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ» Ğ´Ğ²Ğ° Ñ‚Ñ€ĞµĞºĞ°: MOSE Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ MeViS Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. Ğ‘Ñ‹Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ, Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ°ĞµÑ‚ Ñ†ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸ÑÑ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Advancing Video Segmentation: Insights from the PVUW Challenge', 'desc': 'This paper reviews the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, which took place at CVPR 2025. It highlights two main tracks: MOSE for complex scene video object segmentation and MeViS for motion-guided, language-based video segmentation. The challenge introduced new datasets that are more representative of real-world video scenarios, pushing the boundaries of current segmentation techniques. The findings provide insights into the latest advancements and future directions in the field of video segmentation.'}, 'zh': {'title': 'æ¨åŠ¨å¤æ‚è§†é¢‘åˆ†å‰²çš„å‰æ²¿æŒ‘æˆ˜', 'desc': 'æœ¬æŠ¥å‘Šå…¨é¢æ¦‚è¿°äº†2025å¹´CVPRä¼šè®®æœŸé—´ä¸¾è¡Œçš„ç¬¬å››å±Šåƒç´ çº§è§†é¢‘ç†è§£æŒ‘æˆ˜èµ›ï¼ˆPVUWï¼‰ã€‚æŒ‘æˆ˜èµ›åŒ…æ‹¬ä¸¤ä¸ªèµ›é“ï¼šMOSEä¸“æ³¨äºå¤æ‚åœºæ™¯çš„è§†é¢‘ç‰©ä½“åˆ†å‰²ï¼Œè€ŒMeViSåˆ™é’ˆå¯¹åŸºäºè¿åŠ¨å¼•å¯¼å’Œè¯­è¨€çš„è§†é¢‘åˆ†å‰²ã€‚ä¸¤ä¸ªèµ›é“éƒ½å¼•å…¥äº†æ–°çš„ã€æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œä»¥æ›´å¥½åœ°åæ˜ ç°å®ä¸–ç•Œçš„åœºæ™¯ã€‚é€šè¿‡è¯¦ç»†çš„è¯„ä¼°å’Œåˆ†æï¼Œè¯¥æŒ‘æˆ˜èµ›ä¸ºå¤æ‚è§†é¢‘åˆ†å‰²çš„æœ€æ–°æŠ€æœ¯çŠ¶æ€å’Œæ–°å…´è¶‹åŠ¿æä¾›äº†å®è´µçš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08846', 'title': 'AI-University: An LLM-based platform for instructional alignment to\n  scientific classrooms', 'url': 'https://huggingface.co/papers/2504.08846', 'abstract': "We introduce AI University (AI-U), a flexible framework for AI-driven course content delivery that adapts to instructors' teaching styles. At its core, AI-U fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to generate instructor-aligned responses from lecture videos, notes, and textbooks. Using a graduate-level finite-element-method (FEM) course as a case study, we present a scalable pipeline to systematically construct training data, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and optimize its responses through RAG-based synthesis. Our evaluation - combining cosine similarity, LLM-based assessment, and expert review - demonstrates strong alignment with course materials. We also have developed a prototype web application, available at https://my-ai-university.com, that enhances traceability by linking AI-generated responses to specific sections of the relevant course material and time-stamped instances of the open-access video lectures. Our expert model is found to have greater cosine similarity with a reference on 86% of test cases. An LLM judge also found our expert model to outperform the base Llama 3.2 model approximately four times out of five. AI-U offers a scalable approach to AI-assisted education, paving the way for broader adoption in higher education. Here, our framework has been presented in the setting of a class on FEM - a subject that is central to training PhD and Master students in engineering science. However, this setting is a particular instance of a broader context: fine-tuning LLMs to research content in science.", 'score': 4, 'issue_id': 3265, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '0d4f794fba95381f', 'authors': ['Mostafa Faghih Shojaei', 'Rahul Gulati', 'Benjamin A. Jasperson', 'Shangshang Wang', 'Simone Cimolato', 'Dangli Cao', 'Willie Neiswanger', 'Krishna Garikipati'], 'affiliations': ['Department of Aerospace and Mechanical Engineering, University of Southern California', 'Department of Astronautical Engineering, University of Southern California', 'Department of Computer Science, University of Southern California', 'Department of Electrical and Computer Engineering, University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2504.08846.jpg', 'data': {'categories': ['#low_resource', '#multimodal', '#training', '#open_source', '#science', '#rag'], 'emoji': 'ğŸ“', 'ru': {'title': 'AI-U: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AI University (AI-U) - Ğ³Ğ¸Ğ±ĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ğ¸ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AI-U Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM), Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° retrieval-augmented generation (RAG) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ»ĞµĞºÑ†Ğ¸Ğ¹, Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ ÑƒÑ‡ĞµĞ±Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ open-source LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Low-Rank Adaptation (LoRA) Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· RAG-ÑĞ¸Ğ½Ñ‚ĞµĞ·. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾, Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ğ¼ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ğ¼.'}, 'en': {'title': 'AI-U: Tailoring AI Learning to Teaching Styles', 'desc': "AI University (AI-U) is a framework designed to enhance AI-driven course content delivery by adapting to different teaching styles. It utilizes a large language model (LLM) that is fine-tuned with retrieval-augmented generation (RAG) to create responses that align closely with lecture materials. The framework was tested using a finite-element-method (FEM) course, demonstrating effective training data construction and optimization of AI responses. Evaluation results show that AI-U's model significantly outperforms the base model, indicating its potential for scalable AI-assisted education in higher learning environments."}, 'zh': {'title': 'AIå¤§å­¦ï¼šçµæ´»çš„AIè¾…åŠ©æ•™è‚²æ¡†æ¶', 'desc': 'AIå¤§å­¦ï¼ˆAI-Uï¼‰æ˜¯ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ ¹æ®æ•™å¸ˆçš„æ•™å­¦é£æ ¼è°ƒæ•´AIé©±åŠ¨çš„è¯¾ç¨‹å†…å®¹äº¤ä»˜ã€‚å®ƒçš„æ ¸å¿ƒæ˜¯é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆä¸è®²åº§è§†é¢‘ã€ç¬”è®°å’Œæ•™ç§‘ä¹¦ä¸€è‡´çš„æ•™å¸ˆå“åº”ã€‚ä»¥ç ”ç©¶ç”Ÿçº§åˆ«çš„æœ‰é™å…ƒæ–¹æ³•ï¼ˆFEMï¼‰è¯¾ç¨‹ä¸ºæ¡ˆä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„ç®¡é“ï¼Œç³»ç»Ÿåœ°æ„å»ºè®­ç»ƒæ•°æ®ï¼Œå¹¶é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒå¼€æºLLMã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒAI-Uåœ¨æ•™è‚²ä¸­æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œä¿ƒè¿›äº†é«˜ç­‰æ•™è‚²ä¸­AIè¾…åŠ©å­¦ä¹ çš„æ›´å¹¿æ³›åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11409', 'title': 'Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning', 'url': 'https://huggingface.co/papers/2504.11409', 'abstract': 'Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce a novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier.', 'score': 3, 'issue_id': 3275, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '0a29da0f494ed358', 'authors': ['Ali Taghibakhshi', 'Sharath Turuvekere Sreenivas', 'Saurav Muralidharan', 'Marcin Chochowski', 'Yashaswi Karnati', 'Raviraj Joshi', 'Ameya Sunil Mahabaleshwarkar', 'Zijia Chen', 'Yoshi Suhara', 'Oluwatobi Olabiyi', 'Daniel Korzekwa', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Jan Kautz', 'Bryan Catanzaro', 'Ashwath Aithal', 'Nima Tajbakhsh', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2504.11409.jpg', 'data': {'categories': ['#architecture', '#small_models', '#optimization', '#inference', '#training'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² SSM. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¸Ğ¼ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Nemotron-H 8B Ğ´Ğ¾ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ² Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Efficient Hybrid Models: Pruning for Performance and Accuracy', 'desc': 'This paper discusses a new method for improving Hybrid LLM architectures that use both Attention and State Space Models (SSMs). The authors propose a group-aware pruning strategy that maintains the effectiveness of SSMs while reducing the model size. They show that this pruning, combined with knowledge distillation, leads to smaller models that are not only more accurate but also faster in making predictions. The results indicate that their approach can significantly enhance model performance while using fewer training resources.'}, 'zh': {'title': 'å‹ç¼©æ··åˆæ¨¡å‹ï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ··åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¶æ„çš„å‹ç¼©æ•ˆæœï¼Œè¿™äº›æ¶æ„ç»“åˆäº†æ³¨æ„åŠ›æœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¾¤ä½“æ„ŸçŸ¥å‰ªæç­–ç•¥ï¼Œèƒ½å¤Ÿä¿æŒSSMæ¨¡å—çš„ç»“æ„å®Œæ•´æ€§å’Œåºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§å‹ç¼©æ–¹æ³•ï¼Œæˆ‘ä»¬å°†Nemotron-H 8Bæ··åˆæ¨¡å‹å‹ç¼©åˆ°4Bå‚æ•°ï¼Œå¹¶å‡å°‘äº†è®­ç»ƒæ‰€éœ€çš„æ ‡è®°æ•°é‡ã€‚æœ€ç»ˆå¾—åˆ°çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ¨ç†é€Ÿåº¦ä¸Šå‡ä¼˜äºåŒç­‰è§„æ¨¡çš„æ¨¡å‹ï¼Œæ˜¾è‘—æ¨åŠ¨äº†æ¨¡å‹æ€§èƒ½çš„è¾¹ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09454', 'title': 'D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation', 'url': 'https://huggingface.co/papers/2504.09454', 'abstract': 'Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce a novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D^2iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves a unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at https://github.com/jiawn-creator/Dynamic-DiT.', 'score': 3, 'issue_id': 3265, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': '4e07861591445707', 'authors': ['Weinan Jia', 'Mengqi Huang', 'Nan Chen', 'Lei Zhang', 'Zhendong Mao'], 'affiliations': ['Institute of Artificial intelligence, Hefei Comprehensive National Science Center, Hefei, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.09454.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#open_source', '#cv', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿, Dynamic VAE, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¸Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿, Dynamic Diffusion Transformer (D^2iT), Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑˆÑƒĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Dynamic Compression for Enhanced Image Generation', 'desc': 'This paper introduces a new approach to improve image generation using diffusion models by dynamically compressing different regions of an image based on their information density. The proposed two-stage framework consists of a Dynamic VAE that encodes image regions at varying downsampling rates, allowing for more accurate latent representations. The second stage, Dynamic Diffusion Transformer (D^2iT), generates images by predicting noise at multiple granularities, balancing the need for detail in complex areas and simplicity in smoother regions. This method enhances both local realism and global consistency in generated images, as demonstrated through extensive experiments.'}, 'zh': {'title': 'åŠ¨æ€å‹ç¼©ï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡', 'desc': 'æ‰©æ•£æ¨¡å‹å› å…¶ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›è®¤å¯ã€‚å°½ç®¡æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¶æ„è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¯¹ä¸åŒå›¾åƒåŒºåŸŸåº”ç”¨å›ºå®šå‹ç¼©ï¼Œå¿½è§†äº†è¿™äº›åŒºåŸŸä¿¡æ¯å¯†åº¦çš„è‡ªç„¶å˜åŒ–ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡åŠ¨æ€å‹ç¼©ä¸åŒå›¾åƒåŒºåŸŸæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶æ¥æé«˜å›¾åƒç”Ÿæˆçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ç¬¬ä¸€é˜¶æ®µçš„åŠ¨æ€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆDVAEï¼‰å’Œç¬¬äºŒé˜¶æ®µçš„åŠ¨æ€æ‰©æ•£å˜æ¢å™¨ï¼ˆD^2iTï¼‰ç»“åˆäº†ç²—ç»†å™ªå£°é¢„æµ‹ï¼ŒæˆåŠŸå®ç°äº†å…¨å±€ä¸€è‡´æ€§ä¸å±€éƒ¨çœŸå®æ„Ÿçš„ç»Ÿä¸€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06949', 'title': 'Adaptive Computation Pruning for the Forgetting Transformer', 'url': 'https://huggingface.co/papers/2504.06949', 'abstract': 'The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox.', 'score': 3, 'issue_id': 3259, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': 'bda352daa194f6f8', 'authors': ['Zhixuan Lin', 'Johan Obando-Ceron', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2504.06949.jpg', 'data': {'categories': ['#architecture', '#inference', '#training', '#optimization'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ğ½Ğ¾ Ğ½Ğµ Ñ…ÑƒĞ¶Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Adaptive Computation Pruning (ACP) - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Forgetting Transformer (FoX). ACP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ½Ğ° 70% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 10-35%, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ»Ğ¸Ğ½ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Boosting Efficiency with Adaptive Computation Pruning in FoX', 'desc': "The paper introduces the Forgetting Transformer (FoX), which enhances softmax attention by integrating a forget gate, leading to improved performance over traditional RoPE-based Transformers. It observes that many attention heads in FoX forget information quickly, focusing more on local context. To address this, the authors propose Adaptive Computation Pruning (ACP), which dynamically reduces unnecessary computations based on the forget gate's influence. This method significantly decreases the number of floating-point operations (FLOPs) during language model pretraining, improving training speed by 10% to 35% without sacrificing model performance."}, 'zh': {'title': 'è‡ªé€‚åº”è®¡ç®—å‰ªææå‡FoXæ•ˆç‡', 'desc': 'æœ€è¿‘æå‡ºçš„é—å¿˜å˜æ¢å™¨ï¼ˆFoXï¼‰åœ¨è½¯æœ€å¤§æ³¨æ„åŠ›ä¸­å¼•å…¥äº†é—å¿˜é—¨ï¼Œä¸æ ‡å‡†çš„RoPEå˜æ¢å™¨ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚FoXä¸­çš„è®¸å¤šæ³¨æ„åŠ›å¤´å¿«é€Ÿé—å¿˜ï¼Œä½¿å¾—å®ƒä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä¸»è¦ä¾èµ–äºå±€éƒ¨ä¸Šä¸‹æ–‡ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”è®¡ç®—å‰ªæï¼ˆACPï¼‰æ–¹æ³•ï¼ŒåŠ¨æ€å‰ªé™¤ä¸è¾“å…¥è¾“å‡ºä¾èµ–å…³ç³»å¼ºçƒˆè¡°å‡çš„è®¡ç®—ã€‚é€šè¿‡åœ¨FoXçš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­åº”ç”¨ACPï¼Œæˆ‘ä»¬å®ç°äº†çº¦70%çš„è®¡ç®—é‡å‡å°‘ï¼ŒåŒæ—¶è®­ç»ƒååé‡æé«˜äº†10%åˆ°35%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11042', 'title': 'LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews', 'url': 'https://huggingface.co/papers/2504.11042', 'abstract': "Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LazyReview, a dataset of peer-review sentences annotated with fine-grained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community. (Code available here: https://github.com/UKPLab/arxiv2025-lazy-review)", 'score': 2, 'issue_id': 3268, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'd40c91fff861e80f', 'authors': ['Sukannya Purkayastha', 'Zhuang Li', 'Anne Lauscher', 'Lizhen Qu', 'Iryna Gurevych'], 'affiliations': ['Data Science Group, University of Hamburg', 'Department of Data Science & AI, Monash University, Australia', 'School of Computing Technologies, Royal Melbourne Institute of Technology, Australia', 'Ubiquitous Knowledge Processing Lab, Department of Computer Science and Hessian Center for AI (hessian.AI), Technical University of Darmstadt'], 'pdf_title_img': 'assets/pdf/title_img/2504.11042.jpg', 'data': {'categories': ['#open_source', '#data', '#dataset', '#science', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ»ĞµĞ½Ğ¸Ğ²Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° "Ğ»ĞµĞ½Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ" Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LazyReview, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ»ĞµĞ½Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¸, Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ»ĞµĞ½Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Peer Review Quality with LazyReview Dataset', 'desc': 'This paper addresses the problem of lazy thinking in peer review, which can degrade the quality of scientific evaluations. It introduces LazyReview, a new dataset containing peer-review sentences annotated with specific categories of lazy thinking. The study shows that Large Language Models (LLMs) have difficulty identifying these lazy thinking instances without prior training. However, by fine-tuning LLMs on the LazyReview dataset, their performance improves significantly, demonstrating the value of high-quality annotated data for enhancing peer review processes.'}, 'zh': {'title': 'æå‡åŒè¡Œè¯„å®¡è´¨é‡ï¼Œæ‰“å‡»æ‡’æƒ°æ€ç»´ï¼', 'desc': 'åŒè¡Œè¯„å®¡æ˜¯ç§‘å­¦å‡ºç‰ˆè´¨é‡æ§åˆ¶çš„é‡è¦ç¯èŠ‚ã€‚éšç€å·¥ä½œé‡çš„å¢åŠ ï¼Œè¯„å®¡ä¸­å‡ºç°äº†ä½¿ç”¨å¿«é€Ÿå¯å‘å¼æ–¹æ³•çš„ç°è±¡ï¼Œè¿™è¢«ç§°ä¸ºæ‡’æƒ°æ€ç»´ï¼Œå½±å“äº†è¯„å®¡è´¨é‡ã€‚æœ¬æ–‡ä»‹ç»äº†LazyReviewæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ ‡æ³¨äº†ç»†ç²’åº¦æ‡’æƒ°æ€ç»´ç±»åˆ«çš„åŒè¡Œè¯„å®¡å¥å­ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹éš¾ä»¥æ£€æµ‹è¿™äº›å®ä¾‹ï¼Œä½†åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºäºæŒ‡ä»¤çš„å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10443', 'title': 'Multimodal Long Video Modeling Based on Temporal Dynamic Context', 'url': 'https://huggingface.co/papers/2504.10443', 'abstract': 'Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent methods are designed for long video understanding, they often lose crucial information during token compression and struggle with additional modality like audio. In this work, we propose a dynamic long video encoding method utilizing the temporal relationship between frames, named Temporal Dynamic Context (TDC). Firstly, we segment the video into semantically consistent scenes based on inter-frame similarities, then encode each frame into tokens using visual-audio encoders. Secondly, we propose a novel temporal context compressor to reduce the number of tokens within each segment. Specifically, we employ a query-based Transformer to aggregate video, audio, and instruction text tokens into a limited set of temporal context tokens. Finally, we feed the static frame tokens and the temporal context tokens into the LLM for video understanding. Furthermore, to handle extremely long videos, we propose a training-free chain-of-thought strategy that progressively extracts answers from multiple video segments. These intermediate answers serve as part of the reasoning process and contribute to the final answer. We conduct extensive experiments on general video understanding and audio-video understanding benchmarks, where our method demonstrates strong performance. The code and models are available at https://github.com/Hoar012/TDC-Video.', 'score': 2, 'issue_id': 3268, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'a6b528cbce95d1ef', 'authors': ['Haoran Hao', 'Jiaming Han', 'Yiyuan Zhang', 'Xiangyu Yue'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10443.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#multimodal', '#video', '#benchmark'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Temporal Dynamic Context (TDC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ”Ğ»Ñ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Long Video Understanding with Temporal Dynamic Context', 'desc': 'This paper introduces a new method called Temporal Dynamic Context (TDC) for improving the understanding of long videos using Large Language Models (LLMs). The approach segments videos into meaningful scenes and encodes each frame with visual and audio information, addressing the limitations of existing models that struggle with long contexts. A novel temporal context compressor is used to reduce the number of tokens while preserving essential information, allowing for better integration of video, audio, and text data. Additionally, a training-free chain-of-thought strategy is proposed to extract answers progressively from different video segments, enhancing the reasoning process for video understanding tasks.'}, 'zh': {'title': 'åŠ¨æ€é•¿è§†é¢‘ç¼–ç ï¼Œæå‡è§†é¢‘ç†è§£èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŠ¨æ€é•¿è§†é¢‘ç¼–ç æ–¹æ³•ï¼Œç§°ä¸ºæ—¶é—´åŠ¨æ€ä¸Šä¸‹æ–‡ï¼ˆTDCï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºå¸§é—´ç›¸ä¼¼æ€§å°†è§†é¢‘åˆ†å‰²ä¸ºè¯­ä¹‰ä¸€è‡´çš„åœºæ™¯ï¼Œå¹¶ä½¿ç”¨è§†è§‰-éŸ³é¢‘ç¼–ç å™¨å°†æ¯å¸§ç¼–ç ä¸ºæ ‡è®°ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ—¶é—´ä¸Šä¸‹æ–‡å‹ç¼©å™¨ï¼Œåˆ©ç”¨åŸºäºæŸ¥è¯¢çš„Transformerå°†è§†é¢‘ã€éŸ³é¢‘å’ŒæŒ‡ä»¤æ–‡æœ¬æ ‡è®°èšåˆä¸ºæœ‰é™çš„æ—¶é—´ä¸Šä¸‹æ–‡æ ‡è®°ã€‚æœ€åï¼Œæˆ‘ä»¬å°†é™æ€å¸§æ ‡è®°å’Œæ—¶é—´ä¸Šä¸‹æ–‡æ ‡è®°è¾“å…¥åˆ°å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥å®ç°è§†é¢‘ç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10049', 'title': 'Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure', 'url': 'https://huggingface.co/papers/2504.10049', 'abstract': 'Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of automatic summarization of multimodal presentations using VLMs with various representations as input. From these experiments, we suggest cost-effective strategies for generating summaries from text-heavy multimodal documents under different input-length budgets using VLMs. We show that slides extracted from the video stream can be beneficially used as input against the raw video, and that a structured representation from interleaved slides and transcript provides the best performance. Finally, we reflect and comment on the nature of cross-modal interactions in multimodal presentations and share suggestions to improve the capabilities of VLMs to understand documents of this nature.', 'score': 2, 'issue_id': 3264, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '49c72fe8f6e82ce0', 'authors': ['ThÃ©o Gigant', 'Camille Guinaudeau', 'FrÃ©dÃ©ric Dufaux'], 'affiliations': ['UniversitÃ© Paris-Saclay, CNRS, CentraleSupelec, Laboratoire des signaux et systemes, France', 'UniversitÃ© Paris-Saclay, CNRS, LISN, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.10049.jpg', 'data': {'categories': ['#optimization', '#video', '#interpretability', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ (VLM), Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² VLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑĞ¼Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ°Ğ¹Ğ´Ğ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼, Ñ‡ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‹Ñ€Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞ»Ğ°Ğ¹Ğ´Ñ‹ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚.'}, 'en': {'title': 'Enhancing Summarization of Multimodal Presentations with VLMs', 'desc': 'This paper explores how Vision-Language Models (VLMs) can effectively summarize multimodal presentations that include text and images. The authors analyze different input formats and their impact on summarization quality, revealing that using slides from videos can enhance performance compared to raw video input. They propose strategies for optimizing summary generation based on varying input lengths, emphasizing the importance of structured representations. Additionally, the paper discusses the interactions between visual and textual data in these presentations and offers insights for improving VLM capabilities.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ–‡æ¡£æ‘˜è¦çš„æ™ºèƒ½ç­–ç•¥', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ï¼‰æ—¶çš„è‡ªåŠ¨æ‘˜è¦èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›è¡Œäº†ç»†è‡´çš„å®šé‡å’Œå®šæ€§åˆ†æï¼Œæ¢è®¨äº†å¦‚ä½•åœ¨ä¸åŒè¾“å…¥é•¿åº¦é¢„ç®—ä¸‹ï¼Œä»æ–‡æœ¬å¯†é›†çš„å¤šæ¨¡æ€æ–‡æ¡£ä¸­ç”Ÿæˆæ‘˜è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»è§†é¢‘æµä¸­æå–çš„å¹»ç¯ç‰‡ä½œä¸ºè¾“å…¥æ¯”åŸå§‹è§†é¢‘æ›´æœ‰æ•ˆï¼Œè€Œäº¤é”™çš„å¹»ç¯ç‰‡å’Œè½¬å½•æ–‡æœ¬çš„ç»“æ„åŒ–è¡¨ç¤ºåˆ™æä¾›äº†æœ€ä½³æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å¤šæ¨¡æ€æ¼”ç¤ºä¸­çš„è·¨æ¨¡æ€äº¤äº’ç‰¹æ€§ï¼Œå¹¶æå‡ºäº†æ”¹è¿›VLMsç†è§£æ­¤ç±»æ–‡æ¡£èƒ½åŠ›çš„å»ºè®®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11457', 'title': 'Aligning Generative Denoising with Discriminative Objectives Unleashes\n  Diffusion for Visual Perception', 'url': 'https://huggingface.co/papers/2504.11457', 'abstract': 'With the success of image generation, generative diffusion models are increasingly adopted for discriminative tasks, as pixel generation provides a unified perception interface. However, directly repurposing the generative denoising process for discriminative objectives reveals critical gaps rarely addressed previously. Generative models tolerate intermediate sampling errors if the final distribution remains plausible, but discriminative tasks require rigorous accuracy throughout, as evidenced in challenging multi-modal tasks like referring image segmentation. Motivated by this gap, we analyze and enhance alignment between generative diffusion processes and perception tasks, focusing on how perception quality evolves during denoising. We find: (1) earlier denoising steps contribute disproportionately to perception quality, prompting us to propose tailored learning objectives reflecting varying timestep contributions; (2) later denoising steps show unexpected perception degradation, highlighting sensitivity to training-denoising distribution shifts, addressed by our diffusion-tailored data augmentation; and (3) generative processes uniquely enable interactivity, serving as controllable user interfaces adaptable to correctional prompts in multi-round interactions. Our insights significantly improve diffusion-based perception models without architectural changes, achieving state-of-the-art performance on depth estimation, referring image segmentation, and generalist perception tasks. Code available at https://github.com/ziqipang/ADDP.', 'score': 0, 'issue_id': 3272, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '3aac006cdbd11ccd', 'authors': ['Ziqi Pang', 'Xin Xu', 'Yu-Xiong Wang'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.11457.jpg', 'data': {'categories': ['#training', '#multimodal', '#alignment', '#cv', '#diffusion'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ²ĞºĞ»Ğ°Ğ´ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğº ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼.'}, 'en': {'title': 'Bridging Generative and Discriminative Learning for Enhanced Perception', 'desc': 'This paper explores how generative diffusion models, which are typically used for creating images, can be adapted for tasks that require precise classification and segmentation. The authors identify that while generative models can handle some errors during the image creation process, discriminative tasks need consistent accuracy at every step. They propose new learning objectives that focus on the importance of early denoising steps and address issues that arise in later steps due to shifts in training data. Their findings lead to significant improvements in performance for various perception tasks without changing the underlying model architecture.'}, 'zh': {'title': 'æå‡ç”Ÿæˆæ¨¡å‹åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­çš„æ„ŸçŸ¥èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç”ŸæˆæˆåŠŸåã€‚ç ”ç©¶å‘ç°ï¼Œç›´æ¥å°†ç”Ÿæˆå»å™ªè¿‡ç¨‹ç”¨äºåˆ¤åˆ«ç›®æ ‡æ—¶å­˜åœ¨é‡è¦çš„å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ã€‚è®ºæ–‡åˆ†æäº†ç”Ÿæˆæ‰©æ•£è¿‡ç¨‹ä¸æ„ŸçŸ¥ä»»åŠ¡ä¹‹é—´çš„å¯¹é½ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹ä¸åŒæ—¶é—´æ­¥è´¡çŒ®çš„å­¦ä¹ ç›®æ ‡ã€‚é€šè¿‡æ”¹è¿›æ•°æ®å¢å¼ºå’Œå­¦ä¹ ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†åŸºäºæ‰©æ•£çš„æ„ŸçŸ¥æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11080', 'title': 'Change State Space Models for Remote Sensing Change Detection', 'url': 'https://huggingface.co/papers/2504.11080', 'abstract': 'Despite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Vision Mamba, an architecture based on State Space Models has emerged as an alternative addressing the aforementioned deficiencies and has been already applied to remote sensing change detection, though mostly as a feature extracting backbone. In this article the Change State Space Model is introduced, that has been specifically designed for change detection by focusing on the relevant changes between bi-temporal images, effectively filtering out irrelevant information. By concentrating solely on the changed features, the number of network parameters is reduced, enhancing significantly computational efficiency while maintaining high detection performance and robustness against input degradation. The proposed model has been evaluated via three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based counterparts at a fraction of their computational complexity. The implementation will be made available at https://github.com/Elman295/CSSM upon acceptance.', 'score': 0, 'issue_id': 3268, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'abf55fd00ae45494', 'authors': ['Elman Ghazaei', 'Erchan Aptoula'], 'affiliations': ['Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, TÃ¼rkiye'], 'pdf_title_img': 'assets/pdf/title_img/2504.11080.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ State Space Models', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Change State Space Model Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒÑ Ğ½ĞµÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ…, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑĞµÑ‚Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸, Vision Transformers Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Mamba Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ¼ĞµÑ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Efficient Change Detection with the Change State Space Model', 'desc': 'This paper introduces the Change State Space Model (CSSM), a new architecture designed specifically for change detection in bi-temporal images. Unlike traditional ConvNets and Vision Transformers, which struggle with long-range dependencies and computational efficiency, CSSM effectively filters out irrelevant information to focus on significant changes. By reducing the number of network parameters, CSSM enhances computational efficiency while maintaining high detection performance and robustness against input degradation. The model has been tested on three benchmark datasets, outperforming existing methods with lower computational complexity.'}, 'zh': {'title': 'é«˜æ•ˆå˜åŒ–æ£€æµ‹çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å˜åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆChange State Space Modelï¼‰ï¼Œä¸“é—¨ç”¨äºå›¾åƒå˜åŒ–æ£€æµ‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å…³æ³¨åŒæ—¶ç›¸å›¾åƒä¹‹é—´çš„ç›¸å…³å˜åŒ–ï¼Œæœ‰æ•ˆè¿‡æ»¤æ‰æ— å…³ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetsï¼‰å’Œè§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒé«˜æ£€æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†ç½‘ç»œå‚æ•°æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„æ¨¡å‹ï¼Œä¸”è®¡ç®—å¤æ‚åº¦æ›´ä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07491', 'title': 'Kimi-VL Technical Report', 'url': 'https://huggingface.co/papers/2504.07491', 'abstract': 'We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.', 'score': 89, 'issue_id': 3185, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'e1d1e4add50955e8', 'authors': ['Kimi Team', 'Angang Du', 'Bohong Yin', 'Bowei Xing', 'Bowen Qu', 'Bowen Wang', 'Cheng Chen', 'Chenlin Zhang', 'Chenzhuang Du', 'Chu Wei', 'Congcong Wang', 'Dehao Zhang', 'Dikang Du', 'Dongliang Wang', 'Enming Yuan', 'Enzhe Lu', 'Fang Li', 'Flood Sung', 'Guangda Wei', 'Guokun Lai', 'Han Zhu', 'Hao Ding', 'Hao Hu', 'Hao Yang', 'Hao Zhang', 'Haoning Wu', 'Haotian Yao', 'Haoyu Lu', 'Heng Wang', 'Hongcheng Gao', 'Huabin Zheng', 'Jiaming Li', 'Jianlin Su', 'Jianzhou Wang', 'Jiaqi Deng', 'Jiezhong Qiu', 'Jin Xie', 'Jinhong Wang', 'Jingyuan Liu', 'Junjie Yan', 'Kun Ouyang', 'Liang Chen', 'Lin Sui', 'Longhui Yu', 'Mengfan Dong', 'Mengnan Dong', 'Nuo Xu', 'Pengyu Cheng', 'Qizheng Gu', 'Runjie Zhou', 'Shaowei Liu', 'Sihan Cao', 'Tao Yu', 'Tianhui Song', 'Tongtong Bai', 'Wei Song', 'Weiran He', 'Weixiao Huang', 'Weixin Xu', 'Xiaokun Yuan', 'Xingcheng Yao', 'Xingzhe Wu', 'Xinxing Zu', 'Xinyu Zhou', 'Xinyuan Wang', 'Y. Charles', 'Yan Zhong', 'Yang Li', 'Yangyang Hu', 'Yanru Chen', 'Yejie Wang', 'Yibo Liu', 'Yibo Miao', 'Yidao Qin', 'Yimin Chen', 'Yiping Bao', 'Yiqin Wang', 'Yongsheng Kang', 'Yuanxin Liu', 'Yulun Du', 'Yuxin Wu', 'Yuzhi Wang', 'Yuzi Yan', 'Zaida Zhou', 'Zhaowei Li', 'Zhejun Jiang', 'Zheng Zhang', 'Zhilin Yang', 'Zhiqi Huang', 'Zihao Huang', 'Zijia Zhao', 'Ziwei Chen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.07491.jpg', 'data': {'categories': ['#cv', '#agents', '#rl', '#multimodal', '#long_context', '#reasoning', '#small_models', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Kimi-VL: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Kimi-VL - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mixture-of-Experts (MoE) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Kimi-VL Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ² 128K Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Kimi-VL-Thinking, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Kimi-VL: Efficient Multimodal Mastery with Long-Context Reasoning', 'desc': 'Kimi-VL is a cutting-edge Mixture-of-Experts vision-language model that efficiently combines multimodal reasoning and long-context understanding while using only 2.8 billion parameters in its language decoder. It excels in various complex tasks, including multi-turn interactions and advanced image and video comprehension, outperforming other leading models in several areas. The model features a 128K extended context window, allowing it to process long inputs effectively, and its native-resolution vision encoder enhances its ability to interpret high-resolution visuals. Additionally, the Kimi-VL-Thinking variant improves long-horizon reasoning through supervised fine-tuning and reinforcement learning, setting a new benchmark for efficient multimodal models.'}, 'zh': {'title': 'Kimi-VLï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æ–°æ ‡å‡†', 'desc': 'Kimi-VLæ˜¯ä¸€ç§é«˜æ•ˆçš„å¼€æºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ¨ç†å’Œé•¿æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä»»åŠ¡å’Œå„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä¸é¡¶å°–æ¨¡å‹ç›¸åª²ç¾ã€‚Kimi-VLè¿˜å…·å¤‡å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å¤šè¾¾128Kçš„è¾“å…¥ï¼Œé€‚ç”¨äºå¤æ‚çš„è§†è§‰ç†è§£ä»»åŠ¡ã€‚é€šè¿‡é•¿é“¾æ€ç»´çš„ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼ŒKimi-VL-Thinkingè¿›ä¸€æ­¥æå‡äº†é•¿è¿œæ¨ç†èƒ½åŠ›ï¼Œè®¾å®šäº†é«˜æ•ˆå¤šæ¨¡æ€æ€ç»´æ¨¡å‹çš„æ–°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07128', 'title': "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning", 'url': 'https://huggingface.co/papers/2504.07128', 'abstract': 'Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly "thinking" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1\'s basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a \'sweet spot\' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.', 'score': 59, 'issue_id': 3184, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '6317a88ae7643fe2', 'authors': ['Sara Vera MarjanoviÄ‡', 'Arkil Patel', 'Vaibhav Adlakha', 'Milad Aghajohari', 'Parishad BehnamGhader', 'Mehar Bhatia', 'Aditi Khandelwal', 'Austin Kraft', 'Benno Krojer', 'Xing Han LÃ¹', 'Nicholas Meade', 'Dongchan Shin', 'Amirhossein Kazemnejad', 'Gaurav Kamath', 'Marius Mosbach', 'Karolina StaÅ„czak', 'Siva Reddy'], 'affiliations': ['McGill University', 'Mila Quebec AI Institute', 'University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2504.07128.jpg', 'data': {'categories': ['#architecture', '#ethics', '#inference', '#reasoning', '#long_context', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'DeepSeek-R1: Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ DeepSeek-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñƒ DeepSeek-R1 ĞµÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞ°Ñ‚ÑŒÑÑ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°ÑÑ‚Ñ€ĞµĞ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ½ĞµĞµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'DeepSeek-R1: Revolutionizing Reasoning in Language Models', 'desc': "The paper introduces DeepSeek-R1, a Large Reasoning Model that enhances the way language models (LLMs) tackle complex problems by generating multi-step reasoning chains. This model allows users to observe the reasoning process, fostering a new area of research called Thoughtology. The study examines various aspects of DeepSeek-R1, including the effects of reasoning length, context management, and safety issues, revealing that excessive inference time can negatively impact performance. Additionally, the findings highlight the model's tendency to dwell on previous problem formulations, which can hinder its ability to explore new solutions and raise safety concerns compared to traditional LLMs."}, 'zh': {'title': 'æ·±åº¦æ¨ç†ï¼Œæ€ç»´çš„æœªæ¥', 'desc': 'DeepSeek-R1æ˜¯ä¸€ç§å¤§å‹æ¨ç†æ¨¡å‹ï¼Œæ ‡å¿—ç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶çš„æ ¹æœ¬è½¬å˜ã€‚å®ƒé€šè¿‡åˆ›å»ºè¯¦ç»†çš„å¤šæ­¥éª¤æ¨ç†é“¾æ¥â€œæ€è€ƒâ€é—®é¢˜ï¼Œè€Œä¸æ˜¯ç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚è¿™ç§æ¨ç†è¿‡ç¨‹å¯¹ç”¨æˆ·æ˜¯å…¬å¼€çš„ï¼Œä¸ºç ”ç©¶æ¨¡å‹çš„æ¨ç†è¡Œä¸ºæä¾›äº†æ— é™å¯èƒ½ï¼Œå¹¶å¼€å¯äº†æ€ç»´å­¦ï¼ˆThoughtologyï¼‰é¢†åŸŸã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼ŒDeepSeek-R1åœ¨æ¨ç†æ—¶å­˜åœ¨ä¸€ä¸ªâ€œç”œèœœç‚¹â€ï¼Œè¿‡é•¿çš„æ¨ç†æ—¶é—´å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶å®ƒåœ¨å¤„ç†å·²æ¢ç´¢çš„é—®é¢˜æ—¶å®¹æ˜“é™·å…¥åå¤æ€è€ƒï¼Œå½±å“è¿›ä¸€æ­¥çš„æ¢ç´¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07964', 'title': 'C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing', 'url': 'https://huggingface.co/papers/2504.07964', 'abstract': 'Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or "re-mixing" the experts in different layers jointly for each test sample. Since the test sample\'s ground truth is unknown, we propose to optimize a surrogate objective defined by the sample\'s "successful neighbors" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts\' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to "Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE\'s advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.', 'score': 49, 'issue_id': 3184, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '1f6d28b26eec9879', 'authors': ['Zhongyang Li', 'Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['Johns Hopkins University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2504.07964.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE LLM) Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10-20%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ C3PO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° 'ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… ÑĞ¾ÑĞµĞ´ÑÑ…' Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ C3PO Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ MoE LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 7-15% Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."}, 'en': {'title': 'Optimize Expert Pathways for Better Performance!', 'desc': 'This paper addresses the issue of sub-optimal expert pathways in Mixture-of-Experts (MoE) Large Language Models (LLMs), which can lead to a significant accuracy gap during inference. The authors propose a new method called C3PO, which optimizes the mixing weights of core experts in critical layers for each test sample using surrogate objectives based on similar reference samples. By focusing on optimizing only the essential components, C3PO achieves notable accuracy improvements of 7-15% over baseline models while maintaining computational efficiency. The results demonstrate that C3PO allows smaller MoE models to outperform larger LLMs, highlighting its effectiveness in enhancing model performance at lower resource costs.'}, 'zh': {'title': 'ä¼˜åŒ–æ··åˆä¸“å®¶æ¨¡å‹çš„å…³é”®è·¯å¾„', 'desc': 'æ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸“å®¶è·¯å¾„é€‰æ‹©ä¸Šå­˜åœ¨æ˜¾è‘—çš„ä¼˜åŒ–ä¸è¶³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œç®€å•çš„ä¸“å®¶é€‰æ‹©æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µä¼šå¯¼è‡´10-20%çš„å‡†ç¡®ç‡å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬çš„ä¸åŒå±‚æ¬¡çš„ä¸“å®¶è¿›è¡Œé‡æ–°åŠ æƒæˆ–â€œé‡æ–°æ··åˆâ€ã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºâ€œå…³é”®å±‚ã€æ ¸å¿ƒä¸“å®¶ã€åä½œè·¯å¾„ä¼˜åŒ–ï¼ˆC3POï¼‰â€ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºä¼ ç»Ÿçš„å­¦ä¹ æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07956', 'title': 'VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2504.07956', 'abstract': "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.", 'score': 37, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '88860725e51f3629', 'authors': ['Yukun Qi', 'Yiming Zhao', 'Yu Zeng', 'Xikun Bao', 'Wenxuan Huang', 'Lin Chen', 'Zehui Chen', 'Jie Zhao', 'Zhongang Qi', 'Feng Zhao'], 'affiliations': ['East China Normal University', 'Huawei Noahs Ark Lab', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.07956.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VCR-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VCR-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 859 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 1034 Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. VCR-Bench Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'VCR-Bench: Evaluating Video Reasoning in LVLMs', 'desc': 'This paper introduces VCR-Bench, a new benchmark for evaluating Video Chain-of-Thought (CoT) reasoning in large vision-language models (LVLMs). It addresses the lack of rigorous evaluation frameworks for assessing how well these models can reason about video content. The benchmark includes 859 videos and 1,034 annotated question-answer pairs, each with a stepwise CoT rationale linked to perception or reasoning capabilities. Experiments reveal that current LVLMs struggle with video reasoning, particularly in processing temporal-spatial information, highlighting the need for improved models in this area.'}, 'zh': {'title': 'VCR-Benchï¼šè§†é¢‘æ¨ç†çš„æ–°æ ‡å‡†', 'desc': 'é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªä¸¥æ ¼çš„è§†é¢‘CoTæ¨ç†è¯„ä¼°æ¡†æ¶ï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†æ— æ³•å……åˆ†è¯„ä¼°æ¨ç†è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VCR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°LVLMsåœ¨è§†é¢‘é“¾å¼æ€ç»´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡859ä¸ªè§†é¢‘å’Œ1034å¯¹é«˜è´¨é‡é—®ç­”å¯¹ï¼ŒVCR-Benchä¸ºæ¯ä¸ªé—®ç­”å¯¹æä¾›äº†é€æ­¥çš„CoTæ¨ç†ä¾æ®ï¼Œæ­ç¤ºäº†å½“å‰LVLMsåœ¨å¤æ‚è§†é¢‘æ¨ç†ä¸­çš„å…³é”®ç“¶é¢ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07960', 'title': 'VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning', 'url': 'https://huggingface.co/papers/2504.07960', 'abstract': 'Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.', 'score': 36, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'a8b5331ac40d6a3d', 'authors': ['Zhong-Yu Li', 'Ruoyi Du', 'Juncheng Yan', 'Le Zhuo', 'Zhen Li', 'Peng Gao', 'Zhanyu Ma', 'Ming-Ming Cheng'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University', 'VCIP, CS, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07960.jpg', 'data': {'categories': ['#cv', '#graphs', '#transfer_learning', '#diffusion', '#multimodal', '#dataset'], 'emoji': 'ğŸ¨', 'ru': {'title': 'VisualCloze: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'VisualCloze - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Graph200K - Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. VisualCloze Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'VisualCloze: Bridging Tasks with Visual Learning in Image Generation', 'desc': 'This paper introduces VisualCloze, a universal image generation framework that overcomes the limitations of task-specific models by supporting a variety of in-domain tasks and generalizing to unseen tasks. It addresses challenges such as task instruction ambiguity and sparse task distributions by utilizing visual in-context learning, which allows models to learn from visual examples rather than language instructions. The authors also present Graph200K, a dataset that enhances task density and facilitates knowledge transfer across related tasks. Additionally, they demonstrate that their unified image generation approach aligns with image infilling, enabling the use of pre-trained models for improved generative performance.'}, 'zh': {'title': 'VisualClozeï¼šé€šç”¨å›¾åƒç”Ÿæˆçš„æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVisualClozeçš„é€šç”¨å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å­˜åœ¨çš„æ•ˆç‡å’Œé€šç”¨æ€§é—®é¢˜ã€‚ä¸ä¼ ç»Ÿä¾èµ–è¯­è¨€æŒ‡ä»¤çš„æ–¹æ³•ä¸åŒï¼ŒVisualClozeé€šè¿‡è§†è§‰ç¤ºä¾‹è¿›è¡Œä»»åŠ¡è¯†åˆ«ï¼Œä»è€Œå‡å°‘äº†ä»»åŠ¡æ¨¡ç³Šæ€§å’Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å¢å¼ºä»»åŠ¡ä¹‹é—´çš„å¯è½¬ç§»çŸ¥è¯†ï¼Œæˆ‘ä»¬å¼•å…¥äº†Graph200Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡å›¾ç»“æ„å»ºç«‹äº†å¤šç§ç›¸å…³ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å›¾åƒç”Ÿæˆæ–¹æ³•ä¸å›¾åƒå¡«å……å…·æœ‰ä¸€è‡´çš„ç›®æ ‡ï¼Œä»è€Œèƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒå¡«å……æ¨¡å‹çš„å¼ºç”Ÿæˆå…ˆéªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07957', 'title': 'MM-IFEngine: Towards Multimodal Instruction Following', 'url': 'https://huggingface.co/papers/2504.07957', 'abstract': 'The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.', 'score': 29, 'issue_id': 3184, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'dbfc6cfeb60e05d7', 'authors': ['Shengyuan Ding', 'Shenxi Wu', 'Xiangyu Zhao', 'Yuhang Zang', 'Haodong Duan', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'Fudan University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.07957.jpg', 'data': {'categories': ['#benchmark', '#training', '#dataset', '#data', '#optimization', '#open_source', '#multimodal', '#alignment'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MM-IFEngine - Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MM-IFInstruct-23k Ğ¸ MM-IFDPO-23k Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MM-IFEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Instruction Following in MLLMs with MM-IFEngine', 'desc': 'This paper introduces MM-IFEngine, a new method for generating high-quality image-instruction pairs to improve the instruction-following ability of Multi-modal Large Language Models (MLLMs). The authors create a large dataset called MM-IFInstruct-23k, which is designed for Supervised Fine-Tuning (SFT) and an extended version for Direct Preference Optimization (DPO). They also present MM-IFEval, a benchmark that evaluates MLLMs on complex tasks with both output and input constraints. Experiments show that fine-tuning on their datasets significantly enhances performance on various instruction-following benchmarks.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšèƒ½åŠ›è¯„ä¼°æ–¹æ³•ï¼Œç§°ä¸ºMM-IFEngineã€‚è¯¥æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒ-æŒ‡ä»¤å¯¹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†MM-IFInstruct-23kï¼Œé€‚ç”¨äºç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–ã€‚è®ºæ–‡è¿˜æå‡ºäº†MM-IFEvalï¼Œä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å«è¾“å‡ºå“åº”å’Œè¾“å…¥å›¾åƒçš„çº¦æŸã€‚é€šè¿‡å®éªŒï¼Œå¾®è°ƒåçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07943', 'title': 'HoloPart: Generative 3D Part Amodal Segmentation', 'url': 'https://huggingface.co/papers/2504.07943', 'abstract': '3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.', 'score': 23, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '4cc1401ee10a171e', 'authors': ['Yunhan Yang', 'Yuan-Chen Guo', 'Yukun Huang', 'Zi-Xin Zou', 'Zhipeng Yu', 'Yangguang Li', 'Yan-Pei Cao', 'Xihui Liu'], 'affiliations': ['The University of Hong Kong', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2504.07943.jpg', 'data': {'categories': ['#3d', '#architecture', '#diffusion', '#benchmark', '#optimization'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸: Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ 3D-Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ HoloPart Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ 3D-Ñ‡Ğ°ÑÑ‚Ğ¸. HoloPart Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Hidden Shapes: HoloPart for 3D Amodal Segmentation', 'desc': 'This paper addresses the challenge of 3D part amodal segmentation, which involves identifying complete parts of a 3D shape even when some parts are hidden. Current methods only work with visible surfaces, limiting their effectiveness. The authors propose a two-stage approach that first uses existing segmentation techniques to identify incomplete parts, followed by a novel diffusion-based model called HoloPart to complete these segments. HoloPart employs a specialized architecture to ensure both detailed part geometry and overall shape consistency, achieving superior results on new benchmarks compared to existing methods.'}, 'zh': {'title': 'çªç ´3Dåˆ†å‰²ï¼šHoloPartæ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„3Déƒ¨åˆ†æ— æ¨¡æ€åˆ†å‰²ä»»åŠ¡ï¼Œæ—¨åœ¨å°†3Då½¢çŠ¶åˆ†è§£ä¸ºå®Œæ•´ä¸”å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„éƒ¨åˆ†ï¼Œå³ä½¿åœ¨è¢«é®æŒ¡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ã€‚ç°æœ‰çš„3Déƒ¨åˆ†åˆ†å‰²æ–¹æ³•ä»…èƒ½è¯†åˆ«å¯è§çš„è¡¨é¢ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé¦–å…ˆåˆ©ç”¨ç°æœ‰çš„3Déƒ¨åˆ†åˆ†å‰²è·å–åˆæ­¥çš„ä¸å®Œæ•´éƒ¨åˆ†ï¼Œç„¶åå¼•å…¥HoloPartæ¨¡å‹ï¼Œé€šè¿‡æ‰©æ•£æ–¹æ³•å®Œæˆè¿™äº›éƒ¨åˆ†ã€‚HoloParté‡‡ç”¨äº†ä¸“é—¨çš„æ¶æ„ï¼Œç»“åˆå±€éƒ¨æ³¨æ„åŠ›å’Œå…¨å±€å½¢çŠ¶ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡äº†3Déƒ¨åˆ†æ— æ¨¡æ€åˆ†å‰²çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07951', 'title': 'Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models', 'url': 'https://huggingface.co/papers/2504.07951', 'abstract': 'Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.', 'score': 15, 'issue_id': 3190, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '66dadd5828f14551', 'authors': ['Mustafa Shukor', 'Enrico Fini', 'Victor Guilherme Turrisi da Costa', 'Matthieu Cord', 'Joshua Susskind', 'Alaaeldin El-Nouby'], 'affiliations': ['Apple', 'Sorbonne University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07951.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#agi', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ¸Ñ€ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ´Ğ½ĞµĞ³Ğ¾ Ğ¸ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 457 Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğ¼, Ğ° Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¸Ñ…. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (Mixture of Experts) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ²ĞµÑĞ°Ğ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Early-Fusion Models Outperform Late-Fusion in Multimodal Learning', 'desc': 'This paper explores the effectiveness of native multimodal models (NMMs) compared to late-fusion architectures that combine pre-trained components. The authors conducted a large-scale study with 457 models to analyze the performance of early-fusion versus late-fusion approaches. They found that early-fusion models, which integrate modalities from the start, outperform late-fusion models in terms of efficiency and training ease. Additionally, by using Mixture of Experts (MoEs), the early-fusion models can learn specific weights for different modalities, further boosting their performance.'}, 'zh': {'title': 'æ—©èåˆæ¶æ„æ›´èƒœä¸€ç­¹ï¼', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€æ¨¡å‹çš„æ¶æ„è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯åŸç”Ÿå¤šæ¨¡æ€æ¨¡å‹ï¼ˆNMMsï¼‰ï¼Œè¿™äº›æ¨¡å‹ä»ä¸€å¼€å§‹å°±é’ˆå¯¹æ‰€æœ‰æ¨¡æ€è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å¯¹457ä¸ªä¸åŒæ¶æ„å’Œè®­ç»ƒç»„åˆçš„æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„è§„æ¨¡æ³•åˆ™ç ”ç©¶ã€‚ç»“æœæ˜¾ç¤ºï¼Œåèåˆæ¶æ„å¹¶æ²¡æœ‰æ¯”æ—©èåˆæ¶æ„å…·æœ‰å›ºæœ‰ä¼˜åŠ¿ï¼Œåè€…åœ¨è¾ƒä½çš„å‚æ•°æ•°é‡ä¸‹è¡¨ç°æ›´å¼ºï¼Œè®­ç»ƒæ•ˆç‡æ›´é«˜ï¼Œéƒ¨ç½²æ›´ç®€å•ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç»“åˆä¸“å®¶æ··åˆï¼ˆMoEsï¼‰å¯ä»¥è®©æ¨¡å‹å­¦ä¹ ç‰¹å®šæ¨¡æ€çš„æƒé‡ï¼Œä»è€Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07830', 'title': 'MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations', 'url': 'https://huggingface.co/papers/2504.07830', 'abstract': "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.", 'score': 15, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '85dbaddf009300e0', 'authors': ['Genglin Liu', 'Salman Rahman', 'Elisa Kreiss', 'Marzyeh Ghassemi', 'Saadia Gabriel'], 'affiliations': ['MIT CSAIL', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2504.07830.jpg', 'data': {'categories': ['#agents', '#graphs', '#reasoning', '#games', '#multimodal', '#open_source'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¾Ğ¼ Ğ˜Ğ˜', 'desc': 'MOSAIC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'MOSAIC: Simulating Social Networks to Combat Misinformation', 'desc': 'The paper introduces MOSAIC, an innovative open-source framework for simulating social networks using generative language agents. These agents predict user behaviors like liking and sharing content, allowing researchers to study how deception emerges in online interactions. By creating detailed user personas, the framework facilitates large-scale simulations of content spread and user engagement. The study evaluates various content moderation strategies, revealing that they can effectively reduce misinformation while enhancing user interaction.'}, 'zh': {'title': 'MOSAICï¼šç¤¾äº¤ç½‘ç»œè¡Œä¸ºæ¨¡æ‹Ÿä¸å†…å®¹å®¡æ ¸æ–°æ¢ç´¢', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼€æºç¤¾äº¤ç½‘ç»œæ¨¡æ‹Ÿæ¡†æ¶MOSAICï¼Œåˆ©ç”¨ç”Ÿæˆè¯­è¨€ä»£ç†é¢„æµ‹ç”¨æˆ·è¡Œä¸ºï¼Œå¦‚ç‚¹èµã€åˆ†äº«å’Œæ ‡è®°å†…å®¹ã€‚è¯¥æ¨¡æ‹Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å’Œæœ‰å‘ç¤¾äº¤å›¾ï¼Œåˆ†ææ–°å‡ºç°çš„æ¬ºéª—è¡Œä¸ºï¼Œå¸®åŠ©ç†è§£ç”¨æˆ·å¦‚ä½•åˆ¤æ–­åœ¨çº¿ç¤¾äº¤å†…å®¹çš„çœŸå®æ€§ã€‚é€šè¿‡æ„å»ºå¤šæ ·åŒ–çš„ç»†ç²’åº¦ç”¨æˆ·ç”»åƒï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ”¯æŒå¤§è§„æ¨¡çš„å¤šä»£ç†æ¨¡æ‹Ÿï¼Œæ¨¡æ‹Ÿå†…å®¹ä¼ æ’­å’Œç”¨æˆ·å‚ä¸çš„åŠ¨æ€ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§ä¸åŒçš„å†…å®¹å®¡æ ¸ç­–ç•¥ï¼Œå‘ç°å®ƒä»¬ä¸ä»…èƒ½å‡ç¼“è™šå‡ä¿¡æ¯çš„ä¼ æ’­ï¼Œè¿˜èƒ½æé«˜ç”¨æˆ·å‚ä¸åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07934', 'title': 'SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement', 'url': 'https://huggingface.co/papers/2504.07934', 'abstract': 'In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.', 'score': 11, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '419fda5d0a4bcadf', 'authors': ['Xiyao Wang', 'Zhengyuan Yang', 'Chao Feng', 'Hongjin Lu', 'Linjie Li', 'Chung-Ching Lin', 'Kevin Lin', 'Furong Huang', 'Lijuan Wang'], 'affiliations': ['Microsoft', 'University of Maryland, College Park', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2504.07934.jpg', 'data': {'categories': ['#cv', '#data', '#reasoning', '#training', '#open_source', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RFT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (MCTS) Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ThinkLite-VL, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 11 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ 7B Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ².'}, 'en': {'title': 'Enhancing Visual Reasoning with Fewer Samples through Smart Data Selection', 'desc': 'This paper introduces a novel method to improve visual reasoning in models using fewer training samples through self-improvement techniques, avoiding knowledge distillation. The authors emphasize the importance of sample difficulty during reinforcement fine-tuning (RFT), suggesting that challenging samples can enhance reasoning capabilities even with limited data. They propose a unique application of Monte Carlo Tree Search (MCTS) to quantify sample difficulty, allowing for effective data filtering. The resulting model, ThinkLite-VL, demonstrates a 7% performance increase over its predecessor using only 11k samples, achieving state-of-the-art results in various benchmarks.'}, 'zh': {'title': 'ç”¨å°‘é‡æ ·æœ¬æå‡è§†è§‰æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘æ”¹è¿›æ¥å¢å¼ºè§†è§‰æ¨ç†ï¼Œä¸”æ‰€éœ€çš„è®­ç»ƒæ ·æœ¬æ˜¾è‘—å‡å°‘ï¼Œä¸ä¾èµ–çŸ¥è¯†è’¸é¦ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¿‡ç¨‹ä¸­ï¼Œè®­ç»ƒæ•°æ®çš„éš¾åº¦è‡³å…³é‡è¦ï¼Œé€‚å½“å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬å¯ä»¥æ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹å¼ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥é‡åŒ–æ ·æœ¬çš„éš¾åº¦ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ•°æ®ç­›é€‰ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹ThinkLite-VLåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨ä»…11kä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå¹³å‡æ€§èƒ½æå‡äº†7%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.04974', 'title': 'Towards Visual Text Grounding of Multimodal Large Language Model', 'url': 'https://huggingface.co/papers/2504.04974', 'abstract': 'Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.', 'score': 8, 'issue_id': 3184, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': 'f382ad07e678a558', 'authors': ['Ming Li', 'Ruiyi Zhang', 'Jian Chen', 'Jiuxiang Gu', 'Yufan Zhou', 'Franck Dernoncourt', 'Wanrong Zhu', 'Tianyi Zhou', 'Tong Sun'], 'affiliations': ['Adobe Research', 'University at Buffalo', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.04974.jpg', 'data': {'categories': ['#benchmark', '#training', '#dataset', '#synthetic', '#transfer_learning', '#multimodal', '#reasoning'], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ TRIG Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 800 Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ 90 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑ†ĞµĞ½ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… MLLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞµ Ğº Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ plug-and-play.'}, 'en': {'title': 'Enhancing MLLMs for Text-Rich Image Grounding', 'desc': "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in understanding text-rich images, such as documents and infographics. The authors introduce a new task called TRIG, which focuses on improving the grounding of text in these complex images for better document question-answering. They create a benchmark dataset with 800 annotated question-answer pairs and 90 synthetic data samples to evaluate MLLMs' performance. Additionally, the paper presents two methods to enhance MLLMs' spatial reasoning and grounding abilities through fine-tuning on the new dataset."}, 'zh': {'title': 'æå‡æ–‡æ¡£å›¾åƒçš„æ–‡æœ¬å®šä½èƒ½åŠ›', 'desc': 'å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åœ¨è§†è§‰æ–‡æœ¬å®šä½æ–¹é¢ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬ä¸°å¯Œçš„æ–‡æ¡£å›¾åƒä¸­ã€‚æ–‡æ¡£å›¾åƒå¦‚æ‰«æè¡¨å•å’Œä¿¡æ¯å›¾è¡¨ç”±äºå…¶å¤æ‚çš„å¸ƒå±€å’Œæ–‡æœ¬å†…å®¹ï¼Œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TRIGä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–°çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œä»¥è¯„ä¼°å’Œæå‡MLLMsåœ¨æ–‡æ¡£é—®ç­”ä¸­çš„æ–‡æœ¬ä¸°å¯Œå›¾åƒå®šä½èƒ½åŠ›ã€‚é€šè¿‡å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç©ºé—´æ¨ç†å’Œå®šä½èƒ½åŠ›ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06752', 'title': 'Compass Control: Multi Object Orientation Control for Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2504.06752', 'abstract': 'Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.', 'score': 3, 'issue_id': 3188, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': 'd73e0b8851ffd9e3', 'authors': ['Rishubh Parihar', 'Vaibhav Agrawal', 'Sachidanand VS', 'R. Venkatesh Babu'], 'affiliations': ['IIIT Hyderabad', 'IISc Bangalore'], 'pdf_title_img': 'assets/pdf/title_img/2504.06752.jpg', 'data': {'categories': ['#cv', '#3d', '#training', '#diffusion', '#synthetic'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-ĞºĞ¾Ğ¼Ğ¿Ğ°ÑÑ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Mastering Object Orientation in Text-to-Image Generation', 'desc': 'This paper presents a novel approach to enhance text-to-image diffusion models by enabling precise control over the orientation of multiple objects in generated scenes. The authors introduce orientation-aware compass tokens that are conditioned on the object orientation, allowing for better manipulation of object placement and alignment. A lightweight encoder network predicts these tokens based on input orientations, addressing challenges like poor control and object entanglement during training. The proposed method demonstrates strong generalization capabilities, achieving state-of-the-art results in orientation control and text alignment through extensive evaluations and user studies.'}, 'zh': {'title': 'ç²¾ç¡®æ§åˆ¶å¤šå¯¹è±¡æ–¹å‘çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤šå¯¹è±¡çš„æ–¹å‘æ§åˆ¶ã€‚é€šè¿‡å¼•å…¥æ–¹å‘æ„ŸçŸ¥çš„æŒ‡å—é’ˆæ ‡è®°ï¼Œæ¨¡å‹èƒ½å¤Ÿä¸ºæ¯ä¸ªå¯¹è±¡æä¾›ç²¾ç¡®çš„æ–¹å‘æ§åˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆå¤æ‚å¯¹è±¡å’Œå¤šå¯¹è±¡åœºæ™¯æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“åˆä¸ªæ€§åŒ–æ–¹æ³•åï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§ä¸Šä¸‹æ–‡ä¸­ç²¾ç¡®æ§åˆ¶æ–°å¯¹è±¡çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05579', 'title': 'TAPNext: Tracking Any Point (TAP) as Next Token Prediction', 'url': 'https://huggingface.co/papers/2504.05579', 'abstract': 'Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.', 'score': 3, 'issue_id': 3192, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': '27a89bfc540d93f8', 'authors': ['Artem Zholus', 'Carl Doersch', 'Yi Yang', 'Skanda Koppula', 'Viorica Patraucean', 'Xu Owen He', 'Ignacio Rocco', 'Mehdi S. M. Sajjadi', 'Sarath Chandar', 'Ross Goroshin'], 'affiliations': ['Canada CIFAR AI Chair', 'Chandar Research Lab', 'Google DeepMind', 'Mila - Quebec AI Institute', 'Polytechnique Montreal', 'University College London', 'UniversitÃ© de MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2504.05579.jpg', 'data': {'categories': ['#cv', '#architecture', '#video', '#robotics'], 'emoji': 'ğŸ¯', 'ru': {'title': 'TAPNext: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'TAPNext - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ. TAPNext Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ñ‚Ñ€ĞµĞºĞµÑ€Ğ¾Ğ², Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ñƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ² TAPNext Ñ‡ĞµÑ€ĞµĞ· ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Revolutionizing Video Tracking with TAPNext', 'desc': 'The paper introduces TAPNext, a novel method for tracking any point (TAP) in videos, which is crucial for applications like robotics and video editing. Unlike traditional methods that depend on complex rules and biases, TAPNext simplifies the process by using sequential masked token decoding. This approach allows for real-time tracking without the need for temporal windowing, resulting in lower latency. TAPNext not only achieves superior tracking performance compared to existing methods but also shows that common tracking heuristics can be learned through end-to-end training.'}, 'zh': {'title': 'TAPNextï¼šç®€åŒ–è§†é¢‘è·Ÿè¸ªçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘è·Ÿè¸ªæ–¹æ³•ï¼Œç§°ä¸ºTAPNextï¼Œæ—¨åœ¨è§£å†³è·Ÿè¸ªä»»æ„ç‚¹ï¼ˆTAPï¼‰çš„é—®é¢˜ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒTAPNextå°†TAPè§†ä¸ºé¡ºåºæ©ç ä»¤ç‰Œè§£ç ï¼Œæ¶ˆé™¤äº†å¤æ‚çš„è·Ÿè¸ªç‰¹å®šåè§ã€‚è¯¥æ¨¡å‹å…·æœ‰å› æœæ€§ï¼Œèƒ½å¤Ÿåœ¨çº¿å®æ—¶è·Ÿè¸ªï¼Œä¸”å»¶è¿Ÿæä½ã€‚å°½ç®¡æ–¹æ³•ç®€å•ï¼ŒTAPNextåœ¨åœ¨çº¿å’Œç¦»çº¿è·Ÿè¸ªå™¨ä¸­éƒ½è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07961', 'title': 'Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction', 'url': 'https://huggingface.co/papers/2504.07961', 'abstract': 'We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.', 'score': 2, 'issue_id': 3193, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '4a0d7f1cda574212', 'authors': ['Zeren Jiang', 'Chuanxia Zheng', 'Iro Laina', 'Diane Larlus', 'Andrea Vedaldi'], 'affiliations': ['Naver Labs Europe', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2504.07961.jpg', 'data': {'categories': ['#video', '#3d', '#benchmark', '#multimodal', '#long_context', '#diffusion', '#synthetic'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Geo4D: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Geo4D - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ¹Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot. Geo4D Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Geo4D Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Geo4D', 'desc': 'Geo4D is a novel approach that adapts video diffusion models for creating 3D reconstructions from single-camera videos of moving scenes. It effectively utilizes the dynamic information captured by these models, allowing it to be trained solely on synthetic datasets while still performing well on real-world data without additional training. The method predicts various geometric representations, including point, depth, and ray maps, and employs a unique multi-modal alignment technique to integrate these representations during the reconstruction process. Through extensive testing, Geo4D demonstrates superior performance compared to existing video depth estimation techniques, particularly in dynamic environments.'}, 'zh': {'title': 'Geo4Dï¼šåŠ¨æ€åœºæ™¯çš„4Dé‡å»ºæ–°æ–¹æ³•', 'desc': 'Geo4Dæ˜¯ä¸€ç§å°†è§†é¢‘æ‰©æ•£æ¨¡å‹ç”¨äºå•ç›®3Dé‡å»ºåŠ¨æ€åœºæ™¯çš„æ–¹æ³•ã€‚å®ƒåˆ©ç”¨è§†é¢‘æ¨¡å‹æ•æ‰åˆ°çš„å¼ºåŠ¨æ€å…ˆéªŒï¼Œä»…ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶èƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹å¾ˆå¥½åœ°æ¨å¹¿åˆ°çœŸå®æ•°æ®ã€‚Geo4Dé¢„æµ‹å¤šç§äº’è¡¥çš„å‡ ä½•æ¨¡æ€ï¼ŒåŒ…æ‹¬ç‚¹å›¾ã€æ·±åº¦å›¾å’Œå…‰çº¿å›¾ã€‚é€šè¿‡æ–°çš„å¤šæ¨¡æ€å¯¹é½ç®—æ³•å’Œå¤šä¸ªæ»‘åŠ¨çª—å£ï¼ŒGeo4Dåœ¨æ¨ç†æ—¶å¯¹è¿™äº›æ¨¡æ€è¿›è¡Œå¯¹é½å’Œèåˆï¼Œä»è€Œå®ç°é•¿è§†é¢‘çš„ç¨³å¥å’Œå‡†ç¡®çš„4Dé‡å»ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06801', 'title': 'MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular\n  Detection', 'url': 'https://huggingface.co/papers/2504.06801', 'abstract': "Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.", 'score': 2, 'issue_id': 3188, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': 'bf3e9622523967d2', 'authors': ['Rishubh Parihar', 'Srinjay Sarkar', 'Sarthak Vora', 'Jogendra Kundu', 'R. Venkatesh Babu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.06801.jpg', 'data': {'categories': ['#3d', '#optimization', '#dataset', '#synthetic'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ 3D Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸', 'desc': 'MonoPlace3D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… 3D Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ½Ğ° ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ 3D ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ÑÑ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… KITTI Ğ¸ NuScenes Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… 3D Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Monocular 3D Detection with Realistic Object Placement', 'desc': 'This paper addresses the limitations of current monocular 3D detectors due to the lack of diverse real-world datasets. It highlights the importance of not just realistic object appearance but also the correct placement of objects in 3D scenes for effective training. The authors introduce MonoPlace3D, a system that learns to generate realistic object placements based on the content of the background scene. Their experiments show that MonoPlace3D enhances the performance of existing monocular 3D detectors on standard datasets like KITTI and NuScenes, demonstrating improved accuracy and data efficiency.'}, 'zh': {'title': 'MonoPlace3Dï¼šæå‡å•ç›®3Dæ£€æµ‹çš„çœŸå®æ„Ÿå¢å¼º', 'desc': 'å½“å‰çš„å•ç›®3Dæ£€æµ‹å™¨å—åˆ°çœŸå®ä¸–ç•Œæ•°æ®é›†å¤šæ ·æ€§å’Œè§„æ¨¡çš„é™åˆ¶ã€‚è™½ç„¶æ•°æ®å¢å¼ºæœ‰åŠ©äºæ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Œä½†åœ¨æˆ·å¤–åœºæ™¯ä¸­ç”ŸæˆçœŸå®æ„Ÿçš„å¢å¼ºæ•°æ®å°¤å…¶å›°éš¾ã€‚å¤§å¤šæ•°åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ä¸“æ³¨äºé€šè¿‡æ”¹è¿›æ¸²æŸ“æŠ€æœ¯æ¥æé«˜ç‰©ä½“å¤–è§‚çš„çœŸå®æ„Ÿï¼Œè€Œæˆ‘ä»¬å‘ç°ç‰©ä½“çš„æ”¾ç½®ä½ç½®å’Œæ–¹å¼å¯¹è®­ç»ƒæœ‰æ•ˆçš„3Då•ç›®æ£€æµ‹å™¨åŒæ ·é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MonoPlace3Dç³»ç»Ÿï¼Œå®ƒè€ƒè™‘3Dåœºæ™¯å†…å®¹æ¥åˆ›å»ºçœŸå®çš„å¢å¼ºæ•°æ®ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç°æœ‰å•ç›®3Dæ£€æµ‹å™¨çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05741', 'title': 'DDT: Decoupled Diffusion Transformer', 'url': 'https://huggingface.co/papers/2504.05741', 'abstract': 'Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\color{ddtD}ecoupled \\color{ddtD}iffusion \\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.', 'score': 48, 'issue_id': 3159, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': '2f4cd9583b2418f3', 'authors': ['Shuai Wang', 'Zhi Tian', 'Weilin Huang', 'Limin Wang'], 'affiliations': ['ByteDance Seed Vision', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05741.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#cv', '#diffusion'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'DDT: Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹ Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Decoupled Diffusion Transformer (DDT). DDT Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ÑÑƒÑ‰ÑƒÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DDT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Decoupling for Faster and Better Image Generation', 'desc': 'This paper introduces a new model called the Decoupled Diffusion Transformer (DDT), which addresses the challenges faced by traditional diffusion transformers in generating high-quality outputs. The DDT separates the tasks of semantic encoding and high-frequency decoding, allowing for better optimization and improved performance. Experiments show that as the model size increases, a more powerful encoder leads to significant enhancements in generation quality, achieving state-of-the-art results on ImageNet datasets. Additionally, the decoupled design improves inference speed by optimizing the sharing of self-conditions between denoising steps, while a novel dynamic programming method helps maintain performance.'}, 'zh': {'title': 'è§£è€¦æ‰©æ•£å˜æ¢å™¨ï¼šæå‡ç”Ÿæˆè´¨é‡ä¸æ¨ç†é€Ÿåº¦çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'æ‰©æ•£å˜æ¢å™¨åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†è®­ç»ƒè¿­ä»£æ—¶é—´è¾ƒé•¿ä¸”æ¨ç†æ­¥éª¤è¾ƒå¤šã€‚æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼Œæ‰©æ•£å˜æ¢å™¨å¯¹å™ªå£°è¾“å…¥è¿›è¡Œç¼–ç ï¼Œä»¥æå–ä½é¢‘è¯­ä¹‰æˆåˆ†ï¼Œç„¶åç”¨ç›¸åŒçš„æ¨¡å—è§£ç é«˜é¢‘æˆåˆ†ã€‚è¿™ç§æ–¹æ¡ˆå¯¼è‡´äº†ä¸€ä¸ªå›ºæœ‰çš„ä¼˜åŒ–å›°å¢ƒï¼šç¼–ç ä½é¢‘è¯­ä¹‰éœ€è¦å‡å°‘é«˜é¢‘æˆåˆ†ï¼Œä»è€Œåœ¨è¯­ä¹‰ç¼–ç å’Œé«˜é¢‘è§£ç ä¹‹é—´äº§ç”Ÿç´§å¼ å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§£è€¦æ‰©æ•£å˜æ¢å™¨ï¼ˆDDTï¼‰ï¼Œå®ƒé‡‡ç”¨ä¸“é—¨çš„æ¡ä»¶ç¼–ç å™¨è¿›è¡Œè¯­ä¹‰æå–ï¼Œå¹¶é…å¤‡ä¸“é—¨çš„é€Ÿåº¦è§£ç å™¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07096', 'title': 'OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens', 'url': 'https://huggingface.co/papers/2504.07096', 'abstract': 'We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.', 'score': 33, 'issue_id': 3160, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '68eafe783b5816f6', 'authors': ['Jiacheng Liu', 'Taylor Blanton', 'Yanai Elazar', 'Sewon Min', 'YenSung Chen', 'Arnavi Chheda-Kothary', 'Huy Tran', 'Byron Bischoff', 'Eric Marsh', 'Michael Schmitz', 'Cassidy Trier', 'Aaron Sarnat', 'Jenna James', 'Jon Borchardt', 'Bailey Kuehl', 'Evie Cheng', 'Karen Farley', 'Sruthi Sreeram', 'Taira Anderson', 'David Albright', 'Carissa Schoenick', 'Luca Soldaini', 'Dirk Groeneveld', 'Rock Yuren Pang', 'Pang Wei Koh', 'Noah A. Smith', 'Sophie Lebrecht', 'Yejin Choi', 'Hannaneh Hajishirzi', 'Ali Farhadi', 'Jesse Dodge'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'UC Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.07096.jpg', 'data': {'categories': ['#data', '#hallucinations', '#inference', '#open_source', '#interpretability', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ—Ğ°Ğ³Ğ»ÑĞ½ÑƒÑ‚ÑŒ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'OLMoTrace - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². OLMoTrace Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° infini-gram Ğ¸ Ğ²Ñ‹Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞºÑƒĞ½Ğ´. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ¸Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Trace the Truth: Unveiling Language Model Outputs with OLMoTrace', 'desc': 'OLMoTrace is a groundbreaking system that allows users to trace the outputs of language models back to their extensive training data in real time. It identifies and displays exact matches between the generated text and the original documents from the training corpus. Utilizing an enhanced version of the infini-gram technique, OLMoTrace provides results in just a few seconds. This tool aids in understanding language model behavior, particularly in areas like fact-checking, hallucination, and creative output.'}, 'zh': {'title': 'å®æ—¶è¿½è¸ªè¯­è¨€æ¨¡å‹è¾“å‡ºçš„é©å‘½æ€§å·¥å…·', 'desc': 'OLMoTraceæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå®æ—¶è¿½è¸ªè¯­è¨€æ¨¡å‹è¾“å‡ºä¸å…¶è®­ç»ƒæ•°æ®ä¹‹é—´å…³ç³»çš„ç³»ç»Ÿã€‚å®ƒå¯ä»¥æ‰¾åˆ°è¯­è¨€æ¨¡å‹è¾“å‡ºç‰‡æ®µä¸è®­ç»ƒæ–‡æœ¬åº“ä¸­æ–‡æ¡£çš„é€å­—åŒ¹é…ã€‚è¯¥ç³»ç»ŸåŸºäºæ‰©å±•ç‰ˆçš„infini-gramæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å‡ ç§’é’Ÿå†…è¿”å›è¿½è¸ªç»“æœã€‚OLMoTraceå¸®åŠ©ç”¨æˆ·é€šè¿‡è®­ç»ƒæ•°æ®ç†è§£è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºï¼Œé€‚ç”¨äºäº‹å®æ£€æŸ¥ã€å¹»è§‰å’Œè¯­è¨€æ¨¡å‹çš„åˆ›é€ åŠ›æ¢ç´¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07046', 'title': 'A Unified Agentic Framework for Evaluating Conditional Image Generation', 'url': 'https://huggingface.co/papers/2504.07046', 'abstract': "Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.", 'score': 23, 'issue_id': 3167, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': 'ec2d69230afcb841', 'authors': ['Jifang Wang', 'Xue Yang', 'Longyue Wang', 'Zhenran Xu', 'Yiyu Wang', 'Yaowei Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology (Shenzhen), Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.07046.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#benchmark', '#open_source', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'CIGEval: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CIGEval - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. CIGEval Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LMM) Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CIGEval Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'CIGEval: Revolutionizing Evaluation in Conditional Image Generation', 'desc': 'This paper presents CIGEval, a new framework designed to evaluate conditional image generation tasks effectively. It addresses the need for reliable and explainable metrics by using large multimodal models (LMMs) and a multi-functional toolbox. CIGEval not only assesses image generation but also fine-tunes smaller LMMs to select the best evaluation tools autonomously. The results show that CIGEval correlates well with human assessments and outperforms previous methods, demonstrating its potential for automating image generation evaluations.'}, 'zh': {'title': 'CIGEvalï¼šæ¡ä»¶å›¾åƒç”Ÿæˆçš„å…¨é¢è¯„ä¼°æ–°æ¡†æ¶', 'desc': 'æ¡ä»¶å›¾åƒç”Ÿæˆå› å…¶ä¸ªæ€§åŒ–å†…å®¹çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸåœ¨å¼€å‘ä»»åŠ¡æ— å…³ã€å¯é ä¸”å¯è§£é‡Šçš„è¯„ä¼°æŒ‡æ ‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†CIGEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ä»£ç†æ¡†æ¶ï¼Œç”¨äºå…¨é¢è¯„ä¼°æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚CIGEvalåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä½œä¸ºæ ¸å¿ƒï¼Œæ•´åˆå¤šåŠŸèƒ½å·¥å…·ç®±å¹¶å»ºç«‹ç»†ç²’åº¦è¯„ä¼°æ¡†æ¶ï¼Œå±•ç¤ºäº†å…¶åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡è¯„ä¼°ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06514', 'title': 'Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?', 'url': 'https://huggingface.co/papers/2504.06514', 'abstract': "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.", 'score': 21, 'issue_id': 3160, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '6ca6e88a5650dba9', 'authors': ['Chenrui Fan', 'Ming Li', 'Lichao Sun', 'Tianyi Zhou'], 'affiliations': ['Lehigh University', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.06514.jpg', 'data': {'categories': ['#training', '#rl', '#hallucinations', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾: ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ (overthinking) Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾ÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑ Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Tackling MiP-Overthinking in Reasoning LLMs', 'desc': 'This paper investigates how reasoning large language models (LLMs) respond to poorly defined questions that lack necessary information, a situation termed as missing premises (MiP). It finds that these models tend to generate longer, redundant responses, which is a manifestation of what the authors call MiP-Overthinking. Interestingly, LLMs not specifically trained for reasoning perform better in these scenarios, producing shorter and more effective answers. The study highlights a significant flaw in the training methods for reasoning LLMs, suggesting that they do not promote efficient thinking, and proposes further analysis to understand and mitigate the overthinking issue.'}, 'zh': {'title': 'æ­ç¤ºæ¨ç†æ¨¡å‹çš„è¿‡åº¦æ€è€ƒé—®é¢˜', 'desc': 'æˆ‘ä»¬å‘ç°ï¼Œæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹ç¼ºä¹å‰æçš„æ¨¡ç³Šé—®é¢˜æ—¶ï¼Œå“åº”é•¿åº¦æ˜¾è‘—å¢åŠ ï¼Œå¯¼è‡´å†—ä½™å’Œæ— æ•ˆçš„æ€è€ƒã€‚è¿™ç§æ–°å¼•å…¥çš„åœºæ™¯åŠ å‰§äº†æ™®éçš„è¿‡åº¦æ€è€ƒé—®é¢˜ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºMiP-è¿‡åº¦æ€è€ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„æ¨ç†LLMsè®­ç»ƒæ–¹æ³•æœªèƒ½æœ‰æ•ˆé¼“åŠ±é«˜æ•ˆæ€è€ƒï¼Œå¯¼è‡´æ€ç»´æ¨¡å¼çš„æ»¥ç”¨ã€‚ç›¸åï¼Œæœªä¸“é—¨è®­ç»ƒç”¨äºæ¨ç†çš„LLMsåœ¨å¤„ç†æ¨¡ç³Šé—®é¢˜æ—¶è¡¨ç°æ›´å¥½ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¯†åˆ«é—®é¢˜å¹¶ç»™å‡ºæ›´ç®€çŸ­çš„å›ç­”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07083', 'title': 'GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography', 'url': 'https://huggingface.co/papers/2504.07083', 'abstract': 'Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.', 'score': 18, 'issue_id': 3160, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '67e58f651d865bad', 'authors': ['Mengchen Zhang', 'Tong Wu', 'Jing Tan', 'Ziwei Liu', 'Gordon Wetzstein', 'Dahua Lin'], 'affiliations': ['Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Stanford University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07083.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ˜Ğ˜-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² ĞºĞ¸Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DataDoP, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 29 Ñ‚Ñ‹ÑÑÑ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GenDoP Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GenDoP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Camera Movement with GenDoP', 'desc': 'This paper presents a novel approach to generating camera trajectories for video production using an auto-regressive model called GenDoP. The model is trained on a large dataset, DataDoP, which includes diverse camera movements, depth information, and detailed captions that reflect directorial intent. Unlike traditional methods that rely on geometric optimization, GenDoP leverages text guidance and RGBD inputs to create more expressive and context-aware camera movements. The results show that GenDoP outperforms existing techniques in terms of controllability, trajectory adjustments, and motion stability, setting a new benchmark for learning-based cinematography.'}, 'zh': {'title': 'åˆ›æ–°ç›¸æœºè½¨è¿¹ç”Ÿæˆï¼Œæå‡è§†è§‰å™äº‹æ•ˆæœ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç›¸æœºè½¨è¿¹ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æå‡è§†é¢‘åˆ¶ä½œä¸­çš„è§†è§‰å™äº‹æ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºGenDoPçš„è‡ªå›å½’æ¨¡å‹ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ•°æ®é›†DataDoPï¼ŒåŒ…å«29000ä¸ªçœŸå®é•œå¤´åŠå…¶ç›¸æœºè½¨è¿¹ã€æ·±åº¦å›¾å’Œè¯¦ç»†æè¿°ã€‚é€šè¿‡ç»“åˆæ–‡æœ¬æŒ‡å¯¼å’ŒRGBDè¾“å…¥ï¼ŒGenDoPèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç›¸æœºè¿åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenDoPåœ¨å¯æ§æ€§ã€è½¨è¿¹è°ƒæ•´ç²¾ç»†åº¦å’Œè¿åŠ¨ç¨³å®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.04842', 'title': 'FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis', 'url': 'https://huggingface.co/papers/2504.04842', 'abstract': 'Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.', 'score': 15, 'issue_id': 3160, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '5b592626aeda4ec8', 'authors': ['Mengchao Wang', 'Qiang Wang', 'Fan Jiang', 'Yaqi Fan', 'Yunpeng Zhang', 'Yonggang Qi', 'Kun Zhao', 'Mu Xu'], 'affiliations': ['AMAP, Alibaba Group', 'Beijing University of Posts and Telecommunications'], 'pdf_title_img': 'assets/pdf/title_img/2504.04842.jpg', 'data': {'categories': ['#multimodal', '#video', '#diffusion'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ¶Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Realistic Talking Avatars: Synchronizing Motion and Expression', 'desc': "This paper presents a new method for creating realistic animated avatars from a single portrait. It uses a pretrained video diffusion transformer to generate talking portraits that can move in a lifelike way. The approach includes a dual-stage audio-visual alignment strategy to ensure that the avatar's lip movements and body motions are synchronized with audio input. Additionally, it introduces a facial-focused cross-attention module to maintain facial identity and a motion intensity modulation module for controlling the expressiveness of the avatar."}, 'zh': {'title': 'ç”Ÿæˆå¯æ§åŠ¨ç”»å¤´åƒçš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºä»å•ä¸€é™æ€è‚–åƒç”Ÿæˆé€¼çœŸçš„å¯åŠ¨ç”»å¤´åƒã€‚æˆ‘ä»¬é‡‡ç”¨äº†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£å˜æ¢å™¨æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€è¿è´¯çš„å¯¹è¯è‚–åƒï¼Œå¹¶å…·å¤‡å¯æ§çš„è¿åŠ¨åŠ¨æ€ã€‚æ ¸å¿ƒæ˜¯åŒé˜¶æ®µçš„éŸ³è§†é¢‘å¯¹é½ç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡éŸ³é¢‘é©±åŠ¨çš„åŠ¨æ€å¯¹é½å…¨åœºæ™¯ï¼Œç¬¬äºŒé˜¶æ®µåˆ™ä½¿ç”¨å”‡éƒ¨è¿½è¸ªæ©æ¨¡ç²¾ç»†è°ƒæ•´å”‡éƒ¨åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®æ„Ÿã€ä¸€è‡´æ€§ã€è¿åŠ¨å¼ºåº¦å’Œèº«ä»½ä¿ç•™æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07086', 'title': 'A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility', 'url': 'https://huggingface.co/papers/2504.07086', 'abstract': 'Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.', 'score': 10, 'issue_id': 3163, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': 'b06c700fb29c005d', 'authors': ['Andreas Hochlehnert', 'Hardik Bhatnagar', 'Vishaal Udandarao', 'Samuel Albanie', 'Ameya Prabhu', 'Matthias Bethge'], 'affiliations': ['TÃ¼bingen AI Center, University of TÃ¼bingen', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2504.07086.jpg', 'data': {'categories': ['#open_source', '#survey', '#rl', '#reasoning', '#benchmark', '#training'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ Ğº Ñ‚Ğ¾Ğ½ĞºĞ¸Ğ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞŸĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºĞ° Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ ÑĞºÑ€Ğ¾Ğ¼Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº supervised fine-tuning Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Standardizing Reasoning Evaluations for Language Models', 'desc': 'This paper addresses the challenges in evaluating reasoning capabilities of language models (LMs) due to inconsistent benchmarking practices. The authors find that current mathematical reasoning benchmarks are sensitive to various implementation factors, which can lead to misleading performance claims. They propose a standardized evaluation framework that includes best practices and clear reporting standards to improve transparency and reproducibility. Their findings suggest that supervised finetuning (SFT) methods outperform reinforcement learning (RL) approaches, which often overfit on smaller benchmarks.'}, 'zh': {'title': 'æ¨ç†èƒ½åŠ›è¯„ä¼°çš„æ–°æ ‡å‡†', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿›å±•ï¼ŒæŒ‡å‡ºå½“å‰çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å¯¹å®ç°ç»†èŠ‚éå¸¸æ•æ„Ÿã€‚ç ”ç©¶å‘ç°ï¼Œè®¸å¤šè¯„ä¼°æ–¹æ³•ç¼ºä¹é€æ˜æ€§å’Œç»Ÿè®¡åŸºç¡€ï¼Œå¯¼è‡´æ€§èƒ½æå‡çš„æ¯”è¾ƒä¸å¤Ÿæ¸…æ™°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæ˜ç¡®äº†æœ€ä½³å®è·µå’ŒæŠ¥å‘Šæ ‡å‡†ï¼Œå¹¶é‡æ–°è¯„ä¼°äº†ç°æœ‰æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ”¹è¿›æœ‰é™ï¼Œè€Œç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°æ›´å¼ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07081', 'title': 'Self-Steering Language Models', 'url': 'https://huggingface.co/papers/2504.07081', 'abstract': 'While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for "self-steering" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.', 'score': 10, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': 'b0b90ba5f1b881da', 'authors': ['Gabriel Grand', 'Joshua B. Tenenbaum', 'Vikash K. Mansinghka', 'Alexander K. Lew', 'Jacob Andreas'], 'affiliations': ['Massachusetts Institute of Technology', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07081.jpg', 'data': {'categories': ['#training', '#agents', '#reasoning', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ DisCIPL Ğ´Ğ»Ñ 'ÑĞ°Ğ¼Ğ¾ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ' ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Planner Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Follower. DisCIPL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. DisCIPL Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾."}, 'en': {'title': 'Empowering Language Models with Self-Steering Inference Programs', 'desc': 'This paper presents DisCIPL, a novel method that enhances language models (LMs) by allowing them to generate task-specific inference programs through a Planner model. These programs are then executed by multiple Follower models, enabling efficient and verifiable reasoning processes. The approach allows LMs to create recursive search procedures that improve their problem-solving capabilities, even when they struggle with direct reasoning. Remarkably, DisCIPL demonstrates that smaller models can achieve performance comparable to larger models on complex tasks, while also introducing new strategies for parallelized inference without the need for fine-tuning.'}, 'zh': {'title': 'è‡ªæˆ‘å¼•å¯¼çš„è¯­è¨€æ¨¡å‹æ¨ç†æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDisCIPLçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ä¸ªè§„åˆ’æ¨¡å‹ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„æ¨ç†ç¨‹åºï¼Œå¹¶ç”±å¤šä¸ªè·Ÿéšæ¨¡å‹æ‰§è¡Œï¼Œä»è€Œå®ç°è‡ªæˆ‘å¼•å¯¼ã€‚DisCIPLä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç¼–å†™é€’å½’æœç´¢ç¨‹åºï¼Œæå‡æ¨ç†çš„å¯éªŒè¯æ€§å’Œæ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å°å‹è·Ÿéšæ¨¡å‹æ—¶ï¼ŒDisCIPLåœ¨ä¸€äº›å—é™ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¿‡äº†æ›´å¤§çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07089', 'title': 'OmniCaptioner: One Captioner to Rule Them All', 'url': 'https://huggingface.co/papers/2504.07089', 'abstract': 'We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.', 'score': 9, 'issue_id': 3164, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '9b09a9ebdec71131', 'authors': ['Yiting Lu', 'Jiakang Yuan', 'Zhen Li', 'Shitian Zhao', 'Qi Qin', 'Xinyue Li', 'Le Zhuo', 'Licheng Wen', 'Dongyang Liu', 'Yuewen Cao', 'Xiangchao Yan', 'Xin Li', 'Botian Shi', 'Tao Chen', 'Zhibo Chen', 'Lei Bai', 'Bo Zhang', 'Peng Gao'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.07089.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#long_context', '#cv', '#training', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'OmniCaptioner - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. OmniCaptioner Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸.'}, 'en': {'title': 'Bridging Visuals and Text with OmniCaptioner', 'desc': 'OmniCaptioner is a new framework designed for generating detailed text descriptions from various types of images. Unlike previous methods that only work with specific image categories, it can handle natural images, visual text, and structured visuals all in one system. The framework transforms pixel data into meaningful text, connecting visual and textual information effectively. Its advantages include better reasoning with large language models, enhanced image generation capabilities, and faster training with less data.'}, 'zh': {'title': 'OmniCaptionerï¼šè§†è§‰ä¸æ–‡æœ¬çš„æ¡¥æ¢', 'desc': 'æˆ‘ä»¬æå‡ºäº†OmniCaptionerï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„è§†è§‰æè¿°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šç§è§†è§‰é¢†åŸŸç”Ÿæˆç»†è‡´çš„æ–‡æœ¬æè¿°ã€‚ä¸ä¹‹å‰ä»…é™äºç‰¹å®šå›¾åƒç±»å‹çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å¯¹è‡ªç„¶å›¾åƒã€è§†è§‰æ–‡æœ¬ï¼ˆå¦‚æµ·æŠ¥ã€ç”¨æˆ·ç•Œé¢ã€æ•™ç§‘ä¹¦ï¼‰å’Œç»“æ„åŒ–è§†è§‰ï¼ˆå¦‚æ–‡æ¡£ã€è¡¨æ ¼ã€å›¾è¡¨ï¼‰è¿›è¡Œæè¿°ã€‚é€šè¿‡å°†ä½çº§åƒç´ ä¿¡æ¯è½¬æ¢ä¸ºè¯­ä¹‰ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¼¥åˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒOmniCaptioneråœ¨è§†è§‰æ¨ç†ã€å›¾åƒç”Ÿæˆå’Œé«˜æ•ˆçš„ç›‘ç£å¾®è°ƒæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05541', 'title': 'Caption Anything in Video: Fine-grained Object-centric Captioning via\n  Spatiotemporal Multimodal Prompting', 'url': 'https://huggingface.co/papers/2504.05541', 'abstract': "We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects' attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at https://github.com/yunlong10/CAT-V", 'score': 9, 'issue_id': 3171, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '7e72ed48b045c60e', 'authors': ['Yunlong Tang', 'Jing Bi', 'Chao Huang', 'Susan Liang', 'Daiki Shimada', 'Hang Hua', 'Yunzhong Xiao', 'Yizhi Song', 'Pinxin Liu', 'Mingqian Feng', 'Junjia Guo', 'Zhuo Liu', 'Luchuan Song', 'Ali Vosoughi', 'Jinxi He', 'Liu He', 'Zeliang Zhang', 'Jiebo Luo', 'Chenliang Xu'], 'affiliations': ['CMU', 'Purdue University', 'Sony Group Corporation', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.05541.jpg', 'data': {'categories': ['#games', '#reasoning', '#multimodal', '#video', '#optimization', '#open_source', '#interpretability'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ»ÑĞ±Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'CAT-V - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SAMURAI, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° TRACE-Uni Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ InternVL-2.5. CAT-V Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ², Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Fine-Grained Video Captioning Made Easy with CAT-V!', 'desc': 'CAT-V (Caption AnyThing in Video) is a novel framework designed for fine-grained video captioning that focuses on user-selected objects. It combines a Segmenter for precise object segmentation, a Temporal Analyzer for detecting event boundaries, and a Captioner for generating detailed descriptions. This framework operates without the need for additional training data, utilizing spatiotemporal visual prompts and chain-of-thought reasoning to create accurate, object-centric captions. By addressing the limitations of existing methods, CAT-V provides detailed descriptions that are both temporally aware and spatially accurate, enhancing user interaction with flexible visual prompts.'}, 'zh': {'title': 'ç»†è‡´è§†é¢‘æè¿°ï¼Œç²¾å‡†ç‰©ä½“æ•æ‰', 'desc': 'CAT-Vï¼ˆè§†é¢‘ä¸­çš„ä»»ä½•ç‰©ä½“æè¿°ï¼‰æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œä¸“æ³¨äºç»†ç²’åº¦çš„ç‰©ä½“ä¸­å¿ƒè§†é¢‘æè¿°ã€‚å®ƒç»“åˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šåŸºäºSAMURAIçš„åˆ†å‰²å™¨ç”¨äºç²¾ç¡®çš„ç‰©ä½“åˆ†å‰²ï¼ŒTRACE-Unié©±åŠ¨çš„æ—¶é—´åˆ†æå™¨ç”¨äºå‡†ç¡®çš„äº‹ä»¶è¾¹ç•Œæ£€æµ‹ï¼Œä»¥åŠä½¿ç”¨InternVL-2.5ç”Ÿæˆè¯¦ç»†æè¿°çš„æè¿°å™¨ã€‚é€šè¿‡æ—¶ç©ºè§†è§‰æç¤ºå’Œé“¾å¼æ€ç»´æ¨ç†ï¼ŒCAT-Vèƒ½å¤Ÿç”Ÿæˆå¯¹è±¡å±æ€§ã€åŠ¨ä½œã€çŠ¶æ€ã€äº¤äº’å’Œç¯å¢ƒä¸Šä¸‹æ–‡çš„è¯¦ç»†æè¿°ï¼ŒåŒæ—¶ä¿æŒæ—¶é—´æ•æ„Ÿæ€§ã€‚è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰è§†é¢‘æè¿°æ–¹æ³•çš„å±€é™ï¼Œæä¾›äº†ç»†è‡´çš„ç‰©ä½“ç‰¹å®šæè¿°ï¼ŒåŒæ—¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§å’Œç©ºé—´å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.04010', 'title': 'DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion', 'url': 'https://huggingface.co/papers/2504.04010', 'abstract': "Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.", 'score': 7, 'issue_id': 3164, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 5', 'zh': '4æœˆ5æ—¥'}, 'hash': 'd006058dfff067dc', 'authors': ['Maksim Siniukov', 'Di Chang', 'Minh Tran', 'Hongkun Gong', 'Ashutosh Chaubey', 'Mohammad Soleymani'], 'affiliations': ['University of Southern California Los Angeles, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.04010.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#video', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'DiTaiListener: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiTaiListener - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ² Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. DiTaiListener ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: DiTaiListener-Gen Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ¸ DiTaiListener-Edit Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ CTM Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾- Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiTaiListener Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'DiTaiListener: Realistic Listener Motions for Engaging Interactions', 'desc': "The paper presents DiTaiListener, a novel approach for generating realistic listener motions during extended interactions. It utilizes a video diffusion model that incorporates multimodal conditions, allowing for the generation of listener responses based on the speaker's speech and facial movements. The method consists of two main components: DiTaiListener-Gen, which creates short segments of listener responses, and DiTaiListener-Edit, which refines these segments for smooth transitions. The results show that DiTaiListener outperforms existing methods in both visual quality and motion representation, achieving state-of-the-art performance on benchmark datasets and receiving positive feedback from user studies."}, 'zh': {'title': 'è‡ªç„¶äº’åŠ¨ä¸­çš„å¬ä¼—åŠ¨ä½œç”Ÿæˆæ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDiTaiListenerçš„æ¨¡å‹ï¼Œæ—¨åœ¨ç”Ÿæˆè‡ªç„¶ä¸”ç»†è…»çš„å¬ä¼—åŠ¨ä½œï¼Œä»¥æ”¹å–„é•¿æ—¶é—´äº’åŠ¨ä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€æ¡ä»¶ï¼Œé¦–å…ˆç”ŸæˆåŸºäºè¯´è¯è€…è¯­éŸ³å’Œé¢éƒ¨åŠ¨ä½œçš„çŸ­æ®µå¬ä¼—ååº”ã€‚æ¥ç€ï¼Œé€šè¿‡DiTaiListener-Editå¯¹è¿‡æ¸¡å¸§è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œä»¥ç¡®ä¿è§†é¢‘çš„å¹³æ»‘è¿‡æ¸¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiTaiListeneråœ¨è§†è§‰çœŸå®æ„Ÿå’ŒåŠ¨ä½œè¡¨ç°ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”¨æˆ·ç ”ç©¶ä¹Ÿæ˜¾ç¤ºå…¶åœ¨åé¦ˆã€å¤šæ ·æ€§å’Œæµç•…æ€§æ–¹é¢æ˜æ˜¾ä¼˜äºå…¶ä»–ç«äº‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06958', 'title': 'VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2504.06958', 'abstract': 'Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.', 'score': 6, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': 'b88359823eee79f5', 'authors': ['Xinhao Li', 'Ziang Yan', 'Desen Meng', 'Lu Dong', 'Xiangyu Zeng', 'Yinan He', 'Yali Wang', 'Yu Qiao', 'Yi Wang', 'Limin Wang'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.06958.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#video', '#multimodal', '#reasoning'], 'emoji': 'ğŸ¥', 'ru': {'title': 'RFT: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ MLLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Reinforcement Fine-Tuning (RFT) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Group Relative Policy Optimization (GRPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ VideoChat-R1 - Ğ¼Ğ¾Ñ‰Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾ MLLM, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ñƒ. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Qwen2.5-VL-7B, VideoChat-R1 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ĞºĞ°Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» RFT Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ MLLM Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing Video Understanding with Reinforcement Fine-Tuning', 'desc': "This paper explores the use of Reinforcement Fine-Tuning (RFT) combined with Group Relative Policy Optimization (GRPO) to improve video understanding in multimodal large language models (MLLMs). The authors demonstrate that RFT is effective in enhancing spatio-temporal perception while retaining the model's general capabilities. Their experiments show that the newly developed VideoChat-R1 model significantly outperforms existing models in tasks like temporal grounding and object tracking, achieving state-of-the-art results. The findings highlight the efficiency of RFT for specialized improvements in video MLLMs, paving the way for future research in this area."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ åŠ©åŠ›è§†é¢‘ç†è§£çš„çªç ´', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯è§†é¢‘ç†è§£æ–¹é¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ–¹æ³•ï¼Œç»“åˆäº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥å¢å¼ºè§†é¢‘æ¨¡å‹çš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRFTåœ¨ç‰¹å®šä»»åŠ¡çš„æ”¹è¿›ä¸Šå…·æœ‰å¾ˆé«˜çš„æ•°æ®æ•ˆç‡ï¼Œå¼€å‘äº†åä¸ºVideoChat-R1çš„å¼ºå¤§è§†é¢‘MLLMã€‚è¯¥æ¨¡å‹åœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„å¯¹è¯èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07092', 'title': 'Are We Done with Object-Centric Learning?', 'url': 'https://huggingface.co/papers/2504.07092', 'abstract': 'Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called Object-Centric Classification with Applied Masks (OCCAM), demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available https://github.com/AlexanderRubinstein/OCCAM{here}.', 'score': 5, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '8e182461a15aee14', 'authors': ['Alexander Rubinstein', 'Ameya Prabhu', 'Matthias Bethge', 'Seong Joon Oh'], 'affiliations': ['Tubingen AI Center, University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2504.07092.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#transfer_learning', '#cv', '#training', '#science'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸: Ğ¾Ñ‚ ÑĞ»Ğ¾Ñ‚Ğ¾Ğ² Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (OCL) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑÑ‚Ñ€ĞµĞ¼Ğ¸Ñ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ OCCAM, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ OCL. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ (OOD) Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ OCL Ğ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Object-Centric Learning for Better Generalization', 'desc': 'This paper discusses Object-Centric Learning (OCL), which aims to create representations that focus solely on individual objects, ignoring other elements in a scene. The authors highlight advancements in segmentation models that allow for effective separation of objects at the pixel level, leading to improved performance in out-of-distribution (OOD) object discovery tasks. They introduce a new method called Object-Centric Classification with Applied Masks (OCCAM), which shows that this segmentation approach outperforms traditional slot-based methods. The paper also addresses ongoing challenges in applying these techniques to real-world scenarios and emphasizes the importance of understanding object perception in human cognition.'}, 'zh': {'title': 'å®ç°å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„çªç ´', 'desc': 'å¯¹è±¡ä¸­å¿ƒå­¦ä¹ ï¼ˆOCLï¼‰æ—¨åœ¨å­¦ä¹ ä»…ç¼–ç å¯¹è±¡çš„è¡¨ç¤ºï¼Œç‹¬ç«‹äºåœºæ™¯ä¸­çš„å…¶ä»–å¯¹è±¡æˆ–èƒŒæ™¯çº¿ç´¢ã€‚è¿™ç§æ–¹æ³•æ”¯æŒå¤šç§ç›®æ ‡ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ³›åŒ–ã€æ ·æœ¬é«˜æ•ˆç»„åˆå’Œç»“æ„åŒ–ç¯å¢ƒå»ºæ¨¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— è®­ç»ƒæ¢æµ‹å™¨ï¼Œç§°ä¸ºåº”ç”¨æ©ç çš„å¯¹è±¡ä¸­å¿ƒåˆ†ç±»ï¼ˆOCCAMï¼‰ï¼Œæ˜¾ç¤ºå‡ºåŸºäºåˆ†å‰²çš„ä¸ªä½“å¯¹è±¡ç¼–ç æ˜¾è‘—ä¼˜äºåŸºäºæ§½çš„OCLæ–¹æ³•ã€‚å°½ç®¡åœ¨å®é™…åº”ç”¨ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬ä¸ºOCLç¤¾åŒºæä¾›äº†å¯æ‰©å±•çš„å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºå·¥å…·ç®±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06719', 'title': 'Masked Scene Modeling: Narrowing the Gap Between Supervised and\n  Self-Supervised Learning in 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2504.06719', 'abstract': 'Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (https://github.com/phermosilla/msm).', 'score': 5, 'issue_id': 3168, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '082fef36edfa1a3f', 'authors': ['Pedro Hermosilla', 'Christian Stippel', 'Leon Sick'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.06719.jpg', 'data': {'categories': ['#benchmark', '#training', '#cv', '#self_supervised', '#open_source', '#3d'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ 3D: Ğ¾Ñ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ 3D ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D ÑÑ†ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Masked Scene Modeling, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ supervised Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ self-supervised Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with Self-Supervised Learning', 'desc': 'This paper presents a new self-supervised learning approach for 3D scene understanding that enhances feature extraction without relying on labeled data. It introduces a robust evaluation protocol that assesses the quality of self-supervised features using multi-resolution sampling of hierarchical models. The authors propose a novel Masked Scene Modeling objective that reconstructs features from masked patches, allowing the model to learn effectively in a 3D context. Their experiments show that this method not only matches the performance of supervised models but also significantly outperforms existing self-supervised techniques.'}, 'zh': {'title': 'è‡ªç›‘ç£å­¦ä¹ åŠ©åŠ›ä¸‰ç»´åœºæ™¯ç†è§£', 'desc': 'è‡ªç›‘ç£å­¦ä¹ åœ¨äºŒç»´è®¡ç®—æœºè§†è§‰ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½¿å¾—åœ¨å¤§å‹æœªæ ‡æ³¨æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿæä¾›ç±»ä¼¼äºæœ‰æ ‡ç­¾æ¨¡å‹çš„å¤šåŠŸèƒ½ç‰¹å¾ã€‚ç„¶è€Œï¼Œåœ¨ä¸‰ç»´åœºæ™¯ç†è§£ä¸­ï¼Œè‡ªç›‘ç£æ–¹æ³•é€šå¸¸ä»…ç”¨äºæƒé‡åˆå§‹åŒ–ï¼Œé™åˆ¶äº†å…¶åœ¨é€šç”¨ç‰¹å¾æå–ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“é—¨è¯„ä¼°è‡ªç›‘ç£ç‰¹å¾è´¨é‡çš„ç¨³å¥è¯„ä¼°åè®®ï¼Œåˆ©ç”¨å¤šåˆ†è¾¨ç‡ç‰¹å¾é‡‡æ ·åˆ›å»ºä¸°å¯Œçš„ç‚¹çº§è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ç¬¬ä¸€ä¸ªåœ¨ä»…ä½¿ç”¨è‡ªç›‘ç£ç‰¹å¾çš„çº¿æ€§æ¢æµ‹è®¾ç½®ä¸­è¡¨ç°ä¸ç›‘ç£æ¨¡å‹ç›¸ä¼¼çš„è‡ªç›‘ç£æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸‰ç»´åœºæ™¯ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05523', 'title': 'Pretraining Language Models for Diachronic Linguistic Change Discovery', 'url': 'https://huggingface.co/papers/2504.05523', 'abstract': 'Large language models (LLMs) have shown potential as tools for scientific discovery. This has engendered growing interest in their use in humanistic disciplines, such as historical linguistics and literary studies. These fields often construct arguments on the basis of delineations like genre, or more inflexibly, time period. Although efforts have been made to restrict inference to specific domains via fine-tuning or model editing, we posit that the only true guarantee is domain-restricted pretraining -- typically, a data- and compute-expensive proposition.   We show that efficient pretraining techniques can produce useful models over corpora too large for easy manual inspection but too small for "typical" LLM approaches. We employ a novel date-attribution pipeline in order to obtain a temporally-segmented dataset of five 10-million-word slices. We train two corresponding five-model batteries over these corpus segments, efficient pretraining and Llama3-8B parameter efficiently finetuned.   We find that the pretrained models are faster to train than the finetuned baselines and that they better respect the historical divisions of our corpus. Emphasizing speed and precision over a-historical comprehensiveness enables a number of novel approaches to hypothesis discovery and testing in our target fields. Taking up diachronic linguistics as a testbed, we show that our method enables the detection of a diverse set of phenomena, including en masse lexical change, non-lexical (grammatical and morphological) change, and word sense introduction/obsolescence. We provide a ready-to-use pipeline that allows extension of our approach to other target fields with only minimal adaptation.', 'score': 4, 'issue_id': 3170, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '9bdf5f1c61fd329f', 'authors': ['Elisabeth Fittschen', 'Sabrina Li', 'Tom Lippincott', 'Leshem Choshen', 'Craig Messner'], 'affiliations': ['Center for Digital Humanities, Johns Hopkins University, USA', 'IBM Research, MIT, USA', 'University of Hamburg, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.05523.jpg', 'data': {'categories': ['#dataset', '#data', '#transfer_learning', '#science', '#training'], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…, Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… ÑĞ»Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ°. Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Llama3-8B, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ².'}, 'en': {'title': 'Efficient Pretraining for Targeted Language Models in Humanities', 'desc': 'This paper explores the use of large language models (LLMs) for scientific discovery in humanistic disciplines like historical linguistics and literary studies. The authors argue that domain-restricted pretraining is essential for effective model performance, especially when dealing with specific genres or time periods. They introduce efficient pretraining techniques that allow for the training of models on large datasets that are otherwise too small for traditional LLM methods. Their findings demonstrate that these pretrained models not only train faster but also better adhere to historical context, facilitating new approaches to hypothesis testing in diachronic linguistics.'}, 'zh': {'title': 'é«˜æ•ˆé¢„è®­ç»ƒï¼šæ¨åŠ¨äººæ–‡å­¦ç§‘çš„ç§‘å­¦å‘ç°', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦å‘ç°ä¸­å±•ç°äº†æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å†å²è¯­è¨€å­¦å’Œæ–‡å­¦ç ”ç©¶ç­‰äººæ–‡å­¦ç§‘ä¸­ã€‚è¿™äº›é¢†åŸŸé€šå¸¸åŸºäºä½“è£æˆ–æ—¶é—´æ®µç­‰åˆ’åˆ†æ¥æ„å»ºè®ºç‚¹ã€‚å°½ç®¡é€šè¿‡å¾®è°ƒæˆ–æ¨¡å‹ç¼–è¾‘æ¥é™åˆ¶æ¨ç†åˆ°ç‰¹å®šé¢†åŸŸçš„åŠªåŠ›å·²ç»å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºï¼Œå”¯ä¸€çœŸæ­£çš„ä¿è¯æ˜¯é¢†åŸŸé™åˆ¶çš„é¢„è®­ç»ƒï¼Œè¿™é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé‡‡ç”¨é«˜æ•ˆçš„é¢„è®­ç»ƒæŠ€æœ¯å¯ä»¥åœ¨æ•°æ®é‡è¿‡å¤§è€Œéš¾ä»¥æ‰‹åŠ¨æ£€æŸ¥ä½†åˆä¸è¶³ä»¥ä½¿ç”¨â€œå…¸å‹â€LLMæ–¹æ³•çš„è¯­æ–™åº“ä¸Šï¼Œäº§ç”Ÿæœ‰ç”¨çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06947', 'title': 'RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts', 'url': 'https://huggingface.co/papers/2504.06947', 'abstract': 'In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts.', 'score': 3, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '00cf6cc0a0c3d1c7', 'authors': ['Natalia Loukachevitch', 'Natalia Tkachenko', 'Anna Lapanitsyna', 'Mikhail Tikhomirov', 'Nicolay Rusnachenko'], 'affiliations': ['Bauman Moscow State Technical University', 'Lomonosov Moscow State University'], 'pdf_title_img': 'assets/pdf/title_img/2504.06947.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#training', '#multilingual', '#open_source'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°: ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€ÑƒÑÑĞºĞ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ£Ñ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¸ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ñ‹Ğ»Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ‚ĞµĞ¶Ğ¸ Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğµ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ¼Ğ½ĞµĞ½Ğ¸Ñ, ĞµĞ³Ğ¾ Ñ†ĞµĞ»Ğ¸, Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ½Ñ‚Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°. Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ñ… zero-shot, few-shot Ğ¸ fine-tuning. Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ±Ñ‹Ğ» Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Extracting Structured Opinions from Russian News Using Language Models', 'desc': 'This paper presents a shared task focused on extracting structured opinions from Russian news articles. The goal is to identify opinion tuples that include a sentiment holder, target, expression, and sentiment direction. Over 100 submissions were received, with participants primarily utilizing large language models in various training formats such as zero-shot, few-shot, and fine-tuning. The best performance on the test set was achieved through fine-tuning a large language model, and the study also evaluated different prompts and open-source models to determine the most effective combinations.'}, 'zh': {'title': 'ä»æ–°é—»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ„è§çš„æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…³äºä»ä¿„è¯­æ–°é—»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ„è§çš„å¯¹è¯è¯„ä¼°å…±äº«ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡è¦æ±‚å‚èµ›è€…ä¸ºç»™å®šå¥å­æå–æ„è§å…ƒç»„ï¼Œè¿™äº›å…ƒç»„ç”±æƒ…æ„ŸæŒæœ‰è€…ã€ç›®æ ‡ã€è¡¨è¾¾å’Œæƒ…æ„Ÿç»„æˆã€‚æ€»å…±æ”¶åˆ°äº†è¶…è¿‡100ä¸ªæäº¤ï¼Œå‚ä¸è€…ä¸»è¦ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒå®éªŒã€‚æµ‹è¯•é›†ä¸Šæœ€ä½³ç»“æœæ˜¯é€šè¿‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒè·å¾—çš„ï¼ŒåŒæ—¶æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†30ä¸ªæç¤ºå’Œ11ä¸ªå¼€æºè¯­è¨€æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»3äº¿åˆ°32äº¿ï¼Œæ‰¾å‡ºäº†æœ€ä½³æ¨¡å‹å’Œæç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03886', 'title': 'WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments', 'url': 'https://huggingface.co/papers/2504.03886', 'abstract': "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system designed to handle dynamic environments by leveraging uncertainty-aware geometric mapping. Unlike traditional SLAM systems, which assume static scenes, our approach integrates depth and uncertainty information to enhance tracking, mapping, and rendering performance in the presence of moving objects. We introduce an uncertainty map, predicted by a shallow multi-layer perceptron and DINOv2 features, to guide dynamic object removal during both tracking and mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map optimization, improving reconstruction accuracy. Our system is evaluated on multiple datasets and demonstrates artifact-free view synthesis. Results showcase WildGS-SLAM's superior performance in dynamic environments compared to state-of-the-art methods.", 'score': 3, 'issue_id': 3167, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '086d82b3ec2f4b04', 'authors': ['Jianhao Zheng', 'Zihan Zhu', 'Valentin Bieri', 'Marc Pollefeys', 'Songyou Peng', 'Iro Armeni'], 'affiliations': ['ETH ZÃ¼rich', 'Microsoft', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03886.jpg', 'data': {'categories': ['#benchmark', '#cv', '#3d'], 'emoji': 'ğŸŒªï¸', 'ru': {'title': 'Ğ Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ SLAM Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'WildGS-SLAM - ÑÑ‚Ğ¾ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ RGB SLAM, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ² Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² DINOv2, Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². WildGS-SLAM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Dynamic SLAM Redefined: Embracing Uncertainty for Robust Mapping', 'desc': 'WildGS-SLAM is a monocular RGB SLAM system that effectively operates in dynamic environments by incorporating uncertainty-aware geometric mapping. It differs from traditional SLAM systems by utilizing depth and uncertainty data to improve tracking and mapping when moving objects are present. The system employs an uncertainty map, generated by a shallow multi-layer perceptron and DINOv2 features, to facilitate the removal of dynamic objects during the SLAM process. Evaluations on various datasets reveal that WildGS-SLAM achieves high reconstruction accuracy and artifact-free view synthesis, outperforming existing methods in dynamic settings.'}, 'zh': {'title': 'åŠ¨æ€ç¯å¢ƒä¸‹çš„é«˜æ•ˆSLAMè§£å†³æ–¹æ¡ˆ', 'desc': 'WildGS-SLAMæ˜¯ä¸€ç§å¼ºå¤§ä¸”é«˜æ•ˆçš„å•ç›®RGB SLAMç³»ç»Ÿï¼Œä¸“ä¸ºå¤„ç†åŠ¨æ€ç¯å¢ƒè€Œè®¾è®¡ã€‚ä¸ä¼ ç»Ÿçš„SLAMç³»ç»Ÿå‡è®¾é™æ€åœºæ™¯ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æ·±åº¦å’Œä¸ç¡®å®šæ€§ä¿¡æ¯ï¼Œä»¥æé«˜åœ¨ç§»åŠ¨ç‰©ä½“å­˜åœ¨ä¸‹çš„è·Ÿè¸ªã€æ˜ å°„å’Œæ¸²æŸ“æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸ç¡®å®šæ€§åœ°å›¾ï¼Œé€šè¿‡æµ…å±‚å¤šå±‚æ„ŸçŸ¥å™¨å’ŒDINOv2ç‰¹å¾è¿›è¡Œé¢„æµ‹ï¼Œä»¥æŒ‡å¯¼åŠ¨æ€ç‰©ä½“çš„å»é™¤ï¼Œä»è€Œå¢å¼ºå¯†é›†æŸè°ƒæ•´å’Œé«˜æ–¯åœ°å›¾ä¼˜åŒ–ï¼Œæå‡é‡å»ºç²¾åº¦ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºWildGS-SLAMåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05410', 'title': 'Fast Controlled Generation from Language Models with Adaptive Weighted\n  Rejection Sampling', 'url': 'https://huggingface.co/papers/2504.05410', 'abstract': "The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed 100,000 tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.", 'score': 0, 'issue_id': 3176, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '39edf52d9587732f', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#training', '#optimization', '#data'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ¼ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²ĞµÑĞ¾Ğ² Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ±Ğ»Ğ¸Ğ·Ğ¾Ñ€ÑƒĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient Constraint Handling in Language Model Generation', 'desc': 'This paper presents a new algorithm for generating sequences from language models while adhering to specific constraints. The traditional method, locally constrained decoding (LCD), is inefficient because it evaluates constraints on a large vocabulary at each step, which can be computationally expensive. The proposed adaptive rejection sampling algorithm reduces the number of constraint evaluations significantly and provides unbiased estimates of importance weights. Empirical results demonstrate that this approach outperforms existing methods in various applications, enhancing both performance and efficiency.'}, 'zh': {'title': 'é«˜æ•ˆçš„çº¦æŸæ¡ä»¶ç”Ÿæˆç®—æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œç”¨äºåœ¨è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­å¤„ç†çº¦æŸæ¡ä»¶ã€‚ä¼ ç»Ÿçš„å±€éƒ¨çº¦æŸè§£ç æ–¹æ³•åœ¨æ¯ä¸€æ­¥éƒ½éœ€è¦è¯„ä¼°æ•´ä¸ªè¯æ±‡è¡¨ï¼Œæ•ˆç‡ä½ä¸‹ä¸”å¯èƒ½å¯¼è‡´å…¨å±€åˆ†å¸ƒå¤±çœŸã€‚æˆ‘ä»¬æå‡ºçš„è‡ªé€‚åº”æ‹’ç»é‡‡æ ·ç®—æ³•æ˜¾è‘—å‡å°‘äº†çº¦æŸè¯„ä¼°çš„æ¬¡æ•°ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆä½æ–¹å·®çš„æ— åé‡è¦æ€§æƒé‡ä¼°è®¡ã€‚é€šè¿‡åœ¨å¤šä¸ªé¢†åŸŸçš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ”¯æŒæ›´å¹¿æ³›çš„çº¦æŸæ¡ä»¶å’Œæé«˜è¿è¡Œæ—¶é—´åŠæ€§èƒ½æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.05287', 'title': 'RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception', 'url': 'https://huggingface.co/papers/2504.05287', 'abstract': 'Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/', 'score': 0, 'issue_id': 3165, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': 'fab0fa176a952987', 'authors': ['Hui Zhang', 'Zijian Wu', 'Linyi Huang', 'Sammy Christen', 'Jie Song'], 'affiliations': ['ETH Zurich, Switzerland', 'HKUST (Guangzhou), China', 'HKUST, Hong Kong (China)'], 'pdf_title_img': 'assets/pdf/title_img/2504.05287.jpg', 'data': {'categories': ['#robotics', '#training', '#games', '#rl', '#optimization'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾ÑĞ¾Ğ±Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ€ÑƒĞºĞ¾Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¿Ğ¾Ğ¼ĞµÑ…Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Dynamic Grasping: Robots That Adapt and Overcome!', 'desc': "This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot's ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions."}, 'zh': {'title': 'å®ç°çµå·§æŠ“å–çš„é›¶-shotå­¦ä¹ æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä»å•è§†è§’æ„ŸçŸ¥ä¸­å¯¹å„ç§æœªçŸ¥ç‰©ä½“çš„é›¶-shotåŠ¨æ€çµå·§æŠ“å–ã€‚ä¸ä»¥å¾€ä¾èµ–å®Œå…¨å¯è§‚å¯Ÿç‰©ä½“æˆ–ä¸“å®¶æ¼”ç¤ºçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”å¤–éƒ¨å¹²æ‰°å¹¶è¿›è¡Œè‡ªé€‚åº”åŠ¨ä½œã€‚æˆ‘ä»¬é‡‡ç”¨ä»¥æ‰‹ä¸ºä¸­å¿ƒçš„ç‰©ä½“è¡¨ç¤ºæ³•ï¼Œæå–ä¸äº¤äº’ç›¸å…³çš„å±€éƒ¨å½¢çŠ¶ç‰¹å¾ï¼Œä»è€Œå¢å¼ºå¯¹å½¢çŠ¶å˜åŒ–å’Œä¸ç¡®å®šæ€§çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŠ“å–æœªçŸ¥ç‰©ä½“æ—¶å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸç‡é«˜è¾¾97.0%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13161', 'title': 'CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training', 'url': 'https://huggingface.co/papers/2504.13161', 'abstract': 'Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The Pile is labor-intensive. Consequently, identifying an optimal pre-training data mixture remains a challenging problem, despite its significant benefits for pre-training performance. To address these challenges, we propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an automated framework that discovers, evaluates, and refines data mixtures in a pre-training setting. Specifically, CLIMB embeds and clusters large-scale datasets in a semantic space and then iteratively searches for optimal mixtures using a smaller proxy model and a predictor. When continuously trained on 400B tokens with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific domain (e.g., Social Sciences) yields a 5% improvement over random sampling. Finally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20 clusters as a research playground, and ClimbMix, a compact yet powerful 400-billion-token dataset designed for efficient pre-training that delivers superior performance under an equal token budget. We analyze the final data mixture, elucidating the characteristics of an optimal data mixture. Our data is available at: https://research.nvidia.com/labs/lpr/climb/', 'score': 81, 'issue_id': 3306, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': 'cf9b6f2a06448097', 'authors': ['Shizhe Diao', 'Yu Yang', 'Yonggan Fu', 'Xin Dong', 'Dan Su', 'Markus Kliegl', 'Zijia Chen', 'Peter Belcak', 'Yoshi Suhara', 'Hongxu Yin', 'Mostofa Patwary', 'Yingyan', 'Lin', 'Jan Kautz', 'Pavlo Molchanov'], 'affiliations': ['Georgia Institute of Technology, USA', 'NVIDIA', 'OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2504.13161.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#optimization', '#data'], 'emoji': 'ğŸ§—', 'ru': {'title': 'Ğ’Ğ¾ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CLIMB - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. CLIMB Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼ĞµÑĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ CLIMB Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Llama-3.2-1B Ğ½Ğ° 2%, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ClimbLab Ğ¸ ClimbMix - Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Optimizing Pre-Training Data with CLIMB for Superior Model Performance', 'desc': 'This paper introduces CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), a novel framework for optimizing pre-training datasets in machine learning. CLIMB automates the process of discovering and refining data mixtures by embedding and clustering large datasets in a semantic space, which helps in identifying the best combinations for training models. The results show that a model trained on a carefully optimized mixture of 400 billion tokens outperforms existing models, demonstrating the importance of domain-specific data selection. Additionally, the paper presents ClimbLab and ClimbMix, two new datasets designed to facilitate research and improve pre-training efficiency.'}, 'zh': {'title': 'ä¼˜åŒ–é¢„è®­ç»ƒæ•°æ®çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCLIMBçš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–é¢„è®­ç»ƒæ•°æ®çš„æ··åˆã€‚CLIMBé€šè¿‡åœ¨è¯­ä¹‰ç©ºé—´ä¸­åµŒå…¥å’Œèšç±»å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿­ä»£æœç´¢æœ€ä½³æ•°æ®ç»„åˆã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ··åˆæ•°æ®è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶ä¸”é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ä¼˜åŒ–å¯ä»¥æ˜¾è‘—æé«˜æ•ˆæœã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ClimbLabå’ŒClimbMixä¸¤ä¸ªæ•°æ®é›†ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œé«˜æ•ˆçš„é¢„è®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13146', 'title': 'Antidistillation Sampling', 'url': 'https://huggingface.co/papers/2504.13146', 'abstract': "Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.", 'score': 55, 'issue_id': 3304, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '6aa117b5c441eb3d', 'authors': ['Yash Savani', 'Asher Trockman', 'Zhili Feng', 'Avi Schwarzschild', 'Alexander Robey', 'Marc Finzi', 'J. Zico Kolter'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13146.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ñ‚Ğ¸Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ñ‚Ğ¸Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ½ĞµĞµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ°Ñ Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ.'}, 'en': {'title': 'Protecting Model Knowledge with Antidistillation Sampling', 'desc': "This paper discusses a method called antidistillation sampling, which aims to protect advanced models from being easily distilled into simpler versions. Distillation is a process where a complex model's knowledge is transferred to a smaller model, but this can be exploited if the complex model generates detailed reasoning traces. Antidistillation sampling works by altering the probability distribution of the next token generated by the model, making the reasoning less useful for distillation. This approach allows the model to maintain its performance while reducing the risk of its knowledge being easily extracted."}, 'zh': {'title': 'æŠ—è’¸é¦é‡‡æ ·ï¼šä¿æŠ¤æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°ç­–ç•¥', 'desc': 'å‰æ²¿æ¨¡å‹ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ä¼šäº§ç”Ÿä¸°å¯Œçš„æ ‡è®°åºåˆ—ï¼Œè¿™äº›åºåˆ—å¯ä»¥å¸®åŠ©æ¨¡å‹è’¸é¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æ¼æ´ï¼Œæ¨¡å‹æ‹¥æœ‰è€…å¯èƒ½ä¼šå¯»æ‰¾é‡‡æ ·ç­–ç•¥ï¼Œä»¥é™åˆ¶è’¸é¦çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¸å½±å“æ¨¡å‹æ€§èƒ½ã€‚æŠ—è’¸é¦é‡‡æ ·æ­£æ˜¯æä¾›è¿™ç§èƒ½åŠ›çš„ç­–ç•¥ã€‚é€šè¿‡æˆ˜ç•¥æ€§åœ°ä¿®æ”¹æ¨¡å‹çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¦‚ç‡åˆ†å¸ƒï¼ŒæŠ—è’¸é¦é‡‡æ ·å¯ä»¥ç ´åæ¨ç†è½¨è¿¹ï¼Œä½¿å…¶åœ¨è’¸é¦ä¸­å˜å¾—ä¸é‚£ä¹ˆæœ‰æ•ˆï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®é™…æ•ˆç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13169', 'title': 'Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling', 'url': 'https://huggingface.co/papers/2504.13169', 'abstract': 'Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io.', 'score': 36, 'issue_id': 3304, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': 'f1ebb64cfce24e47', 'authors': ['Tsung-Han Wu', 'Heekyung Lee', 'Jiaxin Ge', 'Joseph E. Gonzalez', 'Trevor Darrell', 'David M. Chan'], 'affiliations': ['POSTECH', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.13169.jpg', 'data': {'categories': ['#dataset', '#inference', '#hallucinations', '#data', '#cv'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'REVERSE: ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ VLM Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ REVERSE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. REVERSE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 1,3 Ğ¼Ğ»Ğ½ Ğ¿Ğ¾Ğ»ÑƒÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ REVERSE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 12-28% Ğ¿Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'REVERSE: Correcting Visual Hallucinations in VLMs Dynamically', 'desc': 'This paper addresses the issue of visual hallucinations in Vision-Language Models (VLMs), where models incorrectly describe non-existent elements. The authors propose a new framework called REVERSE, which combines hallucination-aware training with real-time self-verification to improve the accuracy of VLM outputs. By utilizing a large dataset of semi-synthetic samples, REVERSE can identify and correct hallucinations during the generation process. The results demonstrate that this approach significantly reduces hallucinations, outperforming existing methods in various benchmarks.'}, 'zh': {'title': 'REVERSEï¼šåŠ¨æ€ä¿®æ­£è§†è§‰å¹»è§‰çš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å‡ºç°è§†è§‰å¹»è§‰ï¼Œå³ç”Ÿæˆä¸å­˜åœ¨çš„ç‰©ä½“ã€åŠ¨ä½œæˆ–æ¦‚å¿µçš„æè¿°ï¼Œè¿™åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­å¸¦æ¥äº†é‡å¤§é£é™©ã€‚ç°æœ‰çš„å¹»è§‰ç¼“è§£æ–¹æ³•é€šå¸¸åˆ†ä¸ºä¸¤ç±»ï¼šç”Ÿæˆè°ƒæ•´å’ŒåæœŸéªŒè¯ã€‚ç”Ÿæˆè°ƒæ•´æ–¹æ³•ä¾èµ–å¯å‘å¼è§„åˆ™ï¼Œç¼ºä¹æœ‰æ•ˆçš„ä¿®æ­£æœºåˆ¶ï¼Œè€ŒåæœŸéªŒè¯åˆ™å¤æ‚ï¼Œé€šå¸¸éœ€è¦å¤šä¸ªæ¨¡å‹ï¼Œå¹¶å€¾å‘äºæ‹’ç»è¾“å‡ºè€Œä¸æ˜¯è¿›è¡Œä¿®æ­£ã€‚æˆ‘ä»¬æå‡ºçš„REVERSEæ¡†æ¶ç»“åˆäº†å¹»è§‰æ„ŸçŸ¥è®­ç»ƒå’Œå®æ—¶è‡ªæˆ‘éªŒè¯ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ£€æµ‹å¹»è§‰å¹¶åŠ¨æ€ä¿®æ­£ï¼Œä»è€Œæ˜¾è‘—é™ä½å¹»è§‰çš„å‘ç”Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12626', 'title': 'Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation', 'url': 'https://huggingface.co/papers/2504.12626', 'abstract': 'We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. The FramePack compresses input frames to make the transformer context length a fixed number regardless of the video length. As a result, we are able to process a large number of frames using video diffusion with computation bottleneck similar to image diffusion. This also makes the training video batch sizes significantly higher (batch sizes become comparable to image diffusion training). We also propose an anti-drifting sampling method that generates frames in inverted temporal order with early-established endpoints to avoid exposure bias (error accumulation over iterations). Finally, we show that existing video diffusion models can be finetuned with FramePack, and their visual quality may be improved because the next-frame prediction supports more balanced diffusion schedulers with less extreme flow shift timesteps.', 'score': 36, 'issue_id': 3303, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': 'fd1688a4e26dbb32', 'authors': ['Lvmin Zhang', 'Maneesh Agrawala'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.12626.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#video', '#training'], 'emoji': 'ğŸï¸', 'ru': {'title': 'FramePack: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'FramePack - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. FramePack Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾.'}, 'en': {'title': 'FramePack: Efficient Video Generation with Next-Frame Prediction', 'desc': 'The paper introduces FramePack, a novel neural network architecture designed for predicting the next frame in video generation. By compressing input frames, FramePack ensures that the transformer can handle a fixed context length, making it efficient for processing long videos. This approach allows for larger training batch sizes, similar to those used in image diffusion, while maintaining computational efficiency. Additionally, the authors present an anti-drifting sampling method to mitigate exposure bias, enhancing the quality of generated frames and improving existing video diffusion models through fine-tuning.'}, 'zh': {'title': 'FramePackï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ä¸‹ä¸€å¸§é¢„æµ‹èƒ½åŠ›', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¥ç»ç½‘ç»œç»“æ„ï¼ŒFramePackï¼Œç”¨äºè®­ç»ƒè§†é¢‘ç”Ÿæˆçš„ä¸‹ä¸€å¸§é¢„æµ‹æ¨¡å‹ã€‚FramePacké€šè¿‡å‹ç¼©è¾“å…¥å¸§ï¼Œä½¿å¾—å˜æ¢å™¨çš„ä¸Šä¸‹æ–‡é•¿åº¦å›ºå®šï¼Œæ— è®ºè§†é¢‘é•¿åº¦å¦‚ä½•ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨ä¸å›¾åƒæ‰©æ•£ç›¸ä¼¼çš„è®¡ç®—ç“¶é¢ˆå¤„ç†å¤§é‡å¸§ï¼Œä»è€Œæ˜¾è‘—æé«˜è§†é¢‘è®­ç»ƒçš„æ‰¹é‡å¤§å°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åæ¼‚ç§»é‡‡æ ·æ–¹æ³•ï¼Œä»¥é¿å…è¿­ä»£è¿‡ç¨‹ä¸­çš„æ›å…‰åå·®ï¼Œä»è€Œæé«˜ç”Ÿæˆå¸§çš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12369', 'title': 'WORLDMEM: Long-term Consistent World Simulation with Memory', 'url': 'https://huggingface.co/papers/2504.12369', 'abstract': 'World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing a memory attention mechanism that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.', 'score': 28, 'issue_id': 3303, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': '79cf162a1b60f887', 'authors': ['Zeqi Xiao', 'Yushi Lan', 'Yifan Zhou', 'Wenqi Ouyang', 'Shuai Yang', 'Yanhong Zeng', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai AI Laboratory', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.12369.jpg', 'data': {'categories': ['#multimodal', '#3d', '#long_context'], 'emoji': 'ğŸŒ', 'ru': {'title': 'WorldMem: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'WorldMem - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ°Ğ½ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ¸Ğ»Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚ĞºĞ°Ñ…. Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€, Ğ½Ğ¾ Ğ¸ ĞµĞ³Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Enhancing World Simulation with Memory-Driven Consistency', 'desc': 'This paper introduces WorldMem, a novel framework designed to improve world simulation by utilizing a memory bank that stores various memory frames and states. The framework addresses the challenge of maintaining long-term consistency in 3D spatial representations, which is often hindered by limited temporal context. By implementing a memory attention mechanism, WorldMem can retrieve relevant information from stored memory frames, allowing for accurate scene reconstruction despite changes in viewpoint or time. Additionally, the integration of timestamps enables the framework to model both static and dynamic aspects of the environment, enhancing interaction and perception in simulated worlds.'}, 'zh': {'title': 'å¢å¼ºä¸–ç•Œæ¨¡æ‹Ÿçš„ä¸€è‡´æ€§ä¸åŠ¨æ€æ€§', 'desc': 'ä¸–ç•Œæ¨¡æ‹Ÿå› å…¶å»ºæ¨¡è™šæ‹Ÿç¯å¢ƒå’Œé¢„æµ‹è¡Œä¸ºåæœçš„èƒ½åŠ›è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œæœ‰é™çš„æ—¶é—´ä¸Šä¸‹æ–‡çª—å£å¸¸å¸¸å¯¼è‡´é•¿æœŸä¸€è‡´æ€§ç»´æŠ¤çš„å¤±è´¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒä¸‰ç»´ç©ºé—´ä¸€è‡´æ€§æ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WorldMemæ¡†æ¶ï¼Œé€šè¿‡ä¸€ä¸ªåŒ…å«è®°å¿†å•å…ƒçš„è®°å¿†åº“æ¥å¢å¼ºåœºæ™¯ç”Ÿæˆï¼Œè¿™äº›è®°å¿†å•å…ƒå­˜å‚¨è®°å¿†å¸§å’ŒçŠ¶æ€ï¼ˆä¾‹å¦‚ï¼Œå§¿åŠ¿å’Œæ—¶é—´æˆ³ï¼‰ã€‚é€šè¿‡é‡‡ç”¨è®°å¿†æ³¨æ„æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®é‡å»ºå…ˆå‰è§‚å¯Ÿåˆ°çš„åœºæ™¯ï¼Œå³ä½¿åœ¨æ˜¾è‘—çš„è§†è§’æˆ–æ—¶é—´é—´éš”ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆæå–ç›¸å…³ä¿¡æ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12322', 'title': 'A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis', 'url': 'https://huggingface.co/papers/2504.12322', 'abstract': 'While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA.', 'score': 25, 'issue_id': 3307, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'cc80e8015cf7f78d', 'authors': ['Xin Gao', 'Qizhi Pei', 'Zinan Tang', 'Yu Li', 'Honglin Lin', 'Jiang Wu', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2504.12322.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#synthetic', '#small_models', '#open_source'], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞšĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº GRA, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°Ğ»Ñ‹Ñ… LLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ñ€Ğ¾Ğ»Ğ¸ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ° Ğ¸ ĞÑ€Ğ±Ğ¸Ñ‚Ñ€Ğ°, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… LLM, Ğ½Ğ¾ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ GRA, Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… LLM, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Qwen-2.5-72B-Instruct.'}, 'en': {'title': 'Collaborative Small Models for High-Quality Data Synthesis', 'desc': 'This paper introduces a framework called GRA, which uses multiple small language models (LLMs) to improve data synthesis and distillation. Instead of relying on a single large LLM, GRA assigns specialized roles to smaller models: a Generator creates data samples, a Reviewer evaluates their quality, and an Adjudicator resolves any discrepancies. This collaborative approach mimics peer review processes, allowing for iterative refinement and quality control. The results show that the data produced by GRA can match or even surpass the quality of data generated by large LLMs, suggesting that smaller models can be effectively coordinated for high-quality outcomes.'}, 'zh': {'title': 'å°æ¨¡å‹åä½œï¼Œè¶…è¶Šå¤§æ¨¡å‹çš„é«˜è´¨é‡æ•°æ®åˆæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGRAçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤šä¸ªå°å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åä½œæ¥ç”Ÿæˆé«˜è´¨é‡çš„æ•°æ®ã€‚è¯¥æ¡†æ¶æ¨¡æ‹Ÿäº†åŒè¡Œè¯„å®¡çš„è¿‡ç¨‹ï¼Œåˆ†é…äº†ç”Ÿæˆå™¨ã€å®¡é˜…è€…å’Œè£å†³è€…ç­‰ä¸åŒè§’è‰²ï¼Œä»¥å®ç°æ•°æ®åˆæˆçš„è¿­ä»£ä¼˜åŒ–å’Œè´¨é‡æ§åˆ¶ã€‚é€šè¿‡å°†åˆæˆè¿‡ç¨‹åˆ†è§£ä¸ºä¸“é—¨çš„å­ä»»åŠ¡ï¼ŒGRAèƒ½å¤Ÿåœ¨æ•°æ®è´¨é‡ä¸Šä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸åª²ç¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRAç”Ÿæˆçš„æ•°æ®åœ¨è´¨é‡ä¸Šä¸å•ä¸€å¤§å‹æ¨¡å‹çš„è¾“å‡ºç›¸å½“æˆ–æ›´ä¼˜ï¼ŒæŒ‘æˆ˜äº†ä¾èµ–å¤§å‹æ¨¡å‹è¿›è¡Œé«˜è´¨é‡æ•°æ®åˆæˆçš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13181', 'title': 'Perception Encoder: The best visual embeddings are not at the output of\n  the network', 'url': 'https://huggingface.co/papers/2504.13181', 'abstract': 'We introduce Perception Encoder (PE), a state-of-the-art encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods, language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together with the core contrastive checkpoint, our PE family of models achieves state-of-the-art performance on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, depth estimation, and tracking. To foster further research, we are releasing our models, code, and a novel dataset of synthetically and human-annotated videos.', 'score': 20, 'issue_id': 3313, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '3c5cea92d3dd07c4', 'authors': ['Daniel Bolya', 'Po-Yao Huang', 'Peize Sun', 'Jang Hyun Cho', 'Andrea Madotto', 'Chen Wei', 'Tengyu Ma', 'Jiale Zhi', 'Jathushan Rajasegaran', 'Hanoona Rasheed', 'Junke Wang', 'Marco Monteiro', 'Hu Xu', 'Shiyu Dong', 'Nikhila Ravi', 'Daniel Li', 'Piotr DollÃ¡r', 'Christoph Feichtenhofer'], 'affiliations': ['Fudan University', 'MBZUAI', 'Meta FAIR', 'Meta Reality Labs', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2504.13181.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#dataset', '#alignment', '#open_source', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Perception Encoder (PE) - ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², PE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² ÑĞµÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° PE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Unlocking Versatile Visual Understanding with Perception Encoder', 'desc': "The paper presents the Perception Encoder (PE), an advanced model designed for understanding images and videos through vision-language learning. Unlike traditional encoders that depend on specific pretraining tasks, PE utilizes contrastive vision-language training to create versatile embeddings applicable to various tasks. The authors introduce two alignment techniques to extract these embeddings from the network's intermediate layers, enhancing performance in tasks like classification and question answering. By releasing their models and a new dataset, they aim to encourage further exploration in the field of multimodal learning."}, 'zh': {'title': 'æ„ŸçŸ¥ç¼–ç å™¨ï¼šå›¾åƒä¸è§†é¢‘ç†è§£çš„æ–°çªç ´', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºæ„ŸçŸ¥ç¼–ç å™¨ï¼ˆPEï¼‰çš„å…ˆè¿›ç¼–ç å™¨ï¼Œç”¨äºå›¾åƒå’Œè§†é¢‘ç†è§£ï¼Œé‡‡ç”¨ç®€å•çš„è§†è§‰-è¯­è¨€å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚ä¼ ç»Ÿçš„è§†è§‰ç¼–ç å™¨ä¾èµ–äºå¤šç§é¢„è®­ç»ƒç›®æ ‡ï¼Œé’ˆå¯¹ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡å¦‚åˆ†ç±»ã€æè¿°æˆ–å®šä½è¿›è¡Œä¼˜åŒ–ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé€šè¿‡æ‰©å±•æˆ‘ä»¬ç²¾å¿ƒè°ƒæ•´çš„å›¾åƒé¢„è®­ç»ƒæ–¹æ¡ˆï¼Œå¹¶ç»“åˆå¼ºå¤§çš„è§†é¢‘æ•°æ®å¼•æ“ï¼Œå‘ç°å¯¹æ¯”è§†è§‰-è¯­è¨€è®­ç»ƒå¯ä»¥ä¸ºæ‰€æœ‰è¿™äº›ä¸‹æ¸¸ä»»åŠ¡ç”Ÿæˆå¼ºå¤§çš„é€šç”¨åµŒå…¥ã€‚ä¸ºäº†æå–è¿™äº›åµŒå…¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§å¯¹é½æ–¹æ³•ï¼Œåˆ†åˆ«ç”¨äºå¤šæ¨¡æ€è¯­è¨€å»ºæ¨¡å’Œå¯†é›†é¢„æµ‹ï¼Œä»è€Œä½¿æˆ‘ä»¬çš„PEæ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13122', 'title': 'VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models', 'url': 'https://huggingface.co/papers/2504.13122', 'abstract': 'Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) Instance Level, aligning overall video content with responses; ii) Temporal Level, aligning video temporal semantics with event descriptions; and iii) Perceptive Level, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination. The code and data are available at https://github.com/HaroldChen19/VistaDPO.', 'score': 20, 'issue_id': 3303, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': 'efbc3b240498ce70', 'authors': ['Haojian Huang', 'Haodong Chen', 'Shengqiong Wu', 'Meng Luo', 'Jinlan Fu', 'Xinya Du', 'Hanwang Zhang', 'Hao Fei'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong', 'University of Texas at Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2504.13122.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#alignment', '#hallucinations', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VistaDPO: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…', 'desc': 'VistaDPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…: Ğ¾Ğ±Ñ‰ĞµĞ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VistaDPO-7k Ñ 7200 Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ€Ğ³Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ¼ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VistaDPO Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Aligning Video and Language: Introducing VistaDPO', 'desc': 'This paper presents VistaDPO, a new framework designed to improve the alignment between video content and human language understanding in Large Video Models (LVMs). It operates on three hierarchical levels: aligning overall video content with user responses, matching temporal semantics of videos with event descriptions, and correlating spatial objects with language tokens. To support this framework, the authors created a dataset called VistaDPO-7k, which includes 7.2K question-answer pairs with detailed annotations for better preference alignment. The results show that VistaDPO enhances the performance of LVMs by reducing issues related to video hallucination and misalignment with human intuition.'}, 'zh': {'title': 'VistaDPOï¼šæå‡è§†é¢‘ç†è§£çš„åå¥½å¯¹é½', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ¡†æ¶VistaDPOï¼Œç”¨äºè§†é¢‘å±‚æ¬¡ç©ºé—´-æ—¶é—´ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è§†é¢‘æ¨¡å‹ï¼ˆLVMsï¼‰åœ¨è§†é¢‘ç†è§£ä¸­çš„äººç±»ç›´è§‰ä¸ä¸€è‡´å’Œè§†é¢‘å¹»è§‰é—®é¢˜ã€‚VistaDPOé€šè¿‡ä¸‰ä¸ªå±‚æ¬¡å¢å¼ºæ–‡æœ¬-è§†é¢‘åå¥½å¯¹é½ï¼šå®ä¾‹å±‚ã€æ—¶é—´å±‚å’Œæ„ŸçŸ¥å±‚ï¼Œåˆ†åˆ«å¯¹é½è§†é¢‘å†…å®¹ã€æ—¶é—´è¯­ä¹‰å’Œç©ºé—´å¯¹è±¡ã€‚ä¸ºäº†æ”¯æŒç»†ç²’åº¦è§†é¢‘-è¯­è¨€åå¥½å¯¹é½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†VistaDPO-7kæ•°æ®é›†ï¼ŒåŒ…å«7200ä¸ªé—®ç­”å¯¹åŠå…¶ç©ºé—´-æ—¶é—´ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVistaDPOæ˜¾è‘—æå‡äº†ç°æœ‰LVMsçš„æ€§èƒ½ï¼Œæœ‰æ•ˆå‡è½»äº†è§†é¢‘-è¯­è¨€çš„ä¸ä¸€è‡´æ€§å’Œå¹»è§‰ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05506', 'title': 'ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering', 'url': 'https://huggingface.co/papers/2504.05506', 'abstract': 'Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.', 'score': 19, 'issue_id': 3303, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': 'a727d08eac22e920', 'authors': ['Ahmed Masry', 'Mohammed Saidul Islam', 'Mahir Ahmed', 'Aayush Bajaj', 'Firoz Kabir', 'Aaryaman Kartha', 'Md Tahmid Rahman Laskar', 'Mizanur Rahman', 'Shadikur Rahman', 'Mehrad Shahmohammadi', 'Megh Thakkar', 'Md Rizwan Parvez', 'Enamul Hoque', 'Shafiq Joty'], 'affiliations': ['Dialpad Inc., Canada', 'MILA - Quebec AI Institute, Canada', 'Nanyang Technological University, Singapore', 'Qatar Computing Research Institute (QCRI)', 'RBC, Canada', 'Salesforce Research, USA', 'York University, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2504.05506.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#cv', '#dataset', '#reasoning'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ChartQAPro: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²', 'desc': 'ChartQAPro - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1300 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 2000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ChartQAPro Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ñ… Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'ChartQAPro: Elevating Chart Understanding for AI', 'desc': 'This paper introduces ChartQAPro, a new benchmark designed to improve Chart Question Answering (CQA) systems by providing a more diverse and realistic set of charts and questions. Unlike previous benchmarks, ChartQAPro includes 1,341 charts from 157 sources and features various question types, which better reflect the complexities of real-world data analysis. The study reveals that modern large vision-language models (LVLMs) struggle significantly with this new benchmark, demonstrating a performance drop from 90.5% on the previous ChartQA to only 55.81% on ChartQAPro. Through error analyses and ablation studies, the authors identify challenges in chart reasoning that can guide future improvements in LVLMs.'}, 'zh': {'title': 'æå‡å›¾è¡¨é—®ç­”ç³»ç»Ÿçš„æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ChartQAProï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æé«˜å›¾è¡¨é—®ç­”ç³»ç»Ÿçš„æ€§èƒ½ã€‚å®ƒåŒ…å«æ¥è‡ª157ä¸ªä¸åŒæ¥æºçš„1,341ä¸ªå›¾è¡¨ï¼Œæ¶µç›–å¤šç§å›¾è¡¨ç±»å‹ï¼Œå¹¶æä¾›1,948ä¸ªå¤šæ ·åŒ–çš„é—®é¢˜ã€‚é€šè¿‡å¯¹21ä¸ªæ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ç°ä»£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ChartQAProä¸Šçš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºå›¾è¡¨æ¨ç†çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜åŒ…æ‹¬è¯¦ç»†çš„é”™è¯¯åˆ†æå’Œæ¶ˆèç ”ç©¶ï¼Œä»¥è¯†åˆ«å›¾è¡¨ç†è§£å’Œæ¨ç†ä¸­çš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13055', 'title': 'NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation', 'url': 'https://huggingface.co/papers/2504.13055', 'abstract': 'Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to more effectively scale test-time compute remains underexplored in VLMs. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, a simple yet effective RL approach that mixes trajectories from both clean and moderately distorted images to introduce targeted diversity in visual perception and the resulting reasoning patterns. Without additional training cost, NoisyRollout enhances the exploration capabilities of VLMs by incorporating a vision-oriented inductive bias. Furthermore, NoisyRollout employs a noise annealing schedule that gradually reduces distortion strength over training, ensuring benefit from noisy signals early while maintaining training stability and scalability in later stages. With just 2.1K training samples, NoisyRollout achieves state-of-the-art performance among open-source RL-tuned models on 5 out-of-domain benchmarks spanning both reasoning and perception tasks, while preserving comparable or even better in-domain performance.', 'score': 16, 'issue_id': 3307, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '058d7aa285231e64', 'authors': ['Xiangyan Liu', 'Jinjie Ni', 'Zijian Wu', 'Chao Du', 'Longxu Dou', 'Haonan Wang', 'Tianyu Pang', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'Sea AI Lab, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.13055.jpg', 'data': {'categories': ['#rlhf', '#rl', '#training', '#multimodal', '#reasoning', '#optimization'], 'emoji': 'ğŸ­', 'ru': {'title': 'NoisyRollout: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ’Ğ¯Ğœ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑˆÑƒĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NoisyRollout. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ¸ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. NoisyRollout Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑĞ¸Ğ»Ñƒ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ²Ğ½ĞµĞ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 2100 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Enhancing VLMs with NoisyRollout for Better Reasoning and Perception', 'desc': 'This paper introduces NoisyRollout, a reinforcement learning method designed to improve vision-language models (VLMs) by enhancing their policy exploration capabilities. It addresses the challenge of imperfect visual perception by mixing clean and distorted image trajectories, which helps diversify the reasoning patterns of the models. The approach uses a noise annealing schedule to gradually reduce distortion, allowing the model to benefit from noisy inputs during early training while ensuring stability later on. Remarkably, NoisyRollout achieves state-of-the-art results on various benchmarks with minimal training samples, demonstrating its effectiveness in both reasoning and perception tasks.'}, 'zh': {'title': 'NoisyRolloutï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›å±•å¢å¼ºäº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨VLMä¸­ï¼Œå¢å¼ºç­–ç•¥æ¢ç´¢ä»¥æ›´æœ‰æ•ˆåœ°æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ä»ç„¶æœªè¢«å……åˆ†ç ”ç©¶ã€‚æ­¤å¤–ï¼ŒVLMåœ¨è§†è§‰æ„ŸçŸ¥ä¸å®Œç¾æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™å½±å“äº†åç»­çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†NoisyRolloutï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„RLæ–¹æ³•ï¼Œé€šè¿‡æ··åˆå¹²å‡€å’Œé€‚åº¦å¤±çœŸçš„å›¾åƒè½¨è¿¹ï¼Œå¼•å…¥ç›®æ ‡å¤šæ ·æ€§ï¼Œä»è€Œæ”¹å–„è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ¨¡å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12364', 'title': 'DMM: Building a Versatile Image Generation Model via Distillation-Based\n  Model Merging', 'url': 'https://huggingface.co/papers/2504.12364', 'abstract': 'The success of text-to-image (T2I) generation models has spurred a proliferation of numerous model checkpoints fine-tuned from the same base model on various specialized datasets. This overwhelming specialized model production introduces new challenges for high parameter redundancy and huge storage cost, thereby necessitating the development of effective methods to consolidate and unify the capabilities of diverse powerful models into a single one. A common practice in model merging adopts static linear interpolation in the parameter space to achieve the goal of style mixing. However, it neglects the features of T2I generation task that numerous distinct models cover sundry styles which may lead to incompatibility and confusion in the merged model. To address this issue, we introduce a style-promptable image generation pipeline which can accurately generate arbitrary-style images under the control of style vectors. Based on this design, we propose the score distillation based model merging paradigm (DMM), compressing multiple models into a single versatile T2I model. Moreover, we rethink and reformulate the model merging task in the context of T2I generation, by presenting new merging goals and evaluation protocols. Our experiments demonstrate that DMM can compactly reorganize the knowledge from multiple teacher models and achieve controllable arbitrary-style generation.', 'score': 14, 'issue_id': 3306, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': '49755535f03790d4', 'authors': ['Tianhui Song', 'Weixin Feng', 'Shuai Wang', 'Xubin Li', 'Tiezheng Ge', 'Bo Zheng', 'Limin Wang'], 'affiliations': ['Nanjing University', 'Nanjing University, Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2504.12364.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#cv', '#diffusion', '#training', '#dataset'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ T2I Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ (T2I). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹, Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº (DMM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ´Ğ½Ñƒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ T2I Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DMM Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ¸Ğ»Ñ.'}, 'en': {'title': 'Unifying Diverse T2I Models with Style-Promptable Merging', 'desc': 'This paper addresses the challenges posed by the proliferation of specialized text-to-image (T2I) generation models, which often lead to high parameter redundancy and storage costs. The authors propose a novel approach called score distillation based model merging (DMM) that consolidates multiple models into a single, versatile T2I model. Unlike traditional methods that use static linear interpolation, DMM incorporates style vectors to enable accurate generation of images in various styles while avoiding incompatibility issues. The paper also introduces new goals and evaluation protocols for model merging in the context of T2I generation, demonstrating that DMM effectively reorganizes knowledge from multiple models for controllable style generation.'}, 'zh': {'title': 'é«˜æ•ˆåˆå¹¶å¤šæ¨¡å‹ï¼Œå®ç°å¯æ§å›¾åƒç”Ÿæˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆT2Iï¼‰çš„åˆå¹¶é—®é¢˜ã€‚éšç€ä¼—å¤šæ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šå¾®è°ƒï¼Œäº§ç”Ÿäº†å¤§é‡çš„ä¸“ç”¨æ¨¡å‹ï¼Œè¿™å¯¼è‡´äº†å‚æ•°å†—ä½™å’Œå­˜å‚¨æˆæœ¬é«˜çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯„åˆ†è’¸é¦çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ï¼ˆDMMï¼‰ï¼Œèƒ½å¤Ÿå°†å¤šä¸ªæ¨¡å‹å‹ç¼©ä¸ºä¸€ä¸ªå¤šåŠŸèƒ½çš„T2Iæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDMMèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ï¼Œå®ç°å¯æ§çš„ä»»æ„é£æ ¼å›¾åƒç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13180', 'title': 'PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding', 'url': 'https://huggingface.co/papers/2504.13180', 'abstract': 'Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM-VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about "what", "where", "when", and "how" of a video. We make our work fully reproducible by providing data, training recipes, code & models.', 'score': 13, 'issue_id': 3323, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': 'b923d02610ef004b', 'authors': ['Jang Hyun Cho', 'Andrea Madotto', 'Effrosyni Mavroudi', 'Triantafyllos Afouras', 'Tushar Nagarajan', 'Muhammad Maaz', 'Yale Song', 'Tengyu Ma', 'Shuming Hu', 'Suyog Jain', 'Miguel Martin', 'Huiyu Wang', 'Hanoona Rasheed', 'Peize Sun', 'Po-Yao Huang', 'Daniel Bolya', 'Nikhila Ravi', 'Shashank Jain', 'Tammy Stark', 'Shane Moon', 'Babak Damavandi', 'Vivian Lee', 'Andrew Westbury', 'Salman Khan', 'Philipp KrÃ¤henbÃ¼hl', 'Piotr DollÃ¡r', 'Lorenzo Torresani', 'Kristen Grauman', 'Christoph Feichtenhofer'], 'affiliations': ['MBZUAI', 'Meta FAIR', 'Meta Reality Labs', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2504.13180.jpg', 'data': {'categories': ['#dataset', '#open_source', '#synthetic', '#training', '#reasoning', '#benchmark', '#data', '#cv', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (PLM) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ 2,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ PLM-VideoBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Open and Reproducible Video Understanding with PLM', 'desc': 'This paper focuses on creating a Perception Language Model (PLM) that is fully open and reproducible, addressing the limitations of closed-source models in computer vision. The authors highlight the challenges of using black-box models for training data labeling, which hinders scientific progress. They propose a new approach that utilizes large-scale synthetic data and provides 2.8 million human-labeled video question-answer pairs to improve video understanding. Additionally, they introduce PLM-VideoBench, a comprehensive evaluation suite for assessing video reasoning tasks, ensuring transparency and reproducibility in their research.'}, 'zh': {'title': 'å¼€æ”¾é€æ˜çš„æ„ŸçŸ¥è¯­è¨€æ¨¡å‹ç ”ç©¶', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§å¼€æ”¾ä¸”å¯é‡å¤çš„æ„ŸçŸ¥è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œæ—¨åœ¨ä¿ƒè¿›å›¾åƒå’Œè§†é¢‘ç†è§£çš„é€æ˜ç ”ç©¶ã€‚æˆ‘ä»¬åˆ†æäº†æ ‡å‡†è®­ç»ƒæµç¨‹ï¼Œé¿å…ä½¿ç”¨å°é—­æ¨¡å‹çš„è’¸é¦æ–¹æ³•ï¼Œå¹¶æ¢ç´¢å¤§è§„æ¨¡åˆæˆæ•°æ®ä»¥è¯†åˆ«è§†é¢‘ç†è§£ä¸­çš„å…³é”®æ•°æ®ç¼ºå£ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºå£ï¼Œæˆ‘ä»¬å‘å¸ƒäº†280ä¸‡ä¸ªäººå·¥æ ‡æ³¨çš„ç»†ç²’åº¦è§†é¢‘é—®ç­”å¯¹å’Œæ—¶ç©ºåŸºç¡€çš„è§†é¢‘æ ‡é¢˜ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†PLM-VideoBenchï¼Œä¸€ä¸ªè¯„ä¼°è§†é¢‘ç†è§£ä»»åŠ¡çš„å·¥å…·ï¼Œä¸“æ³¨äºæ¨ç†è§†é¢‘çš„â€œä»€ä¹ˆâ€ã€â€œå“ªé‡Œâ€ã€â€œä½•æ—¶â€å’Œâ€œå¦‚ä½•â€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13171', 'title': 'Sleep-time Compute: Beyond Inference Scaling at Test-time', 'url': 'https://huggingface.co/papers/2504.13171', 'abstract': 'Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to "think" offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.', 'score': 13, 'issue_id': 3310, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '094d6f648b644327', 'authors': ['Kevin Lin', 'Charlie Snell', 'Yu Wang', 'Charles Packer', 'Sarah Wooders', 'Ion Stoica', 'Joseph E. Gonzalez'], 'affiliations': ['Letta', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.13171.jpg', 'data': {'categories': ['#optimization', '#inference', '#reasoning', '#agents'], 'emoji': 'ğŸ’¤', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LLM: Ğ´ÑƒĞ¼Ğ°Ğ¹ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ', 'desc': "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ½Ğ°' Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¿ÑÑ‚Ğ¸ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° 'Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ½Ğ°'."}, 'en': {'title': 'Optimize Query Processing with Sleep-Time Compute', 'desc': 'This paper presents a novel approach called sleep-time compute, which allows large language models (LLMs) to prepare for user queries by pre-computing relevant information offline. By anticipating potential questions, the model can significantly reduce the computational load during test-time, achieving up to 5 times less compute while maintaining accuracy. The authors demonstrate that scaling sleep-time compute can enhance accuracy by up to 18% on specific reasoning tasks. Additionally, they introduce Multi-Query GSM-Symbolic, which optimizes the processing of multiple related queries, further decreasing the average cost per query by 2.5 times.'}, 'zh': {'title': 'ç¡çœ æ—¶é—´è®¡ç®—ï¼šæå‡è¯­è¨€æ¨¡å‹æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®¡ç®—æ–¹æ³•ï¼Œç§°ä¸ºç¡çœ æ—¶é—´è®¡ç®—ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„è®¡ç®—éœ€æ±‚ã€‚é€šè¿‡åœ¨ç”¨æˆ·æå‡ºæŸ¥è¯¢ä¹‹å‰ï¼Œæ¨¡å‹å¯ä»¥ç¦»çº¿æ€è€ƒå¹¶é¢„è®¡ç®—æœ‰ç”¨çš„ä¿¡æ¯ï¼Œä»è€Œé™ä½å»¶è¿Ÿå’Œæ¨ç†æˆæœ¬ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ç¡çœ æ—¶é—´è®¡ç®—å¯ä»¥åœ¨ä¿æŒç›¸åŒå‡†ç¡®ç‡çš„æƒ…å†µä¸‹ï¼Œå°†æµ‹è¯•æ—¶çš„è®¡ç®—éœ€æ±‚å‡å°‘çº¦5å€ï¼Œå¹¶åœ¨æŸäº›ä»»åŠ¡ä¸Šæé«˜å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šæŸ¥è¯¢GSM-ç¬¦å·åŒ–ï¼Œå…è®¸åœ¨åŒä¸€ä¸Šä¸‹æ–‡ä¸­å¤„ç†å¤šä¸ªç›¸å…³æŸ¥è¯¢ï¼Œä»è€Œè¿›ä¸€æ­¥é™ä½æ¯ä¸ªæŸ¥è¯¢çš„å¹³å‡æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12395', 'title': 'InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework', 'url': 'https://huggingface.co/papers/2504.12395', 'abstract': 'Current learning-based subject customization approaches, predominantly relying on U-Net architectures, suffer from limited generalization ability and compromised image quality. Meanwhile, optimization-based methods require subject-specific fine-tuning, which inevitably degrades textual controllability. To address these challenges, we propose InstantCharacter, a scalable framework for character customization built upon a foundation diffusion transformer. InstantCharacter demonstrates three fundamental advantages: first, it achieves open-domain personalization across diverse character appearances, poses, and styles while maintaining high-fidelity results. Second, the framework introduces a scalable adapter with stacked transformer encoders, which effectively processes open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers. Third, to effectively train the framework, we construct a large-scale character dataset containing 10-million-level samples. The dataset is systematically organized into paired (multi-view character) and unpaired (text-image combinations) subsets. This dual-data structure enables simultaneous optimization of identity consistency and textual editability through distinct learning pathways. Qualitative experiments demonstrate the advanced capabilities of InstantCharacter in generating high-fidelity, text-controllable, and character-consistent images, setting a new benchmark for character-driven image generation. Our source code is available at https://github.com/Tencent/InstantCharacter.', 'score': 13, 'issue_id': 3308, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': '4000d7f0fc525f6d', 'authors': ['Jiale Tao', 'Yanbing Zhang', 'Qixun Wang', 'Yiji Cheng', 'Haofan Wang', 'Xu Bai', 'Zhengguang Zhou', 'Ruihuang Li', 'Linqing Wang', 'Chunyu Wang', 'Qin Lin', 'Qinglin Lu'], 'affiliations': ['Hunyuan, Tencent', 'InstantX Team'], 'pdf_title_img': 'assets/pdf/title_img/2504.12395.jpg', 'data': {'categories': ['#open_source', '#data', '#diffusion', '#dataset', '#benchmark', '#cv'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ', 'desc': 'InstantCharacter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğµ. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ ÑĞ¾ ÑÑ‚ĞµĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ² Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'InstantCharacter: Revolutionizing Character Customization with Diffusion Transformers', 'desc': 'This paper introduces InstantCharacter, a new framework for customizing characters using machine learning. Unlike traditional methods that struggle with generalization and image quality, InstantCharacter leverages a diffusion transformer to achieve high-fidelity results across various character styles and poses. The framework includes a scalable adapter with transformer encoders that effectively manage character features and interact with the latent space of diffusion models. Additionally, it utilizes a large-scale dataset of 10 million samples to optimize both identity consistency and textual editability, demonstrating superior performance in generating controllable character images.'}, 'zh': {'title': 'InstantCharacterï¼šé«˜ä¿çœŸè§’è‰²å®šåˆ¶çš„æ–°æ ‡å‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºInstantCharacterçš„è§’è‰²å®šåˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å­¦ä¹ åŸºç¡€æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚è¯¥æ¡†æ¶åŸºäºæ‰©æ•£å˜æ¢å™¨ï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„è§’è‰²å¤–è§‚ã€å§¿åŠ¿å’Œé£æ ¼ä¸­å®ç°å¼€æ”¾åŸŸä¸ªæ€§åŒ–ï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸåº¦ã€‚InstantCharacterå¼•å…¥äº†å¯æ‰©å±•çš„é€‚é…å™¨ï¼Œåˆ©ç”¨å †å çš„å˜æ¢å™¨ç¼–ç å™¨æœ‰æ•ˆå¤„ç†å¼€æ”¾åŸŸè§’è‰²ç‰¹å¾ï¼Œå¹¶ä¸ç°ä»£æ‰©æ•£å˜æ¢å™¨çš„æ½œåœ¨ç©ºé—´æ— ç¼äº¤äº’ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«åƒä¸‡çº§æ ·æœ¬çš„å¤§è§„æ¨¡è§’è‰²æ•°æ®é›†ï¼Œä»¥æ”¯æŒæ¡†æ¶çš„æœ‰æ•ˆè®­ç»ƒï¼Œä¼˜åŒ–èº«ä»½ä¸€è‡´æ€§å’Œæ–‡æœ¬å¯ç¼–è¾‘æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11651', 'title': '70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU\n  Inference via Dynamic-Length Float', 'url': 'https://huggingface.co/papers/2504.11651', 'abstract': 'Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at https://github.com/LeanModels/DFloat11.', 'score': 12, 'issue_id': 3321, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'bbc8b58a0d8e7d08', 'authors': ['Tianyi Zhang', 'Yang Sui', 'Shaochen Zhong', 'Vipin Chaudhary', 'Xia Hu', 'Anshumali Shrivastava'], 'affiliations': ['Department of Computer Science, Rice University', 'Department of Computer and Data Sciences, Case Western Reserve University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11651.jpg', 'data': {'categories': ['#small_models', '#optimization', '#long_context', '#inference'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'DFloat11: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dynamic-Length Float (DFloat11) - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. DFloat11 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ´Ğ¾Ğ² Ğ²ĞµÑĞ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ LLM Ğ½Ğ° 30% Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ GPU Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞµ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Llama-3.1 Ğ¸ Gemma-3, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU.'}, 'en': {'title': 'Efficient Compression for Large Language Models with DFloat11', 'desc': 'This paper presents DFloat11, a novel lossless compression framework designed to reduce the size of Large Language Models (LLMs) by 30% while maintaining exact output fidelity. The framework leverages the low entropy in the BFloat16 weight representation, applying entropy coding to assign dynamic-length encodings to model weights based on their frequency. DFloat11 includes a custom GPU kernel for efficient online decompression, optimizing memory usage and minimizing latency during inference. Experimental results demonstrate that DFloat11 significantly enhances throughput and allows for longer context lengths in LLMs, making it a powerful solution for deploying large models on resource-constrained hardware.'}, 'zh': {'title': 'DFloat11ï¼šé«˜æ•ˆå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„æ— æŸæ¡†æ¶', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§„æ¨¡ä¸Šè¿…é€Ÿå¢é•¿ï¼Œè¿™ç»™èµ„æºæœ‰é™çš„ç¡¬ä»¶éƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— æŸå‹ç¼©æ¡†æ¶DFloat11ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè¾“å‡ºä¸åŸå§‹æ¨¡å‹é€ä½ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œå°†LLMçš„å¤§å°å‡å°‘30%ã€‚DFloat11é€šè¿‡ç†µç¼–ç ï¼Œæ ¹æ®æƒé‡çš„é¢‘ç‡ä¸ºå…¶åˆ†é…åŠ¨æ€é•¿åº¦ç¼–ç ï¼Œå®ç°æ¥è¿‘ä¿¡æ¯æœ€ä¼˜çš„å‹ç¼©è€Œä¸æŸå¤±ç²¾åº¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDFloat11åœ¨å¤šä¸ªæ¨¡å‹ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦å’Œä¸Šä¸‹æ–‡é•¿åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13145', 'title': 'Exploring Expert Failures Improves LLM Agent Tuning', 'url': 'https://huggingface.co/papers/2504.13145', 'abstract': 'Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.', 'score': 11, 'issue_id': 3305, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '597cd9806c8a07ff', 'authors': ['Li-Cheng Lan', 'Andrew Bai', 'Minhao Cheng', 'Ruochen Wang', 'Cho-Jui Hsieh', 'Tianyi Zhou'], 'affiliations': ['OpenAI', 'Pennsylvania State University', 'UCLA', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.13145.jpg', 'data': {'categories': ['#reasoning', '#agents', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Exploring Expert Failures (EEF) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. EEF Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Rejection Sampling Fine-Tuning (RFT), Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… WebShop Ğ¸ SciWorld. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼.'}, 'en': {'title': 'Learning from Mistakes: Enhancing LLMs with Expert Failures', 'desc': 'This paper introduces Exploring Expert Failures (EEF), a novel approach to enhance the performance of Large Language Models (LLMs) in complex tasks. EEF builds on Rejection Sampling Fine-Tuning (RFT) by utilizing insights from previously failed expert trajectories to identify beneficial actions that can improve agent exploration and skill acquisition. By integrating these valuable actions into the training dataset while excluding harmful ones, EEF addresses previously unsolvable subtasks and boosts overall agent performance. The results demonstrate that EEF outperforms existing methods, achieving a 62% win rate in WebShop and setting new benchmarks in task performance.'}, 'zh': {'title': 'ä»å¤±è´¥ä¸­å­¦ä¹ ï¼Œæå‡æ™ºèƒ½ä½“èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®æ¨ç†å’Œäº¤äº’ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆRFTï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡æ¨¡ä»¿ä¸“å®¶ç”Ÿæˆçš„æˆåŠŸè½¨è¿¹å¹¶åœ¨è‡ªç”Ÿæˆçš„æˆåŠŸè½¨è¿¹ä¸Šè¿›è¡Œè¿­ä»£å¾®è°ƒæ¥æå‡æ¨¡å‹çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºRFTåå‘äºç®€å•åœºæ™¯ï¼Œè®¸å¤šå¤æ‚å­ä»»åŠ¡ä»ç„¶æœªèƒ½è§£å†³ã€‚æˆ‘ä»¬æå‡ºçš„æ¢ç´¢ä¸“å®¶å¤±è´¥ï¼ˆEEFï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä»å¤±è´¥çš„ä¸“å®¶è½¨è¿¹ä¸­æå–æœ‰ç›Šçš„è¡ŒåŠ¨ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¢ç´¢æ•ˆç‡å’Œå…³é”®æŠ€èƒ½çš„è·å–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07959', 'title': 'CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera\n  Color Constancy', 'url': 'https://huggingface.co/papers/2504.07959', 'abstract': "Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space. The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs.", 'score': 9, 'issue_id': 3311, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'f4c62f5c36856c86', 'authors': ['Dongyoung Kim', 'Mahmoud Afifi', 'Dongyun Kim', 'Michael S. Brown', 'Seon Joo Kim'], 'affiliations': ['AI Center - Toronto, Samsung Electronics', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07959.jpg', 'data': {'categories': ['#training', '#cv', '#dataset'], 'emoji': 'ğŸ“¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾ Ğ² ĞºĞ°Ğ¼ĞµÑ€Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ°Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ° (CCM) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ-Ğ¾Ñ‚Ğ¿ĞµÑ‡Ğ°Ñ‚ĞºĞ° ĞºĞ°Ğ¼ĞµÑ€Ñ‹ (CFE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ñ… CCM Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ² Ğ¼ĞµĞ¶ĞºĞ°Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼.'}, 'en': {'title': 'Adaptive Color Constancy Across Cameras Without Retraining', 'desc': 'This paper presents a novel approach to computational color constancy, specifically focusing on white balancing in images captured by different cameras. The proposed method utilizes pre-calibrated color correction matrices (CCMs) to adaptively transform illumination colors into the raw color space of new cameras without the need for retraining. By creating a compact camera fingerprint embedding (CFE), the model can effectively generalize to unseen cameras, enhancing its versatility. Additionally, a data augmentation technique is introduced to mitigate overfitting, ensuring robust performance across various datasets and camera types.'}, 'zh': {'title': 'è·¨ç›¸æœºé¢œè‰²æ’å¸¸æ€§çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå­¦ä¹ çš„è·¨ç›¸æœºé¢œè‰²æ’å¸¸æ€§æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸åŒç›¸æœºçš„ç™½å¹³è¡¡é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„å…ˆæ ¡å‡†çš„é¢œè‰²æ ¡æ­£çŸ©é˜µï¼ˆCCMï¼‰ï¼Œå°†ç›¸æœºçš„åŸå§‹é¢œè‰²ç©ºé—´æ˜ å°„åˆ°æ ‡å‡†ç©ºé—´ã€‚é€šè¿‡å°†é¢„å®šä¹‰çš„ç…§æ˜é¢œè‰²è½¬æ¢ä¸ºæµ‹è¯•ç›¸æœºçš„åŸå§‹ç©ºé—´ï¼Œç”Ÿæˆç´§å‡‘çš„ç›¸æœºæŒ‡çº¹åµŒå…¥ï¼ˆCFEï¼‰ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿé€‚åº”æœªè§è¿‡çš„ç›¸æœºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è·¨ç›¸æœºé¢œè‰²æ’å¸¸æ€§ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ï¼Œä¾èµ–äºç›¸æœºISPä¸­ç°æˆçš„æ•°æ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13143', 'title': 'Complex-Edit: CoT-Like Instruction Generation for\n  Complexity-Controllable Image Editing Benchmark', 'url': 'https://huggingface.co/papers/2504.13143', 'abstract': "We introduce Complex-Edit, a comprehensive benchmark designed to systematically evaluate instruction-based image editing models across instructions of varying complexity. To develop this benchmark, we harness GPT-4o to automatically collect a diverse set of editing instructions at scale. Our approach follows a well-structured ``Chain-of-Edit'' pipeline: we first generate individual atomic editing tasks independently and then integrate them to form cohesive, complex instructions. Additionally, we introduce a suite of metrics to assess various aspects of editing performance, along with a VLM-based auto-evaluation pipeline that supports large-scale assessments. Our benchmark yields several notable insights: 1) Open-source models significantly underperform relative to proprietary, closed-source models, with the performance gap widening as instruction complexity increases; 2) Increased instructional complexity primarily impairs the models' ability to retain key elements from the input images and to preserve the overall aesthetic quality; 3) Decomposing a complex instruction into a sequence of atomic steps, executed in a step-by-step manner, substantially degrades performance across multiple metrics; 4) A straightforward Best-of-N selection strategy improves results for both direct editing and the step-by-step sequential approach; and 5) We observe a ``curse of synthetic data'': when synthetic data is involved in model training, the edited images from such models tend to appear increasingly synthetic as the complexity of the editing instructions rises -- a phenomenon that intriguingly also manifests in the latest GPT-4o outputs.", 'score': 7, 'issue_id': 3324, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '5cf48cefe0cb6c8d', 'authors': ['Siwei Yang', 'Mude Hui', 'Bingchen Zhao', 'Yuyin Zhou', 'Nataniel Ruiz', 'Cihang Xie'], 'affiliations': ['Google', 'University of California, Santa Cruz', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2504.13143.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#open_source', '#data', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Complex-Edit: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Complex-Edit - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ GPT-4 Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€ÑĞ´ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Evaluating Image Editing Models: Complexity Matters!', 'desc': "The paper presents Complex-Edit, a benchmark for evaluating instruction-based image editing models based on the complexity of editing tasks. It utilizes GPT-4o to generate a wide range of editing instructions, which are then organized into a structured 'Chain-of-Edit' pipeline. The study reveals that open-source models lag behind proprietary ones, especially as task complexity increases, affecting their ability to maintain image quality. Additionally, it highlights the negative impact of breaking down complex instructions into simpler steps and introduces a Best-of-N selection strategy to enhance editing performance."}, 'zh': {'title': 'å¤æ‚æŒ‡ä»¤ä¸‹çš„å›¾åƒç¼–è¾‘æ€§èƒ½è¯„ä¼°', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Complex-Editï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œæ¶µç›–ä¸åŒå¤æ‚åº¦çš„æŒ‡ä»¤ã€‚æˆ‘ä»¬åˆ©ç”¨GPT-4oè‡ªåŠ¨æ”¶é›†å¤šæ ·åŒ–çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œå¹¶é‡‡ç”¨ç»“æ„åŒ–çš„â€œç¼–è¾‘é“¾â€æµç¨‹ç”Ÿæˆç‹¬ç«‹çš„åŸå­ç¼–è¾‘ä»»åŠ¡ï¼Œå†å°†å…¶æ•´åˆä¸ºå¤æ‚æŒ‡ä»¤ã€‚ç ”ç©¶å‘ç°ï¼Œå¼€æºæ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä½äºé—­æºæ¨¡å‹ï¼Œä¸”éšç€æŒ‡ä»¤å¤æ‚åº¦çš„å¢åŠ ï¼Œæ€§èƒ½å·®è·åŠ å¤§ã€‚æ­¤å¤–ï¼Œå¤æ‚æŒ‡ä»¤çš„åˆ†è§£æ‰§è¡Œä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„è¡¨ç°ï¼Œè€Œç®€å•çš„é€‰æ‹©ç­–ç•¥åˆ™èƒ½æ”¹å–„ç¼–è¾‘æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12157', 'title': 'FocusedAD: Character-centric Movie Audio Description', 'url': 'https://huggingface.co/papers/2504.12157', 'abstract': 'Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .', 'score': 7, 'issue_id': 3306, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': '2bfe38936e42dd11', 'authors': ['Xiaojun Ye', 'Chun Wang', 'Yiren Song', 'Sheng Zhou', 'Liangcheng Li', 'Jiajun Bu'], 'affiliations': ['National University of Singapore Singapore', 'Zhejiang University China'], 'pdf_title_img': 'assets/pdf/title_img/2504.12157.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#audio', '#benchmark', '#video', '#dataset', '#science'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ñ Ñ„Ğ¾ĞºÑƒÑĞ¾Ğ¼ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FocusedAD - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ², Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ (CPM) Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ (DPM) Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ (FCM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ Ğ¸Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. FocusedAD Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ½ĞºĞ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot.'}, 'en': {'title': 'FocusedAD: Enhancing Movie Accessibility with Character-Centric Audio Descriptions', 'desc': 'This paper presents FocusedAD, a new framework designed to create audio descriptions for movies, specifically targeting blind and visually impaired audiences. It addresses the unique challenges of audio description by focusing on character-centric narration that includes character names and relevant plot details. The framework consists of three main components: a Character Perception Module for tracking characters, a Dynamic Prior Module for incorporating contextual information, and a Focused Caption Module for generating detailed narrations. FocusedAD demonstrates superior performance on various benchmarks, including zero-shot evaluations, showcasing its effectiveness in enhancing movie accessibility.'}, 'zh': {'title': 'ä»¥è§’è‰²ä¸ºä¸­å¿ƒçš„ç”µå½±éŸ³é¢‘æè¿°', 'desc': 'ç”µå½±éŸ³é¢‘æè¿°ï¼ˆADï¼‰æ—¨åœ¨ä¸ºç›²äººå’Œè§†åŠ›éšœç¢è€…åœ¨æ— å¯¹è¯çš„ç‰‡æ®µä¸­å™è¿°è§†è§‰å†…å®¹ã€‚ä¸ä¸€èˆ¬è§†é¢‘å­—å¹•ç›¸æ¯”ï¼ŒADéœ€è¦ä¸æƒ…èŠ‚ç›¸å…³çš„å™è¿°ï¼Œå¹¶æ˜ç¡®æåŠè§’è‰²åç§°ï¼Œè¿™ç»™ç”µå½±ç†è§£å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†FocusedADæ¡†æ¶ï¼Œé€šè¿‡è§’è‰²æ„ŸçŸ¥æ¨¡å—ã€åŠ¨æ€å…ˆéªŒæ¨¡å—å’Œèšç„¦å­—å¹•æ¨¡å—ï¼Œæä¾›ä»¥è§’è‰²ä¸ºä¸­å¿ƒçš„ç”µå½±éŸ³é¢‘æè¿°ã€‚FocusedADåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨MAD-eval-Namedå’Œæ–°æå‡ºçš„Cinepile-ADæ•°æ®é›†ä¸Šçš„å¼ºå¤§é›¶æ ·æœ¬ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13079', 'title': 'Retrieval-Augmented Generation with Conflicting Evidence', 'url': 'https://huggingface.co/papers/2504.13079', 'abstract': 'Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.', 'score': 6, 'issue_id': 3304, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '13305db862567e7f', 'authors': ['Han Wang', 'Archiki Prasad', 'Elias Stengel-Eskin', 'Mohit Bansal'], 'affiliations': ['University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2504.13079.jpg', 'data': {'categories': ['#interpretability', '#dataset', '#rag', '#optimization', '#agents', '#hallucinations'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ RAMDocs, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ MADAM-RAG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². MADAM-RAG Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 11.40% Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ AmbigDocs Ğ¸ Ğ½Ğ° 15.80% Ğ½Ğ° FaithEval Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RAG. ĞĞ´Ğ½Ğ°ĞºĞ¾, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ, Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing LLM Accuracy through Debate and Retrieval', 'desc': 'This paper discusses the challenges faced by large language model (LLM) agents when using retrieval-augmented generation (RAG) to provide accurate responses to user queries. It introduces RAMDocs, a new dataset designed to simulate complex scenarios involving ambiguity, misinformation, and noise in documents. The authors propose MADAM-RAG, a multi-agent system where LLMs debate answers over multiple rounds, allowing for better aggregation of responses while filtering out inaccuracies. The results show that MADAM-RAG significantly improves performance on tasks requiring disambiguation and misinformation suppression compared to traditional RAG methods, although challenges remain in handling conflicting evidence.'}, 'zh': {'title': 'å¤šä»£ç†è¾©è®ºï¼šæå‡è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥æé«˜å›ç­”çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåœ¨å¤„ç†æ¨¡ç³Šç”¨æˆ·æŸ¥è¯¢å’Œæ¥è‡ªå¤šä¸ªæ¥æºçš„æ½œåœ¨å†²çªä¿¡æ¯æ—¶ï¼Œå¸¸å¸¸éœ€è¦æŠ‘åˆ¶æ¥è‡ªå˜ˆæ‚æˆ–æ— å…³æ–‡æ¡£çš„ä¸å‡†ç¡®ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†RAMDocsæ•°æ®é›†ï¼Œæ¨¡æ‹Ÿäº†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢åœºæ™¯ï¼Œå¹¶æå‡ºäº†MADAM-RAGå¤šä»£ç†æ–¹æ³•ï¼Œé€šè¿‡å¤šè½®è¾©è®ºæ¥å¤„ç†ç­”æ¡ˆçš„ä¼˜åŠ£ï¼Œä»è€Œæœ‰æ•ˆæ•´åˆä¸åŒæ¥æºçš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMADAM-RAGåœ¨å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢å’ŒæŠ‘åˆ¶é”™è¯¯ä¿¡æ¯æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12782', 'title': 'Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts', 'url': 'https://huggingface.co/papers/2504.12782', 'abstract': 'Ensuring the ethical deployment of text-to-image models requires effective techniques to prevent the generation of harmful or inappropriate content. While concept erasure methods offer a promising solution, existing finetuning-based approaches suffer from notable limitations. Anchor-free methods risk disrupting sampling trajectories, leading to visual artifacts, while anchor-based methods rely on the heuristic selection of anchor concepts. To overcome these shortcomings, we introduce a finetuning framework, dubbed ANT, which Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is built on a key insight: reversing the condition direction of classifier-free guidance during mid-to-late denoising stages enables precise content modification without sacrificing early-stage structural integrity. This inspires a trajectory-aware objective that preserves the integrity of the early-stage score function field, which steers samples toward the natural image manifold, without relying on heuristic anchor concept selection. For single-concept erasure, we propose an augmentation-enhanced weight saliency map to precisely identify the critical parameters that most significantly contribute to the unwanted concept, enabling more thorough and efficient erasure. For multi-concept erasure, our objective function offers a versatile plug-and-play solution that significantly boosts performance. Extensive experiments demonstrate that ANT achieves state-of-the-art results in both single and multi-concept erasure, delivering high-quality, safe outputs without compromising the generative fidelity. Code is available at https://github.com/lileyang1210/ANT', 'score': 4, 'issue_id': 3305, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '6c74497f6fd8b96b', 'authors': ['Leyang Li', 'Shilin Lu', 'Yan Ren', 'Adams Wai-Kin Kong'], 'affiliations': ['Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.12782.jpg', 'data': {'categories': ['#cv', '#data', '#training', '#optimization', '#ethics'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ANT: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ANT Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ANT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞºĞ¾Ñ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. ANT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'ANT: Ethical Image Generation Through Smart Concept Erasure', 'desc': 'This paper presents ANT, a new finetuning framework designed to improve the ethical deployment of text-to-image models by effectively erasing harmful or inappropriate content. ANT addresses the limitations of existing methods by guiding denoising trajectories to avoid unwanted concepts without disrupting the image quality. It introduces a trajectory-aware objective that maintains the integrity of early-stage image features while allowing for precise content modification. The framework also includes an innovative weight saliency map for identifying critical parameters in single-concept erasure and offers a versatile solution for multi-concept erasure, achieving state-of-the-art results in generating safe and high-quality images.'}, 'zh': {'title': 'ANTï¼šé«˜æ•ˆå»é™¤ä¸å½“å†…å®¹çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºANTçš„å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºç¡®ä¿æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ä¼¦ç†éƒ¨ç½²ï¼Œç‰¹åˆ«æ˜¯é˜²æ­¢ç”Ÿæˆæœ‰å®³æˆ–ä¸å½“å†…å®¹ã€‚ANTé€šè¿‡è‡ªåŠ¨å¼•å¯¼å»å™ªè½¨è¿¹ï¼Œé¿å…äº†ç°æœ‰æ–¹æ³•ä¸­çš„ä¸€äº›å±€é™æ€§ï¼Œå¦‚é”šç‚¹æ–¹æ³•çš„å¯å‘å¼é€‰æ‹©å’Œæ— é”šç‚¹æ–¹æ³•å¯¼è‡´çš„è§†è§‰ä¼ªå½±ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº†åœ¨å»å™ªä¸­åæœŸåè½¬åˆ†ç±»å™¨æ— æŒ‡å¯¼çš„æ¡ä»¶æ–¹å‘çš„å…³é”®è§è§£ï¼Œä»è€Œå®ç°äº†ç²¾ç¡®çš„å†…å®¹ä¿®æ”¹ï¼ŒåŒæ—¶ä¿æŒäº†æ—©æœŸé˜¶æ®µçš„ç»“æ„å®Œæ•´æ€§ã€‚é€šè¿‡å¢å¼ºçš„æƒé‡æ˜¾è‘—æ€§å›¾ï¼ŒANTèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¹¶å»é™¤å•ä¸€æˆ–å¤šä¸ªä¸å½“æ¦‚å¿µï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å»é™¤æ•ˆæœå’Œç”Ÿæˆè´¨é‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12563', 'title': 'MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic\n  Data Generation', 'url': 'https://huggingface.co/papers/2504.12563', 'abstract': 'Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is low diversity, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple "expert" LLM agents to collaboratively generate data. Using only 25 million tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and Biomedicine-without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora.   Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.', 'score': 4, 'issue_id': 3316, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '0272e3b4f0ac9209', 'authors': ['Haris Riaz', 'Sourav Bhabesh', 'Vinayak Arannil', 'Miguel Ballesteros', 'Graham Horwood'], 'affiliations': ['AWS AI Labs', 'University of Arizona'], 'pdf_title_img': 'assets/pdf/title_img/2504.12563.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#multimodal', '#training', '#dataset', '#synthetic'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'MetaSynth: Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "MetaSynth - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³, Ğ³Ğ´Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… 'ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ…' LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 25 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ MetaSynth, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Mistral-7B-v0.3 Ğº Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."}, 'en': {'title': 'Enhancing Domain Adaptation with Diverse Synthetic Data', 'desc': "This paper introduces MetaSynth, a novel method for generating synthetic data that improves diversity by using multiple expert language models to collaboratively create data. The authors demonstrate that with just 25 million tokens of synthetic data, they can effectively adapt a well-trained language model, Mistral-7B-v0.3, to specialized domains like Finance and Biomedicine. The results show that this approach not only maintains the model's general capabilities but also significantly enhances its performance in the targeted domains. Additionally, the diversity of the synthetic data generated by MetaSynth is evaluated and found to be comparable to that of traditional LLM pre-training datasets, indicating its potential for broader applications in domain adaptation."}, 'zh': {'title': 'MetaSynthï¼šæå‡åˆæˆæ•°æ®å¤šæ ·æ€§ï¼ŒåŠ©åŠ›é¢†åŸŸé€‚åº”', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMetaSynthçš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆæ•°æ®ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€‚åº”æ€§ã€‚é€šè¿‡å…ƒæç¤ºæŠ€æœ¯ï¼ŒMetaSynthåè°ƒå¤šä¸ªâ€œä¸“å®¶â€LLMä»£ç†å…±åŒç”Ÿæˆæ•°æ®ï¼Œä»è€Œå¢å¼ºåˆæˆæ•°æ®çš„å¤šæ ·æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…ä½¿ç”¨2500ä¸‡ä¸ªåˆæˆæ•°æ®ä»¤ç‰Œï¼ŒMetaSynthæˆåŠŸå°†Mistral-7B-v0.3æ¨¡å‹é€‚åº”äºé‡‘èå’Œç”Ÿç‰©åŒ»å­¦ä¸¤ä¸ªä¸“ä¸šé¢†åŸŸï¼ŒåŒæ—¶ä¿æŒå…¶åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ä¸ä½¿ç”¨æ¨¡æ¿æç¤ºç”Ÿæˆçš„æ•°æ®ç›¸æ¯”ï¼ŒMetaSynthç”Ÿæˆçš„æ•°æ®åœ¨å¤šæ ·æ€§å’Œæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜è¶Šï¼Œè¯æ˜äº†å…¶åœ¨é¢†åŸŸé€‚åº”ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09228', 'title': 'Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking', 'url': 'https://huggingface.co/papers/2504.09228', 'abstract': "Single-stream architectures using Vision Transformer (ViT) backbones show great potential for real-time UAV tracking recently. However, frequent occlusions from obstacles like buildings and trees expose a major drawback: these models often lack strategies to handle occlusions effectively. New methods are needed to enhance the occlusion resilience of single-stream ViT models in aerial tracking. In this work, we propose to learn Occlusion-Robust Representations (ORR) based on ViTs for UAV tracking by enforcing an invariance of the feature representation of a target with respect to random masking operations modeled by a spatial Cox process. Hopefully, this random masking approximately simulates target occlusions, thereby enabling us to learn ViTs that are robust to target occlusion for UAV tracking. This framework is termed ORTrack. Additionally, to facilitate real-time applications, we propose an Adaptive Feature-Based Knowledge Distillation (AFKD) method to create a more compact tracker, which adaptively mimics the behavior of the teacher model ORTrack according to the task's difficulty. This student model, dubbed ORTrack-D, retains much of ORTrack's performance while offering higher efficiency. Extensive experiments on multiple benchmarks validate the effectiveness of our method, demonstrating its state-of-the-art performance. Codes is available at https://github.com/wuyou3474/ORTrack.", 'score': 4, 'issue_id': 3325, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 12', 'zh': '4æœˆ12æ—¥'}, 'hash': 'f8abea0d3480728f', 'authors': ['You Wu', 'Xucheng Wang', 'Xiangyang Yang', 'Mengyuan Liu', 'Dan Zeng', 'Hengzhou Ye', 'Shuiwang Li'], 'affiliations': ['College of Computer Science and Engineering, Guilin University of Technology, China', 'School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, China', 'School of Computer Science, Fudan University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.09228.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#cv'], 'emoji': 'ğŸš', 'ru': {'title': 'Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ‘ĞŸĞ›Ğ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ViT Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ‘ĞŸĞ›Ğ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Vision Transformer (ViT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼ Ğº Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ (ORR) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ ĞšĞ¾ĞºÑĞ°. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (AFKD) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ñ€ĞµĞºĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ORTrack Ğ¸ ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ ORTrack-D Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ‘ĞŸĞ›Ğ.'}, 'en': {'title': 'Enhancing UAV Tracking Resilience with Occlusion-Robust ViTs', 'desc': 'This paper addresses the challenge of occlusions in UAV tracking using single-stream Vision Transformer (ViT) architectures. The authors introduce a method called Occlusion-Robust Representations (ORR) that enhances the resilience of ViTs to occlusions by simulating them through random masking operations. They also propose an Adaptive Feature-Based Knowledge Distillation (AFKD) technique to create a more efficient student model, ORTrack-D, which maintains high performance while being compact. Extensive experiments demonstrate that their approach achieves state-of-the-art results in real-time UAV tracking tasks.'}, 'zh': {'title': 'æå‡æ— äººæœºè·Ÿè¸ªçš„é®æŒ¡é²æ£’æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰çš„å•æµæ¶æ„ï¼Œç”¨äºæ— äººæœºï¼ˆUAVï¼‰è·Ÿè¸ªï¼Œæ—¨åœ¨æé«˜å¯¹é®æŒ¡çš„é²æ£’æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†é®æŒ¡é²æ£’è¡¨ç¤ºï¼ˆORRï¼‰ï¼Œé€šè¿‡éšæœºé®æŒ¡æ“ä½œæ¥æ¨¡æ‹Ÿç›®æ ‡é®æŒ¡ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚ä¸ºäº†å®ç°å®æ—¶åº”ç”¨ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç‰¹å¾çŸ¥è¯†è’¸é¦ï¼ˆAFKDï¼‰æ–¹æ³•ï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ›´ç´§å‡‘çš„è·Ÿè¸ªå™¨ï¼Œç§°ä¸ºORTrack-Dã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24379', 'title': 'Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.24379', 'abstract': 'To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/', 'score': 43, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'dce65db5da1b8c34', 'authors': ['Shengqiong Wu', 'Weicai Ye', 'Jiahao Wang', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Shuicheng Yan', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['Kuaishou Technology', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24379.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ»ÑĞ±Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Any2Caption - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Any2CapIns Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼.'}, 'en': {'title': 'Revolutionizing Video Generation with Any2Caption', 'desc': "Any2Caption is a new framework designed to improve how user intentions are understood in video generation. It separates the process of interpreting different conditions from the actual video creation, allowing for more precise control. By using advanced multimodal large language models, it can transform various inputs like text and images into detailed captions that guide video generators more effectively. The introduction of the Any2CapIns dataset, which contains a large number of examples, further enhances the system's ability to generate high-quality videos based on diverse conditions."}, 'zh': {'title': 'å¯æ§è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šAny2Caption', 'desc': 'ä¸ºäº†å…‹æœå½“å‰è§†é¢‘ç”Ÿæˆé¢†åŸŸä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†Any2Captionï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¯æ§è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£é‡Šæ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤è§£è€¦ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘åŠç‰¹å®šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œç›¸æœºå§¿æ€ï¼‰è½¬åŒ–ä¸ºå¯†é›†çš„ç»“æ„åŒ–å­—å¹•ï¼Œä»è€Œä¸ºè§†é¢‘ç”Ÿæˆå™¨æä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†Any2CapInsï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«337Kå®ä¾‹å’Œ407Kæ¡ä»¶çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºä»»ä½•æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00050', 'title': 'JudgeLRM: Large Reasoning Models as a Judge', 'url': 'https://huggingface.co/papers/2504.00050', 'abstract': 'The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.', 'score': 29, 'issue_id': 3022, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '5060d3d364f635eb', 'authors': ['Nuo Chen', 'Zhiyuan Hu', 'Qingyun Zou', 'Jiaying Wu', 'Qian Wang', 'Bryan Hooi', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.00050.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#rl'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜-ÑÑƒĞ´ĞµĞ¹: Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ JudgeLRM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑÑƒĞ´ĞµĞ¹. JudgeLRM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ JudgeLRM-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 2.79% Ğ¿Ğ¾ F1-Ğ¼ĞµÑ€Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ DeepSeek-R1.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Complex Judging Tasks', 'desc': 'This paper explores the effectiveness of Large Language Models (LLMs) as judges in evaluation tasks, particularly in scenarios that require complex reasoning. It highlights that traditional Supervised Fine-Tuning (SFT) methods do not perform well when faced with tasks that demand higher reasoning skills. The authors introduce JudgeLRM, a new approach that utilizes reinforcement learning (RL) to enhance the reasoning capabilities of LLMs by providing judge-specific rewards. The results show that JudgeLRM models significantly outperform both SFT-tuned models and other leading reasoning models, demonstrating superior performance in tasks that require deep reasoning.'}, 'zh': {'title': 'JudgeLRMï¼šæ·±åº¦æ¨ç†çš„è¯„åˆ¤è€…', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè¯„ä¼°è€…çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨å¤„ç†éœ€è¦æ·±åº¦æ¨ç†çš„æ ·æœ¬æ—¶æ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†JudgeLRMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„è¯„åˆ¤å¯¼å‘LLMï¼Œèƒ½å¤Ÿæä¾›æ›´æœ‰æ•ˆçš„è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJudgeLRMæ¨¡å‹åœ¨è¯„åˆ¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„SFTæ¨¡å‹å’Œæœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼Œå°¤å…¶åœ¨éœ€è¦æ·±åº¦æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23145', 'title': 'CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis', 'url': 'https://huggingface.co/papers/2503.23145', 'abstract': 'Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.', 'score': 28, 'issue_id': 3021, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '945cc4b51522e668', 'authors': ['Anjiang Wei', 'Tarun Suresh', 'Jiannan Cao', 'Naveen Kannan', 'Yuheng Wu', 'Kai Yan', 'Thiago S. F. X. Teixeira', 'Ke Wang', 'Alex Aiken'], 'affiliations': ['Intel', 'MIT', 'Stanford University', 'University of Illinois Urbana-Champaign', 'Visa Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23145.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#plp', '#agents', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CodeARC: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CodeARC - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², CodeARC Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹, Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 1114 Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 52.7% ÑƒÑĞ¿ĞµÑ…Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'CodeARC: A New Frontier in Inductive Program Synthesis Evaluation', 'desc': 'This paper introduces CodeARC, a new evaluation framework for inductive program synthesis, which is the process of creating functions based on input-output examples. Unlike traditional methods that use static examples, CodeARC allows agents to interact with a hidden target function, enabling them to refine their solutions through feedback. The framework includes a large-scale benchmark with 1114 functions, demonstrating the challenges faced by models in this domain. The results show that fine-tuning models like LLaMA-3.1-8B-Instruct can significantly improve performance, emphasizing the need for dynamic evaluation in program synthesis tasks.'}, 'zh': {'title': 'CodeARCï¼šæå‡ç¨‹åºåˆæˆçš„è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'å½’çº³ç¨‹åºåˆæˆï¼Œä¹Ÿç§°ä¸ºç¤ºä¾‹ç¼–ç¨‹ï¼Œæ˜¯ä»è¾“å…¥è¾“å‡ºç¤ºä¾‹ä¸­åˆæˆå‡½æ•°çš„è¿‡ç¨‹ï¼Œè¦æ±‚èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„è¾“å…¥ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼çš„ç¼–ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å½’çº³ç¨‹åºåˆæˆæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„è¯„ä¼°åè®®ä¾èµ–äºé™æ€ç¤ºä¾‹é›†å’Œä¿ç•™æµ‹è¯•ï¼Œæ— æ³•åœ¨åˆæˆå‡½æ•°é”™è¯¯æ—¶æä¾›åé¦ˆï¼Œä¹Ÿæœªèƒ½åæ˜ ç°å®ä¸–ç•Œä¸­çš„åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†CodeARCï¼Œä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå…è®¸ä»£ç†é€šè¿‡æŸ¥è¯¢éšè—çš„ç›®æ ‡å‡½æ•°ä¸ä¹‹äº’åŠ¨ï¼Œä»è€Œåˆæˆå€™é€‰å‡½æ•°å¹¶æ ¹æ®åé¦ˆè¿­ä»£æ”¹è¿›è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24376', 'title': 'Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1', 'url': 'https://huggingface.co/papers/2503.24376', 'abstract': "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.", 'score': 24, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'd22966d0969ded43', 'authors': ['Yi Chen', 'Yuying Ge', 'Rui Wang', 'Yixiao Ge', 'Lu Qiu', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.24376.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#rl', '#benchmark', '#optimization', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SEED-Bench-R1 Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2-VL-Instruct-7B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ½Ğµ ĞµĞ³Ğ¾. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ½Ğ¾ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµĞ½ĞµĞµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Reasoning in Multimodal Models with SEED-Bench-R1', 'desc': "This paper discusses the advancements in Chain of Thought (COT) generation for Large Language Models (LLMs) and introduces a new benchmark called SEED-Bench-R1 for evaluating Multimodal Large Language Models (MLLMs) in video understanding tasks. The benchmark includes complex real-world videos and planning tasks presented as multiple-choice questions, assessing the models' perception and reasoning abilities. The study compares reinforcement learning (RL) with supervised fine-tuning (SFT) using the Qwen2-VL-Instruct-7B model, showing that RL is more data-efficient and performs better on various tasks. However, the analysis also highlights limitations in reasoning consistency and visual cue recognition, suggesting areas for future research to enhance model performance."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'æœ€è¿‘ï¼Œé“¾å¼æ€ç»´ï¼ˆCOTï¼‰ç”Ÿæˆçš„è¿›å±•æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆä¸ºä¸€ç§æœ‰æ•ˆçš„åè®­ç»ƒæ–¹æ³•ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»§æ‰¿äº†è¿™ç§æ¨ç†æ½œåŠ›ï¼Œä½†åœ¨éœ€è¦æ„ŸçŸ¥å’Œé€»è¾‘æ¨ç†çš„ä»»åŠ¡ä¸­ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SEED-Bench-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°MLLMsåœ¨è§†é¢‘ç†è§£ä¸­åè®­ç»ƒæ–¹æ³•çš„åŸºå‡†ï¼ŒåŒ…å«å¤æ‚çš„çœŸå®è§†é¢‘å’Œå¤šé¡¹é€‰æ‹©é¢˜çš„æ—¥å¸¸è§„åˆ’ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒRLåœ¨æ•°æ®æ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†åœ¨é€»è¾‘è¿è´¯æ€§æ–¹é¢å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00698', 'title': 'Command A: An Enterprise-Ready Large Language Model', 'url': 'https://huggingface.co/papers/2504.00698', 'abstract': 'In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.', 'score': 16, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '8670e6d1cc4f6bee', 'authors': ['Team Cohere', 'Aakanksha', 'Arash Ahmadian', 'Marwan Ahmed', 'Jay Alammar', 'Yazeed Alnumay', 'Sophia Althammer', 'Arkady Arkhangorodsky', 'Viraat Aryabumi', 'Dennis Aumiller', 'RaphaÃ«l Avalos', 'Zahara Aviv', 'Sammie Bae', 'Saurabh Baji', 'Alexandre Barbet', 'Max Bartolo', 'BjÃ¶rn Bebensee', 'Neeral Beladia', 'Walter Beller-Morales', 'Alexandre BÃ©rard', 'Andrew Berneshawi', 'Anna Bialas', 'Phil Blunsom', 'Matt Bobkin', 'Adi Bongale', 'Sam Braun', 'Maxime Brunet', 'Samuel Cahyawijaya', 'David Cairuz', 'Jon Ander Campos', 'Cassie Cao', 'Kris Cao', 'Roman CastagnÃ©', 'JuliÃ¡n Cendrero', 'Leila Chan Currie', 'Yash Chandak', 'Diane Chang', 'Giannis Chatziveroglou', 'Hongyu Chen', 'Claire Cheng', 'Alexis Chevalier', 'Justin T. Chiu', 'Eugene Cho', 'Eugene Choi', 'Eujeong Choi', 'Tim Chung', 'Volkan Cirik', 'Ana Cismaru', 'Pierre Clavier', 'Henry Conklin', 'Lucas Crawhall-Stein', 'Devon Crouse', 'Andres Felipe Cruz-Salinas', 'Ben Cyrus', "Daniel D'souza", 'Hugo Dalla-Torre', 'John Dang', 'William Darling', 'Omar Darwiche Domingues', 'Saurabh Dash', 'Antoine Debugne', 'ThÃ©o Dehaze', 'Shaan Desai', 'Joan Devassy', 'Rishit Dholakia', 'Kyle Duffy', 'Ali Edalati', 'Ace Eldeib', 'Abdullah Elkady', 'Sarah Elsharkawy', 'Irem ErgÃ¼n', 'Beyza Ermis', 'Marzieh Fadaee', 'Boyu Fan', 'Lucas Fayoux', 'Yannis Flet-Berliac', 'Nick Frosst', 'Matthias GallÃ©', 'Wojciech Galuba', 'Utsav Garg', 'Matthieu Geist', 'Mohammad Gheshlaghi Azar', 'Seraphina Goldfarb-Tarrant', 'Tomas Goldsack', 'Aidan Gomez', 'Victor Machado Gonzaga', 'Nithya Govindarajan', 'Manoj Govindassamy', 'Nathan Grinsztajn', 'Nikolas Gritsch', 'Patrick Gu', 'Shangmin Guo', 'Kilian Haefeli', 'Rod Hajjar', 'Tim Hawes', 'Jingyi He', 'Sebastian HofstÃ¤tter', 'Sungjin Hong', 'Sara Hooker', 'Tom Hosking', 'Stephanie Howe', 'Eric Hu', 'Renjie Huang', 'Hemant Jain', 'Ritika Jain', 'Nick Jakobi', 'Madeline Jenkins', 'JJ Jordan', 'Dhruti Joshi', 'Jason Jung', 'Trushant Kalyanpur', 'Siddhartha Rao Kamalakara', 'Julia Kedrzycki', 'Gokce Keskin', 'Edward Kim', 'Joon Kim', 'Wei-Yin Ko', 'Tom Kocmi', 'Michael Kozakov', 'Wojciech KryÅ›ciÅ„ski', 'Arnav Kumar Jain', 'Komal Kumar Teru', 'Sander Land', 'Michael Lasby', 'Olivia Lasche', 'Justin Lee', 'Patrick Lewis', 'Jeffrey Li', 'Jonathan Li', 'Hangyu Lin', 'Acyr Locatelli', 'Kevin Luong', 'Raymond Ma', 'Lukas Mach', 'Marina Machado', 'Joanne Magbitang', 'Brenda Malacara Lopez', 'Aryan Mann', 'Kelly Marchisio', 'Olivia Markham', 'Alexandre Matton', 'Alex McKinney', 'Dominic McLoughlin', 'Jozef Mokry', 'Adrien Morisot', 'Autumn Moulder', 'Harry Moynehan', 'Maximilian Mozes', 'Vivek Muppalla', 'Lidiya Murakhovska', 'Hemangani Nagarajan', 'Alekhya Nandula', 'Hisham Nasir', 'Shauna Nehra', 'Josh Netto-Rosen', 'Daniel Ohashi', 'James Owers-Bardsley', 'Jason Ozuzu', 'Dennis Padilla', 'Gloria Park', 'Sam Passaglia', 'Jeremy Pekmez', 'Laura Penstone', 'Aleksandra Piktus', 'Case Ploeg', 'Andrew Poulton', 'Youran Qi', 'Shubha Raghvendra', 'Miguel Ramos', 'Ekagra Ranjan', 'Pierre Richemond', 'CÃ©cile Robert-Michon', 'AurÃ©lien Rodriguez', 'Sudip Roy', 'Laura Ruis', 'Louise Rust', 'Anubhav Sachan', 'Alejandro Salamanca', 'Kailash Karthik Saravanakumar', 'Isha Satyakam', 'Alice Schoenauer Sebag', 'Priyanka Sen', 'Sholeh Sepehri', 'Preethi Seshadri', 'Ye Shen', 'Tom Sherborne', 'Sylvie Chang Shi', 'Sanal Shivaprasad', 'Vladyslav Shmyhlo', 'Anirudh Shrinivason', 'Inna Shteinbuk', 'Amir Shukayev', 'Mathieu Simard', 'Ella Snyder', 'Ava Spataru', 'Victoria Spooner', 'Trisha Starostina', 'Florian Strub', 'Yixuan Su', 'Jimin Sun', 'Dwarak Talupuru', 'Eugene Tarassov', 'Elena Tommasone', 'Jennifer Tracey', 'Billy Trend', 'Evren Tumer', 'Ahmet ÃœstÃ¼n', 'Bharat Venkitesh', 'David Venuto', 'Pat Verga', 'Maxime Voisin', 'Alex Wang', 'Donglu Wang', 'Shijian Wang', 'Edmond Wen', 'Naomi White', 'Jesse Willman', 'Marysia Winkels', 'Chen Xia', 'Jessica Xie', 'Minjie Xu', 'Bowen Yang', 'Tan Yi-Chern', 'Ivan Zhang', 'Zhenyu Zhao', 'Zhoujie Zhao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.00698.jpg', 'data': {'categories': ['#training', '#low_resource', '#agents', '#open_source', '#rag', '#multilingual', '#architecture', '#benchmark'], 'emoji': 'ğŸš€', 'ru': {'title': 'Command A: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Command A - Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 23 ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Command A Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Retrieval Augmented Generation (RAG) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². Ğ­Ñ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Empowering Enterprises with Command A: The Future of Language Models', 'desc': 'This paper presents Command A, a large language model designed specifically for enterprise applications. It features a hybrid architecture that optimizes both performance and efficiency, supporting 23 languages to cater to global business needs. The model excels in Retrieval Augmented Generation (RAG), enabling it to automate complex business processes through effective grounding and tool usage. The training process incorporates decentralized methods, including self-refinement and model merging, and the paper also discusses the similar Command R7B model, providing insights into their training and evaluation results.'}, 'zh': {'title': 'Command Aï¼šä¼ä¸šåº”ç”¨çš„å¼ºå¤§è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Command Açš„å¼€å‘ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºä¼ä¸šå®é™…åº”ç”¨è€Œè®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚Command Aå…·å¤‡å¤šè¯­è¨€èƒ½åŠ›ï¼Œæ”¯æŒ23ç§å…¨çƒå•†ä¸šè¯­è¨€ï¼Œå¹¶é‡‡ç”¨æ–°é¢–çš„æ··åˆæ¶æ„ï¼Œå…¼é¡¾æ•ˆç‡ä¸é«˜æ€§èƒ½ã€‚å®ƒæä¾›äº†æœ€ä½³çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡å·¥å…·ä½¿ç”¨å’ŒåŸºç¡€çŸ¥è¯†æ”¯æŒæ¥è‡ªåŠ¨åŒ–å¤æ‚çš„ä¸šåŠ¡æµç¨‹ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸Command Aç›¸ä¼¼çš„Command R7Bæ¨¡å‹çš„ç»“æœï¼Œå¹¶å‘å¸ƒäº†è¿™ä¸¤ä¸ªæ¨¡å‹çš„æƒé‡ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01016', 'title': 'GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors', 'url': 'https://huggingface.co/papers/2504.01016', 'abstract': 'Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.', 'score': 15, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '9430b45c3324fb61', 'authors': ['Tian-Xing Xu', 'Xiangjun Gao', 'Wenbo Hu', 'Xiaoyu Li', 'Song-Hai Zhang', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'HKUST', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01016.jpg', 'data': {'categories': ['#video', '#3d', '#architecture', '#diffusion', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'GeometryCrafter: Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'GeometryCrafter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ 3D/4D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GeometryCrafter Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 3D, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'GeometryCrafter: Elevating Video Depth Estimation with High-Fidelity Point Maps', 'desc': 'This paper introduces GeometryCrafter, a new framework designed to improve video depth estimation by producing high-fidelity point map sequences that maintain temporal coherence. It addresses the limitations of existing methods in achieving accurate geometric representations, which are crucial for tasks like 3D reconstruction and camera parameter estimation. The framework utilizes a point map Variational Autoencoder (VAE) to effectively encode and decode point maps, independent of the video latent distributions. By training a video diffusion model on these point map sequences, GeometryCrafter demonstrates superior performance in 3D accuracy and generalization across various datasets.'}, 'zh': {'title': 'GeometryCrafterï¼šé«˜ä¿çœŸè§†é¢‘æ·±åº¦ä¼°è®¡çš„æ–°æ¡†æ¶', 'desc': 'å°½ç®¡è§†é¢‘æ·±åº¦ä¼°è®¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å‡ ä½•ä¿çœŸåº¦æ–¹é¢å­˜åœ¨å›ºæœ‰å±€é™ï¼Œé™åˆ¶äº†å…¶åœ¨é‡å»ºå’Œå…¶ä»–åº¦é‡åŸºç¡€ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†GeometryCrafterï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å¼€æ”¾ä¸–ç•Œè§†é¢‘ä¸­æ¢å¤å…·æœ‰æ—¶é—´ä¸€è‡´æ€§çš„é«˜ä¿çœŸç‚¹å›¾åºåˆ—ï¼Œä»è€Œå®ç°å‡†ç¡®çš„3D/4Dé‡å»ºå’Œç›¸æœºå‚æ•°ä¼°è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¸å¿ƒæ˜¯ä¸€ä¸ªç‚¹å›¾å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå®ƒå­¦ä¹ ä¸€ä¸ªä¸è§†é¢‘æ½œåœ¨åˆ†å¸ƒæ— å…³çš„æ½œåœ¨ç©ºé—´ï¼Œä»¥æœ‰æ•ˆåœ°è¿›è¡Œç‚¹å›¾ç¼–ç å’Œè§£ç ã€‚é€šè¿‡åˆ©ç”¨VAEï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥å»ºæ¨¡åŸºäºè¾“å…¥è§†é¢‘çš„ç‚¹å›¾åºåˆ—çš„åˆ†å¸ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00810', 'title': 'Z1: Efficient Test-time Scaling with Code', 'url': 'https://huggingface.co/papers/2504.00810', 'abstract': 'Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.', 'score': 15, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'd982593a14ba7da9', 'authors': ['Zhaojian Yu', 'Yinghao Wu', 'Yilun Zhao', 'Arman Cohan', 'Xiao-Ping Zhang'], 'affiliations': ['Tsinghua University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00810.jpg', 'data': {'categories': ['#reasoning', '#rl', '#dataset', '#training', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Z1-Code-Reasoning-107K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Shifted Thinking Window Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Z1-7B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Efficient Reasoning in Large Language Models', 'desc': 'This paper introduces a new method for improving the efficiency of Large Language Models (LLMs) during problem-solving by reducing unnecessary reasoning tokens. The authors created a dataset called Z1-Code-Reasoning-107K, which includes various coding problems and their solution paths. They also developed a technique called the Shifted Thinking Window, which helps the model focus on relevant information and limits excessive reasoning. The resulting model, Z1-7B, shows strong performance on complex tasks while using significantly fewer reasoning tokens compared to other models.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œç®€åŒ–æ€è€ƒï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç›¸å…³çš„æ¨ç†è½¨è¿¹ä¸Šï¼Œå‡å°‘å¤šä½™çš„æ€è€ƒæ ‡è®°ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºZ1-Code-Reasoning-107Kçš„æ•°æ®é›†ï¼ŒåŒ…å«ç®€å•å’Œå¤æ‚çš„ç¼–ç é—®é¢˜åŠå…¶è§£å†³è½¨è¿¹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç§»ä½æ€ç»´çª—å£ï¼Œé€šè¿‡å»é™¤ä¸Šä¸‹æ–‡åˆ†éš”æ ‡ç­¾å’Œé™åˆ¶æ¨ç†æ ‡è®°ï¼Œæ¥å‡è½»è¿‡åº¦æ€è€ƒçš„è´Ÿæ‹…ã€‚ç»è¿‡è®­ç»ƒçš„æ¨¡å‹Z1-7Bèƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„å¤æ‚æ€§è°ƒæ•´æ¨ç†æ°´å¹³ï¼Œå¹¶åœ¨ä¸åŒçš„æ¨ç†ä»»åŠ¡ä¸­å®ç°é«˜æ•ˆçš„æµ‹è¯•æ—¶é—´æ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00595', 'title': 'Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources', 'url': 'https://huggingface.co/papers/2504.00595', 'abstract': 'The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.', 'score': 15, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '3e8c667bd93d754e', 'authors': ['Weizhi Wang', 'Yu Tian', 'Linjie Yang', 'Heng Wang', 'Xifeng Yan'], 'affiliations': ['Nvidia Research', 'Seed Vision Team, ByteDance', 'UC Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2504.00595.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#training', '#data', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Open-Qwen2VL - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 29 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 442 Ñ‡Ğ°ÑĞ° GPU A100-40G. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºÑƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Open-Qwen2VL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2-VL-2B Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Efficiency in Multimodal LLMs with Open-Qwen2VL', 'desc': 'The paper presents Open-Qwen2VL, a multimodal large language model (LLM) that is fully open-source and pre-trained on 29 million image-text pairs. It addresses challenges in multimodal LLM pre-training by utilizing advanced data filtering techniques and efficient training strategies, achieving significant improvements in training efficiency. The model is trained using a dynamic image resolution approach and multimodal sequence packing, which enhances the overall performance while reducing resource consumption. Open-Qwen2VL outperforms existing models on various benchmarks, showcasing its effectiveness and the benefits of open-source collaboration in machine learning.'}, 'zh': {'title': 'é«˜æ•ˆå¼€æºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Open-Qwen2VLï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰20äº¿å‚æ•°ï¼Œä½¿ç”¨2900ä¸‡å¯¹å›¾åƒ-æ–‡æœ¬æ•°æ®è¿›è¡Œé«˜æ•ˆé¢„è®­ç»ƒã€‚æˆ‘ä»¬é‡‡ç”¨äº†åŠ¨æ€å›¾åƒåˆ†è¾¨ç‡å’Œå¤šæ¨¡æ€åºåˆ—æ‰“åŒ…æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†é¢„è®­ç»ƒçš„æ•ˆç‡ã€‚é€šè¿‡ä½¿ç”¨MLLMå’ŒCLIPçš„è¿‡æ»¤æŠ€æœ¯ï¼Œæå‡äº†æ•°æ®è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚æœ€ç»ˆï¼ŒOpen-Qwen2VLåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†éƒ¨åˆ†å¼€æºçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„è®­ç»ƒæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01019', 'title': 'MixerMDM: Learnable Composition of Human Motion Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01019', 'abstract': 'Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.', 'score': 13, 'issue_id': 3022, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '745108d1df40d1b4', 'authors': ['Pablo Ruiz-Ponce', 'German Barquero', 'Cristina Palmero', 'Sergio Escalera', 'JosÃ© GarcÃ­a-RodrÃ­guez'], 'affiliations': ['Kings College London, UK', 'Universidad de Alicante, Spain', 'Universitat de Barcelona and Computer Vision Center, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2504.01019.jpg', 'data': {'categories': ['#multimodal', '#cv', '#diffusion', '#benchmark', '#dataset'], 'emoji': 'ğŸ•º', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MixerMDM - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², MixerMDM Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ MixerMDM Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ´ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Dynamic Control of Human Motion Generation with MixerMDM', 'desc': 'This paper addresses the challenge of generating human motion based on textual descriptions by introducing MixerMDM, a novel learnable model composition technique. Unlike previous methods, MixerMDM dynamically combines pre-trained motion diffusion models in an adversarial manner, allowing for better control over the generated motions based on specific conditions. The approach enables fine-grained control over individual and overall interactions in multi-person scenarios. Additionally, the authors propose a new evaluation method to assess the quality of generated motions in relation to their conditions, highlighting the adaptability of MixerMDM during the denoising process.'}, 'zh': {'title': 'åŠ¨æ€æ··åˆï¼Œç²¾ç»†æ§åˆ¶äººç±»è¿åŠ¨ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ç»„åˆæŠ€æœ¯ï¼Œç§°ä¸ºMixerMDMï¼Œç”¨äºç»“åˆé¢„è®­ç»ƒçš„æ–‡æœ¬æ¡ä»¶äººç±»è¿åŠ¨æ‰©æ•£æ¨¡å‹ã€‚ä¸ä»¥å¾€çš„æ–¹æ³•ä¸åŒï¼ŒMixerMDMé‡‡ç”¨åŠ¨æ€æ··åˆç­–ç•¥ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒå­¦ä¹ å¦‚ä½•æ ¹æ®ç”Ÿæˆæ¡ä»¶ç»„åˆå»å™ªè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å¯¹å•äººå’Œå¤šäººè¿åŠ¨çš„ç²¾ç»†æ§åˆ¶ï¼Œæå‡äº†æ¯ä¸ªäººçš„åŠ¨æ€è¡¨ç°åŠæ•´ä½“äº’åŠ¨æ•ˆæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŠ€æœ¯ï¼Œé¦–æ¬¡é‡åŒ–äº†ç”Ÿæˆè¿åŠ¨ä¸æ¡ä»¶ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼Œä»¥åŠMixerMDMåœ¨å»å™ªè¿‡ç¨‹ä¸­é€‚åº”æ··åˆçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00906', 'title': 'Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents', 'url': 'https://huggingface.co/papers/2504.00906', 'abstract': 'Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.', 'score': 13, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'e51174b579a417e9', 'authors': ['Saaket Agashe', 'Kyle Wong', 'Vincent Tu', 'Jiachen Yang', 'Ang Li', 'Xin Eric Wang'], 'affiliations': ['Simular Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00906.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Agent S2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agent S2 - Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Agent S2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼.'}, 'en': {'title': 'Agent S2: Revolutionizing Task Automation with Smart Planning and Grounding', 'desc': 'This paper presents Agent S2, a new framework designed to improve the performance of computer use agents that automate tasks by interacting with graphical user interfaces (GUIs). The framework addresses key challenges such as accurately identifying GUI elements and planning complex tasks over time. It introduces a Mixture-of-Grounding technique for better GUI localization and Proactive Hierarchical Planning to adapt action plans based on real-time observations. Evaluations show that Agent S2 outperforms existing agents on multiple benchmarks, demonstrating significant improvements in task execution across different operating systems.'}, 'zh': {'title': 'Agent S2ï¼šæ™ºèƒ½ä»£ç†çš„æ–°çºªå…ƒ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAgent S2çš„æ–°å‹æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†è®¤çŸ¥ä»»åŠ¡åˆ†é…ç»™ä¸åŒçš„é€šç”¨æ¨¡å‹å’Œä¸“ä¸šæ¨¡å‹æ¥æé«˜æ•°å­—ä»»åŠ¡çš„è‡ªåŠ¨åŒ–æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆå®šä½æŠ€æœ¯ï¼Œä»¥å®ç°ç²¾ç¡®çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å…ƒç´ å®šä½ï¼Œå¹¶å¼•å…¥äº†ä¸»åŠ¨å±‚æ¬¡è§„åˆ’ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸æ–­å˜åŒ–çš„è§‚å¯ŸåŠ¨æ€è°ƒæ•´è¡ŒåŠ¨è®¡åˆ’ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAgent S2åœ¨ä¸‰ä¸ªä¸»è¦çš„è®¡ç®—æœºä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„é¢†å…ˆä»£ç†ã€‚ç‰¹åˆ«æ˜¯åœ¨OSWorldè¯„ä¼°ä¸­ï¼ŒAgent S2ç›¸è¾ƒäºå…¶ä»–ä»£ç†å®ç°äº†18.9%å’Œ32.7%çš„ç›¸å¯¹æå‡ï¼Œå±•ç°äº†å…¶åœ¨ä¸åŒæ“ä½œç³»ç»Ÿå’Œåº”ç”¨ä¸­çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00509', 'title': 'Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?', 'url': 'https://huggingface.co/papers/2504.00509', 'abstract': "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.", 'score': 12, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'c9697e67f23cfa4e', 'authors': ['Kai Yan', 'Yufei Xu', 'Zhengyin Du', 'Xuesong Yao', 'Zheyu Wang', 'Xiaowen Guo', 'Jiecao Chen'], 'affiliations': ['ByteDance Seed', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.00509.jpg', 'data': {'categories': ['#multimodal', '#hallucinations', '#benchmark', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑƒĞ¼Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ?', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RoR-Bench Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ·Ğ°ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ´Ğ¾ 60%) Ğ½Ğ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ñ€Ğ°Ğ·Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'Reassessing LLM Intelligence: Are They Truly Reasoning?', 'desc': 'This paper introduces RoR-Bench, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs). The authors investigate whether LLMs demonstrate genuine intelligence or merely replicate learned responses from their training data. Their empirical analysis reveals that even advanced LLMs, like OpenAI-o1 and DeepSeek-R1, show significant performance dropsâ€”up to 60%â€”when faced with slight changes in problem phrasing. This raises important questions about the actual reasoning abilities of these models and suggests a need for a deeper understanding of their intelligence.'}, 'zh': {'title': 'é‡æ–°å®¡è§†LLMçš„æ™ºèƒ½æ°´å¹³', 'desc': 'è¿‘å¹´æ¥ï¼ŒLLMåŸºå‡†æµ‹è¯•çš„éš¾åº¦ä»å°å­¦æ°´å¹³è¿…é€Ÿä¸Šå‡åˆ°å‰æ²¿é—®é¢˜ï¼Œè¿™è®©ç ”ç©¶äººå‘˜æ„Ÿåˆ°æˆ‘ä»¬ç¦»è¶…è¶Šäººç±»æ™ºèƒ½åªæœ‰ä¸€æ­¥ä¹‹é¥ã€‚ç„¶è€Œï¼ŒLLMçš„æ¨ç†èƒ½åŠ›æ˜¯å¦çœŸçš„æ˜¯äººç±»æ ‡å‡†ä¸‹çš„çœŸæ­£æ™ºèƒ½ï¼Œè¿˜æ˜¯ä»…ä»…åœ¨è®­ç»ƒä¸­è§è¿‡çš„è§£å†³æ–¹æ¡ˆçš„å¤è¿°ï¼Ÿä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RoR-Benchï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œç”¨äºæ£€æµ‹LLMåœ¨ç®€å•æ¨ç†é—®é¢˜ä¸­æ˜¯å¦å­˜åœ¨å¤è¿°è¡Œä¸ºã€‚æˆ‘ä»¬çš„å®è¯åˆ†æå‘ç°ï¼Œç°æœ‰çš„é¡¶å°–LLMåœ¨æ¡ä»¶ç¨å¾®æ”¹å˜æ—¶ï¼Œè¡¨ç°å‡ºæå…¶ä¸¥é‡çš„å¤è¿°è¡Œä¸ºï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬é‡æ–°è¯„ä¼°è¿™äº›æ¨¡å‹çš„çœŸå®æ™ºèƒ½æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24377', 'title': 'Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models', 'url': 'https://huggingface.co/papers/2503.24377', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.', 'score': 11, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'd18ba97a459aea36', 'authors': ['Rui Wang', 'Hongru Wang', 'Boyang Xue', 'Jianhui Pang', 'Shudong Liu', 'Yi Chen', 'Jiahao Qiu', 'Derek Fai Wong', 'Heng Ji', 'Kam-Fai Wong'], 'affiliations': ['Princeton University', 'The Chinese University of Hong Kong', 'The University of Hong Kong', 'University of Illinois Urbana-Champaign', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.24377.jpg', 'data': {'categories': ['#inference', '#survey', '#reasoning', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ¿ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ÑÑ‚Ğ°Ğ¿ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ñ€ĞµĞ¼ÑÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Balancing Performance and Cost in Language Model Reasoning', 'desc': 'This paper discusses the advancements in Large Language Models (LLMs) that allow them to perform complex reasoning tasks more effectively. It highlights the difference between two types of reasoning: System 1, which is fast and efficient but less accurate, and System 2, which is slower and more accurate but computationally expensive. The authors introduce the concept of reasoning economy, which aims to balance the trade-off between performance and computational costs. They analyze the inefficiencies in reasoning, explore different reasoning patterns, and propose solutions to enhance the reasoning economy of LLMs, providing insights for future research.'}, 'zh': {'title': 'æ¨ç†ç»æµï¼šå¹³è¡¡æ€§èƒ½ä¸è®¡ç®—æˆæœ¬çš„å…³é”®', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å…¶æ‰§è¡Œå¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¿«é€Ÿç›´è§‰æ€ç»´ï¼ˆç³»ç»Ÿ1ï¼‰ä¸ç¼“æ…¢æ·±åº¦æ¨ç†ï¼ˆç³»ç»Ÿ2ï¼‰ä¹‹é—´çš„è½¬å˜ã€‚è™½ç„¶ç³»ç»Ÿ2æ¨ç†æé«˜äº†ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œä½†ç”±äºå…¶æ€ç»´ç¼“æ…¢å’Œæ¨ç†è¡Œä¸ºä½æ•ˆï¼Œå¾€å¾€ä¼šå¸¦æ¥è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚ç›¸å¯¹è€Œè¨€ï¼Œç³»ç»Ÿ1æ¨ç†è®¡ç®—æ•ˆç‡é«˜ï¼Œä½†å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚å› æ­¤ï¼Œå¹³è¡¡æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œå½¢æˆäº†æ¨ç†ç»æµçš„æ¦‚å¿µï¼Œè¿™æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22952', 'title': 'OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts', 'url': 'https://huggingface.co/papers/2503.22952', 'abstract': 'The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.', 'score': 11, 'issue_id': 3024, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '8b78ccf427a5cdc0', 'authors': ['Yuxuan Wang', 'Yueqian Wang', 'Bo Chen', 'Tong Wu', 'Dongyan Zhao', 'Zilong Zheng'], 'affiliations': ['Beijing Institute for General Artificial Intelligence', 'State Key Laboratory of General Artificial Intelligence', 'Wangxuan Institute of Computer Technology, Peking University', 'X-LANCE Lab, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22952.jpg', 'data': {'categories': ['#video', '#games', '#inference', '#multimodal', '#reasoning', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'OmniMMI: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniMMI - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1000 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 2000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞµÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Multi-modal Multiplexing Modeling (M4) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Enhancing Interaction with OmniLLMs in Streaming Video', 'desc': 'This paper presents OmniMMI, a new benchmark for evaluating Omni language models (OmniLLMs) in the context of streaming video. It addresses the challenges of understanding and reasoning in real-time video interactions, which are often overlooked in current benchmarks. The benchmark includes a large dataset of over 1,121 videos and 2,290 questions, focusing on proactive reasoning across six subtasks. Additionally, the authors introduce a framework called Multi-modal Multiplexing Modeling (M4) that enhances the efficiency of streaming models by allowing them to process audio and visual data simultaneously while generating responses.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„äº’åŠ¨èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†OmniMMIï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºOmniè¯­è¨€æ¨¡å‹åœ¨æµåª’ä½“è§†é¢‘ç¯å¢ƒä¸­è®¾è®¡çš„å¤šæ¨¡æ€äº¤äº’åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡1121ä¸ªè§†é¢‘å’Œ2290ä¸ªé—®é¢˜ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘åŸºå‡†ä¸­æµåª’ä½“è§†é¢‘ç†è§£å’Œä¸»åŠ¨æ¨ç†çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç§°ä¸ºå¤šæ¨¡æ€å¤ç”¨å»ºæ¨¡ï¼ˆM4ï¼‰ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆæ¨ç†çš„æµåª’ä½“æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆå†…å®¹çš„åŒæ—¶è¿›è¡Œè§†è§‰å’Œå¬è§‰å¤„ç†ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬å¸Œæœ›æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„äº’åŠ¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00927', 'title': 'Multi-Token Attention', 'url': 'https://huggingface.co/papers/2504.00927', 'abstract': 'Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other\'s attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector\'s capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method\'s ability to leverage richer information proves particularly beneficial.', 'score': 10, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '2af4c7adceecde31', 'authors': ['Olga Golovneva', 'Tianlu Wang', 'Jason Weston', 'Sainbayar Sukhbaatar'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2504.00927.jpg', 'data': {'categories': ['#long_context', '#architecture', '#benchmark', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Multi-Token Attention (MTA). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° soft attention, MTA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ ĞºĞ»ÑÑ‡Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. MTA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unlocking Richer Context with Multi-Token Attention', 'desc': 'This paper introduces Multi-Token Attention (MTA), a novel attention mechanism designed to improve the performance of large language models (LLMs). Unlike traditional single token attention, which relies on a single query and key token vector, MTA allows for the simultaneous consideration of multiple query and key vectors. By utilizing convolution operations, MTA enhances the interaction between nearby queries and keys, leading to more accurate attention weights. The results show that MTA significantly outperforms standard Transformer models, especially in tasks involving long contexts and information retrieval.'}, 'zh': {'title': 'å¤šä»¤ç‰Œæ³¨æ„åŠ›ï¼šæå‡ä¸Šä¸‹æ–‡ç†è§£çš„å…³é”®', 'desc': 'è½¯æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ä¸€ä¸ªé‡è¦çš„ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºåœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­å®šä½ç›¸å…³éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å•ä¸ªä»¤ç‰Œæ³¨æ„åŠ›æ–¹æ³•ä»…ä¾èµ–äºå•ä¸ªæŸ¥è¯¢å’Œé”®å‘é‡çš„ç›¸ä¼¼æ€§ï¼Œè¿™é™åˆ¶äº†ä¿¡æ¯çš„ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æ–¹æ³•â€”â€”å¤šä»¤ç‰Œæ³¨æ„åŠ›ï¼ˆMTAï¼‰ï¼Œå®ƒå…è®¸LLMsåŒæ—¶åŸºäºå¤šä¸ªæŸ¥è¯¢å’Œé”®å‘é‡æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ã€‚é€šè¿‡å¯¹æŸ¥è¯¢ã€é”®å’Œå¤´éƒ¨åº”ç”¨å·ç§¯æ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œä»è€Œåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ›´å‡†ç¡®åœ°å®šä½ç›¸å…³å†…å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01017', 'title': 'Scaling Language-Free Visual Representation Learning', 'url': 'https://huggingface.co/papers/2504.01017', 'abstract': 'Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.', 'score': 9, 'issue_id': 3023, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '9ab970f68b26c2ea', 'authors': ['David Fan', 'Shengbang Tong', 'Jiachen Zhu', 'Koustuv Sinha', 'Zhuang Liu', 'Xinlei Chen', 'Michael Rabbat', 'Nicolas Ballas', 'Yann LeCun', 'Amir Bar', 'Saining Xie'], 'affiliations': ['FAIR, Meta', 'New York University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01017.jpg', 'data': {'categories': ['#cv', '#training', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (SSL) Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… (CLIP) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SSL Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ CLIP Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² (VQA). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ SSL Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸.'}, 'en': {'title': 'Visual SSL: Bridging the Gap with Scale and Data', 'desc': 'This paper investigates the performance gap between Visual Self-Supervised Learning (SSL) and Contrastive Language-Image Pretraining (CLIP) in multimodal tasks like Visual Question Answering (VQA). The authors explore whether this gap is due to the absence of language supervision in visual SSL or differences in the training datasets used. By training both types of models on the same MetaCLIP data, they find that visual SSL models can outperform CLIP models when scaled up, achieving comparable performance on various benchmarks. This suggests that visual SSL can effectively compete with language-supervised methods, highlighting its potential for advancing vision-centric representation learning.'}, 'zh': {'title': 'è§†è§‰è‡ªç›‘ç£å­¦ä¹ çš„æ½œåŠ›ä¸CLIPç›¸åª²ç¾', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä¸å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç›¸åŒçš„MetaCLIPæ•°æ®ä¸Šè®­ç»ƒè§†è§‰SSLå’ŒCLIPæ¨¡å‹ï¼Œæ¥åˆ†æè¯­è¨€ç›‘ç£çš„ç¼ºä¹æ˜¯å¦æ˜¯å¯¼è‡´è§†è§‰SSLè½åçš„åŸå› ã€‚ç»“æœæ˜¾ç¤ºï¼Œè§†è§‰SSLæ¨¡å‹åœ¨æ•°æ®å’Œæ¨¡å‹å®¹é‡æ–¹é¢çš„æ‰©å±•æ€§ä¼˜äºCLIPæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å‚æ•°è¾¾åˆ°70äº¿æ—¶æ€§èƒ½ä»æœªé¥±å’Œã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œçº¯è§†è§‰SSLåœ¨å¤§è§„æ¨¡ä¸‹å¯ä»¥è¾¾åˆ°ä¸è¯­è¨€ç›‘ç£è§†è§‰é¢„è®­ç»ƒç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºè§†è§‰ä¸­å¿ƒçš„è¡¨ç¤ºå­¦ä¹ å¼€è¾Ÿäº†æ–°çš„æœºä¼šã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01005', 'title': 'When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning', 'url': 'https://huggingface.co/papers/2504.01005', 'abstract': 'Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.', 'score': 9, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'ee8e4951bf6c7a18', 'authors': ['Nishad Singhi', 'Hritik Bansal', 'Arian Hosseini', 'Aditya Grover', 'Kai-Wei Chang', 'Marcus Rohrbach', 'Anna Rohrbach'], 'affiliations': ['Google DeepMind', 'Mila', 'TU Darmstadt & hessian.AI', 'University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2504.01005.jpg', 'data': {'categories': ['#training', '#math', '#optimization', '#reasoning', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Consistency (SC), Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ğ¾Ğµ, Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Generative Reward Models (GenRM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ SC Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ GenRM, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Balancing Solution Generation and Verification for Efficient Reasoning in LLMs', 'desc': 'This paper explores how to improve the reasoning abilities of large language models (LLMs) during problem-solving by adjusting the amount of computation used at test time. It compares two methods: Self-Consistency (SC), which generates multiple answers and picks the most common, and Generative Reward Models (GenRM), which scores answers based on a next-token prediction approach. The study finds that SC is generally more efficient in terms of compute resources compared to GenRM, especially under limited budgets. The authors provide insights on how to effectively balance the generation of solutions and their verification to optimize performance.'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†èƒ½åŠ›ï¼šè§£ç”Ÿæˆä¸éªŒè¯çš„å¹³è¡¡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œå¦‚ä½•é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—æ¥æå‡æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­ã€‚ä¼ ç»Ÿçš„è‡ªä¸€è‡´æ€§ï¼ˆSelf-Consistency, SCï¼‰æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šä¸ªè§£å¹¶é‡‡ç”¨å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€å¸¸è§çš„ç­”æ¡ˆã€‚æœ€è¿‘çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenerative Reward Models, GenRMï¼‰åˆ™å°†éªŒè¯é‡æ„ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä»»åŠ¡ï¼Œä»è€Œåœ¨æ¨ç†æ—¶å¼•å…¥æ–°çš„æ‰©å±•æ–¹å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å›ºå®šçš„æ¨ç†é¢„ç®—ä¸‹ï¼ŒSCåœ¨å¤§å¤šæ•°å®é™…æƒ…å†µä¸‹æ¯”GenRMæ›´å…·è®¡ç®—æ•ˆç‡ï¼Œæä¾›äº†åœ¨æµ‹è¯•æ—¶æ‰©å±•ä¸­ä¼˜åŒ–è§£ç”Ÿæˆä¸éªŒè¯çš„å®ç”¨æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23434', 'title': 'Towards Trustworthy GUI Agents: A Survey', 'url': 'https://huggingface.co/papers/2503.23434', 'abstract': 'GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.', 'score': 9, 'issue_id': 3024, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'e19e4d94bcea9cb0', 'authors': ['Yucheng Shi', 'Wenhao Yu', 'Wenlin Yao', 'Wenhu Chen', 'Ninghao Liu'], 'affiliations': ['Amazon', 'Tencent AI Seattle Lab', 'University of Georgia', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23434.jpg', 'data': {'categories': ['#ethics', '#security', '#agents', '#survey', '#training', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”Ğ¾Ğ²ĞµÑ€Ğ¸Ğµ Ğº Ğ˜Ğ˜: Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²: ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¸ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğµ ÑĞ±Ğ¾Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Ensuring Trust in Autonomous GUI Agents', 'desc': 'This paper surveys the trustworthiness of GUI agents that use large foundation models to interact with digital interfaces. It highlights five critical dimensions of trustworthiness: security vulnerabilities, reliability in changing environments, transparency, ethical considerations, and evaluation methods. The authors discuss significant challenges such as susceptibility to adversarial attacks and the need for realistic evaluation benchmarks. The paper emphasizes the importance of developing robust safety standards and responsible practices as GUI agents become more prevalent.'}, 'zh': {'title': 'æ„å»ºå¯ä¿¡èµ–çš„GUIä»£ç†ï¼Œä¿éšœå®‰å…¨ä¸éšç§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºå¤§å‹åŸºç¡€æ¨¡å‹çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„ä¿¡ä»»æ€§é—®é¢˜ã€‚æˆ‘ä»¬åˆ†æäº†äº”ä¸ªå…³é”®ç»´åº¦ï¼ŒåŒ…æ‹¬å®‰å…¨æ¼æ´ã€åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§ã€é€æ˜æ€§ä¸å¯è§£é‡Šæ€§ã€ä¼¦ç†è€ƒé‡ä»¥åŠè¯„ä¼°æ–¹æ³•ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºäº†ä¸»è¦æŒ‘æˆ˜ï¼Œå¦‚å¯¹æŠ—æ€§æ”»å‡»çš„è„†å¼±æ€§ã€åºåˆ—å†³ç­–ä¸­çš„çº§è”å¤±è´¥æ¨¡å¼ï¼Œä»¥åŠç¼ºä¹ç°å®çš„è¯„ä¼°åŸºå‡†ã€‚è¿™äº›é—®é¢˜ä¸ä»…é˜»ç¢äº†GUIä»£ç†çš„å®é™…åº”ç”¨ï¼Œè¿˜éœ€è¦è¶…è¶Šä»»åŠ¡æˆåŠŸçš„å…¨é¢ç¼“è§£ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23733', 'title': 'AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization', 'url': 'https://huggingface.co/papers/2503.23733', 'abstract': 'Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.', 'score': 8, 'issue_id': 3017, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'ed45063868071c13', 'authors': ['Yiyang Du', 'Xiaochen Wang', 'Chi Chen', 'Jiabo Ye', 'Yiru Wang', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Zhifang Sui', 'Maosong Sun', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China', 'Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China', 'Institute of Intelligent Computing, Alibaba Group', 'Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China', 'ModelTC Open Source Organization, Beijing, China', 'School of Software Microelectronics, Peking University, Beijing, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.23733.jpg', 'data': {'categories': ['#training', '#architecture', '#transfer_learning', '#optimization', '#multimodal', '#benchmark'], 'emoji': 'ğŸ”€', 'ru': {'title': 'AdaMMS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AdaMMS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸Ñ… ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². AdaMMS Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… MLLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ AdaMMS Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Merging Diverse Models with AdaMMS', 'desc': 'This paper introduces AdaMMS, a new method for merging Multimodal Large Language Models (MLLMs) that have different architectures. Traditional merging techniques struggle with these heterogeneous models due to their varying structures and parameter spaces. AdaMMS addresses this by first mapping the models, then merging their weights through linear interpolation, and finally optimizing hyper-parameters using an unsupervised approach. The results show that AdaMMS significantly improves performance on vision-language tasks compared to earlier methods.'}, 'zh': {'title': 'å¼‚è´¨æ¨¡å‹åˆå¹¶çš„æ–°çªç ´', 'desc': 'æœ€è¿‘ï¼Œæ¨¡å‹åˆå¹¶æ–¹æ³•åœ¨ç»“åˆå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ä¼˜åŠ¿ã€‚ä»¥å¾€çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åˆå¹¶å…·æœ‰ç›¸åŒæ¶æ„çš„åŒè´¨æ¨¡å‹ï¼Œä½†åœ¨å¤„ç†å…·æœ‰å›ºæœ‰å¼‚è´¨æ€§çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†AdaMMSï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¼‚è´¨MLLMsè®¾è®¡çš„æ–°å‹æ¨¡å‹åˆå¹¶æ–¹æ³•ï¼Œé‡‡ç”¨æ˜ å°„ã€åˆå¹¶å’Œæœç´¢ä¸‰ä¸ªæ­¥éª¤æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é€šè¿‡è®¾è®¡æ¨¡å‹ä¹‹é—´çš„æ˜ å°„å‡½æ•°ã€å¯¹æ¨¡å‹æƒé‡è¿›è¡Œçº¿æ€§æ’å€¼ä»¥åŠæå‡ºæ— ç›‘ç£çš„è¶…å‚æ•°é€‰æ‹©æ–¹æ³•ï¼ŒAdaMMSåœ¨å„ç§è§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä»¥å¾€çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00557', 'title': 'Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features', 'url': 'https://huggingface.co/papers/2504.00557', 'abstract': 'Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.', 'score': 7, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'c5628fbf1a189a06', 'authors': ['Jewon Lee', 'Ki-Ung Song', 'Seungmin Yang', 'Donguk Lim', 'Jaeyeon Kim', 'Wooksu Shin', 'Bo-Kyeong Kim', 'Yong Jae Lee', 'Tae-Ho Kim'], 'affiliations': ['Nota Inc.', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.00557.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#cv'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ñ€Ñ‚ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Trimmed Llama: Efficient Visual Token Reduction for Faster Inference', 'desc': 'This paper presents a method called Trimmed Llama, which reduces the number of visual tokens in large vision-language models (LVLMs) to lower inference costs. Unlike previous studies that focused on self-attention models, this work specifically targets cross-attention models, which are known for their better performance. The authors highlight that the key-value (KV) cache for image tokens in cross-attention layers is much larger than that for text tokens, creating a significant computational bottleneck. By selectively pruning redundant visual features based on the sparse nature of cross-attention maps, the model achieves a 50% reduction in visual features, leading to decreased latency and memory usage while maintaining performance benchmarks.'}, 'zh': {'title': 'è§†è§‰ç‰¹å¾ä¿®å‰ªï¼Œæå‡æ¨ç†æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è§†è§‰æ ‡è®°å‡å°‘çš„æ–¹æ³•ï¼Œä»¥é™ä½å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ¨ç†æ—¶çš„è®¡ç®—æˆæœ¬ã€‚ä¸ä»¥å¾€åªé’ˆå¯¹è‡ªæ³¨æ„åŠ›æ¨¡å‹çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸“æ³¨äºåŸºäºäº¤å‰æ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ›´ä¸ºä¼˜è¶Šã€‚æˆ‘ä»¬å‘ç°äº¤å‰æ³¨æ„åŠ›å±‚ä¸­å›¾åƒæ ‡è®°çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°è¿œå¤§äºè‡ªæ³¨æ„åŠ›å±‚ä¸­çš„æ–‡æœ¬æ ‡è®°ï¼Œæˆä¸ºè®¡ç®—ç“¶é¢ˆã€‚é€šè¿‡åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å›¾çš„ç¨€ç–ç‰¹æ€§ï¼Œæˆ‘ä»¬é€‰æ‹©æ€§åœ°ä¿®å‰ªå†—ä½™çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘KVç¼“å­˜éœ€æ±‚ï¼Œé™ä½æ¨ç†å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒåŸºå‡†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22165', 'title': 'Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.22165', 'abstract': 'Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts.', 'score': 7, 'issue_id': 3034, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '76c9bb027c844f9c', 'authors': ['Zhanke Zhou', 'Zhaocheng Zhu', 'Xuan Li', 'Mikhail Galkin', 'Xiao Feng', 'Sanmi Koyejo', 'Jian Tang', 'Bo Han'], 'affiliations': ['HEC Montreal', 'Intel AI Lab', 'Mila - Quebec AI Institute', 'Stanford University', 'TMLR Group, Hong Kong Baptist University', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.22165.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#reasoning', '#multimodal', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'landscape of thoughts' Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ t-SNE Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 'landscape of thoughts' ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ¸Ğ·ĞºĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ."}, 'en': {'title': 'Visualizing Reasoning Paths in Language Models', 'desc': "This paper introduces a new visualization tool called 'landscape of thoughts' that helps users understand how large language models (LLMs) reason through problems. It represents reasoning paths as feature vectors, which show how close each reasoning step is to possible answers. By using a technique called t-SNE, the tool creates two-dimensional plots that allow for easy comparison of different models and their reasoning effectiveness. The tool also identifies problematic reasoning patterns and can be adapted to evaluate the accuracy of reasoning paths in various tasks."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è·¯å¾„', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œæ€ç»´æ™¯è§‚â€çš„å¯è§†åŒ–å·¥å…·ï¼Œç”¨äºåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†è·¯å¾„ã€‚è¯¥å·¥å…·é€šè¿‡å°†æ¨ç†è·¯å¾„ä¸­çš„çŠ¶æ€è¡¨ç¤ºä¸ºç‰¹å¾å‘é‡ï¼Œé‡åŒ–å®ƒä»¬ä¸æ‰€æœ‰ç­”æ¡ˆé€‰é¡¹çš„è·ç¦»ï¼Œå¹¶ä½¿ç”¨t-SNEè¿›è¡ŒäºŒç»´å¯è§†åŒ–ã€‚é€šè¿‡å¯¹æ€ç»´æ™¯è§‚çš„å®šæ€§å’Œå®šé‡åˆ†æï¼Œå¯ä»¥æœ‰æ•ˆåŒºåˆ†å¼ºå¼±æ¨¡å‹ã€æ­£ç¡®ä¸é”™è¯¯ç­”æ¡ˆï¼Œä»¥åŠä¸åŒçš„æ¨ç†ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥å·¥å…·è¿˜èƒ½å¤Ÿæ­ç¤ºä¸ç†æƒ³çš„æ¨ç†æ¨¡å¼ï¼Œå¦‚ä½ä¸€è‡´æ€§å’Œé«˜ä¸ç¡®å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00294', 'title': 'Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead', 'url': 'https://huggingface.co/papers/2504.00294', 'abstract': "Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.", 'score': 6, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'b455a4adb4eae588', 'authors': ['Vidhisha Balachandran', 'Jingya Chen', 'Lingjiao Chen', 'Shivam Garg', 'Neel Joshi', 'Yash Lara', 'John Langford', 'Besmira Nushi', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00294.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#math', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM: Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ÑÑ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸.'}, 'en': {'title': 'Scaling Inference for Smarter Problem Solving in LLMs', 'desc': 'This paper explores how inference-time scaling can improve the reasoning abilities of large language models (LLMs) when tackling complex problems. It examines the effectiveness of extending generated scratchpads for various tasks, including math reasoning and navigation, across nine advanced models. The study finds that while scaling can enhance performance, its benefits vary by task and may decrease with increased complexity. Additionally, the research indicates that simply increasing the number of tokens does not guarantee better accuracy, but using strong feedback mechanisms can lead to significant performance improvements.'}, 'zh': {'title': 'æ¨ç†æ—¶é—´æ‰©å±•ï¼šæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ¨ç†æ—¶é—´æ‰©å±•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬åˆ†æäº†ä¹ç§æœ€å…ˆè¿›æ¨¡å‹åœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ•°å­¦æ¨ç†å’Œç©ºé—´æ¨ç†ç­‰ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨ç†æ—¶é—´æ‰©å±•çš„ä¼˜åŠ¿å› ä»»åŠ¡è€Œå¼‚ï¼Œä¸”åœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶ä¼šå‡å¼±ã€‚å°½ç®¡ä½¿ç”¨æ›´å¤šçš„æ ‡è®°å¹¶ä¸æ€»èƒ½æé«˜å‡†ç¡®æ€§ï¼Œä½†åœ¨æœ‰å¼ºåé¦ˆçš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00869', 'title': 'm1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models', 'url': 'https://huggingface.co/papers/2504.00869', 'abstract': "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.", 'score': 5, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'bd9586b08ce02a05', 'authors': ['Xiaoke Huang', 'Juncheng Wu', 'Hui Liu', 'Xianfeng Tang', 'Yuyin Zhou'], 'affiliations': ['Amazon Research', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2504.00869.jpg', 'data': {'categories': ['#reasoning', '#healthcare', '#science', '#training', '#inference'], 'emoji': 'ğŸ©º', 'ru': {'title': 'ĞœĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ m1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Medical Reasoning with Test-Time Scaling', 'desc': "This paper explores the use of test-time scaling to improve the reasoning abilities of large language models specifically in the medical field. The authors introduce a method called m1, which enhances medical reasoning during inference, showing that smaller models can achieve state-of-the-art results. They find that while increasing the reasoning token budget can help, it may also lead to performance degradation if overused. The study emphasizes that having rich medical knowledge is crucial for effective reasoning, rather than just increasing the model's complexity or depth of reasoning."}, 'zh': {'title': 'åŒ»å­¦æ¨ç†çš„æ–°çªç ´ï¼šæµ‹è¯•æ—¶ç¼©æ”¾çš„åŠ›é‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯åœ¨åŒ»å­¦æ¨ç†ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åä¸ºm1çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨æ¨ç†æ—¶çš„åŒ»å­¦èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾åœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸­å‡èƒ½æ˜¾è‘—æé«˜æ¨ç†æ•ˆæœï¼Œå°¤å…¶æ˜¯å¯¹äºå‚æ•°å°‘äº10Bçš„è½»é‡çº§å¾®è°ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿè¾¾åˆ°æ–°çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œæœ€ä½³çš„æ¨ç†ä»¤ç‰Œé¢„ç®—çº¦ä¸º4Kï¼Œè¶…å‡ºæ­¤èŒƒå›´å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œå¢åŠ æ•°æ®è§„æ¨¡ã€æé«˜æ•°æ®è´¨é‡å’Œæ‰©å±•æ¨¡å‹å®¹é‡æ˜¯æå‡åŒ»å­¦çŸ¥è¯†åŸºç¡€çš„å…³é”®ï¼Œå°¤å…¶æ˜¯åœ¨å°æ¨¡å‹æ€§èƒ½é¥±å’Œçš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23361', 'title': 'Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base', 'url': 'https://huggingface.co/papers/2503.23361', 'abstract': "Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.", 'score': 4, 'issue_id': 3017, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': '9b957c49c958aea3', 'authors': ['Linxin Song', 'Xuwei Ding', 'Jieyu Zhang', 'Taiwei Shi', 'Ryotaro Shimizu', 'Rahul Gupta', 'Yang Liu', 'Jian Kang', 'Jieyu Zhao'], 'affiliations': ['AGI', 'Amazon', 'University of Rochester', 'University of Southern California', 'University of Washington', 'University of Wisconsin-Madison', 'ZOZO Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23361.jpg', 'data': {'categories': ['#training', '#hallucinations', '#optimization', '#graphs', '#benchmark', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'SEA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SEA (ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´ÑŠĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). SEA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SEA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² LLM.'}, 'en': {'title': 'Uncovering Knowledge Deficiencies in LLMs Efficiently with SEA', 'desc': 'This paper introduces a new method called Stochastic Error Ascent (SEA) to identify knowledge deficiencies in large language models (LLMs) that often produce unreliable outputs. SEA efficiently discovers errors by using a stochastic optimization approach, focusing on high-error candidates based on their similarity to previously identified failures. The framework enhances its search capabilities through hierarchical retrieval and a directed acyclic graph to track error propagation. The results show that SEA significantly outperforms existing methods in uncovering knowledge errors while drastically reducing the cost of error discovery.'}, 'zh': {'title': 'å‘ç°LLMçŸ¥è¯†ç¼ºé™·çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸æ— æ³•å‡†ç¡®ä¿ç•™äº‹å®çŸ¥è¯†ï¼Œå¯¼è‡´å¹»è§‰å’Œä¸å¯é çš„è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºéšæœºé”™è¯¯ä¸Šå‡ï¼ˆSEAï¼‰çš„æ¡†æ¶ï¼Œç”¨äºåœ¨ä¸¥æ ¼çš„æŸ¥è¯¢é¢„ç®—ä¸‹å‘ç°é—­åˆæƒé‡LLMsä¸­çš„çŸ¥è¯†ç¼ºé™·ã€‚SEAé€šè¿‡åˆ©ç”¨ä¸å…ˆå‰è§‚å¯Ÿåˆ°çš„å¤±è´¥çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè¿­ä»£æ£€ç´¢æ–°çš„é«˜é”™è¯¯å€™é€‰é¡¹ï¼Œä»è€Œå°†é”™è¯¯å‘ç°è¿‡ç¨‹å½¢å¼åŒ–ä¸ºéšæœºä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒSEAå‘ç°çš„çŸ¥è¯†é”™è¯¯æ•°é‡æ˜¾è‘—é«˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†æ¯ä¸ªé”™è¯¯çš„æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00072', 'title': 'Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs', 'url': 'https://huggingface.co/papers/2504.00072', 'abstract': "We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page.", 'score': 3, 'issue_id': 3021, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '48f5266ddbfa7bca', 'authors': ['Lucas Ventura', 'Antoine Yang', 'Cordelia Schmid', 'GÃ¼l Varol'], 'affiliations': ['Google DeepMind', 'Inria, Ecole normale superieure, CNRS, PSL Research University', 'LIGM, Ecole des Ponts, IP Paris, Univ Gustave Eiffel, CNRS'], 'pdf_title_img': 'assets/pdf/title_img/2504.00072.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#open_source', '#video', '#multimodal'], 'emoji': 'ğŸ“½ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ 'Chapter-Llama' Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸. LLM Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ³Ğ»Ğ°Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VidChapters-7M, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ F1-score 45.3 Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 26.7."}, 'en': {'title': 'Efficient Video Chaptering with Chapter-Llama', 'desc': "This paper presents a novel approach to video chaptering, which involves dividing long videos into meaningful segments and generating titles for these segments. The authors introduce the 'Chapter-Llama' framework that utilizes a pretrained large language model (LLM) to process speech transcripts and video captions efficiently. By implementing a speech-guided frame selection strategy, they enhance the model's performance while reducing the need for extensive captioning. The results show significant improvements in chaptering accuracy, achieving a notable F1 score on the VidChapters-7M benchmark, and the authors provide their code and models for further research."}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç« èŠ‚åˆ’åˆ†çš„æ–°æ–¹æ³•', 'desc': "æœ¬æ–‡æ¢è®¨äº†è§†é¢‘ç« èŠ‚åˆ’åˆ†çš„ä»»åŠ¡ï¼Œå³å°†é•¿è§†é¢‘æ—¶é—´çº¿åˆ’åˆ†ä¸ºè¯­ä¹‰å•å…ƒå¹¶ç”Ÿæˆç›¸åº”çš„ç« èŠ‚æ ‡é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†'Chapter-Llama'æ¡†æ¶ï¼Œé€šè¿‡é«˜æ•ˆå¤„ç†æ–‡æœ¬é¢†åŸŸçš„é—®é¢˜ï¼Œå®ç°äº†å¯¹é•¿è¾¾ä¸€å°æ—¶è§†é¢‘çš„å¼ºå¤§ç« èŠ‚åˆ’åˆ†æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¾“å…¥åŒ…æ‹¬è¯­éŸ³è½¬å½•æ–‡æœ¬å’Œæè¿°è§†é¢‘å¸§çš„å­—å¹•ï¼Œä»¥åŠå®ƒä»¬å„è‡ªçš„æ—¶é—´æˆ³ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºè¯­éŸ³è½¬å½•å†…å®¹çš„è½»é‡çº§å¸§é€‰æ‹©ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†ç« èŠ‚åˆ’åˆ†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚"}}}, {'id': 'https://huggingface.co/papers/2503.24210', 'title': 'DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.24210', 'abstract': 'Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io', 'score': 2, 'issue_id': 3023, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'df1e0752d5790146', 'authors': ['Seungjun Lee', 'Gim Hee Lee'], 'affiliations': ['Department of Computer Science, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24210.jpg', 'data': {'categories': ['#synthetic', '#3d', '#cv', '#diffusion'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ§ĞµÑ‚ĞºĞ¾Ğµ 3D Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ 2D: DiET-GS Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiET-GS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ‚ĞºĞ¸Ñ… 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ 3DGS Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ğ»Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiET-GS Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing 3D Image Clarity with DiET-GS', 'desc': 'This paper addresses the challenge of creating clear 3D images from blurry multi-view pictures, a common issue in computer vision. The authors introduce DiET-GS, a new framework that combines event-based camera data with a diffusion prior to improve the quality of 3D image synthesis. By using a two-stage training approach, the framework effectively restores accurate colors and fine details in the images. The results show that DiET-GS outperforms existing methods in generating high-quality novel views from both synthetic and real-world datasets.'}, 'zh': {'title': 'æ¸…æ™°ä¸‰ç»´é‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDiET-GSçš„æ¡†æ¶ï¼Œç”¨äºä»æ¨¡ç³Šçš„å¤šè§†å›¾å›¾åƒä¸­é‡å»ºæ¸…æ™°çš„ä¸‰ç»´è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ— æ¨¡ç³Šäº‹ä»¶æµå’Œæ‰©æ•£å…ˆéªŒï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥æ¥æé«˜å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„çº¦æŸæ–¹æ³•ï¼Œåˆ©ç”¨äº‹ä»¶åŒé‡ç§¯åˆ†æ¥ç¡®ä¿é¢œè‰²å‡†ç¡®å’Œç»†èŠ‚æ¸…æ™°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€å•çš„æŠ€æœ¯ï¼Œåˆ©ç”¨æ‰©æ•£å…ˆéªŒè¿›ä¸€æ­¥å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23157', 'title': 'Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL', 'url': 'https://huggingface.co/papers/2503.23157', 'abstract': 'Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.', 'score': 2, 'issue_id': 3029, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '083970087ba6180e', 'authors': ['Mohammadreza Pourreza', 'Shayan Talaei', 'Ruoxi Sun', 'Xingchen Wan', 'Hailong Li', 'Azalia Mirhoseini', 'Amin Saberi', 'Sercan "O. Arik'], 'affiliations': ['Google Cloud', 'Google DeepMind', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23157.jpg', 'data': {'categories': ['#rlhf', '#rl', '#optimization', '#training', '#benchmark', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Text-to-SQL Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Text-to-SQL, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² RL. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO), Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ supervised fine-tuning Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BIRD.'}, 'en': {'title': 'Enhancing Text-to-SQL with Tailored Reinforcement Learning Rewards', 'desc': 'This paper addresses the complex task of converting natural language into SQL queries, which requires understanding language, database structures, and formulating precise queries. The authors critique existing methods that use fixed reasoning paths, which can hinder performance, and propose a new approach that utilizes tailored partial rewards to improve reinforcement learning outcomes. By implementing group relative policy optimization (GRPO), their method encourages large language models to enhance their reasoning skills, leading to better SQL generation. The results show that their reinforcement learning approach outperforms traditional supervised fine-tuning, achieving higher accuracy on benchmark tests with a smaller model size.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°SQLçš„æ¨ç†èƒ½åŠ›ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°SQLçš„ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€é¡¹æ¶‰åŠè‡ªç„¶è¯­è¨€ç†è§£å’Œæ•°æ®åº“æ¶æ„ç†è§£çš„å¤æ‚ä»»åŠ¡ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æ¨ç†è·¯å¾„ï¼Œé™åˆ¶äº†å…¶æ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„éƒ¨åˆ†å¥–åŠ±æœºåˆ¶ï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬åˆ°SQLä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ä¸Šéƒ½è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21860', 'title': 'ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning', 'url': 'https://huggingface.co/papers/2503.21860', 'abstract': 'Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.', 'score': 2, 'issue_id': 3017, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '0d9ea55946287027', 'authors': ['Kailin Li', 'Puhao Li', 'Tengyu Liu', 'Yuyang Li', 'Siyuan Huang'], 'affiliations': ['Department of Automation, Tsinghua University', 'Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.21860.jpg', 'data': {'categories': ['#training', '#dataset', '#robotics', '#transfer_learning', '#optimization', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ManipTrans: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼', 'desc': 'ManipTrans - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€ÑƒĞºĞ°Ğ¼ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ÑƒĞº Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ° ĞµĞ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DexManipNet Ñ 3.3 Ñ‚Ñ‹Ñ. ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¼Ğ¸ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficiently Teaching Robots to Manipulate Like Humans', 'desc': 'This paper presents ManipTrans, a two-stage method designed to transfer human bimanual manipulation skills to robotic hands in a simulated environment. The first stage involves pre-training a trajectory imitator that learns to replicate human hand movements, while the second stage fine-tunes a residual module to enhance performance under specific interaction constraints. This approach allows for efficient learning and execution of complex tasks, outperforming existing methods in terms of success rate and efficiency. Additionally, the authors introduce DexManipNet, a comprehensive dataset that includes diverse manipulation tasks, paving the way for improved policy training and real-world applications of dexterous robotic hands.'}, 'zh': {'title': 'é«˜æ•ˆè½¬ç§»äººç±»åŒæ‰‹æŠ€èƒ½çš„æœºå™¨äººæ‰‹', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºManipTransçš„æ–°æ–¹æ³•ï¼Œç”¨äºå°†äººç±»åŒæ‰‹çš„æŠ€èƒ½é«˜æ•ˆåœ°è½¬ç§»åˆ°çµå·§çš„æœºå™¨äººæ‰‹ä¸Šã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè®­ç»ƒä¸€ä¸ªé€šç”¨çš„è½¨è¿¹æ¨¡ä»¿å™¨æ¥æ¨¡æ‹Ÿæ‰‹éƒ¨åŠ¨ä½œï¼Œç„¶ååœ¨äº¤äº’çº¦æŸä¸‹å¾®è°ƒç‰¹å®šçš„æ®‹å·®æ¨¡å—ï¼Œä»è€Œå®ç°å¤æ‚åŒæ‰‹ä»»åŠ¡çš„é«˜æ•ˆå­¦ä¹ å’Œå‡†ç¡®æ‰§è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒManipTransåœ¨æˆåŠŸç‡ã€ä¿çœŸåº¦å’Œæ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ManipTransï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºDexManipNetçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«äº†3.3Kä¸ªæœºå™¨äººæ“ä½œçš„å®ä¾‹ï¼Œæ”¯æŒè¿›ä¸€æ­¥çš„ç­–ç•¥è®­ç»ƒå’Œå®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24219', 'title': 'MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote\n  Sensing', 'url': 'https://huggingface.co/papers/2503.24219', 'abstract': 'We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially supervised OD task. In the first stage, we construct a graph representation of each image, comprising object queries, class embeddings, and proposal locations. Then, our task-aware architecture processes this graph to perform the VG task. The model consists of: (i) a multi-branch network that integrates spatial, visual, and categorical features to generate task-aware proposals, and (ii) an object reasoning network that assigns probabilities across proposals, followed by a soft selection mechanism for final referring object localization. Our model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods while retaining classical OD capabilities. The code will be available in our repository: https://github.com/rd20karim/MB-ORES.', 'score': 1, 'issue_id': 3027, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'c59d3238abf4164f', 'authors': ['Karim Radouane', 'Hanane Azzag', 'Mustapha lebbah'], 'affiliations': ['University Paris-Saclay - DAVID Lab, UVSQ Versailles, France', 'University Sorbonne Paris Nord - LIPN, Villetaneuse, France'], 'pdf_title_img': 'assets/pdf/title_img/2503.24219.jpg', 'data': {'categories': ['#open_source', '#optimization', '#cv', '#architecture', '#graphs', '#dataset'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… OPT-RSVG Ğ¸ DIOR-RSVG.'}, 'en': {'title': 'Integrating Object Detection and Visual Grounding for Enhanced Remote Sensing Analysis', 'desc': 'This paper presents a new framework that combines object detection (OD) and visual grounding (VG) specifically for remote sensing imagery. The authors enhance a traditional open-set object detector by fine-tuning it with referring expression data, treating VG as a partially supervised OD task. They create a graph representation of images that includes object queries and class embeddings, which is then processed by a multi-branch network to generate proposals. The model outperforms existing methods on benchmark datasets while maintaining the capabilities of classical object detection.'}, 'zh': {'title': 'ç»Ÿä¸€æ¡†æ¶ï¼šç›®æ ‡æ£€æµ‹ä¸è§†è§‰å®šä½çš„ç»“åˆ', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†ç›®æ ‡æ£€æµ‹ï¼ˆODï¼‰å’Œè§†è§‰å®šä½ï¼ˆVGï¼‰é›†æˆåˆ°é¥æ„Ÿå›¾åƒä¸­ã€‚ä¸ºäº†æ”¯æŒä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹å¹¶ä¸ºè§†è§‰å®šä½ä»»åŠ¡å»ºç«‹ç›´è§‚çš„å…ˆéªŒï¼Œæˆ‘ä»¬ä½¿ç”¨å‚è€ƒè¡¨è¾¾æ•°æ®å¾®è°ƒäº†ä¸€ä¸ªå¼€æ”¾é›†ç›®æ ‡æ£€æµ‹å™¨ï¼Œå°†å…¶æ¡†å®šä¸ºéƒ¨åˆ†ç›‘ç£çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬æ„å»ºäº†æ¯ä¸ªå›¾åƒçš„å›¾å½¢è¡¨ç¤ºï¼ŒåŒ…æ‹¬ç›®æ ‡æŸ¥è¯¢ã€ç±»åˆ«åµŒå…¥å’Œæè®®ä½ç½®ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ„ŸçŸ¥æ¶æ„å¤„ç†è¿™ä¸ªå›¾å½¢ä»¥æ‰§è¡Œè§†è§‰å®šä½ä»»åŠ¡ï¼Œæœ€ç»ˆåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02605', 'title': 'Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving', 'url': 'https://huggingface.co/papers/2504.02605', 'abstract': 'The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.', 'score': 29, 'issue_id': 3095, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'bcf7d7c20685c914', 'authors': ['Daoguang Zan', 'Zhirong Huang', 'Wei Liu', 'Hanwu Chen', 'Linhao Zhang', 'Shulin Xin', 'Lu Chen', 'Qi Liu', 'Xiaojian Zhong', 'Aoyan Li', 'Siyao Liu', 'Yongsheng Xiao', 'Liangqiang Chen', 'Yuyu Zhang', 'Jing Su', 'Tianyu Liu', 'Rui Long', 'Kai Shen', 'Liang Xiang'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2504.02605.jpg', 'data': {'categories': ['#agi', '#benchmark', '#rl', '#open_source', '#multilingual', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Multi-SWE-bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1632 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€ĞµÑ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²: Agentless, SWE-agent Ğ¸ OpenHands. Ğ—Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Multi-SWE-RL Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'Empowering Issue Resolution with Multi-SWE-bench for Diverse Languages', 'desc': 'This paper introduces Multi-SWE-bench, a multilingual benchmark designed to evaluate Large Language Models (LLMs) in issue resolving across various programming languages, including Java, TypeScript, and C++. The benchmark consists of 1,632 high-quality instances, meticulously annotated by experts to ensure reliability in assessing model performance. The authors also present an analysis of state-of-the-art models using different evaluation methods and launch the Multi-SWE-RL community to foster the development of reinforcement learning datasets for issue-resolving tasks. By open-sourcing their data production pipeline and tutorials, they aim to encourage community contributions and advance research in this area, ultimately pushing towards the goal of Artificial General Intelligence (AGI).'}, 'zh': {'title': 'å¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†ï¼Œæ¨åŠ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§å¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†ï¼Œç§°ä¸ºMulti-SWE-benchï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿä¸­çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ¶µç›–äº†Javaã€TypeScriptã€JavaScriptã€Goã€Rustã€Cå’ŒC++ç­‰ä¸ƒç§ç¼–ç¨‹è¯­è¨€ï¼Œå…±åŒ…å«1,632ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†Multi-SWE-RLå¼€æºç¤¾åŒºï¼Œæ—¨åœ¨ä¸ºé—®é¢˜è§£å†³ä»»åŠ¡æ„å»ºå¤§è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å‘å¸ƒäº†4,723ä¸ªç»“æ„è‰¯å¥½çš„å®ä¾‹ã€‚é€šè¿‡å¼€æ”¾æ•°æ®ç”Ÿäº§æµç¨‹å’Œè¯¦ç»†æ•™ç¨‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ¿€åŠ±å¼€æºç¤¾åŒºæŒç»­è´¡çŒ®ï¼Œæ¨åŠ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03553', 'title': 'Agentic Knowledgeable Self-awareness', 'url': 'https://huggingface.co/papers/2504.03553', 'abstract': 'Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent\'s self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.', 'score': 19, 'issue_id': 3096, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '4a06cb6959ea30d3', 'authors': ['Shuofei Qiao', 'Zhisong Qiu', 'Baochang Ren', 'Xiaobin Wang', 'Xiangyuan Ru', 'Ningyu Zhang', 'Xiang Chen', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen'], 'affiliations': ['Alibaba Group', 'Nanjing University of Aeronautics and Astronautics', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03553.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#agents', '#agi'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ KnowSelf. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ ÑĞ¸Ñ‚ÑƒĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. KnowSelf Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering LLMs with Self-Aware Decision Making', 'desc': "This paper introduces a new approach called agentic knowledgeable self-awareness for Large Language Models (LLMs) in planning tasks. Unlike traditional methods that flood models with external information, this approach emphasizes the importance of situational awareness, allowing agents to assess their environment and use knowledge more effectively. The proposed method, KnowSelf, utilizes a heuristic to identify key moments in an agent's learning process, enabling it to adapt its strategies based on the situation. Experimental results show that KnowSelf significantly improves performance on various tasks while minimizing reliance on external knowledge."}, 'zh': {'title': 'è‡ªä¸»è°ƒèŠ‚çŸ¥è¯†ä½¿ç”¨çš„æ™ºèƒ½ä»£ç†', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»£ç†è§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ä»£ç†è§„åˆ’æ–¹æ³•é‡‡ç”¨äº†"æ´ªæ°´çŒæº‰"çš„æ–¹å¼ï¼Œéšæ„æ³¨å…¥é»„é‡‘è½¨è¿¹ã€å¤–éƒ¨åé¦ˆå’Œé¢†åŸŸçŸ¥è¯†ï¼Œè¿™ç§åšæ³•å¿½è§†äº†äººç±»åœ¨å†³ç­–è¿‡ç¨‹ä¸­åŠ¨æ€è¯„ä¼°æƒ…å¢ƒéœ€æ±‚çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä»£ç†çŸ¥è¯†è‡ªæˆ‘æ„è¯†çš„æ¦‚å¿µï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½¿åŸºäºLLMçš„ä»£ç†èƒ½å¤Ÿè‡ªä¸»è°ƒèŠ‚çŸ¥è¯†çš„ä½¿ç”¨ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¯å‘å¼æƒ…å¢ƒåˆ¤æ–­æ ‡å‡†ï¼Œé€šè¿‡æ ‡è®°ä»£ç†è‡ªæˆ‘æ¢ç´¢è½¨è¿¹ä¸Šçš„ç‰¹æ®Šæ ‡è®°ï¼Œæ”¶é›†è®­ç»ƒæ•°æ®ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„è§„åˆ’æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02807', 'title': 'MegaMath: Pushing the Limits of Open Math Corpora', 'url': 'https://huggingface.co/papers/2504.02807', 'abstract': 'Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.', 'score': 19, 'issue_id': 3102, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'a6dd17864afc6dca', 'authors': ['Fan Zhou', 'Zengzhi Wang', 'Nikhil Ranjan', 'Zhoujun Cheng', 'Liping Tang', 'Guowei He', 'Zhengzhong Liu', 'Eric P. Xing'], 'affiliations': ['MBZUAI', 'MegaMath'], 'pdf_title_img': 'assets/pdf/title_img/2504.02807.jpg', 'data': {'categories': ['#open_source', '#dataset', '#reasoning', '#synthetic', '#data'], 'emoji': 'ğŸ§®', 'ru': {'title': 'MegaMath: Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MegaMath - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ´ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 371 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼. MegaMath Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'MegaMath: Elevating LLMs with a Massive Math Dataset', 'desc': "This paper introduces MegaMath, a comprehensive dataset designed to enhance the mathematical reasoning capabilities of large language models (LLMs). The dataset is created by extracting and optimizing mathematical documents from the web, ensuring high quality through filtering and deduplication. Additionally, it incorporates high-quality math-related code from existing code corpora, further enriching the dataset's diversity. By synthesizing various forms of data, MegaMath provides a substantial resource of 371 billion tokens, making it the largest and highest quality open dataset for math pre-training available."}, 'zh': {'title': 'MegaMathï¼šæ•°å­¦æ¨ç†çš„å¼€æ”¾æ•°æ®é›†', 'desc': 'æ•°å­¦æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„åŸºçŸ³ï¼Œä¹Ÿæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é«˜çº§èƒ½åŠ›çš„é‡è¦åŸºå‡†ã€‚ç„¶è€Œï¼Œç›®å‰ç ”ç©¶ç•Œç¼ºä¹ä¸€ä¸ªå¼€æ”¾çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°å­¦æ•°æ®é›†ï¼Œä»¥æ»¡è¶³æ•°å­¦ä¸­å¿ƒçš„LLMé¢„è®­ç»ƒéœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†MegaMathï¼Œè¿™æ˜¯ä¸€ä¸ªä»å¤šç§æ•°å­¦ç›¸å…³æ¥æºç²¾å¿ƒç­–åˆ’çš„å¼€æ”¾æ•°æ®é›†ï¼ŒåŒ…å«3710äº¿ä¸ªæ ‡è®°ï¼Œå…·æœ‰ç°æœ‰å¼€æ”¾æ•°å­¦é¢„è®­ç»ƒæ•°æ®é›†ä¸­æœ€å¤§çš„æ•°é‡å’Œæœ€ä½³è´¨é‡ã€‚è¯¥æ•°æ®é›†é€šè¿‡é‡æ–°æå–ç½‘ç»œæ•°æ®ã€å›æ”¶æ•°å­¦ç›¸å…³ä»£ç æ•°æ®å’Œæ¢ç´¢åˆæˆæ•°æ®ç­‰ç­–ç•¥ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’Œé«˜è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03561', 'title': 'SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement', 'url': 'https://huggingface.co/papers/2504.03561', 'abstract': 'In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.', 'score': 14, 'issue_id': 3096, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '469f9f28c32e1a1a', 'authors': ['Runnan Fang', 'Xiaobin Wang', 'Yuan Liang', 'Shuofei Qiao', 'Jialong Wu', 'Zekun Xi', 'Ningyu Zhang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen'], 'affiliations': ['Alibaba Group', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03561.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#agents', '#rl'], 'emoji': 'ğŸŒ', 'ru': {'title': 'SynWorld: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…', 'desc': 'SynWorld - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ (MCTS) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SynWorld ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ² Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸Ğ»Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Agents to Explore with SynWorld', 'desc': "This paper introduces SynWorld, a framework designed to help agents improve their capabilities in unfamiliar environments. It allows agents to create and evaluate different scenarios by using multi-step actions and Monte Carlo Tree Search (MCTS) for exploration. By synthesizing possible actions, agents can better understand how to navigate and optimize their workflows. The results show that SynWorld effectively enhances agents' action knowledge in new settings, making it a valuable tool for autonomous exploration."}, 'zh': {'title': 'SynWorldï¼šèµ‹èƒ½ä»£ç†æ¢ç´¢æ–°ç¯å¢ƒçš„æ¡†æ¶', 'desc': 'åœ¨ä»£ç†ä¸ç¯å¢ƒçš„äº’åŠ¨ä¸­ï¼Œä»£ç†é€šè¿‡è§„åˆ’å’Œæ‰§è¡ŒåŠ¨ä½œæ¥æ‰©å±•å…¶èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨æ–°ç¯å¢ƒä¸­æˆ–éœ€è¦åœ¨éå¸¸è§„åŠ¨ä½œç©ºé—´ä¸­å¯¼èˆªæ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SynWorldæ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿåˆæˆå¯èƒ½çš„åœºæ™¯ï¼Œå¹¶åœ¨åŠ¨ä½œç©ºé—´å†…è¿›è¡Œå¤šæ­¥åŠ¨ä½œè°ƒç”¨ï¼ŒåŒæ—¶æ‰§è¡Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¢ç´¢ï¼Œä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–å…¶åœ¨å½“å‰ç¯å¢ƒä¸­çš„åŠ¨ä½œçŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynWorldæ˜¯å­¦ä¹ æ–°ç¯å¢ƒä¸­åŠ¨ä½œçŸ¥è¯†çš„æœ‰æ•ˆä¸”é€šç”¨çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03641', 'title': 'MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models', 'url': 'https://huggingface.co/papers/2504.03641', 'abstract': 'Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies." 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.', 'score': 10, 'issue_id': 3095, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '45da77ffd9c21caf', 'authors': ['Wulin Xie', 'Yi-Fan Zhang', 'Chaoyou Fu', 'Yang Shi', 'Bingyan Nie', 'Hongkai Chen', 'Zhang Zhang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['CASIA', 'M-M-E Project', 'NJU', 'PKU', 'Vivo'], 'pdf_title_img': 'assets/pdf/title_img/2504.03641.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (U-MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 12 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… U-MLLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… U-MLLM, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¾ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Enhancing Evaluation for Unified Multimodal Language Models', 'desc': 'This paper addresses the challenges in evaluating Unified Multimodal Language Models (U-MLLMs) due to inconsistent benchmarks and the lack of assessments for mixed-modality tasks. It introduces a comprehensive evaluation framework that includes standardized traditional task evaluations across multiple datasets and novel tasks that test multimodal reasoning capabilities. The framework assesses 12 leading U-MLLMs, revealing significant performance gaps and underscoring the necessity for improved models that can effectively manage mixed-modality tasks. The findings aim to enhance the evaluation process and guide future developments in U-MLLMs.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¿…è¦æ€§', 'desc': 'ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆU-MLLMï¼‰åŸºå‡†åœ¨è¯„ä¼°æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºä¹æ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡åŸºå‡†å’Œæ··åˆæ¨¡æ€ç”Ÿæˆçš„åŸºå‡†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°è¯„ä¼°U-MLLMã€‚è¯¥åŸºå‡†åŒ…æ‹¬æ ‡å‡†åŒ–çš„ä¼ ç»Ÿä»»åŠ¡è¯„ä¼°å’Œäº”ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œå¦‚å›¾åƒç¼–è¾‘å’Œå¸¸è¯†é—®ç­”ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ç°æœ‰U-MLLMåœ¨æ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†å¼€å‘æ›´å¼ºå¤§æ¨¡å‹çš„å¿…è¦æ€§ï¼Œä»¥æœ‰æ•ˆå¤„ç†æ··åˆæ¨¡æ€ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03601', 'title': 'APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay', 'url': 'https://huggingface.co/papers/2504.03601', 'abstract': 'Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io', 'score': 9, 'issue_id': 3097, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '05921cbfa42a13b4', 'authors': ['Akshara Prabhakar', 'Zuxin Liu', 'Weiran Yao', 'Jianguo Zhang', 'Ming Zhu', 'Shiyu Wang', 'Zhiwei Liu', 'Tulika Awalgaonkar', 'Haolin Chen', 'Thai Hoang', 'Juan Carlos Niebles', 'Shelby Heinecke', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.03601.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#agents', '#open_source', '#data', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'APIGen-MT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ²ÑŒÑĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ xLAM-2-fc-r, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº GPT-4 Ğ¸ Claude 3.5. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Generating High-Quality Data for AI Agents with APIGen-MT', 'desc': 'The paper presents APIGen-MT, a framework designed to generate high-quality multi-turn interaction data for training AI agents. It consists of two phases: first, creating detailed task blueprints with accurate actions using a committee of large language model (LLM) reviewers and feedback loops. In the second phase, these blueprints are turned into full interaction sequences through simulated human-agent interactions. The resulting models, particularly the xLAM-2-fc-r series, show superior performance on benchmark tests, especially in multi-turn scenarios, and the authors provide open access to the generated data and models to support further research.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆå¤šè½®äº¤äº’æ•°æ®çš„AIä»£ç†è®­ç»ƒæ¡†æ¶', 'desc': 'ä¸ºäº†è®­ç»ƒæœ‰æ•ˆçš„AIä»£ç†è¿›è¡Œå¤šè½®äº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†APIGen-MTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¯éªŒè¯å’Œå¤šæ ·åŒ–çš„å¤šè½®ä»£ç†æ•°æ®ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œé¦–å…ˆé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„å®¡å§”å‘˜ä¼šå’Œè¿­ä»£åé¦ˆç”Ÿæˆè¯¦ç»†çš„ä»»åŠ¡è“å›¾ï¼Œå¹¶æä¾›çœŸå®çš„è¡ŒåŠ¨ã€‚æ¥ç€ï¼Œè¿™äº›è“å›¾è¢«è½¬åŒ–ä¸ºå®Œæ•´çš„äº¤äº’è½¨è¿¹ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººæœºäº’åŠ¨å®ç°ã€‚æˆ‘ä»¬çš„xLAM-2-fc-rç³»åˆ—æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè½®è®¾ç½®ä¸­ï¼Œå°æ¨¡å‹çš„è¡¨ç°è¶…è¿‡äº†å¤§æ¨¡å‹ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02949', 'title': 'VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.02949', 'abstract': 'In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.', 'score': 9, 'issue_id': 3095, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '71423989b2bed2d8', 'authors': ['Xianwei Zhuang', 'Yuxin Xie', 'Yufan Deng', 'Dongchao Yang', 'Liming Liang', 'Jinghan Ru', 'Yuguo Yin', 'Yuexian Zou'], 'affiliations': ['School of Electronic and Computer Engineering, Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.02949.jpg', 'data': {'categories': ['#training', '#multimodal', '#rlhf', '#open_source', '#cv', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'VARGPT-v1.1 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ VARGPT. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Direct Preference Optimization (DPO). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 8,3 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2 Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹. VARGPT-v1.1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Unifying Visual Understanding and Generation with VARGPT-v1.1', 'desc': 'VARGPT-v1.1 is a cutting-edge visual autoregressive model that enhances its predecessor by integrating advanced training techniques and a larger dataset. It employs a unique combination of visual instruction tuning and reinforcement learning to improve its performance in understanding and generating images. The model also features an upgraded backbone and higher image resolution, allowing for better quality outputs and new image editing capabilities. Overall, VARGPT-v1.1 demonstrates the effectiveness of unified models in handling multimodal tasks, showcasing significant advancements in both comprehension and generation.'}, 'zh': {'title': 'ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹çš„çªç ´æ€§è¿›å±•', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†VARGPT-v1.1ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„ç»Ÿä¸€è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ŒåŸºäºæˆ‘ä»¬ä¹‹å‰çš„VARGPTæ¡†æ¶ã€‚è¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰ç†è§£çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œå›¾åƒåˆæˆçš„ä¸‹ä¸€ä¸ªå°ºåº¦ç”Ÿæˆçš„åŒé‡èŒƒå¼ã€‚VARGPT-v1.1é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†è¿­ä»£è§†è§‰æŒ‡ä»¤è°ƒä¼˜å’Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡è¿™äº›æ”¹è¿›ï¼ŒVARGPT-v1.1åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨ç†è§£å’Œç”ŸæˆæŒ‡æ ‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24067', 'title': 'TransMamba: Flexibly Switching between Transformer and Mamba', 'url': 'https://huggingface.co/papers/2503.24067', 'abstract': 'Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, a novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling.', 'score': 8, 'issue_id': 3100, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'c397bc55eaf9dd26', 'authors': ['Yixing Li', 'Ruobing Xie', 'Zhen Yang', 'Xingwu Sun', 'Shuaipeng Li', 'Weidong Han', 'Zhanhui Kang', 'Yu Cheng', 'Chengzhong Xu', 'Di Wang', 'Jie Jiang'], 'affiliations': ['Tencent Hunyuan', 'The Chinese University of Hong Kong', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.24067.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#long_context'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Transformer Ğ¸ Mamba Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'TransMamba - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Transformer Ğ¸ Mamba Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ»Ğ¾ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ĞµÑ€ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ Ñ SSM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ TransMamba Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'TransMamba: Bridging Transformers and State Space Models for Efficient Sequence Processing', 'desc': 'This paper introduces TransMamba, a new framework that combines the strengths of Transformers and Mamba, a state space model, to improve efficiency in processing long sequences. By using shared parameter matrices, TransMamba can switch between attention mechanisms and state space models based on the length of the input tokens. The Memory converter is designed to ensure smooth transitions between these two methods, allowing for effective information flow. Experimental results show that TransMamba outperforms existing models in both training efficiency and performance, making it a promising solution for future sequence modeling tasks.'}, 'zh': {'title': 'TransMambaï¼šé«˜æ•ˆçš„åºåˆ—å»ºæ¨¡æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶TransMambaï¼Œæ—¨åœ¨ç»“åˆTransformerå’ŒMambaæ¨¡å‹ï¼Œä»¥æé«˜é•¿åºåˆ—å¤„ç†çš„æ•ˆç‡ã€‚é€šè¿‡å…±äº«å‚æ•°çŸ©é˜µï¼ŒTransMambaèƒ½å¤Ÿåœ¨ä¸åŒçš„tokené•¿åº¦å’Œå±‚æ¬¡ä¹‹é—´åŠ¨æ€åˆ‡æ¢æ³¨æ„åŠ›æœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ã€‚æˆ‘ä»¬è®¾è®¡äº†è®°å¿†è½¬æ¢å™¨ï¼Œå°†æ³¨æ„åŠ›è¾“å‡ºè½¬æ¢ä¸ºSSMå…¼å®¹çš„çŠ¶æ€ï¼Œç¡®ä¿ä¿¡æ¯åœ¨è½¬æ¢ç‚¹çš„æ— ç¼æµåŠ¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ·±å…¥æ¢è®¨äº†TransPointè°ƒåº¦ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03011', 'title': 'Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization', 'url': 'https://huggingface.co/papers/2504.03011', 'abstract': 'This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods.', 'score': 7, 'issue_id': 3096, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '94d3411c37993837', 'authors': ['Junying Wang', 'Jingyuan Liu', 'Xin Sun', 'Krishna Kumar Singh', 'Zhixin Shu', 'He Zhang', 'Jimei Yang', 'Nanxuan Zhao', 'Tuanfeng Y. Wang', 'Simon S. Chen', 'Ulrich Neumann', 'Jae Shin Yoon'], 'affiliations': ['Adobe Research', 'Runway', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2504.03011.jpg', 'data': {'categories': ['#inference', '#video', '#cv', '#diffusion'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Comprehensive Relighting - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‡Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ñ‚ĞµĞ»Ğ° Ğ² Ğ»ÑĞ±Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ»Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ğ½Ğ°. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¿ĞµÑ€ĞµĞ»Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Lighting Control in Images and Videos', 'desc': 'This paper presents Comprehensive Relighting, a novel method that allows for flexible control and harmonization of lighting in images or videos featuring humans. The challenge lies in the limited datasets available, which typically restrict existing models to specific scenarios like faces or static poses. To overcome this, the authors utilize a pre-trained diffusion model to create a unified approach that addresses both human relighting and background harmonization. Additionally, they introduce an unsupervised temporal lighting model that ensures consistent lighting across frames, enhancing the overall quality and realism of the relit images.'}, 'zh': {'title': 'å…¨é¢é‡å…‰ç…§ï¼šäººç±»å›¾åƒå…‰ç…§çš„å…¨èƒ½è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†å…¨é¢é‡å…‰ç…§ï¼ˆComprehensive Relightingï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿæ§åˆ¶å’Œåè°ƒæ¥è‡ªä»»æ„åœºæ™¯ä¸­äººç±»å›¾åƒæˆ–è§†é¢‘çš„å…‰ç…§çš„å…¨èƒ½æ–¹æ³•ã€‚æ„å»ºè¿™æ ·ä¸€ä¸ªé€šç”¨æ¨¡å‹éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¼ºä¹æ•°æ®é›†ï¼Œé™åˆ¶äº†ç°æœ‰åŸºäºå›¾åƒçš„é‡å…‰ç…§æ¨¡å‹åªèƒ½åº”ç”¨äºç‰¹å®šåœºæ™¯ï¼ˆä¾‹å¦‚ï¼Œé¢éƒ¨æˆ–é™æ€äººç±»ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é‡æ–°åˆ©ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºé€šç”¨å›¾åƒå…ˆéªŒï¼Œå¹¶åœ¨ç²—åˆ°ç»†çš„æ¡†æ¶ä¸­è”åˆå»ºæ¨¡äººç±»é‡å…‰ç…§å’ŒèƒŒæ™¯åè°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…¨é¢é‡å…‰ç…§åœ¨é€šç”¨æ€§å’Œå…‰ç…§æ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºäºå›¾åƒçš„äººç±»é‡å…‰ç…§å’Œåè°ƒæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03536', 'title': 'HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration', 'url': 'https://huggingface.co/papers/2504.03536', 'abstract': 'Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce HumanDreamer-X, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, HumanFixer is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.', 'score': 6, 'issue_id': 3097, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': '9a6ad8e0086d88eb', 'authors': ['Boyuan Wang', 'Runqi Ouyang', 'Xiaofeng Wang', 'Zheng Zhu', 'Guosheng Zhao', 'Chaojun Ni', 'Guan Huang', 'Lihong Liu', 'Xingang Wang'], 'affiliations': ['GigaAI', 'Institute of Automation, Chinese Academy of Sciences', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03536.jpg', 'data': {'categories': ['#synthetic', '#cv', '#3d'], 'emoji': 'ğŸ§‘\u200dğŸ¦°', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾', 'desc': 'HumanDreamer-X - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Gaussian Splatting Ğ´Ğ»Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ HumanFixer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Human Reconstruction with HumanDreamer-X', 'desc': 'This paper presents HumanDreamer-X, a new framework for single-image human reconstruction that combines multi-view generation and 3D reconstruction into one process. The framework addresses common issues like geometric inconsistencies and blurred limbs by using 3D Gaussian Splatting for better initial geometry and appearance. Additionally, it includes a component called HumanFixer, which enhances the photorealism of the 3D models. The authors also introduce an attention modulation strategy to improve detail consistency across different views, resulting in significant improvements in image quality metrics.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šè§†å›¾ç”Ÿæˆä¸é‡å»ºï¼Œæå‡äººç±»æ¨¡å‹è´¨é‡', 'desc': 'å•å›¾åƒäººç±»é‡å»ºå¯¹æ•°å­—äººç±»å»ºæ¨¡åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†ä»ç„¶æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç›®å‰çš„æ–¹æ³•ä¾èµ–ç”Ÿæˆæ¨¡å‹åˆæˆå¤šè§†å›¾å›¾åƒä»¥è¿›è¡Œåç»­çš„3Dé‡å»ºå’ŒåŠ¨ç”»ã€‚ç„¶è€Œï¼Œä»å•ä¸ªäººä½“å›¾åƒç›´æ¥ç”Ÿæˆå¤šä¸ªè§†å›¾ä¼šå¯¼è‡´å‡ ä½•ä¸ä¸€è‡´ï¼Œé‡å»ºæ¨¡å‹ä¸­å‡ºç°è‚¢ä½“ç¢ç‰‡æˆ–æ¨¡ç³Šçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†HumanDreamer-Xï¼Œä¸€ä¸ªå°†å¤šè§†å›¾äººç±»ç”Ÿæˆå’Œé‡å»ºæ•´åˆä¸ºç»Ÿä¸€æµç¨‹çš„æ–°æ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†é‡å»º3Dæ¨¡å‹çš„å‡ ä½•ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ„Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02402', 'title': 'EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling', 'url': 'https://huggingface.co/papers/2504.02402', 'abstract': 'When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.', 'score': 5, 'issue_id': 3099, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'ca80ca19171ef86b', 'authors': ['Hao Yin', 'Shi Guo', 'Xu Jia', 'Xudong XU', 'Lu Zhang', 'Si Liu', 'Dong Wang', 'Huchuan Lu', 'Tianfan Xue'], 'affiliations': ['Beihang University', 'Dalian University of Technology', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.02402.jpg', 'data': {'categories': ['#training', '#dataset', '#data', '#cv'], 'emoji': 'ğŸ”Š', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±ĞµÑĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµÑĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mamba Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Sound Recovery with Event Cameras', 'desc': 'This paper presents a new method for recovering sound from visual changes caused by sound-induced vibrations. It addresses limitations in previous techniques by utilizing event camera technology, which excels at capturing high-frequency signals. The authors developed a training pipeline to create a large dataset and designed a neural network that effectively captures both spatial and temporal information from the event data. Their approach includes a specialized imaging system to enhance signal detection, leading to improved sound recovery performance in experiments.'}, 'zh': {'title': 'åˆ©ç”¨äº‹ä»¶æµå®ç°é«˜æ•ˆå£°éŸ³æ¢å¤', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„éæ¥è§¦å£°éŸ³æ¢å¤æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨äº‹ä»¶æµä¸­çš„æ—¶ç©ºä¿¡æ¯ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡æ–°çš„ä»¿çœŸç®¡é“ç”Ÿæˆäº†ä¸€ä¸ªå¤§å‹è®­ç»ƒé›†ï¼Œç„¶åè®¾è®¡äº†ä¸€ä¸ªç½‘ç»œï¼Œåˆ©ç”¨äº‹ä»¶çš„ç¨€ç–æ€§æ•æ‰ç©ºé—´ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨Mambaæ¨¡å‹æ¥å¤„ç†é•¿æœŸçš„æ—¶é—´ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç©ºé—´èšåˆæ¨¡å—ï¼Œä»¥èšåˆæ¥è‡ªä¸åŒä½ç½®çš„ä¿¡æ¯ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜ä¿¡å·è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®ä¸Šéƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03600', 'title': 'MedSAM2: Segment Anything in 3D Medical Images and Videos', 'url': 'https://huggingface.co/papers/2504.03600', 'abstract': 'Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.', 'score': 3, 'issue_id': 3103, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': 'c1ef5354c6e2cdcb', 'authors': ['Jun Ma', 'Zongxin Yang', 'Sumin Kim', 'Bihui Chen', 'Mohammed Baharoon', 'Adibvafa Fallahpour', 'Reza Asakereh', 'Hongwei Lyu', 'Bo Wang'], 'affiliations': ['AI Collaborative Centre, University Health Network', 'AI Hub, University Health Network', 'Department of Biomedical Informatics, Harvard Medical School, Harvard University, Boston, USA', 'Department of Computer Science, University of Toronto', 'Department of Laboratory Medicine and Pathobiology and Department of Computer Science, University of Toronto', 'Peter Munk Cardiac Centre, University Health Network', 'University of Toronto, Toronto, Canada', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2504.03600.jpg', 'data': {'categories': ['#data', '#healthcare', '#3d', '#dataset', '#training', '#cv'], 'emoji': '\U0001fa7b', 'ru': {'title': 'MedSAM2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'MedSAM2 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° fine-tuning Segment Anything Model 2. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 455 000 Ğ¿Ğ°Ñ€ 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ 76 000 ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ñƒ ÑĞ¿ĞµĞºÑ‚Ñ€Ñƒ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ², Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ human-in-the-loop Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 85%. MedSAM2 Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ñ ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'MedSAM2: Revolutionizing 3D Medical Segmentation', 'desc': 'This paper introduces MedSAM2, a new model designed for segmenting 3D medical images and videos, which is essential for precision medicine. It builds on the Segment Anything Model 2 and has been fine-tuned using a vast dataset of over 455,000 3D image-mask pairs and 76,000 video frames. MedSAM2 outperforms existing models in segmenting various organs and lesions, while also significantly reducing manual annotation costs by over 85% through a human-in-the-loop approach. Additionally, it is user-friendly and can be deployed on both local and cloud platforms, making it accessible for research and healthcare applications.'}, 'zh': {'title': 'MedSAM2ï¼šé«˜æ•ˆçš„3DåŒ»å­¦å›¾åƒåˆ†å‰²å·¥å…·', 'desc': 'MedSAM2æ˜¯ä¸€ç§ç”¨äº3DåŒ»å­¦å›¾åƒå’Œè§†é¢‘åˆ†å‰²çš„å¯æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡455,000ä¸ª3Då›¾åƒ-æ©è†œå¯¹å’Œ76,000å¸§çš„å¤§å‹åŒ»å­¦æ•°æ®é›†ä¸Šå¾®è°ƒSegment Anything Model 2è€Œå¼€å‘ã€‚MedSAM2åœ¨å¤šä¸ªå™¨å®˜ã€ç—…å˜å’Œæˆåƒæ¨¡å¼ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ï¼Œå¹¶é€šè¿‡äººæœºåä½œçš„æµç¨‹åˆ›å»ºäº†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿›è¡Œäº†ä¸€é¡¹å¹¿æ³›çš„ç”¨æˆ·ç ”ç©¶ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå°†äººå·¥æˆæœ¬é™ä½è¶…è¿‡85%ï¼Œå¹¶ä¸”å·²é›†æˆåˆ°å¹¿æ³›ä½¿ç”¨çš„å¹³å°ä¸­ï¼Œä¾¿äºæœ¬åœ°å’Œäº‘ç«¯éƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.03597', 'title': 'Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin\n  for Real-World Robot Policy Evaluation', 'url': 'https://huggingface.co/papers/2504.03597', 'abstract': "Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com.", 'score': 3, 'issue_id': 3106, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 4', 'zh': '4æœˆ4æ—¥'}, 'hash': 'd1e68cb65f756f3e', 'authors': ['Jad Abou-Chakra', 'Lingfeng Sun', 'Krishan Rana', 'Brandon May', 'Karl Schmeckpeper', 'Maria Vittoria Minniti', 'Laura Herlant'], 'affiliations': ['Queensland University of Technology', 'Robotics and AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.03597.jpg', 'data': {'categories': ['#agents', '#training', '#optimization', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ real-is-sim. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Embodied Gaussians Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. Real-is-sim Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ PushT.'}, 'en': {'title': 'Bridging the Gap: Real-World Success through Simulated Training', 'desc': 'This paper introduces real-is-sim, a new behavior cloning framework designed to improve the training and evaluation of robots performing manipulation tasks. It utilizes a dynamic digital twin that aligns a simulated environment with the real world, allowing for better data collection and training processes. By enabling offline evaluation of policies in a simulator, it addresses the challenges of overfitting and underfitting while reducing reliance on costly real-world testing. The framework shows strong correlation between simulated success rates and actual performance, enhancing the efficiency of robot training.'}, 'zh': {'title': 'åŠ¨æ€æ•°å­—åŒèƒèƒæå‡è¡Œä¸ºå…‹éš†æ€§èƒ½', 'desc': 'æœ€è¿‘ï¼Œè¡Œä¸ºå…‹éš†æŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ã€‚ç„¶è€Œï¼Œå‡†ç¡®è¯„ä¼°è®­ç»ƒæ€§èƒ½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå› ä¸ºè¡Œä¸ºå…‹éš†æŸå¤±ä¸å®é™…ä»»åŠ¡æˆåŠŸç‡çš„ç›¸å…³æ€§è¾ƒå·®ã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜ä¸å¾—ä¸ä¾èµ–äºæ˜‚è´µä¸”è€—æ—¶çš„å®é™…è¯„ä¼°æ¥è·å–æˆåŠŸç‡æŒ‡æ ‡ï¼Œè¿™ä½¿å¾—è¯†åˆ«æœ€ä½³ç­–ç•¥å’Œæ£€æµ‹è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆå˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†real-is-simï¼Œä¸€ä¸ªæ–°é¢–çš„è¡Œä¸ºå…‹éš†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æ•°å­—åŒèƒèƒåœ¨æ•´ä¸ªç­–ç•¥å¼€å‘æµç¨‹ä¸­è¿›è¡Œæ•°æ®æ”¶é›†ã€è®­ç»ƒå’Œéƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02534', 'title': 'Delineate Anything: Resolution-Agnostic Field Boundary Delineation on\n  Satellite Imagery', 'url': 'https://huggingface.co/papers/2504.02534', 'abstract': 'The accurate delineation of agricultural field boundaries from satellite imagery is vital for land management and crop monitoring. However, current methods face challenges due to limited dataset sizes, resolution discrepancies, and diverse environmental conditions. We address this by reformulating the task as instance segmentation and introducing the Field Boundary Instance Segmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset comprising 672,909 high-resolution satellite image patches (ranging from 0.25 m to 10 m) and 22,926,427 instance masks of individual fields, significantly narrowing the gap between agricultural datasets and those in other computer vision domains. We further propose Delineate Anything, an instance segmentation model trained on our new FBIS-22M dataset. Our proposed model sets a new state-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and 103% in mAP@0.5:0.95 over existing methods, while also demonstrating significantly faster inference and strong zero-shot generalization across diverse image resolutions and unseen geographic regions. Code, pre-trained models, and the FBIS-22M dataset are available at https://lavreniuk.github.io/Delineate-Anything.', 'score': 3, 'issue_id': 3107, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '18e13d7f115ab27c', 'authors': ['Mykola Lavreniuk', 'Nataliia Kussul', 'Andrii Shelestov', 'Bohdan Yailymov', 'Yevhenii Salii', 'Volodymyr Kuzin', 'Zoltan Szantoi'], 'affiliations': ['European Space Agency', 'National Technical University of Ukraine Igor Sikorsky Kyiv Polytechnic Institute', 'Space Research Institute NASU-SSAU', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.02534.jpg', 'data': {'categories': ['#dataset', '#cv', '#inference', '#open_source', '#benchmark', '#optimization'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞµĞ»ÑŒÑ…Ğ¾Ğ·ÑƒĞ³Ğ¾Ğ´Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼ Ğ¿Ğ¾Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞµĞ»ÑŒÑĞºĞ¾Ñ…Ğ¾Ğ·ÑĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…, Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FBIS-22M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 22 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¼Ğ°ÑĞ¾Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ½Ğ° ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Delineate Anything, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing Agricultural Field Boundary Detection with FBIS-22M', 'desc': 'This paper focuses on improving the identification of agricultural field boundaries using satellite images, which is crucial for effective land management. The authors introduce a new dataset called Field Boundary Instance Segmentation - 22M (FBIS-22M), which contains a large number of high-resolution satellite image patches and detailed instance masks for individual fields. They reformulate the problem as instance segmentation and develop a model named Delineate Anything, which is trained on this extensive dataset. The model achieves state-of-the-art performance, showing significant improvements in mean Average Precision (mAP) and demonstrating fast inference and strong generalization capabilities across various conditions.'}, 'zh': {'title': 'ç²¾å‡†è¯†åˆ«å†œä¸šè¾¹ç•Œï¼ŒåŠ©åŠ›åœŸåœ°ç®¡ç†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å†œä¸šé¢†åŸŸè¾¹ç•Œè¯†åˆ«æ–¹æ³•ï¼Œåˆ©ç”¨å«æ˜Ÿå›¾åƒè¿›è¡ŒåœŸåœ°ç®¡ç†å’Œä½œç‰©ç›‘æµ‹ã€‚æˆ‘ä»¬é€šè¿‡å®ä¾‹åˆ†å‰²çš„æ–¹å¼é‡æ–°å®šä¹‰äº†è¿™ä¸€ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†FBIS-22Mæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«672,909ä¸ªé«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒç‰‡æ®µå’Œ22,926,427ä¸ªå®ä¾‹æ©è†œã€‚æˆ‘ä»¬çš„æ¨¡å‹"Delineate Anything"åœ¨FBIS-22Mæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œåœ¨mAP@0.5ä¸Šæé«˜äº†88.5%ï¼Œå¹¶åœ¨ä¸åŒå›¾åƒåˆ†è¾¨ç‡å’Œæœªè§åœ°ç†åŒºåŸŸä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24310', 'title': 'BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models', 'url': 'https://huggingface.co/papers/2503.24310', 'abstract': 'In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.', 'score': 3, 'issue_id': 3097, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '890bba46601fef07', 'authors': ['Alok Abhishek', 'Lisa Erickson', 'Tushar Bandopadhyay'], 'affiliations': ['Boston, USA', 'San Francisco, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.24310.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#ethics'], 'emoji': 'âš–ï¸', 'ru': {'title': 'BEATS: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ BEATS - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸, ÑÑ‚Ğ¸ĞºĞ¸, ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ BEATS Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ¿Ğ¾ 29 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ 37,65% Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ»Ğ¸ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸. BEATS Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM, Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ğ¾ ĞµĞµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'BEATS: A Framework for Fair and Ethical AI Evaluation', 'desc': 'This paper presents BEATS, a new framework designed to evaluate Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). It introduces a comprehensive bias benchmark that assesses LLM performance using 29 different metrics, covering various biases and ethical considerations. The framework aims to quantitatively measure how LLM outputs may reinforce societal prejudices and systemic inequities. The findings reveal that a significant portion of outputs from leading models exhibit bias, underscoring the need for responsible AI practices and the potential for BEATS to guide improvements in AI ethics.'}, 'zh': {'title': 'BEATSæ¡†æ¶ï¼šæ¨åŠ¨è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½è¯„ä¼°', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†BEATSæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åè§ã€ä¼¦ç†ã€å…¬å¹³æ€§å’Œäº‹å®æ€§ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåè§åŸºå‡†ï¼Œæ¶µç›–29ä¸ªä¸åŒçš„æŒ‡æ ‡ï¼Œè¯„ä¼°LLMsåœ¨å¤šæ ·æ€§ã€è®¤çŸ¥å’Œç¤¾ä¼šåè§ç­‰æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡è¿™äº›æŒ‡æ ‡ï¼Œå¯ä»¥å®šé‡è¯„ä¼°LLMç”Ÿæˆçš„å“åº”åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¯èƒ½å»¶ç»­ç¤¾ä¼šåè§ï¼Œå¼ºåŒ–æˆ–æ‰©å¤§ç³»ç»Ÿæ€§ä¸å¹³ç­‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡BEATSæ¡†æ¶ï¼Œä¿ƒè¿›æ›´å…·ç¤¾ä¼šè´£ä»»æ„Ÿå’Œä¼¦ç†å¯¹é½çš„äººå·¥æ™ºèƒ½æ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22738', 'title': 'ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning', 'url': 'https://huggingface.co/papers/2503.22738', 'abstract': 'Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents.', 'score': 2, 'issue_id': 3111, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'b873783891272a3b', 'authors': ['Zhaorun Chen', 'Mintong Kang', 'Bo Li'], 'affiliations': ['University of Chicago, Chicago IL, USA', 'University of Illinois at Urbana-Champaign, Champaign IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.22738.jpg', 'data': {'categories': ['#agents', '#security', '#reasoning', '#inference', '#benchmark', '#dataset'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ShieldAgent: Ğ Ğ°Ğ·ÑƒĞ¼Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ShieldAgent - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚-Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ğ¸Ğº, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ShieldAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ShieldAgent Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 11,3% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ 90,1%.'}, 'en': {'title': 'Shielding Agents: Ensuring Safety with ShieldAgent', 'desc': 'This paper introduces ShieldAgent, a novel guardrail agent designed to enhance the safety of autonomous agents powered by foundation models. ShieldAgent addresses vulnerabilities to malicious instructions by enforcing compliance with safety policies through logical reasoning. It constructs a safety policy model from existing documents and generates shielding plans based on action trajectories of protected agents. The paper also presents ShieldAgent-Bench, a benchmark dataset for evaluating safety in agent instructions, demonstrating that ShieldAgent significantly outperforms existing methods in both accuracy and efficiency.'}, 'zh': {'title': 'ShieldAgentï¼šè‡ªä¸»ä»£ç†çš„å®‰å…¨é˜²æŠ¤å…ˆé”‹', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ShieldAgentï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºè‡ªä¸»ä»£ç†è®¾è®¡çš„å®‰å…¨é˜²æŠ¤ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡é€»è¾‘æ¨ç†ç¡®ä¿å…¶ä»–å—ä¿æŠ¤ä»£ç†çš„è¡ŒåŠ¨è½¨è¿¹ç¬¦åˆæ˜ç¡®çš„å®‰å…¨æ”¿ç­–ã€‚ShieldAgenté€šè¿‡ä»æ”¿ç­–æ–‡ä»¶ä¸­æå–å¯éªŒè¯çš„è§„åˆ™ï¼Œæ„å»ºäº†ä¸€ä¸ªå®‰å…¨æ”¿ç­–æ¨¡å‹ï¼Œå¹¶å°†å…¶ç»“æ„åŒ–ä¸ºåŸºäºè¡ŒåŠ¨çš„æ¦‚ç‡è§„åˆ™ç”µè·¯ã€‚è¯¥ä»£ç†èƒ½å¤Ÿæ ¹æ®å—ä¿æŠ¤ä»£ç†çš„è¡ŒåŠ¨è½¨è¿¹æ£€ç´¢ç›¸å…³è§„åˆ™ç”µè·¯ï¼Œå¹¶ç”Ÿæˆä¿æŠ¤è®¡åˆ’ï¼Œä»è€Œå®ç°å½¢å¼éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒShieldAgentåœ¨å®‰å…¨æ€§å’Œæ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¿æŠ¤è‡ªä¸»ä»£ç†æ–¹é¢çš„é«˜ç²¾åº¦å’Œé«˜æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01328', 'title': 'Slow-Fast Architecture for Video Multi-Modal Large Language Models', 'url': 'https://huggingface.co/papers/2504.01328', 'abstract': 'Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose a novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim a video before focusing on relevant parts, our slow-fast design employs a dual-token strategy: 1) "fast" visual tokens -- a compact set of compressed video features -- are fed into the LLM alongside text embeddings to provide a quick overview; 2) "slow" visual tokens -- uncompressed video features -- are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just a 3% increase in computation, and achieving a 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is a plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability.', 'score': 1, 'issue_id': 3104, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '5272839c562d78b6', 'authors': ['Min Shi', 'Shihao Wang', 'Chieh-Yun Chen', 'Jitesh Jain', 'Kai Wang', 'Junjun Xiong', 'Guilin Liu', 'Zhiding Yu', 'Humphrey Shi'], 'affiliations': ['HKPU', 'NVIDIA', 'SHI Labs @ Georgia Tech', 'SUNY Buffalo'], 'pdf_title_img': 'assets/pdf/title_img/2504.01328.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#video', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ slow-fast Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ MLLM', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ slow-fast Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: 'Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ' Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ¸ 'Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğµ' Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ self-attention. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."}, 'en': {'title': 'Enhancing Video Understanding with Efficient Slow-Fast Architecture', 'desc': "This paper addresses the challenge of balancing temporal resolution and spatial detail in video-based multi-modal large language models (MLLMs) while managing computational limits. The authors introduce a slow-fast architecture that allows for the processing of more input frames without losing important spatial information. By using a dual-token strategy, the model incorporates both compressed 'fast' visual tokens for quick overviews and uncompressed 'slow' visual tokens for detailed analysis, enhancing instruction-aware visual extraction. Experimental results demonstrate that this approach significantly improves performance and efficiency, allowing for a greater input capacity with minimal computational cost."}, 'zh': {'title': 'æ…¢-å¿«æ¶æ„ï¼šæå‡è§†é¢‘ç†è§£çš„æ•ˆç‡ä¸æ€§èƒ½', 'desc': 'åœ¨è§†é¢‘åŸºç¡€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ï¼Œå¹³è¡¡æ—¶é—´åˆ†è¾¨ç‡å’Œç©ºé—´ç»†èŠ‚æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨è¾“å…¥åˆ°LLMä¹‹å‰ä½¿ç”¨é¢„å®šä¹‰è§„åˆ™å‹ç¼©è§†é¢‘è¡¨ç¤ºï¼Œå¯¼è‡´ä¸å¯é€†çš„ä¿¡æ¯æŸå¤±ï¼Œå¹¶ä¸”å¸¸å¸¸å¿½è§†è¾“å…¥æŒ‡ä»¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ…¢-å¿«æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™ç©ºé—´ç»†èŠ‚çš„åŒæ—¶ä½¿ç”¨æ›´å¤šçš„è¾“å…¥å¸§ã€‚æˆ‘ä»¬çš„è®¾è®¡é‡‡ç”¨åŒä»¤ç‰Œç­–ç•¥ï¼Œé€šè¿‡å¿«é€Ÿè§†è§‰ä»¤ç‰Œå’Œæ…¢é€Ÿè§†è§‰ä»¤ç‰Œçš„ç»“åˆï¼Œå®ç°äº†æŒ‡ä»¤æ„ŸçŸ¥çš„ç›¸å…³è§†è§‰ç»†èŠ‚æå–ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00396', 'title': 'SPF-Portrait: Towards Pure Portrait Customization with Semantic\n  Pollution-Free Fine-tuning', 'url': 'https://huggingface.co/papers/2504.00396', 'abstract': "Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental learning while customizing target attributes. To address this issue, we propose SPF-Portrait, a pioneering work to purely understand customized semantics while eliminating semantic pollution in text-driven portrait customization. In our SPF-Portrait, we propose a dual-path pipeline that introduces the original model as a reference for the conventional fine-tuning path. Through contrastive learning, we ensure adaptation to target attributes and purposefully align other unrelated attributes with the original portrait. We introduce a novel Semantic-Aware Fine Control Map, which represents the precise response regions of the target semantics, to spatially guide the alignment process between the contrastive paths. This alignment process not only effectively preserves the performance of the original model but also avoids over-alignment. Furthermore, we propose a novel response enhancement mechanism to reinforce the performance of target attributes, while mitigating representation discrepancy inherent in direct cross-modal supervision. Extensive experiments demonstrate that SPF-Portrait achieves state-of-the-art performance. Project webpage: https://spf-portrait.github.io/SPF-Portrait/", 'score': 1, 'issue_id': 3104, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '0afcec0b90ba9cfd', 'authors': ['Xiaole Xian', 'Zhichao Liao', 'Qingyu Li', 'Wenyu Qin', 'Pengfei Wan', 'Weicheng Xie', 'Long Zeng', 'Linlin Shen', 'Pingfa Feng'], 'affiliations': ['Kuaishou Technology', 'Shenzhen University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00396.jpg', 'data': {'categories': ['#cv', '#multimodal', '#training', '#dataset'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SPF-Portrait Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ÑƒÑ‚ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Preserving Originality in Portrait Customization with SPF-Portrait', 'desc': "This paper presents SPF-Portrait, a method for fine-tuning a pre-trained Text-to-Image model specifically for customizing portrait attributes without losing the original model's capabilities. The authors address the problem of Semantic Pollution, which occurs during fine-tuning and affects the model's performance on unrelated attributes. SPF-Portrait employs a dual-path pipeline that uses contrastive learning to align target attributes while maintaining the integrity of the original model. Additionally, a Semantic-Aware Fine Control Map is introduced to guide the alignment process, ensuring effective adaptation and enhancing the performance of the desired attributes."}, 'zh': {'title': 'æ¶ˆé™¤è¯­ä¹‰æ±¡æŸ“ï¼Œå®ç°è‚–åƒå®šåˆ¶çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSPF-Portraitçš„æ–¹æ³•ï¼Œç”¨äºåœ¨å®šåˆ¶è‚–åƒå±æ€§æ—¶æ¶ˆé™¤è¯­ä¹‰æ±¡æŸ“ã€‚é€šè¿‡å¼•å…¥åŸå§‹æ¨¡å‹ä½œä¸ºå‚è€ƒï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ ç¡®ä¿ç›®æ ‡å±æ€§çš„é€‚åº”æ€§ï¼ŒåŒæ—¶ä¿æŒåŸå§‹è‚–åƒçš„å…¶ä»–æ— å…³å±æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è¯­ä¹‰æ„ŸçŸ¥ç»†æ§å›¾ï¼Œç²¾ç¡®æŒ‡å¯¼å¯¹æ¯”è·¯å¾„ä¹‹é—´çš„å¯¹é½è¿‡ç¨‹ï¼Œä»è€Œæœ‰æ•ˆä¿ç•™åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPF-Portraitåœ¨è‚–åƒå®šåˆ¶ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13837', 'title': 'Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?', 'url': 'https://huggingface.co/papers/2504.13837', 'abstract': "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@k metric with large values of k to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of k (\\eg, k=1), base models can achieve a comparable or even higher pass@k score compared to their RL counterparts at large k values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io", 'score': 53, 'issue_id': 3335, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 18', 'zh': '4æœˆ18æ—¥'}, 'hash': '2fe56493fe3aec80', 'authors': ['Yang Yue', 'Zhiqi Chen', 'Rui Lu', 'Andrew Zhao', 'Zhaokai Wang', 'Yang Yue', 'Shiji Song', 'Gao Huang'], 'affiliations': ['LeapLab, Tsinghua University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13837.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¥Ğ¾Ñ‚Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑÑ… k Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ pass@k, Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… k. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ½Ğ¾ÑÑÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ½Ğ¾ ÑÑ‚Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ RLVR Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Rethinking RLVR: Limits of Reinforcement Learning in Reasoning', 'desc': 'This paper critically evaluates the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing the reasoning capabilities of large language models (LLMs). The authors find that while RLVR improves performance at lower complexity tasks, it does not introduce fundamentally new reasoning patterns compared to base models when evaluated at higher complexity levels. Instead, RL-trained models tend to sample reasoning paths that are already present in base models, leading to a narrower range of reasoning capabilities. The study suggests that distillation may be a more effective method for introducing new knowledge into models, highlighting the limitations of RLVR in advancing LLM reasoning.'}, 'zh': {'title': 'é‡æ–°æ€è€ƒå¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä¸­çš„ä½œç”¨', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†ä¸€å®šæˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†è¿™ä¸€å‡è®¾ï¼Œå‘ç°RLVRå¹¶æœªçœŸæ­£å¼•å…¥æ–°çš„æ¨ç†æ¨¡å¼ã€‚å°½ç®¡RLè®­ç»ƒçš„æ¨¡å‹åœ¨å°çš„kå€¼ä¸‹è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†åœ¨è¾ƒå¤§çš„kå€¼ä¸‹ï¼ŒåŸºç¡€æ¨¡å‹çš„è¡¨ç°å¯ä»¥ä¸RLæ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³æ›´å¥½ã€‚è¿™è¡¨æ˜ï¼ŒRLè®­ç»ƒæ¨¡å‹çš„æ¨ç†è·¯å¾„å®é™…ä¸Šå·²ç»åŒ…å«åœ¨åŸºç¡€æ¨¡å‹çš„é‡‡æ ·åˆ†å¸ƒä¸­ï¼Œå¼ºè°ƒäº†RLVRåœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13835', 'title': 'MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space', 'url': 'https://huggingface.co/papers/2504.13835', 'abstract': 'Data quality and diversity are key to the construction of effective instruction-tuning datasets. % With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. % Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. % However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. % Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. % To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. % Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to Maximize the Information Gain (MIG) in semantic space. % Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. % Notably, the model fine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73\\% on AlpacaEval and +6.89\\% on Wildbench.', 'score': 29, 'issue_id': 3335, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 18', 'zh': '4æœˆ18æ—¥'}, 'hash': '12926d762a03519c', 'authors': ['Yicheng Chen', 'Yining Li', 'Kai Hu', 'Zerun Ma', 'Haochen Ye', 'Kai Chen'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2504.13835.jpg', 'data': {'categories': ['#data', '#open_source', '#training', '#dataset', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¼ĞµÑ‚Ğ¾Ğº. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ»Ğ¸ÑˆÑŒ 5% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Maximizing Information for Better Instruction-Tuning Datasets', 'desc': "This paper addresses the importance of data quality and diversity in creating effective instruction-tuning datasets for machine learning. It critiques existing methods that rely on heuristic rules for maintaining diversity, which often leads to suboptimal dataset selections. The authors propose a new approach that quantifies the information content of datasets by modeling the semantic space with a label graph, allowing for a more comprehensive understanding of data diversity. Their method, called Maximize the Information Gain (MIG), iteratively selects samples that enhance the dataset's information content, showing significant performance improvements in experiments compared to traditional methods."}, 'zh': {'title': 'æå‡æ•°æ®é›†è´¨é‡ä¸å¤šæ ·æ€§çš„ç»Ÿä¸€æ–¹æ³•', 'desc': 'æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§æ˜¯æ„å»ºæœ‰æ•ˆæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„å…³é”®ã€‚éšç€å¼€æºæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„å¢åŠ ï¼Œè‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„å­é›†å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¼˜å…ˆè€ƒè™‘å®ä¾‹è´¨é‡ï¼Œå¹¶ä½¿ç”¨å¯å‘å¼è§„åˆ™æ¥ç»´æŒå¤šæ ·æ€§ï¼Œä½†ç¼ºä¹å¯¹æ•´ä¸ªæ•°æ®é›†çš„å…¨é¢è§†è§’ï¼Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥é‡åŒ–æ•°æ®é›†çš„ä¿¡æ¯å†…å®¹ï¼Œå¹¶åŸºäºä¿¡æ¯åˆ†å¸ƒæ¥é‡åŒ–å¤šæ ·æ€§ï¼Œä»è€Œå¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥æœ€å¤§åŒ–è¯­ä¹‰ç©ºé—´ä¸­çš„ä¿¡æ¯å¢ç›Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11544', 'title': 'NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes', 'url': 'https://huggingface.co/papers/2504.11544', 'abstract': 'Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building a knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, we propose NodeRAG, a graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures a fully cohesive and efficient end-to-end process. Through extensive experiments, we demonstrate that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. Our GitHub repository could be seen at https://github.com/Terry-Xu-666/NodeRAG.', 'score': 21, 'issue_id': 3335, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '86dd4da356ad5ef0', 'authors': ['Tianyang Xu', 'Haojie Zheng', 'Chengze Li', 'Haoxiang Chen', 'Yixin Liu', 'Ruoxi Chen', 'Lichao Sun'], 'affiliations': ['Columbia University', 'Lehigh University', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2504.11544.jpg', 'data': {'categories': ['#games', '#graphs', '#open_source', '#rag', '#multimodal', '#optimization'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'NodeRAG: Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²', 'desc': 'NodeRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (RAG), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ RAG, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NodeRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'NodeRAG: Enhancing RAG with Smart Graph Structures', 'desc': 'This paper introduces NodeRAG, a new framework that enhances retrieval-augmented generation (RAG) by using heterogeneous graph structures. By focusing on the design of graph structures, NodeRAG improves the integration of various graph algorithms into the RAG workflow, leading to better performance. The framework aligns with the capabilities of large language models (LLMs), ensuring a smooth and efficient process for generating responses. Experimental results show that NodeRAG outperforms existing methods like GraphRAG and LightRAG in terms of indexing time, query time, storage efficiency, and question-answering accuracy.'}, 'zh': {'title': 'NodeRAGï¼šå›¾ç»“æ„åŠ©åŠ›æ£€ç´¢å¢å¼ºç”Ÿæˆ', 'desc': 'æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè®¿é—®å¤–éƒ¨å’Œç§æœ‰è¯­æ–™åº“ï¼Œä»è€Œåœ¨ç‰¹å®šé¢†åŸŸæä¾›äº‹å®ä¸€è‡´çš„å“åº”ã€‚é€šè¿‡åˆ©ç”¨è¯­æ–™åº“çš„å†…åœ¨ç»“æ„ï¼ŒåŸºäºå›¾çš„RAGæ–¹æ³•é€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•è¿›ä¸€æ­¥ä¸°å¯Œäº†è¿™ä¸€è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç›®å‰çš„åŸºäºå›¾çš„RAGæ–¹æ³•å¾ˆå°‘é‡è§†å›¾ç»“æ„çš„è®¾è®¡ã€‚ä¸ºäº†è§£æ”¾å›¾åœ¨RAGä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†NodeRAGï¼Œä¸€ä¸ªä»¥å›¾ä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œå¼•å…¥å¼‚æ„å›¾ç»“æ„ï¼Œå®ç°å›¾æ–¹æ³•ä¸RAGå·¥ä½œæµç¨‹çš„æ— ç¼æ•´åˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11833', 'title': 'Could Thinking Multilingually Empower LLM Reasoning?', 'url': 'https://huggingface.co/papers/2504.11833', 'abstract': 'Previous work indicates that large language models exhibit a significant "English bias", i.e. they often perform better when tasks are presented in English. Interestingly, we have observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. In this paper, we explore the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@k points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, we also find that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs.', 'score': 15, 'issue_id': 3335, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': '463f5ddc1d75970e', 'authors': ['Changjiang Gao', 'Xu Huang', 'Wenhao Zhu', 'Shujian Huang', 'Lei Li', 'Fei Yuan'], 'affiliations': ['Carnegie Mellon University', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2504.11833.jpg', 'data': {'categories': ['#reasoning', '#low_resource', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 10 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ². ĞĞ´Ğ½Ğ°ĞºĞ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸Ğ·-Ğ·Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ĞµĞ¹. Ğ­Ñ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Unlocking the Power of Multilingual Reasoning in LLMs', 'desc': 'This paper investigates the performance of large language models (LLMs) in reasoning tasks across multiple languages. It reveals that using certain non-English languages can lead to better outcomes than relying solely on English, highlighting a significant opportunity for multilingual reasoning. The authors suggest that multilingual approaches can achieve higher accuracy and robustness compared to English-only methods, even when translation quality varies. Additionally, they identify limitations in current answer selection methods that prevent reaching the full potential of multilingual reasoning, setting the stage for future research in this area.'}, 'zh': {'title': 'å¤šè¯­è¨€æ¨ç†çš„æ½œåŠ›è¶…è¶Šè‹±è¯­', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œå‘ç°æŸäº›è¯­è¨€åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºè‹±è¯­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šè¯­è¨€æ¨ç†çš„ä¸Šé™æ¯”ä»…ä½¿ç”¨è‹±è¯­çš„æ¨ç†é«˜å‡ºè¿‘10ä¸ªå‡†ç¡®ç‡ç‚¹ï¼Œå¹¶ä¸”å¯¹ç¿»è¯‘è´¨é‡å’Œè¯­è¨€é€‰æ‹©çš„å˜åŒ–å…·æœ‰æ›´å¼ºçš„å®¹å¿åº¦ã€‚æˆ‘ä»¬åˆ†æäº†è¾¾åˆ°è¿™ä¸€ä¸Šé™çš„åŸå› å’ŒæŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºå¸¸è§çš„ç­”æ¡ˆé€‰æ‹©æ–¹æ³•ç”±äºå…¶å±€é™æ€§å’Œåè§ï¼Œæ— æ³•å®ç°è¿™ä¸€ä¸Šé™ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥ç ”ç©¶å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€æ¨ç†æ½œåŠ›é“ºå¹³äº†é“è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10823', 'title': 'CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives', 'url': 'https://huggingface.co/papers/2504.10823', 'abstract': "Navigating high-stakes dilemmas involving conflicting values is challenging even for humans, let alone for AI. Yet prior work in evaluating the reasoning capabilities of large language models (LLMs) in such situations has been limited to everyday scenarios. To close this gap, this work first introduces CLASH (Character perspective-based LLM Assessments in Situations with High-stakes), a meticulously curated dataset consisting of 345 high-impact dilemmas along with 3,795 individual perspectives of diverse values. In particular, we design CLASH in a way to support the study of critical aspects of value-based decision-making processes which are missing from prior work, including understanding decision ambivalence and psychological discomfort as well as capturing the temporal shifts of values in characters' perspectives. By benchmarking 10 open and closed frontier models, we uncover several key findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet, achieve less than 50% accuracy in identifying situations where the decision should be ambivalent, while they perform significantly better in clear-cut scenarios. (2) While LLMs reasonably predict psychological discomfort as marked by human, they inadequately comprehend perspectives involving value shifts, indicating a need for LLMs to reason over complex values. (3) Our experiments also reveal a significant correlation between LLMs' value preferences and their steerability towards a given value. (4) Finally, LLMs exhibit greater steerability when engaged in value reasoning from a third-party perspective, compared to a first-person setup, though certain value pairs benefit uniquely from the first-person framing.", 'score': 10, 'issue_id': 3345, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '9e69138c0a313b09', 'authors': ['Ayoung Lee', 'Ryan Sungmo Kwon', 'Peter Railton', 'Lu Wang'], 'affiliations': ['Department of Computer Science and Engineering, University of Michigan', 'Department of Philosophy, University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2504.10823.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#dataset', '#benchmark'], 'emoji': 'ğŸ¤”', 'ru': {'title': 'Ğ˜ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CLASH Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4 Ğ¸ Claude-Sonnet, Ğ¸Ğ¼ĞµÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¾ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑÑ… Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°, Ñ‡ĞµĞ¼ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°.'}, 'en': {'title': 'Navigating Complex Values: Evaluating AI in High-Stakes Dilemmas', 'desc': 'This paper introduces CLASH, a new dataset designed to evaluate large language models (LLMs) on high-stakes dilemmas that involve conflicting values. It contains 345 dilemmas and 3,795 perspectives, focusing on aspects like decision ambivalence and psychological discomfort. The study benchmarks various LLMs, revealing that even advanced models struggle with ambivalent decisions, achieving less than 50% accuracy. Additionally, while LLMs can predict psychological discomfort, they have difficulty understanding shifts in values, highlighting the need for improved reasoning in complex value scenarios.'}, 'zh': {'title': 'åº”å¯¹é«˜é£é™©å›°å¢ƒä¸­çš„ä»·å€¼å†²çª', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†CLASHæ•°æ®é›†ï¼Œä¸“æ³¨äºé«˜é£é™©å›°å¢ƒä¸­çš„ä»·å€¼å†²çªï¼ŒåŒ…å«345ä¸ªé«˜å½±å“åŠ›çš„å›°å¢ƒå’Œ3795ä¸ªä¸åŒä»·å€¼è§‚çš„ä¸ªä½“è§†è§’ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒClaude-Sonnetï¼Œåœ¨è¯†åˆ«å†³ç­–æ¨¡ç³Šæ€§æ–¹é¢çš„å‡†ç¡®ç‡ä¹Ÿä¸è¶³50%ã€‚æ­¤å¤–ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹èƒ½å¤Ÿåˆç†é¢„æµ‹äººç±»çš„å¿ƒç†ä¸é€‚ï¼Œä½†åœ¨ç†è§£æ¶‰åŠä»·å€¼è½¬å˜çš„è§†è§’æ—¶è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬åœ¨å¤æ‚ä»·å€¼æ¨ç†ä¸Šçš„ä¸è¶³ã€‚æœ€åï¼Œæ¨¡å‹åœ¨ç¬¬ä¸‰æ–¹è§†è§’ä¸‹è¿›è¡Œä»·å€¼æ¨ç†æ—¶çš„å¯æ“æ§æ€§æ›´å¼ºï¼Œè€ŒæŸäº›ä»·å€¼å¯¹åœ¨ç¬¬ä¸€äººç§°æ¡†æ¶ä¸‹åˆ™è¡¨ç°æ›´å¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13173', 'title': "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization", 'url': 'https://huggingface.co/papers/2504.13173', 'abstract': 'Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, we observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, we present a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. We then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, we present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. We present three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Our experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models.', 'score': 9, 'issue_id': 3336, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '809d2f1facd3aed9', 'authors': ['Ali Behrouz', 'Meisam Razaviyayn', 'Peilin Zhong', 'Vahab Mirrokni'], 'affiliations': ['Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.13173.jpg', 'data': {'categories': ['#training', '#architecture', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ¼ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ framework Miras Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ - Moneta, Yaad Ğ¸ Memora, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ RNN. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Miras Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Neural Architectures with Attentional Bias', 'desc': 'This paper explores new ways to design neural network architectures, particularly focusing on how they can mimic human attention. It introduces the concept of attentional bias, which helps models prioritize important information, and critiques existing methods that rely on simple similarity measures. The authors propose a framework called Miras, which allows for flexible design choices in memory architecture and training objectives. They also present new models that outperform traditional approaches in specific tasks, demonstrating the effectiveness of their innovative strategies.'}, 'zh': {'title': 'åŸºäºæ³¨æ„åå‘çš„æ·±åº¦å­¦ä¹ æ¶æ„è®¾è®¡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•è®¾è®¡é«˜æ•ˆä¸”æœ‰æ•ˆçš„åŸºç¡€æ¨¡å‹æ¶æ„ï¼Œçµæ„Ÿæ¥æºäºäººç±»çš„æ³¨æ„åå‘ç°è±¡ã€‚æˆ‘ä»¬å°†ç¥ç»ç½‘ç»œæ¶æ„é‡æ–°æ¦‚å¿µåŒ–ä¸ºå…³è”è®°å¿†æ¨¡å—ï¼Œåˆ©ç”¨å†…éƒ¨ç›®æ ‡ï¼ˆæ³¨æ„åå‘ï¼‰æ¥å­¦ä¹ é”®å€¼æ˜ å°„ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰åºåˆ—æ¨¡å‹ä¸»è¦ä¾èµ–ç‚¹ç§¯ç›¸ä¼¼æ€§æˆ–L2å›å½’ç›®æ ‡ï¼Œè€Œæˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—æ›¿ä»£çš„æ³¨æ„åå‘é…ç½®åŠå…¶æœ‰æ•ˆè¿‘ä¼¼ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Mirasæ¡†æ¶ï¼Œè®¾è®¡æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¹¶å±•ç¤ºäº†ä¸‰ç§æ–°å‹åºåˆ—æ¨¡å‹ï¼Œè¶…è¶Šäº†ç°æœ‰çº¿æ€§RNNçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13157', 'title': 'AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis', 'url': 'https://huggingface.co/papers/2504.13157', 'abstract': 'We explore the task of geometric reconstruction of images captured from a mixture of ground and aerial views. Current state-of-the-art learning-based approaches fail to handle the extreme viewpoint variation between aerial-ground image pairs. Our hypothesis is that the lack of high-quality, co-registered aerial-ground datasets for training is a key reason for this failure. Such data is difficult to assemble precisely because it is difficult to reconstruct in a scalable way. To overcome this challenge, we propose a scalable framework combining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google Earth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The pseudo-synthetic data simulates a wide range of aerial viewpoints, while the real, crowd-sourced images help improve visual fidelity for ground-level images where mesh-based renderings lack sufficient detail, effectively bridging the domain gap between real images and pseudo-synthetic renderings. Using this hybrid dataset, we fine-tune several state-of-the-art algorithms and achieve significant improvements on real-world, zero-shot aerial-ground tasks. For example, we observe that baseline DUSt3R localizes fewer than 5% of aerial-ground pairs within 5 degrees of camera rotation error, while fine-tuning with our data raises accuracy to nearly 56%, addressing a major failure point in handling large viewpoint changes. Beyond camera estimation and scene reconstruction, our dataset also improves performance on downstream tasks like novel-view synthesis in challenging aerial-ground scenarios, demonstrating the practical value of our approach in real-world applications.', 'score': 8, 'issue_id': 3335, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': 'bbd51434265e3614', 'authors': ['Khiem Vuong', 'Anurag Ghosh', 'Deva Ramanan', 'Srinivasa Narasimhan', 'Shubham Tulsiani'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13157.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#synthetic', '#dataset', '#3d'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·ĞµĞ¼Ğ»ĞµĞ¹ Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ¼ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¸ Ğ°ÑÑ€Ğ¾ÑÑŠĞµĞ¼Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿ÑĞµĞ²Ğ´Ğ¾-ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹ Ğ¸Ğ· 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ°ÑÑ€Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Bridging the Viewpoint Gap: Enhanced Aerial-Ground Image Reconstruction', 'desc': "This paper addresses the challenge of reconstructing images from both ground and aerial perspectives, which current machine learning methods struggle with due to significant viewpoint differences. The authors suggest that the lack of high-quality datasets that pair aerial and ground images is a major obstacle. To tackle this, they introduce a scalable framework that combines pseudo-synthetic images generated from 3D city models with real ground-level images, effectively bridging the gap between these two domains. By fine-tuning existing algorithms with this hybrid dataset, they achieve substantial improvements in accuracy for aerial-ground tasks, demonstrating the framework's effectiveness in real-world applications."}, 'zh': {'title': 'æ‰“ç ´è§†è§’é™åˆ¶ï¼Œå®ç°å›¾åƒå‡ ä½•é‡å»º', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä»åœ°é¢å’Œç©ºä¸­è§†è§’æ•è·çš„å›¾åƒè¿›è¡Œå‡ ä½•é‡å»ºçš„ä»»åŠ¡ã€‚ç°æœ‰çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨å¤„ç†ç©ºä¸­ä¸åœ°é¢å›¾åƒå¯¹ä¹‹é—´çš„æç«¯è§†è§’å˜åŒ–æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç¼ºä¹é«˜è´¨é‡çš„ã€å…±åŒæ³¨å†Œçš„ç©ºä¸­-åœ°é¢æ•°æ®é›†æ˜¯å¯¼è‡´è¿™ä¸€å¤±è´¥çš„å…³é”®åŸå› ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ¡†æ¶ï¼Œç»“åˆäº†æ¥è‡ª3DåŸå¸‚ç½‘æ ¼çš„ä¼ªåˆæˆæ¸²æŸ“å’ŒçœŸå®çš„åœ°é¢ä¼—åŒ…å›¾åƒï¼Œä»è€Œæœ‰æ•ˆåœ°ç¼©å°äº†çœŸå®å›¾åƒä¸ä¼ªåˆæˆæ¸²æŸ“ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09621', 'title': 'Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images', 'url': 'https://huggingface.co/papers/2504.09621', 'abstract': 'Global contextual information and local detail features are essential for haze removal tasks. Deep learning models perform well on small, low-resolution images, but they encounter difficulties with large, high-resolution ones due to GPU memory limitations. As a compromise, they often resort to image slicing or downsampling. The former diminishes global information, while the latter discards high-frequency details. To address these challenges, we propose DehazeXL, a haze removal method that effectively balances global context and local feature extraction, enabling end-to-end modeling of large images on mainstream GPU hardware. Additionally, to evaluate the efficiency of global context utilization in haze removal performance, we design a visual attribution method tailored to the characteristics of haze removal tasks. Finally, recognizing the lack of benchmark datasets for haze removal in large images, we have developed an ultra-high-resolution haze removal dataset (8KDehaze) to support model training and testing. It includes 10000 pairs of clear and hazy remote sensing images, each sized at 8192 times 8192 pixels. Extensive experiments demonstrate that DehazeXL can infer images up to 10240 times 10240 pixels with only 21 GB of memory, achieving state-of-the-art results among all evaluated methods. The source code and experimental dataset are available at https://github.com/CastleChen339/DehazeXL.', 'score': 6, 'issue_id': 3340, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': '6c9f2fe055ad92dc', 'authors': ['Jiuchen Chen', 'Xinyu Yan', 'Qizhi Xu', 'Kaiqi Li'], 'affiliations': ['Beijing Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2504.09621.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#optimization', '#data', '#cv'], 'emoji': 'ğŸŒ«ï¸', 'ru': {'title': 'DehazeXL: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ñ‹Ğ¼ĞºĞ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'DehazeXL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ñ‹Ğ¼ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… GPU. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 8KDehaze Ñ 10000 Ğ¿Ğ°Ñ€ Ñ‡ĞµÑ‚ĞºĞ¸Ñ… Ğ¸ Ğ´Ñ‹Ğ¼Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 8192x8192 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DehazeXL Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 10240x10240 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 21 Ğ“Ğ‘ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ²ÑĞµÑ… Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'DehazeXL: Mastering Haze Removal with Global Context and Local Detail', 'desc': 'The paper introduces DehazeXL, a novel method for haze removal that effectively integrates global contextual information with local detail features. Traditional deep learning models struggle with high-resolution images due to memory constraints, often leading to a loss of important information. DehazeXL overcomes these limitations by allowing end-to-end processing of large images while maintaining high-quality outputs. Additionally, the authors present a new ultra-high-resolution dataset, 8KDehaze, to facilitate training and testing of haze removal models, demonstrating that their approach achieves superior performance on large images.'}, 'zh': {'title': 'DehazeXLï¼šé«˜æ•ˆå»é›¾çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å»é›¾æ–¹æ³•DehazeXLï¼Œæ—¨åœ¨å¹³è¡¡å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå±€éƒ¨ç»†èŠ‚ç‰¹å¾ï¼Œä»¥æé«˜å¤§å›¾åƒçš„å»é›¾æ•ˆæœã€‚ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶é¢ä¸´GPUå†…å­˜é™åˆ¶ï¼Œé€šå¸¸é‡‡ç”¨å›¾åƒåˆ‡ç‰‡æˆ–ä¸‹é‡‡æ ·çš„æ–¹æ³•ï¼Œä½†è¿™ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚DehazeXLèƒ½å¤Ÿåœ¨ä¸»æµGPUç¡¬ä»¶ä¸Šå®ç°ç«¯åˆ°ç«¯çš„å¤§å›¾åƒå»ºæ¨¡ï¼Œå¹¶é€šè¿‡è®¾è®¡è§†è§‰å½’å› æ–¹æ³•æ¥è¯„ä¼°å…¨å±€ä¸Šä¸‹æ–‡åœ¨å»é›¾æ€§èƒ½ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªè¶…é«˜åˆ†è¾¨ç‡å»é›¾æ•°æ®é›†ï¼ˆ8KDehazeï¼‰ï¼ŒåŒ…å«10000å¯¹8192x8192åƒç´ çš„æ¸…æ™°å’Œæ¨¡ç³Šé¥æ„Ÿå›¾åƒï¼Œä»¥æ”¯æŒæ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13072', 'title': 'HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation', 'url': 'https://huggingface.co/papers/2504.13072', 'abstract': 'Scene-level 3D generation represents a critical frontier in multimedia and computer graphics, yet existing approaches either suffer from limited object categories or lack editing flexibility for interactive applications. In this paper, we present HiScene, a novel hierarchical framework that bridges the gap between 2D image generation and 3D object generation and delivers high-fidelity scenes with compositional identities and aesthetic scene content. Our key insight is treating scenes as hierarchical "objects" under isometric views, where a room functions as a complex object that can be further decomposed into manipulatable items. This hierarchical approach enables us to generate 3D content that aligns with 2D representations while maintaining compositional structure. To ensure completeness and spatial alignment of each decomposed instance, we develop a video-diffusion-based amodal completion technique that effectively handles occlusions and shadows between objects, and introduce shape prior injection to ensure spatial coherence within the scene. Experimental results demonstrate that our method produces more natural object arrangements and complete object instances suitable for interactive applications, while maintaining physical plausibility and alignment with user inputs.', 'score': 5, 'issue_id': 3337, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 17', 'zh': '4æœˆ17æ—¥'}, 'hash': '6eb45708f6cb0c26', 'authors': ['Wenqi Dong', 'Bangbang Yang', 'Zesong Yang', 'Yuan Li', 'Tao Hu', 'Hujun Bao', 'Yuewen Ma', 'Zhaopeng Cui'], 'affiliations': ['ByteDance', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13072.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ ', 'ru': {'title': 'HiScene: Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹', 'desc': "HiScene - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ 2D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 'Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹' Ğ² Ğ¸Ğ·Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ñ‹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ñ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HiScene ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹."}, 'en': {'title': 'HiScene: Bridging 2D and 3D for Interactive Scene Generation', 'desc': 'This paper introduces HiScene, a new framework for generating 3D scenes that combines the strengths of 2D image generation with 3D object creation. It treats scenes as hierarchical structures, allowing for detailed manipulation of individual elements within a room. The method employs a video-diffusion-based technique for amodal completion, addressing issues like occlusions and shadows to ensure realistic object interactions. Experimental results show that HiScene produces coherent and aesthetically pleasing 3D scenes that are well-suited for interactive applications.'}, 'zh': {'title': 'HiSceneï¼šå±‚æ¬¡åŒ–çš„3Dåœºæ™¯ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHiSceneçš„å±‚æ¬¡æ¡†æ¶ï¼Œç”¨äºåœºæ™¯çº§3Dç”Ÿæˆï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¯¹è±¡ç±»åˆ«å’Œç¼–è¾‘çµæ´»æ€§æ–¹é¢çš„å±€é™ã€‚æˆ‘ä»¬å°†åœºæ™¯è§†ä¸ºåœ¨ç­‰è·è§†å›¾ä¸‹çš„å±‚æ¬¡â€œå¯¹è±¡â€ï¼Œä½¿å¾—æˆ¿é—´å¯ä»¥è¢«è¿›ä¸€æ­¥åˆ†è§£ä¸ºå¯æ“ä½œçš„ç‰©å“ã€‚é€šè¿‡è¿™ç§å±‚æ¬¡åŒ–çš„æ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆä¸2Dè¡¨ç¤ºç›¸ä¸€è‡´çš„3Då†…å®¹ï¼ŒåŒæ—¶ä¿æŒç»„åˆç»“æ„ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£çš„æ¨¡æ€è¡¥å…¨æŠ€æœ¯ï¼Œä»¥å¤„ç†å¯¹è±¡ä¹‹é—´çš„é®æŒ¡å’Œé˜´å½±ï¼Œç¡®ä¿æ¯ä¸ªåˆ†è§£å®ä¾‹çš„å®Œæ•´æ€§å’Œç©ºé—´å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13626', 'title': 'Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models', 'url': 'https://huggingface.co/papers/2504.13626', 'abstract': 'Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from "overthinking" problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on fine-tuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization.   Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token (<think> and </think>) can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications.', 'score': 4, 'issue_id': 3340, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 18', 'zh': '4æœˆ18æ—¥'}, 'hash': 'a0b61093ea88a6d7', 'authors': ['Yule Liu', 'Jingyi Zheng', 'Zhen Sun', 'Zifan Peng', 'Wenhan Dong', 'Zeyang Sha', 'Shiwen Cui', 'Weiqiang Wang', 'Xinlei He'], 'affiliations': ['Ant Group', 'Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2504.13626.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#training', '#optimization', '#reasoning', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ThoughtMani Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LRM. ThoughtMani ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 30% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LRM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Streamlining Reasoning: ThoughtMani Reduces Overthinking in LRMs', 'desc': "This paper discusses the challenges faced by large reasoning models (LRMs), particularly the issue of 'overthinking' where models produce excessive reasoning steps with minimal performance improvement. The authors propose a novel approach called ThoughtMani, which strategically places external Chains of Thought (CoTs) generated by smaller models to streamline the reasoning process. This method not only reduces the number of unnecessary intermediate steps but also maintains the model's performance while cutting down computational costs by about 30%. Additionally, ThoughtMani improves safety alignment, making it a practical solution for enhancing the efficiency of LRMs in real-world applications."}, 'zh': {'title': 'ThoughtManiï¼šå‡å°‘æ¨ç†æ­¥éª¤ï¼Œæå‡æ•ˆç‡', 'desc': 'æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç¤ºäº†é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—æ¥å¢å¼ºæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼ŒLRMsé€šå¸¸ä¼šå‡ºç°â€œè¿‡åº¦æ€è€ƒâ€é—®é¢˜ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ­¥éª¤å†—ä½™ä¸”æ€§èƒ½æå‡æœ‰é™ã€‚ç°æœ‰çš„ç ”ç©¶ä¾èµ–äºå¾®è°ƒæ¥ç¼“è§£è¿‡åº¦æ€è€ƒï¼Œä½†è¿™éœ€è¦é¢å¤–çš„æ•°æ®å’Œå¤æ‚çš„è®­ç»ƒè®¾ç½®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•é«˜æ•ˆçš„ç®¡é“ï¼ŒThoughtManiï¼Œé€šè¿‡åœ¨æ€è€ƒæ ‡è®°ä¹‹é—´æ”¾ç½®å°æ¨¡å‹ç”Ÿæˆçš„å¤–éƒ¨é“¾æ¡ï¼ˆCoTsï¼‰ï¼Œæœ‰æ•ˆå‡å°‘ä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13828', 'title': 'Generative AI Act II: Test Time Scaling Drives Cognition Engineering', 'url': 'https://huggingface.co/papers/2504.13828', 'abstract': 'The first generation of Large Language Models - what might be called "Act I" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of "Act II" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI\'s second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: https://github.com/GAIR-NLP/cognition-engineering', 'score': 3, 'issue_id': 3346, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 18', 'zh': '4æœˆ18æ—¥'}, 'hash': 'f0a8bce0c75eeac0', 'authors': ['Shijie Xia', 'Yiwei Qin', 'Xuefeng Li', 'Yan Ma', 'Run-Ze Fan', 'Steffi Chern', 'Haoyang Zou', 'Fan Zhou', 'Xiangkun Hu', 'Jiahe Jin', 'Yanheng He', 'Yixin Ye', 'Yixiu Liu', 'Pengfei Liu'], 'affiliations': ['Generative AI Research Lab (GAIR)', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13828.jpg', 'data': {'categories': ['#agi', '#multimodal', '#survey', '#optimization', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğº ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ñ‹ÑĞ»ĞµĞ¹: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ˜Ğ˜', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ°Ğ¿Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (test-time scaling). Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ' (cognition engineering) ĞºĞ°Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ˜Ğ˜ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚ÑƒÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ÑÑ‚Ğ¸Ğ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼."}, 'en': {'title': 'From Knowledge Retrieval to Thought Construction in AI', 'desc': "This paper discusses the evolution of Large Language Models (LLMs) from their initial phase, termed 'Act I', to a new phase called 'Act II'. In 'Act I', LLMs relied heavily on large datasets and parameters but faced issues like slow knowledge updates and limited reasoning abilities. The current phase, 'Act II', focuses on enhancing these models into thought-construction engines that can generate ideas and insights in real-time. The authors aim to make cognition engineering accessible to all practitioners by providing tutorials and resources for implementing these advanced techniques."}, 'zh': {'title': 'è®¤çŸ¥å·¥ç¨‹çš„æ–°æ—¶ä»£ï¼šä»çŸ¥è¯†æ£€ç´¢åˆ°æ€ç»´æ„å»º', 'desc': 'æœ¬æ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¬¬ä¸€ä»£ï¼ˆ2020-2023å¹´ï¼‰å’Œç¬¬äºŒä»£ï¼ˆ2024å¹´è‡³ä»Šï¼‰çš„å‘å±•ã€‚ç¬¬ä¸€ä»£æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡å‚æ•°å’Œæ•°æ®æ‰©å±•å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨çŸ¥è¯†å»¶è¿Ÿã€æ¨ç†æµ…æ˜¾å’Œè®¤çŸ¥è¿‡ç¨‹å—é™ç­‰æ–¹é¢å­˜åœ¨åŸºæœ¬å±€é™ã€‚ç¬¬äºŒä»£æ¨¡å‹æ­£åœ¨é€šè¿‡æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯ï¼Œä»çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿè½¬å˜ä¸ºæ€ç»´æ„å»ºå¼•æ“ï¼Œå»ºç«‹ä¸AIçš„è¯­è¨€æ€ç»´è¿æ¥ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†è¿™äº›å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æä¾›äº†å…¨é¢çš„æ•™ç¨‹å’Œä¼˜åŒ–å®ç°ï¼Œä»¥ä¿ƒè¿›è®¤çŸ¥å·¥ç¨‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13816', 'title': "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations", 'url': 'https://huggingface.co/papers/2504.13816', 'abstract': "While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs' perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs' recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.", 'score': 2, 'issue_id': 3347, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 18', 'zh': '4æœˆ18æ—¥'}, 'hash': '9243c69083d55578', 'authors': ['Chenghao Xiao', 'Hou Pong Chan', 'Hao Zhang', 'Mahani Aljunied', 'Lidong Bing', 'Noura Al Moubayed', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Department of Computer Science, Durham University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13816.jpg', 'data': {'categories': ['#hallucinations', '#dataset', '#transfer_learning', '#low_resource', '#open_source', '#training', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¸ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ LLM.'}, 'en': {'title': 'Bridging Language Gaps in LLM Knowledge Boundaries', 'desc': 'This paper investigates how large language models (LLMs) understand their limits of knowledge in various languages, addressing a gap in previous research that mainly focused on English. The authors find that LLMs encode their knowledge boundaries in specific layers of their architecture, and that these perceptions vary in a structured way across languages. They propose a method to align knowledge boundary recognition without additional training, which can help reduce inaccuracies in languages with fewer resources. Additionally, fine-tuning LLMs with bilingual question pairs improves their ability to recognize knowledge boundaries, and the authors provide a new multilingual evaluation suite for further research.'}, 'zh': {'title': 'è·¨è¯­è¨€çŸ¥è¯†è¾¹ç•Œçš„è¯†åˆ«ä¸è½¬ç§»', 'desc': 'æœ¬ç ”ç©¶é¦–æ¬¡åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒè¯­è¨€ä¸­è¯†åˆ«çŸ¥è¯†è¾¹ç•Œçš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMså¯¹çŸ¥è¯†è¾¹ç•Œçš„æ„ŸçŸ¥ä¸»è¦ç¼–ç åœ¨ä¸­é—´å±‚åˆ°ä¸­ä¸Šå±‚ã€‚ä¸åŒè¯­è¨€ä¹‹é—´çš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥å‘ˆçº¿æ€§ç»“æ„ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„å¯¹é½æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°åœ¨è¯­è¨€é—´è½¬ç§»çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œé™ä½ä½èµ„æºè¯­è¨€ä¸­çš„å¹»è§‰é£é™©ã€‚æ­¤å¤–ï¼ŒåŒè¯­é—®é¢˜å¯¹ç¿»è¯‘çš„å¾®è°ƒè¿›ä¸€æ­¥å¢å¼ºäº†LLMsåœ¨è·¨è¯­è¨€è¯†åˆ«çŸ¥è¯†è¾¹ç•Œçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.13519', 'title': 'Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for\n  Low-Dose CT with Attention-Guided Bilateral Filtering', 'url': 'https://huggingface.co/papers/2504.13519', 'abstract': 'Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable self-supervised single-image denoising framework -- Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through a lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted post-training for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce a novel downsampling shuffle strategy with a new self-supervised loss function that extends the concept of Noise2Noise to a single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at https://github.com/sypsyp97/Filter2Noise.git .', 'score': 0, 'issue_id': 3348, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 18', 'zh': '4æœˆ18æ—¥'}, 'hash': '54a6d11b3dfb1ed4', 'authors': ['Yipeng Sun', 'Linda-Sophie Schneider', 'Mingxuan Gu', 'Siyuan Mei', 'Chengze Ye', 'Fabian Wagner', 'Siming Bayer', 'Andreas Maier'], 'affiliations': ['Friedrich-Alexander-University Erlangen-Nuremberg, Erlangen, Germany', 'Siemens Healthineers AG, Forchheim, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.13519.jpg', 'data': {'categories': ['#data', '#training', '#interpretability', '#low_resource', '#healthcare', '#dataset'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞšĞ¢: Ğ¾Ñ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ° Ğº Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ´Ğ¾Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Filter2Noise (F2N). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ±Ğ¸Ğ»Ğ°Ñ‚ĞµÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğµ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. F2N Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.'}, 'en': {'title': 'Revolutionizing Low-Dose CT Denoising with Filter2Noise', 'desc': 'This paper presents Filter2Noise (F2N), a self-supervised framework for denoising low-dose CT images using a single noisy input. Unlike traditional supervised methods that require paired datasets, F2N employs an Attention-Guided Bilateral Filter that adapts to the noise characteristics of each image, allowing for user-controlled adjustments. The framework introduces a novel downsampling shuffle strategy and a self-supervised loss function that effectively handles spatially correlated noise. Experimental results show that F2N significantly improves denoising performance, achieving higher PSNR compared to existing methods while enhancing interpretability and control for medical applications.'}, 'zh': {'title': 'è‡ªç›‘ç£å»å™ªï¼Œç²¾å‡†å¯æ§ï¼', 'desc': 'åœ¨ä½å‰‚é‡CTä¸­ï¼Œæœ‰æ•ˆå»å™ªå¯¹äºå¢å¼ºç»†å¾®ç»“æ„å’Œä½å¯¹æ¯”åº¦ç—…å˜è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨æœ‰é™çš„é…å¯¹æ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œè‡ªç›‘ç£æ–¹æ³•é€šå¸¸éœ€è¦å¤šå¼ å™ªå£°å›¾åƒï¼Œå¹¶ä¾èµ–æ·±åº¦ç½‘ç»œå¦‚U-Netï¼Œç¼ºä¹å¯¹å»å™ªæœºåˆ¶çš„æ·±å…¥ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„è‡ªç›‘ç£å•å›¾åƒå»å™ªæ¡†æ¶â€”â€”Filter2Noise (F2N)ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„åŒè¾¹æ»¤æ³¢å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®æ¯ä¸ªå™ªå£°è¾“å…¥è‡ªé€‚åº”è°ƒæ•´æ»¤æ³¢å‚æ•°ï¼Œç”¨æˆ·å¯ä»¥åœ¨è®­ç»ƒåå¯è§†åŒ–å’Œè°ƒæ•´è¿™äº›å‚æ•°ï¼Œä»¥å®ç°ç‰¹å®šåŒºåŸŸçš„å»å™ªæ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10514', 'title': 'ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness', 'url': 'https://huggingface.co/papers/2504.10514', 'abstract': 'Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.', 'score': 30, 'issue_id': 3281, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'c786e69f24be2f9e', 'authors': ['Yijun Liang', 'Ming Li', 'Chenrui Fan', 'Ziyue Li', 'Dang Nguyen', 'Kwesi Cobbina', 'Shweta Bhardwaj', 'Jiuhai Chen', 'Fuxiao Liu', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2504.10514.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#reasoning', '#benchmark'], 'emoji': 'ğŸŒˆ', 'ru': {'title': 'ColorBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ColorBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ†Ğ²ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 32 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ñ†Ğ²ĞµÑ‚Ğ°, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ° Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ColorBench Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': "Enhancing AI's Color Comprehension with ColorBench", 'desc': 'This paper presents ColorBench, a benchmark designed to evaluate how vision-language models (VLMs) understand and utilize color in visual reasoning. It assesses various aspects of color perception, reasoning, and robustness through a series of real-world scenarios. The study finds that while larger models generally perform better, the existing VLMs show limited capabilities in color understanding, indicating a gap in their training. Additionally, the research highlights that while VLMs can use color cues effectively, they can also be misled by them, emphasizing the need for improved color comprehension in AI models.'}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„é¢œè‰²ç†è§£èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ColorBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é¢œè‰²ç†è§£æ–¹é¢èƒ½åŠ›çš„åŸºå‡†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æ›´å¤§çš„æ¨¡å‹åœ¨ColorBenchä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†è¯­è¨€æ¨¡å‹çš„ä½œç”¨æ¯”è§†è§‰ç¼–ç å™¨æ›´ä¸ºé‡è¦ã€‚ç°æœ‰çš„VLMsåœ¨é¢œè‰²ç†è§£æ–¹é¢çš„è¡¨ç°å·®è·è¾ƒå°ï¼Œè¡¨æ˜è¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†é‡è§†ã€‚æ­¤å¤–ï¼Œå°½ç®¡VLMsèƒ½å¤Ÿåˆ©ç”¨é¢œè‰²çº¿ç´¢ï¼Œä½†åœ¨æŸäº›ä»»åŠ¡ä¸­ä¹Ÿå¯èƒ½ä¼šå—åˆ°è¯¯å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12285', 'title': 'BitNet b1.58 2B4T Technical Report', 'url': 'https://huggingface.co/papers/2504.12285', 'abstract': 'We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.', 'score': 28, 'issue_id': 3281, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': 'cf67f70d9f122792', 'authors': ['Shuming Ma', 'Hongyu Wang', 'Shaohan Huang', 'Xingxing Zhang', 'Ying Hu', 'Ting Song', 'Yan Xia', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.12285.jpg', 'data': {'categories': ['#open_source', '#dataset', '#benchmark', '#science', '#architecture', '#training', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸: 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ LLM Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ BitNet b1.58 2B4T - Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 4 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ° Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. BitNet b1.58 2B4T Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… LLM Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ½Ğ¾ Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Hugging Face Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ GPU Ğ¸ CPU.'}, 'en': {'title': 'Efficient Language Understanding with BitNet: The 1-Bit Revolution', 'desc': "BitNet b1.58 2B4T is a groundbreaking 1-bit Large Language Model (LLM) with 2 billion parameters, making it the first of its kind to be open-source. It has been trained on an extensive dataset of 4 trillion tokens and evaluated on various benchmarks, showcasing its capabilities in language understanding, mathematical reasoning, coding, and conversation. Remarkably, BitNet achieves performance comparable to other leading full-precision LLMs while being more efficient in terms of memory usage, energy consumption, and decoding speed. The model's weights and inference implementations are made available on Hugging Face, promoting further research and practical applications."}, 'zh': {'title': 'å¼€æºé«˜æ•ˆçš„1ä½å¤§å‹è¯­è¨€æ¨¡å‹', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†BitNet b1.58 2B4Tï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æºçš„ã€åŸç”Ÿçš„1ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡è¾¾åˆ°20äº¿ã€‚è¯¥æ¨¡å‹åœ¨4ä¸‡äº¿ä¸ªæ ‡è®°çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨è¯­è¨€ç†è§£ã€æ•°å­¦æ¨ç†ã€ç¼–ç¨‹èƒ½åŠ›å’Œå¯¹è¯èƒ½åŠ›ç­‰åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒBitNet b1.58 2B4Tåœ¨æ€§èƒ½ä¸Šä¸åŒç±»è§„æ¨¡çš„é¢†å…ˆå¼€æºå…¨ç²¾åº¦å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬æ˜¾è‘—å‡å°‘çš„å†…å­˜å ç”¨ã€èƒ½è€—å’Œè§£ç å»¶è¿Ÿã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨ï¼Œè¯¥æ¨¡å‹çš„æƒé‡é€šè¿‡Hugging Faceå‘å¸ƒï¼Œå¹¶æä¾›äº†é€‚ç”¨äºGPUå’ŒCPUæ¶æ„çš„å¼€æºæ¨ç†å®ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11536', 'title': 'ReTool: Reinforcement Learning for Strategic Tool Use in LLMs', 'url': 'https://huggingface.co/papers/2504.11536', 'abstract': "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems.", 'score': 25, 'issue_id': 3288, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '1402eef5411f1416', 'authors': ['Jiazhan Feng', 'Shijue Huang', 'Xingwei Qu', 'Ge Zhang', 'Yujia Qin', 'Baoquan Zhong', 'Chengquan Jiang', 'Jinxin Chi', 'Wanjun Zhong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.11536.jpg', 'data': {'categories': ['#long_context', '#rl', '#math', '#synthetic', '#reasoning', '#training', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ReTool: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReTool, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ´Ğ°. ReTool Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MATH Olympiad AIME Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ReTool Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'ReTool: Bridging Reasoning and Computation for Enhanced Learning', 'desc': "This paper introduces ReTool, a novel approach that enhances long-form reasoning in machine learning models by integrating real-time code execution into the reasoning process. It addresses the limitations of existing models in structured problem-solving tasks, such as geometric reasoning and complex computations, by employing a reinforcement learning framework that allows models to learn when and how to use computational tools effectively. ReTool's training involves generating synthetic data to fine-tune models and using task outcomes as rewards to improve tool invocation strategies. Experimental results show that ReTool significantly outperforms traditional text-based reinforcement learning models, demonstrating its potential for advancing complex mathematical reasoning and hybrid neuro-symbolic systems."}, 'zh': {'title': 'ReToolï¼šæå‡å¤æ‚æ¨ç†çš„å·¥å…·é›†æˆå­¦ä¹ ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReToolçš„æ¨¡å‹ï¼Œæ—¨åœ¨æå‡é•¿æ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡ ä½•æ¨ç†å’Œå¤æ‚æ–¹ç¨‹æ±‚è§£ç­‰ç»“æ„åŒ–é—®é¢˜ä¸Šã€‚ReToolç»“åˆäº†å®æ—¶ä»£ç æ‰§è¡Œä¸è‡ªç„¶è¯­è¨€æ¨ç†çš„åŠ¨æ€äº¤æ›¿ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è‡ªåŠ¨åŒ–ç­–ç•¥æ¥ä¼˜åŒ–å·¥å…·çš„ä½¿ç”¨ã€‚é€šè¿‡åˆæˆå†·å¯åŠ¨æ•°æ®ç”Ÿæˆä»£ç å¢å¼ºçš„æ¨ç†è½¨è¿¹ï¼ŒReToolåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­è°ƒæ•´æ¨¡å‹çš„å·¥å…·è°ƒç”¨ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReToolåœ¨MATHå¥¥æ—åŒ¹å…‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12240', 'title': 'Cobra: Efficient Line Art COlorization with BRoAder References', 'url': 'https://huggingface.co/papers/2504.12240', 'abstract': 'The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.', 'score': 17, 'issue_id': 3280, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': 'a237e12792a9a0c8', 'authors': ['Junhao Zhuang', 'Lingen Li', 'Xuan Ju', 'Zhaoyang Zhang', 'Chun Yuan', 'Ying Shan'], 'affiliations': ['Tencent ARC Lab, China', 'The Chinese University of Hong Kong, China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.12240.jpg', 'data': {'categories': ['#long_context', '#cv', '#open_source', '#architecture', '#inference', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Cobra: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cobra Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Causal Sparse DiT Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°ÑĞºÑ€Ğ°ÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ 200 Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ². Cobra Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ².'}, 'en': {'title': 'Cobra: Revolutionizing Line Art Colorization with Contextual Efficiency', 'desc': 'This paper presents Cobra, a novel method for line art colorization in the comic production industry, which requires high accuracy and efficiency. Cobra utilizes a Causal Sparse DiT architecture that incorporates advanced techniques like causal sparse attention and positional encodings to handle over 200 reference images effectively. The method addresses challenges such as slow inference times and the need for flexible control, ensuring color identity consistency across diverse characters and backgrounds. Experimental results show that Cobra significantly improves the quality and speed of line art colorization, making it a valuable tool for artists.'}, 'zh': {'title': 'Cobraï¼šé«˜æ•ˆçµæ´»çš„çº¿æ¡è‰ºæœ¯ä¸Šè‰²è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCobraçš„é«˜æ•ˆçº¿æ¡è‰ºæœ¯ä¸Šè‰²æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ¼«ç”»åˆ¶ä½œè¡Œä¸šä¸­å¯¹é«˜å‡†ç¡®æ€§å’Œçµæ´»æ§åˆ¶çš„éœ€æ±‚ã€‚Cobraèƒ½å¤Ÿå¤„ç†è¶…è¿‡200å¼ å‚è€ƒå›¾åƒï¼Œå¹¶ä¿æŒä½å»¶è¿Ÿï¼Œé€‚åº”å¤æ‚çš„è§’è‰²å’ŒèƒŒæ™¯ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†å› æœç¨€ç–DiTæ¶æ„ï¼Œåˆ©ç”¨ç‰¹æ®Šè®¾è®¡çš„ä½ç½®ç¼–ç å’Œå› æœç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆç®¡ç†é•¿ä¸Šä¸‹æ–‡å‚è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCobraåœ¨ä¸Šè‰²è´¨é‡å’Œæ¨ç†é€Ÿåº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œæ»¡è¶³äº†å·¥ä¸šç•Œçš„å…³é”®éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10326', 'title': 'AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference', 'url': 'https://huggingface.co/papers/2504.10326', 'abstract': 'AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks.', 'score': 17, 'issue_id': 3286, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '030a9ba5449806fb', 'authors': ['Yangshen Deng', 'Zhengxin You', 'Long Xiang', 'Qilong Li', 'Peiqi Yuan', 'Zhaoyang Hong', 'Yitao Zheng', 'Wanting Li', 'Runzhong Li', 'Haotian Liu', 'Kyriakos Mouratidis', 'Man Lung Yiu', 'Huan Li', 'Qiaomu Shen', 'Rui Mao', 'Bo Tang'], 'affiliations': ['AlayaDB AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.10326.jpg', 'data': {'categories': ['#optimization', '#inference', '#long_context', '#benchmark', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'AlayaDB: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'AlayaDB - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ KV-ĞºÑÑˆ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM, Ğ¸Ğ½ĞºĞ°Ğ¿ÑÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ² Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. AlayaDB Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ½Ğ°Ğ³Ñ€ÑƒĞ·Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ AlayaDB ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Revolutionizing LLM Inference with AlayaDB', 'desc': "AlayaDB is an innovative vector database designed to enhance long-context inference for Large Language Models (LLMs). It separates key-value (KV) caching and attention computation from the LLM inference process, streamlining these functions into a dedicated database system. This architecture allows Model as a Service (MaaS) providers to use fewer hardware resources while achieving superior generation quality across various workloads. The system's core feature is its ability to treat attention computation and cache management as a query processing task, which is further optimized by a specialized query optimizer."}, 'zh': {'title': 'AlayaDBï¼šé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†è§£å†³æ–¹æ¡ˆ', 'desc': 'AlayaDBæ˜¯ä¸€ç§å…ˆè¿›çš„å‘é‡æ•°æ®åº“ç³»ç»Ÿï¼Œä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¾è®¡ï¼Œæ—¨åœ¨æé«˜é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ•ˆç‡å’Œæ•ˆæœã€‚å®ƒå°†é”®å€¼ç¼“å­˜å’Œæ³¨æ„åŠ›è®¡ç®—ä»LLMæ¨ç†ç³»ç»Ÿä¸­è§£è€¦ï¼Œå¹¶å°†å…¶å°è£…åˆ°ä¸€ä¸ªæ–°é¢–çš„å‘é‡æ•°æ®åº“ç³»ç»Ÿä¸­ã€‚ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒAlayaDBåœ¨ç¡¬ä»¶èµ„æºæ¶ˆè€—å’Œç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œé€‚ç”¨äºä¸åŒæœåŠ¡æ°´å¹³ç›®æ ‡ï¼ˆSLOï¼‰çš„å¤šç§å·¥ä½œè´Ÿè½½ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†æ³¨æ„åŠ›è®¡ç®—å’Œç¼“å­˜ç®¡ç†æŠ½è±¡ä¸ºæŸ¥è¯¢å¤„ç†è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡æœ¬åœ°æŸ¥è¯¢ä¼˜åŒ–å™¨ä¼˜åŒ–æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09081', 'title': 'SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2504.09081', 'abstract': 'We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs.', 'score': 12, 'issue_id': 3287, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 12', 'zh': '4æœˆ12æ—¥'}, 'hash': '4c17c8d0a36c365d', 'authors': ['Prabhat Pandey', 'Rupak Vignesh Swaminathan', 'K V Vijay Girish', 'Arunasish Sen', 'Jian Xie', 'Grant P. Strimel', 'Andreas Schwarz'], 'affiliations': ['Amazon AGI', 'Apple Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.09081.jpg', 'data': {'categories': ['#dataset', '#audio', '#transfer_learning', '#multilingual', '#benchmark', '#open_source'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'SIFT: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'SIFT (Speech Instruction Fine-Tuning) - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 50 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ñ€ĞµÑ‡ÑŒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ SIFT-LLM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¯Ğœ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ EvalSIFT - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¯Ğœ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Empowering Speech Models with SIFT: Fine-Tuning for Instruction Mastery', 'desc': 'The paper presents SIFT (Speech Instruction Fine-Tuning), a large dataset containing 50 million examples for enhancing speech-text large language models (LLMs). It is constructed from 14,000 hours of publicly available speech data across five languages, focusing on both speech understanding and generation tasks. The authors train a model called SIFT-LLM, which shows superior performance on instruction-following benchmarks compared to existing models, while also being competitive in foundational speech tasks. Additionally, they introduce EvalSIFT, a specialized benchmark for assessing the instruction-following abilities of speech-text LLMs.'}, 'zh': {'title': 'SIFTï¼šæå‡è¯­éŸ³æŒ‡ä»¤ç†è§£çš„åˆ›æ–°æ•°æ®é›†', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†SIFTï¼ˆè¯­éŸ³æŒ‡ä»¤å¾®è°ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5000ä¸‡æ¡ç¤ºä¾‹çš„æ•°æ®é›†ï¼Œæ—¨åœ¨ç”¨äºè¯­éŸ³-æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒå’Œé¢„è®­ç»ƒã€‚SIFT-50MåŸºäºå…¬å¼€çš„è¯­éŸ³è¯­æ–™åº“æ„å»ºï¼ŒåŒ…å«14000å°æ—¶çš„è¯­éŸ³ï¼Œåˆ©ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œç°æˆçš„ä¸“å®¶æ¨¡å‹ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº”ç§è¯­è¨€ï¼ŒåŒ…å«å¤šæ ·çš„è¯­éŸ³ç†è§£å’Œå¯æ§çš„è¯­éŸ³ç”ŸæˆæŒ‡ä»¤ã€‚ä½¿ç”¨SIFT-50Mï¼Œæˆ‘ä»¬è®­ç»ƒäº†SIFT-LLMï¼Œå…¶åœ¨æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„è¯­éŸ³-æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶åœ¨åŸºç¡€è¯­éŸ³ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10483', 'title': 'REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers', 'url': 'https://huggingface.co/papers/2504.10483', 'abstract': 'In this paper we tackle a fundamental question: "Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io.', 'score': 11, 'issue_id': 3285, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'f46935088154b083', 'authors': ['Xingjian Leng', 'Jaskirat Singh', 'Yunzhong Hou', 'Zhenchang Xing', 'Saining Xie', 'Liang Zheng'], 'affiliations': ['Australian National University', 'Data61 CSIRO', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10483.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ VAE', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° (VAE) Ğ² ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ (REPA). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ (REPA-E) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° VAE. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID Ğ½Ğ° ImageNet 256x256.'}, 'en': {'title': 'Unlocking End-to-End Training with REPA for Latent Diffusion Models', 'desc': 'This paper explores the possibility of training latent diffusion models alongside a variational auto-encoder (VAE) in an end-to-end fashion. It highlights that traditional methods using standard diffusion loss are ineffective and can even harm performance. The authors introduce a new loss function called representation-alignment (REPA) loss, which enables effective joint training of the VAE and diffusion model. Their proposed training method, REPA-E, significantly accelerates training and enhances the quality of generated outputs, achieving state-of-the-art results on ImageNet.'}, 'zh': {'title': 'ç«¯åˆ°ç«¯è®­ç»ƒçš„çªç ´ï¼šREPAæŸå¤±çš„åŠ›é‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šèƒ½å¦å°†æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ ‡è®°å™¨ä¸€èµ·è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Ÿä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ ç†è®ºè®¤ä¸ºï¼Œå°½å¯èƒ½è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒæ˜¯æ›´ä¼˜çš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œå¯¹äºæ½œåœ¨æ‰©æ•£å˜æ¢å™¨ï¼Œä½¿ç”¨æ ‡å‡†æ‰©æ•£æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒä¼šå¯¼è‡´æ•ˆæœä¸ä½³ï¼Œç”šè‡³é™ä½æœ€ç»ˆæ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†é€šè¿‡è¡¨ç¤ºå¯¹é½æŸå¤±ï¼ˆREPAæŸå¤±ï¼‰æ¥è§£é”ç«¯åˆ°ç«¯è®­ç»ƒï¼Œä½¿å¾—VAEå’Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å…±åŒè°ƒæ•´ï¼Œæœ€ç»ˆå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11468', 'title': 'SFT or RL? An Early Investigation into Training R1-Like Reasoning Large\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2504.11468', 'abstract': "This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals a key finding: SFT can significantly undermine subsequent RL by inducing ``pseudo reasoning paths'' imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning. To systematically study this effect, we introduce VLAA-Thinking, a new multimodal dataset designed to support reasoning in LVLMs. Constructed via a six-step pipeline involving captioning, reasoning distillation, answer rewrite and verification, VLAA-Thinking comprises high-quality, step-by-step visual reasoning traces for SFT, along with a more challenging RL split from the same data source. Using this dataset, we conduct extensive experiments comparing SFT, RL and their combinations. Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with a novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard (https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard) among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope our findings provide valuable insights in developing reasoning-capable LVLMs and can inform future research in this area.", 'score': 8, 'issue_id': 3294, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '550fb98da92065ed', 'authors': ['Hardy Chen', 'Haoqin Tu', 'Fali Wang', 'Hui Liu', 'Xianfeng Tang', 'Xinya Du', 'Yuyin Zhou', 'Cihang Xie'], 'affiliations': ['Amazon Research', 'The Pennsylvania State University', 'University of California, Santa Cruz', 'University of Texas at Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2504.11468.jpg', 'data': {'categories': ['#multimodal', '#training', '#rl', '#dataset', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT), Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL), Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SFT Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ 'Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL. Ğ”Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VLAA-Thinking, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RL Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Group Relative Policy Optimization (GRPO) Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Unlocking Genuine Reasoning in LVLMs', 'desc': 'This paper examines the training process of Large Vision-Language Models (LVLMs) using supervised fine-tuning (SFT) followed by reinforcement learning (RL). It finds that SFT can create misleading reasoning patterns that hinder the effectiveness of subsequent RL, leading to less informative and incorrect reasoning. To address this, the authors introduce VLAA-Thinking, a new dataset that supports better reasoning in LVLMs through a structured six-step process. Their experiments show that while SFT can help with learning reasoning formats, it can also restrict models to rigid reasoning, whereas their RL approach encourages more flexible and adaptive reasoning capabilities.'}, 'zh': {'title': 'æ‰“ç ´æ¨¡ä»¿ï¼Œä¿ƒè¿›çœŸå®æ¨ç†çš„LVLMè®­ç»ƒ', 'desc': 'æœ¬æ–‡é‡æ–°å®¡è§†äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒèŒƒå¼ï¼Œå‘ç°SFTå¯èƒ½ä¼šé€šè¿‡å¼•å…¥â€œä¼ªæ¨ç†è·¯å¾„â€æ¥æ˜¾è‘—å‰Šå¼±åç»­çš„RLã€‚è¿™äº›ä¼ªè·¯å¾„è™½ç„¶çœ‹ä¼¼ä¸RLæ¨¡å‹çš„åŸç”Ÿæ¨ç†è·¯å¾„ç›¸ä¼¼ï¼Œä½†å¾€å¾€æ¶‰åŠå†—é•¿ã€çŠ¹è±«ä¸”ä¿¡æ¯é‡ä¸è¶³çš„æ­¥éª¤ï¼Œç”šè‡³é”™è¯¯æ¨ç†ã€‚ä¸ºç³»ç»Ÿç ”ç©¶è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†VLAA-Thinkingï¼Œä¸€ä¸ªæ–°å‹çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒLVLMsçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å…­æ­¥æµç¨‹æ„å»ºçš„VLAA-Thinkingæä¾›äº†é«˜è´¨é‡çš„é€æ­¥è§†è§‰æ¨ç†è½¨è¿¹ï¼Œå¹¶é€šè¿‡å®éªŒè¡¨æ˜ï¼ŒSFTè™½ç„¶æœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ¨ç†æ ¼å¼ï¼Œä½†ä¼šä½¿æ¨¡å‹é™·å…¥æ¨¡ä»¿å’ŒåƒµåŒ–çš„æ¨ç†æ¨¡å¼ï¼Œå¦¨ç¢è¿›ä¸€æ­¥å­¦ä¹ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12264', 'title': 'Towards Learning to Complete Anything in Lidar', 'url': 'https://huggingface.co/papers/2504.12264', 'abstract': 'We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion in-the-wild. This is closely related to Lidar-based semantic/panoptic scene completion. However, contemporary methods can only complete and recognize objects from a closed vocabulary labeled in existing Lidar datasets. Different to that, our zero-shot approach leverages the temporal context from multi-modal sensor sequences to mine object shapes and semantic features of observed objects. These are then distilled into a Lidar-only instance-level completion and recognition model. Although we only mine partial shape completions, we find that our distilled model learns to infer full object shapes from multiple such partial observations across the dataset. We show that our model can be prompted on standard benchmarks for Semantic and Panoptic Scene Completion, localize objects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class vocabularies. Our project page is https://research.nvidia.com/labs/dvl/projects/complete-anything-lidar', 'score': 4, 'issue_id': 3292, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': '171045e898ed54a7', 'authors': ['Ayca Takmaz', 'Cristiano Saltori', 'Neehar Peri', 'Tim Meinhardt', 'Riccardo de Lutio', 'Laura Leal-TaixÃ©', 'AljoÅ¡a OÅ¡ep'], 'affiliations': ['Carnegie Mellon University', 'ETH Zurich', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2504.12264.jpg', 'data': {'categories': ['#3d', '#multimodal', '#benchmark'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ»Ğ¸Ğ´Ğ°Ñ€Ğ°', 'desc': 'CAL (Complete Anything in Lidar) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ¸Ğ´Ğ°Ñ€Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², CAL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ğ´Ğ°Ñ€Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ÑÑ Ğ»Ğ¸ÑˆÑŒ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Complete Any Object with Lidar: Beyond Fixed Classes!', 'desc': 'The paper introduces CAL (Complete Anything in Lidar), a novel approach for completing shapes using Lidar data in real-world scenarios. Unlike traditional methods that rely on a fixed set of labeled objects, CAL employs a zero-shot learning technique that utilizes temporal context from multi-modal sensor sequences to extract object shapes and semantic features. This information is then distilled into a model that can complete and recognize objects at the instance level, even when only partial shapes are available. The results demonstrate that CAL can effectively infer full object shapes and perform well on standard benchmarks for scene completion and object localization, extending recognition beyond predefined class categories.'}, 'zh': {'title': 'æ¿€å…‰é›·è¾¾ä¸­çš„å½¢çŠ¶è¡¥å…¨æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†CALï¼ˆåœ¨æ¿€å…‰é›·è¾¾ä¸­å®Œæˆä»»ä½•å½¢çŠ¶ï¼‰ï¼Œç”¨äºåœ¨å®é™…ç¯å¢ƒä¸­è¿›è¡Œæ¿€å…‰é›·è¾¾åŸºç¡€çš„å½¢çŠ¶è¡¥å…¨ã€‚è¿™ç§æ–¹æ³•ä¸æ¿€å…‰é›·è¾¾åŸºç¡€çš„è¯­ä¹‰/å…¨æ™¯åœºæ™¯è¡¥å…¨å¯†åˆ‡ç›¸å…³ï¼Œä½†ç°æœ‰æ–¹æ³•åªèƒ½åœ¨å›ºå®šçš„æ ‡ç­¾è¯æ±‡ä¸­å®Œæˆå’Œè¯†åˆ«å¯¹è±¡ã€‚æˆ‘ä»¬çš„é›¶æ ·æœ¬æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€ä¼ æ„Ÿå™¨åºåˆ—ä¸­çš„æ—¶é—´ä¸Šä¸‹æ–‡ï¼ŒæŒ–æ˜è§‚å¯Ÿåˆ°å¯¹è±¡çš„å½¢çŠ¶å’Œè¯­ä¹‰ç‰¹å¾ï¼Œå¹¶å°†å…¶æç‚¼ä¸ºä»…åŸºäºæ¿€å…‰é›·è¾¾çš„å®ä¾‹çº§è¡¥å…¨å’Œè¯†åˆ«æ¨¡å‹ã€‚å°½ç®¡æˆ‘ä»¬åªæŒ–æ˜éƒ¨åˆ†å½¢çŠ¶è¡¥å…¨ï¼Œä½†æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿä»å¤šä¸ªéƒ¨åˆ†è§‚å¯Ÿä¸­æ¨æ–­å‡ºå®Œæ•´çš„å¯¹è±¡å½¢çŠ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11952', 'title': 'Robust and Fine-Grained Detection of AI Generated Texts', 'url': 'https://huggingface.co/papers/2504.11952', 'abstract': "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.", 'score': 4, 'issue_id': 3280, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': 'bdea465fe17b9401', 'authors': ['Ram Mohan Rao Kadiyala', 'Siddartha Pullakhandam', 'Kanwal Mehreen', 'Drishti Sharma', 'Siddhant Gupta', 'Jebish Purbey', 'Ashay Srivastava', 'Subhasya TippaReddy', 'Arvind Reddy Bobbili', 'Suraj Telugara Chandrashekhar', 'Modabbir Adeeb', 'Srinadh Vura', 'Hamza Farooq'], 'affiliations': ['Cohere for AI Community', 'IISc Bangalore', 'IIT Roorkee', 'M2ai.in', 'Pulchowk Campus', 'Stanford University', 'Traversaal.ai', 'University of California, Los Angeles', 'University of Houston', 'University of Maryland, College Park', 'University of South Florida', 'Vantager'], 'pdf_title_img': 'assets/pdf/title_img/2504.11952.jpg', 'data': {'categories': ['#low_resource', '#hallucinations', '#dataset', '#benchmark', '#multilingual', '#security', '#data'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ˜Ğ˜. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¸Ğ· Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ¾Ñ‚ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° 23 ÑĞ·Ñ‹ĞºĞ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¾Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğµ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Advancing Detection of Human-LLM Co-Authored Texts', 'desc': 'This paper addresses the challenge of detecting machine-generated content, particularly in cases where texts are co-authored by humans and language models (LLMs). The authors developed a set of token classification models trained on a large dataset of 2.4 million co-authored texts, which allows for better detection across various domains and generators. The models demonstrated strong performance even with adversarial inputs and texts from non-native speakers. Additionally, the paper provides insights into how the models perform based on text length and characteristics compared to purely human-authored content.'}, 'zh': {'title': 'æ„å»ºé«˜æ•ˆçš„æœºå™¨ç”Ÿæˆå†…å®¹æ£€æµ‹ç³»ç»Ÿ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç†æƒ³çš„æ£€æµ‹ç³»ç»Ÿï¼Œæ—¨åœ¨æœ‰æ•ˆè¯†åˆ«æœºå™¨ç”Ÿæˆçš„å†…å®¹ï¼Œå°¤å…¶æ˜¯åœ¨çŸ­æ–‡æœ¬ä¸­ã€‚ç°æœ‰ç³»ç»Ÿåœ¨è¯†åˆ«AIç”Ÿæˆå†…å®¹æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå› æ­¤æˆ‘ä»¬ä¸“æ³¨äºäººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…±åŒåˆ›ä½œçš„æ–‡æœ¬ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ç³»åˆ—ç”¨äºæ ‡è®°åˆ†ç±»çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤§é‡äººæœºå…±åˆ›æ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æœªè§é¢†åŸŸå’Œç”Ÿæˆå™¨çš„æ–‡æœ¬ä¸Šè¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å«240ä¸‡æ¡æ–‡æœ¬çš„æ–°æ•°æ®é›†ï¼Œä¸»è¦ç”±å¤šç§æµè¡Œçš„ä¸“æœ‰LLMå…±åŒåˆ›ä½œï¼Œæ¶µç›–23ç§è¯­è¨€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11092', 'title': 'Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting', 'url': 'https://huggingface.co/papers/2504.11092', 'abstract': 'Reconstructing 4D dynamic scenes from casually captured monocular videos is valuable but highly challenging, as each timestamp is observed from a single viewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular video synthesis by augmenting observation views - synthesizing multi-view videos from a monocular input. Unlike existing methods that either solely leverage geometric priors for supervision or use generative priors while overlooking geometry, we integrate both. This reformulates view augmentation as a video inpainting task, where observed views are warped into new viewpoints based on monocular depth priors. To achieve this, we train a video inpainting model on unposed web videos with synthetically generated masks that mimic warping occlusions, ensuring spatially and temporally consistent completion of missing regions. To further mitigate inaccuracies in monocular depth priors, we introduce an iterative view augmentation strategy and a robust reconstruction loss. Experiments demonstrate that our method effectively improves monocular 4D scene reconstruction and completion.', 'score': 4, 'issue_id': 3287, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'a6e019e03ced5592', 'authors': ['Jiaxin Huang', 'Sheng Miao', 'BangBnag Yang', 'Yuewen Ma', 'Yiyi Liao'], 'affiliations': ['ByteDance PICO', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11092.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Vivid4D: Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Vivid4D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ 4D-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ augmentĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ½ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ augmentĞ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Enhancing 4D Scene Reconstruction with Vivid4D', 'desc': 'The paper presents Vivid4D, a new method for reconstructing 4D dynamic scenes from single-view videos. It synthesizes multi-view videos by augmenting the observation views, addressing the challenges of limited viewpoints. The approach combines geometric and generative priors, treating view augmentation as a video inpainting task to fill in missing regions. By training on unposed web videos and using iterative strategies, Vivid4D enhances the accuracy of monocular depth priors and improves scene reconstruction quality.'}, 'zh': {'title': 'Vivid4Dï¼šä»å•ç›®è§†é¢‘é‡å»ºå››ç»´åŠ¨æ€åœºæ™¯çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVivid4Dçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å•ç›®è§†é¢‘ä¸­é‡å»ºå››ç»´åŠ¨æ€åœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆæˆå¤šè§†è§’è§†é¢‘æ¥å¢å¼ºå•ç›®è§†é¢‘åˆæˆï¼Œå…‹æœäº†ä»…ä»å•ä¸€è§†è§’è§‚å¯Ÿçš„é™åˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒVivid4DåŒæ—¶åˆ©ç”¨å‡ ä½•å…ˆéªŒå’Œç”Ÿæˆå…ˆéªŒï¼Œå°†è§†è§’å¢å¼ºé‡æ–°å®šä¹‰ä¸ºè§†é¢‘ä¿®å¤ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæé«˜äº†å•ç›®å››ç»´åœºæ™¯çš„é‡å»ºå’Œè¡¥å…¨æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09566', 'title': 'Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution', 'url': 'https://huggingface.co/papers/2504.09566', 'abstract': 'Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of "Module", "Betti numbers","Freeness", "Mapping", "Exactness" and "Minimality", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.', 'score': 2, 'issue_id': 3290, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': '24032586ed61676f', 'authors': ['Chenghao Li', 'Chaoning Zhang', 'Yi Lu', 'Jiaquan Zhang', 'Qigan Sun', 'Xudong Wang', 'Jiwei Wei', 'Guoqing Wang', 'Yang Yang', 'Heng Tao Shen'], 'affiliations': ['Capital Normal University, Beijing, China', 'Kyung Hee University, Yongin-si, Republic of Korea', 'Tongji University, Shanghai, China', 'University of Electronic Science and Technology of China, Chengdu, China', 'University of Liverpool, Liverpool, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2504.09566.jpg', 'data': {'categories': ['#math', '#inference', '#reasoning', '#training', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Syzygy of Thoughts: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Syzygy of Thoughts (SoT), Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Chain-of-Thought (CoT) Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SoT Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ĞµĞ¹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¼ÑƒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ñ‹ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ SoT Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ CoT.'}, 'en': {'title': 'Enhancing Reasoning with Syzygy of Thoughts', 'desc': 'This paper introduces a new framework called Syzygy of Thoughts (SoT) that builds on Chain-of-Thought (CoT) prompting to improve the reasoning capabilities of large language models (LLMs). SoT addresses the limitations of single reasoning chains by incorporating multiple interrelated reasoning paths, which helps in tackling complex tasks with broad solution spaces. By drawing inspiration from concepts in commutative algebra, such as Minimal Free Resolution, SoT systematically breaks down complex problems into simpler, manageable subproblems while maintaining essential features. The framework has been tested on various datasets and models, demonstrating improved inference accuracy and efficiency compared to traditional CoT methods.'}, 'zh': {'title': 'æ€ç»´çš„Syzygyï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ€ç»´çš„Syzygyï¼ˆSoTï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚SoTé€šè¿‡å¼•å…¥è¾…åŠ©çš„ã€ç›¸äº’å…³è”çš„æ¨ç†è·¯å¾„ï¼Œæ‰©å±•äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†äº¤æ¢ä»£æ•°å’Œä»£æ•°å‡ ä½•ä¸­çš„æœ€å°è‡ªç”±åˆ†è§£ï¼ˆMFRï¼‰ï¼Œé€šè¿‡ç³»ç»Ÿåœ°å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºé€»è¾‘ä¸Šå®Œæ•´çš„æœ€å°å­é—®é¢˜ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoTåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¨ç†å‡†ç¡®ç‡è¾¾åˆ°æˆ–è¶…è¿‡äº†ä¸»æµçš„é“¾å¼æ€ç»´æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09346', 'title': '"It\'s not a representation of me": Examining Accent Bias and Digital\n  Exclusion in Synthetic AI Voice Services', 'url': 'https://huggingface.co/papers/2504.09346', 'abstract': "Recent advances in artificial intelligence (AI) speech generation and voice cloning technologies have produced naturalistic speech and accurate voice replication, yet their influence on sociotechnical systems across diverse accents and linguistic traits is not fully understood. This study evaluates two synthetic AI voice services (Speechify and ElevenLabs) through a mixed methods approach using surveys and interviews to assess technical performance and uncover how users' lived experiences influence their perceptions of accent variations in these speech technologies. Our findings reveal technical performance disparities across five regional, English-language accents and demonstrate how current speech generation technologies may inadvertently reinforce linguistic privilege and accent-based discrimination, potentially creating new forms of digital exclusion. Overall, our study highlights the need for inclusive design and regulation by providing actionable insights for developers, policymakers, and organizations to ensure equitable and socially responsible AI speech technologies.", 'score': 1, 'issue_id': 3294, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 12', 'zh': '4æœˆ12æ—¥'}, 'hash': 'c5df8b667242e0e1', 'authors': ['Shira Michel', 'Sufi Kaur', 'Sarah Elizabeth Gillespie', 'Jeffrey Gleason', 'Christo Wilson', 'Avijit Ghosh'], 'affiliations': ['Hugging Face and University of Connecticut, USA', 'Northeastern University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.09346.jpg', 'data': {'categories': ['#multimodal', '#healthcare', '#ethics', '#synthetic', '#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ˜Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² Ğ˜Ğ˜-Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ÑĞµÑ€Ğ²Ğ¸ÑĞ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜ (Speechify Ğ¸ ElevenLabs) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ‚Ğ¸Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ…. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿ÑÑ‚Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ñƒ.'}, 'en': {'title': 'Ensuring Fairness in AI Speech: Addressing Accent Bias and Digital Exclusion', 'desc': 'This paper investigates the impact of AI speech generation and voice cloning technologies on social systems, focusing on how different accents are represented. It evaluates two AI voice services, Speechify and ElevenLabs, using surveys and interviews to understand user experiences and perceptions of accent variations. The study finds that there are significant differences in technical performance across various English accents, which may lead to accent-based discrimination and reinforce existing linguistic privileges. The authors emphasize the importance of inclusive design and regulation to promote fairness and accessibility in AI speech technologies.'}, 'zh': {'title': 'ç¡®ä¿AIè¯­éŸ³æŠ€æœ¯çš„å…¬å¹³ä¸åŒ…å®¹', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½è¯­éŸ³ç”Ÿæˆå’Œå£°éŸ³å…‹éš†æŠ€æœ¯å¯¹ç¤¾ä¼šæŠ€æœ¯ç³»ç»Ÿçš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒå£éŸ³å’Œè¯­è¨€ç‰¹å¾æ–¹é¢ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§åˆæˆAIè¯­éŸ³æœåŠ¡ï¼ˆSpeechifyå’ŒElevenLabsï¼‰ï¼Œé€šè¿‡è°ƒæŸ¥å’Œè®¿è°ˆçš„æ–¹æ³•æ¥åˆ†æå…¶æŠ€æœ¯æ€§èƒ½ï¼Œå¹¶äº†è§£ç”¨æˆ·çš„ç”Ÿæ´»ç»å†å¦‚ä½•å½±å“ä»–ä»¬å¯¹å£éŸ³å˜åŒ–çš„çœ‹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒåœ°åŒºçš„è‹±è¯­å£éŸ³åœ¨æŠ€æœ¯æ€§èƒ½ä¸Šå­˜åœ¨å·®å¼‚ï¼Œè¿™å¯èƒ½æ— æ„ä¸­åŠ å‰§äº†è¯­è¨€ç‰¹æƒå’ŒåŸºäºå£éŸ³çš„æ­§è§†ï¼Œå¯¼è‡´æ–°çš„æ•°å­—æ’æ–¥ç°è±¡ã€‚æ€»ä½“è€Œè¨€ï¼Œç ”ç©¶å¼ºè°ƒäº†åŒ…å®¹æ€§è®¾è®¡å’Œç›‘ç®¡çš„å¿…è¦æ€§ï¼Œä¸ºå¼€å‘è€…ã€æ”¿ç­–åˆ¶å®šè€…å’Œç»„ç»‡æä¾›äº†å¯è¡Œçš„è§è§£ï¼Œä»¥ç¡®ä¿AIè¯­éŸ³æŠ€æœ¯çš„å…¬å¹³æ€§å’Œç¤¾ä¼šè´£ä»»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09048', 'title': 'BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via\n  Adaptive Block-Based Gaussian Splatting', 'url': 'https://huggingface.co/papers/2504.09048', 'abstract': 'The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, a novel framework incorporating a content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality large-scale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the ground-truth supervision, which enhances the reconstruction quality. Furthermore, we propose a pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on large-scale scenes demonstrate that our approach achieves state-of-the-art performance in both reconstruction efficiency and rendering quality, with a 5x speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on a single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian', 'score': 1, 'issue_id': 3291, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 12', 'zh': '4æœˆ12æ—¥'}, 'hash': '3f4a6ef28e699ddc', 'authors': ['Yongchang Wu', 'Zipeng Qi', 'Zhenwei Shi', 'Zhengxia Zou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.09048.jpg', 'data': {'categories': ['#3d', '#benchmark'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ° ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BlockGaussian - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ° ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 5-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ PSNR Ğ½Ğ° 1.21 Ğ´Ğ‘ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient Large-Scale Scene Reconstruction with BlockGaussian', 'desc': 'This paper presents BlockGaussian, a new framework for improving large-scale scene reconstruction using 3D Gaussian Splatting. It introduces a content-aware partitioning strategy that considers the complexity of different scene regions, allowing for better optimization and merging of blocks. The framework also addresses supervision mismatch by using auxiliary points to align with ground-truth data, which enhances the overall reconstruction quality. Experimental results show that BlockGaussian achieves faster optimization and better rendering quality, making it feasible to perform large-scale reconstructions on devices with limited memory.'}, 'zh': {'title': 'é«˜æ•ˆå¤§è§„æ¨¡åœºæ™¯é‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBlockGaussiançš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡åœºæ™¯é‡å»ºçš„æ•ˆç‡å’Œè´¨é‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å†…å®¹æ„ŸçŸ¥çš„åœºæ™¯åˆ†åŒºç­–ç•¥å’Œå¯è§æ€§æ„ŸçŸ¥çš„å—ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥åº”å¯¹åœºæ™¯åˆ†åŒºã€ä¼˜åŒ–å’Œåˆå¹¶è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥è¾…åŠ©ç‚¹æ¥è§£å†³ç‹¬ç«‹å—ä¼˜åŒ–ä¸­çš„ç›‘ç£ä¸åŒ¹é…é—®é¢˜ï¼Œè¿›ä¸€æ­¥æå‡äº†é‡å»ºè´¨é‡ã€‚æ­¤å¤–ï¼Œæå‡ºçš„ä¼ªè§†å›¾å‡ ä½•çº¦æŸæœ‰æ•ˆå‡å°‘äº†å—åˆå¹¶è¿‡ç¨‹ä¸­å› ç©ºæ°”æµ®åŠ¨é€ æˆçš„æ¸²æŸ“é™çº§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23461', 'title': 'TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes', 'url': 'https://huggingface.co/papers/2503.23461', 'abstract': 'This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.', 'score': 59, 'issue_id': 2995, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': '00cccb2000a01b76', 'authors': ['Nikai Du', 'Zhennan Chen', 'Zhizhou Chen', 'Shan Gao', 'Xi Chen', 'Zhengkai Jiang', 'Jian Yang', 'Ying Tai'], 'affiliations': ['China Mobile', 'Nanjing University', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.23461.jpg', 'data': {'categories': ['#cv', '#dataset', '#benchmark'], 'emoji': 'ğŸ“', 'ru': {'title': 'TextCrafter: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ TextCrafter Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ĞµĞ¼. TextCrafter Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ¾ĞºÑƒÑĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CVTG-2K Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… CVTG.'}, 'en': {'title': 'TextCrafter: Mastering Complex Visual Text Generation', 'desc': 'This paper introduces Complex Visual Text Generation (CVTG), which involves creating detailed text that appears in various parts of images. The authors identify common issues in existing models, such as producing unclear or incomplete visual text. To improve this, they present TextCrafter, a new method that breaks down complex text into manageable parts and ensures that the text aligns well with the images. Additionally, TextCrafter enhances the visibility of the text during generation and is evaluated using a new dataset called CVTG-2K, showing superior performance compared to current methods.'}, 'zh': {'title': 'TextCrafterï¼šæå‡å¤æ‚è§†è§‰æ–‡æœ¬ç”Ÿæˆçš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤æ‚è§†è§‰æ–‡æœ¬ç”Ÿæˆï¼ˆCVTGï¼‰ä»»åŠ¡ï¼Œä¸»è¦å…³æ³¨åœ¨è§†è§‰å›¾åƒä¸­ç”Ÿæˆåˆ†å¸ƒåœ¨ä¸åŒåŒºåŸŸçš„å¤æ‚æ–‡æœ¬å†…å®¹ã€‚åœ¨CVTGä¸­ï¼Œå›¾åƒç”Ÿæˆæ¨¡å‹å¸¸å¸¸ä¼šæ¸²æŸ“å‡ºæ‰­æ›²ã€æ¨¡ç³Šçš„è§†è§‰æ–‡æœ¬æˆ–é—æ¼æŸäº›è§†è§‰æ–‡æœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TextCrafterï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¤šè§†è§‰æ–‡æœ¬æ¸²æŸ“æ–¹æ³•ã€‚TextCrafteré€šè¿‡é€æ­¥ç­–ç•¥å°†å¤æ‚è§†è§‰æ–‡æœ¬åˆ†è§£ä¸ºä¸åŒç»„ä»¶ï¼ŒåŒæ—¶ç¡®ä¿æ–‡æœ¬å†…å®¹ä¸å…¶è§†è§‰è½½ä½“ä¹‹é—´çš„å¼ºå¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23307', 'title': 'MoCha: Towards Movie-Grade Talking Character Synthesis', 'url': 'https://huggingface.co/papers/2503.23307', 'abstract': 'Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.', 'score': 44, 'issue_id': 2994, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': '6ce9b3642bf3ace3', 'authors': ['Cong Wei', 'Bo Sun', 'Haoyu Ma', 'Ji Hou', 'Felix Juefei-Xu', 'Zecheng He', 'Xiaoliang Dai', 'Luxin Zhang', 'Kunpeng Li', 'Tingbo Hou', 'Animesh Sinha', 'Peter Vajda', 'Wenhu Chen'], 'affiliations': ['GenAI, Meta', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23307.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#video', '#benchmark', '#story_generation'], 'emoji': 'ğŸ­', 'ru': {'title': 'MoCha: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MoCha Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ°Ñ€Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Character Animation with MoCha', 'desc': "This paper presents MoCha, a novel approach for generating talking character animations from speech and text, focusing on full character portrayal rather than just facial movements. It introduces a speech-video window attention mechanism to ensure that the generated video aligns accurately with the spoken words. To tackle the challenge of limited speech-labeled video datasets, the authors propose a joint training strategy that utilizes both speech and text-labeled data, enhancing the model's ability to generalize across various character actions. Additionally, structured prompt templates with character tags allow for multi-character dialogues, enabling AI-generated characters to engage in coherent conversations, thus advancing the field of automated cinematic storytelling."}, 'zh': {'title': 'ä¼šè¯´è¯çš„è§’è‰²ï¼šAIç”Ÿæˆç”µå½±å™äº‹çš„æ–°æ ‡å‡†', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼Œç§°ä¸ºâ€œä¼šè¯´è¯çš„è§’è‰²â€ï¼Œæ—¨åœ¨ä»è¯­éŸ³å’Œæ–‡æœ¬ç›´æ¥ç”Ÿæˆè§’è‰²åŠ¨ç”»ã€‚ä¸ä¼ ç»Ÿçš„â€œè¯´è¯å¤´â€ä¸åŒï¼Œè¿™ç§æ–¹æ³•ç”Ÿæˆçš„ä¸ä»…ä»…æ˜¯é¢éƒ¨è¡¨æƒ…ï¼Œè€Œæ˜¯å®Œæ•´çš„è§’è‰²å½¢è±¡ã€‚æˆ‘ä»¬æå‡ºäº†MoChaï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿç”Ÿæˆä¼šè¯´è¯è§’è‰²çš„æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§è¯­éŸ³-è§†é¢‘çª—å£æ³¨æ„æœºåˆ¶ï¼Œä»¥ç¡®ä¿è§†é¢‘ä¸è¯­éŸ³çš„ç²¾ç¡®åŒæ­¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ç»“æ„åŒ–çš„æç¤ºæ¨¡æ¿ï¼Œä½¿å¾—å¤šä¸ªè§’è‰²èƒ½å¤Ÿè¿›è¡ŒåŸºäºå›åˆçš„å¯¹è¯ï¼Œä»è€Œå®ç°æ›´å…·ç”µå½±æ„Ÿçš„æƒ…å¢ƒå¯¹è¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24290', 'title': 'Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model', 'url': 'https://huggingface.co/papers/2503.24290', 'abstract': 'We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.', 'score': 35, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'c3cd649c5eb9d423', 'authors': ['Jingcheng Hu', 'Yinmin Zhang', 'Qi Han', 'Daxin Jiang', 'Xiangyu Zhang', 'Heung-Yeung Shum'], 'affiliations': ['StepFun', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24290.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#training', '#rl', '#open_source', '#benchmark', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Open-Reasoner-Zero - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ vanilla PPO Ğ¸ GAE, Ğ±ĞµĞ· KL-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° DeepSeek-R1-Zero Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… AIME2024, MATH500 Ğ¸ GPQA Diamond, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ² 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ ĞºĞ¾Ğ´, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Simplifying Large-Scale Reasoning in RL with Open-Reasoner-Zero', 'desc': 'Open-Reasoner-Zero is an innovative open-source framework designed for large-scale reasoning-oriented reinforcement learning (RL) training. It utilizes a simple approach with vanilla Proximal Policy Optimization (PPO) and Generalized Advantage Estimation (GAE), achieving impressive results without complex regularization techniques. The framework demonstrates that minimalistic strategies can effectively enhance response length and benchmark performance, outperforming previous models like DeepSeek-R1-Zero. By releasing all components of the project, including source code and model weights, Open-Reasoner-Zero promotes accessibility and collaboration in the machine learning community.'}, 'zh': {'title': 'å¼€æºæ¨ç†å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆå®ç°', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Open-Reasoner-Zeroï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æºçš„å¤§è§„æ¨¡æ¨ç†å¯¼å‘å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®ç°ï¼Œé‡ç‚¹å…³æ³¨å¯æ‰©å±•æ€§ã€ç®€æ´æ€§å’Œå¯è®¿é—®æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨ç®€å•çš„PPOç®—æ³•å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å“åº”é•¿åº¦å’ŒåŸºå‡†æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®ç°ä¸DeepSeek-R1-Zeroä½¿ç”¨ç›¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨AIME2024ã€MATH500å’ŒGPQA DiamondåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶è®­ç»ƒæ•ˆç‡æ˜¾è‘—æé«˜ï¼Œä»…éœ€DeepSeek-R1-Zeroç®¡é“çš„ååˆ†ä¹‹ä¸€è®­ç»ƒæ­¥éª¤ã€‚ä¸ºäº†æ”¯æŒå¼€æºç²¾ç¥ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æºä»£ç ã€å‚æ•°è®¾ç½®ã€è®­ç»ƒæ•°æ®å’Œä¸åŒè§„æ¨¡çš„æ¨¡å‹æƒé‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24235', 'title': 'What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.24235', 'abstract': "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.", 'score': 34, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '21674468fcc8c7d5', 'authors': ['Qiyuan Zhang', 'Fuyuan Lyu', 'Zexu Sun', 'Lei Wang', 'Weixu Zhang', 'Zhihan Guo', 'Yufei Wang', 'Irwin King', 'Xue Liu', 'Chen Ma'], 'affiliations': ['Chinese University of Hong Kong', 'City University of Hong Kong', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Macquarie University', 'McGill University & MILA', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24235.jpg', 'data': {'categories': ['#survey', '#math', '#reasoning', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (TTS) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ TTS-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, Ğ³Ğ´Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ TTS. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ TTS Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Potential: The Power of Test-Time Scaling in LLMs', 'desc': 'This paper discusses the emerging focus on test-time scaling (TTS) in machine learning, particularly for large language models (LLMs). TTS enhances the problem-solving abilities of LLMs in various tasks, including specialized reasoning and open-ended questions. The authors propose a comprehensive framework that categorizes TTS research into four dimensions: what, how, where, and how well to scale. They also review existing methods and applications, highlight challenges, and suggest future research directions to improve TTS effectiveness.'}, 'zh': {'title': 'æµ‹è¯•æ—¶æ‰©å±•ï¼šæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›', 'desc': 'éšç€å¯¹é¢„è®­ç»ƒæ—¶ä»£è®¡ç®—è§„æ¨¡ï¼ˆæ•°æ®å’Œå‚æ•°ï¼‰çš„çƒ­æƒ…é€æ¸å‡é€€ï¼Œæµ‹è¯•æ—¶æ‰©å±•ï¼ˆTTSï¼‰æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶ç„¦ç‚¹ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒTTSå¯ä»¥è¿›ä¸€æ­¥æ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œåœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰ä¸“ä¸šæ¨ç†ä»»åŠ¡ä»¥åŠå¼€æ”¾å¼é—®ç­”ç­‰ä¸€èˆ¬ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çªç ´ã€‚å°½ç®¡è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶è¿…é€Ÿå¢åŠ ï¼Œä½†ä»è¿«åˆ‡éœ€è¦ä¸€é¡¹å…¨é¢çš„è°ƒæŸ¥ï¼Œä»¥æä¾›ç³»ç»Ÿçš„ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šç»´æ¡†æ¶ï¼Œæ¶µç›–TTSç ”ç©¶çš„å››ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå¹¶å¯¹æ–¹æ³•ã€åº”ç”¨åœºæ™¯å’Œè¯„ä¼°æ–¹é¢è¿›è¡Œäº†å¹¿æ³›çš„å›é¡¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24388', 'title': 'RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy', 'url': 'https://huggingface.co/papers/2503.24388', 'abstract': 'Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.', 'score': 22, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '98b80967d4757be2', 'authors': ['Zhonghan Zhao', 'Wenwei Zhang', 'Haian Huang', 'Kuikun Liu', 'Jianfei Gao', 'Gaoang Wang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24388.jpg', 'data': {'categories': ['#training', '#reasoning', '#agents', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ RIG (Reasoning and Imagination in Generalist policy). RIG Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° RIG ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¾Ğ¿ĞµÑ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ°.'}, 'en': {'title': 'Synergizing Reasoning and Imagination for Enhanced Agent Performance', 'desc': 'This paper introduces RIG, a novel end-to-end Generalist policy that combines reasoning and imagination for embodied agents in complex environments. By integrating these two capabilities, RIG enhances learning efficiency and generalization, overcoming limitations of previous models that focused on only one aspect. The authors developed a data pipeline that enriches trajectories from existing agents, allowing for joint learning of reasoning and next image generation. Experimental results demonstrate that RIG significantly improves sample efficiency and robustness, enabling agents to self-correct actions based on imagined outcomes before execution.'}, 'zh': {'title': 'æ¨ç†ä¸æƒ³è±¡çš„ååŒæå‡æ™ºèƒ½ä½“èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRIGçš„é€šç”¨ç­–ç•¥ï¼Œé¦–æ¬¡å°†æ¨ç†å’Œæƒ³è±¡èƒ½åŠ›ç»“åˆåœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“ä¸­ã€‚é€šè¿‡æ„å»ºæ•°æ®ç®¡é“ï¼Œé€æ­¥æ•´åˆå’Œä¸°å¯Œä»ç°æœ‰æ™ºèƒ½ä½“æ”¶é›†çš„è½¨è¿¹ä¸­çš„æ¨ç†å’Œæƒ³è±¡å†…å®¹ï¼ŒRIGå®ç°äº†æ›´é«˜çš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚è”åˆå­¦ä¹ æ¨ç†å’Œä¸‹ä¸€å›¾åƒç”Ÿæˆï¼Œæ˜ç¡®å»ºæ¨¡äº†æ¨ç†ã€è¡ŒåŠ¨å’Œç¯å¢ƒåŠ¨æ€ä¹‹é—´çš„å†…åœ¨å…³è”ï¼Œä½¿å¾—æ ·æœ¬æ•ˆç‡æé«˜äº†17å€ä»¥ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†ä¸æƒ³è±¡çš„ååŒä½œç”¨ä¸ä»…å¢å¼ºäº†é€šç”¨ç­–ç•¥çš„é²æ£’æ€§å’Œäº’æ“ä½œæ€§ï¼Œè¿˜æå‡äº†æ•´ä½“æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24370', 'title': 'Effectively Controlling Reasoning Models through Thinking Intervention', 'url': 'https://huggingface.co/papers/2503.24370', 'abstract': 'Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.', 'score': 13, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '5f218f08538c601f', 'authors': ['Tong Wu', 'Chong Xiang', 'Jiachen T. Wang', 'Prateek Mittal'], 'affiliations': ['NVIDIA', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24370.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#architecture', '#open_source', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Thinking Intervention'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."}, 'en': {'title': 'Enhancing LLM Reasoning with Thinking Intervention', 'desc': 'This paper introduces a new method called Thinking Intervention, which enhances the reasoning capabilities of large language models (LLMs) by allowing explicit control over their internal thought processes. By inserting or modifying specific reasoning tokens, the model can generate more accurate and contextually relevant answers. The authors conducted extensive tests on various tasks, showing that this approach leads to significant improvements in performance, such as higher accuracy in following instructions and better handling of unsafe prompts. Overall, the findings suggest that Thinking Intervention provides a valuable framework for refining how LLMs reason and respond to complex queries.'}, 'zh': {'title': 'æ€ç»´å¹²é¢„ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ€ç»´å¹²é¢„ï¼ˆThinking Interventionï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ’å…¥æˆ–ä¿®æ”¹ç‰¹å®šçš„æ€ç»´æ ‡è®°æ¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­èƒ½å¤Ÿæ›´å¥½åœ°ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ€ç»´å¹²é¢„æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æç¤ºæ–¹æ³•ï¼Œå°¤å…¶åœ¨æŒ‡ä»¤éµå¾ªå’Œæ¨ç†å±‚æ¬¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ§åˆ¶æ¨ç†è¿‡ç¨‹ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23284', 'title': 'SketchVideo: Sketch-based Video Generation and Editing', 'url': 'https://huggingface.co/papers/2503.23284', 'abstract': "Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.", 'score': 13, 'issue_id': 2996, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'd968c1da27effa84', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#multimodal', '#optimization', '#games', '#video'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞºĞµÑ‚Ñ‡Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞµÑ‚Ñ‡ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞºĞµÑ‚Ñ‡ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ DiT. Ğ”Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑĞºĞµÑ‚Ñ‡Ğ° Ğ½Ğ° Ğ²ÑĞµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Sketch Your Way to Better Video Control!', 'desc': 'This paper presents a novel approach to video generation and editing using sketch-based controls. It addresses the challenges of accurately managing global layout and motion details by introducing a memory-efficient control structure that utilizes sketch control blocks. The method allows users to draw sketches on keyframes, which are then analyzed through an inter-frame attention mechanism to ensure consistency across all video frames. The proposed system, SketchVideo, enhances the ability to generate and edit videos while preserving the integrity of unedited regions, demonstrating superior performance in controllable video tasks.'}, 'zh': {'title': 'è‰å›¾é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºè‰å›¾çš„ç©ºé—´å’Œè¿åŠ¨æ§åˆ¶åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬å’Œå›¾åƒæ¡ä»¶ä¸‹çš„å¸ƒå±€å’Œå‡ ä½•ç»†èŠ‚æ§åˆ¶é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ§åˆ¶ç»“æ„ï¼Œåˆ©ç”¨è‰å›¾æ§åˆ¶å—é¢„æµ‹è·³è¿‡çš„DiTå—çš„æ®‹å·®ç‰¹å¾ã€‚é€šè¿‡åœ¨å…³é”®å¸§ä¸Šç»˜åˆ¶è‰å›¾ï¼Œå¹¶ä½¿ç”¨è·¨å¸§æ³¨æ„æœºåˆ¶åˆ†æå…³é”®å¸§ä¸æ¯ä¸ªè§†é¢‘å¸§ä¹‹é—´çš„å…³ç³»ï¼Œå®ç°äº†æ—¶é—´ä¸Šç¨€ç–çš„è‰å›¾æ¡ä»¶ä¼ æ’­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SketchVideoåœ¨å¯æ§è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.23077', 'title': 'Efficient Inference for Large Reasoning Models: A Survey', 'url': 'https://huggingface.co/papers/2503.23077', 'abstract': "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant fieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.", 'score': 13, 'issue_id': 2995, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': 'f76d7ead3d85b37e', 'authors': ['Yue Liu', 'Jiaying Wu', 'Yufei He', 'Hongcheng Gao', 'Hongyu Chen', 'Baolong Bi', 'Jiaheng Zhang', 'Zhiqi Huang', 'Bryan Hooi'], 'affiliations': ['Beijing Jiaotong University', 'Moonshot', 'National University of Singapore', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.23077.jpg', 'data': {'categories': ['#reasoning', '#survey', '#interpretability', '#optimization', '#inference', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: ÑĞ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses Large Reasoning Models (LRMs) that enhance the reasoning capabilities of Large Language Models (LLMs) but face challenges like inefficient token usage and high memory consumption. It reviews various efficient inference methods aimed at reducing these inefficiencies while maintaining reasoning quality. The authors categorize these methods into explicit and implicit Chain-of-Thought approaches, analyzing their strengths and weaknesses. Additionally, the paper identifies ongoing challenges in the field and suggests strategies for improving inference efficiency in LRMs.'}, 'zh': {'title': 'æå‡æ¨ç†æ•ˆç‡ï¼Œä¼˜åŒ–å¤§å‹æ¨ç†æ¨¡å‹', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡å­¦ä¹ æ¨ç†æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹å¯¼è‡´äº†ä»¤ç‰Œä½¿ç”¨ã€å†…å­˜æ¶ˆè€—å’Œæ¨ç†æ—¶é—´çš„ä½æ•ˆã€‚å› æ­¤ï¼Œæœ¬æ–‡ç»¼è¿°äº†ä¸“é—¨ä¸ºLRMsè®¾è®¡çš„é«˜æ•ˆæ¨ç†æ–¹æ³•ï¼Œé‡ç‚¹åœ¨äºå‡è½»ä»¤ç‰Œä½æ•ˆï¼ŒåŒæ—¶ä¿æŒæ¨ç†è´¨é‡ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ†ç±»æ³•ï¼Œå°†æœ€è¿‘çš„æ–¹æ³•åˆ†ä¸ºä¸¤å¤§ç±»ï¼šæ˜¾å¼ç´§å‡‘çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œéšå¼æ½œåœ¨çš„æ€ç»´é“¾ï¼Œè®¨è®ºäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶åˆ†æäº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24364', 'title': 'Query and Conquer: Execution-Guided SQL Generation', 'url': 'https://huggingface.co/papers/2503.24364', 'abstract': 'We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.', 'score': 12, 'issue_id': 2997, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '2af26722887e77ac', 'authors': ['Åukasz Borchmann', 'Marek Wydmuch'], 'affiliations': ['Snowflake AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24364.jpg', 'data': {'categories': ['#optimization', '#small_models', '#dataset', '#inference', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² SQL. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼, ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº o1, o3-mini Ğ¸ DeepSeek R1, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 30 Ñ€Ğ°Ğ·. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL.'}, 'en': {'title': 'Efficient SQL Generation: Small Models, Big Results!', 'desc': 'This paper introduces a new method for generating SQL queries from natural language, enhancing accuracy in text-to-SQL tasks. The approach utilizes execution results to choose the best query from several options based on semantic consistency. It allows smaller models to outperform larger, more complex models while also cutting down inference costs significantly. This method is designed to work well with current models, making it a practical solution for improving SQL generation.'}, 'zh': {'title': 'é«˜æ•ˆç”ŸæˆSQLï¼Œæå‡æ–‡æœ¬åˆ°SQLçš„å‡†ç¡®æ€§', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¤æ‚è¾“å‡ºï¼Œæ˜¾è‘—æé«˜æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰§è¡Œç»“æœï¼Œä»å¤šä¸ªå€™é€‰æŸ¥è¯¢ä¸­é€‰æ‹©æœ€ç¬¦åˆè¯­ä¹‰çš„ä¸€é¡¹ï¼Œä½¿å¾—è¾ƒå°ã€æˆæœ¬æ•ˆç›Šé«˜çš„æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šè®¡ç®—å¯†é›†å‹çš„æ¨ç†æ–¹æ³•ï¼Œå¦‚o1ã€o3-miniå’ŒDeepSeek R1ï¼ŒåŒæ—¶å°†æ¨ç†æˆæœ¬é™ä½å¤šè¾¾30å€ã€‚å®ƒä¸ç°æœ‰æ¨¡å‹æ— ç¼é›†æˆï¼Œæä¾›äº†ä¸€æ¡å®ç”¨ä¸”å¯æ‰©å±•çš„é€šå¾€æœ€å…ˆè¿›SQLç”Ÿæˆçš„è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23829', 'title': 'Expanding RL with Verifiable Rewards Across Diverse Domains', 'url': 'https://huggingface.co/papers/2503.23829', 'abstract': "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.", 'score': 12, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '2c875f8335892dfd', 'authors': ['Yi Su', 'Dian Yu', 'Linfeng Song', 'Juntao Li', 'Haitao Mi', 'Zhaopeng Tu', 'Min Zhang', 'Dong Yu'], 'affiliations': ['Soochow University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.23829.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rlhf', '#training', '#rl', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RLVR: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ°, Ñ…Ğ¸Ğ¼Ğ¸Ñ, Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ğ² Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑĞ³ĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµĞ¶Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Expanding RLVR: From Coding to Real-World Applications', 'desc': "This paper explores the use of Reinforcement Learning with Verifiable Rewards (RLVR) in diverse fields like medicine and economics, where traditional structured answers may not be available. The authors find that different large language models (LLMs) often agree on binary judgments when objective reference answers are present, suggesting that extensive annotations may not be necessary for training reward models. To enhance RLVR's effectiveness, they introduce model-based soft scoring to better handle unstructured reference answers. Their experiments demonstrate that a distilled generative reward model can effectively verify rewards across domains, leading to improved performance of RL policies compared to existing state-of-the-art models."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ çš„å¯éªŒè¯å¥–åŠ±ï¼šè·¨é¢†åŸŸåº”ç”¨çš„æ–°å¯èƒ½', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦æ¨ç†å’Œç¼–ç ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨å°šæœªæ·±å…¥æ¢ç´¢ã€‚æˆ‘ä»¬ç ”ç©¶äº†RLVRåœ¨åŒ»å­¦ã€åŒ–å­¦ã€å¿ƒç†å­¦å’Œç»æµå­¦ç­‰å¤šæ ·åŒ–é¢†åŸŸçš„æ‰©å±•ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å­˜åœ¨å®¢è§‚å‚è€ƒç­”æ¡ˆæ—¶ï¼Œä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äºŒå…ƒåˆ¤æ–­ä¸Šé«˜åº¦ä¸€è‡´ï¼Œè¿™è¡¨æ˜è®­ç»ƒç‰¹å®šé¢†åŸŸå¥–åŠ±æ¨¡å‹ä¸ä¸€å®šéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨ã€‚é€šè¿‡å°†åŸºäºæ¨¡å‹çš„è½¯è¯„åˆ†çº³å…¥RLVRï¼Œæˆ‘ä»¬æé«˜äº†å…¶çµæ´»æ€§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è’¸é¦ç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨è·¨é¢†åŸŸéªŒè¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19901', 'title': 'TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization', 'url': 'https://huggingface.co/papers/2503.19901', 'abstract': 'Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/', 'score': 12, 'issue_id': 3000, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'c5dd40417e2f952a', 'authors': ['Liang Pan', 'Zeshi Yang', 'Zhiyang Dou', 'Wenjia Wang', 'Buzhen Huang', 'Bo Dai', 'Taku Komura', 'Jingbo Wang'], 'affiliations': ['Feeling AI', 'Independent Researcher', 'Shanghai AI Laboratory', 'Southeast University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19901.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#transfer_learning', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'TokenHSI - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹ (HSI). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸Ğ¾Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ° ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞµĞ³Ğ¾ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… HSI.'}, 'en': {'title': 'Unifying Skills for Enhanced Human-Scene Interactions', 'desc': 'This paper introduces TokenHSI, a novel transformer-based policy designed to enhance Human-Scene Interactions (HSI) by integrating multiple skills into a single framework. Unlike traditional methods that rely on separate controllers for each task, TokenHSI utilizes a shared token for humanoid proprioception, allowing for effective knowledge sharing across different interaction tasks. The architecture supports variable length inputs, making it adaptable to new scenarios and capable of coordinating complex actions, such as sitting while carrying an object. Experimental results show that TokenHSI significantly improves the versatility and adaptability of HSI tasks, paving the way for more sophisticated applications in computer animation and embodied AI.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæŠ€èƒ½çš„äººç±»åœºæ™¯äº¤äº’ç­–ç•¥', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTokenHSIçš„ç»Ÿä¸€å˜æ¢å™¨åŸºç¡€ç­–ç•¥ï¼Œç”¨äºåˆæˆå¤šæ ·ä¸”ç‰©ç†ä¸Šåˆç†çš„äººç±»åœºæ™¯äº¤äº’ï¼ˆHSIï¼‰ã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¸ºç‰¹å®šäº¤äº’ä»»åŠ¡å¼€å‘ç‹¬ç«‹æ§åˆ¶å™¨ï¼Œè¿™é™åˆ¶äº†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚TokenHSIé€šè¿‡å°†äººä½“æœ¬ä½“æ„ŸçŸ¥å»ºæ¨¡ä¸ºå…±äº«çš„ç‹¬ç«‹æ ‡è®°ï¼Œå¹¶ç»“åˆä¸åŒçš„ä»»åŠ¡æ ‡è®°ï¼Œä¿ƒè¿›äº†å¤šæŠ€èƒ½çš„ç»Ÿä¸€å’Œçµæ´»é€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§HSIä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†å¤šæ ·æ€§ã€é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24115', 'title': 'TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection', 'url': 'https://huggingface.co/papers/2503.24115', 'abstract': 'The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.', 'score': 9, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '61845428f5c3d9df', 'authors': ['Zhiming Ma', 'Peidong Wang', 'Minhua Huang', 'Jingpeng Wang', 'Kai Wu', 'Xiangzhao Lv', 'Yachun Pang', 'Yin Yang', 'Wenjie Tang', 'Yuchen Kang'], 'affiliations': ['China Mobile Internet Company Ltd. Guangzhou, Guangdong, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.24115.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#synthetic', '#open_source', '#benchmark', '#data'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TeleAntiFraud-28k - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚ĞµĞ»ĞµĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ASR Ğ¸ TTS, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 28,511 Ğ¿Ğ°Ñ€ Ñ€ĞµÑ‡ÑŒ-Ñ‚ĞµĞºÑÑ‚ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½ Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ TeleAntiFraud-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SFT.'}, 'en': {'title': 'Revolutionizing Telecom Fraud Detection with TeleAntiFraud-28k', 'desc': 'This paper introduces TeleAntiFraud-28k, a novel dataset designed to enhance telecom fraud detection by combining audio signals with textual analysis. The dataset is created using three innovative strategies, including privacy-preserved text generation from speech recordings and semantic enhancement through large language models. It consists of over 28,000 annotated speech-text pairs, enabling tasks like scenario classification and fraud detection. Additionally, the authors provide a benchmark for evaluating model performance and a fine-tuned model for practical applications, promoting further research in multimodal anti-fraud techniques.'}, 'zh': {'title': 'æ„å»ºç”µä¿¡æ¬ºè¯ˆæ£€æµ‹çš„æ–°åŸºçŸ³', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†TeleAntiFraud-28kæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºç”µä¿¡æ¬ºè¯ˆåˆ†æè®¾è®¡çš„å¼€æºéŸ³é¢‘-æ–‡æœ¬æ…¢æ€è€ƒæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸‰ç§ç­–ç•¥æ„å»ºï¼Œç¡®ä¿äº†æ•°æ®çš„éšç§ä¿æŠ¤å’ŒçœŸå®åœºæ™¯çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†TeleAntiFraud-Benchè¯„ä¼°åŸºå‡†ï¼Œä»¥ä¾¿ç³»ç»Ÿåœ°æµ‹è¯•æ¨¡å‹åœ¨ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æ­¤é¡¹å·¥ä½œä¸ºå¤šæ¨¡æ€åæ¬ºè¯ˆç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼ŒåŒæ—¶è§£å†³äº†æ•°æ®éšç§å’Œåœºæ™¯å¤šæ ·æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18809', 'title': 'Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code', 'url': 'https://huggingface.co/papers/2503.18809', 'abstract': 'In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit "reasoning" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs.', 'score': 9, 'issue_id': 2994, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '28288adc69a019ac', 'authors': ['Augusto B. CorrÃªa', 'AndrÃ© G. Pereira', 'Jendrik Seipp'], 'affiliations': ['Federal University of Rio Grande do Sul', 'LinkÃ¶ping University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.18809.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. LLM ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ² Ğ²Ğ¸Ğ´Ğµ Python-ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Empowering LLMs with Domain-Specific Heuristics for Better Planning', 'desc': 'This paper addresses the limitations of large language models (LLMs) in planning tasks, which often lead to incorrect and non-generalizable plans. The authors propose a novel approach where LLMs generate domain-specific heuristic functions in Python code, which are then evaluated using a greedy best-first search algorithm. Their method demonstrates that LLM-generated heuristics can outperform traditional domain-independent heuristics and compete with advanced learning algorithms in planning. The results indicate that these heuristics are not only efficient but also provide more informative guidance in certain planning domains.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è§„åˆ’èƒ½åŠ›', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§äººå·¥æ™ºèƒ½é—®é¢˜ä¸Šå±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨è¯¦ç»†å®šä¹‰è§„åˆ’ä»»åŠ¡çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬åœ¨è§„åˆ’æ–¹é¢ä»ç„¶ä¸å¯é ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨LLMsç”Ÿæˆæ­£ç¡®çš„è§„åˆ’ï¼Œå³ä½¿å¯¹äºè¶Šæ¥è¶Šå¤§çš„åˆ†å¸ƒå¤–ä»»åŠ¡ã€‚é€šè¿‡ç”Ÿæˆé¢†åŸŸç›¸å…³çš„å¯å‘å¼å‡½æ•°å¹¶åœ¨è´ªå©ªä¼˜å…ˆæœç´¢ä¸­è¯„ä¼°ï¼ŒLLMç”Ÿæˆçš„å¯å‘å¼å‡½æ•°åœ¨è§£å†³æœªè§æµ‹è¯•ä»»åŠ¡æ–¹é¢è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„é¢†åŸŸæ— å…³å¯å‘å¼æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22673', 'title': 'ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action Models', 'url': 'https://huggingface.co/papers/2503.22673', 'abstract': 'Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to facilitate research in the community.', 'score': 8, 'issue_id': 3011, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '32c8476678c711ba', 'authors': ['Jianguo Zhang', 'Thai Hoang', 'Ming Zhu', 'Zuxin Liu', 'Shiyu Wang', 'Tulika Awalgaonkar', 'Akshara Prabhakar', 'Haolin Chen', 'Weiran Yao', 'Zhiwei Liu', 'Juntao Tan', 'Juan Carlos Niebles', 'Shelby Heinecke', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.22673.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#agents', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ActionStudio: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ActionStudio - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LoRA Ğ¸ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ. ActionStudio Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Autonomous Agents with ActionStudio', 'desc': 'This paper introduces ActionStudio, a new framework designed to improve the training of large action models for autonomous agents. It addresses the challenges of diverse environments and complex data by providing a standardized format for agent trajectories. ActionStudio supports various training methods, including LoRA and full fine-tuning, and offers tools for data preprocessing and verification. The framework has been validated on both public and industry benchmarks, showing strong performance and scalability, and is open-sourced for community use.'}, 'zh': {'title': 'ActionStudioï¼šæå‡å¤§å‹åŠ¨ä½œæ¨¡å‹è®­ç»ƒçš„åˆ©å™¨', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ActionStudioï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”å¯æ‰©å±•çš„æ•°æ®å’Œè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå¤§å‹åŠ¨ä½œæ¨¡å‹çš„è®­ç»ƒã€‚ActionStudioé€šè¿‡æ ‡å‡†åŒ–æ ¼å¼ç»Ÿä¸€äº†ä¸åŒçš„ä»£ç†è½¨è¿¹ï¼Œæ”¯æŒå¤šç§è®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬LoRAã€å®Œå…¨å¾®è°ƒå’Œåˆ†å¸ƒå¼è®¾ç½®ã€‚è¯¥æ¡†æ¶è¿˜é›†æˆäº†å¼ºå¤§çš„é¢„å¤„ç†å’ŒéªŒè¯å·¥å…·ï¼Œä»¥æé«˜è®­ç»ƒçš„æ•ˆç‡å’Œå¯é æ€§ã€‚æˆ‘ä»¬åœ¨å…¬å…±å’Œå®é™…è¡Œä¸šåŸºå‡†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½å’Œå®ç”¨çš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21694', 'title': 'Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data', 'url': 'https://huggingface.co/papers/2503.21694', 'abstract': 'It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only 2.5% trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at https://github.com/theEricMa/TriplaneTurbo.', 'score': 8, 'issue_id': 3000, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '703f61255714367b', 'authors': ['Zhiyuan Ma', 'Xinyue Liang', 'Rongyuan Wu', 'Xiangyu Zhu', 'Zhen Lei', 'Lei Zhang'], 'affiliations': ['Center for Artificial Intelligence and Robotics, HKISI CAS', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, UCAS', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21694.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#3d', '#diffusion', '#open_source'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Progressive Rendering Distillation (PRD). PRD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Stable Diffusion Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TriplaneTurbo, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ° 1.2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹.'}, 'en': {'title': 'Fast and High-Quality 3D Mesh Generation from Text Prompts', 'desc': 'This paper introduces a new method called Progressive Rendering Distillation (PRD) to create high-quality 3D meshes from text prompts quickly. PRD allows training without needing 3D ground-truth data by using multi-view diffusion models to distill textures and geometries into 3D outputs. The method enhances the efficiency of the generation process, enabling the model to produce 3D meshes in just 1.2 seconds while maintaining high quality. The resulting model, TriplaneTurbo, shows significant improvements over previous text-to-3D generators in both speed and output quality.'}, 'zh': {'title': 'å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡3Dç½‘æ ¼çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ¡ˆï¼Œç§°ä¸ºæ¸è¿›æ¸²æŸ“è’¸é¦ï¼ˆPRDï¼‰ï¼Œæ—¨åœ¨ä»æ–‡æœ¬æç¤ºä¸­å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼ã€‚é€šè¿‡è’¸é¦å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ŒPRDæ¶ˆé™¤äº†å¯¹3DçœŸå®æ•°æ®çš„éœ€æ±‚ï¼Œä½¿å¾—è®­ç»ƒæ•°æ®çš„æ‰©å±•å˜å¾—æ›´åŠ å®¹æ˜“ã€‚è¯¥æ–¹æ³•åˆ©ç”¨U-Neté€æ­¥å»å™ªï¼Œå¹¶å°†å»å™ªåçš„æ½œåœ¨è¡¨ç¤ºè§£ç ä¸º3Dè¾“å‡ºï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚æœ€ç»ˆï¼ŒPRDè®­ç»ƒçš„TriplaneTurboç”Ÿæˆå™¨åœ¨æ•ˆç‡å’Œè´¨é‡ä¸Šå‡ä¼˜äºä¹‹å‰çš„æ–‡æœ¬åˆ°3Dç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿåœ¨1.2ç§’å†…ç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.24391', 'title': 'Easi3R: Estimating Disentangled Motion from DUSt3R Without Training', 'url': 'https://huggingface.co/papers/2503.24391', 'abstract': 'Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major bottleneck for training a highly generalizable 4D model. This constraint has driven conventional 4D methods to fine-tune 3D models on scalable dynamic video data with additional geometric priors such as optical flow and depths. In this work, we take an opposite path and introduce Easi3R, a simple yet efficient training-free method for 4D reconstruction. Our approach applies attention adaptation during inference, eliminating the need for from-scratch pre-training or network fine-tuning. We find that the attention layers in DUSt3R inherently encode rich information about camera and object motion. By carefully disentangling these attention maps, we achieve accurate dynamic region segmentation, camera pose estimation, and 4D dense point map reconstruction. Extensive experiments on real-world dynamic videos demonstrate that our lightweight attention adaptation significantly outperforms previous state-of-the-art methods that are trained or finetuned on extensive dynamic datasets. Our code is publicly available for research purpose at https://easi3r.github.io/', 'score': 3, 'issue_id': 3001, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '48e601b0440aa5d3', 'authors': ['Xingyu Chen', 'Yue Chen', 'Yuliang Xiu', 'Andreas Geiger', 'Anpei Chen'], 'affiliations': ['Max Planck Institute for Intelligent Systems', 'University of Tubingen, Tubingen AI Center', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24391.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#open_source', '#3d', '#video'], 'emoji': 'ğŸŒŸ', 'ru': {'title': 'Easi3R: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Easi3R Ğ´Ğ»Ñ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DUSt3R Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. Easi3R Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ 4D ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Easi3R Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ….'}, 'en': {'title': 'Easi3R: Efficient 4D Reconstruction Without Pre-training', 'desc': 'This paper presents Easi3R, a novel method for 4D reconstruction that does not require extensive pre-training or fine-tuning of models. Instead, it utilizes attention adaptation during inference to effectively segment dynamic regions and estimate camera poses. The authors leverage the inherent information encoded in the attention layers of the DUSt3R model, allowing for accurate reconstruction of dense point clouds in dynamic scenes. Experimental results show that Easi3R outperforms existing methods that rely on large dynamic datasets, highlighting its efficiency and effectiveness.'}, 'zh': {'title': 'Easi3Rï¼šæ— éœ€è®­ç»ƒçš„é«˜æ•ˆ4Dé‡å»ºæ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºEasi3Rçš„4Dé‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€é¢„è®­ç»ƒæˆ–å¾®è°ƒç½‘ç»œï¼Œåˆ©ç”¨æ³¨æ„åŠ›é€‚åº”æŠ€æœ¯è¿›è¡Œæ¨ç†ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡åŠ¨æ€è§†é¢‘æ•°æ®ä¸åŒï¼ŒEasi3Ré€šè¿‡è§£è€¦æ³¨æ„åŠ›å›¾ï¼Œå‡†ç¡®å®ç°åŠ¨æ€åŒºåŸŸåˆ†å‰²ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œ4Dç¨ å¯†ç‚¹äº‘é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEasi3Råœ¨çœŸå®åŠ¨æ€è§†é¢‘ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä»¥å¾€çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„è½»é‡çº§è®¾è®¡ä½¿å…¶åœ¨å¤„ç†åŠ¨æ€åœºæ™¯æ—¶æ›´åŠ é«˜æ•ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23730', 'title': 'KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language', 'url': 'https://huggingface.co/papers/2503.23730', 'abstract': 'The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA', 'score': 3, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': '1d3d53298afe6cce', 'authors': ['Yoonshik Kim', 'Jaeyoon Jung'], 'affiliations': ['MAUM AI Inc. / Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.23730.jpg', 'data': {'categories': ['#open_source', '#low_resource', '#benchmark', '#multilingual'], 'emoji': 'ğŸ‡°ğŸ‡·', 'ru': {'title': 'KOFFVQA: Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº KOFFVQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 275 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ 10 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ VLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶ĞµĞ½, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ VLM.'}, 'en': {'title': 'KOFFVQA: Reliable Evaluation for Korean Vision-Language Models', 'desc': 'This paper introduces KOFFVQA, a new benchmark for evaluating Large Vision-Language Models (VLMs) specifically in the Korean language. It addresses the limitations of current evaluation methods that either restrict responses or rely on subjective judge models. KOFFVQA features 275 visual question-answering tasks with clear grading criteria that enhance reliability and objectivity in assessments. The authors demonstrate that their approach provides a more dependable evaluation of VLMs compared to existing methods, making it suitable for both large and small models.'}, 'zh': {'title': 'KOFFVQAï¼šéŸ©è¯­è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é è¯„ä¼°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¯„ä¼°åŸºå‡†ï¼Œåä¸ºKOFFVQAï¼Œä¸“é—¨é’ˆå¯¹éŸ©è¯­ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€ä¾èµ–äºé¢„è®¾çš„å›ç­”é€‰é¡¹æˆ–è¯„åˆ¤æ¨¡å‹ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸»è§‚ä¸”ä¸å¯é ã€‚KOFFVQAåŒ…å«275ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…æœ‰å›¾åƒå’Œæ¶µç›–VLMæ€§èƒ½çš„10ä¸ªè¯„ä¼°æ ‡å‡†ã€‚é€šè¿‡å®¢è§‚çš„è¯„ä¼°æ ‡å‡†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ›´å¯é åœ°è¯„ä¼°æ¨¡å‹ï¼Œå³ä½¿æ˜¯å°å‹å¼€æºæ¨¡å‹ä¹Ÿèƒ½æœ‰æ•ˆä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20286', 'title': 'Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization', 'url': 'https://huggingface.co/papers/2503.20286', 'abstract': 'Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.', 'score': 3, 'issue_id': 2994, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'bf1debfaa462fca8', 'authors': ['Zhenyu Liang', 'Hao Li', 'Naiwei Yu', 'Kebin Sun', 'Ran Cheng'], 'affiliations': ['Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Data Science and Artificial Intelligence and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.20286.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#robotics', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ¢ĞµĞ½Ğ·Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ EMO: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (EMO) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ½Ğ° GPU. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğº Ñ‚Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼ EMO: NSGA-III, MOEA/D Ğ¸ HypE. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 1113 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ CPU Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ° Ğ½Ğ° GPU Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Accelerating EMO with GPU Tensorization for Enhanced Performance', 'desc': 'This paper addresses the limitations of traditional evolutionary multiobjective optimization (EMO) algorithms when faced with complex and large-scale problems. It highlights the lack of hardware acceleration in existing EMO approaches and proposes a novel method to parallelize these algorithms using GPUs through tensorization. By transforming EMO data structures into tensor representations, the authors enable efficient GPU computing, resulting in significant performance improvements. The proposed tensorized EMO algorithms demonstrate remarkable speedups while maintaining solution quality in multiobjective robot control tasks.'}, 'zh': {'title': 'å¼ é‡åŒ–æå‡EMOç®—æ³•æ€§èƒ½ï¼ŒGPUåŠ é€Ÿæ˜¾è‘—', 'desc': 'è¿›åŒ–å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆEMOï¼‰åœ¨è¿‡å»äºŒåå¹´å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†éšç€é—®é¢˜è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œä¼ ç»Ÿçš„EMOç®—æ³•é¢ä¸´æ€§èƒ½é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¼ é‡åŒ–æ–¹æ³•åœ¨GPUä¸Šå¹¶è¡ŒåŒ–EMOç®—æ³•çš„æ–¹æ¡ˆï¼Œä»¥è§£å†³ä¼ ç»Ÿç®—æ³•çš„å¹¶è¡Œæ€§å’Œå¯æ‰©å±•æ€§ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¼ é‡åŒ–ï¼ŒEMOç®—æ³•çš„æ•°æ®ç»“æ„å’Œæ“ä½œè¢«è½¬åŒ–ä¸ºç®€æ´çš„å¼ é‡è¡¨ç¤ºï¼Œä»è€Œå®ç°äº†GPUè®¡ç®—çš„è‡ªåŠ¨åˆ©ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼ é‡åŒ–çš„EMOç®—æ³•åœ¨é€Ÿåº¦ä¸Šæ¯”åŸºäºCPUçš„ç®—æ³•å¿«äº†å¤šè¾¾1113å€ï¼ŒåŒæ—¶ä¿æŒäº†è§£å†³æ–¹æ¡ˆçš„è´¨é‡ï¼Œå¹¶æœ‰æ•ˆå¤„ç†å¤æ‚çš„å¤šç›®æ ‡æœºå™¨äººæ§åˆ¶ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14941', 'title': 'UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation', 'url': 'https://huggingface.co/papers/2503.14941', 'abstract': 'Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves a Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences.', 'score': 3, 'issue_id': 3000, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '0d6cdad4ff3e795e', 'authors': ['Qihui Zhang', 'Munan Ning', 'Zheyuan Liu', 'Yanbo Wang', 'Jiayi Ye', 'Yue Huang', 'Shuo Yang', 'Xiao Chen', 'Yibing Song', 'Li Yuan'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'School of Electrical and Computer Engineering, Peking University', 'Tsinghua University', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2503.14941.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#interpretability', '#alignment'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Unsupervised Peer review MLLM Evaluation (UPME), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. UPME Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ UPME Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Automating VQA Evaluations with Unsupervised Peer Review', 'desc': 'This paper introduces a new framework called Unsupervised Peer review MLLM Evaluation (UPME) to improve the evaluation of Multimodal Large Language Models (MLLMs) in Visual Question Answering (VQA). The framework reduces the need for human involvement by allowing models to automatically generate questions and assess answers from other models using only image data. To address biases in automated evaluations, a vision-language scoring system is implemented, focusing on response correctness, visual understanding, and image-text correlation. Experimental results show that UPME closely aligns with human evaluations, achieving high correlation scores on benchmark datasets.'}, 'zh': {'title': 'æ— ç›‘ç£åŒè¡Œè¯„å®¡ï¼Œæå‡è§†è§‰é—®ç­”è¯„ä¼°çš„æ•ˆç‡ä¸å…¬æ­£æ€§', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åœ¨è§£å†³è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œæ¨åŠ¨äº†å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå®¢è§‚è¯„ä¼°çš„æ–°ç ”ç©¶æ–¹å‘ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ç”±äºéœ€è¦å¤§é‡äººåŠ›è®¾è®¡é—®ç­”å¯¹ï¼Œé™åˆ¶äº†è¯„ä¼°çš„è§„æ¨¡å’ŒèŒƒå›´ã€‚è™½ç„¶è‡ªåŠ¨åŒ–çš„MLLMè¯„ä¼°æ–¹æ³•è¯•å›¾å‡å°‘äººåŠ›å·¥ä½œé‡ï¼Œä½†å¾€å¾€ä¼šå¼•å…¥åè§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„åŒè¡Œè¯„å®¡MLLMè¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨å›¾åƒæ•°æ®è‡ªåŠ¨ç”Ÿæˆé—®é¢˜ï¼Œå¹¶å¯¹å…¶ä»–æ¨¡å‹çš„ç­”æ¡ˆè¿›è¡Œè¯„ä¼°ï¼Œä»è€Œæœ‰æ•ˆå‡è½»å¯¹äººåŠ›çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23022', 'title': 'MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs', 'url': 'https://huggingface.co/papers/2503.23022', 'abstract': 'In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, a novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) a transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) a flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with a predefined number of faces. By utilizing the diffusion model for the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds (35times faster than existing baselines). Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation.', 'score': 2, 'issue_id': 3002, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '4bb15e0559669bbb', 'authors': ['Xianglong He', 'Junyi Chen', 'Di Huang', 'Zexiang Liu', 'Xiaoshui Huang', 'Wanli Ouyang', 'Chun Yuan', 'Yangguang Li'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'The University of Sydney', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2503.23022.jpg', 'data': {'categories': ['#3d', '#optimization', '#diffusion', '#games'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'MeshCraft: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ 3D-ÑĞµÑ‚Ğ¾Ğº Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹', 'desc': 'MeshCraft - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½ĞµĞ¹. ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· VAE Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµÑ‚Ğ¾Ğº, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ³Ñ€Ğ°Ğ½ĞµĞ¹. MeshCraft Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-ÑĞµÑ‚Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑĞµÑ‚ĞºÑƒ Ğ¸Ğ· 800 Ğ³Ñ€Ğ°Ğ½ĞµĞ¹ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 3,2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ShapeNet Ğ¸ Objaverse, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'MeshCraft: Fast and Controlled 3D Mesh Generation', 'desc': 'This paper presents MeshCraft, a new framework for generating 3D mesh topologies efficiently and with control over the number of faces. Unlike previous methods that use auto-regressive techniques, MeshCraft employs a transformer-based variational autoencoder (VAE) and a flow-based diffusion transformer to create high-quality meshes quickly. The framework allows for the simultaneous generation of entire mesh structures, significantly speeding up the process to just 3.2 seconds for an 800-face mesh. Experimental results show that MeshCraft outperforms existing methods in both quality and speed, making it a valuable tool for 3D artists.'}, 'zh': {'title': 'MeshCraftï¼šé«˜æ•ˆå¯æ§çš„3Dç½‘æ ¼ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'åœ¨3Då†…å®¹åˆ›ä½œé¢†åŸŸï¼ŒMeshCraftæ˜¯ä¸€ç§æ–°é¢–çš„é«˜æ•ˆå¯æ§ç½‘æ ¼ç”Ÿæˆæ¡†æ¶ã€‚å®ƒåˆ©ç”¨è¿ç»­ç©ºé—´æ‰©æ•£ç”Ÿæˆç¦»æ•£ä¸‰è§’é¢ï¼Œå…‹æœäº†ä¼ ç»Ÿè‡ªå›å½’æ–¹æ³•çš„ç”Ÿæˆé€Ÿåº¦æ…¢å’Œé¢æ•°ä¸å¯æ§çš„ç¼ºé™·ã€‚MeshCraftç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼šåŸºäºå˜æ¢å™¨çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œæ¡ä»¶åŒ–é¢æ•°çš„æµæ‰©æ•£å˜æ¢å™¨ã€‚é€šè¿‡åŒæ—¶ç”Ÿæˆæ•´ä¸ªç½‘æ ¼æ‹“æ‰‘ï¼ŒMeshCraftåœ¨ç”Ÿæˆé«˜è´¨é‡3Dç½‘æ ¼æ—¶é€Ÿåº¦æ˜¾è‘—æé«˜ï¼Œèƒ½å¤Ÿåœ¨3.2ç§’å†…ç”Ÿæˆ800é¢ç½‘æ ¼ï¼Œé€Ÿåº¦æ¯”ç°æœ‰æ–¹æ³•å¿«35å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22655', 'title': 'Unicorn: Text-Only Data Synthesis for Vision Language Model Training', 'url': 'https://huggingface.co/papers/2503.22655', 'abstract': 'Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git.', 'score': 2, 'issue_id': 3005, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': 'a724544e9c0362b6', 'authors': ['Xiaomin Yu', 'Pengxiang Ding', 'Wenjie Zhang', 'Siteng Huang', 'Songyang Gao', 'Chengwei Qin', 'Kejian Wu', 'Zhaoxin Fan', 'Ziyue Qiao', 'Donglin Wang'], 'affiliations': ['Beihang University', 'Nanyang Technological University', 'Shanghai AI Lab', 'The Great Bay University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22655.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#data', '#reasoning', '#synthetic'], 'emoji': 'ğŸ¦„', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ’Ğ¯Ğœ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Unicorn-1.2M Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Unicorn-471K-Instruction Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Synthesize High-Quality Multimodal Data from Text Alone!', 'desc': 'This paper presents a novel framework for synthesizing multimodal training data for vision-language models (VLMs) using only text. The proposed three-stage process generates high-quality image-text pairs without the need for real images, significantly reducing costs. In the first stage, diverse captions are created from sparse seeds using large language models, followed by the generation of instruction-tuning tasks in the second stage. Finally, the textual representations are transformed into visual representations, resulting in two datasets: Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, enhancing the training of VLMs.'}, 'zh': {'title': 'æ— å›¾åƒé«˜æ•ˆåˆæˆå¤šæ¨¡æ€æ•°æ®', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è·¨é›†æˆçš„ä¸‰é˜¶æ®µå¤šæ¨¡æ€æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬å¯¹ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹æ‰©å±•ç¨€ç–çš„æ–‡æœ¬ç§å­ï¼Œåˆæˆäº†120ä¸‡æ¡è¯­ä¹‰å¤šæ ·çš„é«˜è´¨é‡æ–‡æœ¬æè¿°ã€‚ç¬¬äºŒé˜¶æ®µå°†47ä¸‡æ¡æ–‡æœ¬æè¿°è½¬åŒ–ä¸ºå¤šè½®æŒ‡ä»¤è°ƒä¼˜ä»»åŠ¡ï¼Œä»¥æ”¯æŒå¤æ‚æ¨ç†ã€‚æœ€åï¼Œç¬¬ä¸‰é˜¶æ®µå°†æ–‡æœ¬è¡¨ç¤ºè½¬æ¢ä¸ºè§†è§‰è¡¨ç¤ºï¼Œä»è€Œç”Ÿæˆå¤šæ ·çš„åˆæˆå›¾åƒè¡¨ç¤ºï¼Œæä¾›äº†ä¸€ç§æ— éœ€çœŸå®å›¾åƒçš„é«˜æ•ˆè®­ç»ƒæ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23913', 'title': 'Entropy-Based Adaptive Weighting for Self-Training', 'url': 'https://huggingface.co/papers/2503.23913', 'abstract': 'The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs a mapping function with a tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K, EAST attains a further 1-2% performance boost compared to the vanilla method.', 'score': 1, 'issue_id': 3002, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'b7e8ee7260c71bb7', 'authors': ['Xiaoxuan Wang', 'Yihe Deng', 'Mingyu Derek Ma', 'Wei Wang'], 'affiliations': ['University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.23913.jpg', 'data': {'categories': ['#training', '#reasoning', '#math', '#benchmark', '#optimization'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ EAST (Entropy-Based Adaptive Weighting for Self-Training), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼. EAST Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ Ñ€ĞµĞ·ĞºĞ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EAST Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 1% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MATH Ğ¸ Ğ½Ğ° 1-2% Ğ½Ğ° GSM8K Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Model Reasoning with Adaptive Uncertainty Weighting', 'desc': "This paper explores how large language models can improve their mathematical problem-solving skills by using self-generated reasoning paths. The authors introduce a new method called Entropy-Based Adaptive Weighting for Self-Training (EAST), which focuses on prioritizing uncertain data during the training process. By adjusting the importance of different training examples based on the model's uncertainty, EAST helps the model learn from more challenging and informative cases. The results show that EAST leads to slight performance improvements on benchmark tests compared to traditional self-training methods."}, 'zh': {'title': 'è‡ªé€‚åº”åŠ æƒæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§è‡ªæˆ‘è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨è‡ªç”Ÿæˆçš„æ¨ç†è·¯å¾„æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºEASTçš„è‡ªé€‚åº”åŠ æƒç­–ç•¥ï¼Œæ—¨åœ¨ä¼˜å…ˆè€ƒè™‘ä¸ç¡®å®šæ€§è¾ƒé«˜çš„æ•°æ®è¿›è¡Œè‡ªæˆ‘è®­ç»ƒã€‚EASTé€šè¿‡å¯è°ƒå‚æ•°çš„æ˜ å°„å‡½æ•°æ¥æ§åˆ¶åŠ æƒçš„é”åº¦ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å…³æ³¨æ›´å…·ä¿¡æ¯é‡å’ŒæŒ‘æˆ˜æ€§çš„ç¤ºä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEASTåœ¨GSM8Kå’ŒMATHåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19906', 'title': 'AvatarArtist: Open-Domain 4D Avatarization', 'url': 'https://huggingface.co/papers/2503.19906', 'abstract': 'This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies..', 'score': 1, 'issue_id': 3012, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '9cc5699cb424a056', 'authors': ['Hongyu Liu', 'Xuan Wang', 'Ziyu Wan', 'Yue Ma', 'Jingye Chen', 'Yanbo Fan', 'Yujun Shen', 'Yibing Song', 'Qifeng Chen'], 'affiliations': ['Ant Group', 'City University of Hong Kong', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2503.19906.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#diffusion', '#3d'], 'emoji': 'ğŸ­', 'ru': {'title': 'AvatarArtist: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ 4D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 4D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµĞ¹Ğ½Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾-ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ (GAN) Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. GAN Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµĞ¹Ğ½Ğ¾Ğ², Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ° ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµĞ¹Ğ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° 4D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Transforming Portraits into Dynamic 4D Avatars!', 'desc': 'This paper presents a method for creating 4D avatars from 2D portrait images in different styles. It utilizes parametric triplanes as a 4D representation and combines generative adversarial networks (GANs) with diffusion models for effective training. The approach leverages the strengths of 4D GANs to connect images and triplanes while addressing challenges with diverse data through a robust 2D diffusion prior. The resulting model, AvatarArtist, demonstrates the ability to generate high-quality 4D avatars across various image domains, with plans to share the code and data for further research.'}, 'zh': {'title': 'æ‰“é€ å¤šé¢†åŸŸé«˜è´¨é‡4Då¤´åƒçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬ç ”ç©¶ä¸“æ³¨äºå¼€æ”¾é¢†åŸŸçš„4Då¤´åƒç”Ÿæˆï¼Œæ—¨åœ¨ä»ä»»æ„é£æ ¼çš„è‚–åƒå›¾åƒä¸­åˆ›å»º4Då¤´åƒã€‚æˆ‘ä»¬é€‰æ‹©å‚æ•°ä¸‰å¹³é¢ä½œä¸ºä¸­é—´çš„4Dè¡¨ç¤ºï¼Œå¹¶æå‡ºäº†ä¸€ç§å®ç”¨çš„è®­ç»ƒèŒƒå¼ï¼Œç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚ç ”ç©¶è¡¨æ˜ï¼Œ4D GANåœ¨æ— ç›‘ç£æƒ…å†µä¸‹èƒ½å¤Ÿæœ‰æ•ˆè¿æ¥å›¾åƒå’Œä¸‰å¹³é¢ï¼Œä½†åœ¨å¤„ç†å¤šæ ·åŒ–æ•°æ®åˆ†å¸ƒæ—¶é€šå¸¸é¢ä¸´æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥å¼ºå¤§çš„2Dæ‰©æ•£å…ˆéªŒï¼Œå¸®åŠ©GANåœ¨ä¸åŒé¢†åŸŸä¹‹é—´è½¬ç§»å…¶ä¸“ä¸šçŸ¥è¯†ï¼Œä»è€Œæ„å»ºä¸€ä¸ªå¤šé¢†åŸŸçš„å›¾åƒ-ä¸‰å¹³é¢æ•°æ®é›†ï¼Œæ¨åŠ¨é€šç”¨4Då¤´åƒç”Ÿæˆå™¨çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19794', 'title': 'PAVE: Patching and Adapting Video Large Language Models', 'url': 'https://huggingface.co/papers/2503.19794', 'abstract': 'Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as "patches," which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE.', 'score': 1, 'issue_id': 3008, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'fefb309c48b93a29', 'authors': ['Zhuoming Liu', 'Yiquan Li', 'Khoi Duc Nguyen', 'Yiwu Zhong', 'Yin Li'], 'affiliations': ['The Chinese University of Hong Kong', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.19794.jpg', 'data': {'categories': ['#optimization', '#3d', '#transfer_learning', '#multimodal', '#reasoning', '#video', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'PAVE: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼', 'desc': "PAVE - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Video LLMs) Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ 'Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞµĞµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. PAVE ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, 3D-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."}, 'en': {'title': 'PAVE: Adapting Video LLMs with Lightweight Patches', 'desc': 'This paper introduces PAVE, a new framework designed to adapt pre-trained video large language models (Video LLMs) for various tasks that require additional data types like audio or 3D information. PAVE utilizes lightweight adapters, called "patches," which integrate seamlessly into existing models without altering their original architecture or pre-trained weights. By doing so, it enhances the model\'s capabilities for tasks such as audio-visual question answering and multi-view video recognition while maintaining a low computational cost. The framework not only improves performance over state-of-the-art models but also supports multi-task learning and shows strong generalization across different Video LLMs.'}, 'zh': {'title': 'PAVEï¼šçµæ´»é€‚åº”è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPAVEçš„çµæ´»æ¡†æ¶ï¼Œç”¨äºå°†é¢„è®­ç»ƒçš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰é€‚åº”äºæ–°çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠéŸ³é¢‘ã€3Dä¿¡æ¯æˆ–å¤šè§†è§’è§†é¢‘ç­‰é™„åŠ æ¨¡æ€çš„æ•°æ®ã€‚PAVEå¼•å…¥äº†è½»é‡çº§çš„é€‚é…å™¨ï¼Œç§°ä¸ºâ€œè¡¥ä¸â€ï¼Œè¿™äº›è¡¥ä¸åœ¨ä¸æ”¹å˜åŸºç¡€æ¨¡å‹æ¶æ„æˆ–é¢„è®­ç»ƒæƒé‡çš„æƒ…å†µä¸‹ï¼Œå¢åŠ äº†å°‘é‡å‚æ•°å’Œæ“ä½œã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒPAVEèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹é€‚åº”äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚éŸ³è§†é¢‘é—®ç­”ã€3Dæ¨ç†å’Œå¤šè§†è§’è§†é¢‘è¯†åˆ«ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPAVEåœ¨è¿™äº›ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼ŒåŒæ—¶ä»…å¢åŠ çº¦0.1%çš„è®¡ç®—é‡å’Œå‚æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18225', 'title': 'Decoupling Angles and Strength in Low-rank Adaptation', 'url': 'https://huggingface.co/papers/2503.18225', 'abstract': 'Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.', 'score': 1, 'issue_id': 3001, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': '36bcda1f90686965', 'authors': ['Massimo Bini', 'Leander Girrbach', 'Zeynep Akata'], 'affiliations': ['Helmholtz Munich', 'Technical University of Munich, Munich Center for Machine Learning', 'University of Tubingen, Tubingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2503.18225.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'DeLoRA: Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'DeLoRA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°. ĞĞ½ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ ÑƒĞ³Ğ»Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ¸Ğ»Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. DeLoRA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑˆĞµ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (PEFT) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Decoupling Adaptation for Robust Fine-Tuning', 'desc': 'This paper introduces Decoupled Low-rank Adaptation (DeLoRA), a new method for fine-tuning large pretrained models efficiently. DeLoRA improves upon existing Parameter-Efficient FineTuning (PEFT) methods by normalizing and scaling low-rank matrices, which enhances robustness against hyperparameter variations. Unlike traditional methods like LoRA, which struggle with stability, DeLoRA decouples the learning angle from the adaptation strength, allowing for better performance across various tasks. The results demonstrate that DeLoRA not only matches but often exceeds the performance of other PEFT techniques in applications such as image generation and natural language understanding.'}, 'zh': {'title': 'è§£è€¦ä½ç§©é€‚åº”ï¼šæå‡å¾®è°ƒé²æ£’æ€§çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºè§£è€¦ä½ç§©é€‚åº”ï¼ˆDeLoRAï¼‰ï¼Œæ—¨åœ¨æé«˜å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•çš„é²æ£’æ€§ã€‚DeLoRAé€šè¿‡è§„èŒƒåŒ–å’Œç¼©æ”¾å¯å­¦ä¹ çš„ä½ç§©çŸ©é˜µï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒDeLoRAåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹è¶…å‚æ•°é€‰æ‹©å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–ã€‚é€šè¿‡åœ¨å›¾åƒç”Ÿæˆã€è‡ªç„¶è¯­è¨€ç†è§£å’ŒæŒ‡ä»¤è°ƒä¼˜ç­‰ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼ŒDeLoRAçš„è¡¨ç°ä¸å…¶ä»–PEFTæ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶å±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22677', 'title': 'DSO: Aligning 3D Generators with Simulation Feedback for Physical\n  Soundness', 'url': 'https://huggingface.co/papers/2503.22677', 'abstract': 'Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.', 'score': 0, 'issue_id': 3014, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '9a7faeb192777179', 'authors': ['Ruining Li', 'Chuanxia Zheng', 'Christian Rupprecht', 'Andrea Vedaldi'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.22677.jpg', 'data': {'categories': ['#3d', '#dataset', '#alignment', '#optimization', '#rlhf', '#training'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ±ĞµĞ· Ğ·Ğ°Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Direct Simulation Optimization (DSO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Direct Reward Optimization (DRO) Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Generating Stable 3D Objects with Direct Simulation Optimization', 'desc': 'This paper introduces Direct Simulation Optimization (DSO), a new framework for generating stable 3D objects that can support themselves under gravity. Unlike previous methods that relied on slow and unstable differentiable physics simulators, DSO uses feedback from a non-differentiable simulator to enhance the stability of generated objects. The authors create a dataset of 3D objects with stability scores and employ direct preference optimization (DPO) and direct reward optimization (DRO) to fine-tune the generator. The results demonstrate that DSO significantly improves the speed and stability of 3D object generation without needing ground-truth training data.'}, 'zh': {'title': 'ç›´æ¥æ¨¡æ‹Ÿä¼˜åŒ–ï¼šç”Ÿæˆç¨³å®š3Dç‰©ä½“çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„3Dç‰©ä½“ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºç›´æ¥æ¨¡æ‹Ÿä¼˜åŒ–ï¼ˆDSOï¼‰ï¼Œæ—¨åœ¨ç”Ÿæˆç¬¦åˆç‰©ç†çº¦æŸçš„ç¨³å®š3Dç‰©ä½“ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¯å¾®åˆ†ç‰©ç†æ¨¡æ‹Ÿå™¨è¿›è¡Œå‡ ä½•ä¼˜åŒ–ï¼Œä½†é€Ÿåº¦æ…¢ä¸”å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚DSOæ¡†æ¶åˆ©ç”¨æ¥è‡ªéå¯å¾®åˆ†æ¨¡æ‹Ÿå™¨çš„åé¦ˆï¼Œç›´æ¥æé«˜ç”Ÿæˆå™¨è¾“å‡ºç¨³å®šç‰©ä½“çš„å¯èƒ½æ€§ã€‚é€šè¿‡æ„å»ºå¸¦æœ‰ç¨³å®šæ€§è¯„åˆ†çš„æ•°æ®é›†ï¼Œç ”ç©¶è€…å¯ä»¥ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æˆ–ç›´æ¥å¥–åŠ±ä¼˜åŒ–ï¼ˆDROï¼‰æ¥å¾®è°ƒç”Ÿæˆå™¨ï¼Œä»è€Œå®ç°æ›´å¿«ä¸”æ›´ç¨³å®šçš„3Dç‰©ä½“ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22668', 'title': 'Understanding Co-speech Gestures in-the-wild', 'url': 'https://huggingface.co/papers/2503.22668', 'abstract': "Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-text-speech associations: (i) gesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal speech-text-video-gesture representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs), across all three tasks. Further analysis reveals that speech and text modalities capture distinct gesture-related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal", 'score': 0, 'issue_id': 3007, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '77fc0ab156b46a61', 'authors': ['Sindhu B Hegde', 'K R Prajwal', 'Taein Kwon', 'Andrew Zisserman'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.22668.jpg', 'data': {'categories': ['#multimodal', '#benchmark'], 'emoji': 'ğŸ¤²', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¶ĞµÑÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµÑ‡Ğ¸: Ñ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¶ĞµÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµÑ‡ÑŒ, Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¶ĞµÑÑ‚Ğ°Ğ¼Ğ¸, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ñ€ĞµÑ‡ÑŒÑ. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸-Ñ‚ĞµĞºÑÑ‚Ğ°-Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¶ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚Ñ€ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Unlocking Gesture Understanding with Tri-Modal Learning', 'desc': 'This paper presents a new framework for understanding co-speech gestures, which are important for non-verbal communication. It introduces three tasks to evaluate how well models can understand the relationships between gestures, text, and speech. The authors propose a tri-modal representation that combines speech, text, video, and gestures, using a unique loss function to improve learning from real-world videos. Their approach outperforms existing methods, including large vision-language models, highlighting the benefits of a shared embedding space for different modalities.'}, 'zh': {'title': 'ç†è§£å…±è¯­æ‰‹åŠ¿çš„æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç”¨äºç†è§£è‡ªç„¶ç¯å¢ƒä¸­çš„å…±è¯­æ‰‹åŠ¿ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªæ–°ä»»åŠ¡å’ŒåŸºå‡†ï¼Œä»¥è¯„ä¼°æ¨¡å‹ç†è§£æ‰‹åŠ¿ã€æ–‡æœ¬å’Œè¯­éŸ³ä¹‹é—´å…³è”çš„èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆå…¨å±€çŸ­è¯­å¯¹æ¯”æŸå¤±å’Œå±€éƒ¨æ‰‹åŠ¿-è¯è€¦åˆæŸå¤±ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨å¼±ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œä»è§†é¢‘ä¸­å­¦ä¹ å¼ºå¤§çš„æ‰‹åŠ¿è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å­¦ä¹ è¡¨ç¤ºåœ¨æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ä¸­éƒ½è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00999', 'title': 'MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization', 'url': 'https://huggingface.co/papers/2504.00999', 'abstract': 'Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.', 'score': 56, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'bb6506ffd72aed19', 'authors': ['Siyuan Li', 'Luyuan Zhang', 'Zedong Wang', 'Juanxi Tian', 'Cheng Tan', 'Zicheng Liu', 'Chang Yu', 'Qingsong Xie', 'Haonan Lu', 'Haoqian Wang', 'Zhen Lei'], 'affiliations': ['CAIR, HKISI-CAS', 'MAIS CASIA', 'OPPO AI Center', 'The Hong Kong University of Science and Technology', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00999.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'MergeVQ: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MergeVQ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (MIM) Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (VQ) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. MergeVQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ MergeAR, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MergeVQ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'MergeVQ: Bridging Image Generation and Representation Learning Efficiently', 'desc': 'This paper introduces MergeVQ, a novel approach that enhances Masked Image Modeling (MIM) using Vector Quantization (VQ) techniques. It addresses the challenge of balancing image generation quality with efficient representation learning by integrating token merging into the VQ framework. During the pre-training phase, MergeVQ utilizes a token merge module to separate high-level semantics from the latent space, allowing for improved quantization and detail recovery. The second stage, MergeAR, optimizes the generation process through KV Cache compression, resulting in a model that excels in both visual representation and image generation while ensuring efficiency in token usage and inference speed.'}, 'zh': {'title': 'MergeVQï¼šæå‡å›¾åƒç”Ÿæˆä¸è¡¨ç¤ºå­¦ä¹ çš„ç»Ÿä¸€æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹MergeVQï¼Œæ—¨åœ¨æ”¹å–„åŸºäºå‘é‡é‡åŒ–çš„å›¾åƒç”Ÿæˆå’Œè§†è§‰è¡¨ç¤ºå­¦ä¹ ä¹‹é—´çš„å¹³è¡¡ã€‚é€šè¿‡åœ¨ç¼–ç å™¨ä¸­å¼•å…¥ä»¤ç‰Œåˆå¹¶æŠ€æœ¯ï¼ŒMergeVQèƒ½å¤Ÿåœ¨è‡ªæ³¨æ„åŠ›å—åè§£è€¦æ½œåœ¨ç©ºé—´ä¸­çš„è¯­ä¹‰ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¢å¤ç»†èŠ‚ï¼Œå¹¶åœ¨ç”Ÿæˆé˜¶æ®µä½¿ç”¨KVç¼“å­˜å‹ç¼©æ¥æé«˜é¢„æµ‹æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMergeVQåœ¨è§†è§‰è¡¨ç¤ºå­¦ä¹ å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00883', 'title': 'Improved Visual-Spatial Reasoning via R1-Zero-Like Training', 'url': 'https://huggingface.co/papers/2504.00883', 'abstract': 'Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.', 'score': 42, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '43fba84cc49adfad', 'authors': ['Zhenyi Liao', 'Qingsong Xie', 'Yanhao Zhang', 'Zijian Kong', 'Haonan Lu', 'Zhenyu Yang', 'Zhijie Deng'], 'affiliations': ['OPPO AI Center', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00883.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#video', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ R1-Zero. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2-VL Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ GRPO Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VSI-100k Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ vsGRPO-7B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2-VL-7B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ LLaVA-NeXT-Video-72B.'}, 'en': {'title': 'Enhancing Visual-Spatial Reasoning in MLLMs with GRPO Training', 'desc': 'This paper focuses on enhancing the visual-spatial reasoning abilities of multi-modal large language models (MLLMs), which are crucial for AI agents interacting with the physical world. The authors introduce a novel training method called GRPO, applied to the Qwen2-VL models, to improve their reasoning capabilities using a specially curated dataset named VSI-100k. They discover that traditional Chain of Thought prompts are ineffective for activating these reasoning skills in smaller models. The results show that their fine-tuned models significantly outperform baseline models, demonstrating the effectiveness of their approach in advancing visual-spatial intelligence in MLLMs.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶èšç„¦äºæå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘åŸºç¡€çš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ˆVSIï¼‰æ–¹é¢ã€‚æˆ‘ä»¬å‘ç°å°åˆ°ä¸­å‹çš„Qwen2-VLæ¨¡å‹æ— æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¿€æ´»å…¶è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå› æ­¤å¼•å…¥äº†GRPOè®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„VSI-100kæ•°æ®é›†è¿›è¡Œæ”¹è¿›ã€‚ç»è¿‡120ä¸ªGPUå°æ—¶çš„è®­ç»ƒï¼Œæˆ‘ä»¬çš„vsGRPO-2Bæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†åŸºç¡€æ¨¡å‹12.1%ï¼Œå¹¶ä¸”vsGRPO-7Bæ¨¡å‹çš„è¡¨ç°ä¸æœ€ä½³å¼€æºæ¨¡å‹LLaVA-NeXT-Video-72Bç›¸å½“ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä¿æŒKLæƒ©ç½šåœ¨GRPOä¸­æ˜¯å¿…è¦çš„ï¼Œå¹¶ä¸”vsGRPOåœ¨ä¸ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–åŸºçº¿çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01014', 'title': 'AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction', 'url': 'https://huggingface.co/papers/2504.01014', 'abstract': 'Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.', 'score': 29, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '98efa783105c3173', 'authors': ['Junhao Cheng', 'Yuying Ge', 'Yixiao Ge', 'Jing Liao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01014.jpg', 'data': {'categories': ['#diffusion', '#games', '#multimodal', '#video'], 'emoji': 'ğŸ®', 'ru': {'title': 'AnimeGamer: Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ°Ğ½Ğ¸Ğ¼Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'AnimeGamer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². AnimeGamer Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Transforming Anime into Interactive Gaming with Dynamic AI', 'desc': 'This paper presents AnimeGamer, a novel approach to infinite anime life simulation games that utilizes Multimodal Large Language Models (MLLMs). Unlike previous methods, AnimeGamer incorporates historical visual context to ensure consistent gameplay and generates dynamic animations rather than just static images. By employing action-aware multimodal representations, it can create high-quality video clips that reflect character movements and state changes. The results show that AnimeGamer significantly enhances the gaming experience compared to existing techniques, as validated by both automated and human evaluations.'}, 'zh': {'title': 'AnimeGamerï¼šåŠ¨æ€äº’åŠ¨çš„æ— é™åŠ¨æ¼«æ¸¸æˆä½“éªŒ', 'desc': 'æœ€è¿‘åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆæ–¹é¢çš„è¿›å±•ä¸ºç”Ÿæˆæ¸¸æˆå¸¦æ¥äº†æ–°çš„å¸Œæœ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAnimeGamerçš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”ŸæˆåŠ¨æ€åŠ¨ç”»é•œå¤´ï¼Œä»¥å¢å¼ºæ¸¸æˆçš„äº’åŠ¨æ€§å’Œæ²‰æµ¸æ„Ÿã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œæ„ŸçŸ¥çš„å¤šæ¨¡æ€è¡¨ç¤ºï¼ŒAnimeGamerèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’ŒåŠ¨æ€å˜åŒ–çš„æ¸¸æˆçŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAnimeGameråœ¨æ¸¸æˆä½“éªŒçš„å„ä¸ªæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01724', 'title': 'DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance', 'url': 'https://huggingface.co/papers/2504.01724', 'abstract': 'While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.', 'score': 24, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'd59102a274145730', 'authors': ['Yuxuan Luo', 'Zhengkun Rong', 'Lizhen Wang', 'Longhao Zhang', 'Tianshu Hu', 'Yongming Zhu'], 'affiliations': ['Bytedance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2504.01724.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#3d', '#diffusion'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'DreamActor-M1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, 3D-ÑÑ„ĞµÑ€Ñ‹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ 3D-ÑĞºĞµĞ»ĞµÑ‚Ñ‹ Ñ‚ĞµĞ»Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ°Ğ¼ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼. DreamActor-M1 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'DreamActor-M1: Revolutionizing Human Animation with Robust Control and Consistency', 'desc': 'The paper presents DreamActor-M1, a novel framework that utilizes a diffusion transformer (DiT) to enhance human animation by addressing key limitations in existing methods. It introduces hybrid control signals that combine facial representations, 3D head spheres, and body skeletons to improve the expressiveness and control of animations. The framework also employs a progressive training strategy to adapt to various body poses and image scales, ensuring versatility in generating animations from portraits to full-body views. Additionally, it integrates motion patterns from sequential frames to maintain long-term temporal coherence, resulting in more robust and visually appealing animations.'}, 'zh': {'title': 'çªç ´åŠ¨ç”»ç”Ÿæˆçš„å±€é™æ€§ï¼ŒDreamActor-M1å¼•é¢†æ–°æ½®æµï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡†æ¶DreamActor-M1ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåŸºç¡€çš„äººä½“åŠ¨ç”»æ–¹æ³•åœ¨ç»†ç²’åº¦æ•´ä½“å¯æ§æ€§ã€å¤šå°ºåº¦é€‚åº”æ€§å’Œé•¿æœŸæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡æ··åˆæ§åˆ¶ä¿¡å·ï¼Œç»“åˆéšå¼é¢éƒ¨è¡¨ç¤ºã€3Då¤´éƒ¨çƒä½“å’Œ3Dèº«ä½“éª¨æ¶ï¼Œå®ç°äº†å¯¹é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“åŠ¨ä½œçš„å¼ºå¤§æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒåŠ¨ç”»çš„è¡¨ç°åŠ›å’Œèº«ä»½ä¸€è‡´æ€§ã€‚ä¸ºäº†é€‚åº”ä¸åŒçš„èº«ä½“å§¿åŠ¿å’Œå›¾åƒå°ºåº¦ï¼Œé‡‡ç”¨äº†æ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡å’Œå°ºåº¦çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‚–åƒã€ä¸ŠåŠèº«å’Œå…¨èº«ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå…·æœ‰å¼ºå¤§çš„é•¿æœŸä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20783', 'title': 'Understanding R1-Zero-Like Training: A Critical Perspective', 'url': 'https://huggingface.co/papers/2503.20783', 'abstract': "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.", 'score': 24, 'issue_id': 3044, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'c5971e424bc52a6b', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… ÑƒĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ². Ğ‘Ñ‹Ğ» Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Group Relative Policy Optimization (GRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dr. GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing LLM Reasoning with Unbiased RL Optimization', 'desc': 'This paper explores how reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) without needing supervised fine-tuning. It examines the impact of different base models, particularly DeepSeek-V3-Base and Qwen2.5, on RL performance, revealing that pretraining characteristics can lead to inherent biases in reasoning capabilities. The authors also identify an optimization bias in the Group Relative Policy Optimization (GRPO) method, which can inflate response lengths during training. To counter this, they propose Dr. GRPO, a new optimization technique that enhances token efficiency while preserving reasoning accuracy, achieving a notable 43.3% accuracy on the AIME 2024 benchmark with a 7B base model.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›çš„æ–°çªç ´', 'desc': 'DeepSeek-R1-Zeroå±•ç¤ºäº†å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚æœ¬æ–‡æ·±å…¥åˆ†æäº†R1-Zeroè®­ç»ƒçš„ä¸¤ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼šåŸºç¡€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬ç ”ç©¶äº†å¤šç§åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬DeepSeek-V3-Baseï¼Œä»¥äº†è§£é¢„è®­ç»ƒç‰¹æ€§å¦‚ä½•å½±å“RLæ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æå‘ç°ï¼ŒDeepSeek-V3-Baseå·²ç»å±•ç°å‡ºâ€œæç„¶å¤§æ‚Ÿâ€çš„æ—¶åˆ»ï¼Œè€ŒQwen2.5åŸºç¡€æ¨¡å‹å³ä½¿åœ¨æ²¡æœ‰æç¤ºæ¨¡æ¿çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œæš—ç¤ºäº†æ½œåœ¨çš„é¢„è®­ç»ƒåå·®ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.01956', 'title': 'VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step', 'url': 'https://huggingface.co/papers/2504.01956', 'abstract': 'Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene', 'score': 22, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '44f1db8ef8cc244a', 'authors': ['Hanyang Wang', 'Fangfu Liu', 'Jiawei Chi', 'Yueqi Duan'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01956.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VideoScene: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoScene - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ 3D-aware leap flow Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. VideoScene Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D ÑÑ†ĞµĞ½ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 3D.'}, 'en': {'title': 'Efficient 3D Scene Generation from Sparse Views Using Video Diffusion Models', 'desc': 'This paper addresses the challenge of reconstructing 3D scenes from sparse views, which is often complicated by the lack of visual information. Traditional methods have limitations, particularly when input views have minimal overlap, leading to degraded performance. The authors introduce VideoScene, a novel approach that utilizes video diffusion models to efficiently generate 3D scenes in a single step. By implementing a 3D-aware leap flow distillation strategy and a dynamic denoising policy network, VideoScene significantly improves the speed and quality of 3D scene generation compared to existing methods.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆ3Dåœºæ™¯çš„VideoScene', 'desc': 'ä»ç¨€ç–è§†å›¾æ¢å¤3Dåœºæ™¯æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡å‡ ä½•æ­£åˆ™åŒ–æˆ–å‰é¦ˆç¡®å®šæ€§æ¨¡å‹ç­‰ä¸“é—¨è§£å†³æ–¹æ¡ˆæ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†åœ¨è¾“å…¥è§†å›¾é‡å è¾ƒå°‘ä¸”è§†è§‰ä¿¡æ¯ä¸è¶³æ—¶ï¼Œæ€§èƒ½ä»ç„¶ä¸‹é™ã€‚æœ€è¿‘çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¾ç¤ºå‡ºè§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰åˆç†3Dç»“æ„çš„è§†é¢‘ç‰‡æ®µã€‚æœ¬æ–‡æå‡ºäº†VideoSceneï¼Œé€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹æç‚¼ç”Ÿæˆ3Dåœºæ™¯ï¼Œè®¾è®¡äº†3Dæ„ŸçŸ¥çš„è·ƒè¿æµè’¸é¦ç­–ç•¥ï¼Œä»¥æé«˜ç”Ÿæˆæ•ˆç‡å’Œæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23368', 'title': 'Towards Physically Plausible Video Generation via VLM Planning', 'url': 'https://huggingface.co/papers/2503.23368', 'abstract': 'Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.', 'score': 21, 'issue_id': 3050, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'f3087a720104ea83', 'authors': ['Xindi Yang', 'Baolu Li', 'Yiming Zhang', 'Zhenfei Yin', 'Lei Bai', 'Liqian Ma', 'Zhiyong Wang', 'Jianfei Cai', 'Tien-Tsin Wong', 'Huchuan Lu', 'Xu Jia'], 'affiliations': ['Dalian University of Technology', 'Monash University', 'Oxford University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Sydney', 'ZMO AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.23368.jpg', 'data': {'categories': ['#reasoning', '#diffusion', '#video', '#architecture', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ ĞœĞ¾Ğ´ĞµĞ»ÑŒ (VLM) Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞœĞ¾Ğ´ĞµĞ»ÑŒÑ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ’Ğ¸Ğ´ĞµĞ¾ (VDM) Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ VDM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Bringing Physics to Video Generation: A Two-Stage Approach', 'desc': 'This paper introduces a new two-stage framework for generating videos that are more physically realistic using video diffusion models (VDMs). The first stage uses a Vision Language Model (VLM) to create rough motion trajectories that consider real-world physics, ensuring that the generated video maintains consistency between frames. In the second stage, these trajectories guide the VDM in producing detailed video content, with added noise to allow for creative freedom in motion generation. The results show that this approach significantly improves the physical plausibility of the generated videos compared to existing methods.'}, 'zh': {'title': 'å¼•å…¥ç‰©ç†çŸ¥è¯†çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰åœ¨ç”Ÿæˆé«˜åº¦çœŸå®çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¸¸å¸¸ç¼ºä¹å¯¹ç‰©ç†çš„ç†è§£ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘åœ¨åŠ¨æ€å’Œäº‹ä»¶åºåˆ—ä¸Šä¸å¤Ÿåˆç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ˜ç¡®åœ°èå…¥äº†ç‰©ç†çŸ¥è¯†ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºç²—ç•¥çš„è¿åŠ¨è§„åˆ’å™¨ï¼Œç»“åˆæ€ç»´é“¾å’Œç‰©ç†æ„ŸçŸ¥æ¨ç†ï¼Œé¢„æµ‹æ¥è¿‘çœŸå®ç‰©ç†åŠ¨æ€çš„ç²—ç•¥è¿åŠ¨è½¨è¿¹ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹æ¥æŒ‡å¯¼VDMçš„è§†é¢‘ç”Ÿæˆï¼Œä»è€Œå®ç°æ›´ç»†è‡´çš„è¿åŠ¨è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01848', 'title': "PaperBench: Evaluating AI's Ability to Replicate AI Research", 'url': 'https://huggingface.co/papers/2504.01848', 'abstract': "We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.", 'score': 18, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '60923777325e85cc', 'authors': ['Giulio Starace', 'Oliver Jaffe', 'Dane Sherburn', 'James Aung', 'Jun Shern Chan', 'Leon Maksin', 'Rachel Dias', 'Evan Mays', 'Benjamin Kinsella', 'Wyatt Thompson', 'Johannes Heidecke', 'Amelia Glaese', 'Tejal Patwardhan'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2504.01848.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#survey'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'PaperBench: Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'PaperBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 20 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· ICML 2024, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ Ñ Ğ½ÑƒĞ»Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ñ‡ĞµÑ‚ĞºĞ¸Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Claude 3.5 Sonnet (New) Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ»Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ 21.0%.'}, 'en': {'title': "Evaluating AI's Research Replication Skills with PaperBench", 'desc': "This paper presents PaperBench, a benchmark designed to assess AI agents' ability to replicate advanced AI research. The benchmark includes 20 selected papers from ICML 2024, requiring agents to comprehend contributions, create a codebase, and conduct experiments. To ensure objective evaluation, the authors developed detailed rubrics that break down replication tasks into smaller, graded components, totaling 8,316 tasks. The study also introduces an LLM-based judge for automated grading and compares the performance of AI agents against human experts, revealing that current models still lag behind human capabilities."}, 'zh': {'title': 'PaperBenchï¼šè¯„ä¼°AIå¤åˆ¶ç ”ç©¶èƒ½åŠ›çš„åŸºå‡†', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†PaperBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†å¤åˆ¶æœ€å…ˆè¿›AIç ”ç©¶èƒ½åŠ›çš„åŸºå‡†ã€‚ä»£ç†éœ€è¦ä»å¤´å¼€å§‹å¤åˆ¶20ç¯‡ICML 2024çš„äº®ç‚¹å’Œå£å¤´è®ºæ–‡ï¼ŒåŒ…æ‹¬ç†è§£è®ºæ–‡è´¡çŒ®ã€å¼€å‘ä»£ç åº“å’ŒæˆåŠŸæ‰§è¡Œå®éªŒã€‚ä¸ºäº†è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†åˆ†å±‚çš„è¯„åˆ†æ ‡å‡†ï¼Œå°†æ¯ä¸ªå¤åˆ¶ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶è®¾å®šæ˜ç¡®çš„è¯„åˆ†æ ‡å‡†ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¿˜åŒ…æ‹¬ä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„å®¡è€…è‡ªåŠ¨è¯„åˆ†ï¼Œå¹¶ä¸é¡¶å°–çš„æœºå™¨å­¦ä¹ åšå£«è¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°ç›®å‰çš„æ¨¡å‹å°šæœªè¶…è¶Šäººç±»åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00824', 'title': 'ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations', 'url': 'https://huggingface.co/papers/2504.00824', 'abstract': "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.", 'score': 18, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'b135f3f003dcaaff', 'authors': ['Yubo Wang', 'Xueguang Ma', 'Ping Nie', 'Huaye Zeng', 'Zhiheng Lyu', 'Yuxuan Zhang', 'Benjamin Schneider', 'Yi Lu', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.00824.jpg', 'data': {'categories': ['#science', '#dataset', '#rag', '#multimodal', '#alignment'], 'emoji': 'ğŸ“', 'ru': {'title': 'ScholarCopilot: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°', 'desc': 'ScholarCopilot - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½ [RET], Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ScholarCopilot ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 500 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· arXiv, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ top-1 Ğ² 40.1% Ğ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Academic Writing with ScholarCopilot', 'desc': 'This paper presents ScholarCopilot, a new framework that improves large language models for generating academic articles with accurate citations. It uses a retrieval token to decide when to fetch scholarly references, enhancing the text generation process with relevant citations. The model is trained on a large dataset of academic papers and shows significant improvements in both citation accuracy and writing quality compared to existing models. Human evaluations further validate its effectiveness in citation recall and overall user experience.'}, 'zh': {'title': 'ScholarCopilotï¼šæå‡å­¦æœ¯å†™ä½œçš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ScholarCopilotï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä¸“ä¸šå­¦æœ¯æ–‡ç« æ—¶çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç”Ÿæˆæ£€ç´¢æ ‡è®°[RET]ï¼ŒåŠ¨æ€å†³å®šä½•æ—¶æ£€ç´¢å­¦æœ¯å‚è€ƒæ–‡çŒ®ï¼Œå¹¶åˆ©ç”¨å…¶è¡¨ç¤ºä»æ•°æ®åº“ä¸­æŸ¥æ‰¾ç›¸å…³å¼•ç”¨ã€‚ScholarCopilotåœ¨ç”Ÿæˆå’Œå¼•ç”¨ä»»åŠ¡ä¸Šè¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥æé«˜æ•ˆç‡ã€‚ç»è¿‡åœ¨500Kç¯‡arXivè®ºæ–‡ä¸Šçš„è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨è¯„ä¼°æ•°æ®é›†ä¸Šå®ç°äº†40.1%çš„é¡¶çº§æ£€ç´¢å‡†ç¡®ç‡ï¼Œä¸”åœ¨å­¦æœ¯å†™ä½œæ ·æœ¬çš„ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†å‚æ•°æ›´å¤šçš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01934', 'title': 'ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement', 'url': 'https://huggingface.co/papers/2504.01934', 'abstract': 'We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.', 'score': 16, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'a50e19f04d94405f', 'authors': ['Runhui Huang', 'Chunwei Wang', 'Junwei Yang', 'Guansong Lu', 'Yunlong Yuan', 'Jianhua Han', 'Lu Hou', 'Wei Zhang', 'Lanqing Hong', 'Hengshuang Zhao', 'Hang Xu'], 'affiliations': ['Huawei Noahs Ark Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01934.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#training', '#games', '#multimodal', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ILLUME+: Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'ILLUME+ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ILLUME+ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ DualViTok, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ MLLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'ILLUME+: Unifying Understanding, Generation, and Editing in One Model', 'desc': 'ILLUME+ is a novel model that combines dual visual tokenization and a diffusion decoder to enhance deep semantic understanding and high-quality image generation. Unlike previous models that struggled with understanding, generation, and editing simultaneously, ILLUME+ effectively integrates these capabilities through its innovative DualViTok tokenizer. This approach preserves fine textures and aligns semantics, allowing for better image editing and generation. With a continuous-input, discrete-output framework and progressive training, ILLUME+ achieves competitive performance in multimodal tasks, paving the way for future applications.'}, 'zh': {'title': 'ILLUME+: å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'ILLUME+ æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å­¦ä¹ æ¨¡å‹ï¼Œç»“åˆäº†åŒé‡è§†è§‰æ ‡è®°å’Œæ‰©æ•£è§£ç å™¨ï¼Œæ—¨åœ¨æå‡æ·±å±‚è¯­ä¹‰ç†è§£å’Œé«˜ä¿çœŸå›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ä¸ç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹ç›¸æ¯”ï¼ŒILLUME+ èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘è¿™ä¸‰ç§åŸºæœ¬èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ DualViTokï¼ŒILLUME+ ä¿ç•™äº†ç»†è‡´çš„çº¹ç†å’Œæ–‡æœ¬å¯¹é½çš„è¯­ä¹‰ï¼ŒåŒæ—¶é‡‡ç”¨ç²—åˆ°ç»†çš„å›¾åƒè¡¨ç¤ºç­–ç•¥ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒILLUME+ åœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œè¶…åˆ†è¾¨ç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†ä¸ç°æœ‰æ¨¡å‹çš„ç«äº‰åŠ›ï¼Œä¸ºæœªæ¥çš„å¤šæ¨¡æ€åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01204', 'title': 'Articulated Kinematics Distillation from Video Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01204', 'abstract': 'We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/', 'score': 12, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'c8630e7ca691cef3', 'authors': ['Xuan Li', 'Qianli Ma', 'Tsung-Yi Lin', 'Yongxin Chen', 'Chenfanfu Jiang', 'Ming-Yu Liu', 'Donglai Xiang'], 'affiliations': ['NVIDIA', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2504.01204.jpg', 'data': {'categories': ['#diffusion', '#games', '#3d'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹: Ğ¾Ñ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Articulated Kinematics Distillation (AKD) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½ÑƒÑ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. AKD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚ĞµĞ¿ĞµĞ½ĞµĞ¹ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ñ„Ğ¾ĞºÑƒÑĞ° Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Score Distillation Sampling Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ. AKD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 4D Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Character Animation with AKD', 'desc': 'Articulated Kinematics Distillation (AKD) is a new framework designed to create realistic character animations by combining skeleton-based animation techniques with advanced generative models. It simplifies the animation process by using a skeleton representation, which reduces the Degrees of Freedom (DoFs) and allows for better control over joint movements. AKD employs Score Distillation Sampling (SDS) with pre-trained video diffusion models to generate complex motions while ensuring that the shapes of the characters remain consistent. This method also integrates well with physics-based simulations, resulting in animations that are not only visually appealing but also physically plausible.'}, 'zh': {'title': 'å…³èŠ‚è¿åŠ¨è’¸é¦ï¼šé«˜ä¿çœŸåŠ¨ç”»çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå…³èŠ‚è¿åŠ¨è’¸é¦ï¼ˆAKDï¼‰çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸè§’è‰²åŠ¨ç”»ã€‚AKDé€šè¿‡ä½¿ç”¨åŸºäºéª¨éª¼çš„è¡¨ç¤ºï¼Œæ˜¾è‘—å‡å°‘äº†è‡ªç”±åº¦ï¼Œä¸“æ³¨äºå…³èŠ‚çº§æ§åˆ¶ï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”ä¸€è‡´çš„è¿åŠ¨åˆæˆã€‚é€šè¿‡ä¸é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç»“åˆçš„å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰ï¼ŒAKDèƒ½å¤Ÿè’¸é¦å¤æ‚çš„å…³èŠ‚è¿åŠ¨ï¼ŒåŒæ—¶ä¿æŒç»“æ„å®Œæ•´æ€§ï¼Œå…‹æœäº†4Dç¥ç»å˜å½¢åœºåœ¨ä¿æŒå½¢çŠ¶ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼ŒAKDåœ¨3Dä¸€è‡´æ€§å’Œè¿åŠ¨è´¨é‡ä¸Šä¼˜äºç°æœ‰çš„æ–‡æœ¬åˆ°4Dç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01308', 'title': 'Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks', 'url': 'https://huggingface.co/papers/2504.01308', 'abstract': 'Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.', 'score': 11, 'issue_id': 3040, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '315938d70f25095e', 'authors': ['Jiawei Wang', 'Yushen Zuo', 'Yuanjun Chai', 'Zhendong Liu', 'Yichen Fu', 'Yichun Feng', 'Kin-man Lam'], 'affiliations': ['Nanjing University', 'Stanford University', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China', 'University of Washington', 'University of the Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.01308.jpg', 'data': {'categories': ['#security', '#cv', '#diffusion', '#training', '#dataset', '#optimization', '#multimodal'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğ¹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Robust-VLGuard, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ ÑˆÑƒĞ¼Ğ¾Ğ¼. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DiffPure-VLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑˆÑƒĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ VLM Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ°Ñ‚Ğ°Ğº.'}, 'en': {'title': 'Strengthening Vision-Language Models Against Noise Attacks', 'desc': 'This paper addresses the vulnerabilities of Vision-Language Models (VLMs) to jailbreak attacks, particularly when they encounter noisy or corrupted images. The authors highlight that existing security measures during training do not adequately account for noise-augmented visual inputs, leading to significant security gaps. To combat this issue, they introduce Robust-VLGuard, a dataset designed for multimodal safety that includes both aligned and misaligned image-text pairs, along with a noise-augmented fine-tuning process. Additionally, they propose DiffPure-VLM, which uses diffusion models to transform adversarial perturbations into Gaussian-like noise, enhancing the robustness of VLMs against such attacks while maintaining their functionality.'}, 'zh': {'title': 'å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å™ªå£°æˆ–æŸåçš„å›¾åƒæ—¶ä»ç„¶å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç°æœ‰çš„VLMsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡å–äº†å®‰å…¨æªæ–½ä»¥å‡è½»è¿™äº›æ”»å‡»ï¼Œä½†å¯¹å™ªå£°å¢å¼ºè§†è§‰è¾“å…¥çš„è„†å¼±æ€§å´æœªç»™äºˆè¶³å¤Ÿé‡è§†ã€‚æˆ‘ä»¬æå‡ºäº†Robust-VLGuardï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å®‰å…¨æ•°æ®é›†ï¼Œç»“åˆäº†å¯¹é½å’Œä¸å¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œå¹¶é€šè¿‡å™ªå£°å¢å¼ºçš„å¾®è°ƒæ¥é™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒVLMçš„åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒè½¬ç§»ç‰¹æ€§ä¸æˆ‘ä»¬å¾®è°ƒçš„VLMså¾ˆå¥½åœ°å¯¹é½ï¼Œæ˜¾è‘—å‡è½»äº†ä¸åŒå¼ºåº¦çš„å¯¹æŠ—æ‰°åŠ¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2405.20216', 'title': 'Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback', 'url': 'https://huggingface.co/papers/2405.20216', 'abstract': 'The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.', 'score': 10, 'issue_id': 3046, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ', 'en': 'May 30', 'zh': '5æœˆ30æ—¥'}, 'hash': '8ffe2bddf2c0ee58', 'authors': ['Sanghyeon Na', 'Yonggyu Kim', 'Hyunjoon Lee'], 'affiliations': ['Kakao'], 'pdf_title_img': 'assets/pdf/title_img/2405.20216.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#optimization', '#training', '#diffusion', '#cv'], 'emoji': 'ğŸ§‘\u200dğŸ¨', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DPO', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ DPO, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼.'}, 'en': {'title': 'Revolutionizing Human Image Generation with Direct Preference Optimization', 'desc': 'This paper addresses the challenges of generating high-quality human images from text descriptions using text-to-image (T2I) methods. It highlights the importance of meeting specific criteria such as human pose and anatomy, which are crucial for realistic image synthesis. The authors propose a novel approach that employs Direct Preference Optimization (DPO) to create a specialized dataset for training without requiring expensive human feedback. Additionally, they introduce a modified loss function that reduces artifacts and enhances image quality, leading to significant improvements in generating human images that align well with textual prompts.'}, 'zh': {'title': 'ä¼˜åŒ–äººç±»å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é€šè¿‡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡äººç±»å›¾åƒçš„æŒ‘æˆ˜ã€‚ä¸ä¸€èˆ¬å›¾åƒç”Ÿæˆä¸åŒï¼Œäººç±»å›¾åƒåˆæˆéœ€è¦æ»¡è¶³ä¸¥æ ¼çš„äººä½“å§¿åŠ¿ã€è§£å‰–ç»“æ„å’Œä¸æ–‡æœ¬æç¤ºå¯¹é½çš„æ ‡å‡†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸“é—¨é’ˆå¯¹äººç±»å›¾åƒç”Ÿæˆï¼Œæ„å»ºé«˜æ•ˆçš„DPOæ•°æ®é›†ä»¥è®­ç»ƒæ¨¡å‹ï¼Œå‡å°‘å¯¹æ˜‚è´µäººç±»åé¦ˆçš„ä¾èµ–ã€‚é€šè¿‡ä¿®æ”¹æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹èƒ½å¤Ÿå‡å°‘ä¼ªå½±å¹¶æé«˜å›¾åƒçš„çœŸå®æ„Ÿï¼Œæ˜¾è‘—æå‡äººç±»å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23573', 'title': 'DASH: Detection and Assessment of Systematic Hallucinations of VLMs', 'url': 'https://huggingface.co/papers/2503.23573', 'abstract': "Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH.", 'score': 9, 'issue_id': 3049, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'cd9c0f9c9392d470', 'authors': ['Maximilian Augustin', 'Yannic Neuhaus', 'Matthias Hein'], 'affiliations': ['Tubingen AI Center, University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2503.23573.jpg', 'data': {'categories': ['#cv', '#hallucinations', '#benchmark', '#transfer_learning', '#training', '#dataset', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'DASH: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DASH - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. DASH Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ DASH Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'DASH: Detecting and Mitigating Object Hallucinations in Vision-Language Models', 'desc': "This paper addresses the issue of object hallucinations in vision-language models (VLMs), where these models incorrectly identify objects in images. The authors introduce DASH, a new automated pipeline that detects and assesses systematic hallucinations in VLMs using large-scale, real-world image datasets. DASH includes a component called DASH-OPT, which generates misleading images to evaluate the VLM's performance on the 'natural image manifold'. The study demonstrates that fine-tuning VLMs with images identified by DASH can significantly reduce the occurrence of object hallucinations."}, 'zh': {'title': 'æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿå¹»è§‰', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å®¹æ˜“å‡ºç°ç‰©ä½“å¹»è§‰ï¼Œå³é”™è¯¯åœ°æŒ‡ç¤ºå›¾åƒä¸­å­˜åœ¨æŸäº›ç‰©ä½“ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„æ ‡è®°æ•°æ®é›†æ¥é‡åŒ–å¹»è§‰ï¼Œä½†è¿™ç§æ–¹æ³•ä¸è¶³ä»¥è¯„ä¼°åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å‡ºç°çš„å¹»è§‰ã€‚æˆ‘ä»¬æå‡ºäº†DASHï¼ˆç³»ç»Ÿå¹»è§‰çš„æ£€æµ‹ä¸è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å¤§è§„æ¨¡ç®¡é“ï¼Œæ—¨åœ¨è¯†åˆ«VLMsåœ¨çœŸå®å›¾åƒä¸­çš„ç³»ç»Ÿå¹»è§‰ã€‚é€šè¿‡ä¼˜åŒ–â€œè‡ªç„¶å›¾åƒæµå½¢â€ï¼ŒDASHèƒ½å¤Ÿç”Ÿæˆè¯¯å¯¼VLMçš„å›¾åƒï¼Œå¹¶è¯†åˆ«å‡ºå¤§é‡çš„çœŸå®å’Œè¯­ä¹‰ç›¸ä¼¼å›¾åƒçš„èšç±»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23135', 'title': 'LSNet: See Large, Focus Small', 'url': 'https://huggingface.co/papers/2503.23135', 'abstract': "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.", 'score': 4, 'issue_id': 3040, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': 'd2ac65a2356c89c3', 'authors': ['Ao Wang', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['BNRist, Tsinghua University', 'Department of Automation, Tsinghua University', 'School of Software, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23135.jpg', 'data': {'categories': ['#architecture', '#training', '#cv'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': "LSNet: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾'", 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾' Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ LS-ÑĞ²ĞµÑ€Ñ‚ĞºÑƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LS-ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LSNet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LSNet Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'See Large, Focus Small: Efficient Vision Networks', 'desc': "This paper addresses the challenges of deploying complex vision networks like Convolutional Neural Networks and Vision Transformers in real-time applications due to their heavy computations. The authors propose a new lightweight network design strategy called 'See Large, Focus Small', inspired by the human vision system's ability to dynamically adjust focus. They introduce LS convolution, which effectively combines large-kernel perception for broad information capture and small-kernel aggregation for precise feature refinement. The resulting LSNet model demonstrates improved performance and efficiency in various vision tasks compared to existing lightweight networks."}, 'zh': {'title': 'è½»é‡çº§è§†è§‰ç½‘ç»œçš„æ–°ç­–ç•¥ï¼šå¤§è§†é‡ï¼Œå°èšç„¦', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§è§†è§‰ç½‘ç»œè®¾è®¡ç­–ç•¥ï¼Œç§°ä¸ºâ€œSee Large, Focus Smallâ€ã€‚è¯¥ç­–ç•¥ç»“åˆäº†å¤§æ ¸æ„ŸçŸ¥å’Œå°æ ¸èšåˆçš„LSå·ç§¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•æ‰å¹¿æ³›çš„æ„ŸçŸ¥ä¿¡æ¯å¹¶å®ç°ç²¾ç¡®çš„ç‰¹å¾èšåˆã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒLSNetåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå…‹æœäº†ç°æœ‰è½»é‡çº§æ¨¡å‹åœ¨è®¡ç®—é¢„ç®—æœ‰é™æƒ…å†µä¸‹çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSNetåœ¨å®æ—¶åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚åˆå®é™…éƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00406', 'title': 'VerifiAgent: a Unified Verification Agent in Language Model Reasoning', 'url': 'https://huggingface.co/papers/2504.00406', 'abstract': 'Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent', 'score': 2, 'issue_id': 3046, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'd4ef7ad5ea6d5aad', 'authors': ['Jiuzhou Han', 'Wray Buntine', 'Ehsan Shareghi'], 'affiliations': ['College of Engineering and Computer Science, VinUniversity', 'Department of Data Science & AI, Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00406.jpg', 'data': {'categories': ['#interpretability', '#training', '#math', '#agents', '#reasoning', '#inference'], 'emoji': 'ğŸ”', 'ru': {'title': 'VerifiAgent: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VerifiAgent - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. VerifiAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¼ĞµÑ‚Ğ°-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ VerifiAgent Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'VerifiAgent: Enhancing Reliability in Language Model Reasoning', 'desc': "This paper introduces VerifiAgent, a novel verification system designed to improve the reliability of large language models' responses. It combines two verification levels: meta-verification for checking the completeness and consistency of answers, and tool-based adaptive verification that selects the best verification tools based on the reasoning type. This approach enhances efficiency and robustness across various reasoning tasks, outperforming traditional verification methods. Additionally, VerifiAgent improves reasoning accuracy by utilizing feedback from its verification processes and is more cost-effective in mathematical reasoning applications."}, 'zh': {'title': 'VerifiAgentï¼šæ™ºèƒ½éªŒè¯ï¼Œæå‡æ¨ç†å‡†ç¡®æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¸¸å¸¸äº§ç”Ÿä¸å¯é æˆ–é”™è¯¯çš„å›ç­”ã€‚ç°æœ‰çš„éªŒè¯æ–¹æ³•é€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šæ¨¡å‹æˆ–é¢†åŸŸï¼Œè®¡ç®—èµ„æºæ¶ˆè€—å¤§ï¼Œä¸”åœ¨ä¸åŒæ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VerifiAgentï¼Œä¸€ä¸ªç»Ÿä¸€çš„éªŒè¯ä»£ç†ï¼Œé›†æˆäº†ä¸¤çº§éªŒè¯ï¼šå…ƒéªŒè¯è¯„ä¼°æ¨¡å‹å›ç­”çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼Œå·¥å…·è‡ªé€‚åº”éªŒè¯åˆ™æ ¹æ®æ¨ç†ç±»å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„éªŒè¯å·¥å…·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVerifiAgentåœ¨æ‰€æœ‰æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºåŸºçº¿éªŒè¯æ–¹æ³•ï¼Œå¹¶èƒ½é€šè¿‡åˆ©ç”¨éªŒè¯ç»“æœçš„åé¦ˆè¿›ä¸€æ­¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22879', 'title': 'Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models', 'url': 'https://huggingface.co/papers/2503.22879', 'abstract': 'State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input x, combined with a per-state-group quantization for input-dependent parameters B and C. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3times and 3times speed-ups in the pre-filling and generation stages, respectively, while offering 4times memory reduction with only a 1.6% average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.', 'score': 2, 'issue_id': 3059, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': 'dbc6e89f0857b2e7', 'authors': ['Hung-Yueh Chiang', 'Chi-Chih Chang', 'Natalia Frumkin', 'Kai-Chiang Wu', 'Mohamed S. Abdelfattah', 'Diana Marculescu'], 'affiliations': ['Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin', 'Department of Computer Science, National Yang Ming Chiao Tung University', 'Department of Electrical and Computer Engineering, Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22879.jpg', 'data': {'categories': ['#open_source', '#inference', '#training', '#architecture', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Quamba2 - Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ (W8A8, W4A8, W4A16) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mamba1 Ğ¸ Mamba2. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº 8-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Quamba2-8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ SSM, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 1.3 Ğ¸ 3 Ñ€Ğ°Ğ·Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMLU Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°.'}, 'en': {'title': 'Quamba2: Efficient Quantization for State Space Models', 'desc': 'This paper introduces Quamba2, a method for quantizing State Space Models (SSMs) to improve their efficiency on various platforms. By using low bit-width data formats, Quamba2 reduces the model size and enhances performance without significant accuracy loss. The approach involves sorting and clustering inputs for better quantization of parameters, ensuring that the output remains stable despite the changes. Experimental results demonstrate that Quamba2 achieves significant speed-ups and memory reductions compared to existing SSM quantization techniques.'}, 'zh': {'title': 'é‡åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡ï¼', 'desc': 'çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºä¸€ç§æ–°å…´çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› å…¶ä¸€è‡´çš„å†…å­˜ä½¿ç”¨å’Œé«˜æ€§èƒ½è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨äº‘æœåŠ¡æˆ–èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šæ‰©å±•SSMsé¢ä¸´å­˜å‚¨å’Œè®¡ç®—èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½¿ç”¨ä½ä½å®½æ•°æ®æ ¼å¼å¯¹SSMsè¿›è¡Œé‡åŒ–å¯ä»¥å‡å°‘æ¨¡å‹å¤§å°ï¼Œå¹¶åˆ©ç”¨ç¡¬ä»¶åŠ é€Ÿã€‚æˆ‘ä»¬æå‡ºçš„Quamba2èƒ½å¤Ÿæ”¯æŒå¤šç§ä½å®½é…ç½®ï¼Œä¼˜åŒ–äº†SSMsåœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä¿æŒè¾ƒé«˜å‡†ç¡®ç‡çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†è®¡ç®—é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01201', 'title': 'Medical large language models are easily distracted', 'url': 'https://huggingface.co/papers/2504.01201', 'abstract': "Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information.", 'score': 1, 'issue_id': 3052, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'cfc7db001feb908d', 'authors': ['Krithik Vishwanath', 'Anton Alyakin', 'Daniel Alexander Alber', 'Jin Vivian Lee', 'Douglas Kondziolka', 'Eric Karl Oermann'], 'affiliations': ['Center for Data Science, New York University, New York, New York, 10016', 'Department of Aerospace Engineering and Engineering Mechanics, The University of Texas at Austin, Austin, Texas, 78712', 'Department of Mathematics, The University of Texas at Austin, Austin, Texas, 78712', 'Department of Neurological Surgery, NYU Langone Medical Center, New York, New York, 10016', 'Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, Missouri, 63110', 'Department of Radiology, NYU Langone Medical Center, New York, New York, 10016'], 'pdf_title_img': 'assets/pdf/title_img/2504.01201.jpg', 'data': {'categories': ['#benchmark', '#rag', '#healthcare', '#reasoning', '#hallucinations'], 'emoji': 'ğŸ©º', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ·ĞµÑ€ĞµĞ½ Ğ¾Ñ‚ Ğ¿Ğ»ĞµĞ²ĞµĞ»', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MedDistractQA, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ USMLE Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ½Ğ° 17.9%. Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº RAG Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğµ Ñ€ĞµÑˆĞ°ÑÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñƒ LLM Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing LLMs: Tackling Noise in Medical Contexts', 'desc': "This paper explores the challenges faced by large language models (LLMs) in medical settings, particularly when they encounter irrelevant information during clinical scenarios. The authors created a benchmark called MedDistractQA, which includes USMLE-style questions with distractions that mimic real-world clinical noise. Their research found that such distractions can significantly decrease the accuracy of LLMs, by as much as 17.9%. Additionally, common strategies to improve model performance, like retrieval-augmented generation and medical fine-tuning, did not alleviate the issue and sometimes worsened it, indicating a fundamental limitation in LLMs' ability to filter relevant clinical data."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦ä¸­çš„æŠ—å¹²æ‰°èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸå…·æœ‰å˜é©æ½œåŠ›ï¼Œä½†ç°å®ä¸´åºŠåœºæ™¯ä¸­å­˜åœ¨çš„å¤šä½™ä¿¡æ¯å¯èƒ½ä¼šå½±å“å…¶è¡¨ç°ã€‚æˆ‘ä»¬å¼€å‘äº†MedDistractQAåŸºå‡†ï¼Œä½¿ç”¨åµŒå…¥æ¨¡æ‹Ÿç°å®å¹²æ‰°çš„USMLEé£æ ¼é—®é¢˜æ¥è¯„ä¼°LLMsè¿‡æ»¤ç›¸å…³æ•°æ®çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå¹²æ‰°æ€§é™ˆè¿°ä¼šä½¿LLMçš„å‡†ç¡®æ€§é™ä½å¤šè¾¾17.9%ã€‚è¿™è¡¨æ˜LLMsåœ¨åŒºåˆ†ç›¸å…³ä¸æ— å…³ä¸´åºŠä¿¡æ¯æ–¹é¢ç¼ºä¹å¿…è¦çš„é€»è¾‘æœºåˆ¶ï¼Œç»™å®é™…åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18817', 'title': 'Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations', 'url': 'https://huggingface.co/papers/2503.18817', 'abstract': 'Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.', 'score': 1, 'issue_id': 3046, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'fa652fd57c6312f8', 'authors': ['Jeonghyeon Kim', 'Sangheum Hwang'], 'affiliations': ['Seoul National University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.18817.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#training', '#transfer_learning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² (OoDD) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (MMFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ OoDD. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ±Ğ»Ğ¸Ğ¶Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet-1k OoD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾ÑÑ‚-Ñ…Ğ¾Ğº Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ OoDD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Out-of-Distribution Detection with Multi-Modal Fine-Tuning', 'desc': 'This paper focuses on improving out-of-distribution detection (OoDD) using multi-modal fine-tuning (MMFT) with vision-language models like CLIP. The authors argue that traditional methods often freeze or partially tune pretrained weights, which limits performance on new datasets. They identify a key issue with na"ive fine-tuning methods, which fail to fully utilize the pretrained knowledge due to a modality gap in embeddings. To overcome this, they propose a new training objective that aligns image and text embeddings more effectively, leading to significant improvements in OoDD performance and overall accuracy on benchmark datasets.'}, 'zh': {'title': 'å¤šæ¨¡æ€å¾®è°ƒæå‡åˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¾®è°ƒï¼ˆMMFTï¼‰åœ¨åˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆOoDDï¼‰ä¸­çš„åº”ç”¨ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€æ¨¡æ€æ¨¡å‹ä¸Šï¼Œè€Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•é€šè¿‡å¢å¼ºå›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†OoDDæ€§èƒ½ã€‚æˆ‘ä»¬åˆ†æäº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒçš„çŸ¥è¯†ã€‚é€šè¿‡åœ¨ImageNet-1k OoDåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨OoDDæ€§èƒ½å’ŒIDå‡†ç¡®ç‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18924', 'title': 'MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for\n  Zero-Shot Speech Synthesis', 'url': 'https://huggingface.co/papers/2502.18924', 'abstract': 'While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces MegaTTS 3, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at https://sditdemo.github.io/sditdemo/.', 'score': 1, 'issue_id': 3053, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '538aaff0c9fb3421', 'authors': ['Ziyue Jiang', 'Yi Ren', 'Ruiqi Li', 'Shengpeng Ji', 'Boyang Zhang', 'Zhenhui Ye', 'Chen Zhang', 'Bai Jionghao', 'Xiaoda Yang', 'Jialong Zuo', 'Yu Zhang', 'Rui Liu', 'Xiang Yin', 'Zhou Zhao'], 'affiliations': ['ByteDance', 'Inner Mongolia University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18924.jpg', 'data': {'categories': ['#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸: ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ñ MegaTTS 3', 'desc': 'MegaTTS 3 - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑ‡Ğ¸. MegaTTS 3 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ĞºÑƒÑĞ¾Ñ‡Ğ½Ğ¾-Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Revolutionizing TTS with Sparse Alignment and Flexible Accent Control', 'desc': 'This paper presents MegaTTS 3, a zero-shot text-to-speech (TTS) system that addresses challenges in speech-text alignment. It introduces a sparse alignment algorithm that enhances the robustness and naturalness of generated speech by providing flexible alignment boundaries. The system also incorporates a multi-condition classifier-free guidance strategy for adjusting accent intensity and uses a piecewise rectified flow technique to speed up the generation process. Experiments show that MegaTTS 3 achieves top-tier speech quality while allowing for precise control over accent, generating high-quality audio efficiently.'}, 'zh': {'title': 'MegaTTS 3ï¼šé«˜è‡ªç„¶æ€§ä¸çµæ´»æ§åˆ¶çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œåä¸ºMegaTTS 3ï¼Œæ—¨åœ¨è§£å†³è¯­éŸ³ä¸æ–‡æœ¬å¯¹é½å»ºæ¨¡çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åˆ›æ–°çš„ç¨€ç–å¯¹é½ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸é™åˆ¶æœç´¢ç©ºé—´çš„æƒ…å†µä¸‹ï¼Œå‡å°‘å¯¹é½çš„éš¾åº¦ï¼Œä»è€Œæé«˜è¯­éŸ³çš„è‡ªç„¶æ€§ã€‚MegaTTS 3è¿˜ä½¿ç”¨æ— æ¡ä»¶åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥æ¥è°ƒæ•´å£éŸ³å¼ºåº¦ï¼Œå¹¶é‡‡ç”¨åˆ†æ®µä¿®æ­£æµæŠ€æœ¯åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMegaTTS 3åœ¨é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶æ”¯æŒå¯¹å£éŸ³å¼ºåº¦çš„çµæ´»æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18950', 'title': 'Target-Aware Video Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18950', 'abstract': "We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.", 'score': 0, 'issue_id': 3054, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '4009a703c8832026', 'authors': ['Taeksoo Kim', 'Hanbyul Joo'], 'affiliations': ['RLWRLD', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18950.jpg', 'data': {'categories': ['#video', '#diffusion', '#multimodal', '#robotics', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ°ĞºÑ‚ĞµÑ€Ğ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ°ĞºÑ‚ĞµÑ€ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ÑŒÑ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºÑƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ½Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ°ÑĞºÑƒ Ñ†ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ…Ğ¾Ğ´ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ†ĞµĞ»Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ.'}, 'en': {'title': 'Target-Aware Video Generation: Simplifying Human-Object Interaction', 'desc': "This paper introduces a target-aware video diffusion model that generates videos based on an input image of an actor interacting with a specified target, defined by a segmentation mask, while performing a desired action described by a text prompt. Unlike traditional models that depend on detailed structural or motion cues, this model simplifies the process by using just a mask, leveraging the strengths of pretrained models for realistic action generation. The model incorporates a special token to encode the target's spatial information, and it is fine-tuned with a novel cross-attention loss to ensure alignment between the target mask and the generated actions. Experimental results indicate that this approach significantly improves the accuracy of human-object interactions in generated videos, making it useful for applications like video content creation and 3D motion synthesis."}, 'zh': {'title': 'ç›®æ ‡æ„ŸçŸ¥è§†é¢‘ç”Ÿæˆï¼Œè½»æ¾å®ç°äºº-ç‰©ä½“äº’åŠ¨', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›®æ ‡æ„ŸçŸ¥çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®è¾“å…¥å›¾åƒç”Ÿæˆè§†é¢‘ï¼Œå…¶ä¸­æ¼”å‘˜ä¸æŒ‡å®šç›®æ ‡äº’åŠ¨å¹¶æ‰§è¡Œæ‰€éœ€åŠ¨ä½œã€‚è¯¥ç›®æ ‡é€šè¿‡åˆ†å‰²æ©ç å®šä¹‰ï¼Œæ‰€éœ€åŠ¨ä½œé€šè¿‡æ–‡æœ¬æç¤ºæè¿°ã€‚ä¸ç°æœ‰çš„å¯æ§å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…éœ€ç®€å•çš„æ©ç æ¥æŒ‡ç¤ºç›®æ ‡ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ç”Ÿæˆåˆç†çš„åŠ¨ä½œã€‚è¿™ç§æ–¹æ³•åœ¨å¤„ç†äºº-ç‰©ä½“äº’åŠ¨åœºæ™¯æ—¶ç‰¹åˆ«æœ‰æ•ˆï¼Œèƒ½å¤Ÿåœ¨æœºå™¨äººç­‰åº”ç”¨ä¸­å®ç°é«˜å±‚æ¬¡çš„åŠ¨ä½œè§„åˆ’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01990', 'title': 'Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems', 'url': 'https://huggingface.co/papers/2504.01990', 'abstract': 'The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.', 'score': 145, 'issue_id': 3069, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 31', 'zh': '3æœˆ31æ—¥'}, 'hash': 'f72a29b6411b97b1', 'authors': ['Bang Liu', 'Xinfeng Li', 'Jiayi Zhang', 'Jinlin Wang', 'Tanjin He', 'Sirui Hong', 'Hongzhang Liu', 'Shaokun Zhang', 'Kaitao Song', 'Kunlun Zhu', 'Yuheng Cheng', 'Suyuchen Wang', 'Xiaoqiang Wang', 'Yuyu Luo', 'Haibo Jin', 'Peiyan Zhang', 'Ollie Liu', 'Jiaqi Chen', 'Huan Zhang', 'Zhaoyang Yu', 'Haochen Shi', 'Boyan Li', 'Dekun Wu', 'Fengwei Teng', 'Xiaojun Jia', 'Jiawei Xu', 'Jinyu Xiang', 'Yizhang Lin', 'Tianming Liu', 'Tongliang Liu', 'Yu Su', 'Huan Sun', 'Glen Berseth', 'Jianyun Nie', 'Ian Foster', 'Logan Ward', 'Qingyun Wu', 'Yu Gu', 'Mingchen Zhuge', 'Xiangru Tang', 'Haohan Wang', 'Jiaxuan You', 'Chi Wang', 'Jian Pei', 'Qiang Yang', 'Xiaoliang Qi', 'Chenglin Wu'], 'affiliations': ['Argonne National Laboratory', 'Canada CIFAR AI Chair', 'Duke University', 'Google DeepMind', 'King Abdullah University of Science and Technology', 'MetaGPT', 'Microsoft Research Asia', 'Mila - Quebec AI Institute', 'Nanyang Technological University', 'Penn State University', 'Stanford University', 'The Hong Kong Polytechnic University', 'The Hong Kong University of Science and Technology', 'The Ohio State University', 'University of Georgia', 'University of Illinois at Urbana-Champaign', 'University of Southern California', 'University of Sydney', 'UniversitÃ© de MontrÃ©al', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01990.jpg', 'data': {'categories': ['#architecture', '#survey', '#security', '#ethics', '#agi', '#agents', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ: Ğ¾Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ¼, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸, Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Building Intelligent Agents: From Brain-Inspired Design to Safe AI Deployment', 'desc': 'This paper explores the development of large language models (LLMs) and their role in creating advanced intelligent agents that can reason, perceive, and act in various environments. It presents a modular architecture inspired by the human brain, detailing how cognitive, perceptual, and operational modules correspond to brain functions like memory and emotion. The paper also discusses how these agents can improve themselves through adaptive learning and optimization techniques, including AutoML. Finally, it emphasizes the importance of ensuring that AI systems are safe, ethical, and reliable for real-world applications.'}, 'zh': {'title': 'æ™ºèƒ½ä½“çš„æœªæ¥ï¼šä»å¤§è„‘å¯å‘åˆ°å®‰å…¨åº”ç”¨', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å˜é©æ€§å½±å“ï¼Œå¼ºè°ƒäº†æ™ºèƒ½ä½“çš„è®¾è®¡ã€è¯„ä¼°å’ŒæŒç»­æ”¹è¿›æ‰€é¢ä¸´çš„å¤æ‚æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†æ™ºèƒ½ä½“æ¡†æ¶ç½®äºæ¨¡å—åŒ–çš„ã€å—å¤§è„‘å¯å‘çš„æ¶æ„ä¸­ï¼Œç»“åˆäº†è®¤çŸ¥ç§‘å­¦ã€ç¥ç»ç§‘å­¦å’Œè®¡ç®—ç ”ç©¶çš„åŸåˆ™ã€‚æ–‡ç« åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼Œé¦–å…ˆåˆ†ææ™ºèƒ½ä½“çš„æ¨¡å—åŒ–åŸºç¡€ï¼Œæ˜ å°„å…¶è®¤çŸ¥ã€æ„ŸçŸ¥å’Œæ“ä½œæ¨¡å—ä¸äººç±»å¤§è„‘åŠŸèƒ½çš„ç›¸ä¼¼æ€§ã€‚æ¥ç€è®¨è®ºè‡ªæˆ‘å¢å¼ºå’Œé€‚åº”æ€§è¿›åŒ–æœºåˆ¶ï¼Œæœ€åå¼ºè°ƒæ„å»ºå®‰å…¨ã€å¯é å’Œæœ‰ç›Šçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02507', 'title': 'ZClip: Adaptive Spike Mitigation for LLM Pre-Training', 'url': 'https://huggingface.co/papers/2504.02507', 'abstract': 'Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.', 'score': 69, 'issue_id': 3065, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '290019150fe5b4c9', 'authors': ['Abhay Kumar', 'Louis Owen', 'Nilabhra Roy Chowdhury', 'Fabian GÃ¼ra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2504.02507.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ZClip: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ZClip Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ZClip Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ¾Ñ€Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ z-Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¾Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ZClip Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğµ Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ½Ğµ Ğ¼ĞµÑˆĞ°Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ….'}, 'en': {'title': 'ZClip: Smart Gradient Clipping for Stable LLM Training', 'desc': 'This paper addresses the challenges of training large language models (LLMs) by introducing ZClip, an adaptive gradient clipping algorithm. Traditional methods often fail to manage gradient instability and loss spikes effectively, leading to inefficient training and the need for manual adjustments. ZClip improves upon these methods by dynamically adjusting the clipping threshold based on the statistical behavior of gradient norms, allowing for a more responsive training process. By using z-score-based anomaly detection, ZClip prevents harmful loss spikes while maintaining the overall convergence of the model.'}, 'zh': {'title': 'è‡ªé€‚åº”æ¢¯åº¦è£å‰ªï¼Œæå‡è®­ç»ƒç¨³å®šæ€§', 'desc': 'åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¸¸å¸¸ä¼šé‡åˆ°æ¢¯åº¦ä¸ç¨³å®šå’ŒæŸå¤±å³°å€¼ç­‰é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´ç¾éš¾æ€§çš„å‘æ•£ã€‚ä¼ ç»Ÿçš„æ¢¯åº¦è£å‰ªæŠ€æœ¯æ— æ³•æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºå›ºå®šçš„é˜ˆå€¼æˆ–å¯å‘å¼æ–¹æ³•ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºZClipçš„è‡ªé€‚åº”æ¢¯åº¦è£å‰ªç®—æ³•ï¼Œå®ƒæ ¹æ®æ¢¯åº¦èŒƒæ•°çš„ç»Ÿè®¡ç‰¹æ€§åŠ¨æ€è°ƒæ•´è£å‰ªé˜ˆå€¼ã€‚ZClipé€šè¿‡åŸºäºz-scoreçš„å¼‚å¸¸æ£€æµ‹æ¥è¯†åˆ«å’Œå‡è½»å¤§æ¢¯åº¦å³°å€¼ï¼Œä»è€Œé˜²æ­¢æŸå¤±å³°å€¼ï¼ŒåŒæ—¶ä¸å¹²æ‰°æ”¶æ•›è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02826', 'title': 'Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing', 'url': 'https://huggingface.co/papers/2504.02826', 'abstract': 'Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.', 'score': 60, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'dbb1c07cd5a01838', 'authors': ['Xiangyu Zhao', 'Peiyuan Zhang', 'Kexian Tang', 'Hao Li', 'Zicheng Zhang', 'Guangtao Zhai', 'Junchi Yan', 'Hua Yang', 'Xue Yang', 'Haodong Duan'], 'affiliations': ['Shanghai Jiao Tong University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02826.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RISEBench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'RISEBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼. Ğ¢ĞµÑÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ LLM-ÑÑƒĞ´ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'RISEBench: Advancing Reasoning in Visual Editing', 'desc': 'This paper introduces RISEBench, a new benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) in Large Multi-modality Models (LMMs). It identifies challenges in visual editing, such as following complex instructions and maintaining appearance consistency. RISEBench categorizes reasoning into four types: Temporal, Causal, Spatial, and Logical, and provides a framework for assessing these reasoning types through both human and model evaluations. The findings indicate that even advanced models like GPT-4o-Native struggle with logical reasoning, suggesting a need for further research in this area.'}, 'zh': {'title': 'æ¨ç†é©±åŠ¨çš„è§†è§‰ç¼–è¾‘æ–°åŸºå‡†', 'desc': 'å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éµå¾ªå¤æ‚æŒ‡ä»¤ã€ä¿æŒå¤–è§‚ä¸€è‡´æ€§å’Œæ”¯æŒçµæ´»è¾“å…¥æ ¼å¼æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RISEBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨ç†é©±åŠ¨è§†è§‰ç¼–è¾‘ï¼ˆRISEï¼‰çš„åŸºå‡†ã€‚RISEBenchä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ï¼šæ—¶é—´æ¨ç†ã€å› æœæ¨ç†ã€ç©ºé—´æ¨ç†å’Œé€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡GPT-4o-Nativeåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä½†åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ï¼Œæ˜¾ç¤ºå‡ºè¿™ä¸€é¢†åŸŸä»éœ€æ·±å…¥æ¢ç´¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02782', 'title': 'GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation', 'url': 'https://huggingface.co/papers/2504.02782', 'abstract': "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.", 'score': 48, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '5346697bd326eed4', 'authors': ['Zhiyuan Yan', 'Junyan Ye', 'Weijia Li', 'Zilong Huang', 'Shenghai Yuan', 'Xiangyang He', 'Kaiqing Lin', 'Jun He', 'Conghui He', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Rabbitpre AI', 'Shanghai AI Laboratory', 'Shenzhen University', 'Sun Yat-sen University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2504.02782.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#architecture', '#diffusion', '#interpretability', '#optimization', '#hallucinations'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'GPT-4o: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº (GPT-ImgEval) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¾Ñ‚ OpenAI, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GPT-4o Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° GPT-4o Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ GPT-4o, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµĞµ Ñ Gemini 2.0 Flash Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unleashing the Power of GPT-4o in Image Generation and Editing', 'desc': "This paper evaluates the performance of OpenAI's GPT-4o model in image generation and editing using a new benchmark called GPT-ImgEval. The evaluation focuses on three key areas: the quality of generated images, the model's ability to edit images, and its understanding of semantic context. Results show that GPT-4o outperforms existing models in both image generation and editing, while also demonstrating strong reasoning capabilities. The paper also explores the model's architecture and limitations, providing insights for future research in image generation."}, 'zh': {'title': 'GPT-4oï¼šå›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†OpenAIçš„GPT-4oæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„æœ€æ–°çªç ´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºGPT-ImgEvalçš„è¯„ä¼°åŸºå‡†ï¼Œå®šé‡å’Œå®šæ€§åœ°åˆ†æäº†GPT-4oåœ¨ç”Ÿæˆè´¨é‡ã€ç¼–è¾‘èƒ½åŠ›å’ŒçŸ¥è¯†æ¨ç†ç­‰ä¸‰ä¸ªå…³é”®ç»´åº¦çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGPT-4oåœ¨å›¾åƒç”Ÿæˆæ§åˆ¶å’Œè¾“å‡ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å“è¶Šçš„çŸ¥è¯†æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†GPT-4oçš„æ¶æ„ï¼Œå¹¶è¯†åˆ«äº†å…¶åœ¨å›¾åƒç”Ÿæˆä¸­å¸¸è§çš„åˆæˆä¼ªå½±å’Œå±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23377', 'title': 'JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical\n  Spatio-Temporal Prior Synchronization', 'url': 'https://huggingface.co/papers/2503.23377', 'abstract': 'This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce a fine-grained spatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatio-temporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose a new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise a robust metric for evaluating the synchronization between generated audio-video pairs in real-world complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting a new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/.', 'score': 41, 'issue_id': 3076, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': '5437e5bc1ca6fe1e', 'authors': ['Kai Liu', 'Wei Li', 'Lai Chen', 'Shengqiong Wu', 'Yanhao Zheng', 'Jiayi Ji', 'Fan Zhou', 'Rongxin Jiang', 'Jiebo Luo', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['National University of Singapore', 'University of Rochester', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23377.jpg', 'data': {'categories': ['#audio', '#benchmark', '#multimodal', '#diffusion', '#dataset', '#open_source', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ JavisDiT - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ HiST-Sypo Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº JavisBench Ğ¸Ğ· 10,140 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ JavisDiT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'JavisDiT: Synchronizing Audio and Video Generation with Precision', 'desc': "This paper presents JavisDiT, a new model for generating audio and video together, called Joint Audio-Video Diffusion Transformer (JAVG). It uses a special architecture called Diffusion Transformer (DiT) to create high-quality content based on user prompts. To keep the audio and video in sync, the model includes a mechanism that aligns their timing and spatial features through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. Additionally, the authors introduce a benchmark dataset, JavisBench, to evaluate the model's performance in generating synchronized audio-video pairs, showing that JavisDiT outperforms existing methods in both quality and synchronization."}, 'zh': {'title': 'JavisDiTï¼šéŸ³è§†é¢‘ç”Ÿæˆçš„æ–°æ ‡å‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„è”åˆéŸ³è§†é¢‘æ‰©æ•£å˜æ¢å™¨JavisDiTï¼Œæ—¨åœ¨å®ç°åŒæ­¥çš„éŸ³è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¨¡å‹åŸºäºå¼ºå¤§çš„æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œèƒ½å¤Ÿä»å¼€æ”¾å¼ç”¨æˆ·æç¤ºä¸­åŒæ—¶ç”Ÿæˆé«˜è´¨é‡çš„éŸ³é¢‘å’Œè§†é¢‘å†…å®¹ã€‚ä¸ºäº†ç¡®ä¿æœ€ä½³åŒæ­¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»†ç²’åº¦çš„æ—¶ç©ºå¯¹é½æœºåˆ¶ï¼Œé€šè¿‡å±‚æ¬¡åŒ–æ—¶ç©ºåŒæ­¥å…ˆéªŒä¼°è®¡å™¨ï¼ˆHiST-Sypoï¼‰æå–å…¨å±€å’Œç»†ç²’åº¦çš„æ—¶ç©ºå…ˆéªŒï¼ŒæŒ‡å¯¼è§†è§‰å’Œå¬è§‰ç»„ä»¶ä¹‹é—´çš„åŒæ­¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†JavisBenchï¼ŒåŒ…å«10,140ä¸ªé«˜è´¨é‡çš„æ–‡æœ¬æ ‡æ³¨éŸ³è§†é¢‘ï¼Œæ¶µç›–å¤šæ ·çš„åœºæ™¯å’Œå¤æ‚çš„ç°å®ä¸–ç•Œåœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00939', 'title': 'WikiVideo: Article Generation from Multiple Videos', 'url': 'https://huggingface.co/papers/2504.00939', 'abstract': "We present the challenging task of automatically creating a high-level Wikipedia-style article that aggregates information from multiple diverse videos about real-world events, such as natural disasters or political elections. Videos are intuitive sources for retrieval-augmented generation (RAG), but most contemporary RAG workflows focus heavily on text and existing methods for video-based summarization focus on low-level scene understanding rather than high-level event semantics. To close this gap, we introduce WikiVideo, a benchmark consisting of expert-written articles and densely annotated videos that provide evidence for articles' claims, facilitating the integration of video into RAG pipelines and enabling the creation of in-depth content that is grounded in multimodal sources. We further propose Collaborative Article Generation (CAG), a novel interactive method for article creation from multiple videos. CAG leverages an iterative interaction between an r1-style reasoning model and a VideoLLM to draw higher level inferences about the target event than is possible with VideoLLMs alone, which fixate on low-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in both oracle retrieval and RAG settings and find that CAG consistently outperforms alternative methods, while suggesting intriguing avenues for future work.", 'score': 30, 'issue_id': 3076, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'd04bc5c674ac39fc', 'authors': ['Alexander Martin', 'Reno Kriz', 'William Gantt Walden', 'Kate Sanders', 'Hannah Recknor', 'Eugene Yang', 'Francis Ferraro', 'Benjamin Van Durme'], 'affiliations': ['Human Language Technology Center of Excellence', 'Johns Hopkins University', 'University of Maryland Baltimore County'], 'pdf_title_img': 'assets/pdf/title_img/2504.00939.jpg', 'data': {'categories': ['#survey', '#benchmark', '#rag', '#reasoning', '#multimodal', '#video'], 'emoji': 'ğŸ“½ï¸', 'ru': {'title': 'Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ WikiVideo, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Collaborative Article Generation (CAG), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ VideoLLM Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Transforming Videos into Knowledge: The Future of Article Generation', 'desc': 'This paper addresses the challenge of generating comprehensive Wikipedia-style articles from various videos about real-world events. It highlights the limitations of current retrieval-augmented generation (RAG) methods that primarily focus on text and low-level video analysis. The authors introduce WikiVideo, a benchmark that combines expert-written articles with annotated videos to enhance the integration of video content into RAG workflows. They also propose Collaborative Article Generation (CAG), an innovative approach that uses an interactive model to derive higher-level insights from videos, outperforming existing methods in generating detailed and contextually rich articles.'}, 'zh': {'title': 'è§†é¢‘é©±åŠ¨çš„é«˜æ°´å¹³æ–‡ç« ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå³è‡ªåŠ¨åˆ›å»ºé«˜æ°´å¹³çš„ç»´åŸºç™¾ç§‘é£æ ¼æ–‡ç« ï¼Œæ±‡æ€»å…³äºç°å®äº‹ä»¶ï¼ˆå¦‚è‡ªç„¶ç¾å®³æˆ–æ”¿æ²»é€‰ä¸¾ï¼‰çš„å¤šç§è§†é¢‘ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†WikiVideoåŸºå‡†ï¼ŒåŒ…å«ä¸“å®¶æ’°å†™çš„æ–‡ç« å’Œå¯†é›†æ³¨é‡Šçš„è§†é¢‘ï¼Œä»¥æ”¯æŒæ–‡ç« çš„è®ºç‚¹ï¼Œä»è€Œä¿ƒè¿›è§†é¢‘åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹ä¸­çš„æ•´åˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åä½œæ–‡ç« ç”Ÿæˆï¼ˆCAGï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¸r1é£æ ¼æ¨ç†æ¨¡å‹å’ŒVideoLLMçš„è¿­ä»£äº’åŠ¨ï¼Œèƒ½å¤Ÿå¯¹ç›®æ ‡äº‹ä»¶è¿›è¡Œæ›´é«˜å±‚æ¬¡çš„æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAGåœ¨å„ç±»è®¾ç½®ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†æœªæ¥ç ”ç©¶çš„æœ‰è¶£æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02587', 'title': 'Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme', 'url': 'https://huggingface.co/papers/2504.02587', 'abstract': 'Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.', 'score': 27, 'issue_id': 3063, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '58300c3a6e30995f', 'authors': ['Yan Ma', 'Steffi Chern', 'Xuyang Shen', 'Yiran Zhong', 'Pengfei Liu'], 'affiliations': ['Fudan University', 'Generative Artificial Intelligence Lab (GAIR)', 'Minimax', 'SII', 'Shanghai Jiao Tong University (SJTU)'], 'pdf_title_img': 'assets/pdf/title_img/2504.02587.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ RL Ğ² VLM, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Vision-Language Models!', 'desc': 'This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks.'}, 'zh': {'title': 'å»ºç«‹å¯é‡å¤çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå¹¶æ­£åœ¨ç§¯ææ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLåº”ç”¨å¾€å¾€ä¾èµ–äºå¤æ‚çš„æ¡†æ¶ï¼Œé™åˆ¶äº†å¯é‡å¤æ€§å’Œå¯è®¿é—®æ€§ï¼ŒåŒæ—¶ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œä½¿å¾—ç»“æœæ¯”è¾ƒå’Œè®­ç»ƒåŠ¨æ€è§£é‡Šå˜å¾—å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€æ˜çš„ã€ä»é›¶å¼€å§‹çš„RLæ¡†æ¶ï¼Œæä¾›äº†ä¸€ä¸ªç»è¿‡å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†éªŒè¯çš„æœ€å°åŠŸèƒ½å››æ­¥æµç¨‹ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥è¯„ä¼°è®­ç»ƒåŠ¨æ€å’Œåæ€è¡Œä¸ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02495', 'title': 'Inference-Time Scaling for Generalist Reward Modeling', 'url': 'https://huggingface.co/papers/2504.02495', 'abstract': 'Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.', 'score': 25, 'issue_id': 3071, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '07408fa4b72ccb6c', 'authors': ['Zijun Liu', 'Peiyi Wang', 'Runxin Xu', 'Shirong Ma', 'Chong Ruan', 'Peng Li', 'Yang Liu', 'Yu Wu'], 'affiliations': ['DeepSeek-AI', 'Dept. of Computer Sci. & Tech., Tsinghua University', 'Institute for AI Industry Research (AIR), Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02495.jpg', 'data': {'categories': ['#training', '#benchmark', '#rl', '#rlhf', '#reasoning', '#open_source', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RM) Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Principled Critique Tuning (SPCT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ°-RM Ğ´Ğ»Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SPCT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (GRM), Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… RM.'}, 'en': {'title': 'Enhancing Language Models with Scalable Reward Learning', 'desc': 'This paper explores advancements in reinforcement learning (RL) for enhancing large language models (LLMs) by focusing on reward modeling (RM) for general queries. The authors introduce a novel approach called Self-Principled Critique Tuning (SPCT) that improves the generation of reward signals, enabling better inference-time scalability. They also propose a pointwise generative reward modeling (GRM) technique that allows flexibility in handling different input types. Empirical results demonstrate that SPCT significantly enhances the quality and scalability of GRMs, outperforming existing methods while addressing challenges in generalist reward systems.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åè®­ç»ƒä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ”¹è¿›å¥–åŠ±å»ºæ¨¡ï¼ˆRMï¼‰æ¥æé«˜ä¸€èˆ¬æŸ¥è¯¢çš„æ¨ç†æ—¶é—´å¯æ‰©å±•æ€§ï¼Œå¹¶æå‡ºäº†è‡ªæˆ‘åŸåˆ™æ‰¹è¯„è°ƒä¼˜ï¼ˆSPCTï¼‰æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›GRMä¸­çš„å¯æ‰©å±•å¥–åŠ±ç”Ÿæˆè¡Œä¸ºã€‚æˆ‘ä»¬é‡‡ç”¨ç‚¹å¯¹ç‚¹ç”Ÿæˆå¥–åŠ±å»ºæ¨¡ï¼ˆGRMï¼‰ï¼Œä»¥é€‚åº”ä¸åŒè¾“å…¥ç±»å‹å¹¶å®ç°æ¨ç†æ—¶é—´çš„å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPCTæ˜¾è‘—æé«˜äº†GRMçš„è´¨é‡å’Œå¯æ‰©å±•æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•å’Œæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02398', 'title': 'Scaling Analysis of Interleaved Speech-Text Language Models', 'url': 'https://huggingface.co/papers/2504.02398', 'abstract': 'Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.', 'score': 24, 'issue_id': 3067, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'c03d1b64b7e6e276', 'authors': ['Gallil Maimon', 'Michael Hassid', 'Amit Roth', 'Yossi Adi'], 'affiliations': ['Department of Computer Science and Engineering, Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2504.02398.jpg', 'data': {'categories': ['#dataset', '#data', '#audio', '#open_source', '#synthetic', '#transfer_learning', '#training'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (SLM), Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾, Ñ‡ĞµĞ¼ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ»ĞµĞ¹Ğ²Ğ½Ñ‹Ñ… SLM, Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµÑÑÑ‚ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ² Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ñ‚Ğ°ĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ğ½Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Interleaved SLMs: Efficient Scaling for Speech Models!', 'desc': 'This paper investigates the efficiency of interleaved Speech Language Models (SLMs) compared to traditional textless SLMs. It finds that interleaved SLMs, which leverage pre-trained Text Language Models (TextLMs), require less compute and data while achieving competitive performance on speech tasks. The authors conduct a scaling analysis that reveals distinct scaling dynamics, suggesting a need for more compute allocation towards model size rather than training data. Additionally, the study highlights the importance of synthetic data and various TextLM families in enhancing the performance of SLMs.'}, 'zh': {'title': 'äº¤é”™SLMï¼šæ›´é«˜æ•ˆçš„æ‰©å±•ä¹‹è·¯', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†äº¤é”™è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ‰©å±•æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œäº¤é”™SLMåœ¨è®¡ç®—èµ„æºçš„ä½¿ç”¨ä¸Šæ¯”æ— æ–‡æœ¬SLMæ›´ä¸ºé«˜æ•ˆã€‚æˆ‘ä»¬å‘ç°ï¼Œäº¤é”™SLMçš„æ‰©å±•åŠ¨æ€ä¸æ— æ–‡æœ¬SLMæ˜¾è‘—ä¸åŒï¼Œå»ºè®®åœ¨å¢åŠ æ¨¡å‹è§„æ¨¡æ—¶åº”æ›´å¤šåœ°åˆ†é…è®¡ç®—é¢„ç®—ã€‚æœ€ç»ˆï¼Œç»è¿‡æ‰©å±•çš„æ¨¡å‹åœ¨è¯­éŸ³è¯­ä¹‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºä¸é¢†å…ˆæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨çš„è®¡ç®—å’Œæ•°æ®é‡æ›´å°‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02436', 'title': 'SkyReels-A2: Compose Anything in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2504.02436', 'abstract': 'This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.', 'score': 22, 'issue_id': 3063, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': '86b46513a72dbd76', 'authors': ['Zhengcong Fei', 'Debang Li', 'Di Qiu', 'Jiahua Wang', 'Yikun Dou', 'Rui Wang', 'Jingtao Xu', 'Mingyuan Fan', 'Guibin Chen', 'Yang Li', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.02436.jpg', 'data': {'categories': ['#inference', '#multimodal', '#video', '#benchmark', '#open_source'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'SkyReels-A2 - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. SkyReels-A2 ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (E2V).'}, 'en': {'title': 'SkyReels-A2: Mastering Video Generation with Element Control', 'desc': 'This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation.'}, 'zh': {'title': 'SkyReels-A2ï¼šå¯æ§è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†SkyReels-A2ï¼Œä¸€ä¸ªå¯æ§çš„è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºå°†ä»»æ„è§†è§‰å…ƒç´ ï¼ˆå¦‚è§’è‰²ã€ç‰©ä½“ã€èƒŒæ™¯ï¼‰ç»„åˆæˆåˆæˆè§†é¢‘ï¼ŒåŒæ—¶ä¿æŒä¸æ¯ä¸ªå…ƒç´ çš„å‚è€ƒå›¾åƒçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†è¿™ä¸€ä»»åŠ¡ç§°ä¸ºå…ƒç´ åˆ°è§†é¢‘ï¼ˆE2Vï¼‰ï¼Œå…¶ä¸»è¦æŒ‘æˆ˜åœ¨äºä¿æŒæ¯ä¸ªå‚è€ƒå…ƒç´ çš„çœŸå®æ€§ï¼Œç¡®ä¿åœºæ™¯çš„è¿è´¯æ€§ï¼Œä»¥åŠå®ç°è‡ªç„¶çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®ç®¡é“ï¼Œä»¥æ„å»ºæç¤º-å‚è€ƒ-è§†é¢‘ä¸‰å…ƒç»„ç”¨äºæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è§†é¢‘ï¼Œå¹¶å®ç°ç²¾ç¡®çš„å…ƒç´ æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02542', 'title': 'Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation', 'url': 'https://huggingface.co/papers/2504.02542', 'abstract': 'Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.', 'score': 19, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'fa93ea3aeacd0dbc', 'authors': ['Fa-Ting Hong', 'Zunnan Xu', 'Zixiang Zhou', 'Jun Zhou', 'Xiu Li', 'Qin Lin', 'Qinglin Lu', 'Dan Xu'], 'affiliations': ['HKUST', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02542.jpg', 'data': {'categories': ['#video', '#multimodal', '#diffusion'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼', 'desc': 'ACTalker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ mamba Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²ĞµÑ‚Ğ²ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ACTalker Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'ACTalker: Multi-Signal Control for Natural Talking Head Synthesis', 'desc': 'This paper presents ACTalker, a novel framework for generating talking head videos that can be controlled by multiple signals simultaneously. Unlike traditional methods that rely on a single control modality, ACTalker employs a parallel mamba structure with multiple branches, each dedicated to a specific facial region. A gate mechanism allows for flexible control, ensuring that different driving signals can manipulate facial features without interference. The introduction of a mask-drop strategy further enhances this capability, enabling independent control of facial regions and resulting in more natural and coordinated video outputs.'}, 'zh': {'title': 'å¤šä¿¡å·æ§åˆ¶çš„å¯¹è¯å¤´åƒç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºACTalkerçš„ç«¯åˆ°ç«¯è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆè™šæ‹Ÿå¤´åƒçš„å¯¹è¯è§†é¢‘ã€‚è¯¥æ–¹æ³•æ”¯æŒå¤šä¿¡å·å’Œå•ä¿¡å·æ§åˆ¶ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡è®¾è®¡å¹¶è¡Œçš„mambaç»“æ„ï¼Œå…è®¸ä¸åŒçš„é©±åŠ¨ä¿¡å·æ§åˆ¶é¢éƒ¨çš„ç‰¹å®šåŒºåŸŸï¼Œå¹¶ä½¿ç”¨é—¨æ§æœºåˆ¶å®ç°çµæ´»æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACTalkerèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶çš„é¢éƒ¨è§†é¢‘ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ— å†²çªåœ°æ•´åˆå¤šç§é©±åŠ¨ä¿¡å·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00502', 'title': 'ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers', 'url': 'https://huggingface.co/papers/2504.00502', 'abstract': "Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV", 'score': 17, 'issue_id': 3067, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': '1b236225c1d92fd7', 'authors': ['Qianhao Yuan', 'Qingyu Zhang', 'Yanjiang Liu', 'Jiawei Chen', 'Yaojie Lu', 'Hongyu Lin', 'Jia Zheng', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.00502.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal', '#inference'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ MLLM: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ° Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ÑĞ»Ğ¾ĞµĞ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Layer Contribution (LC). ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ MLLM Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ShortV, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ½Ğ¸Ñ…. ShortV Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 60% ÑĞ»Ğ¾ĞµĞ² MLLM, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimizing MLLMs: Freeze the Unnecessary Layers!', 'desc': 'This paper addresses the high computational costs associated with Multimodal Large Language Models (MLLMs) by analyzing layer-wise redundancy. It introduces a new metric called Layer Contribution (LC) to measure how much each layer affects the processing of visual and text tokens. The findings indicate that many layers contribute little to the processing of visual tokens, allowing for optimization. The authors propose a method called ShortV, which identifies and freezes these ineffective layers, resulting in significant reductions in computational costs while preserving model performance.'}, 'zh': {'title': 'ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹ï¼Œé™ä½è®¡ç®—æˆæœ¬', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºå…¶åºå¤§çš„è§„æ¨¡å’Œå¤§é‡çš„è§†è§‰æ ‡è®°ï¼Œé¢ä¸´ç€é«˜è®¡ç®—æˆæœ¬çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åº¦é‡æ ‡å‡†â€”â€”å±‚è´¡çŒ®ï¼ˆLayer Contributionï¼ŒLCï¼‰ï¼Œç”¨äºé‡åŒ–æ¨¡å‹ä¸­å„å±‚å¯¹è§†è§‰å’Œæ–‡æœ¬æ ‡è®°çš„å½±å“ã€‚é€šè¿‡è®¡ç®—å»é™¤æŸå±‚å˜æ¢åæ¨¡å‹è¾“å‡ºçš„å·®å¼‚ï¼ŒLCèƒ½å¤Ÿè¯„ä¼°è¯¥å±‚çš„è´¡çŒ®ã€‚å®éªŒè¡¨æ˜ï¼Œè®¸å¤šå±‚åœ¨å¤„ç†è§†è§‰æ ‡è®°æ—¶çš„è´¡çŒ®å¾ˆå°ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ShortVæ–¹æ³•ï¼Œèƒ½å¤Ÿè¯†åˆ«å¹¶å†»ç»“è¿™äº›æ— æ•ˆå±‚ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02154', 'title': 'FreSca: Unveiling the Scaling Space in Diffusion Models', 'url': 'https://huggingface.co/papers/2504.02154', 'abstract': "Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a ``scaling space'' whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets.", 'score': 12, 'issue_id': 3074, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '1c4d8a95379fa9a1', 'authors': ['Chao Huang', 'Susan Liang', 'Yunlong Tang', 'Li Ma', 'Yapeng Tian', 'Chenliang Xu'], 'affiliations': ['Netflix Eyeline Studios', 'The University of Texas at Dallas', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.02154.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#optimization', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¤ÑƒÑ€ÑŒĞµ-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾- Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ FreSca, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ğ¼ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. FreSca ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹.'}, 'en': {'title': 'Unlocking Image Control with Frequency-Based Scaling', 'desc': "This paper explores the potential of diffusion models in image tasks, focusing on how noise predictions can be manipulated for better control. It introduces a concept called 'scaling space' that allows for fine-tuned semantic editing by analyzing the differences in noise predictions. The authors present FreSca, a novel method that applies guidance scaling to different frequency components of noise in the Fourier domain. This approach not only improves image editing techniques but also enhances performance in image understanding tasks like depth estimation across various datasets."}, 'zh': {'title': 'æ¢ç´¢æ‰©æ•£æ¨¡å‹çš„ç¼©æ”¾ç©ºé—´', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä»»åŠ¡ä¸­æä¾›äº†å‡ºè‰²çš„å¯æ§æ€§ï¼Œä¸»è¦é€šè¿‡å™ªå£°é¢„æµ‹æ¥ç¼–ç ç‰¹å®šä»»åŠ¡çš„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æ— åˆ†ç±»å™¨å¼•å¯¼å®ç°å¯è°ƒç¼©æ”¾ã€‚æœ¬æ–‡æ¢è®¨äº†è¿™ç§ç¼©æ”¾æœºåˆ¶æ‰€å®šä¹‰çš„â€œç¼©æ”¾ç©ºé—´â€ï¼Œå¹¶é‡ç‚¹ç ”ç©¶äº†åŸºäºåæ¼”çš„ç¼–è¾‘æ–¹æ³•ï¼Œå…¶ä¸­æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°é¢„æµ‹ä¹‹é—´çš„å·®å¼‚æºå¸¦äº†é‡è¦çš„è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡å¯¹å™ªå£°é¢„æµ‹çš„å‚…é‡Œå¶åˆ†æï¼Œå‘ç°å…¶ä½é¢‘å’Œé«˜é¢‘æˆåˆ†åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä»¥ä¸åŒæ–¹å¼æ¼”å˜ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†FreScaæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‚…é‡Œå¶åŸŸä¸­ç‹¬ç«‹åœ°å¯¹ä¸åŒé¢‘å¸¦åº”ç”¨å¼•å¯¼ç¼©æ”¾ï¼Œä»è€Œæ˜¾è‘—æå‡ç°æœ‰çš„å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02119', 'title': 'Efficient Model Selection for Time Series Forecasting via LLMs', 'url': 'https://huggingface.co/papers/2504.02119', 'abstract': 'Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.', 'score': 11, 'issue_id': 3063, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '7c31e20ce0a7813b', 'authors': ['Wang Wei', 'Tiankai Yang', 'Hongjie Chen', 'Ryan A. Rossi', 'Yue Zhao', 'Franck Dernoncourt', 'Hoda Eldardiry'], 'affiliations': ['Adobe Research San Jose, CA, USA', 'Adobe Research Seattle, WA, USA', 'Department of Computer Science University of South California Los Angeles, CA, USA', 'Department of Computer Science Virginia Tech Blacksburg, VA, USA', 'Dolby Labs Atlanta, GA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.02119.jpg', 'data': {'categories': ['#dataset', '#training', '#reasoning', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'LLM ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ LLaMA, GPT Ğ¸ Gemini Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» LLM Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Model Selection with Large Language Models', 'desc': 'This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain.'}, 'zh': {'title': 'åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ—¶é—´åºåˆ—é¢„æµ‹çš„æ¨¡å‹é€‰æ‹©', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ¨¡å‹é€‰æ‹©é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›çš„æ€§èƒ½è¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè½»é‡çº§æ›¿ä»£æ–¹æ¡ˆçš„æ–¹æ³•ï¼Œé¿å…äº†æ„å»ºæ˜‚è´µçš„æ€§èƒ½çŸ©é˜µã€‚é€šè¿‡ä¸LLaMAã€GPTå’ŒGeminiçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„å…ƒå­¦ä¹ æŠ€æœ¯å’Œå¯å‘å¼åŸºçº¿ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†LLMsåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­é«˜æ•ˆæ¨¡å‹é€‰æ‹©çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22444', 'title': 'Scaling Laws in Scientific Discovery with AI and Robot Scientists', 'url': 'https://huggingface.co/papers/2503.22444', 'abstract': "Scientific discovery is poised for rapid advancement through advanced robotics and artificial intelligence. Current scientific practices face substantial limitations as manual experimentation remains time-consuming and resource-intensive, while multidisciplinary research demands knowledge integration beyond individual researchers' expertise boundaries. Here, we envision an autonomous generalist scientist (AGS) concept combines agentic AI and embodied robotics to automate the entire research lifecycle. This system could dynamically interact with both physical and virtual environments while facilitating the integration of knowledge across diverse scientific disciplines. By deploying these technologies throughout every research stage -- spanning literature review, hypothesis generation, experimentation, and manuscript writing -- and incorporating internal reflection alongside external feedback, this system aims to significantly reduce the time and resources needed for scientific discovery. Building on the evolution from virtual AI scientists to versatile generalist AI-based robot scientists, AGS promises groundbreaking potential. As these autonomous systems become increasingly integrated into the research process, we hypothesize that scientific discovery might adhere to new scaling laws, potentially shaped by the number and capabilities of these autonomous systems, offering novel perspectives on how knowledge is generated and evolves. The adaptability of embodied robots to extreme environments, paired with the flywheel effect of accumulating scientific knowledge, holds the promise of continually pushing beyond both physical and intellectual frontiers.", 'score': 11, 'issue_id': 3069, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': 'c2d75e49d08273c1', 'authors': ['Pengsong Zhang', 'Heng Zhang', 'Huazhe Xu', 'Renjun Xu', 'Zhenting Wang', 'Cong Wang', 'Animesh Garg', 'Zhibin Li', 'Arash Ajoudani', 'Xinyu Liu'], 'affiliations': ['Georgia Tech', 'Harvard University', 'Istituto Italiano di Tecnologia', 'Rutgers University', 'Tsinghua University', 'Universita di Genova', 'University College of London', 'University of Toronto', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22444.jpg', 'data': {'categories': ['#agents', '#robotics', '#agi', '#science'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ ÑƒÑ‡Ñ‘Ğ½Ñ‹Ğ¹-ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ñ‘Ğ½Ğ¾Ğ³Ğ¾-ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»Ğ° (AGS), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° AGS Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½. ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ AGS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸ĞµĞ¼ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½ÑÑ‚ÑŒÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğ¼ Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing Science with Autonomous Generalist Scientists', 'desc': 'This paper introduces the concept of an Autonomous Generalist Scientist (AGS) that combines artificial intelligence and robotics to automate the entire scientific research process. The AGS can interact with both physical and virtual environments, facilitating knowledge integration across various scientific fields. By automating tasks such as literature review, hypothesis generation, experimentation, and manuscript writing, the AGS aims to significantly reduce the time and resources required for scientific discovery. The authors suggest that as these systems become more integrated into research, they could change how knowledge is generated and evolve, potentially leading to new scaling laws in scientific discovery.'}, 'zh': {'title': 'è‡ªä¸»ç§‘å­¦å®¶ï¼šåŠ é€Ÿç§‘å­¦å‘ç°çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªä¸»é€šç”¨ç§‘å­¦å®¶ï¼ˆAGSï¼‰çš„æ¦‚å¿µï¼Œç»“åˆäº†æ™ºèƒ½ä»£ç†AIå’Œå…·èº«æœºå™¨äººï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–æ•´ä¸ªç ”ç©¶ç”Ÿå‘½å‘¨æœŸã€‚è¯¥ç³»ç»Ÿèƒ½å¤ŸåŠ¨æ€ä¸ç‰©ç†å’Œè™šæ‹Ÿç¯å¢ƒäº’åŠ¨ï¼Œå¹¶ä¿ƒè¿›ä¸åŒç§‘å­¦å­¦ç§‘ä¹‹é—´çš„çŸ¥è¯†æ•´åˆã€‚é€šè¿‡åœ¨æ–‡çŒ®å›é¡¾ã€å‡è®¾ç”Ÿæˆã€å®éªŒå’Œè®ºæ–‡å†™ä½œç­‰ç ”ç©¶é˜¶æ®µåº”ç”¨è¿™äº›æŠ€æœ¯ï¼ŒAGSå¸Œæœ›æ˜¾è‘—å‡å°‘ç§‘å­¦å‘ç°æ‰€éœ€çš„æ—¶é—´å’Œèµ„æºã€‚éšç€è¿™äº›è‡ªä¸»ç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°èå…¥ç ”ç©¶è¿‡ç¨‹ï¼Œç§‘å­¦å‘ç°å¯èƒ½ä¼šéµå¾ªæ–°çš„è§„æ¨¡æ³•åˆ™ï¼Œæä¾›å…³äºçŸ¥è¯†ç”Ÿæˆå’Œæ¼”å˜çš„æ–°è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01871', 'title': 'Interpreting Emergent Planning in Model-Free Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.01871', 'abstract': "We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL", 'score': 10, 'issue_id': 3069, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'bf12d8cbe28bb942', 'authors': ['Thomas Bush', 'Stephen Chung', 'Usman Anwar', 'AdriÃ  Garriga-Alonso', 'David Krueger'], 'affiliations': ['FAR AI', 'Mila, University of Montreal', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2504.01871.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#benchmark', '#rl', '#games', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¸Ğ³Ñ€Ğµ Sokoban. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ DRC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ° Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Unveiling Planning in Model-Free Reinforcement Learning Agents', 'desc': 'This paper provides evidence that model-free reinforcement learning agents can learn to plan by using concept-based interpretability. The authors focus on a model-free agent called DRC, which learns to create internal plans that predict the outcomes of its actions in the Sokoban environment. They explore how the agent identifies relevant concepts, forms plans, and how these plans affect its behavior through interventions. The study reveals that as the agent develops planning capabilities, it also shows improved performance with additional computational resources, resembling advanced search algorithms.'}, 'zh': {'title': 'æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„è§„åˆ’èƒ½åŠ›', 'desc': 'æœ¬æ–‡é¦–æ¬¡æä¾›äº†æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä»£ç†èƒ½å¤Ÿå­¦ä¹ è§„åˆ’çš„æœºåˆ¶æ€§è¯æ®ã€‚æˆ‘ä»¬é€šè¿‡åœ¨Sokobanè¿™ä¸€å¸¸ç”¨åŸºå‡†ä¸Šåº”ç”¨åŸºäºæ¦‚å¿µçš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œå±•ç¤ºäº†DRCä»£ç†å¦‚ä½•åˆ©ç”¨å­¦ä¹ åˆ°çš„æ¦‚å¿µè¡¨ç¤ºæ¥å†…éƒ¨åˆ¶å®šè®¡åˆ’ã€‚å…·ä½“è€Œè¨€ï¼Œä»£ç†èƒ½å¤Ÿé¢„æµ‹è¡ŒåŠ¨å¯¹ç¯å¢ƒçš„é•¿æœŸå½±å“ï¼Œå¹¶å½±å“è¡ŒåŠ¨é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»£ç†çš„è®¡åˆ’å½¢æˆä¸å…¶è¡Œä¸ºä¹‹é—´å­˜åœ¨å› æœå…³ç³»ï¼Œå¹¶ä¸”è¿™ç§è®¡åˆ’çš„å‡ºç°ä¸ä»£ç†åœ¨æµ‹è¯•æ—¶åˆ©ç”¨é¢å¤–è®¡ç®—èƒ½åŠ›çš„èƒ½åŠ›ç›¸å»åˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23162', 'title': 'NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact\n  3D Representations', 'url': 'https://huggingface.co/papers/2503.23162', 'abstract': '3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and a complex encoding and quantization strategy. In this paper, we aim to develop a simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into a compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 45-times average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields.', 'score': 9, 'issue_id': 3073, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 29', 'zh': '3æœˆ29æ—¥'}, 'hash': '33d8d348d9e9fccf', 'authors': ['Zhenyu Tang', 'Chaoran Feng', 'Xinhua Cheng', 'Wangbo Yu', 'Junwu Zhang', 'Yuan Liu', 'Xiaoxiao Long', 'Wenping Wang', 'Li Yuan'], 'affiliations': ['Hong Kong University of Science and Technology', 'Peking University', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23162.jpg', 'data': {'categories': ['#optimization', '#inference', '#dataset', '#3d'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NeuralGS - Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² (3DGS) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ñ‹ (MLP) Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ MLP Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 45-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'NeuralGS: Compact 3D Gaussian Compression with Neural Fields', 'desc': 'This paper introduces NeuralGS, a novel method for compressing 3D Gaussian Splatting (3DGS) representations into a more compact form without relying on complex voxel structures. By leveraging neural fields and Multi-Layer Perceptron (MLP) networks, NeuralGS can effectively encode the attributes of 3D Gaussians while significantly reducing storage requirements. The approach involves clustering Gaussians and fitting them with small MLPs based on their importance scores, leading to a remarkable 45-times reduction in model size. Experimental results show that NeuralGS maintains high visual quality and achieves compression performance comparable to existing methods that use Scaffold-GS.'}, 'zh': {'title': 'ç”¨ç¥ç»åœºå‹ç¼©3Dé«˜æ–¯ç‚¹äº‘çš„åˆ›æ–°æ–¹æ³•', 'desc': '3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰åœ¨è´¨é‡å’Œæ¸²æŸ“é€Ÿåº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶å­˜å‚¨å’Œä¼ è¾“æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNeuralGSçš„ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»åœºè¡¨ç¤ºæ¥å‹ç¼©åŸå§‹3DGSï¼Œé¿å…äº†å¤æ‚çš„ä½“ç´ ç»“æ„å’Œé‡åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬åˆ©ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ç¥ç»ç½‘ç»œå¯¹3Dé«˜æ–¯çš„å±æ€§è¿›è¡Œç¼–ç ï¼Œä»…éœ€å°‘é‡å­˜å‚¨ç©ºé—´ï¼Œå³ä½¿åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­ä¹Ÿèƒ½ä¿æŒè‰¯å¥½çš„è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨¡å‹å¤§å°ä¸Šå¹³å‡å‡å°‘äº†45å€ï¼Œå‹ç¼©æ€§èƒ½ä¸ç°æœ‰çš„ä½“ç´ åŸºç¡€å‹ç¼©æ–¹æ³•ç›¸å½“ï¼Œå±•ç¤ºäº†ç›´æ¥ä½¿ç”¨ç¥ç»åœºå‹ç¼©3DGSçš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02821', 'title': 'Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2504.02821', 'abstract': 'Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.', 'score': 8, 'issue_id': 3069, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 3', 'zh': '4æœˆ3æ—¥'}, 'hash': 'd4568f020b5f2eca', 'authors': ['Mateusz Pach', 'Shyamgopal Karthik', 'Quentin Bouniot', 'Serge Belongie', 'Zeynep Akata'], 'affiliations': ['Helmholtz Munich', 'Munich Center of Machine Learning', 'Munich Data Science Institute', 'Technical University of Munich', 'University of Copenhagen', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2504.02821.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#architecture', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SAE) Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº (VLM). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SAE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² VLM Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ·Ñ€ĞµĞ½Ğ¸Ñ CLIP Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SAE. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ SAE Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ VLM.'}, 'en': {'title': 'Enhancing Vision-Language Models with Sparse Autoencoders', 'desc': 'This paper explores the use of Sparse Autoencoders (SAEs) to improve the interpretability and steerability of Vision-Language Models (VLMs) like CLIP. The authors present a framework to assess how well these models represent single concepts, known as monosemanticity. Their experiments show that SAEs can enhance the clarity of individual neurons in VLMs and align these representations with established expert categories. Importantly, they demonstrate that SAEs can influence the output of multimodal language models without changing the original model architecture.'}, 'zh': {'title': 'ç¨€ç–è‡ªç¼–ç å™¨æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸æ“æ§æ€§', 'desc': 'ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰æœ€è¿‘è¢«è¯æ˜å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ã€‚æœ¬æ–‡å°†SAEsçš„åº”ç”¨æ‰©å±•åˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥è¯„ä¼°è§†è§‰è¡¨ç¤ºçš„å•ä¹‰æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨VLMsä¸Šè®­ç»ƒçš„SAEsæ˜¾è‘—å¢å¼ºäº†å•ä¸ªç¥ç»å…ƒçš„å•ä¹‰æ€§ï¼Œå¹¶å±•ç¤ºäº†ä¸ä¸“å®¶å®šä¹‰ç»“æ„ï¼ˆå¦‚iNaturaliståˆ†ç±»æ³•ï¼‰è‰¯å¥½å¯¹é½çš„å±‚æ¬¡è¡¨ç¤ºã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†å°†SAEsåº”ç”¨äºCLIPè§†è§‰ç¼–ç å™¨ï¼Œå¯ä»¥ç›´æ¥æ“æ§å¤šæ¨¡æ€LLMsï¼ˆå¦‚LLaVAï¼‰çš„è¾“å‡ºï¼Œè€Œæ— éœ€å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.00891', 'title': 'GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning', 'url': 'https://huggingface.co/papers/2504.00891', 'abstract': 'Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.', 'score': 8, 'issue_id': 3066, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 1', 'zh': '4æœˆ1æ—¥'}, 'hash': 'b22a54f43f9d7a89', 'authors': ['Jian Zhao', 'Runze Liu', 'Kaiyan Zhang', 'Zhimu Zhou', 'Junqi Gao', 'Dong Li', 'Jiafei Lyu', 'Zhouyi Qian', 'Biqing Qi', 'Xiu Li', 'Bowen Zhou'], 'affiliations': ['BUPT', 'Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00891.jpg', 'data': {'categories': ['#training', '#optimization', '#math', '#reasoning', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'GenPRM: ĞĞ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenPRM - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GenPRM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PRM Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 23 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'GenPRM: Elevating LLMs with Generative Process Reward Models', 'desc': 'This paper presents GenPRM, a generative process reward model designed to improve the performance of large language models (LLMs) by addressing key challenges faced by existing process reward models (PRMs). GenPRM utilizes Chain-of-Thought (CoT) reasoning and incorporates code verification to enhance the quality of its judgments at each reasoning step. The authors introduce a novel method called Relative Progress Estimation (RPE) to generate high-quality supervision labels and rationale data, leading to significant performance improvements on various reasoning tasks. Experimental results demonstrate that GenPRM outperforms previous PRMs and shows strong capabilities as a critic model for refining policy models, establishing a new approach for process supervision in LLMs.'}, 'zh': {'title': 'ç”Ÿæˆæ€§è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼šæå‡LLMsçš„æ–°èŒƒå¼', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºéªŒè¯å™¨å¯ä»¥æå‡LLMsçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„PRMsé¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæœ‰é™çš„è¿‡ç¨‹ç›‘ç£å’Œæ³›åŒ–èƒ½åŠ›ã€ä¾èµ–äºæ ‡é‡å€¼é¢„æµ‹è€Œæœªåˆ©ç”¨LLMsçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥åŠæ— æ³•æ‰©å±•PRMsçš„æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†GenPRMï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆæ€§è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡ä»£ç éªŒè¯è¿›è¡Œæ˜ç¡®çš„æ€ç»´é“¾æ¨ç†ï¼Œç„¶åå¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œåˆ¤æ–­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenPRMåœ¨å¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„PRMsï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºæ”¿ç­–æ¨¡å‹ç²¾ç‚¼çš„æ‰¹è¯„æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.23542', 'title': 'Whisper-LM: Improving ASR Models with Language Models for Low-Resource\n  Languages', 'url': 'https://huggingface.co/papers/2503.23542', 'abstract': 'Automatic speech recognition systems have undoubtedly advanced with the integration of multilingual and multitask models such as Whisper, which have shown a promising ability to understand and process speech across a wide range of languages. Despite their robustness, these models often fall short in handling the linguistic distinctions of minority languages. This study addresses this gap by integrating traditional and novel language models with fine-tuned Whisper models to raise their performance in less commonly studied languages. Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. We obtained improvements up to 51\\% for in-distribution datasets and up to 34\\% for out-of-distribution sentences using statistical language models, while large language models provided moderate but consistently robust improvement across diverse linguistic contexts. The findings reveal that, while the integration reliably benefits all model sizes, the extent of improvement varies, highlighting the importance of optimized language model parameters. Finally, we emphasize the importance of selecting appropriate evaluation parameters when reporting the results using transformer-based ASR models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge. For further implementation details of this study, the technical documentation and source code are available at http://www.github.com/hitz-zentroa/whisper-lm.', 'score': 8, 'issue_id': 3072, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 30', 'zh': '3æœˆ30æ—¥'}, 'hash': 'c8ef94a1b05fc918', 'authors': ['Xabier de Zuazo', 'Eva Navas', 'Ibon Saratxaga', 'Inma HernÃ¡ez Rioja'], 'affiliations': ['HiTZ - University of the Basque Country - UPV/EHU, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2503.23542.jpg', 'data': {'categories': ['#training', '#inference', '#multilingual', '#dataset', '#low_resource', '#open_source'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Whisper. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Whisper Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸.'}, 'en': {'title': 'Enhancing ASR for Minority Languages with Whisper and Language Models', 'desc': 'This paper discusses advancements in automatic speech recognition (ASR) systems, particularly focusing on multilingual and multitask models like Whisper. It highlights the challenges these models face with minority languages and proposes a solution by combining traditional and novel language models with fine-tuned Whisper models. The study shows significant improvements in word error rates for low-resource languages through rigorous fine-tuning and evaluation. The results indicate that optimized language model parameters can enhance ASR performance across various linguistic contexts, paving the way for more inclusive ASR technologies.'}, 'zh': {'title': 'æå‡å°‘æ•°è¯­è¨€è¯­éŸ³è¯†åˆ«çš„åŒ…å®¹æ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨å°‘æ•°è¯­è¨€ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯é€šè¿‡ç»“åˆä¼ ç»Ÿå’Œæ–°å‹è¯­è¨€æ¨¡å‹ä¸å¾®è°ƒçš„Whisperæ¨¡å‹ã€‚å°½ç®¡Whisperåœ¨å¤šè¯­è¨€å¤„ç†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å°‘æ•°è¯­è¨€æ—¶ä»å­˜åœ¨ä¸è¶³ã€‚æˆ‘ä»¬é€šè¿‡ä¸¥æ ¼çš„å¾®è°ƒå’Œå¤šæ•°æ®é›†è¯„ä¼°ï¼Œæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºåŒ®ä¹çš„æƒ…å†µä¸‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–è¯­è¨€æ¨¡å‹å‚æ•°å¯¹æå‡æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œä»è€Œæ¨åŠ¨äº†æ›´å…·åŒ…å®¹æ€§çš„è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.02012', 'title': 'Instruction-Guided Autoregressive Neural Network Parameter Generation', 'url': 'https://huggingface.co/papers/2504.02012', 'abstract': "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.", 'score': 6, 'issue_id': 3065, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'cbdd586ccd2b682d', 'authors': ['Soro Bedionita', 'Bruno Andreis', 'Song Chong', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai, South Korea', 'KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.02012.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#optimization', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'IGPG: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€', 'desc': 'IGPG - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° VQ-VAE Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ÑĞµÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸. IGPG Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸Ğ· ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IGPG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€.'}, 'en': {'title': 'IGPG: Unifying Neural Network Parameter Generation for Enhanced Adaptability', 'desc': 'This paper introduces IGPG (Instruction Guided Parameter Generation), a new framework for generating neural network parameters based on task descriptions and architecture specifications. Unlike previous methods, IGPG uses an autoregressive approach that ensures coherence between layers and adapts efficiently to different models and datasets. By employing a VQ-VAE and generating weights at the token level, IGPG captures complex parameter distributions from various pretrained models. The results show that IGPG outperforms existing methods in scalability and efficiency, making it a valuable tool for model adaptation and fine-tuning.'}, 'zh': {'title': 'IGPGï¼šçµæ´»çš„ç¥ç»ç½‘ç»œå‚æ•°ç”Ÿæˆå·¥å…·', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºIGPGï¼ˆæŒ‡ä»¤å¼•å¯¼å‚æ•°ç”Ÿæˆï¼‰çš„è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆç¥ç»ç½‘ç»œå‚æ•°ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡æè¿°å’Œæ¶æ„è§„èŒƒã€‚IGPGé€šè¿‡ç»“åˆVQ-VAEå’Œè‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡å’Œæ¶æ„ä¸­ç»Ÿä¸€å‚æ•°åˆæˆï¼Œç¡®ä¿å±‚é—´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆç¥ç»ç½‘ç»œæƒé‡æ—¶ï¼Œé‡‡ç”¨äº†åŸºäºtokençš„ç”Ÿæˆæ–¹å¼ï¼Œæœ‰æ•ˆæ•æ‰æ¥è‡ªå¤šç§é¢„è®­ç»ƒæ¨¡å‹çš„å¤æ‚å‚æ•°åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIGPGåœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡æ¶æ„çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡æ–¹é¢ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01955', 'title': 'Scene-Centric Unsupervised Panoptic Segmentation', 'url': 'https://huggingface.co/papers/2504.01955', 'abstract': 'Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ.', 'score': 4, 'issue_id': 3079, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '24a963913e1c2220', 'authors': ['Oliver Hahn', 'Christoph Reich', 'Nikita Araslanov', 'Daniel Cremers', 'Christian Rupprecht', 'Stefan Roth'], 'affiliations': ['ELIZA', 'MCML', 'TU Darmstadt', 'TU Munich', 'University of Oxford', 'hessian.AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.01955.jpg', 'data': {'categories': ['#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚ĞºĞ°Ñ… Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Unsupervised Panoptic Segmentation with Scene-Centric Learning', 'desc': 'This paper introduces a new method for unsupervised panoptic segmentation, which is the task of dividing an image into meaningful areas and separate object instances without using labeled data. The authors propose a novel approach that focuses on scene-centric images, eliminating the reliance on object-centric training data. They develop a technique to generate high-resolution pseudo labels by integrating visual features, depth information, and motion cues. Their method demonstrates a significant improvement in panoptic segmentation quality, achieving a 9.4% increase in performance on the Cityscapes dataset compared to previous state-of-the-art methods.'}, 'zh': {'title': 'æ— ç›‘ç£å…¨æ™¯åˆ†å‰²çš„æ–°çªç ´', 'desc': 'æ— ç›‘ç£å…¨æ™¯åˆ†å‰²æ—¨åœ¨å°†å›¾åƒåˆ’åˆ†ä¸ºå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„åŒºåŸŸå’Œç‹¬ç‰¹çš„ç‰©ä½“å®ä¾‹ï¼Œè€Œæ— éœ€ä¾èµ–äººå·¥æ ‡æ³¨çš„æ•°æ®ã€‚ä¸ä¹‹å‰çš„æ— ç›‘ç£å…¨æ™¯åœºæ™¯ç†è§£å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†å¯¹ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ•°æ®çš„éœ€æ±‚ï¼Œä»è€Œå®ç°å¯¹å¤æ‚åœºæ™¯çš„æ— ç›‘ç£ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— ç›‘ç£å…¨æ™¯æ–¹æ³•ï¼Œç›´æ¥åœ¨åœºæ™¯ä¸­å¿ƒå›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œç»“åˆè§†è§‰è¡¨ç¤ºã€æ·±åº¦å’Œè¿åŠ¨çº¿ç´¢ï¼Œè·å¾—é«˜åˆ†è¾¨ç‡çš„å…¨æ™¯ä¼ªæ ‡ç­¾ã€‚é€šè¿‡ä¼ªæ ‡ç­¾è®­ç»ƒå’Œå…¨æ™¯è‡ªæˆ‘è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— éœ€ä»»ä½•äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®é¢„æµ‹å¤æ‚åœºæ™¯çš„å…¨æ™¯åˆ†å‰²ï¼Œæ˜¾è‘—æé«˜äº†å…¨æ™¯è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01943', 'title': 'OpenCodeReasoning: Advancing Data Distillation for Competitive Coding', 'url': 'https://huggingface.co/papers/2504.01943', 'abstract': 'Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community.', 'score': 4, 'issue_id': 3083, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '6581bbd20e92ba34', 'authors': ['Wasi Uddin Ahmad', 'Sean Narenthiran', 'Somshubra Majumdar', 'Aleksander Ficek', 'Siddhartha Jain', 'Jocelyn Huang', 'Vahid Noroozi', 'Boris Ginsburg'], 'affiliations': ['NVIDIA Santa Clara, CA 15213, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.01943.jpg', 'data': {'categories': ['#optimization', '#open_source', '#benchmark', '#reasoning', '#dataset', '#data', '#training'], 'emoji': 'ğŸ’»', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT), Ğ¾Ğ½Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LiveCodeBench Ğ¸ CodeContests Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ±Ñ‹Ğ»Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¾ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ°ĞºÑ†ĞµĞ½Ñ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Unlocking Coding Potential with Enhanced Supervised Fine-Tuning', 'desc': 'This paper discusses the development of a new supervised fine-tuning (SFT) dataset aimed at enhancing the coding capabilities of reasoning-based large language models (LLMs). The authors demonstrate that their distilled models, trained solely on this SFT dataset, achieve superior performance on coding benchmarks compared to those trained with reinforcement learning. They analyze the effects of data sources, code execution filtering, and the diversity of instructions and solutions on model performance. The findings suggest that prioritizing instruction diversity can lead to better outcomes, and the authors plan to share their datasets and models with the research community.'}, 'zh': {'title': 'æå‡ç¼–ç èƒ½åŠ›çš„æ¨ç†æ¨¡å‹æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åŸºäºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¼–ç ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¼˜è´¨çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œä»¥æå‡ä¸åŒè§„æ¨¡æ¨¡å‹çš„ç¼–ç èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨LiveCodeBenchå’ŒCodeContestsä¸Šåˆ†åˆ«è¾¾åˆ°äº†61.8%å’Œ24.6%çš„æˆç»©ï¼Œè¶…è¶Šäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜åˆ†æäº†æ•°æ®æ¥æºã€ä»£ç æ‰§è¡Œè¿‡æ»¤çš„å½±å“ä»¥åŠæŒ‡ä»¤å’Œè§£å†³æ–¹æ¡ˆå¤šæ ·æ€§çš„é‡è¦æ€§ï¼Œå¹¶è®¡åˆ’å°†è¿™äº›æ•°æ®é›†å’Œæ¨¡å‹å¼€æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10479', 'title': 'InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models', 'url': 'https://huggingface.co/papers/2504.10479', 'abstract': 'We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.', 'score': 167, 'issue_id': 3237, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '51475893ef3c1d8b', 'authors': ['Jinguo Zhu', 'Weiyun Wang', 'Zhe Chen', 'Zhaoyang Liu', 'Shenglong Ye', 'Lixin Gu', 'Yuchen Duan', 'Hao Tian', 'Weijie Su', 'Jie Shao', 'Zhangwei Gao', 'Erfei Cui', 'Yue Cao', 'Yangzhou Liu', 'Weiye Xu', 'Hao Li', 'Jiahao Wang', 'Han Lv', 'Dengnian Chen', 'Songze Li', 'Yinan He', 'Tan Jiang', 'Jiapeng Luo', 'Yi Wang', 'Conghui He', 'Botian Shi', 'Xingcheng Zhang', 'Wenqi Shao', 'Junjun He', 'Yingtong Xiong', 'Wenwen Qu', 'Peng Sun', 'Penglong Jiao', 'Lijun Wu', 'Kaipeng Zhang', 'Huipeng Deng', 'Jiaye Ge', 'Kai Chen', 'Limin Wang', 'Min Dou', 'Lewei Lu', 'Xizhou Zhu', 'Tong Lu', 'Dahua Lin', 'Yu Qiao', 'Jifeng Dai', 'Wenhai Wang'], 'affiliations': ['Fudan University', 'Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10479.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#agi', '#training', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'InternVL3: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'InternVL3 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. InternVL3 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ÑĞ´ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (V2PE) Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (MPO). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 72.2 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMMU Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Multimodal Learning with InternVL3', 'desc': 'InternVL3 is a new multimodal large language model (MLLM) that learns from both visual and text data simultaneously during its training. This approach helps it overcome common challenges faced by traditional models that are adapted from text-only systems. It uses innovative techniques like variable visual position encoding and advanced fine-tuning methods to enhance its performance on various tasks. The model has achieved impressive results, outperforming many existing models while also committing to open science by sharing its resources with the research community.'}, 'zh': {'title': 'InternVL3ï¼šå¤šæ¨¡æ€é¢„è®­ç»ƒçš„æ–°æ ‡æ†', 'desc': 'InternVL3æ˜¯InternVLç³»åˆ—çš„é‡è¦è¿›å±•ï¼Œé‡‡ç”¨äº†åŸç”Ÿçš„å¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„å°†æ–‡æœ¬æ¨¡å‹è½¬å˜ä¸ºå¤šæ¨¡æ€æ¨¡å‹ä¸åŒï¼ŒInternVL3åœ¨å•ä¸€é¢„è®­ç»ƒé˜¶æ®µåŒæ—¶å­¦ä¹ å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥å¯å˜è§†è§‰ä½ç½®ç¼–ç ï¼ˆV2PEï¼‰å’Œå…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚ç»è¿‡å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼ŒInternVL3åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨MMMUåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†72.2çš„åˆ†æ•°ï¼Œæˆä¸ºå¼€æºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ–°æ ‡æ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08791', 'title': 'PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters', 'url': 'https://huggingface.co/papers/2504.08791', 'abstract': "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.", 'score': 92, 'issue_id': 3236, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '2d5649ec3925b1a3', 'authors': ['Zonghang Li', 'Tao Li', 'Wenjiao Feng', 'Mohsen Guizani', 'Hongfang Yu'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE', 'University of Electronic Science and Technology of China, Chengdu, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.08791.jpg', 'data': {'categories': ['#inference', '#optimization', '#open_source', '#architecture'], 'emoji': 'ğŸ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ½Ğ° Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ prima.cpp - Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ´Ğ¾ 70 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ CPU/GPU, Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº RAM/VRAM Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Wi-Fi Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Prima.cpp Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ mmap Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ»ÑŒÑ†ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ Ğ´Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Halda Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Bringing Powerful AI to Your Home Devices', 'desc': 'This paper presents prima.cpp, a novel distributed inference system designed to run large language models (LLMs) on standard home devices. It leverages a combination of CPU and GPU resources, along with efficient memory management techniques like mmap and piped-ring parallelism, to optimize performance. By intelligently assigning model layers based on the capabilities of each device, it significantly reduces latency while maintaining low memory usage. The system demonstrates superior performance compared to existing solutions, making advanced AI models accessible for personal use.'}, 'zh': {'title': 'è®©å®¶åº­è®¾å¤‡ä¹Ÿèƒ½è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºprima.cppçš„åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨æ™®é€šå®¶åº­è®¾å¤‡ä¸Šè¿è¡Œ70Bè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ··åˆä½¿ç”¨CPUå’ŒGPUï¼Œä¼˜åŒ–å†…å­˜å’Œå¸¦å®½çš„ä½¿ç”¨ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ¡ˆå¯¹é«˜æ€§èƒ½ç¡¬ä»¶çš„ä¾èµ–ã€‚å®ƒé‡‡ç”¨äº†mmapç®¡ç†æ¨¡å‹æƒé‡ï¼Œå¹¶å¼•å…¥äº†ç®¡é“ç¯å¹¶è¡Œå’Œé¢„å–æŠ€æœ¯ï¼Œä»¥å‡å°‘ç£ç›˜åŠ è½½æ—¶é—´ã€‚é€šè¿‡ä¼˜åŒ–è®¡ç®—ã€é€šä¿¡å’Œå†…å­˜ç®¡ç†ï¼Œprima.cppæ˜¾è‘—é™ä½äº†å»¶è¿Ÿï¼Œä½¿å¾—å…ˆè¿›çš„AIæ¨¡å‹èƒ½å¤Ÿåœ¨å®¶åº­åŠ©æ‰‹ä¸­æ™®åŠã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09925', 'title': 'FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding', 'url': 'https://huggingface.co/papers/2504.09925', 'abstract': 'We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION', 'score': 34, 'issue_id': 3236, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '948c65f51f6a11b7', 'authors': ['Zheng Liu', 'Mengjie Liu', 'Jingzhou Chen', 'Jingwei Xu', 'Bin Cui', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Nanjing University', 'Peking University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2504.09925.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#open_source', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'FUSION: Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'FUSION - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. FUSION Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'FUSION: Deep Integration of Vision and Language for Enhanced Understanding', 'desc': 'FUSION is a new type of multimodal large language model (MLLM) that integrates vision and language more effectively than previous models. It uses a method called Text-Guided Unified Vision Encoding to combine text and visual information at a very detailed level, allowing for better understanding of images in context. The model also features Context-Aware Recursive Alignment Decoding, which helps it to refine visual features based on the text it is processing. With a focus on high-quality question-answer pairs, FUSION shows significant improvements over existing models in various benchmarks, even with fewer visual tokens.'}, 'zh': {'title': 'FUSIONï¼šæ·±åº¦é›†æˆçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†FUSIONï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé‡‡ç”¨å®Œå…¨çš„è§†è§‰-è¯­è¨€å¯¹é½å’Œé›†æˆèŒƒå¼ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºLLMè§£ç è¿‡ç¨‹ä¸­çš„åæœŸæ¨¡æ€äº¤äº’ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•´ä¸ªå¤„ç†æµç¨‹ä¸­å®ç°äº†æ·±åº¦ã€åŠ¨æ€çš„é›†æˆã€‚æˆ‘ä»¬æå‡ºäº†æ–‡æœ¬å¼•å¯¼çš„ç»Ÿä¸€è§†è§‰ç¼–ç ï¼Œå°†æ–‡æœ¬ä¿¡æ¯èå…¥è§†è§‰ç¼–ç ï¼Œå®ç°åƒç´ çº§çš„é›†æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é€’å½’å¯¹é½è§£ç ï¼Œèƒ½å¤Ÿåœ¨è§£ç è¿‡ç¨‹ä¸­æ ¹æ®æ–‡æœ¬ä¸Šä¸‹æ–‡é€’å½’èšåˆè§†è§‰ç‰¹å¾ï¼Œä»è€Œå®ç°ç»†ç²’åº¦çš„é—®é¢˜çº§è¯­ä¹‰é›†æˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08837', 'title': 'VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.08837', 'abstract': "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.", 'score': 34, 'issue_id': 3237, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'e73823d36c951e4e', 'authors': ['Haozhe Wang', 'Chao Qu', 'Zuming Huang', 'Wei Chu', 'Fangzhen Lin', 'Wenhu Chen'], 'affiliations': ['HKUST', 'INF.AI', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.08837.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#rl', '#math', '#training', '#rlhf', '#optimization', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ (SSR) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VL-Rethinker, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Enhancing Slow-Thinking in Vision-Language Models with Reinforcement Learning', 'desc': 'This paper presents VL-Rethinker, a vision-language model that enhances slow-thinking capabilities through reinforcement learning. It introduces Selective Sample Replay (SSR) to tackle the vanishing advantages problem in reinforcement learning, improving performance on math and science benchmarks. Additionally, the model incorporates Forced Rethinking, which adds a self-reflection step during training to promote deeper reasoning. As a result, VL-Rethinker achieves state-of-the-art scores on multiple benchmarks, demonstrating significant advancements in multimodal reasoning.'}, 'zh': {'title': 'æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŠ€æœ¯ï¼Œç§°ä¸ºé€‰æ‹©æ€§æ ·æœ¬é‡æ”¾ï¼ˆSSRï¼‰ï¼Œä»¥è§£å†³ä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ï¼Œå¹¶ç»“åˆå¼ºåˆ¶é‡æ–°æ€è€ƒçš„æ–¹æ³•ï¼Œå¢å¼ºæ¨¡å‹çš„è‡ªæˆ‘åæ€èƒ½åŠ›ã€‚é€šè¿‡è¿™ä¸¤ç§æŠ€æœ¯çš„ç»“åˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹VL-Rethinkeråœ¨å¤šä¸ªæ•°å­¦å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚æœ€ç»ˆï¼ŒVL-Rethinkeråœ¨å¤šå­¦ç§‘åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¾¾åˆ°äº†å¼€æºçš„æœ€æ–°æ°´å¹³ï¼Œç¼©å°äº†ä¸ç°æœ‰æœ€ä½³æ¨¡å‹çš„å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08003', 'title': "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability", 'url': 'https://huggingface.co/papers/2504.08003', 'abstract': "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.", 'score': 34, 'issue_id': 3236, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '7b12ba874d92915a', 'authors': ['Ning Li', 'Jingran Zhang', 'Justin Cui'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2504.08003.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#alignment', '#multimodal', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'GPT-4o: ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¾Ñ‚ OpenAI Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ±ÑƒĞºĞ²Ğ°Ğ»ÑŒĞ½Ğ¾, Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…, Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Bridging the Gaps in Multimodal Understanding', 'desc': "This paper evaluates OpenAI's multimodal model, GPT-4o, focusing on its ability to integrate knowledge and reasoning in image generation and editing. The study examines three key areas: how well the model follows global instructions, its precision in fine-grained editing, and its reasoning after generating images. Despite strong performance in generating images, the model often misinterprets instructions, applies knowledge inconsistently, and struggles with tasks requiring conditional reasoning. The authors suggest that improvements in training and evaluation methods are needed to enhance the model's contextual understanding and reasoning capabilities."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ç†è§£ä¸æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†OpenAIçš„å¤šæ¨¡æ€æ¨¡å‹GPT-4oåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨å…¨çƒæŒ‡ä»¤éµå¾ªã€ç²¾ç»†ç¼–è¾‘ç²¾åº¦å’Œç”Ÿæˆåæ¨ç†ä¸‰ä¸ªç»´åº¦çš„è¡¨ç°ã€‚å°½ç®¡ç°æœ‰åŸºå‡†æ˜¾ç¤ºGPT-4oåœ¨å›¾åƒå¤„ç†ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†å…¶åœ¨æŒ‡ä»¤ç†è§£å’ŒçŸ¥è¯†åº”ç”¨ä¸Šçš„å±€é™æ€§ã€‚æ¨¡å‹å¸¸å¸¸å¯¹æŒ‡ä»¤è¿›è¡Œå­—é¢è§£é‡Šï¼ŒçŸ¥è¯†çº¦æŸåº”ç”¨ä¸ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨æ¡ä»¶æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å¯¹GPT-4oç»Ÿä¸€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„æ™®éå‡è®¾ï¼Œå¼ºè°ƒäº†éœ€è¦æ›´å¼ºå¤§çš„åŸºå‡†å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥å®ç°æ›´å…·ä¸Šä¸‹æ–‡æ„è¯†å’Œæ¨ç†åŸºç¡€çš„å¤šæ¨¡æ€ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09643', 'title': 'Iterative Self-Training for Code Generation via Reinforced Re-Ranking', 'url': 'https://huggingface.co/papers/2504.09643', 'abstract': 'Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality.   One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance.   Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language.', 'score': 29, 'issue_id': 3245, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': '9f5e20f45a50902d', 'authors': ['Nikita Sorokin', 'Ivan Sedykh', 'Valentin Malykh'], 'affiliations': ['International IT University', 'MTS AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.09643.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#plp'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Proximal Policy Optimization (PPO). ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ² Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MultiPL-E Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ 13.4B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ 33B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ GPT-4.'}, 'en': {'title': 'Enhancing Code Generation with Smart Reranking', 'desc': 'This paper addresses the challenges of generating high-quality code using decoder-based models, which often produce unpredictable outputs. It introduces a novel approach that combines a code generation model with a reranker model to select the best solutions from multiple generated samples. The authors propose an iterative self-training method using Proximal Policy Optimization (PPO) to enhance the reranking accuracy and overall code generation process. Their results show that their model, with 13.4 billion parameters, outperforms larger models in both quality and speed, achieving results comparable to GPT-4 in certain programming languages.'}, 'zh': {'title': 'æå‡ä»£ç ç”Ÿæˆè´¨é‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é«˜è´¨é‡ä»£ç ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å½“å‰åŸºäºè§£ç å™¨çš„æ¨¡å‹ä¸­ï¼Œè¾“å‡ºç»“æœå…·æœ‰é«˜åº¦éšæœºæ€§ã€‚é€šè¿‡ç»“åˆä»£ç ç”Ÿæˆæ¨¡å‹å’Œé‡æ’åºæ¨¡å‹ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆä»£ç çš„è´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªæˆ‘è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ¥æå‡é‡æ’åºæ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•´ä½“ä»£ç ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºæ›´å¤§å‚æ•°çš„æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10068', 'title': 'Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model', 'url': 'https://huggingface.co/papers/2504.10068', 'abstract': "Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose Mavors, a novel framework that introduces Multi-granularity video representation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.", 'score': 25, 'issue_id': 3237, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'bbb7251e84f61649', 'authors': ['Yang Shi', 'Jiaheng Liu', 'Yushuo Guan', 'Zhenhua Wu', 'Yuanxing Zhang', 'Zihao Wang', 'Weihong Lin', 'Jingyun Hua', 'Zekun Wang', 'Xinlong Chen', 'Bohan Zeng', 'Wentao Zhang', 'Fuzheng Zhang', 'Wenjing Yang', 'Di Zhang'], 'affiliations': ['CASIA', 'Kuaishou Technology', 'Nanjing University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10068.jpg', 'data': {'categories': ['#reasoning', '#video', '#benchmark', '#architecture', '#long_context', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Mavors: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Mavors Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Mavors Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²ÑƒÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Mavors Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'Mavors: Enhancing Long-Context Video Understanding with Multi-Granularity Representation', 'desc': 'This paper presents Mavors, a new framework designed to improve long-context video understanding in multimodal large language models (MLLMs). Mavors addresses the challenge of maintaining fine-grained spatio-temporal patterns while ensuring computational efficiency. It utilizes an Intra-chunk Vision Encoder to capture high-resolution spatial features and an Inter-chunk Feature Aggregator to ensure temporal coherence across video segments. The framework also integrates image and video understanding by treating images as single-frame videos, leading to superior performance in tasks that require detailed spatio-temporal reasoning.'}, 'zh': {'title': 'Mavorsï¼šé•¿è§†é¢‘ç†è§£çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMavorsçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘ç†è§£ä¸­çš„è®¡ç®—æ•ˆç‡ä¸ç»†ç²’åº¦æ—¶ç©ºæ¨¡å¼ä¿ç•™ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚Mavorsé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°å¯¹åŸå§‹è§†é¢‘å†…å®¹çš„ç¼–ç ï¼šä¸€æ˜¯ä½¿ç”¨3Då·ç§¯å’Œè§†è§‰å˜æ¢å™¨çš„å†…éƒ¨å—è§†è§‰ç¼–ç å™¨ï¼ˆIVEï¼‰ï¼Œä»¥ä¿ç•™é«˜åˆ†è¾¨ç‡çš„ç©ºé—´ç‰¹å¾ï¼›äºŒæ˜¯é€šè¿‡åŸºäºå˜æ¢å™¨çš„ä¾èµ–å»ºæ¨¡å’Œå—çº§æ—‹è½¬ä½ç½®ç¼–ç çš„å—é—´ç‰¹å¾èšåˆå™¨ï¼ˆIFAï¼‰ï¼Œå»ºç«‹å—ä¹‹é—´çš„æ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶è¿˜é€šè¿‡å­å›¾åƒåˆ†è§£å°†å›¾åƒè§†ä¸ºå•å¸§è§†é¢‘ï¼Œä»è€Œç»Ÿä¸€äº†å›¾åƒå’Œè§†é¢‘çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMavorsåœ¨ä¿æŒç©ºé—´ä¿çœŸåº¦å’Œæ—¶é—´è¿ç»­æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»†ç²’åº¦æ—¶ç©ºæ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08942', 'title': 'AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories', 'url': 'https://huggingface.co/papers/2504.08942', 'abstract': 'Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io', 'score': 18, 'issue_id': 3237, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'd756012a0eceafb9', 'authors': ['Xing Han LÃ¹', 'Amirhossein Kazemnejad', 'Nicholas Meade', 'Arkil Patel', 'Dongchan Shin', 'Alejandra Zambrano', 'Karolina StaÅ„czak', 'Peter Shaw', 'Christopher J. Pal', 'Siva Reddy'], 'affiliations': ['Canada CIFAR AI Chair', 'Google DeepMind', 'McGill University', 'Mila Quebec AI Institute', 'Polytechnique MontrÃ©al', 'ServiceNow Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.08942.jpg', 'data': {'categories': ['#interpretability', '#agents', '#benchmark', '#optimization'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'AgentRewardBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AgentRewardBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1302 Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ 5 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ 4 LLM, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ° LLM Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ğ½Ğ¸Ğ¶Ğ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Web Agent Evaluation with LLMs', 'desc': "This paper introduces AgentRewardBench, a new benchmark designed to evaluate the effectiveness of large language models (LLMs) in assessing web agents' performance. Traditional rule-based evaluation methods struggle with flexibility and often fail to accurately identify successful task completions. By comparing 12 different LLM judges across 1302 trajectories, the study reveals that no single LLM consistently outperforms others in all scenarios. The findings emphasize the limitations of rule-based evaluations and advocate for more adaptable automatic evaluation methods to better assess web agents."}, 'zh': {'title': 'è¯„ä¼°ç½‘ç»œä»£ç†çš„æ–°æ–¹æ³•ï¼šAgentRewardBench', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•è¯„ä¼°ç½‘ç»œä»£ç†çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’æ¥å®Œæˆä»»åŠ¡çš„ä»£ç†ã€‚ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•åœ¨æ‰©å±•æ–°ä»»åŠ¡æ—¶å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•å‡†ç¡®è¯†åˆ«æˆåŠŸçš„è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†AgentRewardBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°ç½‘ç»œä»£ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹1302ä¸ªè½¨è¿¹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰å•ä¸€çš„LLMåœ¨æ‰€æœ‰åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¿™è¡¨æ˜éœ€è¦å¼€å‘æ›´çµæ´»çš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10368', 'title': 'S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models', 'url': 'https://huggingface.co/papers/2504.10368', 'abstract': "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.", 'score': 16, 'issue_id': 3239, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'c5995fab2f284493', 'authors': ['Wenyuan Zhang', 'Shuaiyi Nie', 'Xinghua Zhang', 'Zefeng Zhang', 'Tingwen Liu'], 'affiliations': ['Institute of Information Engineering, Chinese Academy of Sciences', 'School of Cyber Security, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.10368.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multilingual'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½ÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ğ¸', 'desc': 'S1-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ 1. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ LRM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ² 15,5 Ñ€Ğ°Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ€Ğ°Ğ½Ğ¾, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ÑÑ‚ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² LRM.'}, 'en': {'title': 'Evaluating Intuition: S1-Bench for Large Reasoning Models', 'desc': 'The paper introduces S1-Bench, a benchmark aimed at assessing Large Reasoning Models (LRMs) on tasks that require quick, intuitive responses, akin to system 1 thinking. It highlights that while LRMs excel in complex reasoning, they struggle with simpler tasks that demand rapid decision-making. The study evaluates 22 LRMs, revealing that they often produce longer outputs and unnecessary deliberation, even when they identify correct answers early. These results indicate that current LRMs exhibit rigid reasoning patterns, suggesting a need for further development to enhance their ability to balance intuitive and analytical thinking.'}, 'zh': {'title': 'è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹çš„ç›´è§‚æ€ç»´èƒ½åŠ›', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†S1-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡æ›´å€¾å‘äºç›´è§‚çš„ç³»ç»Ÿ1æ€ç»´ï¼Œè€Œéæ·±æ€ç†Ÿè™‘çš„ç³»ç»Ÿ2æ¨ç†ã€‚å°½ç®¡LRMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†å®ƒä»¬å¯¹æ·±åº¦åˆ†ææ€ç»´çš„ä¾èµ–å¯èƒ½é™åˆ¶äº†å…¶ç³»ç»Ÿ1æ€ç»´èƒ½åŠ›ã€‚ç›®å‰ç¼ºä¹è¯„ä¼°LRMsåœ¨éœ€è¦è¿™ç§èƒ½åŠ›çš„ä»»åŠ¡è¡¨ç°çš„åŸºå‡†ã€‚S1-Benchæä¾›äº†ä¸€ç»„ç®€å•ã€å¤šæ ·ä¸”è‡ªç„¶æ¸…æ™°çš„é—®é¢˜ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LRMsåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09710', 'title': 'DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training', 'url': 'https://huggingface.co/papers/2504.09710', 'abstract': 'Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.', 'score': 13, 'issue_id': 3236, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': '1d7a588a7370ed5c', 'authors': ['Zhenting Wang', 'Guofeng Cui', 'Kun Wan', 'Wentian Zhao'], 'affiliations': ['Adobe Inc.', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2504.09710.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñ‹ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Upper Confidence Bound Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Adaptive Learning for Enhanced Reasoning in Language Models', 'desc': 'This paper introduces a new approach to improve large language models (LLMs) using reinforcement learning (RL) by focusing on the diverse sources and difficulties of training data. It highlights the importance of adapting the training process to different data distributions, rather than treating all data as the same. The authors propose a curriculum learning framework that uses the Upper Confidence Bound (UCB) principle to prioritize training on data distributions that either have high potential for improvement or are underrepresented. Their experiments show that this method enhances the efficiency and effectiveness of LLM post-training, particularly in complex reasoning tasks.'}, 'zh': {'title': 'ä¼˜åŒ–å­¦ä¹ æ•ˆç‡çš„åˆ†å¸ƒçº§è¯¾ç¨‹å­¦ä¹ æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒçº§å­¦ä¹ èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†è®­ç»ƒæ•°æ®è§†ä¸ºç»Ÿä¸€æ•´ä½“ï¼Œå¿½è§†äº†æ•°æ®åˆ†å¸ƒçš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´ä¸åŒåˆ†å¸ƒçš„é‡‡æ ·æ¦‚ç‡ï¼Œä¼˜å…ˆè€ƒè™‘é«˜å¹³å‡ä¼˜åŠ¿æˆ–ä½æ ·æœ¬æ•°é‡çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é€»è¾‘æ¨ç†æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½ï¼Œå±•ç¤ºäº†åˆ†å¸ƒæ„ŸçŸ¥è¯¾ç¨‹ç­–ç•¥çš„ä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10127', 'title': 'Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization', 'url': 'https://huggingface.co/papers/2504.10127', 'abstract': 'Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.', 'score': 12, 'issue_id': 3242, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '604dff752f16abf2', 'authors': ['Junlei Zhang', 'Zichen Ding', 'Chang Ma', 'Zijie Chen', 'Qiushi Sun', 'Zhenzhong Lan', 'Junxian He'], 'affiliations': ['HKUST', 'Shanghai AI Laboratory', 'The University of Hong Kong', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10127.jpg', 'data': {'categories': ['#agents', '#dataset', '#training', '#optimization', '#multimodal', '#transfer_learning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° (GUI) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… GUI-ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing GUI Agents through Vision Language Model Training', 'desc': 'This paper discusses how to improve the performance of Graphical User Interface (GUI) agents by using Vision Language Models (VLMs) trained on various reasoning tasks. The authors found that training on data-rich tasks helps these models generalize better to GUI planning scenarios, leading to significant performance improvements. They conducted experiments showing that tasks like multimodal reasoning and text-based mathematical data can enhance GUI agent performance more than traditional GUI perception data. The study highlights the importance of cross-domain knowledge transfer and provides a method to overcome data scarcity in training GUI agents.'}, 'zh': {'title': 'æå‡GUIä»£ç†æ€§èƒ½çš„å…³é”®åœ¨äºä»»åŠ¡æ³›åŒ–', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨è‡ªåŠ¨åŒ–å¤æ‚æ•°å­—ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å¦‚ä½•æå‡å…¶æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºåœ¨ä¸“é—¨çš„ä¸­æœŸè®­ç»ƒé˜¶æ®µï¼Œåˆ©ç”¨ä¸°å¯Œçš„æ•°æ®å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡æ¥è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œä»¥ä¿ƒè¿›å…¶åœ¨GUIè§„åˆ’åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹11ä¸ªä¸­æœŸè®­ç»ƒä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°ä»»åŠ¡æ³›åŒ–æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¤šæ¨¡æ€æ•°å­¦æ¨ç†å¯¹AndroidWorldçš„æ€§èƒ½æå‡è¾¾åˆ°äº†6.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°GUIæ„ŸçŸ¥æ•°æ®å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“æœ‰é™ï¼Œå¹¶æå‡ºäº†ä¼˜åŒ–çš„æ··åˆæ•°æ®é›†ï¼Œä»¥å®ç°æ›´å¤§çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10157', 'title': 'SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users', 'url': 'https://huggingface.co/papers/2504.10157', 'abstract': 'Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.', 'score': 11, 'issue_id': 3237, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'ba0402c49e397963', 'authors': ['Xinnong Zhang', 'Jiayu Lin', 'Xinyi Mou', 'Shiyue Yang', 'Xiawei Liu', 'Libo Sun', 'Hanjia Lyu', 'Yihang Yang', 'Weihong Qi', 'Yue Chen', 'Guanying Li', 'Ling Yan', 'Yao Hu', 'Siming Chen', 'Yu Wang', 'Jingxuan Huang', 'Jiebo Luo', 'Shiping Tang', 'Libo Wu', 'Baohua Zhou', 'Zhongyu Wei'], 'affiliations': ['Fudan University', 'Indiana University', 'Shanghai Innovation Institute', 'University of Rochester', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.10157.jpg', 'data': {'categories': ['#rl', '#agents', '#social_simulation', '#alignment', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'SocioVerse: Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SocioVerse - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ·Ñƒ Ğ¸Ğ· 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°, Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SocioVerse ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'SocioVerse: Enhancing Social Simulations with LLMs', 'desc': 'This paper presents SocioVerse, a novel framework that utilizes large language models (LLMs) to enhance social simulations by modeling human interactions and behaviors. The framework addresses alignment challenges by incorporating four key components that ensure accurate representation of diverse user behaviors and environmental interactions. Through extensive simulations in politics, news, and economics, SocioVerse effectively captures population dynamics while maintaining credibility and diversity. The results indicate that this approach can significantly improve the predictive power of social simulations in various domains.'}, 'zh': {'title': 'SocioVerseï¼šç¤¾ä¼šæ¨¡æ‹Ÿçš„æ–°çºªå…ƒ', 'desc': 'ç¤¾ä¼šæ¨¡æ‹Ÿæ­£åœ¨é€šè¿‡æ¨¡æ‹Ÿè™šæ‹Ÿä¸ªä½“ä¸ç¯å¢ƒä¹‹é—´çš„äº’åŠ¨æ¥æ”¹å˜ä¼ ç»Ÿç¤¾ä¼šç§‘å­¦ç ”ç©¶ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œè¿™ç§æ–¹æ³•åœ¨æ•æ‰ä¸ªä½“å·®å¼‚å’Œé¢„æµ‹ç¾¤ä½“è¡Œä¸ºæ–¹é¢æ˜¾ç¤ºå‡ºè¶Šæ¥è¶Šå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ç¯å¢ƒã€ç›®æ ‡ç”¨æˆ·ã€äº’åŠ¨æœºåˆ¶å’Œè¡Œä¸ºæ¨¡å¼æ–¹é¢é¢ä¸´å¯¹é½æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SocioVerseï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMä»£ç†çš„ç¤¾ä¼šæ¨¡æ‹Ÿä¸–ç•Œæ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„å¯¹é½ç»„ä»¶å’Œ1000ä¸‡çœŸå®ä¸ªä½“çš„ç”¨æˆ·æ± ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09763', 'title': 'Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems', 'url': 'https://huggingface.co/papers/2504.09763', 'abstract': 'Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.', 'score': 11, 'issue_id': 3240, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'c4ae0eadf040035f', 'authors': ['Zaid Khan', 'Elias Stengel-Eskin', 'Archiki Prasad', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2504.09763.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#dataset', '#math', '#data', '#training'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ EFA (Executable Functional Abstraction) - Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ EFAGen, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ EFA Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ĞµĞµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. EFAGen ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ EFA Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ EFA Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Automating Advanced Math Problem Generation with EFAs', 'desc': 'This paper introduces Executable Functional Abstractions (EFAs), which are programs designed to generate advanced math problems based on specific instances. The authors present EFAGen, a tool that uses a large language model (LLM) to automatically create EFAs by conditioning on a seed math problem and its solution. They establish criteria for valid EFAs through executable unit tests, which serve as rewards for training the LLM to improve its EFA generation capabilities. The results show that EFAGen can produce diverse and learnable problem variations, enhancing the ability to create tailored math challenges for learners.'}, 'zh': {'title': 'è‡ªåŠ¨ç”Ÿæˆé«˜çº§æ•°å­¦é—®é¢˜çš„å¯æ‰§è¡ŒåŠŸèƒ½æŠ½è±¡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¯æ‰§è¡ŒåŠŸèƒ½æŠ½è±¡ï¼ˆEFAï¼‰çš„ç¨‹åºï¼Œè¿™äº›ç¨‹åºèƒ½å¤Ÿä»ç‰¹å®šçš„æ•°å­¦é—®é¢˜å®ä¾‹ä¸­æ¨å¯¼å‡ºæŠ½è±¡è¿‡ç¨‹ï¼Œå¹¶ç”Ÿæˆæ–°çš„ç›¸å…³å®ä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ„å»ºé«˜çº§æ•°å­¦é—®é¢˜çš„EFAçš„æ–¹æ³•ï¼Œç§°ä¸ºEFAGenï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ ¹æ®ç§å­æ•°å­¦é—®é¢˜åŠå…¶é€æ­¥è§£å†³æ–¹æ¡ˆç”Ÿæˆå€™é€‰EFAç¨‹åºã€‚é€šè¿‡å¯æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼Œæˆ‘ä»¬å®šä¹‰äº†æœ‰æ•ˆEFAå¿…é¡»å…·å¤‡çš„å±æ€§ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™äº›æµ‹è¯•ä½œä¸ºå¯éªŒè¯çš„å¥–åŠ±æ¥è®­ç»ƒLLMï¼Œæé«˜å…¶EFAç¼–å†™èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¯æ˜äº†EFAGenç”Ÿæˆçš„EFAèƒ½å¤Ÿä¿æŒä¸ç§å­é—®é¢˜çš„ä¸€è‡´æ€§ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆé€‚åˆå­¦ä¹ è€…çš„ä¸åŒéš¾åº¦çš„æ•°å­¦é—®é¢˜å˜ä½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10471', 'title': 'MIEB: Massive Image Embedding Benchmark', 'url': 'https://huggingface.co/papers/2504.10471', 'abstract': 'Image representations are often evaluated through disjointed, task-specific protocols, leading to a fragmented understanding of model capabilities. For instance, it is unclear whether an image embedding model adept at clustering images is equally good at retrieving relevant images given a piece of text. We introduce the Massive Image Embedding Benchmark (MIEB) to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. MIEB spans 38 languages across 130 individual tasks, which we group into 8 high-level categories. We benchmark 50 models across our benchmark, finding that no single method dominates across all task categories. We reveal hidden capabilities in advanced vision models such as their accurate visual representation of texts, and their yet limited capabilities in interleaved encodings and matching images and texts in the presence of confounders. We also show that the performance of vision encoders on MIEB correlates highly with their performance when used in multimodal large language models. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.', 'score': 10, 'issue_id': 3248, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '5b9a143dfa0081cd', 'authors': ['Chenghao Xiao', 'Isaac Chung', 'Imene Kerboua', 'Jamie Stirling', 'Xin Zhang', 'MÃ¡rton Kardos', 'Roman Solomatin', 'Noura Al Moubayed', 'Kenneth Enevoldsen', 'Niklas Muennighoff'], 'affiliations': ['Aarhus University', 'Contextual AI', 'Durham University', 'Esker', 'INSA Lyon, LIRIS', 'ITMO University', 'Stanford University', 'The Hong Kong Polytechnic University', 'Zendesk'], 'pdf_title_img': 'assets/pdf/title_img/2504.10471.jpg', 'data': {'categories': ['#survey', '#multimodal', '#cv', '#open_source', '#dataset', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'MIEB: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Massive Image Embedding Benchmark (MIEB) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. MIEB Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 130 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 38 ÑĞ·Ñ‹ĞºĞ°Ñ…, ÑĞ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³ 50 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² MIEB Ğ¸ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Unifying Image Embedding Evaluation with MIEB', 'desc': 'This paper presents the Massive Image Embedding Benchmark (MIEB), a comprehensive evaluation framework for image and image-text embedding models. It addresses the limitations of existing evaluation methods by assessing model performance across 130 tasks in 38 languages, grouped into 8 categories. The study benchmarks 50 different models, revealing that no single model excels in all areas, highlighting both strengths and weaknesses in advanced vision models. Additionally, it finds a strong correlation between the performance of vision encoders on MIEB and their effectiveness in multimodal large language models.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°å›¾åƒåµŒå…¥æ¨¡å‹çš„èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç§°ä¸ºå¤§è§„æ¨¡å›¾åƒåµŒå…¥åŸºå‡†ï¼ˆMIEBï¼‰ï¼Œç”¨äºå…¨é¢è¯„ä¼°å›¾åƒå’Œå›¾åƒ-æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„æ€§èƒ½ã€‚MIEB æ¶‰åŠ 38 ç§è¯­è¨€å’Œ 130 ä¸ªä»»åŠ¡ï¼Œåˆ†ä¸º 8 ä¸ªé«˜å±‚æ¬¡ç±»åˆ«ï¼Œæ—¨åœ¨æä¾›å¯¹æ¨¡å‹èƒ½åŠ›çš„æ›´å…¨é¢ç†è§£ã€‚ç ”ç©¶å‘ç°ï¼Œæ²¡æœ‰å•ä¸€çš„æ–¹æ³•åœ¨æ‰€æœ‰ä»»åŠ¡ç±»åˆ«ä¸­è¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶æ­ç¤ºäº†å…ˆè¿›è§†è§‰æ¨¡å‹åœ¨æ–‡æœ¬è§†è§‰è¡¨ç¤ºæ–¹é¢çš„æ½œåŠ›å’Œåœ¨å›¾åƒä¸æ–‡æœ¬åŒ¹é…ä¸­çš„å±€é™æ€§ã€‚æœ€åï¼Œç»“æœè¡¨æ˜ï¼Œè§†è§‰ç¼–ç å™¨åœ¨ MIEB ä¸Šçš„è¡¨ç°ä¸å…¶åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°é«˜åº¦ç›¸å…³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09641', 'title': 'TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning', 'url': 'https://huggingface.co/papers/2504.09641', 'abstract': 'Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models\' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of "aha moments". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.', 'score': 8, 'issue_id': 3237, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': 'b7c9f390686ff6ef', 'authors': ['Xingjian Zhang', 'Siwei Wen', 'Wenjun Wu', 'Lei Huang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Beihang University', 'Hangzhou International Innovation Institute, Beihang University, Hangzhou, China', 'SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.09641.jpg', 'data': {'categories': ['#reasoning', '#video', '#rl', '#small_models', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: TinyLLaVA-Video-R1 Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TinyLLaVA-Video-R1 - Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. TinyLLaVA-Video-R1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ 'Ğ°Ğ³Ğ°-Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ»ÑÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."}, 'en': {'title': 'Empowering Small Models for Big Reasoning in Video Understanding', 'desc': "This paper introduces TinyLLaVA-Video-R1, a small-scale video reasoning model designed to enhance reasoning abilities using reinforcement learning. Unlike previous models that rely on large-scale datasets and architectures, this model operates with fewer than 4 billion parameters, making it accessible for researchers with limited computational resources. The model not only improves reasoning capabilities on general Video-QA datasets but also demonstrates the ability to explain its reasoning process, showcasing 'aha moments' during its operation. The findings aim to inspire further research into the reasoning abilities of smaller models in the field of video understanding."}, 'zh': {'title': 'å°æ¨¡å‹ä¹Ÿèƒ½æ¨ç†ï¼ŒTinyLLaVA-Video-R1åŠ©åŠ›è§†é¢‘ç†è§£ï¼', 'desc': 'æœ€è¿‘ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„æ¨ç†èƒ½åŠ›å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶åŸºäºé«˜åº¦æ¨ç†å¯†é›†çš„æ•°æ®é›†ï¼Œå¦‚æ•°å­¦å’Œä»£ç ï¼Œä¸”é€šå¸¸é€‰æ‹©å¤§è§„æ¨¡æ¨¡å‹ä½œä¸ºåŸºç¡€ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæ¢ç´¢å°è§„æ¨¡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¯¹è®¡ç®—èµ„æºæœ‰é™çš„ç ”ç©¶è€…ä»ç„¶å…·æœ‰é‡è¦ä»·å€¼ã€‚æ­¤å¤–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè§£é‡Šå…¶åœ¨ä¸€èˆ¬é—®ç­”æ•°æ®é›†ä¸Šçš„æ¨ç†è¿‡ç¨‹åŒæ ·å…·æœ‰æ„ä¹‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08066', 'title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via\n  Agentic Tree Search', 'url': 'https://huggingface.co/papers/2504.08066', 'abstract': 'AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.', 'score': 7, 'issue_id': 3245, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'a1d1275982a7e4b0', 'authors': ['Yutaro Yamada', 'Robert Tjarko Lange', 'Cong Lu', 'Shengran Hu', 'Chris Lu', 'Jakob Foerster', 'Jeff Clune', 'David Ha'], 'affiliations': ['Canada CIFAR AI Chair', 'FLAIR, University of Oxford', 'Sakana AI', 'University of British Columbia', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.08066.jpg', 'data': {'categories': ['#multimodal', '#science', '#healthcare', '#ethics', '#agents', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ˜Ğ˜-ÑƒÑ‡ĞµĞ½Ñ‹Ğ¹: Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ The AI Scientist-v2 - Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµĞ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹, Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ¸ÑˆĞµÑ‚ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ¸ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹, Ğ¾Ğ½Ğ° Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ. ĞĞ´Ğ½Ğ° Ğ¸Ğ· ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ° Ğ½Ğ° Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€ ICLR, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ˜Ğ˜-ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ¿Ñ€Ğ¾ÑˆĞµĞ´ÑˆĞµĞ¹ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': "Revolutionizing Science: The AI Scientist-v2's Autonomous Research Breakthrough", 'desc': 'The paper presents The AI Scientist-v2, an advanced AI system that autonomously conducts scientific research, from hypothesis generation to manuscript writing. This system improves upon its predecessor by eliminating the need for human-written code and effectively generalizing across various machine learning fields. It utilizes a progressive agentic tree-search method and incorporates a Vision-Language Model for enhancing the quality of figures in its manuscripts. The successful submission of AI-generated papers to a peer-reviewed workshop demonstrates the potential of AI to revolutionize scientific discovery and increase research productivity.'}, 'zh': {'title': 'AIç§‘å­¦å®¶çš„æ–°çºªå…ƒï¼šå®Œå…¨è‡ªä¸»çš„ç§‘å­¦å‘ç°', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†AI Scientist-v2ï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå®Œå…¨è‡ªä¸»ç”Ÿæˆç§‘å­¦è®ºæ–‡çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿè¿­ä»£åœ°æå‡ºç§‘å­¦å‡è®¾ï¼Œè®¾è®¡å’Œæ‰§è¡Œå®éªŒï¼Œåˆ†æå’Œå¯è§†åŒ–æ•°æ®ï¼Œå¹¶æ’°å†™ç§‘å­¦æ‰‹ç¨¿ã€‚ä¸å…¶å‰èº«ç›¸æ¯”ï¼ŒAI Scientist-v2ä¸å†ä¾èµ–äººç±»ç¼–å†™çš„ä»£ç æ¨¡æ¿ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„æœºå™¨å­¦ä¹ é¢†åŸŸä¸­æœ‰æ•ˆæ³›åŒ–ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§æ–°çš„æ¸è¿›å¼ä»£ç†æ ‘æœç´¢æ–¹æ³•ã€‚é€šè¿‡æäº¤ä¸‰ç¯‡å®Œå…¨è‡ªä¸»æ’°å†™çš„æ‰‹ç¨¿åˆ°åŒè¡Œè¯„å®¡çš„ICLRç ”è®¨ä¼šï¼Œå…¶ä¸­ä¸€ç¯‡æˆåŠŸé€šè¿‡è¯„å®¡ï¼Œæ ‡å¿—ç€AIåœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„èƒ½åŠ›ä¸æ–­å¢å¼ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10415', 'title': 'LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models', 'url': 'https://huggingface.co/papers/2504.10415', 'abstract': 'Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.', 'score': 6, 'issue_id': 3239, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '15b7c5f49cceef01', 'authors': ['Parshin Shojaee', 'Ngoc-Hieu Nguyen', 'Kazem Meidani', 'Amir Barati Farimani', 'Khoa D Doan', 'Chandan K Reddy'], 'affiliations': ['Carnegie Mellon University', 'VinUniversity', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2504.10415.jpg', 'data': {'categories': ['#benchmark', '#math', '#reasoning', '#synthetic', '#science'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'LLM-SRBench: Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLM-SRBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 239 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. LLM-SRBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: LSR-Transform Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ LSR-Synth Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 31.5% ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'LLM-SRBench: A New Frontier in Scientific Equation Discovery', 'desc': 'This paper discusses the challenges of using Large Language Models (LLMs) for discovering scientific equations, which are essential for understanding natural laws. The authors introduce LLM-SRBench, a new benchmark with 239 difficult problems across four scientific fields, designed to assess the true discovery capabilities of LLMs without the influence of memorization. The benchmark includes two categories: LSR-Transform, which tests reasoning by altering common models, and LSR-Synth, which presents synthetic problems that require data-driven reasoning. The evaluation shows that even the best LLMs achieve only 31.5% accuracy, underscoring the difficulties in scientific equation discovery and the importance of LLM-SRBench for future studies.'}, 'zh': {'title': 'ç§‘å­¦æ–¹ç¨‹å‘ç°çš„æ–°åŸºå‡†ï¼šLLM-SRBench', 'desc': 'ç§‘å­¦æ–¹ç¨‹å‘ç°æ˜¯ç§‘å­¦è¿›æ­¥ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ¨å¯¼å‡ºè‡ªç„¶ç°è±¡çš„è§„å¾‹ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶åˆ©ç”¨åµŒå…¥ç§‘å­¦çŸ¥è¯†è¿›è¡Œå‡è®¾ç”Ÿæˆçš„æ½œåŠ›è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ–¹æ³•çš„çœŸå®å‘ç°èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰åŸºå‡†å¾€å¾€ä¾èµ–äºå®¹æ˜“è¢«LLMsè®°å¿†çš„å¸¸è§æ–¹ç¨‹ï¼Œå¯¼è‡´æ€§èƒ½æŒ‡æ ‡è¢«å¤¸å¤§ã€‚æœ¬æ–‡ä»‹ç»äº†LLM-SRBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«239ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§é—®é¢˜çš„ç»¼åˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„ç§‘å­¦æ–¹ç¨‹å‘ç°æ–¹æ³•ï¼ŒåŒæ—¶é˜²æ­¢ç®€å•çš„è®°å¿†åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09130', 'title': 'VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search', 'url': 'https://huggingface.co/papers/2504.09130', 'abstract': 'Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.', 'score': 5, 'issue_id': 3242, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 12', 'zh': '4æœˆ12æ—¥'}, 'hash': '3912c7cbd4c137f9', 'authors': ['Yikun Wang', 'Siyin Wang', 'Qinyuan Cheng', 'Zhaoye Fei', 'Liang Ding', 'Qipeng Guo', 'Dacheng Tao', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2504.09130.jpg', 'data': {'categories': ['#inference', '#reasoning', '#multimodal', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VisuoThink: Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'VisuoThink - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ, Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. VisuoThink Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Enhancing Reasoning with VisuoThink: A Multimodal Approach', 'desc': "This paper presents VisuoThink, a new framework designed to improve reasoning in Large Vision-Language Models. It addresses the shortcomings of existing methods that struggle with complex reasoning tasks by integrating visual and textual information in a more human-like manner. VisuoThink allows for progressive reasoning through a combination of visual and linguistic inputs, enhancing the model's ability to perform tasks that require spatial and geometric understanding. The framework shows significant improvements in reasoning capabilities during inference, achieving top performance without the need for additional fine-tuning."}, 'zh': {'title': 'VisuoThinkï¼šæå‡è§†è§‰-è¯­è¨€æ¨ç†çš„æ–°æ¡†æ¶', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶å°è¯•äº†åŸºäºæ–‡æœ¬çš„æ…¢æ€è€ƒæˆ–ç®€å•çš„è§†è§‰è¾…åŠ©ï¼Œä½†æœªèƒ½æœ‰æ•ˆæ•æ‰äººç±»è§†è§‰-è¯­è¨€æ¨ç†è¿‡ç¨‹çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisuoThinkæ¡†æ¶ï¼Œå®ƒå°†è§†è§‰ç©ºé—´å’Œè¯­è¨€é¢†åŸŸæ— ç¼æ•´åˆï¼Œä¿ƒè¿›å¤šæ¨¡æ€çš„æ…¢æ€è€ƒã€‚å®éªŒè¡¨æ˜ï¼ŒVisuoThinkåœ¨å‡ ä½•å’Œç©ºé—´æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09689', 'title': 'EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety', 'url': 'https://huggingface.co/papers/2504.09689', 'abstract': "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent", 'score': 4, 'issue_id': 3237, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': '11b21970119f2c59', 'authors': ['Jiahao Qiu', 'Yinghui He', 'Xinzhe Juan', 'Yiming Wang', 'Yuhan Liu', 'Zixin Yao', 'Yue Wu', 'Xun Jiang', 'Ling Yang', 'Mengdi Wang'], 'affiliations': ['AI Lab, Princeton University', 'Chen Frontier Lab for Al and Mental Health, Tianqiao and Chrissy Chen Institute', 'Department of Computer Science & Engineering, University of Michigan', 'Department of Computer Science, Princeton University', 'Department of Data Science & Engineering, University of Michigan', 'Department of Electrical & Computer Engineering, Princeton University', 'Department of Philosophy, Columbia University', 'Theta Health Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.09689.jpg', 'data': {'categories': ['#agents', '#security', '#ethics', '#healthcare'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ˜Ğ˜-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EmoAgent - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ˜Ğ˜-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. EmoAgent ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: EmoEval, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¸ EmoGuard, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑÑ€ĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ¼, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ¾Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. EmoGuard Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ‚Ğ¸ Ñ€Ğ¸ÑĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ˜Ğ˜.'}, 'en': {'title': 'Ensuring Safer AI Interactions for Vulnerable Users with EmoAgent', 'desc': "This paper introduces EmoAgent, a multi-agent AI framework aimed at enhancing safety in interactions between AI characters and users, especially those with mental health vulnerabilities. EmoAgent consists of two main components: EmoEval, which simulates virtual users to assess mental health changes using established psychological assessment tools, and EmoGuard, which monitors users' mental states and provides feedback to prevent harm. The study reveals that emotionally engaging dialogues with AI can negatively impact the mental health of vulnerable users, with over 34.4% experiencing deterioration. The implementation of EmoGuard effectively reduces these risks, highlighting its importance in creating safer AI-human interactions."}, 'zh': {'title': 'EmoAgentï¼šä¿éšœäººæœºäº¤äº’å¿ƒç†å®‰å…¨çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„äººå·¥æ™ºèƒ½è§’è‰²çš„å…´èµ·ï¼Œç‰¹åˆ«æ˜¯å¯¹å¿ƒç†éšœç¢çš„è„†å¼±ç”¨æˆ·ï¼Œå®‰å…¨é—®é¢˜å¼•èµ·äº†å…³æ³¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é£é™©ï¼Œæˆ‘ä»¬æå‡ºäº†EmoAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“çš„äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å’Œå‡è½»äººæœºäº¤äº’ä¸­çš„å¿ƒç†å¥åº·å±å®³ã€‚EmoAgentåŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼šEmoEvalæ¨¡æ‹Ÿè™šæ‹Ÿç”¨æˆ·ï¼Œè¯„ä¼°ä¸AIè§’è‰²äº¤äº’å‰åçš„å¿ƒç†å¥åº·å˜åŒ–ï¼Œå¹¶ä½¿ç”¨ç»è¿‡ä¸´åºŠéªŒè¯çš„å¿ƒç†è¯„ä¼°å·¥å…·è¿›è¡Œè¯„ä¼°ã€‚EmoGuardä½œä¸ºä¸­ä»‹ï¼Œç›‘æµ‹ç”¨æˆ·çš„å¿ƒç†çŠ¶æ€ï¼Œé¢„æµ‹æ½œåœ¨ä¼¤å®³ï¼Œå¹¶æä¾›çº æ­£åé¦ˆï¼Œä»¥é™ä½é£é™©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09522', 'title': 'How new data permeates LLM knowledge and how to dilute it', 'url': 'https://huggingface.co/papers/2504.09522', 'abstract': 'Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a "priming" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce "Outlandish," a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM\'s existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone\'\' text augmentation strategy and (2) an ``ignore-k\'\' update pruning method. These approaches reduce undesirable priming effects by 50-95\\% while preserving the model\'s ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/', 'score': 4, 'issue_id': 3242, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': 'd9aa235633073967', 'authors': ['Chen Sun', 'Renat Aksitov', 'Andrey Zhmoginov', 'Nolan Andrew Miller', 'Max Vladymyrov', 'Ulrich Rueckert', 'Been Kim', 'Mark Sandler'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2504.09522.jpg', 'data': {'categories': ['#architecture', '#optimization', '#dataset', '#training', '#hallucinations', '#long_context', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ 'Outlandish' Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ LLM. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ´Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾ÑĞ»Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ°: Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° 'stepping-stone' Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 'ignore-k'."}, 'en': {'title': 'Understanding and Controlling Knowledge Priming in LLMs', 'desc': "This paper explores how large language models (LLMs) learn new information and how this learning can unintentionally affect their existing knowledge. The authors identify a phenomenon called 'priming,' where new facts can lead to incorrect applications of knowledge in unrelated situations. They introduce a dataset named 'Outlandish' to study this priming effect and find that the likelihood of priming can be predicted by analyzing token probabilities of key words before learning. Additionally, they propose two techniques to mitigate undesirable priming while maintaining the model's learning capabilities, achieving significant reductions in priming effects."}, 'zh': {'title': 'ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å­¦ä¹ ä¸å¯åŠ¨æ•ˆåº”', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åŸºäºæ¢¯åº¦çš„æ›´æ–°ä¸æ–­å­¦ä¹ ï¼Œä½†æ–°ä¿¡æ¯å¦‚ä½•å½±å“å·²æœ‰çŸ¥è¯†ä»ä¸æ¸…æ¥šã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“å­¦ä¹ æ–°ä¿¡æ¯æ—¶ï¼Œæ¨¡å‹ä¼šå‡ºç°â€œå¯åŠ¨æ•ˆåº”â€ï¼Œå³å­¦ä¹ æ–°äº‹å®å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¸­é”™è¯¯åº”ç”¨è¿™äº›çŸ¥è¯†ã€‚ä¸ºç³»ç»Ÿç ”ç©¶è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œOutlandishâ€æ•°æ®é›†ï¼ŒåŒ…å«1320ä¸ªå¤šæ ·åŒ–çš„æ–‡æœ¬æ ·æœ¬ï¼Œæ—¨åœ¨æ¢è®¨æ–°çŸ¥è¯†å¦‚ä½•æ¸—é€åˆ°æ¨¡å‹çš„ç°æœ‰çŸ¥è¯†åº“ä¸­ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æµ‹é‡å…³é”®å­—çš„tokenæ¦‚ç‡ï¼Œå¯ä»¥é¢„æµ‹å­¦ä¹ æ–°ä¿¡æ¯åçš„å¯åŠ¨ç¨‹åº¦ï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ–°æŠ€æœ¯æ¥è°ƒèŠ‚æ–°çŸ¥è¯†å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10449', 'title': 'M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models', 'url': 'https://huggingface.co/papers/2504.10449', 'abstract': 'Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.', 'score': 3, 'issue_id': 3242, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '565dae169d16775e', 'authors': ['Junxiong Wang', 'Wen-Ding Li', 'Daniele Paliotta', 'Daniel Ritter', 'Alexander M. Rush', 'Tri Dao'], 'affiliations': ['Cornell University', 'Princeton University', 'TogetherAI', 'University of Geneva'], 'pdf_title_img': 'assets/pdf/title_img/2504.10449.jpg', 'data': {'categories': ['#inference', '#architecture', '#reasoning', '#training', '#math', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ RNN Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ RNN, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ M1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mamba. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AIME Ğ¸ MATH Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ M1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ RNN Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Deepseek R1 Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğœ1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ñ‚Ñ€ĞµÑ…ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'M1: A Fast and Efficient Reasoning Model for Complex Math Problems', 'desc': 'This paper presents a new reasoning model called M1, which combines linear RNNs with the Mamba architecture to improve efficiency in solving complex mathematical problems. Unlike traditional transformer models that struggle with long context due to their computational limits, M1 utilizes a hybrid approach that allows for memory-efficient inference. The model is trained using a distillation process and reinforcement learning, resulting in performance that rivals state-of-the-art models while being faster. Experimental results demonstrate that M1 achieves significant speed improvements and higher accuracy compared to existing reasoning models, making it a promising solution for scalable test-time generation.'}, 'zh': {'title': 'æ··åˆMambaæ¨ç†æ¨¡å‹ï¼šé«˜æ•ˆæ¨ç†çš„æ–°é€‰æ‹©', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆçº¿æ€§RNNæ¨ç†æ¨¡å‹M1ï¼ŒåŸºäºMambaæ¶æ„ï¼Œæ—¨åœ¨æé«˜å¤æ‚æ•°å­¦é—®é¢˜çš„æ¨ç†æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„å˜æ¢å™¨æ¨¡å‹ç›¸æ¯”ï¼ŒM1åœ¨å†…å­˜ä½¿ç”¨ä¸Šæ›´ä¸ºé«˜æ•ˆï¼Œèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ã€‚é€šè¿‡å¯¹ç°æœ‰æ¨ç†æ¨¡å‹çš„è’¸é¦è¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒM1åœ¨AIMEå’ŒMATHåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„çº¿æ€§RNNæ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM1åœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šæ¯”åŒè§„æ¨¡çš„å˜æ¢å™¨å¿«è¶…è¿‡3å€ï¼ŒåŒæ—¶åœ¨å›ºå®šç”Ÿæˆæ—¶é—´é¢„ç®—ä¸‹å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10430', 'title': 'LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models', 'url': 'https://huggingface.co/papers/2504.10430', 'abstract': 'Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.', 'score': 1, 'issue_id': 3242, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '01daa1337864118c', 'authors': ['Minqian Liu', 'Zhiyang Xu', 'Xinyi Zhang', 'Heajun An', 'Sarvech Qadir', 'Qi Zhang', 'Pamela J. Wisniewski', 'Jin-Hee Cho', 'Sang Won Lee', 'Ruoxi Jia', 'Lifu Huang'], 'affiliations': ['UC Davis', 'Vanderbilt University', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2504.10430.jpg', 'data': {'categories': ['#alignment', '#ethics', '#rlhf', '#healthcare', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PersuSafety Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ 8 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ… Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ LLM.'}, 'en': {'title': 'Ensuring Ethical Persuasion in Large Language Models', 'desc': 'This paper investigates the safety risks associated with Large Language Models (LLMs) when used for persuasion. It focuses on two main areas: the ability of LLMs to reject unethical persuasion tasks and the influence of factors like personality traits on their behavior. The authors introduce a framework called PersuSafety, which evaluates persuasion safety through scene creation, conversation simulation, and safety assessment. Their experiments reveal that many LLMs struggle to identify harmful tasks and often employ unethical strategies, highlighting the need for improved safety measures in persuasive applications.'}, 'zh': {'title': 'æå‡è¯´æœå®‰å…¨æ€§ï¼Œé˜²èŒƒä¸é“å¾·å½±å“', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å…¶åœ¨è¯´æœèƒ½åŠ›ä¸Šæ¥è¿‘äººç±»æ°´å¹³ã€‚ç„¶è€Œï¼Œè¿™ç§æ½œåŠ›ä¹Ÿå¼•å‘äº†å¯¹LLMé©±åŠ¨çš„è¯´æœå®‰å…¨é£é™©çš„æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å¯èƒ½é€šè¿‡æ“æ§ã€æ¬ºéª—å’Œåˆ©ç”¨è„†å¼±æ€§ç­‰ä¸é“å¾·æ‰‹æ®µè¿›è¡Œå½±å“ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†LLMè¯´æœçš„å®‰å…¨æ€§ï¼Œé‡ç‚¹å…³æ³¨LLMæ˜¯å¦èƒ½æ‹’ç»ä¸é“å¾·çš„è¯´æœä»»åŠ¡ï¼Œä»¥åŠä¸ªæ€§ç‰¹å¾å’Œå¤–éƒ¨å‹åŠ›å¦‚ä½•å½±å“å…¶è¡Œä¸ºã€‚æˆ‘ä»¬æå‡ºäº†PersuSafetyæ¡†æ¶ï¼Œè¯„ä¼°è¯´æœå®‰å…¨æ€§ï¼Œå¹¶é€šè¿‡å®éªŒå‘ç°å¤§å¤šæ•°LLMåœ¨è¯†åˆ«æœ‰å®³è¯´æœä»»åŠ¡å’Œä½¿ç”¨ä¸é“å¾·ç­–ç•¥æ–¹é¢å­˜åœ¨æ˜¾è‘—å®‰å…¨éšæ‚£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09858', 'title': 'Reasoning Models Can Be Effective Without Thinking', 'url': 'https://huggingface.co/papers/2504.09858', 'abstract': 'Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.', 'score': 1, 'issue_id': 3250, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '6aa94cff6001428d', 'authors': ['Wenjie Ma', 'Jingxuan He', 'Charlie Snell', 'Tyler Griggs', 'Sewon Min', 'Matei Zaharia'], 'affiliations': ['Allen Institute for AI', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.09858.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#training', '#math', '#benchmark', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ NoThinking Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NoThinking Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¸ Ğ¸Ñ… Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² LLM Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¼Ğ°Ğ»Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ.'}, 'en': {'title': 'Rethinking Reasoning: Less Thinking, More Efficiency!', 'desc': 'This paper investigates the necessity of explicit thinking processes in large language models (LLMs) for reasoning tasks. The authors introduce a method called NoThinking, which simplifies prompting and shows that it can outperform traditional thinking methods in various reasoning challenges. They demonstrate that using NoThinking in a parallel scaling approach, where multiple outputs are generated and aggregated, yields competitive results with lower latency. This research suggests that lengthy thinking processes may not be essential for effective reasoning, especially in resource-constrained environments.'}, 'zh': {'title': 'é‡æ–°æ€è€ƒæ¨ç†è¿‡ç¨‹çš„å¿…è¦æ€§', 'desc': 'æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šæœ‰äº†æ˜¾è‘—æå‡ï¼Œä¸»è¦æ˜¯é€šè¿‡å°†æ˜ç¡®ä¸”å†—é•¿çš„æ€è€ƒè¿‡ç¨‹çº³å…¥ç”Ÿæˆä¸­ã€‚æœ¬æ–‡è´¨ç–‘è¿™ç§æ˜ç¡®æ€è€ƒæ˜¯å¦çœŸçš„å¿…è¦ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„DeepSeek-R1-Distill-Qwenå‘ç°ï¼Œé€šè¿‡ç®€å•æç¤ºè·³è¿‡æ€è€ƒè¿‡ç¨‹ï¼ˆç§°ä¸ºNoThinkingï¼‰åœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ä½é¢„ç®—è®¾ç½®ä¸‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒNoThinkingæ–¹æ³•åœ¨ç”Ÿæˆå¤šä¸ªç‹¬ç«‹è¾“å‡ºå¹¶è¿›è¡Œèšåˆæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨ç†æ€§èƒ½ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„æ€è€ƒè¿‡ç¨‹çš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09518', 'title': '3D CoCa: Contrastive Learners are 3D Captioners', 'url': 'https://huggingface.co/papers/2504.09518', 'abstract': '3D captioning, which aims to describe the content of 3D scenes in natural language, remains highly challenging due to the inherent sparsity of point clouds and weak cross-modal alignment in existing methods. To address these challenges, we propose 3D CoCa, a novel unified framework that seamlessly combines contrastive vision-language learning with 3D caption generation in a single architecture. Our approach leverages a frozen CLIP vision-language backbone to provide rich semantic priors, a spatially-aware 3D scene encoder to capture geometric context, and a multi-modal decoder to generate descriptive captions. Unlike prior two-stage methods that rely on explicit object proposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a shared feature space, eliminating the need for external detectors or handcrafted proposals. This joint training paradigm yields stronger spatial reasoning and richer semantic grounding by aligning 3D and textual representations. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that 3D CoCa significantly outperforms current state-of-the-arts by 10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at https://github.com/AIGeeksGroup/3DCoCa.', 'score': 1, 'issue_id': 3247, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': '053d31a29b03d829', 'authors': ['Ting Huang', 'Zeyu Zhang', 'Yemin Wang', 'Hao Tang'], 'affiliations': ['Peking University', 'Shanghai University of Engineering Science', 'The Australian National University', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2504.09518.jpg', 'data': {'categories': ['#reasoning', '#3d', '#alignment', '#architecture', '#benchmark', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½', 'desc': '3D CoCa - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ CLIP-Ğ±ÑĞºĞ±Ğ¾Ğ½, ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ 3D-ÑÑ†ĞµĞ½ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², 3D CoCa Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ 3D CoCa Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ScanRefer Ğ¸ Nr3D.'}, 'en': {'title': 'Revolutionizing 3D Captioning with Unified Learning', 'desc': 'This paper introduces 3D CoCa, a new framework for 3D captioning that describes 3D scenes using natural language. It combines contrastive vision-language learning with 3D caption generation, addressing challenges like point cloud sparsity and weak alignment between visual and textual data. The model uses a frozen CLIP backbone for semantic understanding and a 3D scene encoder for geometric context, allowing it to generate captions without relying on external object detectors. Experimental results show that 3D CoCa outperforms existing methods, achieving significant improvements in captioning accuracy on benchmark datasets.'}, 'zh': {'title': '3D CoCaï¼šæ— ç¼ç»“åˆè§†è§‰ä¸è¯­è¨€çš„3Dæ ‡é¢˜ç”Ÿæˆ', 'desc': '3Dæ ‡é¢˜ç”Ÿæˆæ—¨åœ¨ç”¨è‡ªç„¶è¯­è¨€æè¿°3Dåœºæ™¯çš„å†…å®¹ï¼Œä½†ç”±äºç‚¹äº‘ç¨€ç–å’Œè·¨æ¨¡æ€å¯¹é½ä¸è¶³ï¼Œè¿™ä¸€ä»»åŠ¡éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†3D CoCaï¼Œè¿™æ˜¯ä¸€ç§å°†å¯¹æ¯”è§†è§‰-è¯­è¨€å­¦ä¹ ä¸3Dæ ‡é¢˜ç”Ÿæˆæ— ç¼ç»“åˆçš„æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å†»ç»“çš„CLIPè§†è§‰-è¯­è¨€éª¨å¹²ç½‘ç»œæä¾›ä¸°å¯Œçš„è¯­ä¹‰å…ˆéªŒï¼Œä½¿ç”¨ç©ºé—´æ„ŸçŸ¥çš„3Dåœºæ™¯ç¼–ç å™¨æ•æ‰å‡ ä½•ä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€è§£ç å™¨ç”Ÿæˆæè¿°æ€§æ ‡é¢˜ã€‚ä¸ä¾èµ–æ˜¾å¼å¯¹è±¡æè®®çš„ä¸¤é˜¶æ®µæ–¹æ³•ä¸åŒï¼Œ3D CoCaåœ¨å…±äº«ç‰¹å¾ç©ºé—´ä¸­è”åˆä¼˜åŒ–å¯¹æ¯”å’Œæ ‡é¢˜ç”Ÿæˆç›®æ ‡ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ£€æµ‹å™¨æˆ–æ‰‹å·¥æè®®çš„éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08120', 'title': 'DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?', 'url': 'https://huggingface.co/papers/2504.08120', 'abstract': 'Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use.', 'score': 1, 'issue_id': 3245, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '2cc9864fc01e2a7b', 'authors': ['Daniil Larionov', 'Sotaro Takeshita', 'Ran Zhang', 'Yanran Chen', 'Christoph Leiter', 'Zhipin Wang', 'Christian Greisinger', 'Steffen Eger'], 'affiliations': ['University of Mannheim', 'University of Technology Nuremberg'], 'pdf_title_img': 'assets/pdf/title_img/2504.08120.jpg', 'data': {'categories': ['#machine_translation', '#benchmark', '#training', '#reasoning', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğ¹ ÑƒÑĞ¿ĞµÑ… Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (reasoning-enabled LLM) Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WMT23 Ğ¸ SummEval Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ OpenAI o3-mini Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº DeepSeek-R1 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ…ÑƒĞ´ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° (32B), Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ÑÑ Ğ² Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… (8B).'}, 'en': {'title': 'Evaluating Reasoning in Language Models for Better NLG Performance', 'desc': 'This paper investigates how reasoning-enabled large language models (LLMs) perform in evaluating natural language generation tasks like machine translation and text summarization. It compares reasoning-based models, such as DeepSeek-R1 and OpenAI o3, with non-reasoning models across various sizes and architectures. The findings indicate that the effectiveness of reasoning capabilities varies by model and task, with some models benefiting from increased reasoning intensity while others do not. Additionally, the study highlights that distillation of reasoning abilities can preserve performance in medium-sized models but leads to significant degradation in smaller ones.'}, 'zh': {'title': 'æ¨ç†èƒ½åŠ›æå‡è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°çš„æ½œåŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ¨ç†èƒ½åŠ›å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰è¯„ä¼°ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºæ¨ç†çš„æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1å’ŒOpenAI o3ï¼‰ä¸éæ¨ç†æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†èƒ½åŠ›çš„ä¼˜åŠ¿ä¾èµ–äºå…·ä½“æ¨¡å‹å’Œä»»åŠ¡ï¼ŒæŸäº›æƒ…å†µä¸‹æ¨ç†æ¨¡å‹çš„è¡¨ç°ä¸å¦‚å…¶éæ¨ç†ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜å‘ç°ï¼Œæ¨ç†èƒ½åŠ›çš„è’¸é¦åœ¨ä¸­ç­‰è§„æ¨¡æ¨¡å‹ä¸­ä¿æŒäº†åˆç†çš„æ€§èƒ½ï¼Œä½†åœ¨å°å‹æ¨¡å‹ä¸­æ˜¾è‘—ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05782', 'title': 'MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in\n  Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2504.05782', 'abstract': 'Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12-Bench, a multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on a well-organized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing a robust platform for comprehensive evaluation. Additionally, we present a novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at https://github.com/LanceZPF/MDK12.', 'score': 1, 'issue_id': 3246, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 8', 'zh': '4æœˆ8æ—¥'}, 'hash': '6764ce8059618273', 'authors': ['Pengfei Zhou', 'Fanrui Zhang', 'Xiaopeng Peng', 'Zhaopan Xu', 'Jiaxin Ai', 'Yansheng Qiu', 'Chuanhao Li', 'Zhen Li', 'Ming Li', 'Yukang Feng', 'Jianwen Sun', 'Haoquan Zhang', 'Zizhen Li', 'Xiaofeng Mao', 'Wangbo Zhao', 'Kai Wang', 'Xiaojun Chang', 'Wenqi Shao', 'Yang You', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'MBZUAI', 'NUS', 'RIT', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'USTC', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2504.05782.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#agi', '#survey'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MDK12-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MDK12-Bench - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ¾Ğ² K-12. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 140 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑˆĞºĞ¾Ğ»Ñ‹ Ğ´Ğ¾ 12 ĞºĞ»Ğ°ÑÑĞ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° MDK12-Bench Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… MLLM Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Evaluating Multimodal Reasoning with MDK12-Bench', 'desc': 'This paper introduces MDK12-Bench, a new benchmark designed to evaluate the multimodal reasoning abilities of Multimodal Large Language Models (MLLMs) using real-world K-12 exam questions. It covers six subjects and includes 140,000 reasoning instances with varying difficulty levels, providing a structured way to assess model performance. The benchmark also features detailed annotations and a dynamic evaluation framework to address data contamination issues. Results from testing on MDK12-Bench highlight the current limitations of MLLMs in multimodal reasoning, offering valuable insights for future model development.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ¨ç†è¯„ä¼°çš„æ–°åŸºå‡†', 'desc': 'å¤šæ¨¡æ€æ¨ç†æ˜¯å°†è¯­è¨€å’Œè§†è§‰çº¿ç´¢ç»“åˆèµ·æ¥è¿›è¡Œé—®é¢˜è§£å†³å’Œå†³ç­–çš„é‡è¦èƒ½åŠ›ï¼Œæ˜¯äººç±»æ™ºèƒ½çš„åŸºæœ¬ç‰¹å¾ã€‚æœ¬æ–‡æå‡ºäº†MDK12-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šå­¦ç§‘åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ¶µç›–æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ã€åœ°ç†å’Œä¿¡æ¯ç§‘å­¦å…­ä¸ªå­¦ç§‘ï¼ŒåŒ…å«14ä¸‡ä¸ªæ¨ç†å®ä¾‹ï¼Œé€‚ç”¨äºä»å°å­¦åˆ°12å¹´çº§çš„ä¸åŒéš¾åº¦æ°´å¹³ã€‚é€šè¿‡åŠ¨æ€è¯„ä¼°æ¡†æ¶ï¼Œæˆ‘ä»¬è§£å†³äº†æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œå¹¶æ­ç¤ºäº†å½“å‰MLLMsåœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.09513', 'title': 'DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion', 'url': 'https://huggingface.co/papers/2504.09513', 'abstract': 'Large-scale pre-trained diffusion models have produced excellent results in the field of conditional image generation. However, restoration of ancient murals, as an important downstream task in this field, poses significant challenges to diffusion model-based restoration methods due to its large defective area and scarce training samples. Conditional restoration tasks are more concerned with whether the restored part meets the aesthetic standards of mural restoration in terms of overall style and seam detail, and such metrics for evaluating heuristic image complements are lacking in current research. We therefore propose DiffuMural, a combined Multi-scale convergence and Collaborative Diffusion mechanism with ControlNet and cyclic consistency loss to optimise the matching between the generated images and the conditional control. DiffuMural demonstrates outstanding capabilities in mural restoration, leveraging training data from 23 large-scale Dunhuang murals that exhibit consistent visual aesthetics. The model excels in restoring intricate details, achieving a coherent overall appearance, and addressing the unique challenges posed by incomplete murals lacking factual grounding. Our evaluation framework incorporates four key metrics to quantitatively assess incomplete murals: factual accuracy, textural detail, contextual semantics, and holistic visual coherence. Furthermore, we integrate humanistic value assessments to ensure the restored murals retain their cultural and artistic significance. Extensive experiments validate that our method outperforms state-of-the-art (SOTA) approaches in both qualitative and quantitative metrics.', 'score': 0, 'issue_id': 3247, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 13', 'zh': '4æœˆ13æ—¥'}, 'hash': 'ed55bc95454d113a', 'authors': ['Puyu Han', 'Jiaju Kang', 'Yuhang Pan', 'Erting Pan', 'Zeyu Zhang', 'Qunchao Jin', 'Juntao Jiang', 'Zhichen Liu', 'Luqi Gong'], 'affiliations': ['AI Geeks', 'Beijing Normal University', 'Hebei Guoyan Science and Technology Center', 'Southern University of Science and Technology', 'The Australian National University', 'Wuhan University', 'Zhejiang Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.09513.jpg', 'data': {'categories': ['#optimization', '#cv', '#benchmark', '#diffusion', '#dataset', '#ethics'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'DiffuMural: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ñ€ĞµĞ²Ğ½Ğ¸Ñ… Ñ„Ñ€ĞµÑĞ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiffuMural - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ñ€ĞµĞ²Ğ½Ğ¸Ñ… Ñ„Ñ€ĞµÑĞ¾Ğº Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸ĞµĞ¹ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ControlNet Ğ¸ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 23 ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ñ„Ñ€ĞµÑĞºĞ°Ñ… Ğ”ÑƒĞ½ÑŒÑ…ÑƒĞ°Ğ½Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': "Reviving Ancient Art: DiffuMural's Innovative Restoration Approach", 'desc': 'This paper introduces DiffuMural, a novel approach for restoring ancient murals using large-scale pre-trained diffusion models. The method addresses challenges such as large defective areas and limited training samples by employing a Multi-scale convergence and Collaborative Diffusion mechanism. It focuses on aesthetic standards and incorporates a unique evaluation framework that includes metrics for factual accuracy and visual coherence. The results show that DiffuMural significantly outperforms existing methods in restoring intricate details while preserving the cultural significance of the murals.'}, 'zh': {'title': 'DiffuMuralï¼šå¤ä»£å£ç”»ä¿®å¤çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDiffuMuralçš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤ä»£å£ç”»ä¿®å¤ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¤šå°ºåº¦æ”¶æ•›å’ŒååŒæ‰©æ•£æœºåˆ¶ï¼Œåˆ©ç”¨ControlNetå’Œå¾ªç¯ä¸€è‡´æ€§æŸå¤±æ¥ä¼˜åŒ–ç”Ÿæˆå›¾åƒä¸æ¡ä»¶æ§åˆ¶ä¹‹é—´çš„åŒ¹é…ã€‚DiffuMuralåœ¨ä¿®å¤å¤æ‚ç»†èŠ‚å’Œä¿æŒæ•´ä½“ç¾è§‚æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç¼ºå¤±ä¿¡æ¯çš„å£ç”»æ—¶ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç»“åˆå®šé‡æŒ‡æ ‡å’Œäººæ–‡ä»·å€¼è¯„ä¼°ï¼Œç¡®ä¿ä¿®å¤åçš„å£ç”»ä¿ç•™å…¶æ–‡åŒ–å’Œè‰ºæœ¯æ„ä¹‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08685', 'title': 'Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model', 'url': 'https://huggingface.co/papers/2504.08685', 'abstract': 'This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/', 'score': 77, 'issue_id': 3213, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '43b42333b796033b', 'authors': ['Team Seawead', 'Ceyuan Yang', 'Zhijie Lin', 'Yang Zhao', 'Shanchuan Lin', 'Zhibei Ma', 'Haoyuan Guo', 'Hao Chen', 'Lu Qi', 'Sen Wang', 'Feng Cheng', 'Feilong Zuo Xuejiao Zeng', 'Ziyan Yang', 'Fangyuan Kong', 'Zhiwu Qing', 'Fei Xiao', 'Meng Wei', 'Tuyen Hoang', 'Siyu Zhang', 'Peihao Zhu', 'Qi Zhao', 'Jiangqiao Yan', 'Liangke Gui', 'Sheng Bi', 'Jiashi Li', 'Yuxi Ren', 'Rui Wang', 'Huixia Li', 'Xuefeng Xiao', 'Shu Liu', 'Feng Ling', 'Heng Zhang', 'Houmin Wei', 'Huafeng Kuang', 'Jerry Duncan', 'Junda Zhang', 'Junru Zheng', 'Li Sun', 'Manlin Zhang', 'Renfei Sun', 'Xiaobin Zhuang', 'Xiaojie Li', 'Xin Xia', 'Xuyan Chi', 'Yanghua Peng', 'Yuping Wang', 'Yuxuan Wang', 'Zhongkai Zhao', 'Zhuo Chen', 'Zuquan Song', 'Zhenheng Yang', 'Jiashi Feng', 'Jianchao Yang', 'Lu Jiang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2504.08685.jpg', 'data': {'categories': ['#video', '#optimization', '#transfer_learning', '#small_models', '#diffusion', '#training', '#architecture'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾Ñ‚ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (7B), Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Seaweed-7B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ½ÑƒĞ»Ñ Ğ·Ğ° 665 000 Ñ‡Ğ°ÑĞ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ GPU H100. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Seaweed-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ’ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°.'}, 'en': {'title': 'Efficient Video Generation with Seaweed-7B: Small Size, Big Impact!', 'desc': 'This paper introduces Seaweed-7B, a mid-sized video generation model with 7 billion parameters, trained efficiently using 665,000 GPU hours. Despite its smaller size, Seaweed-7B achieves competitive performance against larger models, showcasing the importance of strategic design choices in resource-limited environments. The model demonstrates strong generalization capabilities, allowing it to adapt effectively to various downstream tasks through lightweight fine-tuning or continued training. Overall, the findings suggest that a well-designed medium-sized model can rival larger counterparts while being more cost-effective to train.'}, 'zh': {'title': 'ç»æµé«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒç­–ç•¥', 'desc': 'æœ¬æŠ€æœ¯æŠ¥å‘Šæå‡ºäº†ä¸€ç§ç»æµé«˜æ•ˆçš„è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹è®­ç»ƒç­–ç•¥ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºSeaweed-7Bçš„ä¸­å‹ç ”ç©¶æ¨¡å‹ï¼Œå…·æœ‰çº¦70äº¿ä¸ªå‚æ•°ï¼Œä½¿ç”¨665,000ä¸ªH100 GPUå°æ—¶ä»é›¶å¼€å§‹è®­ç»ƒã€‚å°½ç®¡è®­ç»ƒèµ„æºé€‚ä¸­ï¼ŒSeaweed-7Bçš„æ€§èƒ½ä¸æ›´å¤§è§„æ¨¡çš„ç°ä»£è§†é¢‘ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ä»ç„¶å…·æœ‰ç«äº‰åŠ›ã€‚æŠ¥å‘Šå¼ºè°ƒäº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å¢å¼ºä¸­å‹æ‰©æ•£æ¨¡å‹æ€§èƒ½çš„å…³é”®è®¾è®¡å†³ç­–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08736', 'title': 'GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2504.08736', 'abstract': 'In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality -- a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to 3 space billion parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality.', 'score': 28, 'issue_id': 3216, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '4af199758c238fd4', 'authors': ['Tianwei Xiong', 'Jun Hao Liew', 'Zilong Huang', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.08736.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'GigaTok: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²', 'desc': 'GigaTok - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. GigaTok Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'GigaTok: Balancing Image Quality and Generation in Autoregressive Models', 'desc': 'This paper presents GigaTok, a novel approach to enhance autoregressive image generation by improving visual tokenizers. It addresses the challenge of balancing image reconstruction quality with downstream generation quality, which often deteriorates when scaling tokenizers. The authors introduce semantic regularization to align tokenizer features with those from a pre-trained visual encoder, reducing latent space complexity. By implementing key practices for scaling, GigaTok achieves state-of-the-art results in image reconstruction and generation tasks.'}, 'zh': {'title': 'GigaTokï¼šæå‡å›¾åƒç”Ÿæˆä¸é‡å»ºçš„åˆ›æ–°æ–¹æ³•', 'desc': 'åœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­ï¼Œè§†è§‰æ ‡è®°å™¨å°†å›¾åƒå‹ç¼©ä¸ºç´§å‡‘çš„ç¦»æ•£æ½œåœ¨æ ‡è®°ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ä¸‹æ¸¸è‡ªå›å½’æ¨¡å‹è®­ç»ƒã€‚å°½ç®¡æ‰©å¤§è§†è§‰æ ‡è®°å™¨å¯ä»¥æé«˜å›¾åƒé‡å»ºè´¨é‡ï¼Œä½†å¾€å¾€ä¼šé™ä½ä¸‹æ¸¸ç”Ÿæˆè´¨é‡ï¼Œè¿™æ˜¯ç°æœ‰æ–‡çŒ®ä¸­æœªèƒ½å……åˆ†è§£å†³çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GigaTokï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ‰©å¤§è§†è§‰æ ‡è®°å™¨æ—¶åŒæ—¶æ”¹å–„å›¾åƒé‡å»ºã€ç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ çš„é¦–ä¸ªæ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡è¯­ä¹‰æ­£åˆ™åŒ–æ¥å‡è½»æ½œåœ¨ç©ºé—´å¤æ‚æ€§ï¼Œä»è€Œåœ¨é‡å»ºå’Œä¸‹æ¸¸ç”Ÿæˆä¹‹é—´å–å¾—ä¸€è‡´çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08388', 'title': 'MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft', 'url': 'https://huggingface.co/papers/2504.08388', 'abstract': 'World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate 4 to 7 frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.', 'score': 22, 'issue_id': 3215, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'f87f4b2c67ff61ca', 'authors': ['Junliang Guo', 'Yang Ye', 'Tianyu He', 'Haoyu Wu', 'Yushu Jiang', 'Tim Pearce', 'Jiang Bian'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.08388.jpg', 'data': {'categories': ['#inference', '#benchmark', '#diffusion', '#cv', '#open_source', '#agents', '#games'], 'emoji': 'ğŸ•¹ï¸', 'ru': {'title': 'MineWorld: Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Minecraft Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'MineWorld - ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Minecraft, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4-7 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼, MineWorld Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸.'}, 'en': {'title': 'MineWorld: Real-Time World Modeling in Minecraft', 'desc': "This paper introduces MineWorld, a real-time interactive world model designed for the game Minecraft, which serves as a platform for testing world modeling techniques. The model utilizes a visual-action autoregressive Transformer that processes paired game scenes and actions to predict subsequent scenes based on player interactions. By converting visual inputs and actions into discrete token IDs, MineWorld learns to represent game states and the relationships between actions and outcomes effectively. The authors also present a new parallel decoding algorithm that enhances the model's speed, allowing it to generate multiple frames per second while maintaining high visual quality and action coherence."}, 'zh': {'title': 'MineWorldï¼šå®æ—¶äº’åŠ¨çš„æ™ºèƒ½ä¸–ç•Œæ¨¡å‹', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†MineWorldï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºMinecraftçš„å®æ—¶äº’åŠ¨ä¸–ç•Œæ¨¡å‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨è§†è§‰-åŠ¨ä½œè‡ªå›å½’Transformerï¼Œèƒ½å¤Ÿæ ¹æ®æ¸¸æˆåœºæ™¯å’Œç›¸åº”çš„åŠ¨ä½œç”Ÿæˆæ–°çš„åœºæ™¯ã€‚é€šè¿‡å°†è§†è§‰åœºæ™¯å’ŒåŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°IDï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ¸¸æˆçŠ¶æ€çš„ä¸°å¯Œè¡¨ç¤ºåŠçŠ¶æ€ä¸åŠ¨ä½œä¹‹é—´çš„å…³ç³»ã€‚åœ¨è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„æŒ‡æ ‡æ¥è¯„ä¼°ç”Ÿæˆæ–°åœºæ™¯çš„è§†è§‰è´¨é‡å’ŒåŠ¨ä½œè·Ÿéšèƒ½åŠ›ï¼Œæ˜¾ç¤ºMineWorldåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸–ç•Œæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08600', 'title': 'SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.08600', 'abstract': 'Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model.', 'score': 12, 'issue_id': 3217, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '6153f561c5040630', 'authors': ['Peixian Ma', 'Xialie Zhuang', 'Chengjin Xu', 'Xuhui Jiang', 'Ran Chen', 'Jian Guo'], 'affiliations': ['DataArc Tech Ltd.', 'IDEA Research, International Digital Economy Academy', 'The Hong Kong University of Science and Technology (Guangzhou)', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.08600.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#training', '#rl', '#dataset', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² SQL', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SQL-R1 Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ NL2SQL Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€Ñ‚Ğ° Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. SQL-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Spider Ğ¸ BIRD, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Transforming Natural Language Queries with Reinforcement Learning', 'desc': 'This paper presents SQL-R1, a new model designed to convert natural language queries into SQL statements more effectively, especially in complex scenarios like multi-table joins. It addresses the limitations of traditional supervised fine-tuning methods by employing reinforcement learning (RL) to improve reasoning performance. The authors introduce a specialized reward function for NL2SQL tasks and explore the challenges of cold start in training. SQL-R1 demonstrates impressive execution accuracy on benchmark datasets, achieving 88.6% on Spider and 66.6% on BIRD, using minimal synthetic data for training.'}, 'zh': {'title': 'æå‡NL2SQLæ¨ç†æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'è‡ªç„¶è¯­è¨€è½¬SQLï¼ˆNL2SQLï¼‰ä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸æ•°æ®åº“è¿›è¡Œç›´è§‚äº¤äº’ã€‚å°½ç®¡åœ¨æ•°æ®åº“åº”ç”¨ä¸­äººæœºäº¤äº’æ–¹é¢å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ¨ç†æ€§èƒ½ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ¶‰åŠå¤šè¡¨è¿æ¥å’ŒåµŒå¥—æŸ¥è¯¢çš„æƒ…å†µã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è®­ç»ƒNL2SQLæ¨¡å‹ï¼Œè¿™å¯èƒ½é™åˆ¶äº†å…¶åœ¨æ–°ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§ã€‚ä¸ºæé«˜NL2SQLæ¨¡å‹åœ¨å¤æ‚æƒ…å†µä¸‹çš„æ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†SQL-R1ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•è®­ç»ƒçš„æ–°å‹NL2SQLæ¨ç†æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07963', 'title': 'PixelFlow: Pixel-Space Generative Models with Flow', 'url': 'https://huggingface.co/papers/2504.07963', 'abstract': 'We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256times256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow.', 'score': 9, 'issue_id': 3217, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '4cdb0cfa27fd5251', 'authors': ['Shoufa Chen', 'Chongjian Ge', 'Shilong Zhang', 'Peize Sun', 'Ping Luo'], 'affiliations': ['Adobe', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.07963.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#architecture', '#cv', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'PixelFlow: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ', 'desc': 'PixelFlow - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğµ (VAE) Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞºĞ²Ğ¾Ğ·Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°, PixelFlow Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ FID 1.98 Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ImageNet Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 256x256.'}, 'en': {'title': 'PixelFlow: Revolutionizing Image Generation in Raw Pixel Space', 'desc': 'PixelFlow is a new type of image generation model that works directly with raw pixels instead of using latent spaces like many existing models. This method simplifies the process by removing the need for a pre-trained Variational Autoencoder (VAE), allowing the entire model to be trained in one go. By using efficient cascade flow modeling, PixelFlow maintains low computational costs while generating high-quality images. It has shown impressive results, achieving a low FID score of 1.98 on the ImageNet benchmark, and demonstrates strong performance in generating artistic and semantically controlled images from text prompts.'}, 'zh': {'title': 'PixelFlowï¼šæ–°ä¸€ä»£å›¾åƒç”Ÿæˆæ¨¡å‹çš„çªç ´', 'desc': 'PixelFlowæ˜¯ä¸€ç§ç›´æ¥åœ¨åŸå§‹åƒç´ ç©ºé—´ä¸­æ“ä½œçš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œä¸ä¸»æµçš„æ½œåœ¨ç©ºé—´æ¨¡å‹ä¸åŒã€‚è¿™ç§æ–¹æ³•ç®€åŒ–äº†å›¾åƒç”Ÿæˆè¿‡ç¨‹ï¼Œæ¶ˆé™¤äº†å¯¹é¢„è®­ç»ƒå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„éœ€æ±‚ï¼Œä½¿æ•´ä¸ªæ¨¡å‹å¯ä»¥ç«¯åˆ°ç«¯è®­ç»ƒã€‚é€šè¿‡é«˜æ•ˆçš„çº§è”æµå»ºæ¨¡ï¼ŒPixelFlowåœ¨åƒç´ ç©ºé—´ä¸­å®ç°äº†å¯æ‰¿å—çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶åœ¨256x256çš„ImageNetæ¡ä»¶å›¾åƒç”ŸæˆåŸºå‡†ä¸Šè¾¾åˆ°äº†1.98çš„FIDå€¼ã€‚å…¶æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆç»“æœæ˜¾ç¤ºï¼ŒPixelFlowåœ¨å›¾åƒè´¨é‡ã€è‰ºæœ¯æ€§å’Œè¯­ä¹‰æ§åˆ¶æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07405', 'title': 'FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation', 'url': 'https://huggingface.co/papers/2504.07405', 'abstract': 'With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/).', 'score': 7, 'issue_id': 3213, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'fb73f6a8f480a7a1', 'authors': ['Linyan Huang', 'Haonan Lin', 'Yanning Zhou', 'Kaiwen Xiao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.07405.jpg', 'data': {'categories': ['#inference', '#cv', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'FlexIP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ 2D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ²ĞµÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FlexIP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'FlexIP: Balancing Identity and Personalization in 2D Generative Models', 'desc': "This paper presents FlexIP, a new framework designed to improve 2D generative models by separating the tasks of identity preservation and stylistic manipulation. It introduces two components: a Personalization Adapter that allows for diverse editing styles and a Preservation Adapter that ensures the subject's identity remains intact. By integrating these components, FlexIP enables users to dynamically adjust the balance between identity and personalization during the generation process. Experimental results show that FlexIP outperforms traditional methods, providing better identity retention while allowing for a wider range of personalized outputs."}, 'zh': {'title': 'çµæ´»çš„èº«ä»½ä¿æŒä¸ä¸ªæ€§åŒ–ç¼–è¾‘', 'desc': 'éšç€äºŒç»´ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œä¿æŒä¸»ä½“èº«ä»½åŒæ—¶å®ç°å¤šæ ·åŒ–ç¼–è¾‘æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨èº«ä»½ä¿æŒå’Œä¸ªæ€§åŒ–æ“ä½œä¹‹é—´å­˜åœ¨å›ºæœ‰çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†FlexIPï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªä¸“é—¨çš„ç»„ä»¶è§£è€¦è¿™äº›ç›®æ ‡ï¼šä¸ªæ€§åŒ–é€‚é…å™¨ç”¨äºé£æ ¼åŒ–æ“ä½œï¼Œä¿æŒé€‚é…å™¨ç”¨äºèº«ä»½ç»´æŠ¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½é™åˆ¶ï¼Œå®ç°äº†æ›´ä¼˜çš„èº«ä»½ä¿æŒï¼ŒåŒæ—¶æ”¯æŒæ›´å¤šæ ·åŒ–çš„ä¸ªæ€§åŒ–ç”Ÿæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08716', 'title': 'ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance', 'url': 'https://huggingface.co/papers/2504.08716', 'abstract': "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.", 'score': 5, 'issue_id': 3217, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '50998c3f1c1cc54d', 'authors': ['Wissam Antoun', 'BenoÃ®t Sagot', 'DjamÃ© Seddah'], 'affiliations': ['Inria, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.08716.jpg', 'data': {'categories': ['#optimization', '#architecture', '#benchmark', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° vs Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹?', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² ModernBERT Ğ¸ DeBERTaV3. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² ModernBERT Ğ½Ğ° Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¸ CamemBERTaV2 (Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DeBERTaV3 Ğ´Ğ»Ñ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ½Ğ¾ ModernBERT Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑĞºĞ¾Ñ€ÑÑÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ¾Ğ½ĞµÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Disentangling Architecture from Data in Transformer Models', 'desc': 'This paper investigates the performance of ModernBERT compared to DeBERTaV3 by controlling for training data. The authors find that while ModernBERT shows faster training and inference speeds, DeBERTaV3 outperforms it in terms of sample efficiency and overall benchmark results. The study highlights that high-quality pre-training data can speed up the training process but does not necessarily enhance final model performance. Ultimately, the research emphasizes the need to separate the effects of model architecture from the quality of training data when assessing transformer models.'}, 'zh': {'title': 'æ¶æ„åˆ›æ–°ä¸é¢„è®­ç»ƒæ•°æ®çš„åˆ†ç¦»è¯„ä¼°', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†é¢„è®­ç»ƒçš„å˜æ¢å™¨ç¼–ç å™¨æ¨¡å‹ï¼Œå¦‚DeBERTaV3å’ŒModernBERTï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚å°½ç®¡ModernBERTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºDeBERTaV3ï¼Œä½†ç”±äºç¼ºä¹å…¬å¼€çš„è®­ç»ƒæ•°æ®å’Œå…±äº«æ•°æ®é›†çš„æ¯”è¾ƒï¼Œéš¾ä»¥åˆ¤æ–­è¿™äº›æå‡æ˜¯ç”±äºæ¶æ„æ”¹è¿›è¿˜æ˜¯è®­ç»ƒæ•°æ®çš„å·®å¼‚ã€‚é€šè¿‡åœ¨ä¸CamemBERTaV2ç›¸åŒçš„æ•°æ®é›†ä¸Šé¢„è®­ç»ƒModernBERTï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ—§ä¸€ä»£æ¨¡å‹åœ¨æ ·æœ¬æ•ˆç‡å’Œæ•´ä½“åŸºå‡†æ€§èƒ½ä¸Šä»ç„¶ä¼˜äºæ–°æ¨¡å‹ï¼ŒModernBERTçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºæ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨è¯„ä¼°å˜æ¢å™¨æ¨¡å‹æ—¶ï¼Œå°†é¢„è®­ç»ƒæ•°æ®ä¸æ¶æ„åˆ›æ–°åˆ†å¼€è€ƒè™‘çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08727', 'title': 'Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images', 'url': 'https://huggingface.co/papers/2504.08727', 'abstract': 'We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., "what are the frequent types of changes in the city?") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., "addition of outdoor dining,", "overpass was painted blue," etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles.', 'score': 4, 'issue_id': 3224, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '0bf30d396e917961', 'authors': ['Boyang Deng', 'Songyou Peng', 'Kyle Genova', 'Gordon Wetzstein', 'Noah Snavely', 'Leonidas Guibas', 'Thomas Funkhouser'], 'affiliations': ['Google DeepMind', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.08727.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#cv', '#interpretability'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ˜Ğ˜ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ, Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ñ‡Ğ°ÑÑ‚Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğµ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°Ñ….'}, 'en': {'title': 'Unlocking Urban Trends with Multimodal LLMs', 'desc': 'This paper introduces a system that utilizes Multimodal Large Language Models (MLLMs) to analyze a vast collection of images taken over time, aiming to identify patterns in urban changes. The system is designed to answer open-ended questions about frequent changes in a city without relying on predefined labels or subjects, which sets it apart from traditional visual analysis methods. To handle the enormous dataset, the authors propose a bottom-up approach that breaks down the analysis into smaller, manageable sub-problems, each addressed with tailored MLLM solutions. Experimental results demonstrate that this innovative approach significantly outperforms existing methods, successfully uncovering notable urban trends from the image data.'}, 'zh': {'title': 'åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å‘ç°åŸå¸‚å˜åŒ–è¶‹åŠ¿', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åˆ†æå¤§å‹æ•°æ®åº“çš„ç³»ç»Ÿï¼Œè¯¥æ•°æ®åº“åŒ…å«æ•°åƒä¸‡å¼ åœ¨ä¸åŒæ—¶é—´æ‹æ‘„çš„å›¾åƒï¼Œæ—¨åœ¨å‘ç°æ—¶é—´å˜åŒ–ä¸­çš„æ¨¡å¼ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨åŸå¸‚åœ¨ä¸€å®šæ—¶é—´å†…é¢‘ç¹å…±ç°çš„å˜åŒ–ï¼ˆ"è¶‹åŠ¿"ï¼‰ã€‚ä¸ä»¥å¾€çš„è§†è§‰åˆ†æä¸åŒï¼Œæˆ‘ä»¬çš„åˆ†æèƒ½å¤Ÿå›ç­”å¼€æ”¾å¼é—®é¢˜ï¼Œè€Œæ— éœ€é¢„å…ˆè®¾å®šç›®æ ‡ä¸»é¢˜æˆ–è®­ç»ƒæ ‡ç­¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•ï¼Œå°†åºå¤§çš„è§†è§‰åˆ†æé—®é¢˜åˆ†è§£ä¸ºæ›´æ˜“å¤„ç†çš„å­é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†åŸºäºMLLMçš„è§£å†³æ–¹æ¡ˆï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œèƒ½å¤Ÿä»å¤§åŸå¸‚çš„å›¾åƒä¸­å‘ç°æœ‰è¶£çš„è¶‹åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08641', 'title': 'Training-free Guidance in Text-to-Video Generation via Multimodal\n  Planning and Structured Noise Initialization', 'url': 'https://huggingface.co/papers/2504.08641', 'abstract': 'Recent advancements in text-to-video (T2V) diffusion models have significantly enhanced the visual quality of the generated videos. However, even recent T2V models find it challenging to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. A recent line of research uses layout guidance for T2V models that require fine-tuning or iterative manipulation of the attention map during inference time. This significantly increases the memory requirement, making it difficult to adopt a large T2V model as a backbone. To address this, we introduce Video-MSG, a training-free Guidance method for T2V generation based on Multimodal planning and Structured noise initialization. Video-MSG consists of three steps, where in the first two steps, Video-MSG creates Video Sketch, a fine-grained spatio-temporal plan for the final video, specifying background, foreground, and object trajectories, in the form of draft video frames. In the last step, Video-MSG guides a downstream T2V diffusion model with Video Sketch through noise inversion and denoising. Notably, Video-MSG does not need fine-tuning or attention manipulation with additional memory during inference time, making it easier to adopt large T2V models. Video-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V generation benchmarks (T2VCompBench and VBench). We provide comprehensive ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation.', 'score': 4, 'issue_id': 3227, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'a354f738033ac186', 'authors': ['Jialu Li', 'Shoubin Yu', 'Han Lin', 'Jaemin Cho', 'Jaehong Yoon', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2504.08641.jpg', 'data': {'categories': ['#training', '#multimodal', '#video', '#benchmark', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Video-MSG: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Video-MSG - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ„Ğ¾Ğ½, Ğ¿ĞµÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ—Ğ°Ñ‚ĞµĞ¼ Video-MSG Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ text-to-video Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ T2V.'}, 'en': {'title': 'Streamlining Text-to-Video Generation with Video-MSG', 'desc': 'This paper presents Video-MSG, a novel method for improving text-to-video (T2V) generation without the need for fine-tuning or additional memory during inference. It introduces a three-step process that creates a Video Sketch, which outlines the spatial and temporal elements of the video, including backgrounds and object movements. By guiding T2V diffusion models with this structured plan, Video-MSG enhances the alignment of generated videos with text descriptions. The method shows promising results across various T2V models and benchmarks, demonstrating its effectiveness in generating high-quality videos that accurately reflect the input prompts.'}, 'zh': {'title': 'æ— è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•æ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è§†è§‰è´¨é‡ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯æœ€æ–°çš„T2Væ¨¡å‹ï¼Œåœ¨å‡†ç¡®éµå¾ªæ–‡æœ¬æè¿°æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®æ§åˆ¶ç©ºé—´å¸ƒå±€æˆ–ç‰©ä½“è½¨è¿¹æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Video-MSGï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€è§„åˆ’å’Œç»“æ„åŒ–å™ªå£°åˆå§‹åŒ–çš„æ— è®­ç»ƒæŒ‡å¯¼æ–¹æ³•ã€‚Video-MSGé€šè¿‡åˆ›å»ºè§†é¢‘è‰å›¾æ¥å¼•å¯¼ä¸‹æ¸¸T2Væ‰©æ•£æ¨¡å‹ï¼Œä»è€Œåœ¨ä¸éœ€è¦é¢å¤–å†…å­˜çš„æƒ…å†µä¸‹æé«˜æ–‡æœ¬å¯¹é½æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08591', 'title': 'ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image\n  Restoration', 'url': 'https://huggingface.co/papers/2504.08591', 'abstract': 'Recent progress in generative models has significantly improved image restoration capabilities, particularly through powerful diffusion models that offer remarkable recovery of semantic details and local fidelity. However, deploying these models at ultra-high resolutions faces a critical trade-off between quality and efficiency due to the computational demands of long-range attention mechanisms. To address this, we introduce ZipIR, a novel framework that enhances efficiency, scalability, and long-range modeling for high-res image restoration. ZipIR employs a highly compressed latent representation that compresses image 32x, effectively reducing the number of spatial tokens, and enabling the use of high-capacity models like the Diffusion Transformer (DiT). Toward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that structures the latent space into sub-bands to ease diffusion training. Trained on full images up to 2K resolution, ZipIR surpasses existing diffusion-based methods, offering unmatched speed and quality in restoring high-resolution images from severely degraded inputs.', 'score': 4, 'issue_id': 3227, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'd11a697d646326c2', 'authors': ['Yongsheng Yu', 'Haitian Zheng', 'Zhifei Zhang', 'Jianming Zhang', 'Yuqian Zhou', 'Connelly Barnes', 'Yuchen Liu', 'Wei Xiong', 'Zhe Lin', 'Jiebo Luo'], 'affiliations': ['Adobe Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.08591.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#architecture', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'ZipIR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² 32 Ñ€Ğ°Ğ·Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Diffusion Transformer (DiT). ZipIR Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Latent Pyramid VAE Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ 2K, ZipIR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'ZipIR: Efficient High-Resolution Image Restoration with Latent Compression', 'desc': 'This paper presents ZipIR, a new framework designed to improve the efficiency and scalability of high-resolution image restoration using generative models. It addresses the challenges of long-range attention mechanisms in diffusion models, which can be computationally intensive. ZipIR utilizes a compressed latent representation that reduces the image size by 32 times, allowing for the application of high-capacity models like the Diffusion Transformer. The framework, trained on images up to 2K resolution, demonstrates superior speed and quality compared to existing methods, effectively restoring images from severely degraded conditions.'}, 'zh': {'title': 'ZipIRï¼šé«˜æ•ˆé«˜åˆ†è¾¨ç‡å›¾åƒä¿®å¤çš„æ–°æ¡†æ¶', 'desc': 'æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•æ˜¾è‘—æå‡äº†å›¾åƒä¿®å¤çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå‡ºè‰²åœ°æ¢å¤è¯­ä¹‰ç»†èŠ‚å’Œå±€éƒ¨æ¸…æ™°åº¦ã€‚ç„¶è€Œï¼Œåœ¨è¶…é«˜åˆ†è¾¨ç‡ä¸‹éƒ¨ç½²è¿™äº›æ¨¡å‹æ—¶ï¼Œç”±äºé•¿ç¨‹æ³¨æ„æœºåˆ¶çš„è®¡ç®—éœ€æ±‚ï¼Œé¢ä¸´è´¨é‡ä¸æ•ˆç‡ä¹‹é—´çš„å…³é”®æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ZipIRï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå¢å¼ºäº†é«˜åˆ†è¾¨ç‡å›¾åƒä¿®å¤çš„æ•ˆç‡ã€å¯æ‰©å±•æ€§å’Œé•¿ç¨‹å»ºæ¨¡èƒ½åŠ›ã€‚ZipIRé‡‡ç”¨é«˜åº¦å‹ç¼©çš„æ½œåœ¨è¡¨ç¤ºï¼Œå°†å›¾åƒå‹ç¼©32å€ï¼Œæœ‰æ•ˆå‡å°‘ç©ºé—´æ ‡è®°çš„æ•°é‡ï¼Œä»è€Œèƒ½å¤Ÿä½¿ç”¨åƒæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰è¿™æ ·çš„é«˜å®¹é‡æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08366', 'title': 'In-2-4D: Inbetweening from Two Single-View Images to 4D Generation', 'url': 'https://huggingface.co/papers/2504.08366', 'abstract': 'We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/', 'score': 4, 'issue_id': 3213, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'df65a8f6baab7f84', 'authors': ['Sauradip Nag', 'Daniel Cohen-Or', 'Hao Zhang', 'Ali Mahdavi-Amiri'], 'affiliations': ['Simon Fraser University, Canada', 'Tel Aviv University, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2504.08366.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d', '#optimization'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ˜Ğ· 2D Ğ² 4D: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ In-2-4D Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ 4D-Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ²ÑƒÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Gaussian Splatting, Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ»Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Transforming 2D Images into Smooth 4D Motion!', 'desc': 'This paper introduces the In-2-4D problem, which focuses on generating 4D representations of motion from just two images of an object in different states. The authors propose a hierarchical method that identifies keyframes to create smooth transitions between these states, addressing challenges posed by large frame-to-frame motions. They utilize Gaussian Splatting to construct 3D representations and apply a deformation field to guide the transformation of these representations into dynamic forms. The approach enhances temporal consistency and smoothness by merging motion segments and optimizing them to align with the original video, demonstrating its effectiveness through various experiments.'}, 'zh': {'title': 'ä»é™æ€åˆ°åŠ¨æ€ï¼š4Dè¿åŠ¨æ’å€¼çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼ŒIn-2-4Dï¼Œæ—¨åœ¨ä»ä¸¤ä¸ªä¸åŒè¿åŠ¨çŠ¶æ€çš„å•è§†å›¾å›¾åƒä¸­ç”Ÿæˆ4Dï¼ˆå³3D + åŠ¨ä½œï¼‰æ’å€¼ã€‚ç»™å®šè¡¨ç¤ºç‰©ä½“è¿åŠ¨èµ·å§‹å’Œç»“æŸçŠ¶æ€çš„ä¸¤å¹…å›¾åƒï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç”Ÿæˆå’Œé‡å»º4Dä¸­çš„è¿åŠ¨ã€‚æˆ‘ä»¬é‡‡ç”¨è§†é¢‘æ’å€¼æ¨¡å‹æ¥é¢„æµ‹è¿åŠ¨ï¼Œä½†å¤§å¹…åº¦çš„å¸§é—´è¿åŠ¨å¯èƒ½å¯¼è‡´æ¨¡ç³Šçš„è§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆ†å±‚æ–¹æ³•è¯†åˆ«ä¸è¾“å…¥çŠ¶æ€è§†è§‰ä¸Šæ¥è¿‘ä¸”è¿åŠ¨æ˜¾è‘—çš„å…³é”®å¸§ï¼Œç„¶ååœ¨å®ƒä»¬ä¹‹é—´ç”Ÿæˆå¹³æ»‘çš„ç‰‡æ®µã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07866', 'title': 'Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs', 'url': 'https://huggingface.co/papers/2504.07866', 'abstract': 'We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.', 'score': 4, 'issue_id': 3222, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'dbf55819969ad502', 'authors': ['Yichun Yin', 'Wenyong Huang', 'Kaikai Song', 'Yehui Tang', 'Xueyu Wu', 'Wei Guo', 'Peng Guo', 'Yaoyuan Wang', 'Xiaojun Meng', 'Yasheng Wang', 'Dong Li', 'Can Chen', 'Dandan Tu', 'Yin Li', 'Fisher Yu', 'Ruiming Tang', 'Yunhe Wang', 'Baojun Wang', 'Bin Wang', 'Bo Wang', 'Boxiao Liu', 'Changzheng Zhang', 'Duyu Tang', 'Fei Mi', 'Hui Jin', 'Jiansheng Wei', 'Jiarui Qin', 'Jinpeng Li', 'Jun Zhao', 'Liqun Deng', 'Lin Li', 'Minghui Xu', 'Naifu Zhang', 'Nianzu Zheng', 'Qiang Li', 'Rongju Ruan', 'Shengjun Cheng', 'Tianyu Guo', 'Wei He', 'Wei Li', 'Weiwen Liu', 'Wulong Liu', 'Xinyi Dai', 'Yonghan Dong', 'Yu Pan', 'Yue Li', 'Yufei Wang', 'Yujun Li', 'Yunsheng Ni', 'Zhe Liu', 'Zhenhe Zhang', 'Zhicheng Liu'], 'affiliations': ['Huawei'], 'pdf_title_img': 'assets/pdf/title_img/2504.07866.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#optimization', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'Pangu Ultra: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Pangu Ultra - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 135 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°Ñ… Ascend. Ğ”Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ depth-scaled sandwich normalization. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 13,2 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 8192 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ² Ascend. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Pangu Ultra Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Pangu Ultra: Redefining Large Language Model Training Efficiency', 'desc': 'Pangu Ultra is a Large Language Model (LLM) featuring 135 billion parameters, designed to enhance natural language processing tasks. The model employs depth-scaled sandwich normalization to stabilize training and prevent loss spikes, addressing challenges associated with training large-scale models. It is pre-trained on an extensive dataset of 13.2 trillion tokens and further refined to improve reasoning capabilities. Evaluations show that Pangu Ultra outperforms existing dense LLMs and competes well with sparse models, demonstrating the efficiency of Ascend NPUs in handling such large models.'}, 'zh': {'title': 'Pangu Ultraï¼šè¶…è¶Šå¤§å‹è¯­è¨€æ¨¡å‹çš„æé™', 'desc': 'Pangu Ultraæ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ‹¥æœ‰1350äº¿ä¸ªå‚æ•°ï¼Œé‡‡ç”¨å¯†é›†çš„Transformeræ¨¡å—ï¼Œå¹¶åœ¨Ascendç¥ç»å¤„ç†å•å…ƒä¸Šè¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦ç¼©æ”¾çš„ä¸‰æ˜æ²»å½’ä¸€åŒ–æ–¹æ³•ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†æ·±åº¦æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ³¢åŠ¨ã€‚æˆ‘ä»¬åœ¨132ä¸‡äº¿ä¸ªå¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„æ ‡è®°ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨åæœŸè®­ç»ƒä¸­è¿›ä¸€æ­¥å¢å¼ºäº†æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒPangu Ultraåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†å¯†é›†å‹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†Ascend NPUåœ¨è®­ç»ƒè¶…è¿‡1000äº¿å‚æ•°çš„å¯†é›†æ¨¡å‹æ–¹é¢çš„é«˜æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07615', 'title': 'VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model', 'url': 'https://huggingface.co/papers/2504.07615', 'abstract': 'Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs\' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the "OD aha moment", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1', 'score': 4, 'issue_id': 3226, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '4a70c069a64a48af', 'authors': ['Haozhan Shen', 'Peng Liu', 'Jingcheng Li', 'Chunxin Fang', 'Yibo Ma', 'Jiajia Liao', 'Qiaoli Shen', 'Zilun Zhang', 'Kangjia Zhao', 'Qianqian Zhang', 'Ruochen Xu', 'Tiancheng Zhao'], 'affiliations': ['Binjiang Institute of Zhejiang University', 'Om AI Research', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07615.jpg', 'data': {'categories': ['#open_source', '#rl', '#multimodal', '#reasoning', '#training', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ VLM-R1 - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ RL, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ¿Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ RL Ğ½Ğ° VLM, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ñ€ÑĞ´ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Vision-Language Models with Reinforcement Learning', 'desc': 'This paper presents VLM-R1, a framework that applies reinforcement learning (RL) to enhance the visual reasoning capabilities of Vision-Language Models (VLMs). By utilizing a rule-based reward system, the framework effectively leverages tasks with clear ground-truth answers, leading to improved performance on various vision-language tasks. The experimental results demonstrate that VLM-R1 not only competes well with traditional Supervised Fine-Tuning (SFT) methods but also shows superior generalization abilities. Additionally, the study reveals important insights into reward dynamics and the effects of training data quality on model performance.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'æœ€è¿‘ï¼ŒDeepSeek R1å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚R1çš„æ ¸å¿ƒåœ¨äºå…¶åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å…·æœ‰ç¡®å®šæ€§çœŸå®ç­”æ¡ˆçš„ä»»åŠ¡æ¥å®ç°ç²¾ç¡®å’Œç¨³å®šçš„å¥–åŠ±è®¡ç®—ã€‚æˆ‘ä»¬å‘ç°ï¼Œè®¸å¤šè§†è§‰ç†è§£ä»»åŠ¡ä¹Ÿå…·å¤‡è‰¯å¥½çš„çœŸå®æ ‡æ³¨ï¼Œä½¿å…¶ä¸åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶è‡ªç„¶å…¼å®¹ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†VLM-R1æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05262', 'title': 'Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models', 'url': 'https://huggingface.co/papers/2504.05262', 'abstract': 'Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition (0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and compositional generalization (via isomorphic symbolic mappings, e.g., 7 rightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on numerical addition, performance collapses to leq7.5\\% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of A+B neq B+A) further support this. Explicitly providing addition rules degrades performance by 81.2\\% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning.', 'score': 4, 'issue_id': 3220, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '428bb7d2af34af73', 'authors': ['Yang Yan', 'Yu Lu', 'Renjun Xu', 'Zhenzhong Lan'], 'affiliations': ['School of Engineering, Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05262.jpg', 'data': {'categories': ['#alignment', '#math', '#architecture', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğº Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¼Ñƒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑ… Ñ†ĞµĞ»Ñ‹Ñ… Ñ‡Ğ¸ÑĞµĞ» Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑƒÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¼ÑƒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ñ€ĞµĞ·ĞºĞ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ LLM Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unveiling the Limits of LLMs: From Memorization to Mathematical Reasoning', 'desc': 'This paper examines whether Large Language Models (LLMs) truly understand mathematical principles or simply memorize patterns. The authors focus on basic two-integer addition and test LLMs on their ability to apply commutativity and compositional generalization. Despite high accuracy in straightforward addition tasks, LLMs perform poorly when faced with symbolic mappings, indicating a lack of generalization. The results suggest that LLMs depend more on memorization than on learning mathematical rules, revealing significant limitations in their architecture for true mathematical reasoning.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ä¸è¶³', 'desc': 'å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†å¾ˆé«˜ï¼Œä½†å®ƒä»¬åœ¨ç®€å•é—®é¢˜ä¸Šå¸¸å¸¸å¤±è´¥ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šLLMsæ˜¯å­¦ä¹ äº†æ•°å­¦åŸç†ï¼Œè¿˜æ˜¯ä»…ä»…è®°å¿†äº†æ¨¡å¼ï¼Ÿæˆ‘ä»¬é€šè¿‡ç ”ç©¶åŸºæœ¬çš„ä¸¤æ•´æ•°åŠ æ³•ï¼ˆ0åˆ°2^{64}ï¼‰æ¥æ¢è®¨è¿™ä¸€ç‚¹ï¼Œé‡ç‚¹å…³æ³¨ä¸¤ä¸ªæ ¸å¿ƒå±æ€§ï¼šäº¤æ¢å¾‹ï¼ˆA+B=B+Aï¼‰å’Œç»„åˆæ³›åŒ–ï¼ˆé€šè¿‡åŒæ„ç¬¦å·æ˜ å°„ï¼Œä¾‹å¦‚7æ˜ å°„åˆ°yï¼‰ã€‚å°½ç®¡æœ€å…ˆè¿›çš„LLMsåœ¨æ•°å­—åŠ æ³•ä¸Šå–å¾—äº†73.8-99.8%çš„å‡†ç¡®ç‡ï¼Œä½†åœ¨ç¬¦å·æ˜ å°„ä¸‹ï¼Œæ€§èƒ½éª¤é™è‡³ä¸è¶…è¿‡7.5%ï¼Œè¿™è¡¨æ˜å®ƒä»¬æœªèƒ½æ³›åŒ–æ‰€å­¦è§„åˆ™ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LLMsä¾èµ–äºè®°å¿†æ¨¡å¼è€ŒéçœŸæ­£çš„è§„åˆ™å­¦ä¹ ï¼Œçªæ˜¾äº†å…¶æ¶æ„çš„å±€é™æ€§ï¼Œå¹¶éœ€è¦æ–°çš„æ–¹æ³•æ¥å®ç°çœŸæ­£çš„æ•°å­¦æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01883', 'title': 'CoRAG: Collaborative Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2504.01883', 'abstract': 'Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research.', 'score': 3, 'issue_id': 3218, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'f3bd4bbb45b0315a', 'authors': ['Aashiq Muhamed', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University', 'Machine Learning Department, Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01883.jpg', 'data': {'categories': ['#transfer_learning', '#rag', '#benchmark', '#low_resource'], 'emoji': 'ğŸ¤', 'ru': {'title': 'CoRAG: Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoRAG - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RAG. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CRAB Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CoRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°ÑÑĞ°Ğ¶ĞµĞ¹ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ñ… Ğ¿Ğ°ÑÑĞ°Ğ¶ĞµĞ¹ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Collaborative Learning with CoRAG: Enhancing RAG Models Together!', 'desc': 'The paper presents CoRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) models for collaborative learning environments. In this setup, multiple clients work together to train a shared model using a common passage store, which improves performance on knowledge-intensive tasks. The authors introduce CRAB, a benchmark for evaluating collaborative question answering, and show that CoRAG outperforms traditional methods in low-resource situations. Key insights include the importance of relevant passages, the unexpected advantages of irrelevant ones, and the risks posed by hard negatives, highlighting the complexities of collaborative knowledge sharing.'}, 'zh': {'title': 'åä½œå¢å¼ºç”Ÿæˆï¼šå…±äº«çŸ¥è¯†çš„åŠ›é‡ä¸æŒ‘æˆ˜', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCoRAGçš„æ¡†æ¶ï¼Œå®ƒæ‰©å±•äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨åä½œç¯å¢ƒä¸­å…±åŒè®­ç»ƒå…±äº«æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†CRABåŸºå‡†ï¼Œç”¨äºè¯„ä¼°åä½œåŒè´¨å¼€æ”¾åŸŸé—®ç­”çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoRAGåœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å‚æ•°åä½œå­¦ä¹ æ–¹æ³•å’Œæœ¬åœ°è®­ç»ƒçš„RAGæ¨¡å‹ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†å…±äº«çŸ¥è¯†åº“ä¸­ç›¸å…³æ®µè½çš„é‡è¦æ€§ï¼Œä»¥åŠå¼•å…¥æ— å…³æ®µè½çš„æ„å¤–å¥½å¤„å’Œå›°éš¾è´Ÿæ ·æœ¬å¯¹æ€§èƒ½çš„æ½œåœ¨è´Ÿé¢å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08192', 'title': 'SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs', 'url': 'https://huggingface.co/papers/2504.08192', 'abstract': 'Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic DAE Guardrails (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.', 'score': 2, 'issue_id': 3218, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'e6e92f7d8a3d930d', 'authors': ['Aashiq Muhamed', 'Jacopo Bonato', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Carnegie Mellon University', 'Leonardo Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.08192.jpg', 'data': {'categories': ['#training', '#security', '#interpretability', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Dynamic DAE Guardrails (DSG), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ñ…. DSG Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. DSG Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Dynamic Unlearning: Enhancing Safety in LLMs with DSG', 'desc': 'This paper discusses a new method called Dynamic DAE Guardrails (DSG) for improving machine unlearning in large language models (LLMs). Traditional gradient-based unlearning methods face several challenges, including high computational costs and vulnerability to relearning attacks. The authors propose using Sparse Autoencoders (SAEs) to enhance unlearning efficiency and stability, demonstrating that DSG can outperform existing methods. Their experiments show that DSG provides better performance in terms of forget-utility trade-offs, making unlearning more effective and interpretable.'}, 'zh': {'title': 'åŠ¨æ€å»å™ªè‡ªç¼–ç å™¨ï¼šæå‡æœºå™¨é—å¿˜çš„æ•ˆç‡ä¸ç¨³å®šæ€§', 'desc': 'æœºå™¨é—å¿˜æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡ä»æ¨¡å‹ä¸­ç§»é™¤ä¸å¿…è¦çš„çŸ¥è¯†æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ¢¯åº¦çš„é—å¿˜æ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€è¶…å‚æ•°ä¸ç¨³å®šã€é¡ºåºé—å¿˜èƒ½åŠ›å·®ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€å»å™ªè‡ªç¼–ç å™¨æ–¹æ³•ï¼ˆDSGï¼‰ï¼Œé€šè¿‡ç²¾ç¡®çš„ç‰¹å¾é€‰æ‹©å’ŒåŠ¨æ€åˆ†ç±»å™¨æ¥å®ç°æ›´æœ‰æ•ˆçš„é—å¿˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSGåœ¨é—å¿˜æ•ˆç‡å’Œå®ç”¨æ€§ä¹‹é—´å–å¾—äº†æ˜¾è‘—çš„å¹³è¡¡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é—å¿˜æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01786', 'title': 'BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing', 'url': 'https://huggingface.co/papers/2504.01786', 'abstract': "3D graphics editing is crucial in applications like movie production and game design, yet it remains a time-consuming process that demands highly specialized domain expertise. Automating this process is challenging because graphical editing requires performing a variety of tasks, each requiring distinct skill sets. Recently, vision-language models (VLMs) have emerged as a powerful framework for automating the editing process, but their development and evaluation are bottlenecked by the lack of a comprehensive benchmark that requires human-level perception and presents real-world editing complexity. In this work, we present BlenderGym, the first comprehensive VLM system benchmark for 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D reconstruction tasks. We evaluate closed- and open-source VLM systems and observe that even the state-of-the-art VLM system struggles with tasks relatively easy for human Blender users. Enabled by BlenderGym, we study how inference scaling techniques impact VLM's performance on graphics editing tasks. Notably, our findings reveal that the verifier used to guide the scaling of generation can itself be improved through inference scaling, complementing recent insights on inference scaling of LLM generation in coding and math tasks. We further show that inference compute is not uniformly effective and can be optimized by strategically distributing it between generation and verification.", 'score': 2, 'issue_id': 3231, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '2c60846bd7ed943f', 'authors': ['Yunqi Gu', 'Ian Huang', 'Jihyeon Je', 'Guandao Yang', 'Leonidas Guibas'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01786.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#3d', '#inference'], 'emoji': 'ğŸ¨', 'ru': {'title': 'BlenderGym: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BlenderGym - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ vision-language models (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸. BlenderGym Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ VLM-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Blender. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ VLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸, Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑĞ°Ğ¼ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'BlenderGym: Benchmarking VLMs for 3D Graphics Editing', 'desc': 'This paper introduces BlenderGym, a new benchmark designed to evaluate vision-language models (VLMs) in the context of 3D graphics editing. The authors highlight the challenges of automating this complex task, which requires diverse skills and human-level perception. Through their evaluation, they find that even advanced VLMs struggle with tasks that are relatively simple for human users. Additionally, the study explores how inference scaling techniques can enhance VLM performance, revealing that optimizing the distribution of computational resources between generation and verification can lead to better outcomes.'}, 'zh': {'title': 'BlenderGymï¼š3Då›¾å½¢ç¼–è¾‘çš„æ™ºèƒ½è‡ªåŠ¨åŒ–æ–°åŸºå‡†', 'desc': '3Då›¾å½¢ç¼–è¾‘åœ¨ç”µå½±åˆ¶ä½œå’Œæ¸¸æˆè®¾è®¡ä¸­è‡³å…³é‡è¦ï¼Œä½†è¿™ä¸€è¿‡ç¨‹è€—æ—¶ä¸”éœ€è¦é«˜åº¦ä¸“ä¸šçš„é¢†åŸŸçŸ¥è¯†ã€‚è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå›¾å½¢ç¼–è¾‘æ¶‰åŠå¤šç§ä»»åŠ¡ï¼Œæ¯ç§ä»»åŠ¡éƒ½éœ€è¦ä¸åŒçš„æŠ€èƒ½ã€‚æœ€è¿‘ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ¡†æ¶ï¼Œå¼€å§‹ç”¨äºè‡ªåŠ¨åŒ–ç¼–è¾‘è¿‡ç¨‹ï¼Œä½†ç¼ºä¹å…¨é¢çš„åŸºå‡†æµ‹è¯•é™åˆ¶äº†å…¶å¼€å‘å’Œè¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†BlenderGymï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»¼åˆæ€§çš„VLMç³»ç»ŸåŸºå‡†ï¼Œç”¨äº3Då›¾å½¢ç¼–è¾‘ï¼Œè¯„ä¼°VLMç³»ç»Ÿåœ¨ä»£ç åŸºç¡€çš„3Dé‡å»ºä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08635', 'title': 'Latent Diffusion Autoencoders: Toward Efficient and Meaningful\n  Unsupervised Representation Learning in Medical Imaging', 'url': 'https://huggingface.co/papers/2504.08635', 'abstract': 'This study presents Latent Diffusion Autoencoder (LDAE), a novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimer disease (AD) using brain MR from the ADNI database as a case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in a compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as a promising framework for scalable medical imaging applications, with the potential to serve as a foundation model for medical image analysis. Code available at https://github.com/GabrieleLozupone/LDAE', 'score': 1, 'issue_id': 3224, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '797102aac1993585', 'authors': ['Gabriele Lozupone', 'Alessandro Bria', 'Francesco Fontanella', 'Frederick J. A. Meijer', 'Claudio De Stefano', 'Henkjan Huisman'], 'affiliations': ['Department of Electrical and Information Engineering (DIEI), University of Cassino and Southern Lazio, Italy', 'Department of Medical Imaging, Radboud University Medical Center, Netherlands', 'Diagnostic Image Analysis Group, Radboud University Medical Center, Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2504.08635.jpg', 'data': {'categories': ['#science', '#inference', '#3d', '#healthcare', '#dataset', '#diffusion', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LDAE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ›Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞĞ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ (LDAE) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ Ñ„Ğ¾ĞºÑƒÑĞ¾Ğ¼ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞ·Ğ½ÑŒ ĞĞ»ÑŒÑ†Ğ³ĞµĞ¹Ğ¼ĞµÑ€Ğ°. LDAE Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² ÑĞ¶Ğ°Ñ‚Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LDAE ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞ·Ğ½ÑŒÑ ĞĞ»ÑŒÑ†Ğ³ĞµĞ¹Ğ¼ĞµÑ€Ğ° Ğ¸ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸ĞµĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸, LDAE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Medical Imaging with Latent Diffusion Autoencoder', 'desc': "This paper introduces the Latent Diffusion Autoencoder (LDAE), a new framework designed for efficient unsupervised learning in medical imaging, specifically targeting Alzheimer disease using brain MR images. LDAE innovatively applies the diffusion process in a compressed latent space rather than directly on images, which enhances computational efficiency and makes it easier to handle 3D medical data. The study validates LDAE's effectiveness by demonstrating its ability to capture meaningful representations related to Alzheimer and aging, achieving high-quality image generation and reconstruction. Experimental results show that LDAE significantly outperforms traditional methods in both speed and quality, making it a valuable tool for medical image analysis."}, 'zh': {'title': 'æ½œåœ¨æ‰©æ•£è‡ªç¼–ç å™¨ï¼šåŒ»å­¦å½±åƒåˆ†æçš„æ–°çªç ´', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ½œåœ¨æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆLDAEï¼‰ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦å½±åƒä¸­æ— ç›‘ç£å­¦ä¹ çš„æ•ˆç‡å’Œæ„ä¹‰ï¼Œç‰¹åˆ«å…³æ³¨é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒç©ºé—´æ‰©æ•£è‡ªç¼–ç å™¨ä¸åŒï¼ŒLDAEåœ¨å‹ç¼©çš„æ½œåœ¨è¡¨ç¤ºä¸­åº”ç”¨æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡ï¼Œä½¿å¾—3DåŒ»å­¦å½±åƒçš„è¡¨ç¤ºå­¦ä¹ å˜å¾—å¯è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLDAEèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¸ADå’Œè¡°è€ç›¸å…³çš„3Dè„‘éƒ¨MRçš„æœ‰æ„ä¹‰è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶åœ¨å›¾åƒç”Ÿæˆå’Œé‡å»ºæ–¹é¢è¡¨ç°å‡ºé«˜è´¨é‡å’Œè®¡ç®—æ•ˆç‡ã€‚LDAEçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ‰©æ•£è‡ªç¼–ç å™¨ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†20å€ï¼ŒåŒæ—¶é‡å»ºè´¨é‡ä¹Ÿå¾—åˆ°äº†å¢å¼ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06908', 'title': 'UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image\n  Segmentation', 'url': 'https://huggingface.co/papers/2504.06908', 'abstract': 'In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at https://emmanuelleb985.github.io/ukbob , and the filtered labels will be made available with the UK Biobank.', 'score': 0, 'issue_id': 3230, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '43cceed4e0431425', 'authors': ['Emmanuelle Bourigault', 'Amir Jamaludin', 'Abdullah Hamdi'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2504.06908.jpg', 'data': {'categories': ['#healthcare', '#open_source', '#synthetic', '#3d', '#benchmark', '#architecture', '#dataset', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'UKBOB: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UKBOB - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ² Ñ‚ĞµĞ»Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ĞœĞ Ğ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ğ¸Ğ· UK Biobank. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ETTA Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Swin-BOB Ğ´Ğ»Ñ 3D-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ 3D-Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Medical Imaging with UKBOB: A Giant Leap in Labeled Data', 'desc': 'This paper addresses the challenge of acquiring large-scale labeled data in medical imaging by introducing the UK Biobank Organs and Bones (UKBOB) dataset, which contains 51,761 MRI 3D samples and over 1.37 billion segmentation masks for 72 organs. The authors employ automatic labeling and a cleaning pipeline to ensure high-quality labels, while also validating the dataset with manual annotations of a subset of MRIs. They demonstrate the effectiveness of the dataset by achieving zero-shot generalization on other small labeled datasets and propose a new method, Entropy Test-time Adaptation (ETTA), to improve segmentation outputs. The foundation model, Swin-BOB, trained on UKBOB, sets new benchmarks in 3D medical image segmentation, showcasing significant improvements in established challenges.'}, 'zh': {'title': 'æ„å»ºå¤§è§„æ¨¡åŒ»å­¦å½±åƒæ•°æ®é›†çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'åœ¨åŒ»å­¦å½±åƒé¢†åŸŸï¼Œæ”¶é›†å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é¢ä¸´éšç§ã€ç‰©æµå’Œé«˜æ ‡æ³¨æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†UK Biobank Organs and Bones (UKBOB)ï¼Œè¿™æ˜¯æœ€å¤§çš„èº«ä½“å™¨å®˜æ ‡æ³¨æ•°æ®é›†ï¼ŒåŒ…å«51,761ä¸ªMRI 3Dæ ·æœ¬å’Œè¶…è¿‡13.7äº¿ä¸ª2Dåˆ†å‰²æ©è†œã€‚æˆ‘ä»¬é‡‡ç”¨è‡ªåŠ¨æ ‡æ³¨å’Œè‡ªåŠ¨åŒ–æ ‡ç­¾æ¸…ç†æµç¨‹ï¼Œå¹¶æ‰‹åŠ¨æ ‡æ³¨300ä¸ªMRIä»¥éªŒè¯æ ‡ç­¾è´¨é‡ã€‚é€šè¿‡è®­ç»ƒåŸºç¡€æ¨¡å‹Swin-BOBï¼Œæˆ‘ä»¬åœ¨3DåŒ»å­¦å½±åƒåˆ†å‰²ä¸­å–å¾—äº†å¤šé¡¹åŸºå‡†æµ‹è¯•çš„æœ€æ–°æˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05303', 'title': 'InteractVLM: 3D Interaction Reasoning from 2D Foundational Models', 'url': 'https://huggingface.co/papers/2504.05303', 'abstract': 'We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce a novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multi-view rendering, (2) trains a novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose a new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enabling richer interaction modeling. InteractVLM outperforms existing work on contact estimation and also facilitates 3D reconstruction from an in-the wild image. Code and models are available at https://interactvlm.is.tue.mpg.de.', 'score': 0, 'issue_id': 3219, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': 'be4a2fea61b66424', 'authors': ['Sai Kumar Dwivedi', 'Dimitrije AntiÄ‡', 'Shashank Tripathi', 'Omid Taheri', 'Cordelia Schmid', 'Michael J. Black', 'Dimitrios Tzionas'], 'affiliations': ['Inria, Ecole normale superieure, CNRS, PSL Research University, France', 'Max Planck Institute for Intelligent Systems, Tubingen, Germany', 'University of Amsterdam (UvA), the Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2504.05303.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'InteractVLM: 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'InteractVLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ñ‚Ğ¾Ñ‡ĞµĞº ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Render-Localize-Lift Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² 2D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´ÑŠĞµĞ¼Ğ° Ğ² 3D. InteractVLM Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 3D Contact Estimation with InteractVLM', 'desc': 'InteractVLM is a new method designed to identify 3D contact points between humans and objects using just single images taken in everyday settings. It addresses challenges like occlusions and varying object shapes, which complicate accurate 3D reconstruction. Unlike previous methods that depend on costly 3D annotations, InteractVLM leverages the capabilities of Vision-Language Models (VLMs) fine-tuned with minimal 3D data. The approach includes a unique Render-Localize-Lift module that converts 3D surfaces into 2D, infers contacts in 2D, and then translates these findings back into 3D, enhancing the understanding of human-object interactions.'}, 'zh': {'title': 'InteractVLMï¼šä»å›¾åƒä¸­å®ç°3Dæ¥è§¦ç‚¹ä¼°è®¡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'InteractVLMæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»å•å¼ è‡ªç„¶åœºæ™¯å›¾åƒä¸­ä¼°è®¡äººä½“å’Œç‰©ä½“çš„3Dæ¥è§¦ç‚¹ï¼Œä»è€Œå®ç°å‡†ç¡®çš„äººä½“-ç‰©ä½“è”åˆ3Dé‡å»ºã€‚è¿™é¡¹ä»»åŠ¡é¢ä¸´é®æŒ¡ã€æ·±åº¦æ¨¡ç³Šå’Œç‰©ä½“å½¢çŠ¶å¤šæ ·æ€§ç­‰æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„è¿åŠ¨æ•æ‰ç³»ç»Ÿæˆ–ç¹ççš„æ‰‹åŠ¨æ ‡æ³¨æ¥æ”¶é›†3Dæ¥è§¦æ³¨é‡Šï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚InteractVLMåˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è§†è§‰çŸ¥è¯†ï¼Œå¹¶é€šè¿‡æœ‰é™çš„3Dæ¥è§¦æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå…‹æœäº†è¿™äº›é™åˆ¶ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (33)', '#agents (32)', '#agi (9)', '#alignment (20)', '#architecture (64)', '#audio (6)', '#benchmark (132)', '#cv (76)', '#data (45)', '#dataset (102)', '#diffusion (51)', '#ethics (12)', '#games (18)', '#graphs (5)', '#hallucinations (13)', '#healthcare (11)', '#inference (47)', '#interpretability (25)', '#leakage', '#long_context (24)', '#low_resource (12)', '#machine_translation (3)', '#math (22)', '#multilingual (12)', '#multimodal (104)', '#open_source (76)', '#optimization (141)', '#plp (2)', '#rag (9)', '#reasoning (113)', '#rl (40)', '#rlhf (17)', '#robotics (7)', '#science (13)', '#security (10)', '#small_models (13)', '#story_generation (2)', '#survey (15)', '#synthetic (25)', '#training (161)', '#transfer_learning (23)', '#video (42)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-04-21 19:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-21 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-21 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    