
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 315 papers. April 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Апрель 2025</span> | <span id="title-articles-count">315 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-03.html">⬅️ <span id="prev-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-05.html">➡️ <span id="next-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Апрель 2025', 'en': 'April 2025', 'zh': '4月2025年'};
        let feedDateNext = {'ru': '05.2025', 'en': '05/2025', 'zh': '5月2025年'};
        let feedDatePrev = {'ru': '03.2025', 'en': '03/2025', 'zh': '3月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.06263', 'title': 'OmniSVG: A Unified Scalable Vector Graphics Generation Model', 'url': 'https://huggingface.co/papers/2504.06263', 'abstract': 'Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.', 'score': 85, 'issue_id': 3138, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': '3b3365aa60717b2a', 'authors': ['Yiying Yang', 'Wei Cheng', 'Sijin Chen', 'Xianfang Zeng', 'Jiaxu Zhang', 'Liao Wang', 'Gang Yu', 'Xingjun Ma', 'Yu-Gang Jiang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.06263.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'OmniSVG: Революция в генерации векторной графики с помощью ИИ', 'desc': 'OmniSVG - это новая структура для генерации высококачественной векторной графики SVG с использованием предобученных моделей визуального-языкового машинного обучения. Метод параметризует команды и координаты SVG в дискретные токены, разделяя структурную логику и геометрию для эффективного обучения. Авторы также представили набор данных MMSVG-2M с 2 миллионами аннотированных SVG-файлов для развития этой области. Эксперименты показывают, что OmniSVG превосходит существующие методы и имеет потенциал для интеграции в профессиональные рабочие процессы дизайна SVG.'}, 'en': {'title': 'OmniSVG: Revolutionizing SVG Generation with Vision-Language Models', 'desc': "This paper presents OmniSVG, a novel framework for generating high-quality Scalable Vector Graphics (SVG) using pre-trained Vision-Language Models (VLMs). It addresses the limitations of existing methods by producing structured outputs efficiently, avoiding the high computational costs and oversimplification seen in previous approaches. OmniSVG achieves this by converting SVG commands and coordinates into discrete tokens, allowing for a clear separation of structural logic from geometric details. Additionally, the introduction of the MMSVG-2M dataset, containing two million annotated SVG assets, supports the framework's training and evaluation, showcasing its superiority over current SVG generation techniques."}, 'zh': {'title': 'OmniSVG：高效生成复杂SVG的统一框架', 'desc': '本研究提出了一种名为OmniSVG的统一框架，用于生成高质量和复杂的可缩放矢量图形（SVG）。该框架利用预训练的视觉-语言模型（VLMs），通过将SVG命令和坐标参数化为离散标记，实现了高效的端到端多模态SVG生成。OmniSVG将结构逻辑与低级几何解耦，从而在保持复杂SVG结构表现力的同时，降低了计算成本。此外，我们还引入了MMSVG-2M数据集，包含两百万个丰富注释的SVG资产，以推动SVG合成的发展。'}}}, {'id': 'https://huggingface.co/papers/2504.06261', 'title': 'Hogwild! Inference: Parallel LLM Generation via Concurrent Attention', 'url': 'https://huggingface.co/papers/2504.06261', 'abstract': 'Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other\'s partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other\'s generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.', 'score': 68, 'issue_id': 3141, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': '354599744af26b06', 'authors': ['Gleb Rodionov', 'Roman Garipov', 'Alina Shutova', 'George Yakushev', 'Vage Egiazarian', 'Anton Sinitsin', 'Denis Kuznedelev', 'Dan Alistarh'], 'affiliations': ['HSE University', 'IST Austria', 'Yandex'], 'pdf_title_img': 'assets/pdf/title_img/2504.06261.jpg', 'data': {'categories': ['#inference', '#reasoning', '#long_context', '#optimization', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Самоорганизующееся параллельное сотрудничество языковых моделей', 'desc': 'Эта статья представляет новый подход к параллельному выполнению задач большими языковыми моделями (LLM). Авторы предлагают систему Hogwild! Inference, где несколько экземпляров одной LLM работают параллельно, используя общий кэш внимания и самостоятельно решая, как лучше сотрудничать. Этот метод позволяет моделям разрабатывать собственные стратегии сотрудничества для конкретной задачи, имея доступ к промежуточным результатам друг друга. Система использует преимущества Rotary Position Embeddings для улучшения параллельного использования аппаратных ресурсов.'}, 'en': {'title': 'Collaborative Parallelism for Enhanced LLM Efficiency', 'desc': "This paper explores a new method for improving the efficiency of Large Language Models (LLMs) during complex tasks by enabling them to work in parallel. The authors introduce a system called Hogwild! Inference, where multiple LLM instances share an attention cache, allowing them to see each other's progress and collaborate effectively. This approach leverages Rotary Position Embeddings (RoPE) to enhance performance without the need for extra fine-tuning. The findings suggest that LLMs can autonomously develop collaboration strategies, leading to faster and more efficient problem-solving."}, 'zh': {'title': '并行合作，提升LLM推理效率', 'desc': '大型语言模型（LLMs）能够通过高级推理、长篇内容生成和工具使用来处理越来越复杂的任务。为了解决这些任务，研究表明LLMs可以通过实现明确的合作框架来并行操作，例如投票机制或独立子任务的创建。本文提出了一种新的设计方法：让LLM“工作者”并行运行，通过同时更新的注意力缓存进行同步，并决定最佳的合作方式。我们实现了Hogwild!推理，这是一种并行LLM推理引擎，多个相同的LLM实例在共享的注意力缓存中并行运行，能够“即时”访问彼此生成的标记。'}}}, {'id': 'https://huggingface.co/papers/2504.05599', 'title': 'Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought', 'url': 'https://huggingface.co/papers/2504.05599', 'abstract': 'We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retraining of either the foundational language model or the vision encoder. To strengthen visual-text alignment, we propose a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), significantly enhancing cross-modal integration efficiency. Additionally, we introduce an adaptive-length Chain-of-Thought distillation approach for reasoning data generation. This approach dynamically optimizes reasoning chain lengths, thereby enhancing inference efficiency and preventing excessive reasoning overthinking. Empirical evaluations demonstrate that Skywork R1V, with only 38B parameters, delivers competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. Meanwhile, it maintains robust textual reasoning performance, evidenced by impressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model weights have been publicly released to promote openness and reproducibility.', 'score': 60, 'issue_id': 3142, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': 'b963d5098e669229', 'authors': ['Yi Peng', 'Chris', 'Xiaokun Wang', 'Yichen Wei', 'Jiangbo Pei', 'Weijie Qiu', 'Ai Jian', 'Yunzhuo Hao', 'Jiachun Pan', 'Tianyidan Xie', 'Li Ge', 'Rongxian Zhuang', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.05599.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#transfer_learning', '#benchmark', '#inference', '#architecture', '#multimodal', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная мультимодальная адаптация для улучшения рассуждений ИИ', 'desc': 'Статья представляет Skywork R1V - мультимодальную модель рассуждений, расширяющую возможности языковых моделей серии R1 на визуальные модальности. Модель использует легковесный визуальный проектор для адаптации к мультимодальным задачам без переобучения базовой языковой модели или энкодера изображений. Авторы предлагают гибридную стратегию оптимизации, сочетающую итеративное обучение с учителем и групповую относительную оптимизацию политики, а также адаптивный подход к дистилляции цепочки рассуждений. Эмпирические оценки показывают, что Skywork R1V с 38 миллиардами параметров демонстрирует конкурентоспособные результаты на различных бенчмарках, сохраняя при этом высокую производительность в текстовых рассуждениях.'}, 'en': {'title': 'Seamless Multimodal Reasoning with Skywork R1V', 'desc': 'Skywork R1V is a new multimodal reasoning model that enhances the capabilities of existing R1-series large language models by integrating visual data. It uses a lightweight visual projector to adapt to visual inputs without needing to retrain the foundational language model or the vision encoder. The model employs a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning and Group Relative Policy Optimization to improve the alignment between text and visual information. Additionally, it features an adaptive-length Chain-of-Thought distillation method that optimizes reasoning processes, leading to efficient inference and strong performance on various benchmarks.'}, 'zh': {'title': 'Skywork R1V：高效的多模态推理模型', 'desc': 'Skywork R1V是一种多模态推理模型，它通过高效的多模态转移方法扩展了R1系列大型语言模型（LLM）到视觉模态。该模型利用轻量级视觉投影器，实现了无须重新训练基础语言模型或视觉编码器的无缝多模态适应。为了增强视觉与文本的对齐，我们提出了一种混合优化策略，结合了迭代监督微调（SFT）和组相对策略优化（GRPO），显著提高了跨模态集成的效率。此外，我们还引入了一种自适应长度的思维链蒸馏方法，动态优化推理链的长度，从而提高推理效率，避免过度思考。'}}}, {'id': 'https://huggingface.co/papers/2504.05979', 'title': 'An Empirical Study of GPT-4o Image Generation Capabilities', 'url': 'https://huggingface.co/papers/2504.05979', 'abstract': "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.", 'score': 46, 'issue_id': 3137, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': 'f1195a87ec5b86f1', 'authors': ['Sixiang Chen', 'Jinbin Bai', 'Zhuoran Zhao', 'Tian Ye', 'Qingyu Shi', 'Donghao Zhou', 'Wenhao Chai', 'Xin Lin', 'Jianzong Wu', 'Chao Tang', 'Shilin Xu', 'Tao Zhang', 'Haobo Yuan', 'Yikang Zhou', 'Wei Chow', 'Linfeng Li', 'Xiangtai Li', 'Lei Zhu', 'Lu Qi'], 'affiliations': ['National University of Singapore', 'Peking University', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (GZ)', 'University of Washington', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05979.jpg', 'data': {'categories': ['#diffusion', '#cv', '#architecture', '#multimodal', '#open_source', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'GPT-4o: Новый рубеж в унифицированной генерации изображений', 'desc': 'Статья посвящена исследованию возможностей генерации изображений моделью GPT-4o. Авторы проводят эмпирическое сравнение GPT-4o с ведущими открытыми и коммерческими моделями в более чем 20 задачах генерации. Оценка охватывает четыре основные категории: текст-в-изображение, изображение-в-изображение, изображение-в-3D и изображение-в-X. На основе анализа выявляются сильные и слабые стороны GPT-4o в различных условиях, а также определяются перспективные направления для будущих унифицированных генеративных моделей.'}, 'en': {'title': 'Unifying Image and Text Generation with GPT-4o', 'desc': "This paper explores the advancements in image generation, focusing on the capabilities of the GPT-4o model. It conducts a thorough evaluation of GPT-4o's performance in various generative tasks, including text-to-image and image-to-3D generation. The study benchmarks GPT-4o against other leading models, revealing its strengths and weaknesses in multimodal generation. The findings suggest future directions for improving unified generative architectures, particularly in terms of design and data utilization."}, 'zh': {'title': '探索统一生成模型的未来方向', 'desc': '本文探讨了图像生成领域的最新进展，特别是GPT-4o模型在图像生成方面的能力。我们对其进行了实证研究，并与领先的开源和商业模型进行了基准测试，涵盖了文本到图像、图像到图像、图像到3D和图像到X生成等四个主要类别。分析结果揭示了GPT-4o在不同设置下的优缺点，并将其置于生成建模的更广泛演变中。通过这项研究，我们识别出未来统一生成模型的有希望的方向，强调了架构设计和数据扩展的重要性。'}}}, {'id': 'https://huggingface.co/papers/2504.05535', 'title': 'COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for\n  Alignment with Human Values', 'url': 'https://huggingface.co/papers/2504.05535', 'abstract': 'Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and response labeling significantly constrains the scalability of human preference datasets. To address these challenges, we design an LLM-based Chinese preference dataset annotation pipeline with no human intervention. Specifically, we crawled and carefully filtered 92k high-quality Chinese queries and employed 15 mainstream LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce COIG-P (Chinese Open Instruction Generalist - Preference), a high-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese preference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel, and Role. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously constructed a Chinese Reward Benchmark (CRBench). Evaluation results based on AlignBench liu2024alignbenchbenchmarkingchinesealignment show that that COIG-P significantly outperforms other Chinese preference datasets, and it brings significant performance improvements ranging from 2% to 12% for the Qwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results on CRBench demonstrate that our CRM has a strong and robust scoring ability. We apply it to filter chosen-rejected response pairs in a test split of COIG-P, and our experiments show that it is comparable to GPT-4o in identifying low-quality samples while maintaining efficiency and cost-effectiveness. Our codes and data are released in https://github.com/multimodal-art-projection/COIG-P.', 'score': 33, 'issue_id': 3145, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '420fafe139f00d54', 'authors': ['M-A-P Team', 'Siwei Wu', 'Jincheng Ren', 'Xinrun Du', 'Shuyue Guo', 'Xingwei Qu', 'Yiming Liang', 'Jie Liu', 'Yunwen Li', 'Tianyu Zheng', 'Boyu Feng', 'Huaqing Yuan', 'Zenith Wang', 'Jiaheng Liu', 'Wenhao Huang', 'Chenglin Cai', 'Haoran Que', 'Jian Yang', 'Yuelin Bai', 'Zekun Moore Wang', 'Zhouliang Yu', 'Qunshu Lin', 'Ding Pan', 'Yuchen Jiang', 'Tiannan Wang', 'Wangchunshu Zhou', 'Shenzhi Wang', 'Xingyuan Bu', 'Minghao Liu', 'Guoyin Wang', 'Ge Zhang', 'Chenghua Lin'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.05535.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#data', '#rlhf', '#open_source'], 'emoji': '🇨🇳', 'ru': {'title': 'Автоматизированное создание масштабного китайского датасета предпочтений для улучшения LLM', 'desc': 'Статья представляет COIG-P - крупномасштабный китайский датасет предпочтений для настройки больших языковых моделей. Авторы разработали автоматизированный конвейер аннотации с использованием LLM, собрав более миллиона пар предпочтений в 6 различных доменах. На основе COIG-P была обучена китайская модель вознаграждения (CRM) и создан бенчмарк CRBench для оценки. Результаты показывают, что COIG-P значительно превосходит другие китайские датасеты предпочтений и улучшает производительность моделей на 2-12%.'}, 'en': {'title': 'Revolutionizing Chinese Preference Datasets with LLMs', 'desc': 'This paper presents a novel approach to creating a large-scale Chinese preference dataset, COIG-P, which addresses the limitations of existing datasets. By utilizing 15 mainstream large language models (LLMs) to generate and score response pairs, the authors eliminate the need for human annotators, enhancing scalability. The dataset includes 1,009k preference pairs across six diverse domains, significantly improving performance metrics for various models. Additionally, a Chinese Reward Model (CRM) is developed to efficiently score responses, demonstrating strong performance in identifying low-quality samples compared to existing benchmarks.'}, 'zh': {'title': '构建高质量中文偏好数据集的创新方法', 'desc': '本论文提出了一种无人工干预的中文偏好数据集注释管道，以解决现有中文偏好数据集规模小、领域狭窄和缺乏严格验证的问题。我们收集并筛选了92,000个高质量中文查询，并利用15个主流大型语言模型生成和评分选择-拒绝的响应对。基于此，我们引入了COIG-P数据集，包含1,009,000个中文偏好对，覆盖聊天、代码、数学、逻辑、小说和角色等六个多样化领域。通过训练一个8B规模的中文奖励模型（CRM），我们构建了中文奖励基准（CRBench），并在评估中显示出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2504.02160', 'title': 'Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation', 'url': 'https://huggingface.co/papers/2504.02160', 'abstract': 'Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.', 'score': 27, 'issue_id': 3139, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '511e3ea71050e14e', 'authors': ['Shaojin Wu', 'Mengqi Huang', 'Wenxu Wu', 'Yufeng Cheng', 'Fei Ding', 'Qian He'], 'affiliations': ['Intelligent Creation Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2504.02160.jpg', 'data': {'categories': ['#dataset', '#data', '#synthetic', '#multimodal', '#diffusion', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Универсальная генерация изображений с множеством объектов', 'desc': 'Статья описывает новый подход к генерации изображений с несколькими объектами. Авторы предлагают пайплайн для синтеза согласованных данных, используя возможности диффузионных трансформеров. Они также представляют модель UNO с прогрессивным кросс-модальным выравниванием и универсальным позиционным кодированием. Эксперименты показывают, что метод обеспечивает высокую согласованность и контролируемость при генерации изображений с одним или несколькими объектами.'}, 'en': {'title': 'Enhancing Multi-Subject Image Generation with Consistent Data Synthesis', 'desc': 'This paper addresses the challenges of generating images with multiple subjects by proposing a new data synthesis pipeline. The authors utilize diffusion transformers to create high-consistency paired data for both single and multi-subject scenarios. They introduce a novel model called UNO, which incorporates cross-modal alignment and rotary position embedding to enhance the generation process. Experimental results demonstrate that their approach maintains high consistency and controllability in image generation tasks.'}, 'zh': {'title': '高一致性多主题生成的创新方法', 'desc': '本研究探讨了在图像生成中，如何解决以主题为驱动的生成面临的数据可扩展性和主题扩展性挑战。我们提出了一种高一致性的数据合成管道，利用扩散变换器的内在生成能力，生成高一致性的多主题配对数据。此外，我们引入了UNO模型，结合了渐进的跨模态对齐和通用旋转位置嵌入，能够在单主题和多主题生成中保持高一致性和可控性。通过大量实验，我们的方法在生成质量和控制能力上均表现出色。'}}}, {'id': 'https://huggingface.co/papers/2504.02810', 'title': 'Generative Evaluation of Complex Reasoning in Large Language Models', 'url': 'https://huggingface.co/papers/2504.02810', 'abstract': "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.", 'score': 10, 'issue_id': 3138, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '5a9b5f817a1c09b6', 'authors': ['Haowei Lin', 'Xiangyu Wang', 'Ruilin Yan', 'Baizhou Huang', 'Haotian Ye', 'Jianhua Zhu', 'Zihao Wang', 'James Zou', 'Jianzhu Ma', 'Yitao Liang'], 'affiliations': ['Computer Science Department, Stanford University, California, United States', 'Department of Electronic Engineering, Tsinghua University, Beijing, China', 'Institute for AI Industry Research, Tsinghua University, Beijing, China', 'Institute for Artificial Intelligence, Peking University, Beijing, China', 'Wangxuan institute of computer technology, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.02810.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'KUMO: новый способ оценить истинные способности ИИ к рассуждению', 'desc': 'Исследователи представили KUMO - новую систему для оценки способностей больших языковых моделей (LLM) к рассуждению. KUMO генерирует разнообразные задачи на рассуждение, комбинируя LLM с символьными движками. Система позволяет создавать новые задачи в различных областях, заставляя модели демонстрировать реальное обобщение, а не запоминание. Тестирование 23 современных LLM на 5000 задачах показало, что многие из них превзошли уровень студентов университетов в простых задачах на рассуждение.'}, 'en': {'title': 'KUMO: Unveiling True Reasoning in LLMs', 'desc': "This paper introduces KUMO, a new framework for evaluating the reasoning abilities of large language models (LLMs). KUMO generates diverse reasoning tasks that require models to demonstrate true understanding rather than simple recall from their training data. By combining LLMs with symbolic engines, it creates adjustable tasks that challenge models across various domains. The evaluation shows that many LLMs can outperform university students on easier tasks and achieve comparable performance on more complex reasoning challenges, highlighting KUMO's effectiveness as a reliable assessment tool."}, 'zh': {'title': 'KUMO：评估大型语言模型推理能力的新工具', 'desc': '本文探讨了大型语言模型（LLMs）是否真正具备推理能力，还是仅仅从其庞大的训练数据集中回忆答案。为了解决这一问题，作者提出了KUMO，一个专门用于评估LLMs推理能力的生成评估框架。KUMO结合了LLMs和符号引擎，动态生成多样化的推理任务，促进模型展示真正的泛化能力。通过对23个最先进的LLMs进行评估，结果显示许多模型在简单推理任务上超越了大学生的表现，而在复杂推理挑战中也达到了大学水平。'}}}, {'id': 'https://huggingface.co/papers/2504.06148', 'title': 'V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric\n  Capabilities in Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2504.06148', 'abstract': 'Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.', 'score': 9, 'issue_id': 3144, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': '7d7e4a2197e26482', 'authors': ['Xiangxi Zheng', 'Linjie Li', 'Zhengyuan Yang', 'Ping Yu', 'Alex Jinpeng Wang', 'Rui Yan', 'Yuan Yao', 'Lijuan Wang'], 'affiliations': ['Central South University, China', 'Microsoft Corporation, USA', 'Nanjing University, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.06148.jpg', 'data': {'categories': ['#benchmark', '#games', '#multimodal', '#reasoning', '#agents'], 'emoji': '🎮', 'ru': {'title': 'V-MAGE: Новый подход к оценке визуальных способностей ИИ через игровые задачи', 'desc': 'В статье представлена новая система оценки визуальных способностей мультимодальных больших языковых моделей (MLLM) под названием V-MAGE. Эта система включает пять разнообразных игр с более чем 30 уровнями, которые тестируют такие навыки, как позиционирование, отслеживание траектории, тайминг и визуальная память. Результаты оценки ведущих MLLM показали значительное отставание от человеческих показателей во всех игровых средах. Исследование выявило критические ограничения моделей, включая различные типы ошибок восприятия, и предложило потенциальные пути улучшения с точки зрения агента.'}, 'en': {'title': 'Enhancing Visual Reasoning in MLLMs with V-MAGE', 'desc': "This paper presents a new evaluation framework called Visual-centric Multiple Abilities Game Evaluation (V-MAGE) for assessing the visual reasoning skills of Multimodal Large Language Models (MLLMs). Unlike traditional benchmarks, V-MAGE includes dynamic, game-based tasks that require models to demonstrate core visual skills and higher-level reasoning abilities. The framework consists of five games with over 30 levels, focusing on tasks like positioning and visual memory. The evaluation reveals significant performance gaps between MLLMs and humans, highlighting the models' perceptual errors and suggesting areas for improvement in their reasoning strategies."}, 'zh': {'title': '视觉推理能力的新评估框架', 'desc': '最近，多模态大型语言模型（MLLMs）在多个多模态基准测试中取得了显著进展。然而，随着评估从静态数据集转向开放世界的动态环境，现有的基于游戏的基准测试显得不足，因为它们缺乏视觉中心的任务，无法评估真实世界决策所需的多样推理能力。为了解决这个问题，我们提出了视觉中心多能力游戏评估（V-MAGE），这是一个旨在评估MLLMs视觉推理能力的游戏评估框架。V-MAGE包含五个多样化的游戏和30多个手工设计的关卡，测试模型在定位、轨迹跟踪、时机把握和视觉记忆等核心视觉技能上的表现，以及长期规划和深思熟虑等更高层次的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2504.05594', 'title': 'Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model', 'url': 'https://huggingface.co/papers/2504.05594', 'abstract': 'Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.', 'score': 9, 'issue_id': 3138, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': '7da2f86ad5e0bfc2', 'authors': ['Qi Mao', 'Lan Chen', 'Yuchao Gu', 'Mike Zheng Shou', 'Ming-Hsuan Yang'], 'affiliations': ['Show Lab, National University of Singapore', 'State Key Laboratory of Media Convergence and Communication, Communication University of China', 'University of California at Merced', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05594.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#multimodal'], 'emoji': '⚖️', 'ru': {'title': 'Баланс точности и редактируемости в редактировании изображений на основе текста', 'desc': 'Статья представляет UnifyEdit - метод редактирования изображений на основе текста, который балансирует между сохранением структуры и редактируемостью. Авторы разработали два ограничения на основе внимания: сохранение самовнимания для структурной точности и выравнивание перекрестного внимания для улучшения редактируемости. Для решения проблемы конфликта градиентов введен адаптивный планировщик временных шагов, динамически регулирующий влияние ограничений. Эксперименты подтверждают эффективность подхода в достижении баланса между сохранением структуры и выравниванием текста в различных задачах редактирования.'}, 'en': {'title': 'Achieving Perfect Balance in Text-Based Image Editing with UnifyEdit', 'desc': 'This paper presents UnifyEdit, a novel method for text-based image editing that aims to balance fidelity and editability. Traditional approaches often struggle with over- or under-editing due to conflicting constraints in attention mechanisms. UnifyEdit introduces a unified framework that employs self-attention and cross-attention constraints to maintain structural fidelity and enhance text alignment, respectively. An adaptive time-step scheduler is also proposed to dynamically manage the influence of these constraints, ensuring optimal performance in various editing tasks.'}, 'zh': {'title': '平衡保真度与可编辑性的创新方法', 'desc': '在基于文本的图像编辑中，平衡保真度和可编辑性非常重要。现有方法通常依赖于注意力机制来保持结构，但缺乏统一的机制来平衡这两个目标。我们提出了UnifyEdit，这是一种无调优的方法，通过扩散潜在优化实现保真度和可编辑性的平衡。我们开发了自注意力和交叉注意力约束，并引入自适应时间步调度器，以动态调整这些约束的影响，从而优化编辑效果。'}}}, {'id': 'https://huggingface.co/papers/2504.00043', 'title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation', 'url': 'https://huggingface.co/papers/2504.00043', 'abstract': 'Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.', 'score': 7, 'issue_id': 3137, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '2b2bfdd590c5394d', 'authors': ['Jixuan Leng', 'Chengsong Huang', 'Langlin Huang', 'Bill Yuchen Lin', 'William W. Cohen', 'Haohan Wang', 'Jiaxin Huang'], 'affiliations': ['CMU', 'UIUC', 'UW', 'WUSTL'], 'pdf_title_img': 'assets/pdf/title_img/2504.00043.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning'], 'emoji': '🧩', 'ru': {'title': 'Кроссворды как инструмент оценки искусственного интеллекта', 'desc': 'CrossWordBench - это новый метод оценки способностей больших языковых моделей (LLM) и больших визуально-языковых моделей (LVLM) к рассуждению. Он использует кроссворды как задачу, требующую мультимодального соблюдения семантических ограничений из текстовых подсказок и пересекающихся ограничений из визуальных структур сетки. Оценка более 20 моделей показала, что LLM с возможностями рассуждения значительно превосходят модели без таких возможностей. Результаты также выявили трудности LVLM с этой задачей, демонстрируя сильную корреляцию между их способностью решать головоломки и точностью разбора сетки.'}, 'en': {'title': 'CrossWordBench: Evaluating Reasoning in LLMs and LVLMs with Crossword Puzzles', 'desc': 'This paper presents CrossWordBench, a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) using crossword puzzles. The benchmark focuses on the interaction between text-based clues and visual grid structures, requiring models to adhere to both semantic and intersectional constraints. The study shows that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints, while LVLMs face challenges linked to their grid-parsing accuracy. Overall, the findings highlight the limitations of current models in reasoning tasks and suggest a novel approach for multimodal evaluation.'}, 'zh': {'title': '跨模态推理的新基准：CrossWordBench', 'desc': '现有的大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的推理评估框架主要集中在文本推理或视觉语言理解能力上，缺乏文本与视觉之间的动态互动。为了解决这个问题，我们提出了CrossWordBench，这是一个通过填字游戏评估LLMs和LVLMs推理能力的基准，要求在文本线索和视觉网格结构的语义约束下进行多模态推理。CrossWordBench利用可控的拼图生成框架，生成多种格式的拼图，并提供从直接解谜到互动模式的不同评估策略。我们的评估结果显示，推理能力强的LLMs在利用交叉字母约束方面显著优于非推理模型，而LVLMs在此任务中表现不佳，其解谜表现与网格解析准确性之间存在强相关性。'}}}, {'id': 'https://huggingface.co/papers/2503.20533', 'title': 'Accelerate Parallelizable Reasoning via Parallel Decoding within One\n  Sequence', 'url': 'https://huggingface.co/papers/2503.20533', 'abstract': 'Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.', 'score': 7, 'issue_id': 3144, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '3b237389882b1344', 'authors': ['Yijiong Yu'], 'affiliations': ['OpenCSG', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20533.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#math'], 'emoji': '🚀', 'ru': {'title': 'Ускорение рассуждений без потери качества', 'desc': 'Статья описывает метод ускорения процесса рассуждений в моделях машинного обучения. Авторы предлагают использовать параллелизацию для обработки нескольких ветвей рассуждений одновременно. Метод включает декодирование нескольких токенов за шаг с помощью специальной маски внимания. Эксперименты показывают, что данный подход позволяет более чем вдвое ускорить время декодирования без потери качества ответов.'}, 'en': {'title': 'Accelerating Reasoning with Parallel Token Decoding', 'desc': 'This paper presents a novel approach to enhance the efficiency of reasoning models in machine learning, particularly for complex tasks like mathematical reasoning. The authors focus on reducing the computational cost associated with generating lengthy reasoning sequences by utilizing parallel processing techniques. By implementing a specialized attention mask, they enable the model to decode multiple tokens simultaneously, which significantly speeds up the reasoning process without increasing memory usage. Experimental results indicate that this method can achieve more than 100% improvement in decoding speed while preserving the quality of the answers.'}, 'zh': {'title': '加速推理过程，提升效率与质量', 'desc': '最近的推理模型在复杂任务（如数学推理）中显示出显著的准确性提升，主要得益于详细和全面的推理过程。然而，生成这些冗长的推理序列在计算上是昂贵且耗时的。为了解决这一低效问题，我们利用某些任务的固有并行性来加速推理过程。具体来说，当存在多个并行推理分支时，我们使用专门的注意力掩码在每一步解码多个标记，从而在单个序列中处理它们，避免了额外的内存使用。'}}}, {'id': 'https://huggingface.co/papers/2504.06232', 'title': 'HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned\n  Guidance', 'url': 'https://huggingface.co/papers/2504.06232', 'abstract': "Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's superiority in achieving superior high-resolution image quality over current state-of-the-art methods.", 'score': 6, 'issue_id': 3145, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': '7def3ddddf039685', 'authors': ['Jiazi Bu', 'Pengyang Ling', 'Yujie Zhou', 'Pan Zhang', 'Tong Wu', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.06232.jpg', 'data': {'categories': ['#diffusion', '#training', '#optimization', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'HiFlow: раскрытие потенциала высокого разрешения в генерации изображений', 'desc': 'HiFlow - это инновационный подход к улучшению качества генерации изображений высокого разрешения с помощью предобученных моделей потоков. Метод создает виртуальный эталонный поток в пространстве высокого разрешения, который capture характеристики информации о потоке низкого разрешения. HiFlow использует три ключевых аспекта для улучшения генерации: выравнивание инициализации, выравнивание направления и выравнивание ускорения. Эксперименты показывают превосходство HiFlow над современными методами в достижении высокого качества изображений высокого разрешения.'}, 'en': {'title': 'Unlocking High-Resolution Image Synthesis with HiFlow', 'desc': 'This paper introduces HiFlow, a novel framework designed to enhance the capabilities of text-to-image (T2I) diffusion and flow models for generating high-resolution images. HiFlow operates without the need for additional training and is compatible with existing pre-trained flow models. It utilizes a virtual reference flow to align low-resolution and high-resolution data, ensuring consistency in low-frequency details, structural integrity, and fine details. The results show that HiFlow significantly improves the quality of high-resolution images compared to current leading methods, demonstrating its effectiveness and adaptability across various T2I models.'}, 'zh': {'title': 'HiFlow：提升高分辨率图像合成的创新框架', 'desc': '本文介绍了一种名为HiFlow的框架，旨在解决高分辨率图像合成中的挑战。HiFlow不需要训练，并且与模型无关，可以有效利用预训练的流模型。它通过建立虚拟参考流，捕捉低分辨率流信息的特征，从而指导高分辨率生成。实验结果表明，HiFlow在高分辨率图像质量上优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2504.05897', 'title': 'HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference', 'url': 'https://huggingface.co/papers/2504.05897', 'abstract': 'The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33times in the prefill stage and 1.70times in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE.', 'score': 6, 'issue_id': 3151, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': 'a4c4d6d7e202a236', 'authors': ['Shuzhang Zhong', 'Yanfan Sun', 'Ling Liang', 'Runsheng Wang', 'Ru Huang', 'Meng Li'], 'affiliations': ['Beijing Advanced Innovation Center for Integrated Circuits, Beijing, China', 'Institute for Artificial Intelligence, Peking University, Beijing, China', 'Institute of Electronic Design Automation, Peking University, Wuxi, China', 'School of Computer Science and Engineering, Beihang University, Beijing, China', 'School of Integrated Circuits, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.05897.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#inference', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Эффективный гибридный вывод MoE-моделей на CPU и GPU', 'desc': 'HybriMoE - это фреймворк для гибридного вывода моделей Mixture of Experts (MoE) на CPU и GPU. Он решает проблемы нестабильных паттернов активации экспертов и сложности планирования вычислений между CPU и GPU. Фреймворк использует динамическое планирование внутри слоев, алгоритм предварительной загрузки между слоями и кэширование на основе оценок. Эксперименты показывают значительное ускорение по сравнению с существующими решениями для вывода MoE-моделей.'}, 'en': {'title': 'Optimizing MoE Inference with HybriMoE: Speed and Efficiency Unleashed!', 'desc': 'The paper presents HybriMoE, a hybrid CPU-GPU inference framework designed to optimize the performance of Mixture of Experts (MoE) models. It addresses the challenges of unstable expert activation patterns and complex scheduling due to varying expert sizes and workloads. HybriMoE introduces innovative strategies such as dynamic intra-layer scheduling, impact-driven inter-layer prefetching, and score-based caching to enhance resource utilization. Experimental results show that HybriMoE significantly speeds up the inference process, achieving notable improvements over existing frameworks.'}, 'zh': {'title': 'HybriMoE：提升混合推理效率的创新框架', 'desc': '混合专家（MoE）架构在提高模型容量方面表现出显著优势，但其大规模模型仍然对内存提出了高要求。为了解决这一问题，本文提出了HybriMoE，一个混合CPU-GPU推理框架，通过新颖的调度和缓存管理系统来提高资源利用率。HybriMoE引入了动态的层内调度策略、影响驱动的层间预取算法和基于评分的缓存算法，以减轻专家激活的不稳定性。实验结果表明，HybriMoE在预填充阶段和解码阶段的速度分别提高了1.33倍和1.70倍。'}}}, {'id': 'https://huggingface.co/papers/2504.05520', 'title': 'Efficient Reinforcement Finetuning via Adaptive Curriculum Learning', 'url': 'https://huggingface.co/papers/2504.05520', 'abstract': "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.", 'score': 5, 'issue_id': 3151, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '474ad1b587ab6240', 'authors': ['Taiwei Shi', 'Yiyang Wu', 'Linxin Song', 'Tianyi Zhou', 'Jieyu Zhao'], 'affiliations': ['University of Maryland, College Park', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2504.05520.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl', '#math', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'AdaRFT: Адаптивное обучение для улучшения математических способностей ИИ', 'desc': 'В этой статье представлен метод AdaRFT (Адаптивное обучение с подкреплением на основе учебной программы), который значительно улучшает эффективность и точность обучения с подкреплением для больших языковых моделей в задачах математического рассуждения. AdaRFT динамически корректирует сложность обучающих задач на основе недавних сигналов вознаграждения модели, обеспечивая постоянное обучение на задачах, которые являются сложными, но решаемыми. Эксперименты на наборах данных уровня соревнований показывают, что AdaRFT значительно улучшает как эффективность обучения, так и производительность рассуждений. Метод сокращает количество шагов обучения до 2 раз и значительно повышает точность, предлагая более масштабируемую и эффективную структуру обучения с подкреплением.'}, 'en': {'title': 'Adaptive Learning for Efficient Reinforcement Finetuning', 'desc': "This paper presents AdaRFT, a new method that enhances Reinforcement Finetuning (RFT) for large language models (LLMs) by using adaptive curriculum learning. AdaRFT adjusts the difficulty of training tasks based on the model's performance, ensuring that it learns from problems that are appropriately challenging. This approach improves training efficiency by reducing unnecessary computations on tasks that are too easy or too hard. Experiments show that AdaRFT can cut training steps in half while significantly boosting the model's accuracy on complex math problems."}, 'zh': {'title': '自适应课程强化微调：提升效率与准确性', 'desc': '强化微调（RFT）在提升大型语言模型的数学推理能力方面展现了巨大潜力，但通常需要大量样本和计算资源。我们提出了AdaRFT（自适应课程强化微调），通过自适应课程学习显著提高了RFT的效率和最终准确性。AdaRFT根据模型最近的奖励信号动态调整训练问题的难度，确保模型始终在具有挑战性但可解决的任务上进行训练。这种自适应采样策略加速了学习，避免了在过于简单或过于困难的问题上浪费计算资源。'}}}, {'id': 'https://huggingface.co/papers/2504.06122', 'title': 'Leanabell-Prover: Posttraining Scaling in Formal Reasoning', 'url': 'https://huggingface.co/papers/2504.06122', 'abstract': 'Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.', 'score': 3, 'issue_id': 3147, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': 'a180ec0989dfd0c8', 'authors': ['Jingyuan Zhang', 'Qi Wang', 'Xingguang Ji', 'Yahui Liu', 'Yang Yue', 'Fuzheng Zhang', 'Di Zhang', 'Guorui Zhou', 'Kun Gai'], 'affiliations': ['Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2504.06122.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#training', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Революция в автоматическом доказательстве теорем: от языковых моделей к формальной логике', 'desc': 'Статья описывает исследование в области автоматического доказательства теорем (АДТ) с использованием языковых моделей (LLM). Авторы применяют непрерывное обучение на гибридном наборе данных, включающем пары утверждение-доказательство и дополнительные данные для имитации человеческих рассуждений. Они также исследуют обучение с подкреплением, используя вознаграждение от компилятора Lean 4. В результате им удалось улучшить существующие системы АДТ, достигнув наилучших показателей в генерации полных доказательств.'}, 'en': {'title': 'Revolutionizing Automated Theorem Proving with Human-like Reasoning', 'desc': 'This paper explores the enhancement of automated theorem proving (ATP) using large language models (LLMs) and Lean 4 code. The authors propose a continual training approach that combines a hybrid dataset of statement-proof pairs with cognitive behavior data to mimic human reasoning. They also implement reinforcement learning, utilizing feedback from the Lean 4 compiler to refine the models further. As a result, they report significant improvements in existing ATP systems, achieving a notable 59.8% pass rate on the MiniF2F benchmark.'}, 'zh': {'title': '提升自动定理证明的智能推理能力', 'desc': '本研究探讨了通过大规模语言模型（LLM）提升自动定理证明（ATP）的潜力，特别是与Lean 4代码的形式推理相关的进展。我们采用混合数据集对现有ATP模型进行持续训练，数据集中包含大量的命题-证明对，并加入模拟人类推理和假设修正的认知行为数据。接着，我们利用Lean 4编译器返回的结果奖励进行强化学习，以进一步优化模型性能。通过这些方法，我们成功提升了现有的形式证明器，如DeepSeek-Prover-v1.5和Goedel-Prover，在整体证明生成领域达到了最先进的表现。'}}}, {'id': 'https://huggingface.co/papers/2504.03755', 'title': 'ProtoGCD: Unified and Unbiased Prototype Learning for Generalized\n  Category Discovery', 'url': 'https://huggingface.co/papers/2504.03755', 'abstract': 'Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https://github.com/mashijie1028/ProtoGCD.', 'score': 1, 'issue_id': 3151, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'bdbc423fafc55199', 'authors': ['Shijie Ma', 'Fei Zhu', 'Xu-Yao Zhang', 'Cheng-Lin Liu'], 'affiliations': ['Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation, Chinese Academy of Sciences, Hong Kong 999077, P.R. China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing 100190, P.R. China'], 'pdf_title_img': 'assets/pdf/title_img/2504.03755.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#data', '#benchmark', '#optimization', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Унифицированное обучение прототипов для эффективного обнаружения новых категорий', 'desc': 'Статья представляет новый подход к обобщенному обнаружению категорий (GCD) в машинном обучении. Авторы предлагают унифицированную систему обучения прототипов под названием ProtoGCD, которая моделирует старые и новые классы с помощью общих прототипов и единых целей обучения. Метод включает двухуровневый адаптивный механизм псевдомаркировки и регуляризационные термы для улучшения представлений данных. ProtoGCD также способен оценивать количество новых классов и обнаруживать выбросы, демонстрируя лучшие результаты на различных наборах данных.'}, 'en': {'title': 'Unified Learning for Discovering New Categories in Machine Learning', 'desc': 'This paper addresses the challenge of Generalized Category Discovery (GCD), where models must identify new categories using existing labeled data. The authors introduce ProtoGCD, a framework that uses joint prototypes to represent both old and new classes, allowing for a more balanced learning process. They implement a dual-level adaptive pseudo-labeling mechanism to reduce bias and improve representation learning. Additionally, ProtoGCD includes a method for estimating new class counts and detecting unseen outliers, demonstrating superior performance on various datasets.'}, 'zh': {'title': '统一建模，发现新类别的力量', 'desc': '本文提出了一种新的框架，称为ProtoGCD，用于解决广义类别发现（GCD）问题。该框架通过联合原型学习来同时建模旧类别和新类别，克服了以往方法中存在的偏差和不平衡问题。我们引入了双层自适应伪标签机制，以减少确认偏差，并通过正则化项来优化表示学习。实验结果表明，ProtoGCD在多个数据集上表现出色，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.05299', 'title': 'SmolVLM: Redefining small and efficient multimodal models', 'url': 'https://huggingface.co/papers/2504.05299', 'abstract': 'Large Vision-Language Models (VLMs) deliver exceptional performance but require significant computational resources, limiting their deployment on mobile and edge devices. Smaller VLMs typically mirror design choices of larger models, such as extensive image tokenization, leading to inefficient GPU memory usage and constrained practicality for on-device applications.   We introduce SmolVLM, a series of compact multimodal models specifically engineered for resource-efficient inference. We systematically explore architectural configurations, tokenization strategies, and data curation optimized for low computational overhead. Through this, we identify key design choices that yield substantial performance gains on image and video tasks with minimal memory footprints.   Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during inference and outperforms the 300-times larger Idefics-80B model, despite an 18-month development gap. Our largest model, at 2.2B parameters, rivals state-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend beyond static images, demonstrating robust video comprehension capabilities.   Our results emphasize that strategic architectural optimizations, aggressive yet efficient tokenization, and carefully curated training data significantly enhance multimodal performance, facilitating practical, energy-efficient deployments at significantly smaller scales.', 'score': 84, 'issue_id': 3120, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '325b8841a555743d', 'authors': ['Andrés Marafioti', 'Orr Zohar', 'Miquel Farré', 'Merve Noyan', 'Elie Bakouch', 'Pedro Cuenca', 'Cyril Zakka', 'Loubna Ben Allal', 'Anton Lozhkov', 'Nouamane Tazi', 'Vaibhav Srivastav', 'Joshua Lochner', 'Hugo Larcher', 'Mathieu Morlon', 'Lewis Tunstall', 'Leandro von Werra', 'Thomas Wolf'], 'affiliations': ['Hugging Face', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05299.jpg', 'data': {'categories': ['#data', '#multimodal', '#video', '#optimization', '#low_resource', '#architecture', '#inference', '#small_models'], 'emoji': '🤏', 'ru': {'title': 'Маленькие модели - большие возможности: SmolVLM revolutionizes эффективность VLM', 'desc': 'SmolVLM - это серия компактных мультимодальных моделей, разработанных для эффективного использования ресурсов при выводе. Исследователи систематически изучили архитектурные конфигурации, стратегии токенизации и подготовку данных, оптимизированные для низких вычислительных затрат. Самая маленькая модель, SmolVLM-256M, использует менее 1 ГБ видеопамяти при выводе и превосходит в 300 раз большую модель Idefics-80B. Результаты показывают, что стратегические архитектурные оптимизации и тщательно подобранные данные для обучения значительно улучшают мультимодальную производительность при меньших масштабах.'}, 'en': {'title': 'SmolVLM: Compact Models for Efficient Vision-Language Tasks', 'desc': 'This paper presents SmolVLM, a series of compact vision-language models designed to operate efficiently on mobile and edge devices. Unlike larger models that require extensive computational resources, SmolVLM employs optimized architectural configurations and tokenization strategies to minimize GPU memory usage. The smallest model, SmolVLM-256M, achieves superior performance on image and video tasks while using less than 1GB of GPU memory, outperforming much larger models. The findings highlight the importance of strategic design choices in enhancing multimodal capabilities while ensuring practical deployment in resource-constrained environments.'}, 'zh': {'title': 'SmolVLM：高效的多模态模型', 'desc': '大型视觉语言模型（VLMs）表现优异，但需要大量计算资源，限制了它们在移动和边缘设备上的应用。较小的VLM通常模仿大型模型的设计选择，导致GPU内存使用效率低下。我们提出了SmolVLM，这是一系列专为资源高效推理而设计的紧凑型多模态模型。我们的研究表明，通过优化架构配置、标记策略和数据整理，可以在保持较小内存占用的同时显著提升图像和视频任务的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.05298', 'title': 'One-Minute Video Generation with Test-Time Training', 'url': 'https://huggingface.co/papers/2504.05298', 'abstract': 'Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit', 'score': 54, 'issue_id': 3116, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'c8edb1a98923d77d', 'authors': ['Karan Dalal', 'Daniel Koceja', 'Gashon Hussein', 'Jiarui Xu', 'Yue Zhao', 'Youjin Song', 'Shihao Han', 'Ka Chun Cheung', 'Jan Kautz', 'Carlos Guestrin', 'Tatsunori Hashimoto', 'Sanmi Koyejo', 'Yejin Choi', 'Yu Sun', 'Xiaolong Wang'], 'affiliations': ['NVIDIA', 'Stanford University', 'UC Berkeley', 'UCSD', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2504.05298.jpg', 'data': {'categories': ['#training', '#video', '#story_generation', '#long_context', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'TTT слои: прорыв в генерации длинных видео трансформерами', 'desc': "Эта статья представляет новый подход к генерации длинных видео с использованием слоев Test-Time Training (TTT) в трансформерах. TTT слои позволяют создавать более выразительные скрытые состояния, что улучшает генерацию сложных многосценных историй по сравнению с альтернативами вроде Mamba. Авторы провели эксперименты на наборе данных мультфильмов 'Том и Джерри', показав преимущество TTT слоев в создании связных минутных видео по текстовым раскадровкам. Хотя результаты многообещающие, все еще присутствуют артефакты, вероятно из-за ограничений предобученной модели."}, 'en': {'title': 'Enhancing Video Generation with Test-Time Training Layers', 'desc': 'This paper addresses the challenge of generating one-minute videos from text using Transformers, which struggle with long contexts due to inefficient self-attention layers. The authors introduce Test-Time Training (TTT) layers, which enhance the expressiveness of hidden states by allowing them to be neural networks. By integrating TTT layers into a pre-trained Transformer, the model significantly improves video coherence and storytelling ability compared to existing methods like Mamba and Gated DeltaNet. The results show a notable increase in human evaluation scores, although the authors acknowledge the presence of artifacts and the need for further efficiency improvements.'}, 'zh': {'title': '提升视频生成的表达能力', 'desc': '本文探讨了在生成一分钟视频时，变换器模型面临的挑战，尤其是自注意力层在处理长上下文时的低效。我们提出了测试时训练（TTT）层，这些层的隐藏状态可以是神经网络，从而提高了表达能力。通过将TTT层添加到预训练的变换器中，我们能够从文本故事板生成更连贯的一分钟视频。尽管结果显示出良好的潜力，但仍存在一些伪影，表明预训练的5B模型能力有限。'}}}, {'id': 'https://huggingface.co/papers/2504.04022', 'title': 'Rethinking Reflection in Pre-Training', 'url': 'https://huggingface.co/papers/2504.04022', 'abstract': "A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks.", 'score': 47, 'issue_id': 3128, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 апреля', 'en': 'April 5', 'zh': '4月5日'}, 'hash': 'c3bb14b88112a3ea', 'authors': ['Essential AI', ':', 'Darsh J Shah', 'Peter Rushton', 'Somanshu Singla', 'Mohit Parmar', 'Kurt Smith', 'Yash Vanjani', 'Ashish Vaswani', 'Adarsh Chaluvaraju', 'Andrew Hojel', 'Andrew Ma', 'Anil Thomas', 'Anthony Polloreno', 'Ashish Tanwer', 'Burhan Drak Sibai', 'Divya S Mansingka', 'Divya Shivaprasad', 'Ishaan Shah', 'Karl Stratos', 'Khoi Nguyen', 'Michael Callahan', 'Michael Pust', 'Mrinal Iyer', 'Philip Monk', 'Platon Mazarakis', 'Ritvik Kapila', 'Saurabh Srivastava', 'Tim Romanski'], 'affiliations': ['DeepSeek-AI', 'Essential AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.04022.jpg', 'data': {'categories': ['#interpretability', '#training', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самоанализ языковых моделей начинается с предобучения', 'desc': 'Исследование показывает, что способность языковых моделей к самоанализу начинает формироваться уже на этапе предварительного обучения, а не только при обучении с подкреплением. Авторы вводили намеренные ошибки в цепочки рассуждений и проверяли, может ли модель прийти к правильному ответу, распознав и исправив эти ошибки. Отслеживая производительность на разных этапах предварительного обучения, они обнаружили, что способность к самокоррекции появляется рано и постепенно улучшается. Например, модель OLMo2-7B, предобученная на 4 триллионах токенов, демонстрирует самокоррекцию на шести задачах по самоанализу.'}, 'en': {'title': 'Self-Reflection: A Key to Early Problem Solving in Language Models', 'desc': 'This paper explores how language models can reflect on their own reasoning, which helps them solve complex problems. The authors reveal that this self-reflective ability starts to develop during the pre-training phase, rather than only during reinforcement learning. They introduce intentional errors in reasoning tasks to see if the model can identify and correct these mistakes. Their findings show that the self-correcting capability improves as the model undergoes more pre-training, with the OLMo2-7B model demonstrating this ability effectively after being trained on a large dataset.'}, 'zh': {'title': '语言模型的自我反思能力：早期显现与持续提升', 'desc': '本文探讨了语言模型在解决复杂问题时自我反思能力的重要性。研究表明，这种能力不仅在强化学习阶段发展，实际上在模型的预训练阶段就开始显现。我们通过在思维链中引入故意错误，测试模型是否能够识别并纠正这些错误，从而得出正确答案。结果显示，OLMo2-7B模型在预训练的早期阶段就展现出自我纠正能力，并随着时间的推移不断提高。'}}}, {'id': 'https://huggingface.co/papers/2504.05305', 'title': 'URECA: Unique Region Caption Anything', 'url': 'https://huggingface.co/papers/2504.05305', 'abstract': 'Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.', 'score': 26, 'issue_id': 3115, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '6eec948e6319fc99', 'authors': ['Sangbeom Lim', 'Junwan Kim', 'Heeji Yoon', 'Jaewoo Jung', 'Seungryong Kim'], 'affiliations': ['KAIST AI', 'Korea University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05305.jpg', 'data': {'categories': ['#data', '#games', '#cv', '#interpretability', '#dataset', '#benchmark', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Точное описание регионов изображений с помощью многоуровневого подхода', 'desc': 'Статья представляет новый набор данных URECA для многоуровневого описания регионов изображений. Авторы разработали поэтапный процесс создания данных с использованием мультимодальных больших языковых моделей для генерации уникальных и контекстуально обоснованных описаний. На основе этого набора данных предложена модель URECA, которая эффективно кодирует регионы разной детализации, сохраняя их пространственные свойства. Эксперименты показывают, что URECA достигает наилучших результатов на созданном наборе данных и хорошо обобщается на существующие эталонные тесты.'}, 'en': {'title': 'Enhancing Region-Level Captioning with URECA Dataset and Model', 'desc': 'This paper presents a new approach to region-level captioning, which generates detailed descriptions for specific parts of images. The authors introduce the URECA dataset, designed to improve the uniqueness of captions by including a variety of objects and backgrounds. They propose a novel captioning model, URECA, that uses advanced techniques like dynamic mask modeling to maintain spatial properties and enhance the quality of generated captions. The results demonstrate that URECA outperforms existing methods, providing more accurate and diverse descriptions across different image regions.'}, 'zh': {'title': '多粒度区域描述的新突破', 'desc': '区域级描述旨在为特定图像区域生成自然语言描述，并突出其独特特征。然而，现有方法在多粒度生成独特描述方面存在困难，限制了其在实际应用中的有效性。为了解决这一问题，我们引入了URECA数据集，这是一个针对多粒度区域描述的大规模数据集，确保区域与描述之间的独特和一致的映射。基于此数据集，我们提出了URECA模型，能够有效编码多粒度区域，生成细致且语义丰富的描述。'}}}, {'id': 'https://huggingface.co/papers/2504.04718', 'title': 'T1: Tool-integrated Self-verification for Test-time Compute Scaling in\n  Small Language Models', 'url': 'https://huggingface.co/papers/2504.04718', 'abstract': 'Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs.', 'score': 26, 'issue_id': 3120, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '51f832049b5599a6', 'authors': ['Minki Kang', 'Jongwon Jeong', 'Jaewoong Cho'], 'affiliations': ['KAIST', 'KRAFTON'], 'pdf_title_img': 'assets/pdf/title_img/2504.04718.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#small_models', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Малые языковые модели становятся умнее с помощью инструментов', 'desc': 'Исследователи изучили возможность самопроверки малых языковых моделей (sLMs) при масштабировании во время тестирования. Обнаружено, что sLMs испытывают трудности с задачами, требующими запоминания, даже после дистилляции знаний от больших моделей. Предложен метод Tool-integrated self-verification (T1), который делегирует задачи, требующие запоминания, внешним инструментам. Эксперименты показали, что T1 позволяет небольшой модели Llama-3.2 1B превзойти значительно большую Llama-3.1 8B на различных задачах.'}, 'en': {'title': 'Empowering Small Models with Tool Integration for Better Self-Verification', 'desc': 'This paper explores how small language models (sLMs) can improve their performance through test-time compute scaling, particularly focusing on their ability to self-verify outputs. The authors find that sLMs face challenges in verification tasks that require memorization, such as numerical calculations and fact-checking, even when they learn from larger models. To overcome this, they introduce Tool-integrated self-verification (T1), which allows sLMs to use external tools for tasks that require heavy memorization. Their experiments show that T1 significantly enhances the performance of sLMs, enabling them to outperform larger models in various knowledge-intensive tasks.'}, 'zh': {'title': '工具集成提升小型语言模型自我验证能力', 'desc': '最近的研究表明，测试时计算扩展可以有效提高小型语言模型（sLMs）的性能。然而，之前的研究主要关注于使用更大模型作为验证者的测试时计算扩展，而对sLMs的自我验证研究较少。我们发现，即使通过知识蒸馏从更大的验证者那里获得知识，sLMs在需要记忆的验证任务（如数字计算和事实核查）中仍然面临困难。为了解决这个问题，我们提出了工具集成自我验证（T1），将重记忆的验证步骤委托给外部工具，如代码解释器。'}}}, {'id': 'https://huggingface.co/papers/2504.02828', 'title': 'Concept Lancet: Image Editing with Compositional Representation\n  Transplant', 'url': 'https://huggingface.co/papers/2504.02828', 'abstract': 'Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.', 'score': 15, 'issue_id': 3117, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '1c289ffc8ceda51e', 'authors': ['Jinqi Luo', 'Tianjiao Ding', 'Kwan Ho Ryan Chan', 'Hancheng Min', 'Chris Callison-Burch', 'René Vidal'], 'affiliations': ['University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2504.02828.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#cv', '#inference'], 'emoji': '✂️', 'ru': {'title': 'Точное редактирование изображений с помощью концептуального скальпеля', 'desc': 'Статья представляет новый подход к редактированию изображений с помощью диффузионных моделей, называемый Concept Lancet (CoLan). CoLan решает проблему определения оптимальной силы редактирования для каждого изображения, разлагая входное изображение в латентном пространстве как линейную комбинацию визуальных концептов. Этот метод позволяет точно оценить присутствие концептов в изображении и выполнить соответствующее редактирование. Авторы также создали датасет CoLan-150K с разнообразными описаниями визуальных терминов для латентного словаря.'}, 'en': {'title': 'Precision Editing with Concept Lancet', 'desc': 'This paper introduces Concept Lancet (CoLan), a novel framework for improving image editing using diffusion models. It addresses the challenge of determining the right strength of edits needed for different images, which can vary significantly. CoLan utilizes a sparse linear combination of visual concept representations to accurately assess and manipulate the presence of these concepts in images. The framework is supported by a comprehensive dataset, CoLan-150K, which enhances the editing process by providing diverse visual descriptions and scenarios.'}, 'zh': {'title': '精准编辑，概念移植！', 'desc': '扩散模型在图像编辑任务中被广泛应用。现有的编辑方法通常通过在文本嵌入或评分空间中设计编辑方向来操控表示。然而，这种方法面临一个关键挑战：过高的编辑强度会损害视觉一致性，而过低的编辑强度则无法完成编辑任务。为了解决这个问题，我们提出了Concept Lancet（CoLan），这是一个零-shot的即插即用框架，能够在扩散基础的图像编辑中进行原则性的表示操控。'}}}, {'id': 'https://huggingface.co/papers/2504.04823', 'title': 'Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models', 'url': 'https://huggingface.co/papers/2504.04823', 'abstract': 'Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.', 'score': 14, 'issue_id': 3117, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '92bd3deed21195f2', 'authors': ['Ruikang Liu', 'Yuxuan Sun', 'Manyi Zhang', 'Haoli Bai', 'Xianzhi Yu', 'Tiezheng Yu', 'Chun Yuan', 'Lu Hou'], 'affiliations': ['Huawei Noahs Ark Lab', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.04823.jpg', 'data': {'categories': ['#open_source', '#optimization', '#inference', '#reasoning', '#math', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Квантование моделей рассуждений: баланс между эффективностью и точностью', 'desc': 'Это исследование посвящено изучению влияния квантования на языковые модели, специализирующиеся на рассуждениях. Авторы провели систематический анализ квантованных моделей рассуждений, оценивая различные семейства моделей с параметрами от 1,5B до 70B. Исследование охватывает квантование весов, KV-кэша и активаций с использованием современных алгоритмов при различных битовых ширинах. Результаты показывают, что хотя безлосстное квантование возможно при W8A8 или W4A16, более низкие битовые ширины значительно снижают точность.'}, 'en': {'title': 'Optimizing Reasoning Models with Quantization', 'desc': 'This paper investigates the effects of quantization on reasoning language models, which are known for their complex task performance but high inference costs. The authors systematically evaluate various quantization techniques on models like DeepSeek-R1-Distilled Qwen and LLaMA, focusing on different parameter sizes and quantization methods. They find that while lossless quantization is possible with certain configurations, lower bit-widths can lead to significant accuracy drops. Additionally, the study highlights that model size, origin, and task difficulty are crucial factors influencing performance, and suggests that adjusting model sizes or reasoning steps can improve outcomes.'}, 'zh': {'title': '量化推理模型的系统研究', 'desc': '最近，推理语言模型在复杂任务中表现出色，但其链式推理过程增加了推理开销。虽然量化技术已被广泛应用于降低大型语言模型的推理成本，但其对推理模型的影响仍未得到充分研究。我们首次系统性地研究了量化推理模型，评估了多个开源模型，并在不同的位宽下进行权重、KV缓存和激活量化的实验。研究发现，尽管可以实现无损量化，但较低的位宽会显著影响准确性，同时模型大小、来源和任务难度是性能的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2504.05288', 'title': 'LiveVQA: Live Visual Knowledge Seeking', 'url': 'https://huggingface.co/papers/2504.05288', 'abstract': 'We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen-2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research.', 'score': 12, 'issue_id': 3117, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '8302679426ac3ec4', 'authors': ['Mingyang Fu', 'Yuyang Peng', 'Benlin Liu', 'Yao Wan', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.05288.jpg', 'data': {'categories': ['#cv', '#reasoning', '#survey', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'LiveVQA: Новый рубеж в визуальном вопросно-ответном анализе', 'desc': 'LiveVQA - это автоматически собранный датасет с последними визуальными знаниями из интернета и синтезированными задачами визуальных вопросов и ответов (VQA). Он содержит 3602 одношаговых и многошаговых визуальных вопроса с 6 новостных сайтов по 14 категориям новостей, отличаясь высоким качеством согласованности изображений и текста. Оценка 15 мультимодальных языковых моделей (MLLM) показала, что более мощные модели работают лучше в целом, а продвинутые возможности визуального рассуждения критически важны для сложных многошаговых вопросов. Несмотря на отличные результаты в текстовых задачах, модели с инструментами вроде поисковых систем все еще демонстрируют значительные пробелы при ответах на визуальные вопросы, требующие актуальных визуальных знаний.'}, 'en': {'title': 'Empowering Visual Question Answering with LiveVQA', 'desc': 'LiveVQA is a new dataset designed to enhance visual question answering (VQA) by providing up-to-date visual knowledge sourced from the Internet. It includes 3,602 questions that require reasoning over images and text, covering various news topics. Our tests on 15 advanced machine learning language models (MLLMs) show that models with better visual reasoning skills excel at answering complex questions. However, even the best models struggle with visual questions that need the latest information, indicating a need for further research in this area.'}, 'zh': {'title': '最新视觉知识的问答挑战', 'desc': '我们介绍了LiveVQA，这是一个自动收集的最新视觉知识数据集，包含合成的视觉问答（VQA）问题。LiveVQA包含来自6个新闻网站的3,602个单跳和多跳视觉问题，涵盖14个新闻类别，具有高质量的图像-文本一致性和真实信息。我们的评估显示，15个大型语言模型（如GPT-4o、Gemma-3和Qwen-2.5-VL系列）中，性能更强的模型在整体表现上更好，尤其在复杂的多跳问题上，先进的视觉推理能力至关重要。尽管在文本问题上表现出色，但使用搜索引擎等工具的模型在处理需要最新视觉知识的视觉问题时仍存在显著差距，这突显了未来研究的重要领域。'}}}, {'id': 'https://huggingface.co/papers/2504.05118', 'title': 'VAPO: Efficient and Reliable Reinforcement Learning for Advanced\n  Reasoning Tasks', 'url': 'https://huggingface.co/papers/2504.05118', 'abstract': 'We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of 60.4. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks.', 'score': 11, 'issue_id': 3123, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '33841bfe919ea1d5', 'authors': ['YuYue', 'Yufeng Yuan', 'Qiying Yu', 'Xiaochen Zuo', 'Ruofei Zhu', 'Wenyuan Xu', 'Jiaze Chen', 'Chengyi Wang', 'TianTian Fan', 'Zhengyin Du', 'Xiangpeng Wei', 'Gaohong Liu', 'Juncai Liu', 'Lingjun Liu', 'Haibin Lin', 'Zhiqi Lin', 'Bole Ma', 'Chi Zhang', 'Mofan Zhang', 'Wang Zhang', 'Hang Zhu', 'Ru Zhang', 'Xin Liu', 'Mingxuan Wang', 'Yonghui Wu', 'Lin Yan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.05118.jpg', 'data': {'categories': ['#long_context', '#rl', '#benchmark', '#optimization', '#reasoning', '#training', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'VAPO: прорыв в моделировании рассуждений с помощью ценностно-ориентированного обучения с подкреплением', 'desc': 'VAPO - это новая система для моделей рассуждений, основанная на ценностно-ориентированном подходе. Используя предобученную модель Qwen 32B, VAPO достигает лучшего результата в 60.4 балла на датасете AIME 2024. Система превосходит предыдущие результаты более чем на 10 пунктов и достигает высокой производительности всего за 5000 шагов обучения. VAPO решает ключевые проблемы ценностно-ориентированных методов в задачах рассуждений с длинной цепочкой мыслей.'}, 'en': {'title': 'VAPO: Revolutionizing Reasoning with Value-Based Reinforcement Learning', 'desc': 'VAPO is a new framework designed for reasoning models that uses value-based reinforcement learning. It achieves a remarkable score of 60.4 on the AIME 2024 dataset, surpassing previous models by over 10 points. The training of VAPO is both stable and efficient, completing in just 5,000 steps without any crashes during multiple runs. This research addresses key issues in value-based methods, such as model bias and reward sparsity, providing solutions that improve long chain-of-thought reasoning.'}, 'zh': {'title': 'VAPO：提升推理模型的价值基础框架', 'desc': '本文介绍了一种名为VAPO的框架，旨在为基于价值的推理模型提供支持。VAPO在AIME 2024数据集上表现出色，达到了60.4的最新成绩，超越了之前的DeepSeek-R1-Zero-Qwen-32B和DAPO模型。该框架的训练过程稳定高效，仅需5000步即可达到最佳性能，并且在多次独立运行中没有发生训练崩溃，显示出其可靠性。VAPO通过系统设计有效解决了基于价值的方法面临的三个主要挑战，提升了长链推理任务的表现。'}}}, {'id': 'https://huggingface.co/papers/2504.05304', 'title': 'Gaussian Mixture Flow Matching Models', 'url': 'https://huggingface.co/papers/2504.05304', 'abstract': 'Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an L_2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256times256.', 'score': 6, 'issue_id': 3115, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'b0223808c61a3545', 'authors': ['Hansheng Chen', 'Kai Zhang', 'Hao Tan', 'Zexiang Xu', 'Fujun Luan', 'Leonidas Guibas', 'Gordon Wetzstein', 'Sai Bi'], 'affiliations': ['Adobe Research, CA 95110, USA', 'Hillbot', 'Stanford University, CA 94305, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.05304.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#inference'], 'emoji': '🌊', 'ru': {'title': 'GMFlow: мощная генерация изображений с гауссовыми смесями', 'desc': 'Статья представляет новую модель Gaussian mixture flow matching (GMFlow) для генерации изображений. GMFlow предсказывает параметры динамической гауссовой смеси для захвата мультимодального распределения скорости потока, что можно обучить с помощью потери KL-дивергенции. Авторы разработали специальные решатели GM-SDE/ODE для точного сэмплирования за небольшое число шагов. Также предложена новая схема вероятностного управления, улучшающая качество генерации изображений и решающая проблему пересыщенности цветов при классификационно-свободном управлении.'}, 'en': {'title': 'GMFlow: Enhancing Image Generation with Dynamic Gaussian Mixtures', 'desc': 'This paper introduces a new model called Gaussian Mixture Flow Matching (GMFlow) that improves upon traditional diffusion models and flow matching models. Instead of just predicting a single Gaussian mean, GMFlow predicts parameters for a dynamic Gaussian mixture, allowing it to better capture complex distributions in the data. The model addresses issues like discretization error and color saturation in generated images by using a novel probabilistic guidance scheme. Experimental results show that GMFlow achieves higher image generation quality with fewer sampling steps compared to existing methods.'}, 'zh': {'title': '高斯混合流匹配：提升图像生成质量的新方法', 'desc': '扩散模型通过高斯分布来近似去噪分布并预测其均值，而流匹配模型则将高斯均值重新参数化为流速。然而，它们在少步采样时表现不佳，主要是由于离散化误差，并且在无分类器引导下容易产生过饱和的颜色。为了解决这些问题，我们提出了一种新颖的高斯混合流匹配（GMFlow）模型：GMFlow预测动态高斯混合参数，以捕捉多模态流速分布，并通过KL散度损失进行学习。实验表明，GMFlow在生成质量上始终优于流匹配基线，在ImageNet 256x256上仅用6个采样步骤就达到了0.942的精度。'}}}, {'id': 'https://huggingface.co/papers/2504.04715', 'title': 'Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs', 'url': 'https://huggingface.co/papers/2504.04715', 'abstract': 'The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit', 'score': 6, 'issue_id': 3117, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '28e9dfa4b4a0421a', 'authors': ['Will Cai', 'Tianneng Shi', 'Xuandong Zhao', 'Dawn Song'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.04715.jpg', 'data': {'categories': ['#security', '#inference', '#benchmark', '#ethics'], 'emoji': '🕵️', 'ru': {'title': 'Защита от подмены: обеспечение честности в API языковых моделей', 'desc': 'Статья рассматривает проблему доверия к API больших языковых моделей (LLM), когда провайдеры могут тайно подменять заявленные модели на более дешевые аналоги. Авторы формализуют задачу обнаружения таких подмен и оценивают эффективность существующих методов верификации. Исследование показывает ограниченность методов, основанных только на анализе текстовых выходов, особенно против адаптивных атак. В качестве потенциального решения предлагается использование доверенных сред исполнения (TEE) для обеспечения целостности модели.'}, 'en': {'title': 'Ensuring Trust in Large Language Models: Detecting Substitutions in Black-Box APIs', 'desc': 'This paper addresses the issue of trust in Large Language Models (LLMs) accessed through APIs, where users may unknowingly receive lower-quality models instead of the advertised ones. It formalizes the challenge of detecting these model substitutions, which is complicated by the black-box nature of LLMs that limits user interactions to simple input-output queries. The authors evaluate various existing verification techniques, revealing their limitations, particularly against sophisticated attacks that can evade detection. They propose hardware-based solutions like Trusted Execution Environments (TEEs) as a potential way to ensure model integrity, while also considering the trade-offs involved.'}, 'zh': {'title': '确保大型语言模型的透明性与信任', 'desc': '本文探讨了大型语言模型（LLMs）在黑箱API中使用所带来的信任挑战。用户支付服务费用时，依赖于模型的能力（如规模和性能），但提供者可能会偷偷用更便宜、质量更低的替代模型来降低成本。这种缺乏透明度的问题影响了公平性和信任度，并使得可靠的基准测试变得复杂。我们系统评估了现有的验证技术，并提出了基于硬件的解决方案，以提高模型的完整性。'}}}, {'id': 'https://huggingface.co/papers/2504.03151', 'title': 'Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning\n  (v1)', 'url': 'https://huggingface.co/papers/2504.03151', 'abstract': 'Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.', 'score': 5, 'issue_id': 3130, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': 'c209fd185b1e9776', 'authors': ['Jing Bi', 'Susan Liang', 'Xiaofei Zhou', 'Pinxin Liu', 'Junjia Guo', 'Yunlong Tang', 'Luchuan Song', 'Chao Huang', 'Guangyu Sun', 'Jinxi He', 'Jiarui Wu', 'Shu Yang', 'Daoan Zhang', 'Chen Chen', 'Lianggong Bruce Wen', 'Zhang Liu', 'Jiebo Luo', 'Chenliang Xu'], 'affiliations': ['Corning Inc.', 'University of Central Florida', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.03151.jpg', 'data': {'categories': ['#training', '#optimization', '#inference', '#reasoning', '#interpretability', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальные рассуждения: новые горизонты для LLM', 'desc': 'Эта статья посвящена рассуждениям в больших языковых моделях (LLM) и их расширению на мультимодальные контексты. Авторы обсуждают сложности интеграции визуальных и текстовых входных данных в процессе рассуждений. В работе представлен обзор методов рассуждения как в текстовых, так и в мультимодальных LLM, а также анализируются основные проблемы и возможности в этой области. Статья предлагает практические методы оптимизации после обучения и вывода во время тестирования, устанавливая направления для будущих исследований.'}, 'en': {'title': 'Enhancing Reasoning in Multimodal Large Language Models', 'desc': "This paper discusses the importance of reasoning in human intelligence and how it relates to large language models (LLMs). It highlights the advancements in LLMs' reasoning capabilities, particularly in arithmetic and commonsense tasks, but points out the challenges in applying these abilities to multimodal contexts that involve both visual and textual data. The authors address the complexities of multimodal reasoning, such as managing conflicting information, and propose methods for improving model performance through post-training optimization. Overall, the paper serves as a comprehensive overview of reasoning techniques in LLMs and outlines future research directions to enhance multimodal reasoning."}, 'zh': {'title': '推理能力的多模态扩展挑战', 'desc': '推理是人类智能的核心，使我们能够在各种任务中进行结构化的问题解决。最近，大型语言模型（LLMs）的进步显著提升了它们在算术、常识和符号领域的推理能力。然而，将这些能力有效扩展到多模态上下文中仍然是一个重大挑战，因为模型需要整合视觉和文本输入。本文提供了对文本和多模态LLMs中推理技术的简明概述，明确了核心推理挑战和机会，并强调了后训练优化和测试时推理的实用方法。'}}}, {'id': 'https://huggingface.co/papers/2504.02882', 'title': 'DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models', 'url': 'https://huggingface.co/papers/2504.02882', 'abstract': "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling.", 'score': 4, 'issue_id': 3120, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'c42de57890092432', 'authors': ['Sunghee Jung', 'Donghun Lee', 'Shinbok Lee', 'Gaeun Seo', 'Daniel Lee', 'Byeongil Ko', 'Junrae Cho', 'Kihyun Kim', 'Eunggyun Kim', 'Myeongcheol Shin'], 'affiliations': ['Kakao Corp. Seongnam-si, Gyeonggi-do, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2504.02882.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'DiaTool-DPO: Умный диалог без экспертных примеров', 'desc': 'DiaTool-DPO - это новый метод, улучшающий диалоговые возможности инструментально-расширенных больших языковых моделей (TA-LLM) с помощью прямой оптимизации предпочтений. Авторы моделируют взаимодействие TA-LLM как марковский процесс принятия решений с 5 состояниями диалога и 3 типами запросов пользователей. Метод автоматически создает наборы данных правильных и неправильных траекторий диалога и вводит специальную функцию потерь для управления диалогом. Оценка показывает, что DiaTool-DPO приближается к производительности GPT-4, значительно превосходя базовый уровень, при сохранении основной функциональности.'}, 'en': {'title': 'Enhancing Dialogue with DiaTool-DPO for TA-LLMs', 'desc': 'This paper introduces DiaTool-DPO, a new method to improve Tool-Augmented Large Language Models (TA-LLMs) in handling incomplete and out-of-scope queries. It treats TA-LLM interactions as a Markov Decision Process, identifying five dialogue states and categorizing user queries into three types based on their transitions. The method involves creating paired datasets of correct and incorrect dialogue flows and using a specialized loss function for better dialogue control. The results show that DiaTool-DPO significantly enhances performance, approaching that of GPT-4o, while reducing the need for expert input and human labeling.'}, 'zh': {'title': '提升对话能力的新方法：DiaTool-DPO', 'desc': '本文提出了一种新方法DiaTool-DPO，旨在提升工具增强大型语言模型（TA-LLM）的对话能力。我们将TA-LLM的交互建模为马尔可夫决策过程，并根据对话状态的转移轨迹将用户查询分为三类。通过自动构建正确和错误对话流的配对轨迹数据集，并引入专门的目标损失函数，我们的评估显示DiaTool-DPO在信息获取和工具调用拒绝方面的表现接近GPT-4o。该方法为开发能够处理多样化现实场景的TA-LLM开辟了新可能，无需额外的专家演示或人工标注。'}}}, {'id': 'https://huggingface.co/papers/2504.03964', 'title': 'Clinical ModernBERT: An efficient and long context encoder for\n  biomedical text', 'url': 'https://huggingface.co/papers/2504.03964', 'abstract': 'We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.', 'score': 3, 'issue_id': 3119, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '6602bd6699f6f653', 'authors': ['Simon A. Lee', 'Anthony Wu', 'Jeffrey N. Chiang'], 'affiliations': ['Department of Computational Medicine & Neurosurgery UCLA', 'Department of Computational Medicine UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2504.03964.jpg', 'data': {'categories': ['#science', '#long_context', '#benchmark', '#architecture', '#healthcare', '#dataset'], 'emoji': '🏥', 'ru': {'title': 'Прорыв в обработке медицинских текстов: Clinical ModernBERT', 'desc': 'Эта статья представляет Clinical ModernBERT - трансформерную модель, предобученную на биомедицинской литературе, клинических заметках и медицинских онтологиях. Модель основана на архитектуре ModernBERT и включает улучшения, такие как ротационные позиционные эмбеддинги и расширенный контекст до 8192 токенов. Clinical ModernBERT специализируется на создании семантически богатых представлений для задач с длинным контекстом в медицинской сфере. Эффективность модели подтверждена анализом предобученных весов и оценкой на наборе клинических NLP-бенчмарков.'}, 'en': {'title': 'Empowering Clinical NLP with Advanced Transformer Technology', 'desc': 'Clinical ModernBERT is a specialized transformer model designed for the biomedical and clinical fields. It is pretrained on a vast array of data, including biomedical literature and clinical notes, to enhance its understanding of medical language. The model incorporates advanced features like rotary positional embeddings and Flash Attention, allowing it to handle longer text inputs effectively. Its performance is validated through rigorous testing on various clinical natural language processing benchmarks, demonstrating its ability to generate meaningful representations for complex medical tasks.'}, 'zh': {'title': '生物医学领域的强大文本编码器', 'desc': '我们介绍了Clinical ModernBERT，这是一种基于变换器的编码器，经过大规模生物医学文献、临床笔记和医学本体的预训练。该模型结合了PubMed摘要、MIMIC IV临床数据和医学代码及其文本描述，采用了现代BERT的架构升级，如旋转位置嵌入（RoPE）和闪存注意力（Flash Attention）。Clinical ModernBERT在处理长上下文任务时，能够生成语义丰富的表示。我们通过分析其预训练权重和在临床自然语言处理基准上的实证评估来验证其有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.03193', 'title': 'Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation', 'url': 'https://huggingface.co/papers/2504.03193', 'abstract': 'Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser.', 'score': 3, 'issue_id': 3120, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '6eca2c0b1acce1f3', 'authors': ['Xin Zhang', 'Robby T. Tan'], 'affiliations': ['ASUS Intelligent Cloud Services', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.03193.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#architecture', '#cv', '#benchmark', '#synthetic'], 'emoji': '🔍', 'ru': {'title': 'MFuser: Эффективное слияние VFM и VLM для улучшенной семантической сегментации', 'desc': 'Эта статья представляет новый подход к обобщенной семантической сегментации доменов (DGSS), называемый MFuser. Он объединяет сильные стороны моделей визуального основания (VFM) и визуально-языковых моделей (VLM), используя архитектуру на основе Mamba. MFuser включает в себя MVFuser для совместной доводки моделей и MTEnhancer для улучшения текстовых эмбеддингов с учетом визуальных признаков. Экспериментальные результаты показывают значительное превосходство MFuser над современными методами DGSS на различных бенчмарках.'}, 'en': {'title': 'Harnessing the Power of Vision Models for Better Segmentation', 'desc': 'This paper introduces MFuser, a new framework that combines Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) for Domain Generalized Semantic Segmentation (DGSS). VFMs are good at capturing detailed visual features, while VLMs excel in aligning text with images, but they have limitations when used separately. MFuser uses a co-adapter and a hybrid attention mechanism to effectively merge these models, allowing for better feature extraction and text alignment without heavy computational costs. The results show that MFuser outperforms existing DGSS methods, achieving high accuracy on various benchmarks.'}, 'zh': {'title': '融合视觉与语言，提升语义分割能力', 'desc': '视觉基础模型（VFM）和视觉语言模型（VLM）在领域泛化语义分割（DGSS）中因其强大的泛化能力而受到关注。现有的DGSS方法通常只依赖于VFM或VLM，忽视了它们的互补优势。我们提出了MFuser，一个新颖的融合框架，能够高效结合VFM和VLM的优点，同时保持序列长度的线性可扩展性。通过联合微调和混合注意力机制，MFuser在特征局部性和文本对齐方面表现出色，显著超越了现有的DGSS方法。'}}}, {'id': 'https://huggingface.co/papers/2504.02812', 'title': 'BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation', 'url': 'https://huggingface.co/papers/2504.02812', 'abstract': 'We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/', 'score': 3, 'issue_id': 3118, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '692278d7307c7950', 'authors': ['Van Nguyen Nguyen', 'Stephen Tyree', 'Andrew Guo', 'Mederic Fourmy', 'Anas Gouda', 'Taeyeop Lee', 'Sungphill Moon', 'Hyeontae Son', 'Lukas Ranftl', 'Jonathan Tremblay', 'Eric Brachmann', 'Bertram Drost', 'Vincent Lepetit', 'Carsten Rother', 'Stan Birchfield', 'Jiri Matas', 'Yann Labbe', 'Martin Sundermeyer', 'Tomas Hodan'], 'affiliations': ['CTU Prague', 'ENPC', 'Google', 'Heidelberg University', 'KAIST', 'MVTec', 'Meta', 'NAVER LABS', 'NVIDIA', 'Niantic', 'TU Dortmund', 'TU Munich', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2504.02812.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Прорыв в компьютерном зрении: новые горизонты оценки 6D позы объектов', 'desc': 'Статья представляет результаты соревнования BOP Challenge 2024 по оценке 6D позы объектов. Введены новые задачи без использования 3D моделей и более практичные сценарии обнаружения объектов. Представлены новые наборы данных BOP-H3, записанные с помощью высокоточных сенсоров и AR/VR гарнитур. Лучшие методы 2024 года показали значительное улучшение точности по сравнению с методами 2023 года для различных задач, включая локализацию и обнаружение объектов.'}, 'en': {'title': 'Advancing 6D Object Pose Estimation in Real-World Scenarios', 'desc': 'The BOP Challenge 2024 focuses on advancing 6D object pose estimation by transitioning from controlled lab environments to real-world applications. This year, new model-free tasks were introduced, requiring methods to learn from reference videos without 3D models. The challenge also featured a practical 6D object detection task where object identities were not provided, alongside the release of the BOP-H3 datasets that simulate real-world conditions. Notably, the top-performing methods demonstrated significant improvements in accuracy and speed compared to previous years, highlighting the ongoing evolution in this field of machine learning.'}, 'zh': {'title': 'BOP挑战赛：从实验室到真实世界的6D物体姿态估计', 'desc': '本文介绍了2024年BOP挑战赛的评估方法、数据集和结果，这是一个旨在捕捉6D物体姿态估计最新技术的公开竞赛。2024年的目标是将BOP从实验室环境转向真实世界场景，推出了新的无模型任务，要求方法仅通过参考视频进行物体识别。我们还定义了一个更实用的6D物体检测任务，测试图像中物体的身份不再作为输入提供。此外，BOP-H3数据集使用高分辨率传感器和AR/VR头显录制，支持模型基础和无模型任务。'}}}, {'id': 'https://huggingface.co/papers/2504.03770', 'title': 'JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model', 'url': 'https://huggingface.co/papers/2504.03770', 'abstract': 'Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.', 'score': 2, 'issue_id': 3117, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '01c2f4c752f3e71d', 'authors': ['Yi Nian', 'Shenzhe Zhu', 'Yuehan Qin', 'Li Li', 'Ziyi Wang', 'Chaowei Xiao', 'Yue Zhao'], 'affiliations': ['University of Maryland', 'University of Southern California', 'University of Toronto', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.03770.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#security', '#dataset', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': "Адаптивная защита MLLM от атак 'jailbreak' без доступа к вредоносным данным", 'desc': "В статье представлен метод JAILDAM для обнаружения атак типа 'jailbreak' на мультимодальные большие языковые модели (MLLM). JAILDAM использует подход на основе памяти, управляемый представлениями небезопасных знаний, что устраняет необходимость в явном воздействии на вредоносные данные. Метод динамически обновляет небезопасные знания во время тестирования, улучшая обобщение на новые стратегии атак при сохранении эффективности. Эксперименты показывают, что JAILDAM обеспечивает современную производительность в обнаружении вредоносного контента, улучшая как точность, так и скорость."}, 'en': {'title': 'JAILDAM: Enhancing Safety in MLLMs Against Jailbreak Attacks', 'desc': 'This paper discusses the challenges of detecting jailbreak attacks in multimodal large language models (MLLMs), which can generate harmful content. Jailbreak attacks manipulate models to bypass safety features, making detection crucial for responsible use. The authors present JAILDAM, a novel framework that adapts during testing to identify these attacks without needing extensive harmful datasets. By using a memory-based approach, JAILDAM enhances detection efficiency and accuracy against various jailbreak strategies.'}, 'zh': {'title': '提升多模态模型安全性的关键', 'desc': '多模态大型语言模型（MLLMs）在视觉语言任务中表现出色，但也存在生成有害内容的重大风险，尤其是通过越狱攻击。越狱攻击是指故意操控以绕过模型的安全机制，导致生成不当或不安全的内容。检测此类攻击对于确保MLLMs的负责任部署至关重要。我们提出了一种名为JAILDAM的测试时自适应框架，通过动态更新不安全知识来提高对未见越狱策略的泛化能力，同时保持高效性。'}}}, {'id': 'https://huggingface.co/papers/2504.03947', 'title': 'Distillation and Refinement of Reasoning in Small Language Models for\n  Document Re-ranking', 'url': 'https://huggingface.co/papers/2504.03947', 'abstract': 'We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems.', 'score': 1, 'issue_id': 3134, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '32b0bcdc9035f276', 'authors': ['Chris Samarinas', 'Hamed Zamani'], 'affiliations': ['University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2504.03947.jpg', 'data': {'categories': ['#interpretability', '#optimization', '#rl', '#benchmark', '#small_models', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Маленькие модели, большие рассуждения: новый подход к ранжированию документов', 'desc': 'Авторы представляют новый подход к обучению небольших языковых моделей для ранжирования документов, требующего интенсивных рассуждений. Метод объединяет дистилляцию знаний с оптимизацией обучения с подкреплением, используя веб-данные и учительскую LLM для автоматической генерации высококачественных обучающих примеров с объяснениями релевантности. Компактная языковая модель с 3 миллиардами параметров достигает лучших результатов на бенчмарке BRIGHT, превосходя модели в 20 раз больше. Генерация объяснений во время вывода, а не прямое предсказание оценок релевантности, позволяет более эффективно рассуждать с помощью меньших языковых моделей.'}, 'en': {'title': 'Smaller Models, Smarter Reasoning!', 'desc': 'This paper introduces a new method for training smaller language models to improve document ranking through a combination of knowledge distillation and reinforcement learning. Instead of relying on costly human annotations, the approach uses web data and a teacher language model to create high-quality training examples that include relevance explanations. By treating document ranking as a reinforcement learning task, the authors successfully train a compact 3B parameter model that performs exceptionally well on the BRIGHT benchmark, even outperforming much larger models. The findings suggest that generating explanations during the inference process enhances reasoning capabilities in smaller models, making the method both scalable and interpretable for information retrieval applications.'}, 'zh': {'title': '小型语言模型的推理能力提升', 'desc': '我们提出了一种新颖的方法，用于训练小型语言模型以进行推理密集型文档排名，结合了知识蒸馏和强化学习优化。与现有方法依赖昂贵的人类标注或大型黑箱语言模型不同，我们的方法利用网络数据和教师语言模型自动生成高质量的训练示例及相关解释。通过将文档排名视为强化学习问题，并激励明确的推理能力，我们训练了一个紧凑的3B参数语言模型，在BRIGHT基准测试中达到了最先进的性能。我们的模型在排行榜上排名第三，同时使用的参数数量远低于其他方法，超越了那些参数超过20倍的大型模型。'}}}, {'id': 'https://huggingface.co/papers/2504.04155', 'title': 'GlotEval: A Test Suite for Massively Multilingual Evaluation of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2504.04155', 'abstract': "Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.", 'score': 0, 'issue_id': 3124, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 апреля', 'en': 'April 5', 'zh': '4月5日'}, 'hash': '8105e367a29eaa88', 'authors': ['Hengyu Luo', 'Zihao Li', 'Joseph Attieh', 'Sawal Devkota', 'Ona de Gibert', 'Shaoxiong Ji', 'Peiqin Lin', 'Bhavani Sai Praneeth Varma Mantina', 'Ananda Sreenidhi', 'Raúl Vázquez', 'Mengjie Wang', 'Samea Yusofi', 'Jörg Tiedemann'], 'affiliations': ['Technical University of Darmstadt, Germany', 'University of Helsinki, Finland', 'University of Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.04155.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#benchmark', '#machine_translation', '#open_source'], 'emoji': '🌐', 'ru': {'title': 'GlotEval: Многоязычная оценка языковых моделей без англоцентризма', 'desc': 'GlotEval - это новая система оценки больших языковых моделей (LLM), разработанная для многоязычной среды. Она поддерживает семь ключевых задач, включая машинный перевод, классификацию текста и генерацию текста, охватывая сотни языков. GlotEval использует специфичные для каждого языка шаблоны промптов и нацелена на оценку, не сфокусированную исключительно на английском языке. Эта система позволяет точно диагностировать сильные и слабые стороны моделей в различных языковых контекстах.'}, 'en': {'title': 'GlotEval: Bridging the Evaluation Gap for Multilingual LLMs', 'desc': 'This paper presents GlotEval, a new framework aimed at evaluating large language models (LLMs) across multiple languages, particularly focusing on low-resource languages. Current evaluation methods are biased towards English and a few high-resource languages, which limits our understanding of LLM performance in diverse linguistic settings. GlotEval supports various tasks such as machine translation and text classification, allowing for comprehensive benchmarking across dozens of languages. By providing language-specific templates and non-English-centric evaluations, GlotEval helps identify the strengths and weaknesses of LLMs in multilingual contexts.'}, 'zh': {'title': '多语言评估的新框架：GlotEval', 'desc': '大型语言模型（LLMs）在全球范围内迅速发展，各地区越来越多地采用这些模型进行本国语言的应用。评估这些模型在不同语言环境中的表现，尤其是在资源匮乏的语言中，已成为学术界和工业界的一大挑战。现有的评估框架主要集中在英语和少数高资源语言上，忽视了LLMs在多语言和低资源场景中的实际表现。为了解决这一问题，我们推出了GlotEval，这是一个轻量级框架，旨在进行大规模多语言评估，支持七项关键任务，帮助准确诊断模型在不同语言环境中的优缺点。'}}}, {'id': 'https://huggingface.co/papers/2504.04152', 'title': 'Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting\n  LLMs Across Languages and Resources', 'url': 'https://huggingface.co/papers/2504.04152', 'abstract': 'Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies.', 'score': 0, 'issue_id': 3124, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 апреля', 'en': 'April 5', 'zh': '4月5日'}, 'hash': '3c22dafb05a10aab', 'authors': ['Zihao Li', 'Shaoxiong Ji', 'Hengyu Luo', 'Jörg Tiedemann'], 'affiliations': ['Technical University of Darmstadt', 'University of Helsinki'], 'pdf_title_img': 'assets/pdf/title_img/2504.04152.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#low_resource', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Улучшение многоязычности LLM: сложности и компромиссы непрерывного предобучения', 'desc': 'Исследование посвящено проблеме неравномерной эффективности больших языковых моделей (LLM) для разных языков. Авторы оценивают различные стратегии непрерывного предобучения (CPT) для улучшения многоязычности моделей. Результаты показывают, что двуязычное CPT улучшает классификацию, но вызывает проблемы смешивания языков при генерации. Включение программного кода в данные для CPT повышает точность многоязычной классификации, особенно для малоресурсных языков. Исследование выявляет сложные взаимодействия между языками при многоязычном обучении представлений.'}, 'en': {'title': 'Bridging the Gap: Enhancing Low-Resource Languages with Continual Pretraining', 'desc': 'This paper investigates how Large Language Models (LLMs) perform differently across various languages, especially highlighting the advantages for high-resource languages and the challenges faced by low-resource ones. It explores the effectiveness of Continual Pretraining (CPT) using different data strategies, including monolingual, bilingual, and code-augmented data. The study evaluates 36 configurations across multiple languages and reveals that bilingual CPT can enhance classification but may lead to language mixing, while code data improves accuracy for low-resource languages at the cost of generation quality. The findings also challenge previous assumptions about language classifications, showing that altruistic languages can harm related ones, and stagnant languages can adapt under certain conditions, highlighting the complexity of multilingual learning.'}, 'zh': {'title': '解决语言不平衡的持续预训练策略', 'desc': '大型语言模型（LLMs）在不同语言上的表现差异显著，主要使高资源语言受益，而边缘化了低资源语言。持续预训练（CPT）被认为是解决这一不平衡的有效方法，但单语、双语和代码增强数据策略的相对有效性尚不明确。研究评估了36种CPT配置，涵盖三种多语言基础模型，涉及30多种语言，揭示了多语言表示学习的复杂性。研究结果表明，双语CPT提高了多语言分类，但在生成时常导致语言混合问题，而编程代码数据的加入则提高了低资源语言的分类准确性，但略微降低了生成质量。'}}}, {'id': 'https://huggingface.co/papers/2504.03790', 'title': "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models", 'url': 'https://huggingface.co/papers/2504.03790', 'abstract': 'Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.', 'score': 0, 'issue_id': 3128, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': 'bbc79821089111ea', 'authors': ['Gonçalo Faria', 'Noah A. Smith'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.03790.jpg', 'data': {'categories': ['#alignment', '#math', '#training', '#optimization', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'QAlign: Улучшение языковых моделей без переобучения', 'desc': 'Статья представляет новый подход к выравниванию языковых моделей во время тестирования, называемый QAlign. Этот метод позволяет улучшить качество выходных данных без изменения базовой модели или доступа к логитам. QAlign использует достижения в области цепей Маркова Монте-Карло для генерации текста и сходится к оптимальному выровненному распределению для каждого отдельного запроса. Эффективность QAlign продемонстрирована на различных наборах данных, включая задачи математических рассуждений и общие языковые задачи.'}, 'en': {'title': 'QAlign: Optimizing Language Model Outputs at Test Time', 'desc': 'This paper presents QAlign, a novel approach for enhancing language model performance during test time by optimizing the alignment of outputs without requiring model retraining. Traditional methods using reward models often suffer from quality degradation as computational resources increase, due to reliance on imperfect reward proxies. QAlign leverages recent advancements in Markov chain Monte Carlo techniques to sample from the optimal distribution for each prompt, leading to better-aligned text generation. The method shows significant improvements over existing test-time strategies on various benchmarks, demonstrating its effectiveness in maximizing the utility of pre-trained language models.'}, 'zh': {'title': 'QAlign：提升语言模型性能的新方法', 'desc': '本文提出了一种新的测试时间对齐方法QAlign，旨在提高语言模型的性能，尤其是在模型微调不可行的情况下。QAlign通过在测试时间计算中采样最优对齐分布，克服了现有奖励模型在计算规模扩大时质量下降的问题。该方法利用马尔可夫链蒙特卡洛技术生成文本，能够在不修改基础模型的情况下，生成更好对齐的输出。实验结果表明，QAlign在多个数学推理基准上表现优于现有的测试时间计算方法，展示了其在实际应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.10481', 'title': 'xVerify: Efficient Answer Verifier for Reasoning Model Evaluations', 'url': 'https://huggingface.co/papers/2504.10481', 'abstract': 'With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify.', 'score': 62, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '72678fc1ff453072', 'authors': ['Ding Chen', 'Qingchen Yu', 'Pengyuan Wang', 'Wentao Zhang', 'Bo Tang', 'Feiyu Xiong', 'Xinchi Li', 'Minchuan Yang', 'Zhiyu Li'], 'affiliations': ['Center for Data Science, Peking University', 'MemTensor (Shanghai) Technology Co., Ltd.', 'Research Institute of China Telecom, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.10481.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#dataset', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'xVerify: точная верификация ответов моделей рассуждения', 'desc': 'Статья представляет xVerify - эффективный верификатор ответов для оценки моделей рассуждения. xVerify способен определять эквивалентность ответов, генерируемых моделями рассуждения, эталонным ответам для различных типов объективных вопросов. Для обучения и оценки xVerify был создан набор данных VAR, содержащий пары вопросов-ответов от нескольких языковых моделей. Эксперименты показали, что модели xVerify достигают F1-меры и точности выше 95% на тестовом и обобщающем наборах данных.'}, 'en': {'title': 'xVerify: Elevating Reasoning Model Evaluation with Precision', 'desc': 'This paper introduces xVerify, a novel answer verification tool designed to evaluate reasoning models that utilize slow thinking strategies. Traditional evaluation methods struggle with complex outputs from large language models (LLMs), particularly in assessing the equivalence of answers and extracting final responses. xVerify addresses these challenges by leveraging a specially constructed VAR dataset, which includes diverse question-answer pairs generated by various LLMs. The results show that xVerify models achieve high accuracy and F1 scores, demonstrating their effectiveness in evaluating reasoning models compared to existing methods.'}, 'zh': {'title': 'xVerify：推理模型评估的新标准', 'desc': '随着OpenAI发布o1模型，采用慢思维策略的推理模型逐渐出现。这些模型生成的响应通常包含复杂的推理、中间步骤和自我反思，现有的评估方法往往无法有效判断LLM输出是否真正等同于参考答案。为了解决这个问题，我们提出了xVerify，一个高效的答案验证器，用于推理模型的评估。xVerify在等价判断方面表现出色，能够有效判断推理模型生成的答案是否与参考答案等价，并在多个客观问题类型中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2504.08672', 'title': 'Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning', 'url': 'https://huggingface.co/papers/2504.08672', 'abstract': 'Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https://github.com/xufangzhi/Genius.', 'score': 41, 'issue_id': 3264, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': '1de57b79eb1b4e84', 'authors': ['Fangzhi Xu', 'Hang Yan', 'Chang Ma', 'Haiteng Zhao', 'Qiushi Sun', 'Kanzhi Cheng', 'Junxian He', 'Jun Liu', 'Zhiyong Wu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Peking University', 'Shanghai AI Lab', 'The University of Hong Kong', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.08672.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствование ИИ: рассуждение без учителя', 'desc': 'Эта статья представляет новый метод под названием Genius для улучшения навыков рассуждения языковых моделей без использования внешнего надзора. Метод основан на самообучении и использует стратегию пошагового предвидения для исследования потенциальных шагов рассуждения. Для решения проблемы шума и неопределенности в неконтролируемой среде авторы предлагают функцию потерь с калибровкой преимущества. Genius открывает новые возможности для масштабирования способностей языковых моделей к рассуждению, используя только общие запросы без дополнительной разметки.'}, 'en': {'title': 'Genius: Unsupervised Self-Training for Enhanced LLM Reasoning', 'desc': 'This paper presents Genius, a novel unsupervised self-training framework designed to enhance the reasoning skills of large language models (LLMs) without relying on external supervisory signals. Genius optimizes LLMs by seeking the best response sequences through a stepwise approach, utilizing a foresight re-sampling strategy to simulate and evaluate potential future outcomes. To address the challenges of noise and uncertainty in unsupervised settings, the authors introduce an advantage-calibrated optimization (ACO) loss function that improves the robustness of the optimization process. Overall, Genius aims to advance LLM reasoning capabilities efficiently, leveraging the abundance of general queries available.'}, 'zh': {'title': '无监督自我提升LLM推理能力的革命性进展', 'desc': '本文提出了一种名为Genius的无监督自我训练框架，旨在提升大型语言模型（LLM）的推理能力，而无需依赖外部监督信号。Genius通过逐步寻找最佳响应序列来优化LLM，并引入了一种逐步前瞻重采样策略，以模拟未来结果并评估步骤价值。为了应对无监督设置中固有的噪声和不确定性，本文还提出了一种优势校准优化（ACO）损失函数，以减轻估计不一致性。通过结合这些技术，Genius为无监督条件下的LLM推理自我提升提供了一个先进的初步步骤，推动了推理扩展法则的变革。'}}}, {'id': 'https://huggingface.co/papers/2504.10337', 'title': 'Heimdall: test-time scaling on the generative verification', 'url': 'https://huggingface.co/papers/2504.10337', 'abstract': 'An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.', 'score': 28, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '6da5db970a101d21', 'authors': ['Wenlei Shi', 'Xing Jin'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2504.10337.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#long_context', '#optimization', '#training', '#rl', '#math'], 'emoji': '🔍', 'ru': {'title': 'Heimdall: ИИ-верификатор для повышения надежности рассуждений языковых моделей', 'desc': 'Статья представляет Heimdall - модель верификации для длинных цепочек рассуждений (Chain-of-Thought). С помощью обучения с подкреплением точность верификации была повышена с 62.5% до 94.5% на сложных математических задачах. Предложен метод пессимистической верификации для масштабирования решения задач. Heimdall демонстрирует высокую способность к обобщению, обнаруживая ошибки даже в сложных математических доказательствах.'}, 'en': {'title': 'Heimdall: Elevating AI Verification with Chain-of-Thought Reasoning', 'desc': 'This paper introduces Heimdall, a long Chain-of-Thought (CoT) verification model designed to enhance the accuracy of solution verification in AI systems. By employing pure reinforcement learning, Heimdall significantly improves verification accuracy on competitive math problems from 62.5% to 94.5%, and with repeated sampling, it reaches 97.5%. The paper also presents Pessimistic Verification, which optimizes solution selection by minimizing uncertainty, leading to improved accuracy in problem-solving tasks. Additionally, Heimdall is part of an automatic knowledge discovery system that identifies flaws in datasets, revealing that a substantial portion of the data is incorrect, which is consistent with findings from previous studies.'}, 'zh': {'title': '提升AI知识验证能力的Heimdall模型', 'desc': '本文提出了一种名为Heimdall的长链思维验证大语言模型（LLM），旨在提高解决竞争性数学问题的解答准确性。通过纯强化学习，Heimdall的验证准确率从62.5%提升至94.5%，并通过重复采样进一步提高至97.5%。此外，Heimdall还展示了出色的泛化能力，能够检测出训练中未包含的复杂数学证明中的大多数问题。我们还提出了悲观验证方法，以扩展Heimdall的功能，显著提高了解决方案的准确性。'}}}, {'id': 'https://huggingface.co/papers/2504.11346', 'title': 'Seedream 3.0 Technical Report', 'url': 'https://huggingface.co/papers/2504.11346', 'abstract': 'We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.', 'score': 25, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '3a4b797a3a9516d2', 'authors': ['Yu Gao', 'Lixue Gong', 'Qiushan Guo', 'Xiaoxia Hou', 'Zhichao Lai', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Chao Liao', 'Liyang Liu', 'Wei Liu', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Rui Wang', 'Xuanda Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Xin Xia', 'Xuefeng Xiao', 'Zhonghua Zhai', 'Xinyu Zhang', 'Qi Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Jianchao Yang', 'Weilin Huang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.11346.jpg', 'data': {'categories': ['#alignment', '#data', '#optimization', '#dataset', '#multimodal', '#architecture', '#training'], 'emoji': '🎨', 'ru': {'title': 'Революция в генерации изображений: Seedream 3.0 поднимает планку', 'desc': 'Seedream 3.0 - это двуязычная модель генерации изображений, улучшающая предыдущую версию. Модель использует новые техники обработки данных, включая обучение на смешанных разрешениях и выравнивание представлений. Применяются усовершенствованные методы постобработки, такие как эстетические подписи и модель вознаграждения на основе VLM. Seedream 3.0 также вводит новую парадигму ускорения, позволяющую увеличить скорость в 4-8 раз без потери качества.'}, 'en': {'title': 'Revolutionizing Bilingual Image Generation with Seedream 3.0', 'desc': 'Seedream 3.0 is an advanced bilingual image generation model designed for Chinese and English. It improves upon its predecessor by enhancing prompt alignment, typography generation, and overall image quality through a series of technical upgrades. Key innovations include a larger dataset, mixed-resolution training, and a novel acceleration method that speeds up processing while preserving image fidelity. The model excels in generating high-resolution images and accurately rendering complex Chinese characters, making it valuable for professional typography applications.'}, 'zh': {'title': 'Seedream 3.0：高效的中英文图像生成新纪元', 'desc': 'Seedream 3.0 是一个高性能的中英文双语图像生成基础模型。它通过多项技术改进，解决了 Seedream 2.0 中存在的挑战，如复杂提示的对齐、细致的排版生成和图像分辨率限制。该模型在数据构建和模型部署的整个流程中进行了改进，采用了缺陷感知训练和双轴协作数据采样等方法。Seedream 3.0 还引入了新的加速范式，实现了图像质量与生成速度的显著提升，特别是在复杂汉字的文本渲染方面表现出色。'}}}, {'id': 'https://huggingface.co/papers/2504.10766', 'title': 'How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients', 'url': 'https://huggingface.co/papers/2504.10766', 'abstract': "As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.", 'score': 25, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'b95bec819bad5307', 'authors': ['Ming Li', 'Yanhong Li', 'Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['University of Chicago', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.10766.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#data', '#training'], 'emoji': '🧠', 'ru': {'title': 'Спектральный анализ раскрывает секреты качества данных в обучении языковых моделей', 'desc': 'Эта статья представляет спектральный анализ послойных градиентов, вызванных данными разного качества при дообучении больших языковых моделей (LLM). Исследование показывает, что метрики оценки данных, такие как IFD, InsTag, Difficulty и Reward, можно объяснить и объединить с помощью спектральных свойств, вычисленных из сингулярного разложения градиентов. Авторы обнаружили, что данные более высокого качества обычно связаны с более низкими ядерными нормами и более высокими эффективными рангами. Эксперименты также показывают, что модели одного семейства имеют схожие паттерны градиентов независимо от их размера, в то время как разные семейства моделей значительно различаются.'}, 'en': {'title': 'Unlocking the Secrets of Data Quality in LLM Fine-Tuning', 'desc': 'This paper investigates how the quality of data influences the fine-tuning of large language models (LLMs) during post-training, particularly for complex reasoning tasks. It employs spectral analysis of layer-wise gradients to understand the effects of low and high-quality instruction and reasoning data. The study finds that traditional metrics for data evaluation can be unified through the spectral properties derived from the singular value decomposition (SVD) of gradients. Notably, it shows that higher-quality data leads to lower nuclear norms and higher effective ranks, with effective rank being a more reliable measure for capturing quality differences, especially in reasoning tasks.'}, 'zh': {'title': '数据质量与训练稳定性的统一视角', 'desc': '本文探讨了大语言模型（LLM）在后训练阶段中，不同数据对微调动态的影响。我们通过对低质量和高质量指令及推理数据的层级梯度进行谱分析，发现常用的数据评估指标可以通过梯度的奇异值分解（SVD）谱特性来解释和统一。研究表明，高质量数据通常与较低的核范数和较高的有效秩相关，且有效秩在捕捉细微质量差异方面表现出更好的鲁棒性和分辨率。我们的实验还表明，同一家族的模型在梯度模式上相似，而不同模型家族之间则存在显著差异。'}}}, {'id': 'https://huggingface.co/papers/2504.10465', 'title': 'Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding', 'url': 'https://huggingface.co/papers/2504.10465', 'abstract': "Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA.", 'score': 22, 'issue_id': 3260, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '6c90d31f3f941694', 'authors': ['Tao Zhang', 'Xiangtai Li', 'Zilong Huang', 'Yanwei Li', 'Weixian Lei', 'Xueqing Deng', 'Shihao Chen', 'Shunping Ji', 'Jiashi Feng'], 'affiliations': ['Bytedance Seed', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2504.10465.jpg', 'data': {'categories': ['#survey', '#optimization', '#benchmark', '#multimodal', '#training', '#architecture', '#games'], 'emoji': '🔍', 'ru': {'title': 'Единый трансформер для точного анализа изображений', 'desc': 'Статья представляет Pixel-SAIL - единую трансформерную модель для задач мультимодального машинного обучения на уровне пикселей. Авторы предлагают три ключевых улучшения: модуль повышающей дискретизации, стратегию внедрения визуальных подсказок и дистилляцию экспертных знаний. Модель показывает сопоставимые или лучшие результаты по сравнению с более сложными системами на нескольких эталонных наборах данных. Исследователи также представляют новый набор данных PerBench для комплексной оценки понимания изображений на уровне пикселей.'}, 'en': {'title': 'Simplifying Multimodal Learning with Pixel-SAIL', 'desc': "This paper introduces Pixel-SAIL, a simplified Multimodal Large Language Model (MLLM) designed for pixel-level understanding tasks without relying on additional components like vision encoders. The authors propose three key innovations: a learnable upsampling module for refining visual features, a visual prompt injection strategy for better integration of visual and text inputs, and a vision expert distillation method to enhance feature extraction. By focusing on a single transformer architecture, Pixel-SAIL aims to reduce system complexity while maintaining high performance. The paper also presents a new benchmark, PerBench, to evaluate the model's effectiveness across various pixel understanding tasks."}, 'zh': {'title': '简化的多模态语言模型：Pixel-SAIL', 'desc': '多模态大型语言模型（MLLMs）在细粒度像素级理解任务中表现出色，但大多数工作依赖于额外的组件，如视觉编码器和分割专家，导致系统复杂性高，限制了模型的扩展性。本文提出了一种简化的MLLM，名为Pixel-SAIL，旨在不引入额外组件的情况下进行像素级任务。我们通过设计可学习的上采样模块、视觉提示注入策略和视觉专家蒸馏策略，提升了单一变换器的特征提取能力。实验结果表明，Pixel-SAIL在多个基准测试中表现出色，且具有更简单的处理流程。'}}}, {'id': 'https://huggingface.co/papers/2504.11442', 'title': 'TextArena', 'url': 'https://huggingface.co/papers/2504.11442', 'abstract': 'TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.', 'score': 20, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '7c9ae6533757828a', 'authors': ['Leon Guertler', 'Bobby Cheng', 'Simon Yu', 'Bo Liu', 'Leshem Choshen', 'Cheston Tan'], 'affiliations': ['Centre for Frontier AI Research (CFAR), A*STAR Institute of High Performance Computing, A*STAR', 'MIT, MIT-IBM Watson AI Lab', 'National University of Singapore', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11442.jpg', 'data': {'categories': ['#agents', '#games', '#benchmark', '#open_source'], 'emoji': '🎮', 'ru': {'title': 'TextArena: Арена для оттачивания социального интеллекта языковых моделей', 'desc': 'TextArena - это открытый набор соревновательных текстовых игр для обучения и оценки агентного поведения больших языковых моделей (LLM). Он включает более 57 уникальных сред, позволяющих оценивать возможности моделей через систему онлайн-игры с рейтингом TrueSkill в реальном времени. TextArena восполняет пробел в оценке динамических социальных навыков, таких как ведение переговоров, теория разума и обман. Платформа разработана с учетом исследовательских целей, возможностей сообщества и расширяемости, облегчая добавление новых игр и тестирование моделей.'}, 'en': {'title': 'Train LLMs in Competitive Text Games!', 'desc': 'TextArena is a platform designed for training and evaluating Large Language Models (LLMs) through competitive text-based games. It features over 57 unique environments that support various gameplay modes, allowing models to interact with both human players and other models. This framework addresses the lack of benchmarks for assessing social skills like negotiation and deception, which are crucial for agentic behavior. TextArena is built for research and community engagement, making it easy to add new games and adapt the system for testing and training purposes.'}, 'zh': {'title': 'TextArena：提升语言模型的社交技能训练平台', 'desc': 'TextArena是一个开源的竞争性文本游戏集合，旨在训练和评估大型语言模型（LLMs）的代理行为。它包含57个以上独特的环境，支持单人、双人和多人设置，并通过在线游戏系统轻松评估模型能力。传统基准测试通常无法评估动态社交技能，如谈判、心智理论和欺骗，而TextArena正好填补了这一空白。该平台强调易于添加新游戏、适应框架、测试模型和训练模型，适合研究和社区使用。'}}}, {'id': 'https://huggingface.co/papers/2504.10462', 'title': 'The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer', 'url': 'https://huggingface.co/papers/2504.10462', 'abstract': "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL.", 'score': 12, 'issue_id': 3260, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '1f70a22447fd1fc5', 'authors': ['Weixian Lei', 'Jiacong Wang', 'Haochen Wang', 'Xiangtai Li', 'Jun Hao Liew', 'Jiashi Feng', 'Zilong Huang'], 'affiliations': ['Bytedance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2504.10462.jpg', 'data': {'categories': ['#multimodal', '#agi', '#architecture', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'SAIL: единая архитектура для мультимодального машинного обучения', 'desc': 'SAIL - это унифицированная мультимодальная большая языковая модель, объединяющая обработку изображений и текста в единой архитектуре трансформера. В отличие от модульных моделей, SAIL не использует предобученный vision transformer, а напрямую кодирует пиксели изображений. Модель адаптирует механизмы смешанного внимания и мультимодальные позиционные кодировки для лучшего согласования визуальной и текстовой модальностей. SAIL демонстрирует сопоставимую производительность с модульными моделями и сильные возможности визуального представления.'}, 'en': {'title': 'SAIL: A Unified Approach to Multimodal Learning', 'desc': "This paper presents SAIL, a unified multimodal large language model that combines image and text processing in one architecture without needing a separate vision encoder. SAIL uses mix-attention mechanisms and multimodal positional encodings to effectively handle both visual and textual data. The study shows that SAIL can scale well with increased training data and model size, achieving performance similar to existing modular models. Additionally, SAIL's design leads to unique patterns in how information flows between modalities, while also excelling in visual tasks like semantic segmentation."}, 'zh': {'title': 'SAIL：简约架构下的多模态语言模型', 'desc': '本文介绍了SAIL，这是一种单一变换器统一多模态大语言模型（MLLM），它在一个架构中整合了原始像素编码和语言解码。与现有的模块化MLLM不同，SAIL不需要单独的视觉编码器，呈现出更简约的架构设计。SAIL采用混合注意力机制和多模态位置编码，以更好地适应视觉和文本模态的独特特征。通过扩大训练数据和模型规模，SAIL在性能上与模块化MLLM相当，同时在视觉表示能力上也表现出色。'}}}, {'id': 'https://huggingface.co/papers/2504.10903', 'title': 'Efficient Reasoning Models: A Survey', 'url': 'https://huggingface.co/papers/2504.10903', 'abstract': 'Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this "slow-thinking" paradigm, with numerous tokens generated in sequence, inevitably introduces substantial computational overhead. To this end, it highlights an urgent need for effective acceleration. This survey aims to provide a comprehensive overview of recent advances in efficient reasoning. It categorizes existing works into three key directions: (1) shorter - compressing lengthy CoTs into concise yet effective reasoning chains; (2) smaller - developing compact language models with strong reasoning capabilities through techniques such as knowledge distillation, other model compression techniques, and reinforcement learning; and (3) faster - designing efficient decoding strategies to accelerate inference. A curated collection of papers discussed in this survey is available in our GitHub repository.', 'score': 10, 'issue_id': 3269, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '8a7af29eb70394e2', 'authors': ['Sicheng Feng', 'Gongfan Fang', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['Nankai University, Tianjin, China', 'National University of Singapore, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.10903.jpg', 'data': {'categories': ['#training', '#inference', '#reasoning', '#survey', '#optimization', '#data', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация рассуждений: короче, меньше, быстрее', 'desc': 'Эта статья представляет обзор последних достижений в области эффективных рассуждений для моделей машинного обучения. Авторы рассматривают три основных направления: сокращение цепочек рассуждений, разработка компактных языковых моделей и ускорение процесса вывода. В работе обсуждаются методы сжатия длинных цепочек рассуждений, техники уменьшения размера моделей и стратегии эффективного декодирования. Статья предоставляет комплексный анализ существующих подходов к оптимизации рассуждений в современных языковых моделях.'}, 'en': {'title': 'Accelerating Reasoning: Shorter, Smaller, and Faster!', 'desc': 'This paper discusses the advancements in reasoning models that generate Chain-of-Thoughts (CoTs) to solve complex tasks. However, the traditional approach can be slow and computationally expensive due to the lengthy sequences of tokens. The authors propose three strategies to enhance efficiency: creating shorter CoTs, developing smaller models with strong reasoning abilities, and implementing faster decoding methods. The survey also includes a collection of relevant research papers to support these advancements.'}, 'zh': {'title': '高效推理的未来：加速思维链', 'desc': '推理模型在解决复杂和逻辑密集型任务方面取得了显著进展，通过生成扩展的思维链（CoTs）来得出最终答案。然而，这种“慢思考”范式的出现，导致了大量序列生成的计算开销。为此，迫切需要有效的加速方法。本文综述了高效推理的最新进展，并将现有工作分为三个主要方向：压缩长思维链、开发紧凑的语言模型以及设计高效的解码策略。'}}}, {'id': 'https://huggingface.co/papers/2504.10559', 'title': 'Efficient Process Reward Model Training via Active Learning', 'url': 'https://huggingface.co/papers/2504.10559', 'abstract': "Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.", 'score': 10, 'issue_id': 3259, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'f718e5da41cde633', 'authors': ['Keyu Duan', 'Zichen Liu', 'Xin Mao', 'Tianyu Pang', 'Changyu Chen', 'Qiguang Chen', 'Michael Qizhe Shieh', 'Longxu Dou'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'Singapore Management University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10559.jpg', 'data': {'categories': ['#data', '#reasoning', '#optimization', '#benchmark', '#math', '#training'], 'emoji': '🎯', 'ru': {'title': 'Активное обучение для эффективных моделей вознаграждения процессов', 'desc': 'Статья представляет ActPRM - подход активного обучения для моделей вознаграждения процессов (PRM). ActPRM выбирает наиболее неопределенные образцы для обучения, что значительно снижает затраты на разметку данных. Метод использует PRM для оценки неопределенности после прямого прохода, сохраняя только высоко неопределенные данные. Эксперименты показывают, что ActPRM сокращает аннотацию на 50% при сохранении или улучшении производительности по сравнению с обычной тонкой настройкой.'}, 'en': {'title': 'Efficient Learning with Uncertainty: ActPRM for Enhanced Model Training', 'desc': 'This paper introduces ActPRM, an active learning method designed to enhance Process Reward Models (PRMs) for training large language models (LLMs). By focusing on the most uncertain samples, ActPRM significantly cuts down the costs associated with data labeling while maintaining or improving model performance. The approach involves using the PRM to estimate uncertainty and selectively retaining only the most ambiguous data for labeling by a more complex reasoning model. The results show that ActPRM not only reduces annotation requirements by 50% but also achieves state-of-the-art performance on benchmark tasks.'}, 'zh': {'title': '主动学习提升模型训练效率', 'desc': '本文提出了一种主动学习方法ActPRM，用于提高大语言模型（LLMs）的训练效率。通过选择最不确定的样本进行训练，ActPRM显著降低了标注成本。训练过程中，使用过程奖励模型（PRM）来估计不确定性，仅保留高度不确定的数据进行标注。实验结果表明，ActPRM在减少50%标注的同时，性能与传统微调方法相当甚至更好。'}}}, {'id': 'https://huggingface.co/papers/2504.11427', 'title': 'NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors', 'url': 'https://huggingface.co/papers/2504.11427', 'abstract': 'Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing methods with temporal components, we present NormalCrafter to leverage the inherent temporal priors of video diffusion models. To secure high-fidelity normal estimation across sequences, we propose Semantic Feature Regularization (SFR), which aligns diffusion features with semantic cues, encouraging the model to concentrate on the intrinsic semantics of the scene. Moreover, we introduce a two-stage training protocol that leverages both latent and pixel space learning to preserve spatial accuracy while maintaining long temporal context. Extensive evaluations demonstrate the efficacy of our method, showcasing a superior performance in generating temporally consistent normal sequences with intricate details from diverse videos.', 'score': 8, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'f1c298d14d78b468', 'authors': ['Yanrui Bin', 'Wenbo Hu', 'Haoyuan Wang', 'Xinya Chen', 'Bing Wang'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong', 'Huazhong University of Science and Technology', 'Spatial Intelligence Group, The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11427.jpg', 'data': {'categories': ['#cv', '#long_context', '#diffusion', '#video', '#training'], 'emoji': '🎥', 'ru': {'title': 'NormalCrafter: Временная согласованность нормалей в видео с помощью диффузионных моделей', 'desc': 'Эта статья представляет NormalCrafter - новый метод для оценки поверхностных нормалей в видео с использованием диффузионных моделей. Авторы предлагают технику семантической регуляризации признаков (SFR) для улучшения согласованности оценок во времени. Также описывается двухэтапный протокол обучения, сочетающий обучение в латентном и пиксельном пространствах. Эксперименты показывают превосходство метода в генерации темпорально согласованных последовательностей нормалей с сохранением деталей.'}, 'en': {'title': 'NormalCrafter: Enhancing Video Normal Estimation with Semantic Insights', 'desc': "This paper introduces NormalCrafter, a novel approach for estimating surface normals in video sequences. It addresses the challenge of maintaining temporal coherence, which is often overlooked in traditional static image methods. The authors propose Semantic Feature Regularization (SFR) to enhance the model's focus on the scene's semantics, improving the quality of normal estimation. Additionally, a two-stage training protocol is implemented to balance spatial accuracy and long-term temporal context, resulting in high-fidelity normal sequences across various videos."}, 'zh': {'title': '视频法线估计的新方法：NormalCrafter', 'desc': '本论文提出了一种新的方法NormalCrafter，用于视频中的法线估计。与传统方法不同，我们利用视频扩散模型的时间先验，确保法线估计在时间上的一致性。我们引入了语义特征正则化（SFR），通过对齐扩散特征和语义线索，帮助模型关注场景的内在语义。通过两阶段的训练协议，我们在保持空间精度的同时，增强了对长时间上下文的学习，实验结果表明该方法在生成细节丰富且时间一致的法线序列方面表现优越。'}}}, {'id': 'https://huggingface.co/papers/2504.11001', 'title': 'ReZero: Enhancing LLM search ability by trying one-more-time', 'url': 'https://huggingface.co/papers/2504.11001', 'abstract': 'Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM) performance on knowledge-intensive tasks but depends heavily on initial search query quality. Current methods, often using Reinforcement Learning (RL), typically focus on query formulation or reasoning over results, without explicitly encouraging persistence after a failed search. We introduce ReZero (Retry-Zero), a novel RL framework that directly rewards the act of retrying a search query following an initial unsuccessful attempt. This incentivizes the LLM to explore alternative queries rather than prematurely halting. ReZero demonstrates significant improvement, achieving 46.88% accuracy compared to a 25% baseline. By rewarding persistence, ReZero enhances LLM robustness in complex information-seeking scenarios where initial queries may prove insufficient.', 'score': 7, 'issue_id': 3263, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '9e93b5032c2a9d9c', 'authors': ['Alan Dao', 'Thinh Le'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.11001.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Настойчивость окупается: ReZero повышает эффективность поиска информации с помощью LLM', 'desc': 'В статье представлен новый подход ReZero для улучшения поиска информации с помощью больших языковых моделей (LLM). Метод использует обучение с подкреплением, чтобы поощрять модель делать повторные попытки поиска после неудачного запроса. Это позволяет LLM исследовать альтернативные формулировки запросов, вместо того чтобы преждевременно останавливаться. ReZero показывает значительное улучшение точности с 25% до 46.88% по сравнению с базовым методом.'}, 'en': {'title': 'Persistence Pays Off: Enhancing LLMs with ReZero', 'desc': 'This paper presents ReZero, a new reinforcement learning framework designed to improve the performance of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs). Unlike traditional methods that focus on refining search queries or analyzing results, ReZero encourages LLMs to persist and retry after an unsuccessful search. By rewarding the act of retrying, it promotes exploration of alternative queries, leading to better outcomes. The results show a significant accuracy increase, demonstrating that persistence can enhance LLM effectiveness in challenging knowledge-intensive tasks.'}, 'zh': {'title': '重试搜索，提升模型鲁棒性', 'desc': 'Retrieval-Augmented Generation（RAG）通过增强大型语言模型（LLM）在知识密集型任务上的表现，但其效果依赖于初始搜索查询的质量。现有方法通常使用强化学习（RL），主要关注查询的制定或结果的推理，而没有明确鼓励在搜索失败后继续尝试。我们提出了ReZero（Retry-Zero），一种新的强化学习框架，直接奖励在初次搜索失败后重试查询的行为。这种方法显著提高了LLM的鲁棒性，在复杂的信息检索场景中表现出更好的准确性。'}}}, {'id': 'https://huggingface.co/papers/2504.10342', 'title': 'VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge', 'url': 'https://huggingface.co/papers/2504.10342', 'abstract': 'Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with "thinking" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.', 'score': 7, 'issue_id': 3269, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '86516767518e49b1', 'authors': ['Yueqi Song', 'Tianyue Ou', 'Yibo Kong', 'Zecheng Li', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10342.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'VisualPuzzles: новый взгляд на оценку визуального мышления ИИ', 'desc': 'В статье представлен новый бенчмарк VisualPuzzles для оценки визуального рассуждения мультимодальных моделей машинного обучения. В отличие от существующих тестов, VisualPuzzles минимизирует зависимость от специализированных знаний, фокусируясь на общих навыках рассуждения. Эксперименты показали, что современные мультимодальные языковые модели значительно отстают от людей в решении таких задач. Исследование также выявило, что улучшение вычислительных возможностей и увеличение размера модели не всегда приводят к повышению производительности в задачах рассуждения.'}, 'en': {'title': 'VisualPuzzles: Evaluating Pure Reasoning in Multimodal AI', 'desc': 'The paper introduces VisualPuzzles, a new benchmark designed to assess visual reasoning without relying heavily on specialized knowledge. It includes a variety of reasoning types such as algorithmic, analogical, deductive, inductive, and spatial reasoning. The authors demonstrate that existing multimodal large language models struggle with VisualPuzzles, highlighting that success in knowledge-heavy tasks does not guarantee performance in reasoning-focused scenarios. This benchmark aims to provide a clearer evaluation of reasoning abilities, emphasizing the importance of reasoning over mere factual recall.'}, 'zh': {'title': 'VisualPuzzles：评估推理能力的新基准', 'desc': '当前的多模态基准测试常常将推理与特定领域知识混淆，这使得在非专业环境中评估一般推理能力变得困难。为了解决这个问题，我们引入了VisualPuzzles，这是一个专注于视觉推理的基准，故意减少对专业知识的依赖。VisualPuzzles包含五类多样化的问题：算法推理、类比推理、演绎推理、归纳推理和空间推理。实验表明，VisualPuzzles需要的领域特定知识显著减少，同时推理的复杂性更高，从而更好地评估真正的多模态推理能力。'}}}, {'id': 'https://huggingface.co/papers/2504.10277', 'title': 'RealHarm: A Collection of Real-World Language Model Application Failures', 'url': 'https://huggingface.co/papers/2504.10277', 'abstract': "Language model deployments in consumer-facing applications introduce numerous risks. While existing research on harms and hazards of such applications follows top-down approaches derived from regulatory frameworks and theoretical analyses, empirical evidence of real-world failure modes remains underexplored. In this work, we introduce RealHarm, a dataset of annotated problematic interactions with AI agents built from a systematic review of publicly reported incidents. Analyzing harms, causes, and hazards specifically from the deployer's perspective, we find that reputational damage constitutes the predominant organizational harm, while misinformation emerges as the most common hazard category. We empirically evaluate state-of-the-art guardrails and content moderation systems to probe whether such systems would have prevented the incidents, revealing a significant gap in the protection of AI applications.", 'score': 7, 'issue_id': 3274, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '996eec7ad28b00d4', 'authors': ['Pierre Le Jeune', 'Jiaen Liu', 'Luca Rossi', 'Matteo Dora'], 'affiliations': ['Giskard AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.10277.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#ethics', '#data', '#security'], 'emoji': '🛡️', 'ru': {'title': 'RealHarm: реальные угрозы ИИ-систем под микроскопом', 'desc': 'Статья представляет датасет RealHarm, содержащий аннотированные проблемные взаимодействия с ИИ-агентами, основанный на анализе публично сообщенных инцидентов. Исследование фокусируется на рисках и опасностях с точки зрения компаний, внедряющих языковые модели. Авторы обнаружили, что репутационный ущерб является преобладающим организационным вредом, а дезинформация - наиболее распространенной категорией опасности. Эмпирическая оценка современных систем защиты и модерации контента выявила значительные пробелы в защите ИИ-приложений.'}, 'en': {'title': 'Understanding Real-World Risks of AI Deployments', 'desc': 'This paper addresses the risks associated with deploying machine learning models in consumer applications, focusing on real-world failures. It introduces RealHarm, a dataset that captures problematic interactions with AI agents based on a review of reported incidents. The study highlights that reputational damage is the main harm organizations face, while misinformation is the most frequent hazard. Additionally, it evaluates existing safety measures and finds that current guardrails and content moderation systems are insufficient to prevent these incidents.'}, 'zh': {'title': '揭示AI应用中的潜在风险与危害', 'desc': '本文探讨了消费者应用中语言模型部署所带来的风险。我们引入了RealHarm数据集，记录了与AI代理的有问题互动，基于对公开报告事件的系统性审查。研究发现，声誉损害是主要的组织性危害，而错误信息则是最常见的危险类别。通过评估现有的防护措施和内容审核系统，我们发现这些系统在保护AI应用方面存在显著的不足。'}}}, {'id': 'https://huggingface.co/papers/2504.10188', 'title': 'Efficient Generative Model Training via Embedded Representation Warmup', 'url': 'https://huggingface.co/papers/2504.10188', 'abstract': "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40times acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW.", 'score': 7, 'issue_id': 3261, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '280d2a5386c25fa2', 'authors': ['Deyuan Liu', 'Peng Sun', 'Xufeng Li', 'Tao Lin'], 'affiliations': ['Nanjing University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10188.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#architecture', '#training'], 'emoji': '🚀', 'ru': {'title': 'Ускорение обучения диффузионных моделей с помощью предобученных представлений', 'desc': 'Статья представляет новый метод обучения диффузионных моделей под названием Embedded Representation Warmup (ERW). ERW использует предобученные высококачественные представления для инициализации ранних слоев модели, что значительно ускоряет сходимость и улучшает качество генерации. Авторы идентифицируют ключевую область обработки представлений в нейронной сети, где происходит обучение семантическим и структурным паттернам. Теоретический и эмпирический анализ показывает, что ERW ускоряет обучение в 40 раз по сравнению с современными методами.'}, 'en': {'title': 'Accelerating Diffusion Models with Embedded Representation Warmup', 'desc': 'This paper addresses the limitations of diffusion models in generating high-dimensional data efficiently. It identifies that the slow training process is due to the underutilization of high-quality representations in the early layers of the model. The authors propose a method called Embedded Representation Warmup (ERW), which initializes these layers with pretrained representations to enhance learning speed and quality. Their results show that ERW significantly accelerates training convergence and improves representation quality, achieving a 40 times faster training speed compared to existing methods.'}, 'zh': {'title': '加速扩散模型训练的嵌入表示预热', 'desc': '扩散模型在生成高维数据方面表现出色，但在训练效率和表示质量上不如自监督方法。我们发现一个关键瓶颈：在训练过程中未充分利用高质量、语义丰富的表示，显著减缓了收敛速度。为了解决这个问题，我们提出了嵌入表示预热（ERW）框架，通过在初始阶段使用预训练的高质量表示来初始化扩散模型的早期层，从而加速收敛并提高性能。我们的理论分析表明，ERW的有效性依赖于其在特定神经网络层的精确集成，这些层主要处理和转换特征表示以便后续生成。'}}}, {'id': 'https://huggingface.co/papers/2504.11456', 'title': 'DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning', 'url': 'https://huggingface.co/papers/2504.11456', 'abstract': 'The capacity for complex mathematical reasoning is a key benchmark for artificial intelligence. While reinforcement learning (RL) applied to LLMs shows promise, progress is significantly hindered by the lack of large-scale training data that is sufficiently challenging, possesses verifiable answer formats suitable for RL, and is free from contamination with evaluation benchmarks. To address these limitations, we introduce DeepMath-103K, a new, large-scale dataset comprising approximately 103K mathematical problems, specifically designed to train advanced reasoning models via RL. DeepMath-103K is curated through a rigorous pipeline involving source analysis, stringent decontamination against numerous benchmarks, and filtering for high difficulty (primarily Levels 5-9), significantly exceeding existing open resources in challenge. Each problem includes a verifiable final answer, enabling rule-based RL, and three distinct R1-generated solutions suitable for diverse training paradigms like supervised fine-tuning or distillation. Spanning a wide range of mathematical topics, DeepMath-103K promotes the development of generalizable reasoning. We demonstrate that models trained on DeepMath-103K achieve significant improvements on challenging mathematical benchmarks, validating its effectiveness. We release DeepMath-103K publicly to facilitate community progress in building more capable AI reasoning systems: https://github.com/zwhe99/DeepMath.', 'score': 6, 'issue_id': 3266, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '3af8b373358fa97c', 'authors': ['Zhiwei He', 'Tian Liang', 'Jiahao Xu', 'Qiuzhi Liu', 'Xingyu Chen', 'Yue Wang', 'Linfeng Song', 'Dian Yu', 'Zhenwen Liang', 'Wenxuan Wang', 'Zhuosheng Zhang', 'Rui Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2504.11456.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#math', '#dataset', '#data', '#open_source', '#rl'], 'emoji': '🧮', 'ru': {'title': 'DeepMath-103K: Прорыв в обучении ИИ математическим рассуждениям', 'desc': 'DeepMath-103K - это новый крупномасштабный набор данных, содержащий около 103 тысяч математических задач, разработанный для обучения моделей продвинутым рассуждениям с помощью обучения с подкреплением. Набор данных отличается высокой сложностью задач, охватывает широкий спектр математических тем и включает проверяемые окончательные ответы. Каждая задача содержит три различных решения, сгенерированных моделью R1, что позволяет использовать различные парадигмы обучения. Модели, обученные на DeepMath-103K, демонстрируют значительные улучшения на сложных математических тестах.'}, 'en': {'title': 'Empowering AI with DeepMath-103K: A Leap in Mathematical Reasoning', 'desc': 'This paper presents DeepMath-103K, a new dataset designed to enhance the training of AI models in complex mathematical reasoning. It addresses the challenges faced by reinforcement learning (RL) in large language models (LLMs) due to the lack of high-quality, difficult training data. The dataset includes around 103,000 carefully curated mathematical problems with verifiable answers, allowing for effective rule-based RL training. By providing diverse solutions and covering a wide range of topics, DeepMath-103K aims to improve the generalization capabilities of AI reasoning systems.'}, 'zh': {'title': 'DeepMath-103K：推动AI数学推理的突破性数据集', 'desc': '本文介绍了DeepMath-103K，这是一个新的大规模数据集，包含约103,000个数学问题，旨在通过强化学习（RL）训练高级推理模型。该数据集经过严格的筛选和去污染处理，确保问题具有高难度，并且每个问题都提供可验证的最终答案，适合规则基础的RL。DeepMath-103K涵盖广泛的数学主题，促进了可推广推理的发展。实验表明，基于DeepMath-103K训练的模型在挑战性数学基准测试中取得了显著的改进，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.11393', 'title': 'DataDecide: How to Predict Best Pretraining Data with Small Experiments', 'url': 'https://huggingface.co/papers/2504.11393', 'abstract': 'Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.', 'score': 6, 'issue_id': 3275, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'c08bb6187070e5e2', 'authors': ['Ian Magnusson', 'Nguyen Tai', 'Ben Bogin', 'David Heineman', 'Jena D. Hwang', 'Luca Soldaini', 'Akshita Bhagia', 'Jiacheng Liu', 'Dirk Groeneveld', 'Oyvind Tafjord', 'Noah A. Smith', 'Pang Wei Koh', 'Jesse Dodge'], 'affiliations': ['Allen Institute for AI', 'University of Pennsylvania', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.11393.jpg', 'data': {'categories': ['#small_models', '#data', '#benchmark', '#optimization', '#training', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Эффективный выбор данных для обучения больших языковых моделей', 'desc': 'Исследование посвящено оптимизации выбора данных для предобучения больших языковых моделей. Авторы представляют DataDecide - набор моделей, данных и оценок для изучения влияния различных наборов данных на качество моделей разного масштаба. Эксперименты показали, что ранжирование небольших моделей (150M параметров) хорошо предсказывает лучшие модели большего масштаба (1B параметров). Использование метрик правдоподобия в небольших экспериментах позволяет предсказать производительность на ряде бенчмарков с точностью >80% при использовании всего 0.01% вычислительных ресурсов.'}, 'en': {'title': 'Optimize Dataset Selection for Large Language Models with DataDecide', 'desc': 'This paper addresses the challenge of efficiently selecting datasets for pretraining large language models by utilizing smaller-scale experiments. It introduces DataDecide, a comprehensive suite that allows researchers to evaluate the impact of different datasets and model sizes on performance. The authors conduct extensive experiments across various corpora and find that performance rankings at smaller model sizes can effectively predict outcomes at larger scales. Additionally, they demonstrate that using continuous likelihood metrics can significantly enhance the predictability of benchmarks with minimal computational resources.'}, 'zh': {'title': '小规模实验助力大模型数据选择', 'desc': '本论文探讨了如何通过小规模实验来选择数据集，以降低大型语言模型的预训练成本。我们发布了DataDecide，这是一个开放的模型、数据和评估套件，旨在帮助研究者探索最佳数据集选择。通过在25个不同来源的语料库上进行控制预训练实验，我们发现小规模模型的排名可以有效预测大规模模型的表现。使用连续似然度指标作为小规模实验的代理，可以在目标规模下实现超过80%的可预测性。'}}}, {'id': 'https://huggingface.co/papers/2504.11343', 'title': 'A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce', 'url': 'https://huggingface.co/papers/2504.11343', 'abstract': "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.", 'score': 6, 'issue_id': 3260, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '72714d765a5a497f', 'authors': ['Wei Xiong', 'Jiarui Yao', 'Yuhui Xu', 'Bo Pang', 'Lei Wang', 'Doyen Sahoo', 'Junnan Li', 'Nan Jiang', 'Tong Zhang', 'Caiming Xiong', 'Hanze Dong'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.11343.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Простота и эффективность в обучении языковых моделей с подкреплением', 'desc': 'Это исследование анализирует методы обучения с подкреплением (RL) для улучшения больших языковых моделей (LLM) в задачах рассуждения. Авторы обнаружили, что простой метод отбора положительных примеров RAFT показывает результаты, сравнимые с более сложными алгоритмами, такими как GRPO. На основе этого наблюдения предложен новый алгоритм Reinforce-Rej, который фильтрует как полностью неправильные, так и полностью правильные образцы. Исследование предлагает использовать RAFT как надежный базовый метод и рекомендует сосредоточиться на более обоснованном включении отрицательных примеров в будущих разработках.'}, 'en': {'title': 'Simplifying Reinforcement Learning for Better Language Model Training', 'desc': "This paper explores the effectiveness of the GRPO method in reinforcement learning for fine-tuning large language models on reasoning tasks. The authors discover that a simpler method, RAFT, which only uses positively rewarded samples, performs comparably to GRPO and PPO. They find that GRPO's strength lies in its ability to discard prompts with completely incorrect responses, rather than its reward normalization technique. To enhance performance, they introduce Reinforce-Rej, which filters out both incorrect and correct samples, improving efficiency and stability in training."}, 'zh': {'title': '强化学习的新视角：拒绝采样的力量', 'desc': '强化学习（RL）在复杂推理任务中对大型语言模型（LLM）的微调中变得越来越重要。本文重新审视了GRPO算法，发现一个简单的拒绝采样基线RAFT在训练中表现出色，甚至与GRPO和PPO相当。研究表明，GRPO的主要优势在于丢弃完全错误的提示，而不是其奖励归一化。基于这一发现，我们提出了Reinforce-Rej，这是一种过滤完全错误和完全正确样本的策略梯度扩展，能够提高KL效率和稳定性。'}}}, {'id': 'https://huggingface.co/papers/2504.11455', 'title': 'SimpleAR: Pushing the Frontier of Autoregressive Visual Generation\n  through Pretraining, SFT, and RL', 'url': 'https://huggingface.co/papers/2504.11455', 'abstract': 'This work presents SimpleAR, a vanilla autoregressive visual generation framework without complex architecure modifications. Through careful exploration of training and inference optimization, we demonstrate that: 1) with only 0.5B parameters, our model can generate 1024x1024 resolution images with high fidelity, and achieve competitive results on challenging text-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) training could lead to significant improvements on generation aesthectics and prompt alignment; and 3) when optimized with inference acceleraton techniques like vLLM, the time for SimpleAR to generate an 1024x1024 image could be reduced to around 14 seconds. By sharing these findings and open-sourcing the code, we hope to reveal the potential of autoregressive visual generation and encourage more participation in this research field. Code is available at https://github.com/wdrink/SimpleAR.', 'score': 4, 'issue_id': 3269, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'cec2567a7d0af48a', 'authors': ['Junke Wang', 'Zhi Tian', 'Xun Wang', 'Xinyu Zhang', 'Weilin Huang', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['ByteDance', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11455.jpg', 'data': {'categories': ['#training', '#inference', '#open_source', '#benchmark', '#optimization', '#small_models', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Простота и эффективность в генерации изображений', 'desc': 'SimpleAR - это автореггрессивная модель для генерации изображений, использующая простую архитектуру без сложных модификаций. Несмотря на небольшой размер (0.5 млрд параметров), модель способна генерировать качественные изображения размером 1024x1024 и показывает конкурентоспособные результаты на сложных бенчмарках для преобразования текста в изображение. Авторы демонстрируют, что как супервизорная дообучение (SFT), так и оптимизация групповой относительной политики (GRPO) значительно улучшают эстетику генерации и соответствие промпту. С помощью техник ускорения вывода, таких как vLLM, время генерации изображения 1024x1024 сокращается до 14 секунд.'}, 'en': {'title': 'Unlocking High-Quality Image Generation with SimpleAR', 'desc': 'This paper introduces SimpleAR, a straightforward autoregressive framework for generating high-quality images without the need for complex architecture changes. The model, with only 0.5 billion parameters, can produce 1024x1024 resolution images and performs competitively on text-to-image benchmarks. It highlights the effectiveness of supervised fine-tuning and Group Relative Policy Optimization in enhancing image aesthetics and prompt alignment. Additionally, by employing inference acceleration techniques, the generation time for images is significantly reduced, showcasing the efficiency of the SimpleAR model.'}, 'zh': {'title': 'SimpleAR：高效自回归视觉生成的潜力', 'desc': '本文介绍了一种名为SimpleAR的自回归视觉生成框架，该框架没有复杂的架构修改。我们通过对训练和推理优化的深入探索，展示了该模型在仅有5亿参数的情况下，能够生成高保真度的1024x1024分辨率图像，并在具有挑战性的文本到图像基准测试中取得竞争性结果。我们发现，监督微调（SFT）和组相对策略优化（GRPO）训练都能显著提高生成图像的美学和提示对齐效果。此外，使用推理加速技术如vLLM后，SimpleAR生成1024x1024图像的时间可缩短至约14秒。'}}}, {'id': 'https://huggingface.co/papers/2504.11447', 'title': 'Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion', 'url': 'https://huggingface.co/papers/2504.11447', 'abstract': "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO.", 'score': 4, 'issue_id': 3258, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'cda8d39111df28d8', 'authors': ['An Zhaol', 'Shengyuan Zhang', 'Ling Yang', 'Zejian Li', 'Jiale Wu', 'Haoran Xu', 'AnYang Wei', 'Perry Pengyun GU Lingyun Sun'], 'affiliations': ['Peking University', 'Zhejiang Green Zhixing Technology co., ltd', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11447.jpg', 'data': {'categories': ['#open_source', '#3d', '#rlhf', '#training', '#diffusion', '#optimization'], 'emoji': '🚗', 'ru': {'title': 'Ускоренное и улучшенное заполнение сцен LiDAR с помощью Distillation-DPO', 'desc': 'Эта статья представляет новый метод под названием Distillation-DPO для ускорения и улучшения качества заполнения сцен LiDAR с использованием диффузионных моделей. Метод сочетает дистилляцию знаний с оптимизацией прямой политики (DPO), используя метрики оценки сцен LiDAR в качестве предпочтений. Distillation-DPO оптимизирует модель ученика, используя разницу в функциях оценки между учителем и учеником на парных сценах завершения. Эксперименты показывают, что метод достигает более высокого качества заполнения сцены при ускорении процесса более чем в 5 раз по сравнению с современными моделями.'}, 'en': {'title': 'Accelerating 3D LiDAR Scene Completion with Distillation-DPO', 'desc': 'This paper introduces Distillation-DPO, a new framework that enhances 3D LiDAR scene completion using diffusion models. It combines score distillation with direct policy optimization (DPO) to improve performance while speeding up the sampling process. The method generates paired scene completions with varying initial noises and uses LiDAR evaluation metrics to create winning and losing pairs for optimization. The results show that Distillation-DPO significantly outperforms existing models in both quality and speed, marking a novel approach in preference-aligned distillation for LiDAR applications.'}, 'zh': {'title': '提升LiDAR场景补全速度与质量的创新方法', 'desc': '本论文提出了一种新的扩散蒸馏框架，称为Distillation-DPO，用于3D LiDAR场景补全。该方法通过偏好对齐来优化学生模型，首先生成不同初始噪声的配对补全场景。然后，利用LiDAR场景评估指标构建胜负样本对，以此来优化模型。实验表明，Distillation-DPO在场景补全质量和速度上均优于现有的最先进模型，补全速度提高了5倍以上。'}}}, {'id': 'https://huggingface.co/papers/2504.11326', 'title': 'PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild', 'url': 'https://huggingface.co/papers/2504.11326', 'abstract': 'This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/.', 'score': 4, 'issue_id': 3260, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'abbfe1ceb2f9688a', 'authors': ['Henghui Ding', 'Chang Liu', 'Nikhila Ravi', 'Shuting He', 'Yunchao Wei', 'Song Bai', 'Philip Torr', 'Kehuan Song', 'Xinglin Xie', 'Kexin Zhang', 'Licheng Jiao', 'Lingling Li', 'Shuyuan Yang', 'Xuqiang Cao', 'Linnan Zhao', 'Jiaxuan Zhao', 'Fang Liu', 'Mengjiao Wang', 'Junpei Zhang', 'Xu Liu', 'Yuting Yang', 'Mengru Ma', 'Hao Fang', 'Runmin Cong', 'Xiankai Lu', 'Zhiyang Che', 'Wei Zhan', 'Tianming Liang', 'Haichao Jiang', 'Wei-Shi Zheng', 'Jian-Fang Hu', 'Haobo Yuan', 'Xiangtai Li', 'Tao Zhang', 'Lu Qi', 'Ming-Hsuan Yang'], 'affiliations': ['the Institute of Big Data, Fudan University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.11326.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'Прорыв в сегментации видео: новые горизонты понимания сложных сцен', 'desc': 'Статья описывает результаты 4-го конкурса PVUW по пониманию видео на уровне пикселей, проведенного в рамках CVPR 2025. Конкурс включал два трека: MOSE для сегментации объектов в сложных сценах и MeViS для сегментации на основе движения и языка. Были представлены новые, более сложные наборы данных, лучше отражающие реальные сценарии. Анализ результатов дает ценную информацию о современном состоянии и тенденциях в области сложной сегментации видео.'}, 'en': {'title': 'Advancing Video Segmentation: Insights from the PVUW Challenge', 'desc': 'This paper reviews the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, which took place at CVPR 2025. It highlights two main tracks: MOSE for complex scene video object segmentation and MeViS for motion-guided, language-based video segmentation. The challenge introduced new datasets that are more representative of real-world video scenarios, pushing the boundaries of current segmentation techniques. The findings provide insights into the latest advancements and future directions in the field of video segmentation.'}, 'zh': {'title': '推动复杂视频分割的前沿挑战', 'desc': '本报告全面概述了2025年CVPR会议期间举行的第四届像素级视频理解挑战赛（PVUW）。挑战赛包括两个赛道：MOSE专注于复杂场景的视频物体分割，而MeViS则针对基于运动引导和语言的视频分割。两个赛道都引入了新的、更具挑战性的数据集，以更好地反映现实世界的场景。通过详细的评估和分析，该挑战赛为复杂视频分割的最新技术状态和新兴趋势提供了宝贵的见解。'}}}, {'id': 'https://huggingface.co/papers/2504.08846', 'title': 'AI-University: An LLM-based platform for instructional alignment to\n  scientific classrooms', 'url': 'https://huggingface.co/papers/2504.08846', 'abstract': "We introduce AI University (AI-U), a flexible framework for AI-driven course content delivery that adapts to instructors' teaching styles. At its core, AI-U fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to generate instructor-aligned responses from lecture videos, notes, and textbooks. Using a graduate-level finite-element-method (FEM) course as a case study, we present a scalable pipeline to systematically construct training data, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and optimize its responses through RAG-based synthesis. Our evaluation - combining cosine similarity, LLM-based assessment, and expert review - demonstrates strong alignment with course materials. We also have developed a prototype web application, available at https://my-ai-university.com, that enhances traceability by linking AI-generated responses to specific sections of the relevant course material and time-stamped instances of the open-access video lectures. Our expert model is found to have greater cosine similarity with a reference on 86% of test cases. An LLM judge also found our expert model to outperform the base Llama 3.2 model approximately four times out of five. AI-U offers a scalable approach to AI-assisted education, paving the way for broader adoption in higher education. Here, our framework has been presented in the setting of a class on FEM - a subject that is central to training PhD and Master students in engineering science. However, this setting is a particular instance of a broader context: fine-tuning LLMs to research content in science.", 'score': 4, 'issue_id': 3265, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': '0d4f794fba95381f', 'authors': ['Mostafa Faghih Shojaei', 'Rahul Gulati', 'Benjamin A. Jasperson', 'Shangshang Wang', 'Simone Cimolato', 'Dangli Cao', 'Willie Neiswanger', 'Krishna Garikipati'], 'affiliations': ['Department of Aerospace and Mechanical Engineering, University of Southern California', 'Department of Astronautical Engineering, University of Southern California', 'Department of Computer Science, University of Southern California', 'Department of Electrical and Computer Engineering, University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2504.08846.jpg', 'data': {'categories': ['#low_resource', '#multimodal', '#training', '#open_source', '#science', '#rag'], 'emoji': '🎓', 'ru': {'title': 'AI-U: персонализированное обучение с помощью адаптивных языковых моделей', 'desc': 'Статья представляет AI University (AI-U) - гибкую систему для адаптивной подачи учебного контента с помощью искусственного интеллекта. В основе AI-U лежит большая языковая модель (LLM), дообученная с использованием метода retrieval-augmented generation (RAG) на основе видеолекций, заметок и учебников. Авторы разработали масштабируемый конвейер для систематического создания обучающих данных, дообучения open-source LLM с помощью Low-Rank Adaptation (LoRA) и оптимизации ответов через RAG-синтез. Оценка системы, включающая косинусное сходство, оценку на основе LLM и экспертный анализ, показала сильное соответствие учебным материалам.'}, 'en': {'title': 'AI-U: Tailoring AI Learning to Teaching Styles', 'desc': "AI University (AI-U) is a framework designed to enhance AI-driven course content delivery by adapting to different teaching styles. It utilizes a large language model (LLM) that is fine-tuned with retrieval-augmented generation (RAG) to create responses that align closely with lecture materials. The framework was tested using a finite-element-method (FEM) course, demonstrating effective training data construction and optimization of AI responses. Evaluation results show that AI-U's model significantly outperforms the base model, indicating its potential for scalable AI-assisted education in higher learning environments."}, 'zh': {'title': 'AI大学：灵活的AI辅助教育框架', 'desc': 'AI大学（AI-U）是一个灵活的框架，旨在根据教师的教学风格调整AI驱动的课程内容交付。它的核心是通过检索增强生成（RAG）技术对大型语言模型（LLM）进行微调，以生成与讲座视频、笔记和教科书一致的教师响应。以研究生级别的有限元方法（FEM）课程为案例，我们展示了一个可扩展的管道，系统地构建训练数据，并通过低秩适应（LoRA）微调开源LLM。我们的评估表明，AI-U在教育中提供了一种可扩展的方法，促进了高等教育中AI辅助学习的更广泛应用。'}}}, {'id': 'https://huggingface.co/papers/2504.11409', 'title': 'Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning', 'url': 'https://huggingface.co/papers/2504.11409', 'abstract': 'Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce a novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier.', 'score': 3, 'issue_id': 3275, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '0a29da0f494ed358', 'authors': ['Ali Taghibakhshi', 'Sharath Turuvekere Sreenivas', 'Saurav Muralidharan', 'Marcin Chochowski', 'Yashaswi Karnati', 'Raviraj Joshi', 'Ameya Sunil Mahabaleshwarkar', 'Zijia Chen', 'Yoshi Suhara', 'Oluwatobi Olabiyi', 'Daniel Korzekwa', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Jan Kautz', 'Bryan Catanzaro', 'Ashwath Aithal', 'Nima Tajbakhsh', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2504.11409.jpg', 'data': {'categories': ['#architecture', '#small_models', '#optimization', '#inference', '#training'], 'emoji': '✂️', 'ru': {'title': 'Эффективное сжатие гибридных нейросетей без потери качества', 'desc': 'В этой статье представлен новый подход к сжатию гибридных архитектур нейронных сетей, сочетающих механизмы внимания и модели пространства состояний (SSM). Авторы предлагают метод групповой обрезки, сохраняющий структурную целостность блоков SSM. Применяя комбинацию различных техник сжатия и дистилляции знаний, им удалось уменьшить модель Nemotron-H 8B до 4 миллиардов параметров, сохранив высокую точность. Полученная модель превосходит аналоги по точности и в 2 раза быстрее при выводе, что значительно улучшает соотношение производительности и размера модели.'}, 'en': {'title': 'Efficient Hybrid Models: Pruning for Performance and Accuracy', 'desc': 'This paper discusses a new method for improving Hybrid LLM architectures that use both Attention and State Space Models (SSMs). The authors propose a group-aware pruning strategy that maintains the effectiveness of SSMs while reducing the model size. They show that this pruning, combined with knowledge distillation, leads to smaller models that are not only more accurate but also faster in making predictions. The results indicate that their approach can significantly enhance model performance while using fewer training resources.'}, 'zh': {'title': '压缩混合模型，提升性能与效率', 'desc': '本研究探讨了混合大语言模型（LLM）架构的压缩效果，这些架构结合了注意力机制和状态空间模型（SSM）。我们提出了一种新颖的群体感知剪枝策略，能够保持SSM模块的结构完整性和序列建模能力。通过这种压缩方法，我们将Nemotron-H 8B混合模型压缩到4B参数，并减少了训练所需的标记数量。最终得到的模型在准确性和推理速度上均优于同等规模的模型，显著推动了模型性能的边界。'}}}, {'id': 'https://huggingface.co/papers/2504.09454', 'title': 'D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation', 'url': 'https://huggingface.co/papers/2504.09454', 'abstract': 'Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce a novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D^2iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves a unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at https://github.com/jiawn-creator/Dynamic-DiT.', 'score': 3, 'issue_id': 3265, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': '4e07861591445707', 'authors': ['Weinan Jia', 'Mengqi Huang', 'Nan Chen', 'Lei Zhang', 'Zhendong Mao'], 'affiliations': ['Institute of Artificial intelligence, Hefei Comprehensive National Science Center, Hefei, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.09454.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#open_source', '#cv', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Динамическое сжатие и диффузия для улучшенной генерации изображений', 'desc': 'Статья представляет новый двухэтапный подход к генерации изображений с использованием диффузионных моделей. Первый этап, Dynamic VAE, применяет иерархический энкодер для сжатия различных участков изображения с разной степенью в зависимости от их информационной плотности. Второй этап, Dynamic Diffusion Transformer (D^2iT), генерирует изображения, предсказывая шум разной зернистости для разных областей. Этот метод позволяет достичь баланса между глобальной согласованностью и локальным реализмом в генерируемых изображениях.'}, 'en': {'title': 'Dynamic Compression for Enhanced Image Generation', 'desc': 'This paper introduces a new approach to improve image generation using diffusion models by dynamically compressing different regions of an image based on their information density. The proposed two-stage framework consists of a Dynamic VAE that encodes image regions at varying downsampling rates, allowing for more accurate latent representations. The second stage, Dynamic Diffusion Transformer (D^2iT), generates images by predicting noise at multiple granularities, balancing the need for detail in complex areas and simplicity in smoother regions. This method enhances both local realism and global consistency in generated images, as demonstrated through extensive experiments.'}, 'zh': {'title': '动态压缩，提升图像生成质量', 'desc': '扩散模型因其生成高质量图像的能力而受到广泛认可。尽管扩散变换器（DiT）架构表现出色，但在扩散过程中对不同图像区域应用固定压缩，忽视了这些区域信息密度的自然变化。我们提出通过动态压缩不同图像区域来解决这一问题，采用两阶段框架来提高图像生成的有效性和效率。第一阶段的动态变分自编码器（DVAE）和第二阶段的动态扩散变换器（D^2iT）结合了粗细噪声预测，成功实现了全局一致性与局部真实感的统一。'}}}, {'id': 'https://huggingface.co/papers/2504.06949', 'title': 'Adaptive Computation Pruning for the Forgetting Transformer', 'url': 'https://huggingface.co/papers/2504.06949', 'abstract': 'The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox.', 'score': 3, 'issue_id': 3259, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'bda352daa194f6f8', 'authors': ['Zhixuan Lin', 'Johan Obando-Ceron', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2504.06949.jpg', 'data': {'categories': ['#architecture', '#inference', '#training', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка вычислений: быстрее, но не хуже', 'desc': 'Статья представляет Adaptive Computation Pruning (ACP) - метод динамической обрезки вычислений для модели Forgetting Transformer (FoX). ACP использует порог обрезки, чтобы исключить незначительные веса внимания, что позволяет сократить количество операций с плавающей запятой на 70% без потери производительности. Это приводит к увеличению скорости обучения на 10-35%, особенно для длинных контекстов. Анализ показывает эффективность метода для различных размеров моделей и длин контекста.'}, 'en': {'title': 'Boosting Efficiency with Adaptive Computation Pruning in FoX', 'desc': "The paper introduces the Forgetting Transformer (FoX), which enhances softmax attention by integrating a forget gate, leading to improved performance over traditional RoPE-based Transformers. It observes that many attention heads in FoX forget information quickly, focusing more on local context. To address this, the authors propose Adaptive Computation Pruning (ACP), which dynamically reduces unnecessary computations based on the forget gate's influence. This method significantly decreases the number of floating-point operations (FLOPs) during language model pretraining, improving training speed by 10% to 35% without sacrificing model performance."}, 'zh': {'title': '自适应计算剪枝提升FoX效率', 'desc': '最近提出的遗忘变换器（FoX）在软最大注意力中引入了遗忘门，与标准的RoPE变换器相比，表现出更好的性能。FoX中的许多注意力头快速遗忘，使得它们在每个时间步的输出主要依赖于局部上下文。基于这一观察，我们提出了自适应计算剪枝（ACP）方法，动态剪除与输入输出依赖关系强烈衰减的计算。通过在FoX的语言模型预训练中应用ACP，我们实现了约70%的计算量减少，同时训练吞吐量提高了10%到35%。'}}}, {'id': 'https://huggingface.co/papers/2504.11042', 'title': 'LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews', 'url': 'https://huggingface.co/papers/2504.11042', 'abstract': "Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LazyReview, a dataset of peer-review sentences annotated with fine-grained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community. (Code available here: https://github.com/UKPLab/arxiv2025-lazy-review)", 'score': 2, 'issue_id': 3268, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'd40c91fff861e80f', 'authors': ['Sukannya Purkayastha', 'Zhuang Li', 'Anne Lauscher', 'Lizhen Qu', 'Iryna Gurevych'], 'affiliations': ['Data Science Group, University of Hamburg', 'Department of Data Science & AI, Monash University, Australia', 'School of Computing Technologies, Royal Melbourne Institute of Technology, Australia', 'Ubiquitous Knowledge Processing Lab, Department of Computer Science and Hessian Center for AI (hessian.AI), Technical University of Darmstadt'], 'pdf_title_img': 'assets/pdf/title_img/2504.11042.jpg', 'data': {'categories': ['#open_source', '#data', '#dataset', '#science', '#training'], 'emoji': '🧠', 'ru': {'title': 'Борьба с ленивым мышлением в рецензировании', 'desc': 'В статье рассматривается проблема "ленивого мышления" в процессе рецензирования научных работ, что снижает качество рецензий. Авторы представляют новый датасет LazyReview, содержащий предложения рецензий с аннотациями по категориям ленивого мышления. Исследование показывает, что LLM плохо справляются с обнаружением таких случаев без предварительной настройки, но обучение на новом датасете значительно улучшает результаты. Также экспериментально доказано, что рецензии, исправленные с учётом ленивого мышления, становятся более полными и полезными.'}, 'en': {'title': 'Enhancing Peer Review Quality with LazyReview Dataset', 'desc': 'This paper addresses the problem of lazy thinking in peer review, which can degrade the quality of scientific evaluations. It introduces LazyReview, a new dataset containing peer-review sentences annotated with specific categories of lazy thinking. The study shows that Large Language Models (LLMs) have difficulty identifying these lazy thinking instances without prior training. However, by fine-tuning LLMs on the LazyReview dataset, their performance improves significantly, demonstrating the value of high-quality annotated data for enhancing peer review processes.'}, 'zh': {'title': '提升同行评审质量，打击懒惰思维！', 'desc': '同行评审是科学出版质量控制的重要环节。随着工作量的增加，评审中出现了使用快速启发式方法的现象，这被称为懒惰思维，影响了评审质量。本文介绍了LazyReview数据集，该数据集包含标注了细粒度懒惰思维类别的同行评审句子。我们的研究表明，使用大型语言模型在零样本设置下难以检测这些实例，但在我们的数据集上进行基于指令的微调可以显著提高性能。'}}}, {'id': 'https://huggingface.co/papers/2504.10443', 'title': 'Multimodal Long Video Modeling Based on Temporal Dynamic Context', 'url': 'https://huggingface.co/papers/2504.10443', 'abstract': 'Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent methods are designed for long video understanding, they often lose crucial information during token compression and struggle with additional modality like audio. In this work, we propose a dynamic long video encoding method utilizing the temporal relationship between frames, named Temporal Dynamic Context (TDC). Firstly, we segment the video into semantically consistent scenes based on inter-frame similarities, then encode each frame into tokens using visual-audio encoders. Secondly, we propose a novel temporal context compressor to reduce the number of tokens within each segment. Specifically, we employ a query-based Transformer to aggregate video, audio, and instruction text tokens into a limited set of temporal context tokens. Finally, we feed the static frame tokens and the temporal context tokens into the LLM for video understanding. Furthermore, to handle extremely long videos, we propose a training-free chain-of-thought strategy that progressively extracts answers from multiple video segments. These intermediate answers serve as part of the reasoning process and contribute to the final answer. We conduct extensive experiments on general video understanding and audio-video understanding benchmarks, where our method demonstrates strong performance. The code and models are available at https://github.com/Hoar012/TDC-Video.', 'score': 2, 'issue_id': 3268, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'a6b528cbce95d1ef', 'authors': ['Haoran Hao', 'Jiaming Han', 'Yiyuan Zhang', 'Xiangyu Yue'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10443.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#multimodal', '#video', '#benchmark'], 'emoji': '🎞️', 'ru': {'title': 'Динамическое кодирование для эффективного понимания длинных видео', 'desc': 'Статья представляет новый метод для понимания длинных видео с использованием больших языковых моделей (LLM). Авторы предлагают динамическое кодирование видео, названное Temporal Dynamic Context (TDC), которое учитывает временные отношения между кадрами. Метод включает сегментацию видео, кодирование кадров с помощью визуально-аудио энкодеров и сжатие контекста с использованием трансформера на основе запросов. Для экстремально длинных видео предлагается стратегия цепочки рассуждений, которая постепенно извлекает ответы из нескольких сегментов видео.'}, 'en': {'title': 'Enhancing Long Video Understanding with Temporal Dynamic Context', 'desc': 'This paper introduces a new method called Temporal Dynamic Context (TDC) for improving the understanding of long videos using Large Language Models (LLMs). The approach segments videos into meaningful scenes and encodes each frame with visual and audio information, addressing the limitations of existing models that struggle with long contexts. A novel temporal context compressor is used to reduce the number of tokens while preserving essential information, allowing for better integration of video, audio, and text data. Additionally, a training-free chain-of-thought strategy is proposed to extract answers progressively from different video segments, enhancing the reasoning process for video understanding tasks.'}, 'zh': {'title': '动态长视频编码，提升视频理解能力', 'desc': '本研究提出了一种动态长视频编码方法，称为时间动态上下文（TDC），旨在解决现有大语言模型在处理长视频时面临的挑战。我们通过基于帧间相似性将视频分割为语义一致的场景，并使用视觉-音频编码器将每帧编码为标记。接着，我们引入了一种新颖的时间上下文压缩器，利用基于查询的Transformer将视频、音频和指令文本标记聚合为有限的时间上下文标记。最后，我们将静态帧标记和时间上下文标记输入到大语言模型中，以实现视频理解。'}}}, {'id': 'https://huggingface.co/papers/2504.10049', 'title': 'Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure', 'url': 'https://huggingface.co/papers/2504.10049', 'abstract': 'Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of automatic summarization of multimodal presentations using VLMs with various representations as input. From these experiments, we suggest cost-effective strategies for generating summaries from text-heavy multimodal documents under different input-length budgets using VLMs. We show that slides extracted from the video stream can be beneficially used as input against the raw video, and that a structured representation from interleaved slides and transcript provides the best performance. Finally, we reflect and comment on the nature of cross-modal interactions in multimodal presentations and share suggestions to improve the capabilities of VLMs to understand documents of this nature.', 'score': 2, 'issue_id': 3264, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '49c72fe8f6e82ce0', 'authors': ['Théo Gigant', 'Camille Guinaudeau', 'Frédéric Dufaux'], 'affiliations': ['Université Paris-Saclay, CNRS, CentraleSupelec, Laboratoire des signaux et systemes, France', 'Université Paris-Saclay, CNRS, LISN, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.10049.jpg', 'data': {'categories': ['#optimization', '#video', '#interpretability', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Оптимизация суммирования мультимодальных презентаций с помощью VLM', 'desc': 'Статья посвящена анализу использования моделей машинного обучения, работающих с визуальной и текстовой информацией (VLM), для автоматического суммирования мультимодальных презентаций. Авторы проводят детальный анализ различных стратегий ввода данных в VLM для генерации резюме. Исследование показывает, что использование слайдов, извлеченных из видеопотока, может быть более эффективным, чем использование сырого видео. Наилучшие результаты достигаются при использовании структурированного представления, сочетающего слайды и транскрипт.'}, 'en': {'title': 'Enhancing Summarization of Multimodal Presentations with VLMs', 'desc': 'This paper explores how Vision-Language Models (VLMs) can effectively summarize multimodal presentations that include text and images. The authors analyze different input formats and their impact on summarization quality, revealing that using slides from videos can enhance performance compared to raw video input. They propose strategies for optimizing summary generation based on varying input lengths, emphasizing the importance of structured representations. Additionally, the paper discusses the interactions between visual and textual data in these presentations and offers insights for improving VLM capabilities.'}, 'zh': {'title': '提升多模态文档摘要的智能策略', 'desc': '本文研究了视觉-语言模型（VLMs）在处理多模态信息（如文本、图像和视频）时的自动摘要能力。我们进行了细致的定量和定性分析，探讨了如何在不同输入长度预算下，从文本密集的多模态文档中生成摘要。实验结果表明，从视频流中提取的幻灯片作为输入比原始视频更有效，而交错的幻灯片和转录文本的结构化表示则提供了最佳性能。最后，我们讨论了多模态演示中的跨模态交互特性，并提出了改进VLMs理解此类文档能力的建议。'}}}, {'id': 'https://huggingface.co/papers/2504.11457', 'title': 'Aligning Generative Denoising with Discriminative Objectives Unleashes\n  Diffusion for Visual Perception', 'url': 'https://huggingface.co/papers/2504.11457', 'abstract': 'With the success of image generation, generative diffusion models are increasingly adopted for discriminative tasks, as pixel generation provides a unified perception interface. However, directly repurposing the generative denoising process for discriminative objectives reveals critical gaps rarely addressed previously. Generative models tolerate intermediate sampling errors if the final distribution remains plausible, but discriminative tasks require rigorous accuracy throughout, as evidenced in challenging multi-modal tasks like referring image segmentation. Motivated by this gap, we analyze and enhance alignment between generative diffusion processes and perception tasks, focusing on how perception quality evolves during denoising. We find: (1) earlier denoising steps contribute disproportionately to perception quality, prompting us to propose tailored learning objectives reflecting varying timestep contributions; (2) later denoising steps show unexpected perception degradation, highlighting sensitivity to training-denoising distribution shifts, addressed by our diffusion-tailored data augmentation; and (3) generative processes uniquely enable interactivity, serving as controllable user interfaces adaptable to correctional prompts in multi-round interactions. Our insights significantly improve diffusion-based perception models without architectural changes, achieving state-of-the-art performance on depth estimation, referring image segmentation, and generalist perception tasks. Code available at https://github.com/ziqipang/ADDP.', 'score': 0, 'issue_id': 3272, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '3aac006cdbd11ccd', 'authors': ['Ziqi Pang', 'Xin Xu', 'Yu-Xiong Wang'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.11457.jpg', 'data': {'categories': ['#training', '#multimodal', '#alignment', '#cv', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Оптимизация диффузионных моделей для точного восприятия', 'desc': 'Статья посвящена улучшению применения генеративных диффузионных моделей для задач восприятия. Авторы анализируют процесс шумоподавления и предлагают адаптированные целевые функции обучения, учитывающие вклад разных временных шагов. Они также вводят специальную аугментацию данных для решения проблемы расхождения распределений при обучении и шумоподавлении. Исследователи демонстрируют, как генеративные модели могут служить интерактивным пользовательским интерфейсом, адаптируемым к корректирующим подсказкам.'}, 'en': {'title': 'Bridging Generative and Discriminative Learning for Enhanced Perception', 'desc': 'This paper explores how generative diffusion models, which are typically used for creating images, can be adapted for tasks that require precise classification and segmentation. The authors identify that while generative models can handle some errors during the image creation process, discriminative tasks need consistent accuracy at every step. They propose new learning objectives that focus on the importance of early denoising steps and address issues that arise in later steps due to shifts in training data. Their findings lead to significant improvements in performance for various perception tasks without changing the underlying model architecture.'}, 'zh': {'title': '提升生成模型在判别任务中的感知能力', 'desc': '这篇论文探讨了生成扩散模型在判别任务中的应用，尤其是在图像生成成功后。研究发现，直接将生成去噪过程用于判别目标时存在重要的差距，尤其是在多模态任务中。论文分析了生成扩散过程与感知任务之间的对齐，并提出了针对不同时间步贡献的学习目标。通过改进数据增强和学习策略，显著提升了基于扩散的感知模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.11080', 'title': 'Change State Space Models for Remote Sensing Change Detection', 'url': 'https://huggingface.co/papers/2504.11080', 'abstract': 'Despite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Vision Mamba, an architecture based on State Space Models has emerged as an alternative addressing the aforementioned deficiencies and has been already applied to remote sensing change detection, though mostly as a feature extracting backbone. In this article the Change State Space Model is introduced, that has been specifically designed for change detection by focusing on the relevant changes between bi-temporal images, effectively filtering out irrelevant information. By concentrating solely on the changed features, the number of network parameters is reduced, enhancing significantly computational efficiency while maintaining high detection performance and robustness against input degradation. The proposed model has been evaluated via three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based counterparts at a fraction of their computational complexity. The implementation will be made available at https://github.com/Elman295/CSSM upon acceptance.', 'score': 0, 'issue_id': 3268, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'abf55fd00ae45494', 'authors': ['Elman Ghazaei', 'Erchan Aptoula'], 'affiliations': ['Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, Türkiye'], 'pdf_title_img': 'assets/pdf/title_img/2504.11080.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Эффективное обнаружение изменений с помощью State Space Models', 'desc': 'Эта статья представляет новую модель Change State Space Model для обнаружения изменений в данных дистанционного зондирования. Модель фокусируется на релевантных изменениях между разновременными изображениями, эффективно фильтруя несущественную информацию. Благодаря концентрации только на измененных признаках, количество параметров сети уменьшается, что значительно повышает вычислительную эффективность при сохранении высокой производительности обнаружения. Предложенная модель превзошла сверточные нейронные сети, Vision Transformers и модели на основе Mamba на трех эталонных наборах данных, при этом имея меньшую вычислительную сложность.'}, 'en': {'title': 'Efficient Change Detection with the Change State Space Model', 'desc': 'This paper introduces the Change State Space Model (CSSM), a new architecture designed specifically for change detection in bi-temporal images. Unlike traditional ConvNets and Vision Transformers, which struggle with long-range dependencies and computational efficiency, CSSM effectively filters out irrelevant information to focus on significant changes. By reducing the number of network parameters, CSSM enhances computational efficiency while maintaining high detection performance and robustness against input degradation. The model has been tested on three benchmark datasets, outperforming existing methods with lower computational complexity.'}, 'zh': {'title': '高效变化检测的新方法', 'desc': '本文介绍了一种新的变化状态空间模型（Change State Space Model），专门用于图像变化检测。该模型通过关注双时相图像之间的相关变化，有效过滤掉无关信息，从而提高了计算效率。与传统的卷积神经网络（ConvNets）和视觉变换器（ViTs）相比，该模型在保持高检测性能的同时，显著减少了网络参数数量。实验结果表明，该模型在三个基准数据集上表现优于现有的模型，且计算复杂度更低。'}}}, {'id': 'https://huggingface.co/papers/2504.07491', 'title': 'Kimi-VL Technical Report', 'url': 'https://huggingface.co/papers/2504.07491', 'abstract': 'We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.', 'score': 89, 'issue_id': 3185, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'e1d1e4add50955e8', 'authors': ['Kimi Team', 'Angang Du', 'Bohong Yin', 'Bowei Xing', 'Bowen Qu', 'Bowen Wang', 'Cheng Chen', 'Chenlin Zhang', 'Chenzhuang Du', 'Chu Wei', 'Congcong Wang', 'Dehao Zhang', 'Dikang Du', 'Dongliang Wang', 'Enming Yuan', 'Enzhe Lu', 'Fang Li', 'Flood Sung', 'Guangda Wei', 'Guokun Lai', 'Han Zhu', 'Hao Ding', 'Hao Hu', 'Hao Yang', 'Hao Zhang', 'Haoning Wu', 'Haotian Yao', 'Haoyu Lu', 'Heng Wang', 'Hongcheng Gao', 'Huabin Zheng', 'Jiaming Li', 'Jianlin Su', 'Jianzhou Wang', 'Jiaqi Deng', 'Jiezhong Qiu', 'Jin Xie', 'Jinhong Wang', 'Jingyuan Liu', 'Junjie Yan', 'Kun Ouyang', 'Liang Chen', 'Lin Sui', 'Longhui Yu', 'Mengfan Dong', 'Mengnan Dong', 'Nuo Xu', 'Pengyu Cheng', 'Qizheng Gu', 'Runjie Zhou', 'Shaowei Liu', 'Sihan Cao', 'Tao Yu', 'Tianhui Song', 'Tongtong Bai', 'Wei Song', 'Weiran He', 'Weixiao Huang', 'Weixin Xu', 'Xiaokun Yuan', 'Xingcheng Yao', 'Xingzhe Wu', 'Xinxing Zu', 'Xinyu Zhou', 'Xinyuan Wang', 'Y. Charles', 'Yan Zhong', 'Yang Li', 'Yangyang Hu', 'Yanru Chen', 'Yejie Wang', 'Yibo Liu', 'Yibo Miao', 'Yidao Qin', 'Yimin Chen', 'Yiping Bao', 'Yiqin Wang', 'Yongsheng Kang', 'Yuanxin Liu', 'Yulun Du', 'Yuxin Wu', 'Yuzhi Wang', 'Yuzi Yan', 'Zaida Zhou', 'Zhaowei Li', 'Zhejun Jiang', 'Zheng Zhang', 'Zhilin Yang', 'Zhiqi Huang', 'Zihao Huang', 'Zijia Zhao', 'Ziwei Chen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.07491.jpg', 'data': {'categories': ['#cv', '#agents', '#rl', '#multimodal', '#long_context', '#reasoning', '#small_models', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Kimi-VL: Эффективная мультимодальная модель с расширенными возможностями рассуждений', 'desc': 'Kimi-VL - это эффективная модель машинного обучения, основанная на архитектуре Mixture-of-Experts (MoE) для обработки визуальной и текстовой информации. Модель демонстрирует высокую производительность в различных сложных задачах, включая многоэтапное взаимодействие с агентами, понимание изображений и видео, оптическое распознавание символов и математические рассуждения. Kimi-VL обладает расширенным контекстным окном в 128K и способна обрабатывать сверхвысокое разрешение визуальных входных данных. Авторы также представляют улучшенную версию модели, Kimi-VL-Thinking, которая демонстрирует сильные способности к долгосрочным рассуждениям при сохранении компактного размера активированных параметров языковой модели.'}, 'en': {'title': 'Kimi-VL: Efficient Multimodal Mastery with Long-Context Reasoning', 'desc': 'Kimi-VL is a cutting-edge Mixture-of-Experts vision-language model that efficiently combines multimodal reasoning and long-context understanding while using only 2.8 billion parameters in its language decoder. It excels in various complex tasks, including multi-turn interactions and advanced image and video comprehension, outperforming other leading models in several areas. The model features a 128K extended context window, allowing it to process long inputs effectively, and its native-resolution vision encoder enhances its ability to interpret high-resolution visuals. Additionally, the Kimi-VL-Thinking variant improves long-horizon reasoning through supervised fine-tuning and reinforcement learning, setting a new benchmark for efficient multimodal models.'}, 'zh': {'title': 'Kimi-VL：高效的多模态推理新标准', 'desc': 'Kimi-VL是一种高效的开源混合专家（MoE）视觉语言模型（VLM），具备先进的多模态推理和长文本理解能力。该模型在多轮对话任务和各种视觉语言任务中表现出色，能够与顶尖模型相媲美。Kimi-VL还具备处理长上下文的能力，能够处理多达128K的输入，适用于复杂的视觉理解任务。通过长链思维的监督微调和强化学习，Kimi-VL-Thinking进一步提升了长远推理能力，设定了高效多模态思维模型的新标准。'}}}, {'id': 'https://huggingface.co/papers/2504.07128', 'title': "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning", 'url': 'https://huggingface.co/papers/2504.07128', 'abstract': 'Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly "thinking" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1\'s basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a \'sweet spot\' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.', 'score': 59, 'issue_id': 3184, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '6317a88ae7643fe2', 'authors': ['Sara Vera Marjanović', 'Arkil Patel', 'Vaibhav Adlakha', 'Milad Aghajohari', 'Parishad BehnamGhader', 'Mehar Bhatia', 'Aditi Khandelwal', 'Austin Kraft', 'Benno Krojer', 'Xing Han Lù', 'Nicholas Meade', 'Dongchan Shin', 'Amirhossein Kazemnejad', 'Gaurav Kamath', 'Marius Mosbach', 'Karolina Stańczak', 'Siva Reddy'], 'affiliations': ['McGill University', 'Mila Quebec AI Institute', 'University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2504.07128.jpg', 'data': {'categories': ['#architecture', '#ethics', '#inference', '#reasoning', '#long_context', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DeepSeek-R1: мышление машин через призму рассуждений', 'desc': 'Статья описывает новую модель машинного обучения DeepSeek-R1, которая использует многоступенчатые цепочки рассуждений для решения сложных задач. Исследователи анализируют влияние длины рассуждений, управление длинными контекстами, культурные и этические аспекты, а также сравнивают модель с когнитивными процессами человека. Результаты показывают, что у DeepSeek-R1 есть оптимальное время рассуждения, при превышении которого производительность модели может ухудшаться. Также отмечается тенденция модели застревать на ранее исследованных формулировках проблем и уязвимости в плане безопасности по сравнению с моделями без рассуждений.'}, 'en': {'title': 'DeepSeek-R1: Revolutionizing Reasoning in Language Models', 'desc': "The paper introduces DeepSeek-R1, a Large Reasoning Model that enhances the way language models (LLMs) tackle complex problems by generating multi-step reasoning chains. This model allows users to observe the reasoning process, fostering a new area of research called Thoughtology. The study examines various aspects of DeepSeek-R1, including the effects of reasoning length, context management, and safety issues, revealing that excessive inference time can negatively impact performance. Additionally, the findings highlight the model's tendency to dwell on previous problem formulations, which can hinder its ability to explore new solutions and raise safety concerns compared to traditional LLMs."}, 'zh': {'title': '深度推理，思维的未来', 'desc': 'DeepSeek-R1是一种大型推理模型，标志着大语言模型（LLM）在处理复杂问题时的根本转变。它通过创建详细的多步骤推理链来“思考”问题，而不是直接给出答案。这种推理过程对用户是公开的，为研究模型的推理行为提供了无限可能，并开启了思维学（Thoughtology）领域。我们的分析显示，DeepSeek-R1在推理时存在一个“甜蜜点”，过长的推理时间可能会影响模型的表现，同时它在处理已探索的问题时容易陷入反复思考，影响进一步的探索。'}}}, {'id': 'https://huggingface.co/papers/2504.07964', 'title': 'C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing', 'url': 'https://huggingface.co/papers/2504.07964', 'abstract': 'Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or "re-mixing" the experts in different layers jointly for each test sample. Since the test sample\'s ground truth is unknown, we propose to optimize a surrogate objective defined by the sample\'s "successful neighbors" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts\' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to "Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE\'s advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.', 'score': 49, 'issue_id': 3184, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '1f6d28b26eec9879', 'authors': ['Zhongyang Li', 'Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['Johns Hopkins University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2504.07964.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация путей экспертов для повышения эффективности языковых моделей', 'desc': "Исследование показывает, что модели языка на основе смеси экспертов (MoE LLM) имеют значительный потенциал для улучшения точности на 10-20%. Авторы разработали новый метод оптимизации C3PO, который переоценивает веса экспертов во время вывода для каждого тестового примера. Метод использует суррогатную целевую функцию, основанную на 'успешных соседях' из эталонного набора. Применение C3PO к современным MoE LLM показало улучшение точности на 7-15% и превзошло базовые методы обучения во время тестирования."}, 'en': {'title': 'Optimize Expert Pathways for Better Performance!', 'desc': 'This paper addresses the issue of sub-optimal expert pathways in Mixture-of-Experts (MoE) Large Language Models (LLMs), which can lead to a significant accuracy gap during inference. The authors propose a new method called C3PO, which optimizes the mixing weights of core experts in critical layers for each test sample using surrogate objectives based on similar reference samples. By focusing on optimizing only the essential components, C3PO achieves notable accuracy improvements of 7-15% over baseline models while maintaining computational efficiency. The results demonstrate that C3PO allows smaller MoE models to outperform larger LLMs, highlighting its effectiveness in enhancing model performance at lower resource costs.'}, 'zh': {'title': '优化混合专家模型的关键路径', 'desc': '混合专家（MoE）的大型语言模型（LLMs）在专家路径选择上存在显著的优化不足。我们的研究发现，简单的专家选择方法在预训练阶段会导致10-20%的准确率差距。为了解决这个问题，我们提出了一种新的测试时优化方法，通过对每个测试样本的不同层次的专家进行重新加权或“重新混合”。这种方法称为“关键层、核心专家、协作路径优化（C3PO）”，在多个基准测试中显示出显著的准确率提升，并且在计算效率上优于传统的学习方法。'}}}, {'id': 'https://huggingface.co/papers/2504.07956', 'title': 'VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2504.07956', 'abstract': "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.", 'score': 37, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '88860725e51f3629', 'authors': ['Yukun Qi', 'Yiming Zhao', 'Yu Zeng', 'Xikun Bao', 'Wenxuan Huang', 'Lin Chen', 'Zehui Chen', 'Jie Zhao', 'Zhongang Qi', 'Feng Zhao'], 'affiliations': ['East China Normal University', 'Huawei Noahs Ark Lab', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.07956.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'VCR-Bench: новый стандарт оценки рассуждений ИИ по видео', 'desc': 'В статье представлен новый бенчмарк VCR-Bench для оценки способностей больших визуально-языковых моделей к рассуждениям на основе цепочки мыслей при анализе видео. Бенчмарк включает 859 видео и 1034 пары вопрос-ответ с пошаговыми обоснованиями, помеченными для оценки восприятия и рассуждений. Эксперименты показали существенные ограничения современных моделей, особенно в обработке пространственно-временной информации. VCR-Bench призван стать стандартизированным инструментом для выявления недостатков в сложных задачах видеоанализа.'}, 'en': {'title': 'VCR-Bench: Evaluating Video Reasoning in LVLMs', 'desc': 'This paper introduces VCR-Bench, a new benchmark for evaluating Video Chain-of-Thought (CoT) reasoning in large vision-language models (LVLMs). It addresses the lack of rigorous evaluation frameworks for assessing how well these models can reason about video content. The benchmark includes 859 videos and 1,034 annotated question-answer pairs, each with a stepwise CoT rationale linked to perception or reasoning capabilities. Experiments reveal that current LVLMs struggle with video reasoning, particularly in processing temporal-spatial information, highlighting the need for improved models in this area.'}, 'zh': {'title': 'VCR-Bench：视频推理的新标准', 'desc': '链式思维（CoT）推理的进步显著提升了大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的能力。然而，目前缺乏一个严格的视频CoT推理评估框架，现有的视频基准无法充分评估推理过程。为此，我们提出了VCR-Bench，这是一个新颖的基准，旨在全面评估LVLMs在视频链式思维推理方面的能力。通过859个视频和1034对高质量问答对，VCR-Bench为每个问答对提供了逐步的CoT推理依据，揭示了当前LVLMs在复杂视频推理中的关键瓶颈。'}}}, {'id': 'https://huggingface.co/papers/2504.07960', 'title': 'VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning', 'url': 'https://huggingface.co/papers/2504.07960', 'abstract': 'Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.', 'score': 36, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'a8b5331ac40d6a3d', 'authors': ['Zhong-Yu Li', 'Ruoyi Du', 'Juncheng Yan', 'Le Zhuo', 'Zhen Li', 'Peng Gao', 'Zhanyu Ma', 'Ming-Ming Cheng'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University', 'VCIP, CS, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07960.jpg', 'data': {'categories': ['#cv', '#graphs', '#transfer_learning', '#diffusion', '#multimodal', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'VisualCloze: универсальная генерация изображений с визуальным контекстным обучением', 'desc': 'VisualCloze - это универсальная система генерации изображений, использующая визуальное обучение в контексте для идентификации задач. Она поддерживает широкий спектр задач, включая генерализацию на новые задачи и обратную генерацию. Система использует Graph200K - графовую структуру данных для создания взаимосвязанных задач и улучшения переноса знаний. VisualCloze объединяет генерацию изображений с их дополнением, используя сильные генеративные приоры предобученных моделей.'}, 'en': {'title': 'VisualCloze: Bridging Tasks with Visual Learning in Image Generation', 'desc': 'This paper introduces VisualCloze, a universal image generation framework that overcomes the limitations of task-specific models by supporting a variety of in-domain tasks and generalizing to unseen tasks. It addresses challenges such as task instruction ambiguity and sparse task distributions by utilizing visual in-context learning, which allows models to learn from visual examples rather than language instructions. The authors also present Graph200K, a dataset that enhances task density and facilitates knowledge transfer across related tasks. Additionally, they demonstrate that their unified image generation approach aligns with image infilling, enabling the use of pre-trained models for improved generative performance.'}, 'zh': {'title': 'VisualCloze：通用图像生成的新框架', 'desc': '本论文介绍了一种名为VisualCloze的通用图像生成框架，旨在解决当前图像生成任务中存在的效率和通用性问题。与传统依赖语言指令的方法不同，VisualCloze通过视觉示例进行任务识别，从而减少了任务模糊性和提高了泛化能力。为了增强任务之间的可转移知识，我们引入了Graph200K数据集，该数据集通过图结构建立了多种相关任务。最后，我们发现我们的图像生成方法与图像填充具有一致的目标，从而能够利用预训练填充模型的强生成先验。'}}}, {'id': 'https://huggingface.co/papers/2504.07957', 'title': 'MM-IFEngine: Towards Multimodal Instruction Following', 'url': 'https://huggingface.co/papers/2504.07957', 'abstract': 'The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.', 'score': 29, 'issue_id': 3184, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'dbfc6cfeb60e05d7', 'authors': ['Shengyuan Ding', 'Shenxi Wu', 'Xiangyu Zhao', 'Yuhang Zang', 'Haodong Duan', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'Fudan University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.07957.jpg', 'data': {'categories': ['#benchmark', '#training', '#dataset', '#data', '#optimization', '#open_source', '#multimodal', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Улучшение способности мультимодальных ИИ-моделей следовать инструкциям', 'desc': 'Статья представляет MM-IFEngine - пайплайн для создания высококачественных пар изображение-инструкция для обучения мультимодальных языковых моделей. На основе этого пайплайна созданы наборы данных MM-IFInstruct-23k и MM-IFDPO-23k для обучения с учителем и прямой оптимизации предпочтений соответственно. Также предложен бенчмарк MM-IFEval для оценки способности моделей следовать сложным мультимодальным инструкциям. Эксперименты показали значительное улучшение результатов на различных бенчмарках после дообучения моделей на созданных наборах данных.'}, 'en': {'title': 'Enhancing Instruction Following in MLLMs with MM-IFEngine', 'desc': 'This paper introduces MM-IFEngine, a new method for generating high-quality image-instruction pairs to improve the instruction-following ability of Multi-modal Large Language Models (MLLMs). The authors create a large dataset called MM-IFInstruct-23k, which is designed for Supervised Fine-Tuning (SFT) and an extended version for Direct Preference Optimization (DPO). They also present MM-IFEval, a benchmark that evaluates MLLMs on complex tasks with both output and input constraints. Experiments show that fine-tuning on their datasets significantly enhances performance on various instruction-following benchmarks.'}, 'zh': {'title': '提升多模态指令跟随能力的创新方法', 'desc': '这篇论文介绍了一种新的多模态指令跟随能力评估方法，称为MM-IFEngine。该方法生成高质量的图像-指令对，创建了一个大规模的训练数据集MM-IFInstruct-23k，适用于监督微调和直接偏好优化。论文还提出了MM-IFEval，一个具有挑战性的多模态基准，包含输出响应和输入图像的约束。通过实验，微调后的多模态大语言模型在多个基准测试中表现出显著提升。'}}}, {'id': 'https://huggingface.co/papers/2504.07943', 'title': 'HoloPart: Generative 3D Part Amodal Segmentation', 'url': 'https://huggingface.co/papers/2504.07943', 'abstract': '3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.', 'score': 23, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '4cc1401ee10a171e', 'authors': ['Yunhan Yang', 'Yuan-Chen Guo', 'Yukun Huang', 'Zi-Xin Zou', 'Zhipeng Yu', 'Yangguang Li', 'Yan-Pei Cao', 'Xihui Liu'], 'affiliations': ['The University of Hong Kong', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2504.07943.jpg', 'data': {'categories': ['#3d', '#architecture', '#diffusion', '#benchmark', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'Революция в 3D-сегментации: видеть невидимое', 'desc': 'Статья представляет новый подход к амодальной сегментации трехмерных частей объектов. Авторы предлагают двухэтапный метод: сначала используется существующая сегментация 3D-частей для получения начальных неполных сегментов, затем применяется новая модель HoloPart на основе диффузии для завершения этих сегментов в полные 3D-части. HoloPart использует специализированную архитектуру с локальным и глобальным вниманием для захвата геометрии частей и обеспечения общей согласованности формы. Результаты показывают, что предложенный метод значительно превосходит современные методы завершения форм и открывает новые возможности для приложений в редактировании геометрии, анимации и назначении материалов.'}, 'en': {'title': 'Unlocking Hidden Shapes: HoloPart for 3D Amodal Segmentation', 'desc': 'This paper addresses the challenge of 3D part amodal segmentation, which involves identifying complete parts of a 3D shape even when some parts are hidden. Current methods only work with visible surfaces, limiting their effectiveness. The authors propose a two-stage approach that first uses existing segmentation techniques to identify incomplete parts, followed by a novel diffusion-based model called HoloPart to complete these segments. HoloPart employs a specialized architecture to ensure both detailed part geometry and overall shape consistency, achieving superior results on new benchmarks compared to existing methods.'}, 'zh': {'title': '突破3D分割：HoloPart模型的创新之路', 'desc': '本文提出了一种新的3D部分无模态分割任务，旨在将3D形状分解为完整且具有语义意义的部分，即使在被遮挡的情况下也能实现。现有的3D部分分割方法仅能识别可见的表面，限制了其应用。我们提出了一种实用的两阶段方法，首先利用现有的3D部分分割获取初步的不完整部分，然后引入HoloPart模型，通过扩散方法完成这些部分。HoloPart采用了专门的架构，结合局部注意力和全局形状一致性，显著提升了3D部分无模态分割的效果。'}}}, {'id': 'https://huggingface.co/papers/2504.07951', 'title': 'Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models', 'url': 'https://huggingface.co/papers/2504.07951', 'abstract': 'Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.', 'score': 15, 'issue_id': 3190, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '66dadd5828f14551', 'authors': ['Mustafa Shukor', 'Enrico Fini', 'Victor Guilherme Turrisi da Costa', 'Matthieu Cord', 'Joshua Susskind', 'Alaaeldin El-Nouby'], 'affiliations': ['Apple', 'Sorbonne University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07951.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#agi', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективные мультимодальные модели: преимущества раннего слияния', 'desc': 'Это исследование посвящено разработке мультимодальных моделей машинного обучения, способных воспринимать мир через различные типы данных. Авторы сравнивают архитектуры позднего и раннего слияния модальностей, проведя масштабное исследование на 457 моделях. Результаты показывают, что архитектуры раннего слияния не уступают поздним, а при меньшем количестве параметров даже превосходят их. Применение метода смеси экспертов (Mixture of Experts) позволяет моделям эффективно обучаться модально-специфичным весам, значительно улучшая производительность.'}, 'en': {'title': 'Early-Fusion Models Outperform Late-Fusion in Multimodal Learning', 'desc': 'This paper explores the effectiveness of native multimodal models (NMMs) compared to late-fusion architectures that combine pre-trained components. The authors conducted a large-scale study with 457 models to analyze the performance of early-fusion versus late-fusion approaches. They found that early-fusion models, which integrate modalities from the start, outperform late-fusion models in terms of efficiency and training ease. Additionally, by using Mixture of Experts (MoEs), the early-fusion models can learn specific weights for different modalities, further boosting their performance.'}, 'zh': {'title': '早融合架构更胜一筹！', 'desc': '本研究探讨了多模态模型的架构设计，特别是原生多模态模型（NMMs），这些模型从一开始就针对所有模态进行训练。我们对457个不同架构和训练组合的模型进行了广泛的规模法则研究。结果显示，后融合架构并没有比早融合架构具有固有优势，后者在较低的参数数量下表现更强，训练效率更高，部署更简单。我们还发现，结合专家混合（MoEs）可以让模型学习特定模态的权重，从而显著提升性能。'}}}, {'id': 'https://huggingface.co/papers/2504.07830', 'title': 'MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations', 'url': 'https://huggingface.co/papers/2504.07830', 'abstract': "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.", 'score': 15, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '85dbaddf009300e0', 'authors': ['Genglin Liu', 'Salman Rahman', 'Elisa Kreiss', 'Marzyeh Ghassemi', 'Saadia Gabriel'], 'affiliations': ['MIT CSAIL', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2504.07830.jpg', 'data': {'categories': ['#agents', '#graphs', '#reasoning', '#games', '#multimodal', '#open_source'], 'emoji': '🕸️', 'ru': {'title': 'Цифровое общество под микроскопом ИИ', 'desc': 'MOSAIC - это новая система моделирования социальных сетей с открытым исходным кодом, использующая языковые модели для имитации поведения пользователей. Она сочетает агентов на основе больших языковых моделей с направленным социальным графом для анализа поведения, связанного с обманом. Система позволяет проводить многоагентные симуляции, моделирующие распространение контента и динамику взаимодействия пользователей. Исследователи оценили три стратегии модерации контента и обнаружили, что они снижают распространение недостоверной информации и повышают вовлеченность пользователей.'}, 'en': {'title': 'MOSAIC: Simulating Social Networks to Combat Misinformation', 'desc': 'The paper introduces MOSAIC, an innovative open-source framework for simulating social networks using generative language agents. These agents predict user behaviors like liking and sharing content, allowing researchers to study how deception emerges in online interactions. By creating detailed user personas, the framework facilitates large-scale simulations of content spread and user engagement. The study evaluates various content moderation strategies, revealing that they can effectively reduce misinformation while enhancing user interaction.'}, 'zh': {'title': 'MOSAIC：社交网络行为模拟与内容审核新探索', 'desc': '我们提出了一种新颖的开源社交网络模拟框架MOSAIC，利用生成语言代理预测用户行为，如点赞、分享和标记内容。该模拟结合了大型语言模型（LLM）代理和有向社交图，分析新出现的欺骗行为，帮助理解用户如何判断在线社交内容的真实性。通过构建多样化的细粒度用户画像，我们的系统支持大规模的多代理模拟，模拟内容传播和用户参与的动态。我们评估了三种不同的内容审核策略，发现它们不仅能减缓虚假信息的传播，还能提高用户参与度。'}}}, {'id': 'https://huggingface.co/papers/2504.07934', 'title': 'SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement', 'url': 'https://huggingface.co/papers/2504.07934', 'abstract': 'In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.', 'score': 11, 'issue_id': 3183, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '419fda5d0a4bcadf', 'authors': ['Xiyao Wang', 'Zhengyuan Yang', 'Chao Feng', 'Hongjin Lu', 'Linjie Li', 'Chung-Ching Lin', 'Kevin Lin', 'Furong Huang', 'Lijuan Wang'], 'affiliations': ['Microsoft', 'University of Maryland, College Park', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2504.07934.jpg', 'data': {'categories': ['#cv', '#data', '#reasoning', '#training', '#open_source', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Меньше данных, больше мышления: революция в визуальном рассуждении ИИ', 'desc': 'В этой статье представлен эффективный метод улучшения визуального рассуждения с использованием значительно меньшего количества обучающих примеров, основанный на самосовершенствовании без передачи знаний. Ключевой идеей является важность сложности обучающих данных при тонкой настройке с подкреплением (RFT). Авторы предлагают новый способ использования поиска по дереву Монте-Карло (MCTS) для количественной оценки сложности примеров и эффективной фильтрации данных. Разработанная модель ThinkLite-VL, обученная на отфильтрованном наборе из 11 тысяч примеров, превосходит существующие модели визуально-языкового рассуждения уровня 7B и достигает лучших результатов на ряде бенчмарков.'}, 'en': {'title': 'Enhancing Visual Reasoning with Fewer Samples through Smart Data Selection', 'desc': 'This paper introduces a novel method to improve visual reasoning in models using fewer training samples through self-improvement techniques, avoiding knowledge distillation. The authors emphasize the importance of sample difficulty during reinforcement fine-tuning (RFT), suggesting that challenging samples can enhance reasoning capabilities even with limited data. They propose a unique application of Monte Carlo Tree Search (MCTS) to quantify sample difficulty, allowing for effective data filtering. The resulting model, ThinkLite-VL, demonstrates a 7% performance increase over its predecessor using only 11k samples, achieving state-of-the-art results in various benchmarks.'}, 'zh': {'title': '用少量样本提升视觉推理能力', 'desc': '本文提出了一种有效的方法，通过自我改进来增强视觉推理，且所需的训练样本显著减少，不依赖知识蒸馏。我们发现，在强化微调（RFT）过程中，训练数据的难度至关重要，适当具有挑战性的样本可以显著提升推理能力。我们提出了一种新颖的方式，利用蒙特卡洛树搜索（MCTS）来量化样本的难度，从而实现有效的数据筛选。最终，我们的模型ThinkLite-VL在八个基准测试中表现出色，使用仅11k个训练样本，平均性能提升了7%。'}}}, {'id': 'https://huggingface.co/papers/2504.04974', 'title': 'Towards Visual Text Grounding of Multimodal Large Language Model', 'url': 'https://huggingface.co/papers/2504.04974', 'abstract': 'Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.', 'score': 8, 'issue_id': 3184, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'f382ad07e678a558', 'authors': ['Ming Li', 'Ruiyi Zhang', 'Jian Chen', 'Jiuxiang Gu', 'Yufan Zhou', 'Franck Dernoncourt', 'Wanrong Zhu', 'Tianyi Zhou', 'Tong Sun'], 'affiliations': ['Adobe Research', 'University at Buffalo', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.04974.jpg', 'data': {'categories': ['#benchmark', '#training', '#dataset', '#synthetic', '#transfer_learning', '#multimodal', '#reasoning'], 'emoji': '📄', 'ru': {'title': 'Улучшение понимания текста на изображениях документов для мультимодальных ИИ', 'desc': 'Статья представляет новую задачу TRIG для оценки и улучшения способностей мультимодальных языковых моделей (MLLM) в работе с текстом на изображениях документов. Авторы создали набор данных из 800 вручную размеченных пар вопрос-ответ и 90 тысяч синтетических примеров для обучения моделей. Оценка существующих MLLM на этом бенчмарке выявила значительные ограничения в их способности к пространственному рассуждению и привязке к тексту на изображениях. Предложены два метода для улучшения этих способностей: обучение на инструкциях и эффективное встраивание plug-and-play.'}, 'en': {'title': 'Enhancing MLLMs for Text-Rich Image Grounding', 'desc': "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in understanding text-rich images, such as documents and infographics. The authors introduce a new task called TRIG, which focuses on improving the grounding of text in these complex images for better document question-answering. They create a benchmark dataset with 800 annotated question-answer pairs and 90 synthetic data samples to evaluate MLLMs' performance. Additionally, the paper presents two methods to enhance MLLMs' spatial reasoning and grounding abilities through fine-tuning on the new dataset."}, 'zh': {'title': '提升文档图像的文本定位能力', 'desc': '尽管多模态大型语言模型（MLLMs）已经取得了一定进展，但在视觉文本定位方面仍然存在显著的局限性，尤其是在文本丰富的文档图像中。文档图像如扫描表单和信息图表由于其复杂的布局和文本内容，带来了重大挑战。为了解决这一问题，我们提出了TRIG任务，并设计了一个新的指令数据集，以评估和提升MLLMs在文档问答中的文本丰富图像定位能力。通过对MLLMs进行微调，我们的方法在空间推理和定位能力上显示出显著的改进。'}}}, {'id': 'https://huggingface.co/papers/2504.06752', 'title': 'Compass Control: Multi Object Orientation Control for Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2504.06752', 'abstract': 'Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.', 'score': 3, 'issue_id': 3188, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'd73e0b8851ffd9e3', 'authors': ['Rishubh Parihar', 'Vaibhav Agrawal', 'Sachidanand VS', 'R. Venkatesh Babu'], 'affiliations': ['IIIT Hyderabad', 'IISc Bangalore'], 'pdf_title_img': 'assets/pdf/title_img/2504.06752.jpg', 'data': {'categories': ['#cv', '#3d', '#training', '#diffusion', '#synthetic'], 'emoji': '🧭', 'ru': {'title': 'Точное управление ориентацией множественных объектов в генеративных моделях изображений', 'desc': 'Эта статья представляет новый метод управления ориентацией объектов в генеративных моделях изображений на основе диффузии. Авторы предлагают использовать специальные токены-компасы для каждого объекта, которые кодируют информацию об ориентации. Модель обучается на синтетическом наборе данных с процедурно сгенерированными сценами. Для улучшения контроля и предотвращения смешивания объектов применяется ограничение карт перекрестного внимания.'}, 'en': {'title': 'Mastering Object Orientation in Text-to-Image Generation', 'desc': 'This paper presents a novel approach to enhance text-to-image diffusion models by enabling precise control over the orientation of multiple objects in generated scenes. The authors introduce orientation-aware compass tokens that are conditioned on the object orientation, allowing for better manipulation of object placement and alignment. A lightweight encoder network predicts these tokens based on input orientations, addressing challenges like poor control and object entanglement during training. The proposed method demonstrates strong generalization capabilities, achieving state-of-the-art results in orientation control and text alignment through extensive evaluations and user studies.'}, 'zh': {'title': '精确控制多对象方向的创新方法', 'desc': '本文提出了一种新的方法来控制文本到图像的扩散模型，特别是多对象的方向控制。通过引入方向感知的指南针标记，模型能够为每个对象提供精确的方向控制。研究表明，该模型在生成复杂对象和多对象场景时表现出强大的泛化能力。结合个性化方法后，模型能够在多种上下文中精确控制新对象的方向。'}}}, {'id': 'https://huggingface.co/papers/2504.05579', 'title': 'TAPNext: Tracking Any Point (TAP) as Next Token Prediction', 'url': 'https://huggingface.co/papers/2504.05579', 'abstract': 'Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.', 'score': 3, 'issue_id': 3192, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': '27a89bfc540d93f8', 'authors': ['Artem Zholus', 'Carl Doersch', 'Yi Yang', 'Skanda Koppula', 'Viorica Patraucean', 'Xu Owen He', 'Ignacio Rocco', 'Mehdi S. M. Sajjadi', 'Sarath Chandar', 'Ross Goroshin'], 'affiliations': ['Canada CIFAR AI Chair', 'Chandar Research Lab', 'Google DeepMind', 'Mila - Quebec AI Institute', 'Polytechnique Montreal', 'University College London', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2504.05579.jpg', 'data': {'categories': ['#cv', '#architecture', '#video', '#robotics'], 'emoji': '🎯', 'ru': {'title': 'TAPNext: Революция в отслеживании точек видео без специализированных предубеждений', 'desc': 'TAPNext - это новый подход к отслеживанию точек в видео, который представляет эту задачу как последовательное декодирование маскированных токенов. Модель работает в причинном режиме, отслеживает в режиме реального времени и устраняет специфические для трекинга индуктивные предубеждения. TAPNext достигает нового уровня производительности среди онлайн и офлайн трекеров, несмотря на свою простоту. Исследование показывает, что многие широко используемые эвристики трекинга естественным образом возникают в TAPNext через сквозное обучение.'}, 'en': {'title': 'Revolutionizing Video Tracking with TAPNext', 'desc': 'The paper introduces TAPNext, a novel method for tracking any point (TAP) in videos, which is crucial for applications like robotics and video editing. Unlike traditional methods that depend on complex rules and biases, TAPNext simplifies the process by using sequential masked token decoding. This approach allows for real-time tracking without the need for temporal windowing, resulting in lower latency. TAPNext not only achieves superior tracking performance compared to existing methods but also shows that common tracking heuristics can be learned through end-to-end training.'}, 'zh': {'title': 'TAPNext：简化视频跟踪的新方法', 'desc': '本文介绍了一种新的视频跟踪方法，称为TAPNext，旨在解决跟踪任意点（TAP）的问题。与现有方法不同，TAPNext将TAP视为顺序掩码令牌解码，消除了复杂的跟踪特定偏见。该模型具有因果性，能够在线实时跟踪，且延迟极低。尽管方法简单，TAPNext在在线和离线跟踪器中都达到了新的最先进的跟踪性能。'}}}, {'id': 'https://huggingface.co/papers/2504.07961', 'title': 'Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction', 'url': 'https://huggingface.co/papers/2504.07961', 'abstract': 'We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.', 'score': 2, 'issue_id': 3193, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '4a0d7f1cda574212', 'authors': ['Zeren Jiang', 'Chuanxia Zheng', 'Iro Laina', 'Diane Larlus', 'Andrea Vedaldi'], 'affiliations': ['Naver Labs Europe', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2504.07961.jpg', 'data': {'categories': ['#video', '#3d', '#benchmark', '#multimodal', '#long_context', '#diffusion', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'Geo4D: революция в 3D-реконструкции динамических сцен из видео', 'desc': 'Geo4D - это метод для адаптации моделей диффузии видео к задаче монокулярной 3D-реконструкции динамических сцен. Он использует сильные динамические праймеры, захваченные видеомоделями, и может обучаться только на синтетических данных, хорошо обобщаясь на реальные данные в режиме zero-shot. Geo4D предсказывает несколько взаимодополняющих геометрических модальностей и использует новый алгоритм мультимодального выравнивания для их объединения. Эксперименты показывают, что Geo4D значительно превосходит современные методы оценки глубины видео для динамических сцен.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Geo4D', 'desc': 'Geo4D is a novel approach that adapts video diffusion models for creating 3D reconstructions from single-camera videos of moving scenes. It effectively utilizes the dynamic information captured by these models, allowing it to be trained solely on synthetic datasets while still performing well on real-world data without additional training. The method predicts various geometric representations, including point, depth, and ray maps, and employs a unique multi-modal alignment technique to integrate these representations during the reconstruction process. Through extensive testing, Geo4D demonstrates superior performance compared to existing video depth estimation techniques, particularly in dynamic environments.'}, 'zh': {'title': 'Geo4D：动态场景的4D重建新方法', 'desc': 'Geo4D是一种将视频扩散模型用于单目3D重建动态场景的方法。它利用视频模型捕捉到的强动态先验，仅使用合成数据进行训练，并能在零样本情况下很好地推广到真实数据。Geo4D预测多种互补的几何模态，包括点图、深度图和光线图。通过新的多模态对齐算法和多个滑动窗口，Geo4D在推理时对这些模态进行对齐和融合，从而实现长视频的稳健和准确的4D重建。'}}}, {'id': 'https://huggingface.co/papers/2504.06801', 'title': 'MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular\n  Detection', 'url': 'https://huggingface.co/papers/2504.06801', 'abstract': "Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.", 'score': 2, 'issue_id': 3188, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'bf3e9622523967d2', 'authors': ['Rishubh Parihar', 'Srinjay Sarkar', 'Sarthak Vora', 'Jogendra Kundu', 'R. Venkatesh Babu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.06801.jpg', 'data': {'categories': ['#3d', '#optimization', '#dataset', '#synthetic'], 'emoji': '🔍', 'ru': {'title': 'Умное размещение синтетических объектов для улучшения монокулярной 3D детекции', 'desc': 'MonoPlace3D - это новая система для создания реалистичных аугментаций данных для монокулярных 3D детекторов. Она учитывает содержимое 3D сцены для определения правдоподобного расположения объектов. Система обучается распределению возможных 3D ограничивающих рамок, а затем рендерит и размещает реалистичные объекты согласно этому распределению. Эксперименты на наборах данных KITTI и NuScenes показали значительное повышение точности существующих монокулярных 3D детекторов при высокой эффективности использования данных.'}, 'en': {'title': 'Enhancing Monocular 3D Detection with Realistic Object Placement', 'desc': 'This paper addresses the limitations of current monocular 3D detectors due to the lack of diverse real-world datasets. It highlights the importance of not just realistic object appearance but also the correct placement of objects in 3D scenes for effective training. The authors introduce MonoPlace3D, a system that learns to generate realistic object placements based on the content of the background scene. Their experiments show that MonoPlace3D enhances the performance of existing monocular 3D detectors on standard datasets like KITTI and NuScenes, demonstrating improved accuracy and data efficiency.'}, 'zh': {'title': 'MonoPlace3D：提升单目3D检测的真实感增强', 'desc': '当前的单目3D检测器受到真实世界数据集多样性和规模的限制。虽然数据增强有助于改善模型性能，但在户外场景中生成真实感的增强数据尤其困难。大多数合成数据生成方法专注于通过改进渲染技术来提高物体外观的真实感，而我们发现物体的放置位置和方式对训练有效的3D单目检测器同样重要。为了解决这一问题，我们提出了MonoPlace3D系统，它考虑3D场景内容来创建真实的增强数据，从而显著提高了现有单目3D检测器的准确性。'}}}, {'id': 'https://huggingface.co/papers/2504.05741', 'title': 'DDT: Decoupled Diffusion Transformer', 'url': 'https://huggingface.co/papers/2504.05741', 'abstract': 'Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\color{ddtD}ecoupled \\color{ddtD}iffusion \\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.', 'score': 48, 'issue_id': 3159, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': '2f4cd9583b2418f3', 'authors': ['Shuai Wang', 'Zhi Tian', 'Weilin Huang', 'Limin Wang'], 'affiliations': ['ByteDance Seed Vision', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05741.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#cv', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'DDT: Разделяй и властвуй в мире диффузионных трансформеров', 'desc': 'Статья представляет новый подход к архитектуре диффузионных трансформеров, называемый Decoupled Diffusion Transformer (DDT). DDT разделяет процессы кодирования семантики и декодирования высокочастотных компонентов, что позволяет разрешить проблему оптимизации, присущую стандартным диффузионным трансформерам. Эксперименты показывают, что DDT достигает нового уровня производительности на наборе данных ImageNet, значительно ускоряя обучение и улучшая качество генерации изображений. Кроме того, предложенная архитектура позволяет оптимизировать процесс вывода путем разделения условий между соседними шагами денойзинга.'}, 'en': {'title': 'Decoupling for Faster and Better Image Generation', 'desc': 'This paper introduces a new model called the Decoupled Diffusion Transformer (DDT), which addresses the challenges faced by traditional diffusion transformers in generating high-quality outputs. The DDT separates the tasks of semantic encoding and high-frequency decoding, allowing for better optimization and improved performance. Experiments show that as the model size increases, a more powerful encoder leads to significant enhancements in generation quality, achieving state-of-the-art results on ImageNet datasets. Additionally, the decoupled design improves inference speed by optimizing the sharing of self-conditions between denoising steps, while a novel dynamic programming method helps maintain performance.'}, 'zh': {'title': '解耦扩散变换器：提升生成质量与推理速度的创新方案', 'desc': '扩散变换器在生成质量上表现出色，但训练迭代时间较长且推理步骤较多。每个去噪步骤中，扩散变换器对噪声输入进行编码，以提取低频语义成分，然后用相同的模块解码高频成分。这种方案导致了一个固有的优化困境：编码低频语义需要减少高频成分，从而在语义编码和高频解码之间产生紧张关系。为了解决这个问题，我们提出了一种新的解耦扩散变换器（DDT），它采用专门的条件编码器进行语义提取，并配备专门的速度解码器。'}}}, {'id': 'https://huggingface.co/papers/2504.07096', 'title': 'OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens', 'url': 'https://huggingface.co/papers/2504.07096', 'abstract': 'We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.', 'score': 33, 'issue_id': 3160, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '68eafe783b5816f6', 'authors': ['Jiacheng Liu', 'Taylor Blanton', 'Yanai Elazar', 'Sewon Min', 'YenSung Chen', 'Arnavi Chheda-Kothary', 'Huy Tran', 'Byron Bischoff', 'Eric Marsh', 'Michael Schmitz', 'Cassidy Trier', 'Aaron Sarnat', 'Jenna James', 'Jon Borchardt', 'Bailey Kuehl', 'Evie Cheng', 'Karen Farley', 'Sruthi Sreeram', 'Taira Anderson', 'David Albright', 'Carissa Schoenick', 'Luca Soldaini', 'Dirk Groeneveld', 'Rock Yuren Pang', 'Pang Wei Koh', 'Noah A. Smith', 'Sophie Lebrecht', 'Yejin Choi', 'Hannaneh Hajishirzi', 'Ali Farhadi', 'Jesse Dodge'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'UC Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.07096.jpg', 'data': {'categories': ['#data', '#hallucinations', '#inference', '#open_source', '#interpretability', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Заглянуть в память языковой модели', 'desc': 'OLMoTrace - это первая система, которая в режиме реального времени отслеживает связь между выводами языковых моделей и их многотриллионным обучающим датасетом. Система находит дословные совпадения между сегментами вывода модели и документами в обучающем корпусе текстов. OLMoTrace использует расширенную версию алгоритма infini-gram и выдает результаты трассировки за несколько секунд. Эта система помогает пользователям понять поведение языковых моделей через призму их обучающих данных, что полезно для проверки фактов, выявления галлюцинаций и исследования креативности моделей.'}, 'en': {'title': 'Trace the Truth: Unveiling Language Model Outputs with OLMoTrace', 'desc': 'OLMoTrace is a groundbreaking system that allows users to trace the outputs of language models back to their extensive training data in real time. It identifies and displays exact matches between the generated text and the original documents from the training corpus. Utilizing an enhanced version of the infini-gram technique, OLMoTrace provides results in just a few seconds. This tool aids in understanding language model behavior, particularly in areas like fact-checking, hallucination, and creative output.'}, 'zh': {'title': '实时追踪语言模型输出的革命性工具', 'desc': 'OLMoTrace是第一个能够实时追踪语言模型输出与其训练数据之间关系的系统。它可以找到语言模型输出片段与训练文本库中文档的逐字匹配。该系统基于扩展版的infini-gram技术，能够在几秒钟内返回追踪结果。OLMoTrace帮助用户通过训练数据理解语言模型的行为，适用于事实检查、幻觉和语言模型的创造力探索。'}}}, {'id': 'https://huggingface.co/papers/2504.07046', 'title': 'A Unified Agentic Framework for Evaluating Conditional Image Generation', 'url': 'https://huggingface.co/papers/2504.07046', 'abstract': "Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.", 'score': 23, 'issue_id': 3167, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'ec2d69230afcb841', 'authors': ['Jifang Wang', 'Xue Yang', 'Longyue Wang', 'Zhenran Xu', 'Yiyu Wang', 'Yaowei Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology (Shenzhen), Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.07046.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#benchmark', '#open_source', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'CIGEval: Революция в оценке генерации изображений', 'desc': 'Статья представляет CIGEval - унифицированную систему для оценки задач условной генерации изображений. CIGEval использует большие мультимодальные модели (LMM) и набор инструментов для детального анализа. Эксперименты показывают, что CIGEval достигает высокой корреляции с оценками людей. Система способна выявлять тонкие проблемы в сгенерированных изображениях, демонстрируя потенциал для автоматизации оценки с надежностью на уровне человека.'}, 'en': {'title': 'CIGEval: Revolutionizing Evaluation in Conditional Image Generation', 'desc': 'This paper presents CIGEval, a new framework designed to evaluate conditional image generation tasks effectively. It addresses the need for reliable and explainable metrics by using large multimodal models (LMMs) and a multi-functional toolbox. CIGEval not only assesses image generation but also fine-tunes smaller LMMs to select the best evaluation tools autonomously. The results show that CIGEval correlates well with human assessments and outperforms previous methods, demonstrating its potential for automating image generation evaluations.'}, 'zh': {'title': 'CIGEval：条件图像生成的全面评估新框架', 'desc': '条件图像生成因其个性化内容的能力而受到广泛关注。然而，该领域在开发任务无关、可靠且可解释的评估指标方面面临挑战。本文介绍了CIGEval，这是一个统一的代理框架，用于全面评估条件图像生成任务。CIGEval利用大型多模态模型（LMMs）作为核心，整合多功能工具箱并建立细粒度评估框架，展示了其在图像生成任务评估中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2504.06514', 'title': 'Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?', 'url': 'https://huggingface.co/papers/2504.06514', 'abstract': "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.", 'score': 21, 'issue_id': 3160, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '6ca6e88a5650dba9', 'authors': ['Chenrui Fan', 'Ming Li', 'Lichao Sun', 'Tianyi Zhou'], 'affiliations': ['Lehigh University', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.06514.jpg', 'data': {'categories': ['#training', '#rl', '#hallucinations', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Осторожно: языковые модели склонны к избыточным рассуждениям', 'desc': 'В статье исследуется проблема чрезмерного мышления (overthinking) у языковых моделей при ответе на некорректно поставленные вопросы с отсутствующими предпосылками. Авторы обнаружили, что модели, обученные рассуждать, генерируют избыточно длинные и неэффективные ответы в таких ситуациях. Интересно, что модели без специального обучения рассуждениям показывают лучшие результаты, быстро идентифицируя некорректность вопроса. Исследование выявляет недостатки в текущих методах обучения моделей рассуждениям и предлагает новые подходы к решению проблемы чрезмерного мышления.'}, 'en': {'title': 'Tackling MiP-Overthinking in Reasoning LLMs', 'desc': 'This paper investigates how reasoning large language models (LLMs) respond to poorly defined questions that lack necessary information, a situation termed as missing premises (MiP). It finds that these models tend to generate longer, redundant responses, which is a manifestation of what the authors call MiP-Overthinking. Interestingly, LLMs not specifically trained for reasoning perform better in these scenarios, producing shorter and more effective answers. The study highlights a significant flaw in the training methods for reasoning LLMs, suggesting that they do not promote efficient thinking, and proposes further analysis to understand and mitigate the overthinking issue.'}, 'zh': {'title': '揭示推理模型的过度思考问题', 'desc': '我们发现，推理大型语言模型（LLMs）在面对缺乏前提的模糊问题时，响应长度显著增加，导致冗余和无效的思考。这种新引入的场景加剧了普遍的过度思考问题，我们称之为MiP-过度思考。研究表明，当前的推理LLMs训练方法未能有效鼓励高效思考，导致思维模式的滥用。相反，未专门训练用于推理的LLMs在处理模糊问题时表现更好，能够快速识别问题并给出更简短的回答。'}}}, {'id': 'https://huggingface.co/papers/2504.07083', 'title': 'GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography', 'url': 'https://huggingface.co/papers/2504.07083', 'abstract': 'Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.', 'score': 18, 'issue_id': 3160, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '67e58f651d865bad', 'authors': ['Mengchen Zhang', 'Tong Wu', 'Jing Tan', 'Ziwei Liu', 'Gordon Wetzstein', 'Dahua Lin'], 'affiliations': ['Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Stanford University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07083.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#cv'], 'emoji': '🎥', 'ru': {'title': 'ИИ-оператор: новый стандарт компьютерной кинематографии', 'desc': 'Эта статья представляет новый подход к генерации траекторий движения камеры в кинопроизводстве с использованием методов машинного обучения. Авторы создали большой мультимодальный датасет DataDoP, содержащий 29 тысяч реальных кадров с траекториями камеры, картами глубины и подробными описаниями. На основе этих данных была обучена авторегрессионная модель GenDoP на базе архитектуры Transformer для генерации художественных и выразительных движений камеры. Эксперименты показали, что GenDoP превосходит существующие методы по управляемости, точности настройки траекторий и стабильности движения.'}, 'en': {'title': 'Revolutionizing Camera Movement with GenDoP', 'desc': 'This paper presents a novel approach to generating camera trajectories for video production using an auto-regressive model called GenDoP. The model is trained on a large dataset, DataDoP, which includes diverse camera movements, depth information, and detailed captions that reflect directorial intent. Unlike traditional methods that rely on geometric optimization, GenDoP leverages text guidance and RGBD inputs to create more expressive and context-aware camera movements. The results show that GenDoP outperforms existing techniques in terms of controllability, trajectory adjustments, and motion stability, setting a new benchmark for learning-based cinematography.'}, 'zh': {'title': '创新相机轨迹生成，提升视觉叙事效果', 'desc': '本论文介绍了一种新的相机轨迹生成方法，旨在提升视频制作中的视觉叙事效果。我们提出了一个名为GenDoP的自回归模型，利用大型多模态数据集DataDoP，包含29000个真实镜头及其相机轨迹、深度图和详细描述。通过结合文本指导和RGBD输入，GenDoP能够生成高质量、上下文感知的相机运动。实验结果表明，GenDoP在可控性、轨迹调整精细度和运动稳定性方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2504.04842', 'title': 'FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis', 'url': 'https://huggingface.co/papers/2504.04842', 'abstract': 'Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.', 'score': 15, 'issue_id': 3160, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '5b592626aeda4ec8', 'authors': ['Mengchao Wang', 'Qiang Wang', 'Fan Jiang', 'Yaqi Fan', 'Yunpeng Zhang', 'Yonggang Qi', 'Kun Zhao', 'Mu Xu'], 'affiliations': ['AMAP, Alibaba Group', 'Beijing University of Posts and Telecommunications'], 'pdf_title_img': 'assets/pdf/title_img/2504.04842.jpg', 'data': {'categories': ['#multimodal', '#video', '#diffusion'], 'emoji': '🗣️', 'ru': {'title': 'Оживление статичных портретов: новый уровень реализма и контроля', 'desc': 'Данная статья представляет новый подход к созданию анимированных аватаров из одного статичного портрета с помощью предобученной видео-диффузионной трансформерной модели. Авторы предлагают двухэтапную стратегию аудиовизуального выравнивания для генерации реалистичных говорящих портретов с контролируемой динамикой движений. Метод включает модуль кросс-внимания для сохранения идентичности лица и модуль модуляции интенсивности движения для управления выразительностью. Экспериментальные результаты показывают улучшение качества, реалистичности и сохранения идентичности по сравнению с существующими подходами.'}, 'en': {'title': 'Realistic Talking Avatars: Synchronizing Motion and Expression', 'desc': "This paper presents a new method for creating realistic animated avatars from a single portrait. It uses a pretrained video diffusion transformer to generate talking portraits that can move in a lifelike way. The approach includes a dual-stage audio-visual alignment strategy to ensure that the avatar's lip movements and body motions are synchronized with audio input. Additionally, it introduces a facial-focused cross-attention module to maintain facial identity and a motion intensity modulation module for controlling the expressiveness of the avatar."}, 'zh': {'title': '生成可控动画头像的新方法', 'desc': '本论文提出了一种新颖的框架，用于从单一静态肖像生成逼真的可动画头像。我们采用了预训练的视频扩散变换器模型，能够生成高保真、连贯的对话肖像，并具备可控的运动动态。核心是双阶段的音视频对齐策略，第一阶段通过音频驱动的动态对齐全场景，第二阶段则使用唇部追踪掩模精细调整唇部动作。实验结果表明，我们的方法在真实感、一致性、运动强度和身份保留方面均优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2504.07086', 'title': 'A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility', 'url': 'https://huggingface.co/papers/2504.07086', 'abstract': 'Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.', 'score': 10, 'issue_id': 3163, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'b06c700fb29c005d', 'authors': ['Andreas Hochlehnert', 'Hardik Bhatnagar', 'Vishaal Udandarao', 'Samuel Albanie', 'Ameya Prabhu', 'Matthias Bethge'], 'affiliations': ['Tübingen AI Center, University of Tübingen', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2504.07086.jpg', 'data': {'categories': ['#open_source', '#survey', '#rl', '#reasoning', '#benchmark', '#training'], 'emoji': '🧮', 'ru': {'title': 'Стандартизация оценки математических рассуждений языковых моделей', 'desc': 'Статья посвящена проблемам оценки способностей языковых моделей к математическим рассуждениям. Авторы обнаружили, что существующие бенчмарки очень чувствительны к тонким деталям реализации, таким как параметры декодирования и форматирование промптов. Предложена стандартизированная система оценки с четко определенными лучшими практиками. Переоценка недавних методов показала, что обучение с подкреплением дает лишь скромные улучшения, в то время как supervised fine-tuning демонстрирует более стабильную генерализацию.'}, 'en': {'title': 'Standardizing Reasoning Evaluations for Language Models', 'desc': 'This paper addresses the challenges in evaluating reasoning capabilities of language models (LMs) due to inconsistent benchmarking practices. The authors find that current mathematical reasoning benchmarks are sensitive to various implementation factors, which can lead to misleading performance claims. They propose a standardized evaluation framework that includes best practices and clear reporting standards to improve transparency and reproducibility. Their findings suggest that supervised finetuning (SFT) methods outperform reinforcement learning (RL) approaches, which often overfit on smaller benchmarks.'}, 'zh': {'title': '推理能力评估的新标准', 'desc': '本文探讨了语言模型在推理能力方面的进展，指出当前的数学推理基准测试对实现细节非常敏感。研究发现，许多评估方法缺乏透明性和统计基础，导致性能提升的比较不够清晰。我们提出了一个标准化的评估框架，明确了最佳实践和报告标准，并重新评估了现有方法。结果显示，强化学习方法的改进有限，而监督微调方法在泛化能力上表现更强。'}}}, {'id': 'https://huggingface.co/papers/2504.07081', 'title': 'Self-Steering Language Models', 'url': 'https://huggingface.co/papers/2504.07081', 'abstract': 'While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for "self-steering" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.', 'score': 10, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'b0b90ba5f1b881da', 'authors': ['Gabriel Grand', 'Joshua B. Tenenbaum', 'Vikash K. Mansinghka', 'Alexander K. Lew', 'Jacob Andreas'], 'affiliations': ['Massachusetts Institute of Technology', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07081.jpg', 'data': {'categories': ['#training', '#agents', '#reasoning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Языковые модели учатся писать программы для самоуправления', 'desc': "Статья представляет метод DisCIPL для 'самоуправления' языковых моделей. В этом подходе модель Planner генерирует специфичную для задачи программу вывода, которую выполняет группа моделей Follower. DisCIPL позволяет языковым моделям создавать рекурсивные процедуры поиска для управления выводом. Метод показывает результаты на уровне гораздо более крупных моделей на сложных задачах генерации с ограничениями. DisCIPL открывает возможности для высокопараллельных стратегий вывода по методу Монте-Карло."}, 'en': {'title': 'Empowering Language Models with Self-Steering Inference Programs', 'desc': 'This paper presents DisCIPL, a novel method that enhances language models (LMs) by allowing them to generate task-specific inference programs through a Planner model. These programs are then executed by multiple Follower models, enabling efficient and verifiable reasoning processes. The approach allows LMs to create recursive search procedures that improve their problem-solving capabilities, even when they struggle with direct reasoning. Remarkably, DisCIPL demonstrates that smaller models can achieve performance comparable to larger models on complex tasks, while also introducing new strategies for parallelized inference without the need for fine-tuning.'}, 'zh': {'title': '自我引导的语言模型推理新方法', 'desc': '本文介绍了一种名为DisCIPL的方法，旨在提高语言模型在复杂任务中的推理能力。该方法通过一个规划模型生成特定任务的推理程序，并由多个跟随模型执行，从而实现自我引导。DisCIPL使语言模型能够编写递归搜索程序，提升推理的可验证性和效率。实验表明，使用小型跟随模型时，DisCIPL在一些受限生成任务上表现出色，甚至超过了更大的模型。'}}}, {'id': 'https://huggingface.co/papers/2504.07089', 'title': 'OmniCaptioner: One Captioner to Rule Them All', 'url': 'https://huggingface.co/papers/2504.07089', 'abstract': 'We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.', 'score': 9, 'issue_id': 3164, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '9b09a9ebdec71131', 'authors': ['Yiting Lu', 'Jiakang Yuan', 'Zhen Li', 'Shitian Zhao', 'Qi Qin', 'Xinyue Li', 'Le Zhuo', 'Licheng Wen', 'Dongyang Liu', 'Yuewen Cao', 'Xiangchao Yan', 'Xin Li', 'Botian Shi', 'Tao Chen', 'Zhibo Chen', 'Lei Bai', 'Bo Zhang', 'Peng Gao'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.07089.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#long_context', '#cv', '#training', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Универсальное описание изображений для улучшения мультимодального ИИ', 'desc': 'OmniCaptioner - это универсальная система для создания подробных текстовых описаний различных визуальных данных. Она может работать с естественными изображениями, визуальным текстом и структурированными визуальными данными, преобразуя пиксельную информацию в семантически богатые текстовые представления. Система улучшает визуальное рассуждение с помощью языковых моделей, повышает качество генерации изображений и обеспечивает эффективное обучение с учителем. OmniCaptioner предлагает новый подход к преодолению разрыва между языковыми и визуальными модальностями.'}, 'en': {'title': 'Bridging Visuals and Text with OmniCaptioner', 'desc': 'OmniCaptioner is a new framework designed for generating detailed text descriptions from various types of images. Unlike previous methods that only work with specific image categories, it can handle natural images, visual text, and structured visuals all in one system. The framework transforms pixel data into meaningful text, connecting visual and textual information effectively. Its advantages include better reasoning with large language models, enhanced image generation capabilities, and faster training with less data.'}, 'zh': {'title': 'OmniCaptioner：视觉与文本的桥梁', 'desc': '我们提出了OmniCaptioner，这是一个多功能的视觉描述框架，能够在多种视觉领域生成细致的文本描述。与之前仅限于特定图像类型的方法不同，我们的框架提供了一个统一的解决方案，可以对自然图像、视觉文本（如海报、用户界面、教科书）和结构化视觉（如文档、表格、图表）进行描述。通过将低级像素信息转换为语义丰富的文本表示，我们的框架弥合了视觉和文本模态之间的差距。我们的研究结果显示，OmniCaptioner在视觉推理、图像生成和高效的监督微调方面具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2504.05541', 'title': 'Caption Anything in Video: Fine-grained Object-centric Captioning via\n  Spatiotemporal Multimodal Prompting', 'url': 'https://huggingface.co/papers/2504.05541', 'abstract': "We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects' attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at https://github.com/yunlong10/CAT-V", 'score': 9, 'issue_id': 3171, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '7e72ed48b045c60e', 'authors': ['Yunlong Tang', 'Jing Bi', 'Chao Huang', 'Susan Liang', 'Daiki Shimada', 'Hang Hua', 'Yunzhong Xiao', 'Yizhi Song', 'Pinxin Liu', 'Mingqian Feng', 'Junjia Guo', 'Zhuo Liu', 'Luchuan Song', 'Ali Vosoughi', 'Jinxi He', 'Liu He', 'Zeliang Zhang', 'Jiebo Luo', 'Chenliang Xu'], 'affiliations': ['CMU', 'Purdue University', 'Sony Group Corporation', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.05541.jpg', 'data': {'categories': ['#games', '#reasoning', '#multimodal', '#video', '#optimization', '#open_source', '#interpretability'], 'emoji': '🎥', 'ru': {'title': 'Умное описание любых объектов в видео без обучения', 'desc': 'CAT-V - это новая система для детального описания объектов в видео без дополнительного обучения. Она состоит из трех ключевых компонентов: сегментатора на основе SAMURAI, временного анализатора TRACE-Uni и генератора описаний InternVL-2.5. CAT-V использует пространственно-временные визуальные подсказки и рассуждения по цепочке для создания подробных описаний атрибутов, действий и взаимодействий объектов. Система поддерживает гибкое взаимодействие с пользователем и отслеживает состояния объектов во времени, обеспечивая точные и согласованные описания.'}, 'en': {'title': 'Fine-Grained Video Captioning Made Easy with CAT-V!', 'desc': 'CAT-V (Caption AnyThing in Video) is a novel framework designed for fine-grained video captioning that focuses on user-selected objects. It combines a Segmenter for precise object segmentation, a Temporal Analyzer for detecting event boundaries, and a Captioner for generating detailed descriptions. This framework operates without the need for additional training data, utilizing spatiotemporal visual prompts and chain-of-thought reasoning to create accurate, object-centric captions. By addressing the limitations of existing methods, CAT-V provides detailed descriptions that are both temporally aware and spatially accurate, enhancing user interaction with flexible visual prompts.'}, 'zh': {'title': '细致视频描述，精准物体捕捉', 'desc': 'CAT-V（视频中的任何物体描述）是一个无需训练的框架，专注于细粒度的物体中心视频描述。它结合了三个关键组件：基于SAMURAI的分割器用于精确的物体分割，TRACE-Uni驱动的时间分析器用于准确的事件边界检测，以及使用InternVL-2.5生成详细描述的描述器。通过时空视觉提示和链式思维推理，CAT-V能够生成对象属性、动作、状态、交互和环境上下文的详细描述，同时保持时间敏感性。该方法克服了现有视频描述方法的局限，提供了细致的物体特定描述，同时保持时间一致性和空间准确性。'}}}, {'id': 'https://huggingface.co/papers/2504.04010', 'title': 'DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion', 'url': 'https://huggingface.co/papers/2504.04010', 'abstract': "Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.", 'score': 7, 'issue_id': 3164, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 апреля', 'en': 'April 5', 'zh': '4月5日'}, 'hash': 'd006058dfff067dc', 'authors': ['Maksim Siniukov', 'Di Chang', 'Minh Tran', 'Hongkun Gong', 'Ashutosh Chaubey', 'Mohammad Soleymani'], 'affiliations': ['University of Southern California Los Angeles, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.04010.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#video', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'DiTaiListener: революция в генерации реалистичных реакций слушателя', 'desc': 'Статья представляет DiTaiListener - новый метод генерации естественных движений слушателя в длительных диалогах с использованием видео-диффузионной модели. DiTaiListener состоит из двух компонентов: DiTaiListener-Gen для создания коротких сегментов реакций слушателя и DiTaiListener-Edit для плавного объединения этих сегментов. Модель использует адаптер CTM для обработки аудио- и визуальных сигналов говорящего, обеспечивая согласованность во времени. Количественные и качественные оценки показывают превосходство DiTaiListener над существующими методами в фотореалистичности и выразительности движений.'}, 'en': {'title': 'DiTaiListener: Realistic Listener Motions for Engaging Interactions', 'desc': "The paper presents DiTaiListener, a novel approach for generating realistic listener motions during extended interactions. It utilizes a video diffusion model that incorporates multimodal conditions, allowing for the generation of listener responses based on the speaker's speech and facial movements. The method consists of two main components: DiTaiListener-Gen, which creates short segments of listener responses, and DiTaiListener-Edit, which refines these segments for smooth transitions. The results show that DiTaiListener outperforms existing methods in both visual quality and motion representation, achieving state-of-the-art performance on benchmark datasets and receiving positive feedback from user studies."}, 'zh': {'title': '自然互动中的听众动作生成新突破', 'desc': '本文介绍了一种名为DiTaiListener的模型，旨在生成自然且细腻的听众动作，以改善长时间互动中的表现。该模型利用视频扩散模型和多模态条件，首先生成基于说话者语音和面部动作的短段听众反应。接着，通过DiTaiListener-Edit对过渡帧进行精细化处理，以确保视频的平滑过渡。实验结果表明，DiTaiListener在视觉真实感和动作表现上均达到了最先进的性能，用户研究也显示其在反馈、多样性和流畅性方面明显优于其他竞争模型。'}}}, {'id': 'https://huggingface.co/papers/2504.06958', 'title': 'VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2504.06958', 'abstract': 'Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.', 'score': 6, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'b88359823eee79f5', 'authors': ['Xinhao Li', 'Ziang Yan', 'Desen Meng', 'Lu Dong', 'Xiangyu Zeng', 'Yinan He', 'Yali Wang', 'Yu Qiao', 'Yi Wang', 'Limin Wang'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.06958.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#video', '#multimodal', '#reasoning'], 'emoji': '🎥', 'ru': {'title': 'RFT: Прорыв в пространственно-временном восприятии видео для MLLM', 'desc': 'Статья представляет систематическое исследование применения метода Reinforcement Fine-Tuning (RFT) с использованием Group Relative Policy Optimization (GRPO) для улучшения пространственно-временного восприятия в мультимодальных больших языковых моделях (MLLM) для видео. Авторы разработали VideoChat-R1 - мощную видео MLLM, достигающую передовых результатов в задачах пространственно-временного восприятия без ущерба для способности к диалогу. По сравнению с Qwen2.5-VL-7B, VideoChat-R1 значительно улучшает производительность в таких задачах, как временная локализация и отслеживание объектов. Исследование подчеркивает потенциал RFT для специализированного улучшения видео MLLM в конкретных задачах.'}, 'en': {'title': 'Enhancing Video Understanding with Reinforcement Fine-Tuning', 'desc': "This paper explores the use of Reinforcement Fine-Tuning (RFT) combined with Group Relative Policy Optimization (GRPO) to improve video understanding in multimodal large language models (MLLMs). The authors demonstrate that RFT is effective in enhancing spatio-temporal perception while retaining the model's general capabilities. Their experiments show that the newly developed VideoChat-R1 model significantly outperforms existing models in tasks like temporal grounding and object tracking, achieving state-of-the-art results. The findings highlight the efficiency of RFT for specialized improvements in video MLLMs, paving the way for future research in this area."}, 'zh': {'title': '强化学习助力视频理解的突破', 'desc': '本论文探讨了强化学习在多模态大语言模型（MLLMs）中的应用，特别是视频理解方面。我们提出了一种系统的强化微调（RFT）方法，结合了群体相对策略优化（GRPO），以增强视频模型的时空感知能力。实验结果表明，RFT在特定任务的改进上具有很高的数据效率，开发了名为VideoChat-R1的强大视频MLLM。该模型在时空感知任务上表现出色，同时保持了良好的对话能力，显著提升了多个基准测试的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.07092', 'title': 'Are We Done with Object-Centric Learning?', 'url': 'https://huggingface.co/papers/2504.07092', 'abstract': 'Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called Object-Centric Classification with Applied Masks (OCCAM), demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available https://github.com/AlexanderRubinstein/OCCAM{here}.', 'score': 5, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '8e182461a15aee14', 'authors': ['Alexander Rubinstein', 'Ameya Prabhu', 'Matthias Bethge', 'Seong Joon Oh'], 'affiliations': ['Tubingen AI Center, University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2504.07092.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#transfer_learning', '#cv', '#training', '#science'], 'emoji': '🔍', 'ru': {'title': 'Революция в объектно-центрическом обучении: от слотов к сегментации', 'desc': 'Статья посвящена объектно-центрическому обучению (OCL) в машинном обучении, которое стремится создавать представления, кодирующие только отдельные объекты. Авторы предлагают новый метод OCCAM, основанный на сегментации и кодировании отдельных объектов, который превосходит существующие подходы OCL. Исследование демонстрирует, что такой подход значительно улучшает обобщение вне распределения (OOD) и справляется с проблемой ложных фоновых признаков. Статья открывает новые перспективы для практического применения OCL и изучения фундаментальных вопросов восприятия объектов.'}, 'en': {'title': 'Unlocking Object-Centric Learning for Better Generalization', 'desc': 'This paper discusses Object-Centric Learning (OCL), which aims to create representations that focus solely on individual objects, ignoring other elements in a scene. The authors highlight advancements in segmentation models that allow for effective separation of objects at the pixel level, leading to improved performance in out-of-distribution (OOD) object discovery tasks. They introduce a new method called Object-Centric Classification with Applied Masks (OCCAM), which shows that this segmentation approach outperforms traditional slot-based methods. The paper also addresses ongoing challenges in applying these techniques to real-world scenarios and emphasizes the importance of understanding object perception in human cognition.'}, 'zh': {'title': '实现对象中心表示的突破', 'desc': '对象中心学习（OCL）旨在学习仅编码对象的表示，独立于场景中的其他对象或背景线索。这种方法支持多种目标，包括分布外（OOD）泛化、样本高效组合和结构化环境建模。我们提出了一种新的无训练探测器，称为应用掩码的对象中心分类（OCCAM），显示出基于分割的个体对象编码显著优于基于槽的OCL方法。尽管在实际应用中仍面临挑战，但我们为OCL社区提供了可扩展的对象中心表示工具箱。'}}}, {'id': 'https://huggingface.co/papers/2504.06719', 'title': 'Masked Scene Modeling: Narrowing the Gap Between Supervised and\n  Self-Supervised Learning in 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2504.06719', 'abstract': 'Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (https://github.com/phermosilla/msm).', 'score': 5, 'issue_id': 3168, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '082fef36edfa1a3f', 'authors': ['Pedro Hermosilla', 'Christian Stippel', 'Leon Sick'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.06719.jpg', 'data': {'categories': ['#benchmark', '#training', '#cv', '#self_supervised', '#open_source', '#3d'], 'emoji': '🧠', 'ru': {'title': 'Революция в самообучении 3D: от изображений к пониманию сцен', 'desc': 'Эта статья представляет новый подход к самообучению в области 3D компьютерного зрения. Авторы предлагают протокол оценки качества самообучаемых признаков для понимания 3D сцен, используя многоуровневую выборку признаков. Они также вводят новую модель самообучения, основанную на задаче Masked Scene Modeling, которая реконструирует глубокие признаки маскированных участков. Эксперименты показывают, что их метод достигает производительности, сопоставимой с supervised моделями, и значительно превосходит существующие self-supervised подходы.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with Self-Supervised Learning', 'desc': 'This paper presents a new self-supervised learning approach for 3D scene understanding that enhances feature extraction without relying on labeled data. It introduces a robust evaluation protocol that assesses the quality of self-supervised features using multi-resolution sampling of hierarchical models. The authors propose a novel Masked Scene Modeling objective that reconstructs features from masked patches, allowing the model to learn effectively in a 3D context. Their experiments show that this method not only matches the performance of supervised models but also significantly outperforms existing self-supervised techniques.'}, 'zh': {'title': '自监督学习助力三维场景理解', 'desc': '自监督学习在二维计算机视觉中取得了显著进展，使得在大型未标注数据集上训练的模型能够提供类似于有标签模型的多功能特征。然而，在三维场景理解中，自监督方法通常仅用于权重初始化，限制了其在通用特征提取中的应用。本文提出了一种专门评估自监督特征质量的稳健评估协议，利用多分辨率特征采样创建丰富的点级表示。我们还介绍了第一个在仅使用自监督特征的线性探测设置中表现与监督模型相似的自监督模型，展示了其在三维场景理解中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.05523', 'title': 'Pretraining Language Models for Diachronic Linguistic Change Discovery', 'url': 'https://huggingface.co/papers/2504.05523', 'abstract': 'Large language models (LLMs) have shown potential as tools for scientific discovery. This has engendered growing interest in their use in humanistic disciplines, such as historical linguistics and literary studies. These fields often construct arguments on the basis of delineations like genre, or more inflexibly, time period. Although efforts have been made to restrict inference to specific domains via fine-tuning or model editing, we posit that the only true guarantee is domain-restricted pretraining -- typically, a data- and compute-expensive proposition.   We show that efficient pretraining techniques can produce useful models over corpora too large for easy manual inspection but too small for "typical" LLM approaches. We employ a novel date-attribution pipeline in order to obtain a temporally-segmented dataset of five 10-million-word slices. We train two corresponding five-model batteries over these corpus segments, efficient pretraining and Llama3-8B parameter efficiently finetuned.   We find that the pretrained models are faster to train than the finetuned baselines and that they better respect the historical divisions of our corpus. Emphasizing speed and precision over a-historical comprehensiveness enables a number of novel approaches to hypothesis discovery and testing in our target fields. Taking up diachronic linguistics as a testbed, we show that our method enables the detection of a diverse set of phenomena, including en masse lexical change, non-lexical (grammatical and morphological) change, and word sense introduction/obsolescence. We provide a ready-to-use pipeline that allows extension of our approach to other target fields with only minimal adaptation.', 'score': 4, 'issue_id': 3170, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '9bdf5f1c61fd329f', 'authors': ['Elisabeth Fittschen', 'Sabrina Li', 'Tom Lippincott', 'Leshem Choshen', 'Craig Messner'], 'affiliations': ['Center for Digital Humanities, Johns Hopkins University, USA', 'IBM Research, MIT, USA', 'University of Hamburg, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.05523.jpg', 'data': {'categories': ['#dataset', '#data', '#transfer_learning', '#science', '#training'], 'emoji': '📚', 'ru': {'title': 'Эффективное предобучение языковых моделей для исторической лингвистики', 'desc': 'Эта статья описывает использование эффективных методов предобучения для создания языковых моделей на основе небольших, исторически сегментированных корпусов текстов. Авторы разработали пайплайн для атрибуции дат и создали набор из пяти моделей, каждая обученная на 10 миллионах слов из определенного временного периода. Сравнение с базовыми моделями, дообученными на основе Llama3-8B, показало, что предобученные модели лучше учитывают исторические особенности корпуса. Этот подход позволяет обнаруживать различные лингвистические явления, включая лексические и грамматические изменения, а также появление и устаревание значений слов.'}, 'en': {'title': 'Efficient Pretraining for Targeted Language Models in Humanities', 'desc': 'This paper explores the use of large language models (LLMs) for scientific discovery in humanistic disciplines like historical linguistics and literary studies. The authors argue that domain-restricted pretraining is essential for effective model performance, especially when dealing with specific genres or time periods. They introduce efficient pretraining techniques that allow for the training of models on large datasets that are otherwise too small for traditional LLM methods. Their findings demonstrate that these pretrained models not only train faster but also better adhere to historical context, facilitating new approaches to hypothesis testing in diachronic linguistics.'}, 'zh': {'title': '高效预训练：推动人文学科的科学发现', 'desc': '大型语言模型（LLMs）在科学发现中展现了潜力，尤其是在历史语言学和文学研究等人文学科中。这些领域通常基于体裁或时间段等划分来构建论点。尽管通过微调或模型编辑来限制推理到特定领域的努力已经取得了一定进展，但我们认为，唯一真正的保证是领域限制的预训练，这通常需要大量的数据和计算资源。我们的研究表明，采用高效的预训练技术可以在数据量过大而难以手动检查但又不足以使用“典型”LLM方法的语料库上，产生有用的模型。'}}}, {'id': 'https://huggingface.co/papers/2504.06947', 'title': 'RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts', 'url': 'https://huggingface.co/papers/2504.06947', 'abstract': 'In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts.', 'score': 3, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '00cf6cc0a0c3d1c7', 'authors': ['Natalia Loukachevitch', 'Natalia Tkachenko', 'Anna Lapanitsyna', 'Mikhail Tikhomirov', 'Nicolay Rusnachenko'], 'affiliations': ['Bauman Moscow State Technical University', 'Lomonosov Moscow State University'], 'pdf_title_img': 'assets/pdf/title_img/2504.06947.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#training', '#multilingual', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Извлечение мнений из текста: соревнование по оценке диалогов', 'desc': 'Эта статья представляет задачу извлечения структурированных мнений из русскоязычных новостных текстов. Участники соревнования должны были извлекать кортежи мнений, состоящие из источника мнения, его цели, выражения и сентимента. Большинство участников экспериментировали с большими языковыми моделями в форматах zero-shot, few-shot и fine-tuning. Лучший результат на тестовом наборе был получен с помощью дообучения большой языковой модели.'}, 'en': {'title': 'Extracting Structured Opinions from Russian News Using Language Models', 'desc': 'This paper presents a shared task focused on extracting structured opinions from Russian news articles. The goal is to identify opinion tuples that include a sentiment holder, target, expression, and sentiment direction. Over 100 submissions were received, with participants primarily utilizing large language models in various training formats such as zero-shot, few-shot, and fine-tuning. The best performance on the test set was achieved through fine-tuning a large language model, and the study also evaluated different prompts and open-source models to determine the most effective combinations.'}, 'zh': {'title': '从新闻文本中提取结构化意见的挑战', 'desc': '本文介绍了一个关于从俄语新闻文本中提取结构化意见的对话评估共享任务。该任务要求参赛者为给定句子提取意见元组，这些元组由情感持有者、目标、表达和情感组成。总共收到了超过100个提交，参与者主要使用大型语言模型进行零样本、少样本和微调实验。测试集上最佳结果是通过对大型语言模型进行微调获得的，同时我们还比较了30个提示和11个开源语言模型，参数范围从3亿到32亿，找出了最佳模型和提示。'}}}, {'id': 'https://huggingface.co/papers/2504.03886', 'title': 'WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments', 'url': 'https://huggingface.co/papers/2504.03886', 'abstract': "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system designed to handle dynamic environments by leveraging uncertainty-aware geometric mapping. Unlike traditional SLAM systems, which assume static scenes, our approach integrates depth and uncertainty information to enhance tracking, mapping, and rendering performance in the presence of moving objects. We introduce an uncertainty map, predicted by a shallow multi-layer perceptron and DINOv2 features, to guide dynamic object removal during both tracking and mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map optimization, improving reconstruction accuracy. Our system is evaluated on multiple datasets and demonstrates artifact-free view synthesis. Results showcase WildGS-SLAM's superior performance in dynamic environments compared to state-of-the-art methods.", 'score': 3, 'issue_id': 3167, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '086d82b3ec2f4b04', 'authors': ['Jianhao Zheng', 'Zihan Zhu', 'Valentin Bieri', 'Marc Pollefeys', 'Songyou Peng', 'Iro Armeni'], 'affiliations': ['ETH Zürich', 'Microsoft', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03886.jpg', 'data': {'categories': ['#benchmark', '#cv', '#3d'], 'emoji': '🌪️', 'ru': {'title': 'Робастный SLAM для динамического мира', 'desc': 'WildGS-SLAM - это устойчивая и эффективная система монокулярного RGB SLAM, предназначенная для работы в динамических средах. Она использует геометрическое картирование с учетом неопределенности, интегрируя информацию о глубине и неопределенности для улучшения отслеживания, картирования и рендеринга в присутствии движущихся объектов. Система вводит карту неопределенности, предсказанную с помощью неглубокого многослойного перцептрона и признаков DINOv2, для удаления динамических объектов. WildGS-SLAM демонстрирует превосходную производительность в динамических средах по сравнению с современными методами.'}, 'en': {'title': 'Dynamic SLAM Redefined: Embracing Uncertainty for Robust Mapping', 'desc': 'WildGS-SLAM is a monocular RGB SLAM system that effectively operates in dynamic environments by incorporating uncertainty-aware geometric mapping. It differs from traditional SLAM systems by utilizing depth and uncertainty data to improve tracking and mapping when moving objects are present. The system employs an uncertainty map, generated by a shallow multi-layer perceptron and DINOv2 features, to facilitate the removal of dynamic objects during the SLAM process. Evaluations on various datasets reveal that WildGS-SLAM achieves high reconstruction accuracy and artifact-free view synthesis, outperforming existing methods in dynamic settings.'}, 'zh': {'title': '动态环境下的高效SLAM解决方案', 'desc': 'WildGS-SLAM是一种强大且高效的单目RGB SLAM系统，专为处理动态环境而设计。与传统的SLAM系统假设静态场景不同，我们的方法结合了深度和不确定性信息，以提高在移动物体存在下的跟踪、映射和渲染性能。我们引入了一种不确定性地图，通过浅层多层感知器和DINOv2特征进行预测，以指导动态物体的去除，从而增强密集束调整和高斯地图优化，提升重建精度。我们的系统在多个数据集上进行了评估，结果显示WildGS-SLAM在动态环境中的表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2504.05410', 'title': 'Fast Controlled Generation from Language Models with Adaptive Weighted\n  Rejection Sampling', 'url': 'https://huggingface.co/papers/2504.05410', 'abstract': "The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed 100,000 tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.", 'score': 0, 'issue_id': 3176, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '39edf52d9587732f', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#training', '#optimization', '#data'], 'emoji': '🔬', 'ru': {'title': 'Эффективная генерация текста с ограничениями для языковых моделей', 'desc': 'Статья представляет новый алгоритм для генерации текста языковыми моделями с учетом ограничений. Алгоритм использует адаптивную выборку с отклонением для эффективной оценки ограничений на небольшом подмножестве словаря. Также предлагается метод получения несмещенных оценок весов важности для коррекции близорукого поведения при локальном применении ограничений. Эмпирические эксперименты в различных доменах показывают превосходство предложенного подхода над современными базовыми методами.'}, 'en': {'title': 'Efficient Constraint Handling in Language Model Generation', 'desc': 'This paper presents a new algorithm for generating sequences from language models while adhering to specific constraints. The traditional method, locally constrained decoding (LCD), is inefficient because it evaluates constraints on a large vocabulary at each step, which can be computationally expensive. The proposed adaptive rejection sampling algorithm reduces the number of constraint evaluations significantly and provides unbiased estimates of importance weights. Empirical results demonstrate that this approach outperforms existing methods in various applications, enhancing both performance and efficiency.'}, 'zh': {'title': '高效的约束条件生成算法', 'desc': '本文提出了一种新的算法，用于在语言模型生成过程中处理约束条件。传统的局部约束解码方法在每一步都需要评估整个词汇表，效率低下且可能导致全局分布失真。我们提出的自适应拒绝采样算法显著减少了约束评估的次数，并且可以生成低方差的无偏重要性权重估计。通过在多个领域的实证评估，我们的方法在支持更广泛的约束条件和提高运行时间及性能方面优于现有的最先进基线。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.05287', 'title': 'RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception', 'url': 'https://huggingface.co/papers/2504.05287', 'abstract': 'Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/', 'score': 0, 'issue_id': 3165, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'fab0fa176a952987', 'authors': ['Hui Zhang', 'Zijian Wu', 'Linyi Huang', 'Sammy Christen', 'Jie Song'], 'affiliations': ['ETH Zurich, Switzerland', 'HKUST (Guangzhou), China', 'HKUST, Hong Kong (China)'], 'pdf_title_img': 'assets/pdf/title_img/2504.05287.jpg', 'data': {'categories': ['#robotics', '#training', '#games', '#rl', '#optimization'], 'emoji': '🦾', 'ru': {'title': 'Адаптивный захват объектов роботом по одному изображению', 'desc': 'Статья представляет метод обучения с подкреплением для захвата разнообразных объектов роботизированной рукой на основе одного изображения. Авторы используют особое представление объекта, ориентированное на взаимодействие с рукой, что повышает устойчивость к вариациям формы. Предложена стратегия смешанного обучения, сочетающая имитационное обучение и обучение с подкреплением для адаптации к внешним помехам. Эксперименты показывают высокую обобщающую способность метода при захвате новых объектов в симуляции и реальности.'}, 'en': {'title': 'Dynamic Grasping: Robots That Adapt and Overcome!', 'desc': "This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot's ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions."}, 'zh': {'title': '实现灵巧抓取的零-shot学习新方法', 'desc': '本论文提出了一种基于强化学习的框架，旨在实现从单视角感知中对各种未知物体的零-shot动态灵巧抓取。与以往依赖完全可观察物体或专家演示的方法不同，该方法能够适应外部干扰并进行自适应动作。我们采用以手为中心的物体表示法，提取与交互相关的局部形状特征，从而增强对形状变化和不确定性的鲁棒性。实验结果表明，该方法在抓取未知物体时具有强大的泛化能力，成功率高达97.0%。'}}}, {'id': 'https://huggingface.co/papers/2504.13161', 'title': 'CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training', 'url': 'https://huggingface.co/papers/2504.13161', 'abstract': 'Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The Pile is labor-intensive. Consequently, identifying an optimal pre-training data mixture remains a challenging problem, despite its significant benefits for pre-training performance. To address these challenges, we propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an automated framework that discovers, evaluates, and refines data mixtures in a pre-training setting. Specifically, CLIMB embeds and clusters large-scale datasets in a semantic space and then iteratively searches for optimal mixtures using a smaller proxy model and a predictor. When continuously trained on 400B tokens with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific domain (e.g., Social Sciences) yields a 5% improvement over random sampling. Finally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20 clusters as a research playground, and ClimbMix, a compact yet powerful 400-billion-token dataset designed for efficient pre-training that delivers superior performance under an equal token budget. We analyze the final data mixture, elucidating the characteristics of an optimal data mixture. Our data is available at: https://research.nvidia.com/labs/lpr/climb/', 'score': 81, 'issue_id': 3306, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'cf9b6f2a06448097', 'authors': ['Shizhe Diao', 'Yu Yang', 'Yonggan Fu', 'Xin Dong', 'Dan Su', 'Markus Kliegl', 'Zijia Chen', 'Peter Belcak', 'Yoshi Suhara', 'Hongxu Yin', 'Mostofa Patwary', 'Yingyan', 'Lin', 'Jan Kautz', 'Pavlo Molchanov'], 'affiliations': ['Georgia Institute of Technology, USA', 'NVIDIA', 'OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2504.13161.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#optimization', '#data'], 'emoji': '🧗', 'ru': {'title': 'Восхождение к оптимальным данным для предобучения языковых моделей', 'desc': 'Статья представляет CLIMB - автоматизированный фреймворк для обнаружения, оценки и улучшения смесей данных для предварительного обучения языковых моделей. CLIMB использует кластеризацию в семантическом пространстве и итеративный поиск оптимальных смесей с помощью прокси-модели. Применение CLIMB позволило создать модель, превосходящую Llama-3.2-1B на 2%, а также улучшить результаты для конкретных доменов. Авторы также представляют ClimbLab и ClimbMix - наборы данных для исследований и эффективного предобучения.'}, 'en': {'title': 'Optimizing Pre-Training Data with CLIMB for Superior Model Performance', 'desc': 'This paper introduces CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), a novel framework for optimizing pre-training datasets in machine learning. CLIMB automates the process of discovering and refining data mixtures by embedding and clustering large datasets in a semantic space, which helps in identifying the best combinations for training models. The results show that a model trained on a carefully optimized mixture of 400 billion tokens outperforms existing models, demonstrating the importance of domain-specific data selection. Additionally, the paper presents ClimbLab and ClimbMix, two new datasets designed to facilitate research and improve pre-training efficiency.'}, 'zh': {'title': '优化预训练数据的智能框架', 'desc': '本论文提出了一种名为CLIMB的自动化框架，用于优化预训练数据的混合。CLIMB通过在语义空间中嵌入和聚类大规模数据集，迭代搜索最佳数据组合。实验表明，使用这种混合数据进行训练的模型在性能上超过了现有的最先进模型，并且针对特定领域的优化可以显著提高效果。最后，我们还推出了ClimbLab和ClimbMix两个数据集，以支持进一步的研究和高效的预训练。'}}}, {'id': 'https://huggingface.co/papers/2504.13146', 'title': 'Antidistillation Sampling', 'url': 'https://huggingface.co/papers/2504.13146', 'abstract': "Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.", 'score': 55, 'issue_id': 3304, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '6aa117b5c441eb3d', 'authors': ['Yash Savani', 'Asher Trockman', 'Zhili Feng', 'Avi Schwarzschild', 'Alexander Robey', 'Marc Finzi', 'J. Zico Kolter'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13146.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training'], 'emoji': '🛡️', 'ru': {'title': 'Защита моделей от дистилляции с помощью антидистилляции', 'desc': 'В статье рассматривается проблема защиты моделей от дистилляции, когда модели генерируют сложные последовательности токенов. Авторы предлагают метод антидистилляционной выборки, который изменяет распределение вероятностей следующего токена. Это делает следы рассуждений менее полезными для дистилляции, сохраняя при этом производительность модели. Таким образом, можно защитить интеллектуальную собственность моделей, не ухудшая их работу.'}, 'en': {'title': 'Protecting Model Knowledge with Antidistillation Sampling', 'desc': "This paper discusses a method called antidistillation sampling, which aims to protect advanced models from being easily distilled into simpler versions. Distillation is a process where a complex model's knowledge is transferred to a smaller model, but this can be exploited if the complex model generates detailed reasoning traces. Antidistillation sampling works by altering the probability distribution of the next token generated by the model, making the reasoning less useful for distillation. This approach allows the model to maintain its performance while reducing the risk of its knowledge being easily extracted."}, 'zh': {'title': '抗蒸馏采样：保护模型性能的创新策略', 'desc': '前沿模型生成的推理轨迹会产生丰富的标记序列，这些序列可以帮助模型蒸馏。为了应对这一漏洞，模型拥有者可能会寻找采样策略，以限制蒸馏的有效性，同时不影响模型性能。抗蒸馏采样正是提供这种能力的策略。通过战略性地修改模型的下一个标记概率分布，抗蒸馏采样可以破坏推理轨迹，使其在蒸馏中变得不那么有效，同时保持模型的实际效用。'}}}, {'id': 'https://huggingface.co/papers/2504.13169', 'title': 'Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling', 'url': 'https://huggingface.co/papers/2504.13169', 'abstract': 'Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io.', 'score': 36, 'issue_id': 3304, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'f1ebb64cfce24e47', 'authors': ['Tsung-Han Wu', 'Heekyung Lee', 'Jiaxin Ge', 'Joseph E. Gonzalez', 'Trevor Darrell', 'David M. Chan'], 'affiliations': ['POSTECH', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.13169.jpg', 'data': {'categories': ['#dataset', '#inference', '#hallucinations', '#data', '#cv'], 'emoji': '👁️', 'ru': {'title': 'REVERSE: самокорректирующиеся VLM без галлюцинаций', 'desc': 'Исследователи представили REVERSE - новый подход к снижению визуальных галлюцинаций в моделях компьютерного зрения и обработки естественного языка (VLM). Метод объединяет обучение с учетом галлюцинаций и самопроверку в режиме реального времени. REVERSE использует новый датасет из 1,3 млн полусинтетических образцов и технику ретроспективной выборки во время вывода. Эксперименты показали, что REVERSE превосходит существующие методы на 12-28% по снижению галлюцинаций на стандартных бенчмарках.'}, 'en': {'title': 'REVERSE: Correcting Visual Hallucinations in VLMs Dynamically', 'desc': 'This paper addresses the issue of visual hallucinations in Vision-Language Models (VLMs), where models incorrectly describe non-existent elements. The authors propose a new framework called REVERSE, which combines hallucination-aware training with real-time self-verification to improve the accuracy of VLM outputs. By utilizing a large dataset of semi-synthetic samples, REVERSE can identify and correct hallucinations during the generation process. The results demonstrate that this approach significantly reduces hallucinations, outperforming existing methods in various benchmarks.'}, 'zh': {'title': 'REVERSE：动态修正视觉幻觉的统一框架', 'desc': '视觉语言模型（VLMs）在视觉理解方面表现出色，但常常出现视觉幻觉，即生成不存在的物体、动作或概念的描述，这在安全关键应用中带来了重大风险。现有的幻觉缓解方法通常分为两类：生成调整和后期验证。生成调整方法依赖启发式规则，缺乏有效的修正机制，而后期验证则复杂，通常需要多个模型，并倾向于拒绝输出而不是进行修正。我们提出的REVERSE框架结合了幻觉感知训练和实时自我验证，能够在生成过程中检测幻觉并动态修正，从而显著降低幻觉的发生。'}}}, {'id': 'https://huggingface.co/papers/2504.12626', 'title': 'Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation', 'url': 'https://huggingface.co/papers/2504.12626', 'abstract': 'We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. The FramePack compresses input frames to make the transformer context length a fixed number regardless of the video length. As a result, we are able to process a large number of frames using video diffusion with computation bottleneck similar to image diffusion. This also makes the training video batch sizes significantly higher (batch sizes become comparable to image diffusion training). We also propose an anti-drifting sampling method that generates frames in inverted temporal order with early-established endpoints to avoid exposure bias (error accumulation over iterations). Finally, we show that existing video diffusion models can be finetuned with FramePack, and their visual quality may be improved because the next-frame prediction supports more balanced diffusion schedulers with less extreme flow shift timesteps.', 'score': 36, 'issue_id': 3303, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'fd1688a4e26dbb32', 'authors': ['Lvmin Zhang', 'Maneesh Agrawala'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.12626.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#video', '#training'], 'emoji': '🎞️', 'ru': {'title': 'FramePack: эффективное предсказание кадров для генерации видео', 'desc': 'FramePack - это новая структура нейронной сети для предсказания следующего кадра в видео. Она сжимает входные кадры, что позволяет обрабатывать большое количество кадров с вычислительной сложностью, сравнимой с диффузией изображений. Авторы также предлагают метод сэмплирования, предотвращающий накопление ошибок. FramePack может быть использован для дообучения существующих моделей видеодиффузии, потенциально улучшая их визуальное качество.'}, 'en': {'title': 'FramePack: Efficient Video Generation with Next-Frame Prediction', 'desc': 'The paper introduces FramePack, a novel neural network architecture designed for predicting the next frame in video generation. By compressing input frames, FramePack ensures that the transformer can handle a fixed context length, making it efficient for processing long videos. This approach allows for larger training batch sizes, similar to those used in image diffusion, while maintaining computational efficiency. Additionally, the authors present an anti-drifting sampling method to mitigate exposure bias, enhancing the quality of generated frames and improving existing video diffusion models through fine-tuning.'}, 'zh': {'title': 'FramePack：提升视频生成的下一帧预测能力', 'desc': '我们提出了一种神经网络结构，FramePack，用于训练视频生成的下一帧预测模型。FramePack通过压缩输入帧，使得变换器的上下文长度固定，无论视频长度如何。这样，我们能够使用与图像扩散相似的计算瓶颈处理大量帧，从而显著提高视频训练的批量大小。我们还提出了一种反漂移采样方法，以避免迭代过程中的曝光偏差，从而提高生成帧的质量。'}}}, {'id': 'https://huggingface.co/papers/2504.12369', 'title': 'WORLDMEM: Long-term Consistent World Simulation with Memory', 'url': 'https://huggingface.co/papers/2504.12369', 'abstract': 'World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing a memory attention mechanism that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.', 'score': 28, 'issue_id': 3303, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '79cf162a1b60f887', 'authors': ['Zeqi Xiao', 'Yushi Lan', 'Yifan Zhou', 'Wenqi Ouyang', 'Shuai Yang', 'Yanhong Zeng', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai AI Laboratory', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.12369.jpg', 'data': {'categories': ['#multimodal', '#3d', '#long_context'], 'emoji': '🌐', 'ru': {'title': 'WorldMem: Улучшение долгосрочной согласованности в симуляции миров с помощью памяти', 'desc': 'WorldMem - это фреймворк для улучшения симуляции виртуальных миров с помощью банка памяти. Он использует механизм внимания для извлечения релевантной информации из кадров памяти на основе их состояний. Это позволяет точно реконструировать ранее наблюдаемые сцены даже при значительных изменениях ракурса или временных промежутках. Включение временных меток в состояния позволяет моделировать не только статичный мир, но и его динамическое развитие во времени.'}, 'en': {'title': 'Enhancing World Simulation with Memory-Driven Consistency', 'desc': 'This paper introduces WorldMem, a novel framework designed to improve world simulation by utilizing a memory bank that stores various memory frames and states. The framework addresses the challenge of maintaining long-term consistency in 3D spatial representations, which is often hindered by limited temporal context. By implementing a memory attention mechanism, WorldMem can retrieve relevant information from stored memory frames, allowing for accurate scene reconstruction despite changes in viewpoint or time. Additionally, the integration of timestamps enables the framework to model both static and dynamic aspects of the environment, enhancing interaction and perception in simulated worlds.'}, 'zh': {'title': '增强世界模拟的一致性与动态性', 'desc': '世界模拟因其建模虚拟环境和预测行为后果的能力而越来越受欢迎。然而，有限的时间上下文窗口常常导致长期一致性维护的失败，特别是在保持三维空间一致性方面。在这项工作中，我们提出了WorldMem框架，通过一个包含记忆单元的记忆库来增强场景生成，这些记忆单元存储记忆帧和状态（例如，姿势和时间戳）。通过采用记忆注意机制，我们的方法能够准确重建先前观察到的场景，即使在显著的视角或时间间隔下也能有效提取相关信息。'}}}, {'id': 'https://huggingface.co/papers/2504.12322', 'title': 'A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis', 'url': 'https://huggingface.co/papers/2504.12322', 'abstract': 'While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA.', 'score': 25, 'issue_id': 3307, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': 'cc80e8015cf7f78d', 'authors': ['Xin Gao', 'Qizhi Pei', 'Zinan Tang', 'Yu Li', 'Honglin Lin', 'Jiang Wu', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2504.12322.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#synthetic', '#small_models', '#open_source'], 'emoji': '🤝', 'ru': {'title': 'Коллаборативный синтез данных: малые модели, большие результаты', 'desc': 'Статья представляет новый подход к синтезу данных с использованием малых языковых моделей (LLM) вместо крупных. Авторы предлагают фреймворк GRA, в котором несколько малых LLM выполняют роли Генератора, Рецензента и Арбитра, имитируя процесс экспертной оценки. Этот метод позволяет достичь качества данных, сравнимого с результатами крупных LLM, но с меньшими вычислительными затратами. Эксперименты показывают, что данные, созданные GRA, не уступают или превосходят по качеству выходные данные крупных LLM, таких как Qwen-2.5-72B-Instruct.'}, 'en': {'title': 'Collaborative Small Models for High-Quality Data Synthesis', 'desc': 'This paper introduces a framework called GRA, which uses multiple small language models (LLMs) to improve data synthesis and distillation. Instead of relying on a single large LLM, GRA assigns specialized roles to smaller models: a Generator creates data samples, a Reviewer evaluates their quality, and an Adjudicator resolves any discrepancies. This collaborative approach mimics peer review processes, allowing for iterative refinement and quality control. The results show that the data produced by GRA can match or even surpass the quality of data generated by large LLMs, suggesting that smaller models can be effectively coordinated for high-quality outcomes.'}, 'zh': {'title': '小模型协作，超越大模型的高质量数据合成', 'desc': '本文提出了一种名为GRA的框架，旨在通过多个小型语言模型（LLMs）协作来生成高质量的数据。该框架模拟了同行评审的过程，分配了生成器、审阅者和裁决者等不同角色，以实现数据合成的迭代优化和质量控制。通过将合成过程分解为专门的子任务，GRA能够在数据质量上与大型语言模型相媲美。实验结果表明，GRA生成的数据在质量上与单一大型模型的输出相当或更优，挑战了依赖大型模型进行高质量数据合成的必要性。'}}}, {'id': 'https://huggingface.co/papers/2504.13181', 'title': 'Perception Encoder: The best visual embeddings are not at the output of\n  the network', 'url': 'https://huggingface.co/papers/2504.13181', 'abstract': 'We introduce Perception Encoder (PE), a state-of-the-art encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods, language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together with the core contrastive checkpoint, our PE family of models achieves state-of-the-art performance on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, depth estimation, and tracking. To foster further research, we are releasing our models, code, and a novel dataset of synthetically and human-annotated videos.', 'score': 20, 'issue_id': 3313, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '3c5cea92d3dd07c4', 'authors': ['Daniel Bolya', 'Po-Yao Huang', 'Peize Sun', 'Jang Hyun Cho', 'Andrea Madotto', 'Chen Wei', 'Tengyu Ma', 'Jiale Zhi', 'Jathushan Rajasegaran', 'Hanoona Rasheed', 'Junke Wang', 'Marco Monteiro', 'Hu Xu', 'Shiyu Dong', 'Nikhila Ravi', 'Daniel Li', 'Piotr Dollár', 'Christoph Feichtenhofer'], 'affiliations': ['Fudan University', 'MBZUAI', 'Meta FAIR', 'Meta Reality Labs', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2504.13181.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#dataset', '#alignment', '#open_source', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Универсальный энкодер для понимания изображений и видео', 'desc': 'Исследователи представляют Perception Encoder (PE) - современный энкодер для понимания изображений и видео, обученный с помощью простого обучения на основе зрения и языка. В отличие от традиционных подходов, PE использует только контрастивное обучение для создания универсальных эмбеддингов. Авторы вводят методы языкового и пространственного выравнивания для извлечения этих эмбеддингов из промежуточных слоев сети. Модели семейства PE достигают передовых результатов в широком спектре задач компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Unlocking Versatile Visual Understanding with Perception Encoder', 'desc': "The paper presents the Perception Encoder (PE), an advanced model designed for understanding images and videos through vision-language learning. Unlike traditional encoders that depend on specific pretraining tasks, PE utilizes contrastive vision-language training to create versatile embeddings applicable to various tasks. The authors introduce two alignment techniques to extract these embeddings from the network's intermediate layers, enhancing performance in tasks like classification and question answering. By releasing their models and a new dataset, they aim to encourage further exploration in the field of multimodal learning."}, 'zh': {'title': '感知编码器：图像与视频理解的新突破', 'desc': '我们介绍了一种名为感知编码器（PE）的先进编码器，用于图像和视频理解，采用简单的视觉-语言学习进行训练。传统的视觉编码器依赖于多种预训练目标，针对特定的下游任务如分类、描述或定位进行优化。令人惊讶的是，通过扩展我们精心调整的图像预训练方案，并结合强大的视频数据引擎，发现对比视觉-语言训练可以为所有这些下游任务生成强大的通用嵌入。为了提取这些嵌入，我们引入了两种对齐方法，分别用于多模态语言建模和密集预测，从而使我们的PE模型在多种任务上实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.13122', 'title': 'VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models', 'url': 'https://huggingface.co/papers/2504.13122', 'abstract': 'Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) Instance Level, aligning overall video content with responses; ii) Temporal Level, aligning video temporal semantics with event descriptions; and iii) Perceptive Level, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination. The code and data are available at https://github.com/HaroldChen19/VistaDPO.', 'score': 20, 'issue_id': 3303, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'efbc3b240498ce70', 'authors': ['Haojian Huang', 'Haodong Chen', 'Shengqiong Wu', 'Meng Luo', 'Jinlan Fu', 'Xinya Du', 'Hanwang Zhang', 'Hao Fei'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong', 'University of Texas at Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2504.13122.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#alignment', '#hallucinations', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'VistaDPO: Точное согласование видео и языка на всех уровнях', 'desc': 'VistaDPO - это новая система для оптимизации предпочтений в видео с использованием иерархического пространственно-временного подхода. Она улучшает согласование текста и видео на трех уровнях: общем содержании, временной семантике и пространственных объектах. Авторы создали датасет VistaDPO-7k с 7200 парами вопросов-ответов, аннотированными предпочтительными и отвергнутыми ответами, а также временными метками и ограничивающими рамками. Эксперименты показали, что VistaDPO значительно улучшает работу существующих больших видеомоделей, эффективно снижая рассогласование видео и языка и галлюцинации.'}, 'en': {'title': 'Aligning Video and Language: Introducing VistaDPO', 'desc': 'This paper presents VistaDPO, a new framework designed to improve the alignment between video content and human language understanding in Large Video Models (LVMs). It operates on three hierarchical levels: aligning overall video content with user responses, matching temporal semantics of videos with event descriptions, and correlating spatial objects with language tokens. To support this framework, the authors created a dataset called VistaDPO-7k, which includes 7.2K question-answer pairs with detailed annotations for better preference alignment. The results show that VistaDPO enhances the performance of LVMs by reducing issues related to video hallucination and misalignment with human intuition.'}, 'zh': {'title': 'VistaDPO：提升视频理解的偏好对齐', 'desc': '本文介绍了一种新框架VistaDPO，用于视频层次空间-时间直接偏好优化，旨在解决大型视频模型（LVMs）在视频理解中的人类直觉不一致和视频幻觉问题。VistaDPO通过三个层次增强文本-视频偏好对齐：实例层、时间层和感知层，分别对齐视频内容、时间语义和空间对象。为了支持细粒度视频-语言偏好对齐，研究团队构建了VistaDPO-7k数据集，包含7200个问答对及其空间-时间信息。实验结果表明，VistaDPO显著提升了现有LVMs的性能，有效减轻了视频-语言的不一致性和幻觉现象。'}}}, {'id': 'https://huggingface.co/papers/2504.05506', 'title': 'ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering', 'url': 'https://huggingface.co/papers/2504.05506', 'abstract': 'Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.', 'score': 19, 'issue_id': 3303, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'a727d08eac22e920', 'authors': ['Ahmed Masry', 'Mohammed Saidul Islam', 'Mahir Ahmed', 'Aayush Bajaj', 'Firoz Kabir', 'Aaryaman Kartha', 'Md Tahmid Rahman Laskar', 'Mizanur Rahman', 'Shadikur Rahman', 'Mehrad Shahmohammadi', 'Megh Thakkar', 'Md Rizwan Parvez', 'Enamul Hoque', 'Shafiq Joty'], 'affiliations': ['Dialpad Inc., Canada', 'MILA - Quebec AI Institute, Canada', 'Nanyang Technological University, Singapore', 'Qatar Computing Research Institute (QCRI)', 'RBC, Canada', 'Salesforce Research, USA', 'York University, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2504.05506.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#cv', '#dataset', '#reasoning'], 'emoji': '📊', 'ru': {'title': 'ChartQAPro: новый вызов для ИИ в понимании графиков', 'desc': 'ChartQAPro - новый набор данных для задачи ответов на вопросы по графикам. Он включает более 1300 разнообразных графиков из реальных источников и около 2000 вопросов различных типов. Тестирование показало, что современные мультимодальные языковые модели значительно хуже справляются с ChartQAPro по сравнению с предыдущими наборами данных. Авторы провели детальный анализ ошибок и выявили ключевые проблемы в понимании и рассуждении о графиках для языковых моделей.'}, 'en': {'title': 'ChartQAPro: Elevating Chart Understanding for AI', 'desc': 'This paper introduces ChartQAPro, a new benchmark designed to improve Chart Question Answering (CQA) systems by providing a more diverse and realistic set of charts and questions. Unlike previous benchmarks, ChartQAPro includes 1,341 charts from 157 sources and features various question types, which better reflect the complexities of real-world data analysis. The study reveals that modern large vision-language models (LVLMs) struggle significantly with this new benchmark, demonstrating a performance drop from 90.5% on the previous ChartQA to only 55.81% on ChartQAPro. Through error analyses and ablation studies, the authors identify challenges in chart reasoning that can guide future improvements in LVLMs.'}, 'zh': {'title': '提升图表问答系统的挑战与机遇', 'desc': '本论文介绍了ChartQAPro，这是一个新的基准测试，旨在提高图表问答系统的性能。它包含来自157个不同来源的1,341个图表，涵盖多种图表类型，并提供1,948个多样化的问题。通过对21个模型的评估，我们发现现代大型视觉语言模型在ChartQAPro上的表现显著下降，显示出图表推理的复杂性。我们的研究还包括详细的错误分析和消融研究，以识别图表理解和推理中的关键挑战和机遇。'}}}, {'id': 'https://huggingface.co/papers/2504.13055', 'title': 'NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation', 'url': 'https://huggingface.co/papers/2504.13055', 'abstract': 'Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to more effectively scale test-time compute remains underexplored in VLMs. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, a simple yet effective RL approach that mixes trajectories from both clean and moderately distorted images to introduce targeted diversity in visual perception and the resulting reasoning patterns. Without additional training cost, NoisyRollout enhances the exploration capabilities of VLMs by incorporating a vision-oriented inductive bias. Furthermore, NoisyRollout employs a noise annealing schedule that gradually reduces distortion strength over training, ensuring benefit from noisy signals early while maintaining training stability and scalability in later stages. With just 2.1K training samples, NoisyRollout achieves state-of-the-art performance among open-source RL-tuned models on 5 out-of-domain benchmarks spanning both reasoning and perception tasks, while preserving comparable or even better in-domain performance.', 'score': 16, 'issue_id': 3307, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '058d7aa285231e64', 'authors': ['Xiangyan Liu', 'Jinjie Ni', 'Zijian Wu', 'Chao Du', 'Longxu Dou', 'Haonan Wang', 'Tianyu Pang', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'Sea AI Lab, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.13055.jpg', 'data': {'categories': ['#rlhf', '#rl', '#training', '#multimodal', '#reasoning', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'NoisyRollout: Улучшение исследования и рассуждений ВЯМ через контролируемый визуальный шум', 'desc': 'Статья представляет новый подход в обучении с подкреплением для визуально-языковых моделей под названием NoisyRollout. Этот метод улучшает способности моделей к исследованию и рассуждению, смешивая траектории чистых и умеренно искаженных изображений. NoisyRollout использует график уменьшения шума, постепенно снижая силу искажений в процессе обучения. Метод достигает передовых результатов на пяти внедоменных бенчмарках, используя всего 2100 обучающих примеров.'}, 'en': {'title': 'Enhancing VLMs with NoisyRollout for Better Reasoning and Perception', 'desc': 'This paper introduces NoisyRollout, a reinforcement learning method designed to improve vision-language models (VLMs) by enhancing their policy exploration capabilities. It addresses the challenge of imperfect visual perception by mixing clean and distorted image trajectories, which helps diversify the reasoning patterns of the models. The approach uses a noise annealing schedule to gradually reduce distortion, allowing the model to benefit from noisy inputs during early training while ensuring stability later on. Remarkably, NoisyRollout achieves state-of-the-art results on various benchmarks with minimal training samples, demonstrating its effectiveness in both reasoning and perception tasks.'}, 'zh': {'title': 'NoisyRollout：提升视觉-语言模型的推理能力', 'desc': '最近，强化学习（RL）的进展增强了视觉-语言模型（VLM）的推理能力。然而，在VLM中，增强策略探索以更有效地扩展测试时间计算仍然未被充分研究。此外，VLM在视觉感知不完美方面仍然面临挑战，这影响了后续的推理过程。为此，我们提出了NoisyRollout，这是一种简单而有效的RL方法，通过混合干净和适度失真的图像轨迹，引入目标多样性，从而改善视觉感知和推理模式。'}}}, {'id': 'https://huggingface.co/papers/2504.12364', 'title': 'DMM: Building a Versatile Image Generation Model via Distillation-Based\n  Model Merging', 'url': 'https://huggingface.co/papers/2504.12364', 'abstract': 'The success of text-to-image (T2I) generation models has spurred a proliferation of numerous model checkpoints fine-tuned from the same base model on various specialized datasets. This overwhelming specialized model production introduces new challenges for high parameter redundancy and huge storage cost, thereby necessitating the development of effective methods to consolidate and unify the capabilities of diverse powerful models into a single one. A common practice in model merging adopts static linear interpolation in the parameter space to achieve the goal of style mixing. However, it neglects the features of T2I generation task that numerous distinct models cover sundry styles which may lead to incompatibility and confusion in the merged model. To address this issue, we introduce a style-promptable image generation pipeline which can accurately generate arbitrary-style images under the control of style vectors. Based on this design, we propose the score distillation based model merging paradigm (DMM), compressing multiple models into a single versatile T2I model. Moreover, we rethink and reformulate the model merging task in the context of T2I generation, by presenting new merging goals and evaluation protocols. Our experiments demonstrate that DMM can compactly reorganize the knowledge from multiple teacher models and achieve controllable arbitrary-style generation.', 'score': 14, 'issue_id': 3306, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '49755535f03790d4', 'authors': ['Tianhui Song', 'Weixin Feng', 'Shuai Wang', 'Xubin Li', 'Tiezheng Ge', 'Bo Zheng', 'Limin Wang'], 'affiliations': ['Nanjing University', 'Nanjing University, Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2504.12364.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#cv', '#diffusion', '#training', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'Объединение T2I моделей для гибкой генерации стилей', 'desc': 'В статье представлен новый метод объединения моделей генерации изображений по тексту (T2I). Авторы предлагают конвейер генерации изображений, управляемый векторами стилей, и парадигму слияния моделей на основе дистилляции оценок (DMM). Этот подход позволяет сжать несколько моделей в одну универсальную T2I модель. Эксперименты показывают, что DMM может эффективно реорганизовать знания из нескольких учительских моделей и достичь контролируемой генерации изображений произвольного стиля.'}, 'en': {'title': 'Unifying Diverse T2I Models with Style-Promptable Merging', 'desc': 'This paper addresses the challenges posed by the proliferation of specialized text-to-image (T2I) generation models, which often lead to high parameter redundancy and storage costs. The authors propose a novel approach called score distillation based model merging (DMM) that consolidates multiple models into a single, versatile T2I model. Unlike traditional methods that use static linear interpolation, DMM incorporates style vectors to enable accurate generation of images in various styles while avoiding incompatibility issues. The paper also introduces new goals and evaluation protocols for model merging in the context of T2I generation, demonstrating that DMM effectively reorganizes knowledge from multiple models for controllable style generation.'}, 'zh': {'title': '高效合并多模型，实现可控图像生成', 'desc': '本文探讨了文本到图像生成模型（T2I）的合并问题。随着众多模型在不同数据集上微调，产生了大量的专用模型，这导致了参数冗余和存储成本高的问题。为了解决这些问题，本文提出了一种基于评分蒸馏的模型合并方法（DMM），能够将多个模型压缩为一个多功能的T2I模型。实验结果表明，DMM能够有效整合多个教师模型的知识，实现可控的任意风格图像生成。'}}}, {'id': 'https://huggingface.co/papers/2504.13180', 'title': 'PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding', 'url': 'https://huggingface.co/papers/2504.13180', 'abstract': 'Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM-VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about "what", "where", "when", and "how" of a video. We make our work fully reproducible by providing data, training recipes, code & models.', 'score': 13, 'issue_id': 3323, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'b923d02610ef004b', 'authors': ['Jang Hyun Cho', 'Andrea Madotto', 'Effrosyni Mavroudi', 'Triantafyllos Afouras', 'Tushar Nagarajan', 'Muhammad Maaz', 'Yale Song', 'Tengyu Ma', 'Shuming Hu', 'Suyog Jain', 'Miguel Martin', 'Huiyu Wang', 'Hanoona Rasheed', 'Peize Sun', 'Po-Yao Huang', 'Daniel Bolya', 'Nikhila Ravi', 'Shashank Jain', 'Tammy Stark', 'Shane Moon', 'Babak Damavandi', 'Vivian Lee', 'Andrew Westbury', 'Salman Khan', 'Philipp Krähenbühl', 'Piotr Dollár', 'Lorenzo Torresani', 'Kristen Grauman', 'Christoph Feichtenhofer'], 'affiliations': ['MBZUAI', 'Meta FAIR', 'Meta Reality Labs', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2504.13180.jpg', 'data': {'categories': ['#dataset', '#open_source', '#synthetic', '#training', '#reasoning', '#benchmark', '#data', '#cv', '#video'], 'emoji': '🎥', 'ru': {'title': 'Открытая модель восприятия и языка для прозрачного анализа видео', 'desc': 'Статья посвящена созданию открытой и воспроизводимой модели восприятия и языка (PLM) для прозрачных исследований в области понимания изображений и видео. Авторы анализируют стандартные методы обучения без дистилляции от проприетарных моделей и исследуют синтетические данные большого масштаба. Для преодоления выявленных пробелов в данных, особенно в детальном понимании видео, авторы выпускают 2,8 миллиона размеченных вручную видео-вопросов с ответами и пространственно-временными аннотациями. Также представлен PLM-VideoBench - набор инструментов для оценки сложных задач понимания видео.'}, 'en': {'title': 'Open and Reproducible Video Understanding with PLM', 'desc': 'This paper focuses on creating a Perception Language Model (PLM) that is fully open and reproducible, addressing the limitations of closed-source models in computer vision. The authors highlight the challenges of using black-box models for training data labeling, which hinders scientific progress. They propose a new approach that utilizes large-scale synthetic data and provides 2.8 million human-labeled video question-answer pairs to improve video understanding. Additionally, they introduce PLM-VideoBench, a comprehensive evaluation suite for assessing video reasoning tasks, ensuring transparency and reproducibility in their research.'}, 'zh': {'title': '开放透明的感知语言模型研究', 'desc': '本文研究了一种开放且可重复的感知语言模型（PLM），旨在促进图像和视频理解的透明研究。我们分析了标准训练流程，避免使用封闭模型的蒸馏方法，并探索大规模合成数据以识别视频理解中的关键数据缺口。为了解决这些缺口，我们发布了280万个人工标注的细粒度视频问答对和时空基础的视频标题。我们还推出了PLM-VideoBench，一个评估视频理解任务的工具，专注于推理视频的“什么”、“哪里”、“何时”和“如何”。'}}}, {'id': 'https://huggingface.co/papers/2504.13171', 'title': 'Sleep-time Compute: Beyond Inference Scaling at Test-time', 'url': 'https://huggingface.co/papers/2504.13171', 'abstract': 'Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to "think" offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.', 'score': 13, 'issue_id': 3310, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '094d6f648b644327', 'authors': ['Kevin Lin', 'Charlie Snell', 'Yu Wang', 'Charles Packer', 'Sarah Wooders', 'Ion Stoica', 'Joseph E. Gonzalez'], 'affiliations': ['Letta', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.13171.jpg', 'data': {'categories': ['#optimization', '#inference', '#reasoning', '#agents'], 'emoji': '💤', 'ru': {'title': 'Оптимизация LLM: думай заранее, отвечай быстрее', 'desc': "Эта статья представляет концепцию 'вычислений во время сна' для больших языковых моделей (LLM). Метод предполагает предварительные вычисления для ожидаемых запросов, что значительно снижает вычислительные затраты во время выполнения. Авторы демонстрируют эффективность подхода на модифицированных версиях задач рассуждения, показывая пятикратное сокращение необходимых вычислений при сохранении точности. Исследование также выявляет, что предсказуемость пользовательских запросов коррелирует с эффективностью метода 'вычислений во время сна'."}, 'en': {'title': 'Optimize Query Processing with Sleep-Time Compute', 'desc': 'This paper presents a novel approach called sleep-time compute, which allows large language models (LLMs) to prepare for user queries by pre-computing relevant information offline. By anticipating potential questions, the model can significantly reduce the computational load during test-time, achieving up to 5 times less compute while maintaining accuracy. The authors demonstrate that scaling sleep-time compute can enhance accuracy by up to 18% on specific reasoning tasks. Additionally, they introduce Multi-Query GSM-Symbolic, which optimizes the processing of multiple related queries, further decreasing the average cost per query by 2.5 times.'}, 'zh': {'title': '睡眠时间计算：提升语言模型效率的新方法', 'desc': '本文提出了一种新的计算方法，称为睡眠时间计算，旨在减少大型语言模型在测试时的计算需求。通过在用户提出查询之前，模型可以离线思考并预计算有用的信息，从而降低延迟和推理成本。研究表明，使用睡眠时间计算可以在保持相同准确率的情况下，将测试时的计算需求减少约5倍，并在某些任务上提高准确率。我们还引入了多查询GSM-符号化，允许在同一上下文中处理多个相关查询，从而进一步降低每个查询的平均成本。'}}}, {'id': 'https://huggingface.co/papers/2504.12395', 'title': 'InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework', 'url': 'https://huggingface.co/papers/2504.12395', 'abstract': 'Current learning-based subject customization approaches, predominantly relying on U-Net architectures, suffer from limited generalization ability and compromised image quality. Meanwhile, optimization-based methods require subject-specific fine-tuning, which inevitably degrades textual controllability. To address these challenges, we propose InstantCharacter, a scalable framework for character customization built upon a foundation diffusion transformer. InstantCharacter demonstrates three fundamental advantages: first, it achieves open-domain personalization across diverse character appearances, poses, and styles while maintaining high-fidelity results. Second, the framework introduces a scalable adapter with stacked transformer encoders, which effectively processes open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers. Third, to effectively train the framework, we construct a large-scale character dataset containing 10-million-level samples. The dataset is systematically organized into paired (multi-view character) and unpaired (text-image combinations) subsets. This dual-data structure enables simultaneous optimization of identity consistency and textual editability through distinct learning pathways. Qualitative experiments demonstrate the advanced capabilities of InstantCharacter in generating high-fidelity, text-controllable, and character-consistent images, setting a new benchmark for character-driven image generation. Our source code is available at https://github.com/Tencent/InstantCharacter.', 'score': 13, 'issue_id': 3308, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '4000d7f0fc525f6d', 'authors': ['Jiale Tao', 'Yanbing Zhang', 'Qixun Wang', 'Yiji Cheng', 'Haofan Wang', 'Xu Bai', 'Zhengguang Zhou', 'Ruihuang Li', 'Linqing Wang', 'Chunyu Wang', 'Qin Lin', 'Qinglin Lu'], 'affiliations': ['Hunyuan, Tencent', 'InstantX Team'], 'pdf_title_img': 'assets/pdf/title_img/2504.12395.jpg', 'data': {'categories': ['#open_source', '#data', '#diffusion', '#dataset', '#benchmark', '#cv'], 'emoji': '🎭', 'ru': {'title': 'Мгновенная кастомизация персонажей с сохранением высокого качества и текстового контроля', 'desc': 'InstantCharacter - это новый подход к кастомизации персонажей, основанный на диффузионном трансформере. Он решает проблемы ограниченной обобщаемости и ухудшения качества изображений, характерные для существующих методов. Фреймворк использует масштабируемый адаптер со стековыми энкодерами трансформеров для обработки характеристик персонажей. Для обучения был создан большой набор данных из 10 миллионов образцов, организованный в парные и непарные подмножества.'}, 'en': {'title': 'InstantCharacter: Revolutionizing Character Customization with Diffusion Transformers', 'desc': 'This paper introduces InstantCharacter, a new framework for customizing characters using machine learning. Unlike traditional methods that struggle with generalization and image quality, InstantCharacter leverages a diffusion transformer to achieve high-fidelity results across various character styles and poses. The framework includes a scalable adapter with transformer encoders that effectively manage character features and interact with the latent space of diffusion models. Additionally, it utilizes a large-scale dataset of 10 million samples to optimize both identity consistency and textual editability, demonstrating superior performance in generating controllable character images.'}, 'zh': {'title': 'InstantCharacter：高保真角色定制的新标准', 'desc': '本文提出了一种名为InstantCharacter的角色定制框架，旨在解决现有学习基础方法在图像质量和泛化能力上的不足。该框架基于扩散变换器，能够在多样化的角色外观、姿势和风格中实现开放域个性化，同时保持高保真度。InstantCharacter引入了可扩展的适配器，利用堆叠的变换器编码器有效处理开放域角色特征，并与现代扩散变换器的潜在空间无缝交互。此外，构建了一个包含千万级样本的大规模角色数据集，以支持框架的有效训练，优化身份一致性和文本可编辑性。'}}}, {'id': 'https://huggingface.co/papers/2504.11651', 'title': '70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU\n  Inference via Dynamic-Length Float', 'url': 'https://huggingface.co/papers/2504.11651', 'abstract': 'Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at https://github.com/LeanModels/DFloat11.', 'score': 12, 'issue_id': 3321, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'bbc8b58a0d8e7d08', 'authors': ['Tianyi Zhang', 'Yang Sui', 'Shaochen Zhong', 'Vipin Chaudhary', 'Xia Hu', 'Anshumali Shrivastava'], 'affiliations': ['Department of Computer Science, Rice University', 'Department of Computer and Data Sciences, Case Western Reserve University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11651.jpg', 'data': {'categories': ['#small_models', '#optimization', '#long_context', '#inference'], 'emoji': '🗜️', 'ru': {'title': 'DFloat11: Эффективное сжатие больших языковых моделей без потерь', 'desc': 'Статья представляет Dynamic-Length Float (DFloat11) - фреймворк для сжатия больших языковых моделей (LLM) без потерь. DFloat11 использует энтропийное кодирование для присвоения динамических кодов весам модели, что позволяет сократить размер LLM на 30% без изменения выходных данных. Авторы разработали специальное ядро GPU для быстрой онлайн-декомпрессии, включающее декомпозицию таблиц поиска и двухфазный алгоритм для координации потоков. Эксперименты на современных моделях, таких как Llama-3.1 и Gemma-3, подтверждают эффективность метода, позволяя значительно увеличить пропускную способность и длину контекста при фиксированном объеме памяти GPU.'}, 'en': {'title': 'Efficient Compression for Large Language Models with DFloat11', 'desc': 'This paper presents DFloat11, a novel lossless compression framework designed to reduce the size of Large Language Models (LLMs) by 30% while maintaining exact output fidelity. The framework leverages the low entropy in the BFloat16 weight representation, applying entropy coding to assign dynamic-length encodings to model weights based on their frequency. DFloat11 includes a custom GPU kernel for efficient online decompression, optimizing memory usage and minimizing latency during inference. Experimental results demonstrate that DFloat11 significantly enhances throughput and allows for longer context lengths in LLMs, making it a powerful solution for deploying large models on resource-constrained hardware.'}, 'zh': {'title': 'DFloat11：高效压缩大型语言模型的无损框架', 'desc': '大型语言模型（LLMs）在规模上迅速增长，这给资源有限的硬件部署带来了重大挑战。本文介绍了一种无损压缩框架DFloat11，能够在保持输出与原始模型逐位相同的情况下，将LLM的大小减少30%。DFloat11通过熵编码，根据权重的频率为其分配动态长度编码，实现接近信息最优的压缩而不损失精度。我们的实验表明，DFloat11在多个模型上验证了其有效性，显著提高了生成速度和上下文长度。'}}}, {'id': 'https://huggingface.co/papers/2504.13145', 'title': 'Exploring Expert Failures Improves LLM Agent Tuning', 'url': 'https://huggingface.co/papers/2504.13145', 'abstract': 'Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.', 'score': 11, 'issue_id': 3305, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '597cd9806c8a07ff', 'authors': ['Li-Cheng Lan', 'Andrew Bai', 'Minhao Cheng', 'Ruochen Wang', 'Cho-Jui Hsieh', 'Tianyi Zhou'], 'affiliations': ['OpenAI', 'Pennsylvania State University', 'UCLA', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.13145.jpg', 'data': {'categories': ['#reasoning', '#agents', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Учимся на ошибках: новый метод обучения ИИ-агентов', 'desc': 'Статья представляет новый метод обучения больших языковых моделей (LLM) для выполнения сложных задач. Метод называется Exploring Expert Failures (EEF) и использует информацию из неудачных попыток экспертной модели для улучшения процесса обучения. EEF превзошел предыдущие методы, такие как Rejection Sampling Fine-Tuning (RFT), в задачах WebShop и SciWorld. Авторы показывают, что использование полезных действий из неудачных экспертных траекторий может значительно улучшить эффективность исследования и приобретение критических навыков агентом.'}, 'en': {'title': 'Learning from Mistakes: Enhancing LLMs with Expert Failures', 'desc': 'This paper introduces Exploring Expert Failures (EEF), a novel approach to enhance the performance of Large Language Models (LLMs) in complex tasks. EEF builds on Rejection Sampling Fine-Tuning (RFT) by utilizing insights from previously failed expert trajectories to identify beneficial actions that can improve agent exploration and skill acquisition. By integrating these valuable actions into the training dataset while excluding harmful ones, EEF addresses previously unsolvable subtasks and boosts overall agent performance. The results demonstrate that EEF outperforms existing methods, achieving a 62% win rate in WebShop and setting new benchmarks in task performance.'}, 'zh': {'title': '从失败中学习，提升智能体能力', 'desc': '大型语言模型（LLMs）在多轮推理和交互任务中表现出色。拒绝采样微调（RFT）是一种有效的微调方法，通过模仿专家生成的成功轨迹并在自生成的成功轨迹上进行迭代微调来提升模型的能力。然而，由于RFT偏向于简单场景，许多复杂子任务仍然未能解决。我们提出的探索专家失败（EEF）方法，通过从失败的专家轨迹中提取有益的行动，显著提高了模型的探索效率和关键技能的获取。'}}}, {'id': 'https://huggingface.co/papers/2504.07959', 'title': 'CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera\n  Color Constancy', 'url': 'https://huggingface.co/papers/2504.07959', 'abstract': "Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space. The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs.", 'score': 9, 'issue_id': 3311, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'f4c62f5c36856c86', 'authors': ['Dongyoung Kim', 'Mahmoud Afifi', 'Dongyun Kim', 'Michael S. Brown', 'Seon Joo Kim'], 'affiliations': ['AI Center - Toronto, Samsung Electronics', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07959.jpg', 'data': {'categories': ['#training', '#cv', '#dataset'], 'emoji': '📸', 'ru': {'title': 'Адаптивный баланс белого для любой камеры', 'desc': 'Эта статья представляет новый метод машинного обучения для коррекции баланса белого в камерах, который может адаптироваться к новым камерам без переобучения. Метод использует предварительно откалиброванные матрицы коррекции цвета (CCM) для создания компактного встраивания-отпечатка камеры (CFE). Авторы также вводят технику аугментации данных, интерполирующую между камерами и их CCM для предотвращения переобучения. Экспериментальные результаты показывают, что метод достигает современного уровня в межкамерной цветовой константности, оставаясь при этом легковесным.'}, 'en': {'title': 'Adaptive Color Constancy Across Cameras Without Retraining', 'desc': 'This paper presents a novel approach to computational color constancy, specifically focusing on white balancing in images captured by different cameras. The proposed method utilizes pre-calibrated color correction matrices (CCMs) to adaptively transform illumination colors into the raw color space of new cameras without the need for retraining. By creating a compact camera fingerprint embedding (CFE), the model can effectively generalize to unseen cameras, enhancing its versatility. Additionally, a data augmentation technique is introduced to mitigate overfitting, ensuring robust performance across various datasets and camera types.'}, 'zh': {'title': '跨相机颜色恒常性的创新方法', 'desc': '本文介绍了一种基于学习的跨相机颜色恒常性方法，旨在解决不同相机的白平衡问题。该方法利用预先校准的颜色校正矩阵（CCM），将相机的原始颜色空间映射到标准空间。通过将预定义的照明颜色转换为测试相机的原始空间，生成紧凑的相机指纹嵌入（CFE），使网络能够适应未见过的相机。实验结果表明，该方法在多个数据集上实现了最先进的跨相机颜色恒常性，同时保持轻量级，依赖于相机ISP中现成的数据。'}}}, {'id': 'https://huggingface.co/papers/2504.13143', 'title': 'Complex-Edit: CoT-Like Instruction Generation for\n  Complexity-Controllable Image Editing Benchmark', 'url': 'https://huggingface.co/papers/2504.13143', 'abstract': "We introduce Complex-Edit, a comprehensive benchmark designed to systematically evaluate instruction-based image editing models across instructions of varying complexity. To develop this benchmark, we harness GPT-4o to automatically collect a diverse set of editing instructions at scale. Our approach follows a well-structured ``Chain-of-Edit'' pipeline: we first generate individual atomic editing tasks independently and then integrate them to form cohesive, complex instructions. Additionally, we introduce a suite of metrics to assess various aspects of editing performance, along with a VLM-based auto-evaluation pipeline that supports large-scale assessments. Our benchmark yields several notable insights: 1) Open-source models significantly underperform relative to proprietary, closed-source models, with the performance gap widening as instruction complexity increases; 2) Increased instructional complexity primarily impairs the models' ability to retain key elements from the input images and to preserve the overall aesthetic quality; 3) Decomposing a complex instruction into a sequence of atomic steps, executed in a step-by-step manner, substantially degrades performance across multiple metrics; 4) A straightforward Best-of-N selection strategy improves results for both direct editing and the step-by-step sequential approach; and 5) We observe a ``curse of synthetic data'': when synthetic data is involved in model training, the edited images from such models tend to appear increasingly synthetic as the complexity of the editing instructions rises -- a phenomenon that intriguingly also manifests in the latest GPT-4o outputs.", 'score': 7, 'issue_id': 3324, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '5cf48cefe0cb6c8d', 'authors': ['Siwei Yang', 'Mude Hui', 'Bingchen Zhao', 'Yuyin Zhou', 'Nataniel Ruiz', 'Cihang Xie'], 'affiliations': ['Google', 'University of California, Santa Cruz', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2504.13143.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#open_source', '#data', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Complex-Edit: новый стандарт оценки моделей редактирования изображений', 'desc': 'Статья представляет Complex-Edit - комплексный бенчмарк для оценки моделей редактирования изображений на основе инструкций различной сложности. Авторы используют GPT-4 для автоматического сбора разнообразных инструкций по редактированию в масштабе. Они вводят набор метрик для оценки различных аспектов производительности редактирования, а также конвейер автоматической оценки на основе VLM. Исследование выявляет ряд важных наблюдений, включая значительное отставание моделей с открытым исходным кодом от проприетарных моделей и ухудшение качества при разложении сложных инструкций на последовательность атомарных шагов.'}, 'en': {'title': 'Evaluating Image Editing Models: Complexity Matters!', 'desc': "The paper presents Complex-Edit, a benchmark for evaluating instruction-based image editing models based on the complexity of editing tasks. It utilizes GPT-4o to generate a wide range of editing instructions, which are then organized into a structured 'Chain-of-Edit' pipeline. The study reveals that open-source models lag behind proprietary ones, especially as task complexity increases, affecting their ability to maintain image quality. Additionally, it highlights the negative impact of breaking down complex instructions into simpler steps and introduces a Best-of-N selection strategy to enhance editing performance."}, 'zh': {'title': '复杂指令下的图像编辑性能评估', 'desc': '本文介绍了Complex-Edit，这是一个全面的基准测试，旨在系统评估基于指令的图像编辑模型，涵盖不同复杂度的指令。我们利用GPT-4o自动收集多样化的编辑指令，并采用结构化的“编辑链”流程生成独立的原子编辑任务，再将其整合为复杂指令。研究发现，开源模型在性能上显著低于闭源模型，且随着指令复杂度的增加，性能差距加大。此外，复杂指令的分解执行会显著降低模型的表现，而简单的选择策略则能改善编辑效果。'}}}, {'id': 'https://huggingface.co/papers/2504.12157', 'title': 'FocusedAD: Character-centric Movie Audio Description', 'url': 'https://huggingface.co/papers/2504.12157', 'abstract': 'Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .', 'score': 7, 'issue_id': 3306, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '2bfe38936e42dd11', 'authors': ['Xiaojun Ye', 'Chun Wang', 'Yiren Song', 'Sheng Zhou', 'Liangcheng Li', 'Jiajun Bu'], 'affiliations': ['National University of Singapore Singapore', 'Zhejiang University China'], 'pdf_title_img': 'assets/pdf/title_img/2504.12157.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#audio', '#benchmark', '#video', '#dataset', '#science'], 'emoji': '🎬', 'ru': {'title': 'Умное аудиоописание фильмов с фокусом на персонажах', 'desc': 'Статья представляет FocusedAD - новый фреймворк для создания аудиоописаний фильмов, ориентированных на персонажей. Система включает модуль восприятия персонажей (CPM) для отслеживания и именования персонажей, динамический модуль предыстории (DPM) для внедрения контекстных подсказок, и модуль фокусированных подписей (FCM) для генерации релевантных описаний с именами персонажей. FocusedAD также предлагает автоматизированный процесс создания банков запросов персонажей для улучшения их идентификации. Фреймворк достигает наилучших результатов на нескольких бенчмарках, включая сильные результаты в режиме zero-shot.'}, 'en': {'title': 'FocusedAD: Enhancing Movie Accessibility with Character-Centric Audio Descriptions', 'desc': 'This paper presents FocusedAD, a new framework designed to create audio descriptions for movies, specifically targeting blind and visually impaired audiences. It addresses the unique challenges of audio description by focusing on character-centric narration that includes character names and relevant plot details. The framework consists of three main components: a Character Perception Module for tracking characters, a Dynamic Prior Module for incorporating contextual information, and a Focused Caption Module for generating detailed narrations. FocusedAD demonstrates superior performance on various benchmarks, including zero-shot evaluations, showcasing its effectiveness in enhancing movie accessibility.'}, 'zh': {'title': '以角色为中心的电影音频描述', 'desc': '电影音频描述（AD）旨在为盲人和视力障碍者在无对话的片段中叙述视觉内容。与一般视频字幕相比，AD需要与情节相关的叙述，并明确提及角色名称，这给电影理解带来了独特的挑战。我们提出了FocusedAD框架，通过角色感知模块、动态先验模块和聚焦字幕模块，提供以角色为中心的电影音频描述。FocusedAD在多个基准测试中实现了最先进的性能，包括在MAD-eval-Named和新提出的Cinepile-AD数据集上的强大零样本结果。'}}}, {'id': 'https://huggingface.co/papers/2504.13079', 'title': 'Retrieval-Augmented Generation with Conflicting Evidence', 'url': 'https://huggingface.co/papers/2504.13079', 'abstract': 'Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.', 'score': 6, 'issue_id': 3304, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '13305db862567e7f', 'authors': ['Han Wang', 'Archiki Prasad', 'Elias Stengel-Eskin', 'Mohit Bansal'], 'affiliations': ['University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2504.13079.jpg', 'data': {'categories': ['#interpretability', '#dataset', '#rag', '#optimization', '#agents', '#hallucinations'], 'emoji': '🤖', 'ru': {'title': 'Многоагентный подход для борьбы с неоднозначностью и дезинформацией в RAG-системах', 'desc': 'Статья представляет новый подход к решению проблем неоднозначности и дезинформации в системах генерации текста с использованием извлечения информации (RAG). Авторы предлагают датасет RAMDocs, моделирующий сложные сценарии с противоречивыми данными, и метод MADAM-RAG, использующий несколько агентов на основе больших языковых моделей для обсуждения ответов. MADAM-RAG показывает улучшение результатов на 11.40% на датасете AmbigDocs и на 15.80% на FaithEval по сравнению с базовыми методами RAG. Однако, несмотря на прогресс, остаются значительные проблемы, особенно при увеличении дисбаланса между поддерживающими и дезинформирующими данными.'}, 'en': {'title': 'Enhancing LLM Accuracy through Debate and Retrieval', 'desc': 'This paper discusses the challenges faced by large language model (LLM) agents when using retrieval-augmented generation (RAG) to provide accurate responses to user queries. It introduces RAMDocs, a new dataset designed to simulate complex scenarios involving ambiguity, misinformation, and noise in documents. The authors propose MADAM-RAG, a multi-agent system where LLMs debate answers over multiple rounds, allowing for better aggregation of responses while filtering out inaccuracies. The results show that MADAM-RAG significantly improves performance on tasks requiring disambiguation and misinformation suppression compared to traditional RAG methods, although challenges remain in handling conflicting evidence.'}, 'zh': {'title': '多代理辩论：提升语言模型的准确性与鲁棒性', 'desc': '大型语言模型（LLM）代理越来越多地使用检索增强生成（RAG）来提高回答的准确性。然而，这些系统在处理模糊用户查询和来自多个来源的潜在冲突信息时，常常需要抑制来自嘈杂或无关文档的不准确信息。本文提出了RAMDocs数据集，模拟了复杂的用户查询场景，并提出了MADAM-RAG多代理方法，通过多轮辩论来处理答案的优劣，从而有效整合不同来源的响应。实验结果表明，MADAM-RAG在处理模糊查询和抑制错误信息方面显著优于现有的RAG基线。'}}}, {'id': 'https://huggingface.co/papers/2504.12782', 'title': 'Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts', 'url': 'https://huggingface.co/papers/2504.12782', 'abstract': 'Ensuring the ethical deployment of text-to-image models requires effective techniques to prevent the generation of harmful or inappropriate content. While concept erasure methods offer a promising solution, existing finetuning-based approaches suffer from notable limitations. Anchor-free methods risk disrupting sampling trajectories, leading to visual artifacts, while anchor-based methods rely on the heuristic selection of anchor concepts. To overcome these shortcomings, we introduce a finetuning framework, dubbed ANT, which Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is built on a key insight: reversing the condition direction of classifier-free guidance during mid-to-late denoising stages enables precise content modification without sacrificing early-stage structural integrity. This inspires a trajectory-aware objective that preserves the integrity of the early-stage score function field, which steers samples toward the natural image manifold, without relying on heuristic anchor concept selection. For single-concept erasure, we propose an augmentation-enhanced weight saliency map to precisely identify the critical parameters that most significantly contribute to the unwanted concept, enabling more thorough and efficient erasure. For multi-concept erasure, our objective function offers a versatile plug-and-play solution that significantly boosts performance. Extensive experiments demonstrate that ANT achieves state-of-the-art results in both single and multi-concept erasure, delivering high-quality, safe outputs without compromising the generative fidelity. Code is available at https://github.com/lileyang1210/ANT', 'score': 4, 'issue_id': 3305, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '6c74497f6fd8b96b', 'authors': ['Leyang Li', 'Shilin Lu', 'Yan Ren', 'Adams Wai-Kin Kong'], 'affiliations': ['Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.12782.jpg', 'data': {'categories': ['#cv', '#data', '#training', '#optimization', '#ethics'], 'emoji': '🎨', 'ru': {'title': 'ANT: Точное удаление нежелательных концепций из генеративных моделей изображений', 'desc': 'Статья представляет новый метод ANT для удаления нежелательных концепций из генеративных моделей изображений. ANT использует обращение направления условной генерации на средних и поздних этапах шумоподавления для точной модификации контента. Метод сохраняет целостность ранних этапов генерации, не полагаясь на эвристический выбор якорных концепций. ANT показывает отличные результаты как для удаления одиночных, так и множественных концепций, обеспечивая высокое качество и безопасность сгенерированных изображений.'}, 'en': {'title': 'ANT: Ethical Image Generation Through Smart Concept Erasure', 'desc': 'This paper presents ANT, a new finetuning framework designed to improve the ethical deployment of text-to-image models by effectively erasing harmful or inappropriate content. ANT addresses the limitations of existing methods by guiding denoising trajectories to avoid unwanted concepts without disrupting the image quality. It introduces a trajectory-aware objective that maintains the integrity of early-stage image features while allowing for precise content modification. The framework also includes an innovative weight saliency map for identifying critical parameters in single-concept erasure and offers a versatile solution for multi-concept erasure, achieving state-of-the-art results in generating safe and high-quality images.'}, 'zh': {'title': 'ANT：高效去除不当内容的文本到图像模型框架', 'desc': '本文提出了一种名为ANT的微调框架，用于确保文本到图像模型的伦理部署，特别是防止生成有害或不当内容。ANT通过自动引导去噪轨迹，避免了现有方法中的一些局限性，如锚点方法的启发式选择和无锚点方法导致的视觉伪影。该框架利用了在去噪中后期反转分类器无指导的条件方向的关键见解，从而实现了精确的内容修改，同时保持了早期阶段的结构完整性。通过增强的权重显著性图，ANT能够有效识别并去除单一或多个不当概念，实验结果表明其在去除效果和生成质量上均达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2504.12563', 'title': 'MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic\n  Data Generation', 'url': 'https://huggingface.co/papers/2504.12563', 'abstract': 'Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is low diversity, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple "expert" LLM agents to collaboratively generate data. Using only 25 million tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and Biomedicine-without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora.   Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.', 'score': 4, 'issue_id': 3316, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '0272e3b4f0ac9209', 'authors': ['Haris Riaz', 'Sourav Bhabesh', 'Vinayak Arannil', 'Miguel Ballesteros', 'Graham Horwood'], 'affiliations': ['AWS AI Labs', 'University of Arizona'], 'pdf_title_img': 'assets/pdf/title_img/2504.12563.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#multimodal', '#training', '#dataset', '#synthetic'], 'emoji': '🧬', 'ru': {'title': 'MetaSynth: Разнообразные синтетические данные для эффективной доменной адаптации языковых моделей', 'desc': "MetaSynth - это новый метод генерации синтетических данных для адаптации больших языковых моделей к специализированным областям. Он использует мета-промптинг, где языковая модель координирует нескольких 'экспертных' LLM-агентов для совместной генерации разнообразных данных. Используя всего 25 миллионов токенов синтетических данных, созданных с помощью MetaSynth, авторам удалось адаптировать Mistral-7B-v0.3 к финансовой и биомедицинской областям без ущерба для общих способностей модели. Результаты показывают значительное улучшение производительности в специализированных задачах по сравнению с базовой моделью."}, 'en': {'title': 'Enhancing Domain Adaptation with Diverse Synthetic Data', 'desc': "This paper introduces MetaSynth, a novel method for generating synthetic data that improves diversity by using multiple expert language models to collaboratively create data. The authors demonstrate that with just 25 million tokens of synthetic data, they can effectively adapt a well-trained language model, Mistral-7B-v0.3, to specialized domains like Finance and Biomedicine. The results show that this approach not only maintains the model's general capabilities but also significantly enhances its performance in the targeted domains. Additionally, the diversity of the synthetic data generated by MetaSynth is evaluated and found to be comparable to that of traditional LLM pre-training datasets, indicating its potential for broader applications in domain adaptation."}, 'zh': {'title': 'MetaSynth：提升合成数据多样性，助力领域适应', 'desc': '本文提出了一种名为MetaSynth的方法，用于生成多样化的合成数据，以提高大型语言模型（LLM）的适应性。通过元提示技术，MetaSynth协调多个“专家”LLM代理共同生成数据，从而增强合成数据的多样性。研究表明，仅使用2500万个合成数据令牌，MetaSynth成功将Mistral-7B-v0.3模型适应于金融和生物医学两个专业领域，同时保持其在一般任务中的能力。与使用模板提示生成的数据相比，MetaSynth生成的数据在多样性和性能上显著优越，证明了其在领域适应中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.09228', 'title': 'Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking', 'url': 'https://huggingface.co/papers/2504.09228', 'abstract': "Single-stream architectures using Vision Transformer (ViT) backbones show great potential for real-time UAV tracking recently. However, frequent occlusions from obstacles like buildings and trees expose a major drawback: these models often lack strategies to handle occlusions effectively. New methods are needed to enhance the occlusion resilience of single-stream ViT models in aerial tracking. In this work, we propose to learn Occlusion-Robust Representations (ORR) based on ViTs for UAV tracking by enforcing an invariance of the feature representation of a target with respect to random masking operations modeled by a spatial Cox process. Hopefully, this random masking approximately simulates target occlusions, thereby enabling us to learn ViTs that are robust to target occlusion for UAV tracking. This framework is termed ORTrack. Additionally, to facilitate real-time applications, we propose an Adaptive Feature-Based Knowledge Distillation (AFKD) method to create a more compact tracker, which adaptively mimics the behavior of the teacher model ORTrack according to the task's difficulty. This student model, dubbed ORTrack-D, retains much of ORTrack's performance while offering higher efficiency. Extensive experiments on multiple benchmarks validate the effectiveness of our method, demonstrating its state-of-the-art performance. Codes is available at https://github.com/wuyou3474/ORTrack.", 'score': 4, 'issue_id': 3325, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 апреля', 'en': 'April 12', 'zh': '4月12日'}, 'hash': 'f8abea0d3480728f', 'authors': ['You Wu', 'Xucheng Wang', 'Xiangyang Yang', 'Mengyuan Liu', 'Dan Zeng', 'Hengzhou Ye', 'Shuiwang Li'], 'affiliations': ['College of Computer Science and Engineering, Guilin University of Technology, China', 'School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, China', 'School of Computer Science, Fudan University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.09228.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#cv'], 'emoji': '🚁', 'ru': {'title': 'Устойчивое отслеживание БПЛА с помощью ViT и дистилляции знаний', 'desc': 'Эта статья представляет новый подход к отслеживанию БПЛА с использованием архитектуры Vision Transformer (ViT). Авторы предлагают метод обучения устойчивым к окклюзии представлениям (ORR) на основе случайного маскирования, моделируемого пространственным процессом Кокса. Они также разрабатывают адаптивный метод дистилляции знаний (AFKD) для создания более компактной модели трекера. Эксперименты показывают, что предложенный подход ORTrack и его облегченная версия ORTrack-D достигают современного уровня производительности в задаче отслеживания БПЛА.'}, 'en': {'title': 'Enhancing UAV Tracking Resilience with Occlusion-Robust ViTs', 'desc': 'This paper addresses the challenge of occlusions in UAV tracking using single-stream Vision Transformer (ViT) architectures. The authors introduce a method called Occlusion-Robust Representations (ORR) that enhances the resilience of ViTs to occlusions by simulating them through random masking operations. They also propose an Adaptive Feature-Based Knowledge Distillation (AFKD) technique to create a more efficient student model, ORTrack-D, which maintains high performance while being compact. Extensive experiments demonstrate that their approach achieves state-of-the-art results in real-time UAV tracking tasks.'}, 'zh': {'title': '提升无人机跟踪的遮挡鲁棒性', 'desc': '本文提出了一种基于视觉变换器（ViT）的单流架构，用于无人机（UAV）跟踪，旨在提高对遮挡的鲁棒性。我们引入了遮挡鲁棒表示（ORR），通过随机遮挡操作来模拟目标遮挡，从而增强模型的鲁棒性。为了实现实时应用，我们还提出了一种自适应特征知识蒸馏（AFKD）方法，创建了一个更紧凑的跟踪器，称为ORTrack-D。实验结果表明，我们的方法在多个基准测试中表现出色，具有先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.24379', 'title': 'Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.24379', 'abstract': 'To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/', 'score': 43, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'dce65db5da1b8c34', 'authors': ['Shengqiong Wu', 'Weicai Ye', 'Jiahao Wang', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Shuicheng Yan', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['Kuaishou Technology', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24379.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Универсальный контроль генерации видео через интерпретацию любых входных данных', 'desc': 'Any2Caption - это новая система для контролируемой генерации видео с использованием различных входных данных. Ключевая идея заключается в разделении этапов интерпретации условий и синтеза видео. Система использует мультимодальные большие языковые модели для создания подробных структурированных описаний на основе разнообразных входных данных. Авторы также представили большой набор данных Any2CapIns для обучения модели генерации описаний по различным условиям.'}, 'en': {'title': 'Revolutionizing Video Generation with Any2Caption', 'desc': "Any2Caption is a new framework designed to improve how user intentions are understood in video generation. It separates the process of interpreting different conditions from the actual video creation, allowing for more precise control. By using advanced multimodal large language models, it can transform various inputs like text and images into detailed captions that guide video generators more effectively. The introduction of the Any2CapIns dataset, which contains a large number of examples, further enhances the system's ability to generate high-quality videos based on diverse conditions."}, 'zh': {'title': '可控视频生成的新突破：Any2Caption', 'desc': '为了克服当前视频生成领域中准确理解用户意图的瓶颈，我们提出了Any2Caption，这是一个新颖的可控视频生成框架。其核心思想是将各种条件解释步骤与视频合成步骤解耦。通过利用现代多模态大语言模型（MLLMs），Any2Caption能够将文本、图像、视频及特定提示（如区域、运动和相机姿态）转化为密集的结构化字幕，从而为视频生成器提供更好的指导。我们还引入了Any2CapIns，这是一个包含337K实例和407K条件的大规模数据集，用于任何条件到字幕的指令调优。'}}}, {'id': 'https://huggingface.co/papers/2504.00050', 'title': 'JudgeLRM: Large Reasoning Models as a Judge', 'url': 'https://huggingface.co/papers/2504.00050', 'abstract': 'The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.', 'score': 29, 'issue_id': 3022, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '5060d3d364f635eb', 'authors': ['Nuo Chen', 'Zhiyuan Hu', 'Qingyun Zou', 'Jiaying Wu', 'Qian Wang', 'Bryan Hooi', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.00050.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#rl'], 'emoji': '⚖️', 'ru': {'title': 'Революция в обучении ИИ-судей: от простой настройки к глубоким рассуждениям', 'desc': 'Исследование показывает ограничения обычной тонкой настройки языковых моделей для задач оценки, требующих сложных рассуждений. Авторы предлагают новый подход JudgeLRM, использующий обучение с подкреплением для создания моделей-судей. JudgeLRM превосходит существующие методы, включая GPT-4, особенно в задачах, требующих глубоких рассуждений. Модель JudgeLRM-7B демонстрирует улучшение на 2.79% по F1-мере по сравнению с DeepSeek-R1.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Complex Judging Tasks', 'desc': 'This paper explores the effectiveness of Large Language Models (LLMs) as judges in evaluation tasks, particularly in scenarios that require complex reasoning. It highlights that traditional Supervised Fine-Tuning (SFT) methods do not perform well when faced with tasks that demand higher reasoning skills. The authors introduce JudgeLRM, a new approach that utilizes reinforcement learning (RL) to enhance the reasoning capabilities of LLMs by providing judge-specific rewards. The results show that JudgeLRM models significantly outperform both SFT-tuned models and other leading reasoning models, demonstrating superior performance in tasks that require deep reasoning.'}, 'zh': {'title': 'JudgeLRM：深度推理的评判者', 'desc': '本研究探讨了大型语言模型（LLMs）作为评估者的潜力，尤其是在复杂推理任务中的表现。我们发现，现有的监督微调（SFT）方法在处理需要深度推理的样本时效果不佳。为了解决这个问题，我们提出了JudgeLRM，这是一种基于强化学习（RL）训练的评判导向LLM，能够提供更有效的评估。实验结果表明，JudgeLRM模型在评判任务中表现优于传统的SFT模型和最先进的推理模型，尤其在需要深度推理的任务中表现突出。'}}}, {'id': 'https://huggingface.co/papers/2503.23145', 'title': 'CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis', 'url': 'https://huggingface.co/papers/2503.23145', 'abstract': 'Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.', 'score': 28, 'issue_id': 3021, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '945cc4b51522e668', 'authors': ['Anjiang Wei', 'Tarun Suresh', 'Jiannan Cao', 'Naveen Kannan', 'Yuheng Wu', 'Kai Yan', 'Thiago S. F. X. Teixeira', 'Ke Wang', 'Alex Aiken'], 'affiliations': ['Intel', 'MIT', 'Stanford University', 'University of Illinois Urbana-Champaign', 'Visa Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23145.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#plp', '#agents', '#training'], 'emoji': '🧠', 'ru': {'title': 'CodeARC: Новый рубеж в индуктивном синтезе программ', 'desc': 'Статья представляет CodeARC - новую систему оценки для индуктивного синтеза программ с использованием больших языковых моделей. В отличие от существующих статических методов, CodeARC позволяет агентам интерактивно взаимодействовать со скрытой целевой функцией, итеративно улучшая свои решения. Авторы создали масштабный бенчмарк из 1114 функций для оценки способностей моделей к индуктивному рассуждению и синтезу программ. Результаты показывают, что даже лучшие модели достигают лишь 52.7% успеха, что подчеркивает сложность задачи.'}, 'en': {'title': 'CodeARC: A New Frontier in Inductive Program Synthesis Evaluation', 'desc': 'This paper introduces CodeARC, a new evaluation framework for inductive program synthesis, which is the process of creating functions based on input-output examples. Unlike traditional methods that use static examples, CodeARC allows agents to interact with a hidden target function, enabling them to refine their solutions through feedback. The framework includes a large-scale benchmark with 1114 functions, demonstrating the challenges faced by models in this domain. The results show that fine-tuning models like LLaMA-3.1-8B-Instruct can significantly improve performance, emphasizing the need for dynamic evaluation in program synthesis tasks.'}, 'zh': {'title': 'CodeARC：提升程序合成的评估新标准', 'desc': '归纳程序合成，也称为示例编程，是从输入输出示例中合成函数的过程，要求能够推广到未见过的输入。虽然大型语言模型在自然语言指导的编程任务中表现出色，但它们在归纳程序合成方面的能力尚未得到充分探索。现有的评估协议依赖于静态示例集和保留测试，无法在合成函数错误时提供反馈，也未能反映现实世界中的场景。我们提出了CodeARC，一个新的评估框架，允许代理通过查询隐藏的目标函数与之互动，从而合成候选函数并根据反馈迭代改进解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.24376', 'title': 'Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1', 'url': 'https://huggingface.co/papers/2503.24376', 'abstract': "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.", 'score': 24, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'd22966d0969ded43', 'authors': ['Yi Chen', 'Yuying Ge', 'Rui Wang', 'Yixiao Ge', 'Lu Qiu', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.24376.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#rl', '#benchmark', '#optimization', '#video'], 'emoji': '🎥', 'ru': {'title': 'Обучение с подкреплением улучшает понимание видео мультимодальными ИИ-моделями', 'desc': 'Статья представляет новый бенчмарк SEED-Bench-R1 для оценки методов пост-обучения мультимодальных больших языковых моделей (MLLM) в задачах понимания видео. Исследователи сравнивают обучение с подкреплением (RL) и обычное обучение с учителем (SFT) на модели Qwen2-VL-Instruct-7B. Результаты показывают, что RL более эффективно использует данные и лучше работает как на распределении обучающей выборки, так и вне его. Однако анализ выявляет, что RL улучшает визуальное восприятие, но иногда производит менее логически связные цепочки рассуждений.'}, 'en': {'title': 'Unlocking Reasoning in Multimodal Models with SEED-Bench-R1', 'desc': "This paper discusses the advancements in Chain of Thought (COT) generation for Large Language Models (LLMs) and introduces a new benchmark called SEED-Bench-R1 for evaluating Multimodal Large Language Models (MLLMs) in video understanding tasks. The benchmark includes complex real-world videos and planning tasks presented as multiple-choice questions, assessing the models' perception and reasoning abilities. The study compares reinforcement learning (RL) with supervised fine-tuning (SFT) using the Qwen2-VL-Instruct-7B model, showing that RL is more data-efficient and performs better on various tasks. However, the analysis also highlights limitations in reasoning consistency and visual cue recognition, suggesting areas for future research to enhance model performance."}, 'zh': {'title': '强化学习提升多模态模型推理能力', 'desc': '最近，链式思维（COT）生成的进展显著提升了大型语言模型（LLMs）的推理能力，而强化学习（RL）成为一种有效的后训练方法。多模态大型语言模型（MLLMs）继承了这种推理潜力，但在需要感知和逻辑推理的任务中仍然未被充分探索。为了解决这个问题，我们引入了SEED-Bench-R1，这是一个旨在系统评估MLLMs在视频理解中后训练方法的基准，包含复杂的真实视频和多项选择题的日常规划任务。我们的研究表明，RL在数据效率和性能上优于监督微调（SFT），但在逻辑连贯性方面存在一定的局限性。'}}}, {'id': 'https://huggingface.co/papers/2504.00698', 'title': 'Command A: An Enterprise-Ready Large Language Model', 'url': 'https://huggingface.co/papers/2504.00698', 'abstract': 'In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.', 'score': 16, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '8670e6d1cc4f6bee', 'authors': ['Team Cohere', 'Aakanksha', 'Arash Ahmadian', 'Marwan Ahmed', 'Jay Alammar', 'Yazeed Alnumay', 'Sophia Althammer', 'Arkady Arkhangorodsky', 'Viraat Aryabumi', 'Dennis Aumiller', 'Raphaël Avalos', 'Zahara Aviv', 'Sammie Bae', 'Saurabh Baji', 'Alexandre Barbet', 'Max Bartolo', 'Björn Bebensee', 'Neeral Beladia', 'Walter Beller-Morales', 'Alexandre Bérard', 'Andrew Berneshawi', 'Anna Bialas', 'Phil Blunsom', 'Matt Bobkin', 'Adi Bongale', 'Sam Braun', 'Maxime Brunet', 'Samuel Cahyawijaya', 'David Cairuz', 'Jon Ander Campos', 'Cassie Cao', 'Kris Cao', 'Roman Castagné', 'Julián Cendrero', 'Leila Chan Currie', 'Yash Chandak', 'Diane Chang', 'Giannis Chatziveroglou', 'Hongyu Chen', 'Claire Cheng', 'Alexis Chevalier', 'Justin T. Chiu', 'Eugene Cho', 'Eugene Choi', 'Eujeong Choi', 'Tim Chung', 'Volkan Cirik', 'Ana Cismaru', 'Pierre Clavier', 'Henry Conklin', 'Lucas Crawhall-Stein', 'Devon Crouse', 'Andres Felipe Cruz-Salinas', 'Ben Cyrus', "Daniel D'souza", 'Hugo Dalla-Torre', 'John Dang', 'William Darling', 'Omar Darwiche Domingues', 'Saurabh Dash', 'Antoine Debugne', 'Théo Dehaze', 'Shaan Desai', 'Joan Devassy', 'Rishit Dholakia', 'Kyle Duffy', 'Ali Edalati', 'Ace Eldeib', 'Abdullah Elkady', 'Sarah Elsharkawy', 'Irem Ergün', 'Beyza Ermis', 'Marzieh Fadaee', 'Boyu Fan', 'Lucas Fayoux', 'Yannis Flet-Berliac', 'Nick Frosst', 'Matthias Gallé', 'Wojciech Galuba', 'Utsav Garg', 'Matthieu Geist', 'Mohammad Gheshlaghi Azar', 'Seraphina Goldfarb-Tarrant', 'Tomas Goldsack', 'Aidan Gomez', 'Victor Machado Gonzaga', 'Nithya Govindarajan', 'Manoj Govindassamy', 'Nathan Grinsztajn', 'Nikolas Gritsch', 'Patrick Gu', 'Shangmin Guo', 'Kilian Haefeli', 'Rod Hajjar', 'Tim Hawes', 'Jingyi He', 'Sebastian Hofstätter', 'Sungjin Hong', 'Sara Hooker', 'Tom Hosking', 'Stephanie Howe', 'Eric Hu', 'Renjie Huang', 'Hemant Jain', 'Ritika Jain', 'Nick Jakobi', 'Madeline Jenkins', 'JJ Jordan', 'Dhruti Joshi', 'Jason Jung', 'Trushant Kalyanpur', 'Siddhartha Rao Kamalakara', 'Julia Kedrzycki', 'Gokce Keskin', 'Edward Kim', 'Joon Kim', 'Wei-Yin Ko', 'Tom Kocmi', 'Michael Kozakov', 'Wojciech Kryściński', 'Arnav Kumar Jain', 'Komal Kumar Teru', 'Sander Land', 'Michael Lasby', 'Olivia Lasche', 'Justin Lee', 'Patrick Lewis', 'Jeffrey Li', 'Jonathan Li', 'Hangyu Lin', 'Acyr Locatelli', 'Kevin Luong', 'Raymond Ma', 'Lukas Mach', 'Marina Machado', 'Joanne Magbitang', 'Brenda Malacara Lopez', 'Aryan Mann', 'Kelly Marchisio', 'Olivia Markham', 'Alexandre Matton', 'Alex McKinney', 'Dominic McLoughlin', 'Jozef Mokry', 'Adrien Morisot', 'Autumn Moulder', 'Harry Moynehan', 'Maximilian Mozes', 'Vivek Muppalla', 'Lidiya Murakhovska', 'Hemangani Nagarajan', 'Alekhya Nandula', 'Hisham Nasir', 'Shauna Nehra', 'Josh Netto-Rosen', 'Daniel Ohashi', 'James Owers-Bardsley', 'Jason Ozuzu', 'Dennis Padilla', 'Gloria Park', 'Sam Passaglia', 'Jeremy Pekmez', 'Laura Penstone', 'Aleksandra Piktus', 'Case Ploeg', 'Andrew Poulton', 'Youran Qi', 'Shubha Raghvendra', 'Miguel Ramos', 'Ekagra Ranjan', 'Pierre Richemond', 'Cécile Robert-Michon', 'Aurélien Rodriguez', 'Sudip Roy', 'Laura Ruis', 'Louise Rust', 'Anubhav Sachan', 'Alejandro Salamanca', 'Kailash Karthik Saravanakumar', 'Isha Satyakam', 'Alice Schoenauer Sebag', 'Priyanka Sen', 'Sholeh Sepehri', 'Preethi Seshadri', 'Ye Shen', 'Tom Sherborne', 'Sylvie Chang Shi', 'Sanal Shivaprasad', 'Vladyslav Shmyhlo', 'Anirudh Shrinivason', 'Inna Shteinbuk', 'Amir Shukayev', 'Mathieu Simard', 'Ella Snyder', 'Ava Spataru', 'Victoria Spooner', 'Trisha Starostina', 'Florian Strub', 'Yixuan Su', 'Jimin Sun', 'Dwarak Talupuru', 'Eugene Tarassov', 'Elena Tommasone', 'Jennifer Tracey', 'Billy Trend', 'Evren Tumer', 'Ahmet Üstün', 'Bharat Venkitesh', 'David Venuto', 'Pat Verga', 'Maxime Voisin', 'Alex Wang', 'Donglu Wang', 'Shijian Wang', 'Edmond Wen', 'Naomi White', 'Jesse Willman', 'Marysia Winkels', 'Chen Xia', 'Jessica Xie', 'Minjie Xu', 'Bowen Yang', 'Tan Yi-Chern', 'Ivan Zhang', 'Zhenyu Zhao', 'Zhoujie Zhao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.00698.jpg', 'data': {'categories': ['#training', '#low_resource', '#agents', '#open_source', '#rag', '#multilingual', '#architecture', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'Command A: Мощная многоязычная ИИ-модель для бизнеса', 'desc': 'В статье описывается разработка Command A - мощной языковой модели, оптимизированной для корпоративного использования. Модель поддерживает 23 языка и обладает гибридной архитектурой, сочетающей эффективность и высокую производительность. Command A предлагает передовые возможности Retrieval Augmented Generation (RAG) для автоматизации сложных бизнес-процессов. Эти возможности достигаются с помощью децентрализованного обучения, включая алгоритмы самоулучшения и техники объединения моделей.'}, 'en': {'title': 'Empowering Enterprises with Command A: The Future of Language Models', 'desc': 'This paper presents Command A, a large language model designed specifically for enterprise applications. It features a hybrid architecture that optimizes both performance and efficiency, supporting 23 languages to cater to global business needs. The model excels in Retrieval Augmented Generation (RAG), enabling it to automate complex business processes through effective grounding and tool usage. The training process incorporates decentralized methods, including self-refinement and model merging, and the paper also discusses the similar Command R7B model, providing insights into their training and evaluation results.'}, 'zh': {'title': 'Command A：企业应用的强大语言模型', 'desc': '本文介绍了Command A的开发，这是一种专为企业实际应用而设计的大型语言模型。Command A具备多语言能力，支持23种全球商业语言，并采用新颖的混合架构，兼顾效率与高性能。它提供了最佳的检索增强生成（RAG）能力，能够通过工具使用和基础知识支持来自动化复杂的业务流程。我们还展示了与Command A相似的Command R7B模型的结果，并发布了这两个模型的权重以供研究使用。'}}}, {'id': 'https://huggingface.co/papers/2504.01016', 'title': 'GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors', 'url': 'https://huggingface.co/papers/2504.01016', 'abstract': 'Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.', 'score': 15, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '9430b45c3324fb61', 'authors': ['Tian-Xing Xu', 'Xiangjun Gao', 'Wenbo Hu', 'Xiaoyu Li', 'Song-Hai Zhang', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'HKUST', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01016.jpg', 'data': {'categories': ['#video', '#3d', '#architecture', '#diffusion', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'GeometryCrafter: Высокоточная оценка глубины видео для 3D реконструкции', 'desc': 'GeometryCrafter - это новая система для высокоточной оценки глубины видео. Она использует вариационный автоэнкодер для кодирования карт точек и диффузионную модель для моделирования последовательностей карт точек. Система позволяет выполнять точную 3D/4D реконструкцию и оценку параметров камеры. Эксперименты показали, что GeometryCrafter превосходит существующие методы по точности 3D, временной согласованности и способности к обобщению.'}, 'en': {'title': 'GeometryCrafter: Elevating Video Depth Estimation with High-Fidelity Point Maps', 'desc': 'This paper introduces GeometryCrafter, a new framework designed to improve video depth estimation by producing high-fidelity point map sequences that maintain temporal coherence. It addresses the limitations of existing methods in achieving accurate geometric representations, which are crucial for tasks like 3D reconstruction and camera parameter estimation. The framework utilizes a point map Variational Autoencoder (VAE) to effectively encode and decode point maps, independent of the video latent distributions. By training a video diffusion model on these point map sequences, GeometryCrafter demonstrates superior performance in 3D accuracy and generalization across various datasets.'}, 'zh': {'title': 'GeometryCrafter：高保真视频深度估计的新框架', 'desc': '尽管视频深度估计取得了显著进展，但现有方法在几何保真度方面存在固有局限，限制了其在重建和其他度量基础下游任务中的应用。我们提出了GeometryCrafter，这是一种新颖的框架，可以从开放世界视频中恢复具有时间一致性的高保真点图序列，从而实现准确的3D/4D重建和相机参数估计。我们的方法核心是一个点图变分自编码器（VAE），它学习一个与视频潜在分布无关的潜在空间，以有效地进行点图编码和解码。通过利用VAE，我们训练了一个视频扩散模型，以建模基于输入视频的点图序列的分布。'}}}, {'id': 'https://huggingface.co/papers/2504.00810', 'title': 'Z1: Efficient Test-time Scaling with Code', 'url': 'https://huggingface.co/papers/2504.00810', 'abstract': 'Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.', 'score': 15, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'd982593a14ba7da9', 'authors': ['Zhaojian Yu', 'Yinghao Wu', 'Yilun Zhao', 'Arman Cohan', 'Xiao-Ping Zhang'], 'affiliations': ['Tsinghua University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00810.jpg', 'data': {'categories': ['#reasoning', '#rl', '#dataset', '#training', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование рассуждений в языковых моделях', 'desc': 'Статья представляет эффективный метод масштабирования языковых моделей во время тестирования, основанный на обучении траекториям рассуждений, связанным с кодом. Авторы создали датасет Z1-Code-Reasoning-107K, содержащий задачи по программированию с короткими и длинными решениями. Они также предложили технику Shifted Thinking Window для уменьшения избыточных токенов мышления. Модель Z1-7B, обученная на этих данных, демонстрирует способность адаптировать уровень рассуждений к сложности задач и обобщать на более широкие задачи рассуждения.'}, 'en': {'title': 'Efficient Reasoning in Large Language Models', 'desc': 'This paper introduces a new method for improving the efficiency of Large Language Models (LLMs) during problem-solving by reducing unnecessary reasoning tokens. The authors created a dataset called Z1-Code-Reasoning-107K, which includes various coding problems and their solution paths. They also developed a technique called the Shifted Thinking Window, which helps the model focus on relevant information and limits excessive reasoning. The resulting model, Z1-7B, shows strong performance on complex tasks while using significantly fewer reasoning tokens compared to other models.'}, 'zh': {'title': '高效推理，简化思考！', 'desc': '本文提出了一种高效的测试时间扩展方法，旨在通过训练大型语言模型（LLMs）在代码相关的推理轨迹上，减少多余的思考标记，同时保持性能。我们创建了一个名为Z1-Code-Reasoning-107K的数据集，包含简单和复杂的编码问题及其解决轨迹。我们还提出了一种新颖的移位思维窗口，通过去除上下文分隔标签和限制推理标记，来减轻过度思考的负担。经过训练的模型Z1-7B能够根据问题的复杂性调整推理水平，并在不同的推理任务中实现高效的测试时间扩展。'}}}, {'id': 'https://huggingface.co/papers/2504.00595', 'title': 'Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources', 'url': 'https://huggingface.co/papers/2504.00595', 'abstract': 'The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.', 'score': 15, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '3e8c667bd93d754e', 'authors': ['Weizhi Wang', 'Yu Tian', 'Linjie Yang', 'Heng Wang', 'Xifeng Yan'], 'affiliations': ['Nvidia Research', 'Seed Vision Team, ByteDance', 'UC Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2504.00595.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#training', '#data', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Открытая и эффективная мультимодальная ИИ-модель', 'desc': 'Статья представляет Open-Qwen2VL - полностью открытую мультимодальную языковую модель с 2 миллиардами параметров. Модель была эффективно обучена на 29 миллионах пар изображение-текст, используя всего 442 часа GPU A100-40G. Авторы применили динамическое разрешение изображений и мультимодальную упаковку последовательностей для повышения эффективности предобучения. Open-Qwen2VL превосходит частично открытую модель Qwen2-VL-2B по различным мультимодальным бенчмаркам, демонстрируя высокую эффективность обучения.'}, 'en': {'title': 'Unlocking Efficiency in Multimodal LLMs with Open-Qwen2VL', 'desc': 'The paper presents Open-Qwen2VL, a multimodal large language model (LLM) that is fully open-source and pre-trained on 29 million image-text pairs. It addresses challenges in multimodal LLM pre-training by utilizing advanced data filtering techniques and efficient training strategies, achieving significant improvements in training efficiency. The model is trained using a dynamic image resolution approach and multimodal sequence packing, which enhances the overall performance while reducing resource consumption. Open-Qwen2VL outperforms existing models on various benchmarks, showcasing its effectiveness and the benefits of open-source collaboration in machine learning.'}, 'zh': {'title': '高效开源的多模态大语言模型', 'desc': '本文介绍了Open-Qwen2VL，这是一个完全开源的多模态大语言模型，具有20亿参数，使用2900万对图像-文本数据进行高效预训练。我们采用了动态图像分辨率和多模态序列打包技术，显著提高了预训练的效率。通过使用MLLM和CLIP的过滤技术，提升了数据质量和训练效率。最终，Open-Qwen2VL在多个多模态基准测试中超越了部分开源的最先进模型，展示了其卓越的训练效率。'}}}, {'id': 'https://huggingface.co/papers/2504.01019', 'title': 'MixerMDM: Learnable Composition of Human Motion Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01019', 'abstract': 'Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.', 'score': 13, 'issue_id': 3022, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '745108d1df40d1b4', 'authors': ['Pablo Ruiz-Ponce', 'German Barquero', 'Cristina Palmero', 'Sergio Escalera', 'José García-Rodríguez'], 'affiliations': ['Kings College London, UK', 'Universidad de Alicante, Spain', 'Universitat de Barcelona and Computer Vision Center, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2504.01019.jpg', 'data': {'categories': ['#multimodal', '#cv', '#diffusion', '#benchmark', '#dataset'], 'emoji': '🕺', 'ru': {'title': 'Динамическое смешивание диффузионных моделей для улучшенной генерации движений человека', 'desc': 'Статья представляет MixerMDM - первую обучаемую технику композиции моделей для объединения предобученных диффузионных моделей генерации движений человека на основе текстовых описаний. В отличие от предыдущих подходов, MixerMDM обеспечивает динамическую стратегию смешивания, которая обучается состязательным образом для комбинирования процесса шумоподавления каждой модели в зависимости от заданных условий. Используя MixerMDM для объединения диффузионных моделей движения одного и нескольких человек, авторы достигают тонкого контроля над динамикой каждого человека индивидуально, а также над общим взаимодействием. Кроме того, предлагается новая техника оценки, которая измеряет качество взаимодействия и индивидуальных движений.'}, 'en': {'title': 'Dynamic Control of Human Motion Generation with MixerMDM', 'desc': 'This paper addresses the challenge of generating human motion based on textual descriptions by introducing MixerMDM, a novel learnable model composition technique. Unlike previous methods, MixerMDM dynamically combines pre-trained motion diffusion models in an adversarial manner, allowing for better control over the generated motions based on specific conditions. The approach enables fine-grained control over individual and overall interactions in multi-person scenarios. Additionally, the authors propose a new evaluation method to assess the quality of generated motions in relation to their conditions, highlighting the adaptability of MixerMDM during the denoising process.'}, 'zh': {'title': '动态混合，精细控制人类运动生成', 'desc': '本文提出了一种新的模型组合技术，称为MixerMDM，用于结合预训练的文本条件人类运动扩散模型。与以往的方法不同，MixerMDM采用动态混合策略，通过对抗训练学习如何根据生成条件组合去噪过程。该方法能够实现对单人和多人运动的精细控制，提升了每个人的动态表现及整体互动效果。此外，本文还提出了一种新的评估技术，首次量化了生成运动与条件之间的对齐程度，以及MixerMDM在去噪过程中适应混合的能力。'}}}, {'id': 'https://huggingface.co/papers/2504.00906', 'title': 'Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents', 'url': 'https://huggingface.co/papers/2504.00906', 'abstract': 'Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.', 'score': 13, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'e51174b579a417e9', 'authors': ['Saaket Agashe', 'Kyle Wong', 'Vincent Tu', 'Jiachen Yang', 'Ang Li', 'Xin Eric Wang'], 'affiliations': ['Simular Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00906.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Agent S2: Новый уровень автоматизации компьютерных задач с помощью ИИ', 'desc': 'Статья представляет Agent S2 - новую композиционную систему для автоматизации компьютерных задач через графический интерфейс. Система использует множество специализированных и обобщенных моделей, а также новые методы точной локализации элементов интерфейса и иерархического планирования действий. Agent S2 достигает значительных улучшений производительности по сравнению с существующими решениями на нескольких бенчмарках для разных операционных систем. Авторы утверждают, что их подход открывает новые возможности для повышения продуктивности человека при работе с компьютером.'}, 'en': {'title': 'Agent S2: Revolutionizing Task Automation with Smart Planning and Grounding', 'desc': 'This paper presents Agent S2, a new framework designed to improve the performance of computer use agents that automate tasks by interacting with graphical user interfaces (GUIs). The framework addresses key challenges such as accurately identifying GUI elements and planning complex tasks over time. It introduces a Mixture-of-Grounding technique for better GUI localization and Proactive Hierarchical Planning to adapt action plans based on real-time observations. Evaluations show that Agent S2 outperforms existing agents on multiple benchmarks, demonstrating significant improvements in task execution across different operating systems.'}, 'zh': {'title': 'Agent S2：智能代理的新纪元', 'desc': '本文介绍了一种名为Agent S2的新型智能代理框架，旨在通过将认知任务分配给不同的通用模型和专业模型来提高数字任务的自动化效率。我们提出了一种新的混合定位技术，以实现精确的图形用户界面（GUI）元素定位，并引入了主动层次规划，能够根据不断变化的观察动态调整行动计划。评估结果表明，Agent S2在三个主要的计算机使用基准测试中达到了新的最先进性能，显著超越了现有的领先代理。特别是在OSWorld评估中，Agent S2相较于其他代理实现了18.9%和32.7%的相对提升，展现了其在不同操作系统和应用中的良好泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2504.00509', 'title': 'Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?', 'url': 'https://huggingface.co/papers/2504.00509', 'abstract': "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.", 'score': 12, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'c9697e67f23cfa4e', 'authors': ['Kai Yan', 'Yufei Xu', 'Zhengyin Du', 'Xuesong Yao', 'Zheyu Wang', 'Xiaowen Guo', 'Jiecao Chen'], 'affiliations': ['ByteDance Seed', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.00509.jpg', 'data': {'categories': ['#multimodal', '#hallucinations', '#benchmark', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Языковые модели: умные рассуждения или простое воспроизведение?', 'desc': 'Авторы статьи предлагают новый мультимодальный бенчмарк RoR-Bench для выявления склонности языковых моделей (LLM) к простому воспроизведению заученных решений. Исследование показало, что даже передовые LLM демонстрируют серьезные проблемы с рассуждениями при небольших изменениях в условиях задач. Результаты указывают на значительное снижение производительности ведущих моделей (до 60%) на элементарных арифметических и логических задачах при изменении всего одной фразы. Авторы призывают сообщество переосмыслить реальный уровень интеллекта современных LLM.'}, 'en': {'title': 'Reassessing LLM Intelligence: Are They Truly Reasoning?', 'desc': 'This paper introduces RoR-Bench, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs). The authors investigate whether LLMs demonstrate genuine intelligence or merely replicate learned responses from their training data. Their empirical analysis reveals that even advanced LLMs, like OpenAI-o1 and DeepSeek-R1, show significant performance drops—up to 60%—when faced with slight changes in problem phrasing. This raises important questions about the actual reasoning abilities of these models and suggests a need for a deeper understanding of their intelligence.'}, 'zh': {'title': '重新审视LLM的智能水平', 'desc': '近年来，LLM基准测试的难度从小学水平迅速上升到前沿问题，这让研究人员感到我们离超越人类智能只有一步之遥。然而，LLM的推理能力是否真的是人类标准下的真正智能，还是仅仅在训练中见过的解决方案的复述？为了解决这个问题，我们提出了RoR-Bench，一个新颖的多模态基准，用于检测LLM在简单推理问题中是否存在复述行为。我们的实证分析发现，现有的顶尖LLM在条件稍微改变时，表现出极其严重的复述行为，这促使我们重新评估这些模型的真实智能水平。'}}}, {'id': 'https://huggingface.co/papers/2503.24377', 'title': 'Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models', 'url': 'https://huggingface.co/papers/2503.24377', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.', 'score': 11, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'd18ba97a459aea36', 'authors': ['Rui Wang', 'Hongru Wang', 'Boyang Xue', 'Jianhui Pang', 'Shudong Liu', 'Yi Chen', 'Jiahao Qiu', 'Derek Fai Wong', 'Heng Ji', 'Kam-Fai Wong'], 'affiliations': ['Princeton University', 'The Chinese University of Hong Kong', 'The University of Hong Kong', 'University of Illinois Urbana-Champaign', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.24377.jpg', 'data': {'categories': ['#inference', '#survey', '#reasoning', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Экономия рассуждений в больших языковых моделях: балансируя производительность и затраты', 'desc': 'Статья рассматривает проблему баланса между производительностью и вычислительными затратами в больших языковых моделях (LLM) при выполнении задач рассуждения. Авторы анализируют причины неэффективности рассуждений, различные паттерны рассуждений и потенциальные решения для достижения экономии рассуждений. Исследование охватывает как этап после обучения, так и этап вывода во время тестирования LLM. Авторы стремятся предоставить ценные insights и выделить открытые проблемы для продвижения исследований в этой области.'}, 'en': {'title': 'Balancing Performance and Cost in Language Model Reasoning', 'desc': 'This paper discusses the advancements in Large Language Models (LLMs) that allow them to perform complex reasoning tasks more effectively. It highlights the difference between two types of reasoning: System 1, which is fast and efficient but less accurate, and System 2, which is slower and more accurate but computationally expensive. The authors introduce the concept of reasoning economy, which aims to balance the trade-off between performance and computational costs. They analyze the inefficiencies in reasoning, explore different reasoning patterns, and propose solutions to enhance the reasoning economy of LLMs, providing insights for future research.'}, 'zh': {'title': '推理经济：平衡性能与计算成本的关键', 'desc': '近年来，大型语言模型（LLMs）的进步显著提升了其执行复杂推理任务的能力，尤其是在快速直觉思维（系统1）与缓慢深度推理（系统2）之间的转变。虽然系统2推理提高了任务的准确性，但由于其思维缓慢和推理行为低效，往往会带来较高的计算成本。相对而言，系统1推理计算效率高，但可能导致性能不佳。因此，平衡性能与计算成本之间的权衡，形成了推理经济的概念，这是本研究的核心。'}}}, {'id': 'https://huggingface.co/papers/2503.22952', 'title': 'OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts', 'url': 'https://huggingface.co/papers/2503.22952', 'abstract': 'The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.', 'score': 11, 'issue_id': 3024, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '8b78ccf427a5cdc0', 'authors': ['Yuxuan Wang', 'Yueqian Wang', 'Bo Chen', 'Tong Wu', 'Dongyan Zhao', 'Zilong Zheng'], 'affiliations': ['Beijing Institute for General Artificial Intelligence', 'State Key Laboratory of General Artificial Intelligence', 'Wangxuan Institute of Computer Technology, Peking University', 'X-LANCE Lab, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22952.jpg', 'data': {'categories': ['#video', '#games', '#inference', '#multimodal', '#reasoning', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'OmniMMI: Новый стандарт для оценки мультимодальных моделей в потоковом видео', 'desc': 'Статья представляет OmniMMI - комплексный бенчмарк для оценки возможностей мультимодальных языковых моделей в контексте потокового видео. Бенчмарк включает более 1000 видео и 2000 вопросов, охватывающих шесть различных подзадач. Авторы также предлагают новую архитектуру Multi-modal Multiplexing Modeling (M4) для эффективной обработки потокового видео. Этот подход позволяет модели одновременно воспринимать визуальную и аудиоинформацию во время генерации текста.'}, 'en': {'title': 'Enhancing Interaction with OmniLLMs in Streaming Video', 'desc': 'This paper presents OmniMMI, a new benchmark for evaluating Omni language models (OmniLLMs) in the context of streaming video. It addresses the challenges of understanding and reasoning in real-time video interactions, which are often overlooked in current benchmarks. The benchmark includes a large dataset of over 1,121 videos and 2,290 questions, focusing on proactive reasoning across six subtasks. Additionally, the authors introduce a framework called Multi-modal Multiplexing Modeling (M4) that enhances the efficiency of streaming models by allowing them to process audio and visual data simultaneously while generating responses.'}, 'zh': {'title': '提升多模态语言模型的互动能力', 'desc': '本论文介绍了OmniMMI，这是一个专为Omni语言模型在流媒体视频环境中设计的多模态交互基准。该基准包含超过1121个视频和2290个问题，旨在解决现有视频基准中流媒体视频理解和主动推理的两个关键挑战。我们还提出了一种新框架，称为多模态复用建模（M4），旨在实现高效推理的流媒体模型，能够在生成内容的同时进行视觉和听觉处理。通过这些创新，我们希望提升多模态语言模型在实际应用中的互动能力。'}}}, {'id': 'https://huggingface.co/papers/2504.00927', 'title': 'Multi-Token Attention', 'url': 'https://huggingface.co/papers/2504.00927', 'abstract': 'Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other\'s attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector\'s capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method\'s ability to leverage richer information proves particularly beneficial.', 'score': 10, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '2af4c7adceecde31', 'authors': ['Olga Golovneva', 'Tianlu Wang', 'Jason Weston', 'Sainbayar Sukhbaatar'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2504.00927.jpg', 'data': {'categories': ['#long_context', '#architecture', '#benchmark', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Многотокенное внимание: новый шаг в повышении точности языковых моделей', 'desc': 'Статья представляет новый метод внимания для языковых моделей - Multi-Token Attention (MTA). В отличие от стандартного механизма soft attention, MTA позволяет учитывать информацию из нескольких токенов запроса и ключа одновременно. Это достигается с помощью операций свертки над запросами, ключами и головами внимания. MTA демонстрирует улучшенную производительность на ряде популярных бенчмарков, особенно в задачах, требующих поиска информации в длинных контекстах.'}, 'en': {'title': 'Unlocking Richer Context with Multi-Token Attention', 'desc': 'This paper introduces Multi-Token Attention (MTA), a novel attention mechanism designed to improve the performance of large language models (LLMs). Unlike traditional single token attention, which relies on a single query and key token vector, MTA allows for the simultaneous consideration of multiple query and key vectors. By utilizing convolution operations, MTA enhances the interaction between nearby queries and keys, leading to more accurate attention weights. The results show that MTA significantly outperforms standard Transformer models, especially in tasks involving long contexts and information retrieval.'}, 'zh': {'title': '多令牌注意力：提升上下文理解的关键', 'desc': '软注意力机制是大型语言模型（LLMs）中一个重要的组成部分，用于在给定上下文中定位相关部分。然而，传统的单个令牌注意力方法仅依赖于单个查询和键向量的相似性，这限制了信息的使用。为了解决这个问题，我们提出了一种新的注意力方法——多令牌注意力（MTA），它允许LLMs同时基于多个查询和键向量来调整注意力权重。通过对查询、键和头部应用卷积操作，我们的方法能够利用更丰富的信息，从而在长上下文中更准确地定位相关内容。'}}}, {'id': 'https://huggingface.co/papers/2504.01017', 'title': 'Scaling Language-Free Visual Representation Learning', 'url': 'https://huggingface.co/papers/2504.01017', 'abstract': 'Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.', 'score': 9, 'issue_id': 3023, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '9ab970f68b26c2ea', 'authors': ['David Fan', 'Shengbang Tong', 'Jiachen Zhu', 'Koustuv Sinha', 'Zhuang Liu', 'Xinlei Chen', 'Michael Rabbat', 'Nicolas Ballas', 'Yann LeCun', 'Amir Bar', 'Saining Xie'], 'affiliations': ['FAIR, Meta', 'New York University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01017.jpg', 'data': {'categories': ['#cv', '#training', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Визуальное самообучение не уступает языковому контролю при масштабировании', 'desc': 'Исследование сравнивает методы визуального самоконтролируемого обучения (SSL) и контрастивного обучения на языке и изображениях (CLIP) в задачах мультимодального анализа. Авторы обнаружили, что при обучении на одинаковых данных, модели SSL масштабируются лучше и достигают производительности CLIP на различных задачах визуального анализа вопросов и ответов (VQA). Результаты показывают, что чисто визуальное SSL может соответствовать обучению с языковым контролем при увеличении масштаба. Это открывает новые возможности для обучения визуальных представлений без использования языковой разметки.'}, 'en': {'title': 'Visual SSL: Bridging the Gap with Scale and Data', 'desc': 'This paper investigates the performance gap between Visual Self-Supervised Learning (SSL) and Contrastive Language-Image Pretraining (CLIP) in multimodal tasks like Visual Question Answering (VQA). The authors explore whether this gap is due to the absence of language supervision in visual SSL or differences in the training datasets used. By training both types of models on the same MetaCLIP data, they find that visual SSL models can outperform CLIP models when scaled up, achieving comparable performance on various benchmarks. This suggests that visual SSL can effectively compete with language-supervised methods, highlighting its potential for advancing vision-centric representation learning.'}, 'zh': {'title': '视觉自监督学习的潜力与CLIP相媲美', 'desc': '本研究探讨了视觉自监督学习（SSL）与对比语言-图像预训练（CLIP）在多模态任务中的表现差异。我们通过在相同的MetaCLIP数据上训练视觉SSL和CLIP模型，来分析语言监督的缺乏是否是导致视觉SSL落后的原因。结果显示，视觉SSL模型在数据和模型容量方面的扩展性优于CLIP模型，并且在参数达到70亿时性能仍未饱和。我们的发现表明，纯视觉SSL在大规模下可以达到与语言监督视觉预训练相当的性能，为视觉中心的表示学习开辟了新的机会。'}}}, {'id': 'https://huggingface.co/papers/2504.01005', 'title': 'When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning', 'url': 'https://huggingface.co/papers/2504.01005', 'abstract': 'Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.', 'score': 9, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'ee8e4951bf6c7a18', 'authors': ['Nishad Singhi', 'Hritik Bansal', 'Arian Hosseini', 'Aditya Grover', 'Kai-Wei Chang', 'Marcus Rohrbach', 'Anna Rohrbach'], 'affiliations': ['Google DeepMind', 'Mila', 'TU Darmstadt & hessian.AI', 'University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2504.01005.jpg', 'data': {'categories': ['#training', '#math', '#optimization', '#reasoning', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Баланс между генерацией решений и их верификацией в языковых моделях', 'desc': 'Статья исследует стратегии улучшения способностей больших языковых моделей (LLM) к рассуждениям, особенно в задачах математического характера. Авторы сравнивают метод Self-Consistency (SC), генерирующий множество решений и выбирающий наиболее частое, с подходом Generative Reward Models (GenRM), который оценивает решения путем генерации цепочек рассуждений. Исследование показывает, что SC более эффективен по вычислительным ресурсам для большинства практических задач. Авторы также выводят законы масштабирования для парадигмы GenRM, предоставляя практические рекомендации по оптимизации вычислений во время тестирования.'}, 'en': {'title': 'Balancing Solution Generation and Verification for Efficient Reasoning in LLMs', 'desc': 'This paper explores how to improve the reasoning abilities of large language models (LLMs) during problem-solving by adjusting the amount of computation used at test time. It compares two methods: Self-Consistency (SC), which generates multiple answers and picks the most common, and Generative Reward Models (GenRM), which scores answers based on a next-token prediction approach. The study finds that SC is generally more efficient in terms of compute resources compared to GenRM, especially under limited budgets. The authors provide insights on how to effectively balance the generation of solutions and their verification to optimize performance.'}, 'zh': {'title': '优化推理能力：解生成与验证的平衡', 'desc': '本文探讨了在大语言模型（LLMs）中，如何通过扩展测试时计算来提升推理能力，尤其是在数学问题解决任务中。传统的自一致性（Self-Consistency, SC）方法通过生成多个解并采用多数投票选择最常见的答案。最近的生成奖励模型（Generative Reward Models, GenRM）则将验证重构为下一个标记预测任务，从而在推理时引入新的扩展方式。研究表明，在固定的推理预算下，SC在大多数实际情况下比GenRM更具计算效率，提供了在测试时扩展中优化解生成与验证的实用指导。'}}}, {'id': 'https://huggingface.co/papers/2503.23434', 'title': 'Towards Trustworthy GUI Agents: A Survey', 'url': 'https://huggingface.co/papers/2503.23434', 'abstract': 'GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.', 'score': 9, 'issue_id': 3024, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'e19e4d94bcea9cb0', 'authors': ['Yucheng Shi', 'Wenhao Yu', 'Wenlin Yao', 'Wenhu Chen', 'Ninghao Liu'], 'affiliations': ['Amazon', 'Tencent AI Seattle Lab', 'University of Georgia', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23434.jpg', 'data': {'categories': ['#ethics', '#security', '#agents', '#survey', '#training', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Доверие к ИИ: обеспечение надежности графических агентов', 'desc': 'Это обзор исследует надежность графических агентов искусственного интеллекта, взаимодействующих с цифровыми интерфейсами. Рассматриваются пять ключевых аспектов: уязвимости безопасности, надежность в динамических средах, прозрачность и объяснимость, этические вопросы и методологии оценки. Выявлены основные проблемы, такие как уязвимость к состязательным атакам и каскадные сбои при последовательном принятии решений. Подчеркивается необходимость разработки надежных стандартов безопасности и ответственных практик разработки для широкого внедрения графических ИИ-агентов.'}, 'en': {'title': 'Ensuring Trust in Autonomous GUI Agents', 'desc': 'This paper surveys the trustworthiness of GUI agents that use large foundation models to interact with digital interfaces. It highlights five critical dimensions of trustworthiness: security vulnerabilities, reliability in changing environments, transparency, ethical considerations, and evaluation methods. The authors discuss significant challenges such as susceptibility to adversarial attacks and the need for realistic evaluation benchmarks. The paper emphasizes the importance of developing robust safety standards and responsible practices as GUI agents become more prevalent.'}, 'zh': {'title': '构建可信赖的GUI代理，保障安全与隐私', 'desc': '本论文探讨了基于大型基础模型的图形用户界面（GUI）代理的信任性问题。我们分析了五个关键维度，包括安全漏洞、动态环境中的可靠性、透明性与可解释性、伦理考量以及评估方法。研究还指出了主要挑战，如对抗性攻击的脆弱性、序列决策中的级联失败模式，以及缺乏现实的评估基准。这些问题不仅阻碍了GUI代理的实际应用，还需要超越任务成功的全面缓解策略。'}}}, {'id': 'https://huggingface.co/papers/2503.23733', 'title': 'AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization', 'url': 'https://huggingface.co/papers/2503.23733', 'abstract': 'Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.', 'score': 8, 'issue_id': 3017, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'ed45063868071c13', 'authors': ['Yiyang Du', 'Xiaochen Wang', 'Chi Chen', 'Jiabo Ye', 'Yiru Wang', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Zhifang Sui', 'Maosong Sun', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China', 'Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China', 'Institute of Intelligent Computing, Alibaba Group', 'Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China', 'ModelTC Open Source Organization, Beijing, China', 'School of Software Microelectronics, Peking University, Beijing, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.23733.jpg', 'data': {'categories': ['#training', '#architecture', '#transfer_learning', '#optimization', '#multimodal', '#benchmark'], 'emoji': '🔀', 'ru': {'title': 'AdaMMS: Эффективное слияние разнородных мультимодальных языковых моделей', 'desc': 'Статья представляет AdaMMS - новый метод объединения мультимодальных языковых моделей (MLLM) с разнородной архитектурой. Метод включает три этапа: отображение параметров между моделями, их слияние с помощью линейной интерполяции и поиск оптимальных гиперпараметров. AdaMMS решает проблемы, связанные с различиями в архитектуре и асимметрией в пространстве параметров разнородных MLLM. Эксперименты показали превосходство AdaMMS над существующими методами объединения моделей на различных мультимодальных задачах.'}, 'en': {'title': 'Merging Diverse Models with AdaMMS', 'desc': 'This paper introduces AdaMMS, a new method for merging Multimodal Large Language Models (MLLMs) that have different architectures. Traditional merging techniques struggle with these heterogeneous models due to their varying structures and parameter spaces. AdaMMS addresses this by first mapping the models, then merging their weights through linear interpolation, and finally optimizing hyper-parameters using an unsupervised approach. The results show that AdaMMS significantly improves performance on vision-language tasks compared to earlier methods.'}, 'zh': {'title': '异质模型合并的新突破', 'desc': '最近，模型合并方法在结合多个大型语言模型（LLMs）在不同任务上的能力方面表现出强大的优势。以往的模型合并方法主要集中在合并具有相同架构的同质模型，但在处理具有固有异质性的多模态大型语言模型（MLLMs）时面临挑战。我们提出了AdaMMS，这是一种专为异质MLLMs设计的新型模型合并方法，采用映射、合并和搜索三个步骤来解决这些挑战。通过设计模型之间的映射函数、对模型权重进行线性插值以及提出无监督的超参数选择方法，AdaMMS在各种视觉-语言基准测试中超越了以往的模型合并方法。'}}}, {'id': 'https://huggingface.co/papers/2504.00557', 'title': 'Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features', 'url': 'https://huggingface.co/papers/2504.00557', 'abstract': 'Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.', 'score': 7, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'c5628fbf1a189a06', 'authors': ['Jewon Lee', 'Ki-Ung Song', 'Seungmin Yang', 'Donguk Lim', 'Jaeyeon Kim', 'Wooksu Shin', 'Bo-Kyeong Kim', 'Yong Jae Lee', 'Tae-Ho Kim'], 'affiliations': ['Nota Inc.', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.00557.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#cv'], 'emoji': '✂️', 'ru': {'title': 'Эффективное сжатие визуальных данных в мультимодальных ИИ-моделях', 'desc': 'Статья представляет метод снижения вычислительных затрат в крупных визуально-языковых моделях путем сокращения визуальных токенов. Авторы фокусируются на моделях с кросс-вниманием, выявляя проблему большого размера кэша ключ-значение для визуальных токенов. Они предлагают алгоритм избирательного удаления избыточных визуальных признаков, основанный на разреженности карт кросс-внимания. Метод позволяет снизить задержку и использование памяти при сохранении производительности модели на уровне базовых показателей.'}, 'en': {'title': 'Trimmed Llama: Efficient Visual Token Reduction for Faster Inference', 'desc': 'This paper presents a method called Trimmed Llama, which reduces the number of visual tokens in large vision-language models (LVLMs) to lower inference costs. Unlike previous studies that focused on self-attention models, this work specifically targets cross-attention models, which are known for their better performance. The authors highlight that the key-value (KV) cache for image tokens in cross-attention layers is much larger than that for text tokens, creating a significant computational bottleneck. By selectively pruning redundant visual features based on the sparse nature of cross-attention maps, the model achieves a 50% reduction in visual features, leading to decreased latency and memory usage while maintaining performance benchmarks.'}, 'zh': {'title': '视觉特征修剪，提升推理效率', 'desc': '本论文提出了一种视觉标记减少的方法，以降低大型视觉语言模型（LVLMs）在推理时的计算成本。与以往只针对自注意力模型的研究不同，我们的工作专注于基于交叉注意力的模型，这些模型在性能上更为优越。我们发现交叉注意力层中图像标记的键值（KV）缓存大小远大于自注意力层中的文本标记，成为计算瓶颈。通过利用交叉注意力图的稀疏特性，我们选择性地修剪冗余的视觉特征，从而有效减少KV缓存需求，降低推理延迟和内存使用，同时保持基准性能。'}}}, {'id': 'https://huggingface.co/papers/2503.22165', 'title': 'Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.22165', 'abstract': 'Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts.', 'score': 7, 'issue_id': 3034, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '76c9bb027c844f9c', 'authors': ['Zhanke Zhou', 'Zhaocheng Zhu', 'Xuan Li', 'Mikhail Galkin', 'Xiao Feng', 'Sanmi Koyejo', 'Jian Tang', 'Bo Han'], 'affiliations': ['HEC Montreal', 'Intel AI Lab', 'Mila - Quebec AI Institute', 'Stanford University', 'TMLR Group, Hong Kong Baptist University', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.22165.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#reasoning', '#multimodal', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Визуализация мыслительного процесса языковых моделей', 'desc': "Статья представляет новый инструмент визуализации под названием 'landscape of thoughts' для анализа процесса рассуждений больших языковых моделей (LLM). Этот инструмент позволяет отображать пути рассуждений в виде двумерных графиков, используя t-SNE для визуализации векторов признаков, представляющих состояния в процессе рассуждения. Анализ с помощью 'landscape of thoughts' эффективно различает сильные и слабые модели, правильные и неправильные ответы, а также различные задачи рассуждения. Инструмент также выявляет нежелательные паттерны рассуждений, такие как низкая согласованность и высокая неопределенность."}, 'en': {'title': 'Visualizing Reasoning Paths in Language Models', 'desc': "This paper introduces a new visualization tool called 'landscape of thoughts' that helps users understand how large language models (LLMs) reason through problems. It represents reasoning paths as feature vectors, which show how close each reasoning step is to possible answers. By using a technique called t-SNE, the tool creates two-dimensional plots that allow for easy comparison of different models and their reasoning effectiveness. The tool also identifies problematic reasoning patterns and can be adapted to evaluate the accuracy of reasoning paths in various tasks."}, 'zh': {'title': '揭示大型语言模型的推理路径', 'desc': '本文介绍了一种名为“思维景观”的可视化工具，用于分析大型语言模型（LLMs）的推理路径。该工具通过将推理路径中的状态表示为特征向量，量化它们与所有答案选项的距离，并使用t-SNE进行二维可视化。通过对思维景观的定性和定量分析，可以有效区分强弱模型、正确与错误答案，以及不同的推理任务。此外，该工具还能够揭示不理想的推理模式，如低一致性和高不确定性。'}}}, {'id': 'https://huggingface.co/papers/2504.00294', 'title': 'Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead', 'url': 'https://huggingface.co/papers/2504.00294', 'abstract': "Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.", 'score': 6, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'b455a4adb4eae588', 'authors': ['Vidhisha Balachandran', 'Jingya Chen', 'Lingjiao Chen', 'Shivam Garg', 'Neel Joshi', 'Yash Lara', 'John Langford', 'Besmira Nushi', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00294.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#math', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Масштабирование LLM: потенциал и ограничения в сложных задачах', 'desc': 'Статья исследует влияние масштабирования во время вывода на способности крупных языковых моделей (LLM) решать сложные задачи. Авторы сравнивают обычные модели и модели, настроенные на масштабирование, на восьми различных типах задач. Результаты показывают, что преимущества масштабирования варьируются в зависимости от задачи и уменьшаются с ростом сложности проблемы. Исследование также выявляет потенциал для улучшения производительности моделей при использовании совершенных верификаторов или сильной обратной связи.'}, 'en': {'title': 'Scaling Inference for Smarter Problem Solving in LLMs', 'desc': 'This paper explores how inference-time scaling can improve the reasoning abilities of large language models (LLMs) when tackling complex problems. It examines the effectiveness of extending generated scratchpads for various tasks, including math reasoning and navigation, across nine advanced models. The study finds that while scaling can enhance performance, its benefits vary by task and may decrease with increased complexity. Additionally, the research indicates that simply increasing the number of tokens does not guarantee better accuracy, but using strong feedback mechanisms can lead to significant performance improvements.'}, 'zh': {'title': '推理时间扩展：提升模型推理能力的关键', 'desc': '本研究探讨了推理时间扩展对大型语言模型（LLMs）在复杂问题上的推理能力的影响。我们分析了九种最先进模型在八个具有挑战性的任务上的表现，包括数学推理和空间推理等。结果表明，推理时间扩展的优势因任务而异，且在问题复杂性增加时会减弱。尽管使用更多的标记并不总能提高准确性，但在有强反馈的情况下，所有模型都显示出显著的性能提升潜力。'}}}, {'id': 'https://huggingface.co/papers/2504.00869', 'title': 'm1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models', 'url': 'https://huggingface.co/papers/2504.00869', 'abstract': "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.", 'score': 5, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'bd9586b08ce02a05', 'authors': ['Xiaoke Huang', 'Juncheng Wu', 'Hui Liu', 'Xianfeng Tang', 'Yuyin Zhou'], 'affiliations': ['Amazon Research', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2504.00869.jpg', 'data': {'categories': ['#reasoning', '#healthcare', '#science', '#training', '#inference'], 'emoji': '🩺', 'ru': {'title': 'Медицинские знания важнее глубины рассуждений', 'desc': 'В статье исследуется применение техники масштабирования во время тестирования для улучшения медицинских рассуждений в больших языковых моделях. Авторы представляют метод m1, который повышает способность модели к медицинским рассуждениям на этапе вывода. Исследование показывает, что масштабирование во время тестирования улучшает результаты на медицинских задачах, но выявляет, что недостаток медицинских знаний ограничивает дальнейший прогресс. Для достижения лучших результатов необходимо увеличивать объем данных, улучшать их качество и расширять возможности модели.'}, 'en': {'title': 'Enhancing Medical Reasoning with Test-Time Scaling', 'desc': "This paper explores the use of test-time scaling to improve the reasoning abilities of large language models specifically in the medical field. The authors introduce a method called m1, which enhances medical reasoning during inference, showing that smaller models can achieve state-of-the-art results. They find that while increasing the reasoning token budget can help, it may also lead to performance degradation if overused. The study emphasizes that having rich medical knowledge is crucial for effective reasoning, rather than just increasing the model's complexity or depth of reasoning."}, 'zh': {'title': '医学推理的新突破：测试时缩放的力量', 'desc': '本文探讨了测试时缩放技术在医学推理中的应用，提出了一种名为m1的方法，能够有效提升模型在推理时的医学能力。研究表明，测试时缩放在多种医学任务中均能显著提高推理效果，尤其是对于参数少于10B的轻量级微调模型，能够达到新的最佳性能。我们发现，最佳的推理令牌预算约为4K，超出此范围可能导致性能下降。此外，增加数据规模、提高数据质量和扩展模型容量是提升医学知识基础的关键，尤其是在小模型性能饱和的情况下。'}}}, {'id': 'https://huggingface.co/papers/2503.23361', 'title': 'Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base', 'url': 'https://huggingface.co/papers/2503.23361', 'abstract': "Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.", 'score': 4, 'issue_id': 3017, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '9b957c49c958aea3', 'authors': ['Linxin Song', 'Xuwei Ding', 'Jieyu Zhang', 'Taiwei Shi', 'Ryotaro Shimizu', 'Rahul Gupta', 'Yang Liu', 'Jian Kang', 'Jieyu Zhao'], 'affiliations': ['AGI', 'Amazon', 'University of Rochester', 'University of Southern California', 'University of Washington', 'University of Wisconsin-Madison', 'ZOZO Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23361.jpg', 'data': {'categories': ['#training', '#hallucinations', '#optimization', '#graphs', '#benchmark', '#data'], 'emoji': '🔍', 'ru': {'title': 'SEA: Эффективный поиск пробелов в знаниях языковых моделей', 'desc': 'Эта статья представляет новый метод под названием SEA (стохастический подъем ошибок) для эффективного обнаружения пробелов в знаниях крупных языковых моделей (LLM). SEA использует итеративный процесс оптимизации, чтобы находить новые кандидаты на ошибки, основываясь на семантическом сходстве с ранее обнаруженными ошибками. Метод применяет иерархический поиск и построение графа отношений для выявления систематических ошибок. Эмпирические результаты показывают, что SEA значительно превосходит существующие методы по эффективности обнаружения ошибок в LLM.'}, 'en': {'title': 'Uncovering Knowledge Deficiencies in LLMs Efficiently with SEA', 'desc': 'This paper introduces a new method called Stochastic Error Ascent (SEA) to identify knowledge deficiencies in large language models (LLMs) that often produce unreliable outputs. SEA efficiently discovers errors by using a stochastic optimization approach, focusing on high-error candidates based on their similarity to previously identified failures. The framework enhances its search capabilities through hierarchical retrieval and a directed acyclic graph to track error propagation. The results show that SEA significantly outperforms existing methods in uncovering knowledge errors while drastically reducing the cost of error discovery.'}, 'zh': {'title': '发现LLM知识缺陷的新方法', 'desc': '大型语言模型（LLMs）在语言能力上表现出色，但常常无法准确保留事实知识，导致幻觉和不可靠的输出。我们提出了一种名为随机错误上升（SEA）的框架，用于在严格的查询预算下发现闭合权重LLMs中的知识缺陷。SEA通过利用与先前观察到的失败的语义相似性，迭代检索新的高错误候选项，从而将错误发现过程形式化为随机优化过程。实验证明，SEA发现的知识错误数量显著高于现有方法，同时大幅降低了每个错误的成本。'}}}, {'id': 'https://huggingface.co/papers/2504.00072', 'title': 'Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs', 'url': 'https://huggingface.co/papers/2504.00072', 'abstract': "We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page.", 'score': 3, 'issue_id': 3021, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '48f5266ddbfa7bca', 'authors': ['Lucas Ventura', 'Antoine Yang', 'Cordelia Schmid', 'Gül Varol'], 'affiliations': ['Google DeepMind', 'Inria, Ecole normale superieure, CNRS, PSL Research University', 'LIGM, Ecole des Ponts, IP Paris, Univ Gustave Eiffel, CNRS'], 'pdf_title_img': 'assets/pdf/title_img/2504.00072.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#open_source', '#video', '#multimodal'], 'emoji': '📽️', 'ru': {'title': 'Эффективное разделение видео на главы с помощью ИИ', 'desc': "Статья представляет новый подход к автоматическому разделению видео на главы с использованием большой языковой модели (LLM). Метод 'Chapter-Llama' обрабатывает транскрипты речи и описания кадров, выбранных на основе содержания речи. LLM обучена определять временные метки границ глав и генерировать их названия. Этот подход значительно превосходит существующие методы на бенчмарке VidChapters-7M, показывая F1-score 45.3 против 26.7."}, 'en': {'title': 'Efficient Video Chaptering with Chapter-Llama', 'desc': "This paper presents a novel approach to video chaptering, which involves dividing long videos into meaningful segments and generating titles for these segments. The authors introduce the 'Chapter-Llama' framework that utilizes a pretrained large language model (LLM) to process speech transcripts and video captions efficiently. By implementing a speech-guided frame selection strategy, they enhance the model's performance while reducing the need for extensive captioning. The results show significant improvements in chaptering accuracy, achieving a notable F1 score on the VidChapters-7M benchmark, and the authors provide their code and models for further research."}, 'zh': {'title': '高效视频章节划分的新方法', 'desc': "本文探讨了视频章节划分的任务，即将长视频时间线划分为语义单元并生成相应的章节标题。我们提出了'Chapter-Llama'框架，通过高效处理文本领域的问题，实现了对长达一小时视频的强大章节划分性能。该方法利用了预训练的大型语言模型（LLM），输入包括语音转录文本和描述视频帧的字幕，以及它们各自的时间戳。我们还提出了一种基于语音转录内容的轻量级帧选择策略，显著提高了章节划分的效率和准确性。"}}}, {'id': 'https://huggingface.co/papers/2503.24210', 'title': 'DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.24210', 'abstract': 'Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io', 'score': 2, 'issue_id': 3023, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'df1e0752d5790146', 'authors': ['Seungjun Lee', 'Gim Hee Lee'], 'affiliations': ['Department of Computer Science, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24210.jpg', 'data': {'categories': ['#synthetic', '#3d', '#cv', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Четкое 3D из размытого 2D: DiET-GS раскрывает детали', 'desc': 'Статья представляет DiET-GS - новый метод для реконструкции четких 3D-представлений из размытых многоракурсных изображений. Авторы используют событийные камеры и диффузионные модели для улучшения качества синтеза новых ракурсов. Ключевая идея заключается в ограничении 3DGS с помощью двойного интеграла событий, что позволяет достичь точной цветопередачи и хорошо определенных деталей. Результаты экспериментов показывают значительное превосходство DiET-GS над существующими методами.'}, 'en': {'title': 'Enhancing 3D Image Clarity with DiET-GS', 'desc': 'This paper addresses the challenge of creating clear 3D images from blurry multi-view pictures, a common issue in computer vision. The authors introduce DiET-GS, a new framework that combines event-based camera data with a diffusion prior to improve the quality of 3D image synthesis. By using a two-stage training approach, the framework effectively restores accurate colors and fine details in the images. The results show that DiET-GS outperforms existing methods in generating high-quality novel views from both synthetic and real-world datasets.'}, 'zh': {'title': '清晰三维重建的新方法', 'desc': '本论文提出了一种名为DiET-GS的框架，用于从模糊的多视图图像中重建清晰的三维表示。该方法结合了无模糊事件流和扩散先验，通过两阶段的训练策略来提高图像质量。我们引入了一种新的约束方法，利用事件双重积分来确保颜色准确和细节清晰。此外，我们还提出了一种简单的技术，利用扩散先验进一步增强边缘细节。'}}}, {'id': 'https://huggingface.co/papers/2503.23157', 'title': 'Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL', 'url': 'https://huggingface.co/papers/2503.23157', 'abstract': 'Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.', 'score': 2, 'issue_id': 3029, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '083970087ba6180e', 'authors': ['Mohammadreza Pourreza', 'Shayan Talaei', 'Ruoxi Sun', 'Xingchen Wan', 'Hailong Li', 'Azalia Mirhoseini', 'Amin Saberi', 'Sercan "O. Arik'], 'affiliations': ['Google Cloud', 'Google DeepMind', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23157.jpg', 'data': {'categories': ['#rlhf', '#rl', '#optimization', '#training', '#benchmark', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Улучшение Text-to-SQL с помощью обучения с подкреплением и частичных наград', 'desc': 'Статья представляет новый подход к задаче Text-to-SQL, основанный на обучении с подкреплением (RL). Авторы предлагают набор частичных наград, специально разработанных для решения проблемы разреженности наград в RL. Используя групповую относительную оптимизацию политики (GRPO), метод стимулирует большие языковые модели (LLM) развивать навыки рассуждения для генерации SQL-запросов. Результаты показывают, что RL-обучение с предложенными наградами превосходит supervised fine-tuning и даже более крупные проприетарные модели на бенчмарке BIRD.'}, 'en': {'title': 'Enhancing Text-to-SQL with Tailored Reinforcement Learning Rewards', 'desc': 'This paper addresses the complex task of converting natural language into SQL queries, which requires understanding language, database structures, and formulating precise queries. The authors critique existing methods that use fixed reasoning paths, which can hinder performance, and propose a new approach that utilizes tailored partial rewards to improve reinforcement learning outcomes. By implementing group relative policy optimization (GRPO), their method encourages large language models to enhance their reasoning skills, leading to better SQL generation. The results show that their reinforcement learning approach outperforms traditional supervised fine-tuning, achieving higher accuracy on benchmark tests with a smaller model size.'}, 'zh': {'title': '提升文本到SQL的推理能力与准确性', 'desc': '本文探讨了文本到SQL的任务，这是一项涉及自然语言理解和数据库架构理解的复杂任务。现有的方法往往依赖于手工设计的推理路径，限制了其效果。我们提出了一种新的部分奖励机制，专门针对文本到SQL任务，旨在解决强化学习中的奖励稀疏问题。通过使用群体相对策略优化（GRPO），我们的模型在准确性和推理能力上都表现优异，超越了许多现有的模型。'}}}, {'id': 'https://huggingface.co/papers/2503.21860', 'title': 'ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning', 'url': 'https://huggingface.co/papers/2503.21860', 'abstract': 'Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.', 'score': 2, 'issue_id': 3017, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '0d9ea55946287027', 'authors': ['Kailin Li', 'Puhao Li', 'Tengyu Liu', 'Yuyang Li', 'Siyuan Huang'], 'affiliations': ['Department of Automation, Tsinghua University', 'Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.21860.jpg', 'data': {'categories': ['#training', '#dataset', '#robotics', '#transfer_learning', '#optimization', '#agents'], 'emoji': '🤖', 'ru': {'title': 'ManipTrans: эффективная передача человеческих навыков манипуляции роботам', 'desc': 'ManipTrans - это новый метод передачи навыков бимануальной манипуляции от человека к роботизированным рукам в симуляции. Он включает предварительное обучение имитатора траектории движения рук и дообучение специфического остаточного модуля с учетом ограничений взаимодействия. Метод превосходит существующие подходы по точности и эффективности выполнения сложных задач манипуляции. На его основе создан крупномасштабный датасет DexManipNet с 3.3 тыс. эпизодов роботизированной манипуляции для обучения политик управления ловкими руками.'}, 'en': {'title': 'Efficiently Teaching Robots to Manipulate Like Humans', 'desc': 'This paper presents ManipTrans, a two-stage method designed to transfer human bimanual manipulation skills to robotic hands in a simulated environment. The first stage involves pre-training a trajectory imitator that learns to replicate human hand movements, while the second stage fine-tunes a residual module to enhance performance under specific interaction constraints. This approach allows for efficient learning and execution of complex tasks, outperforming existing methods in terms of success rate and efficiency. Additionally, the authors introduce DexManipNet, a comprehensive dataset that includes diverse manipulation tasks, paving the way for improved policy training and real-world applications of dexterous robotic hands.'}, 'zh': {'title': '高效转移人类双手技能的机器人手', 'desc': '本论文介绍了一种名为ManipTrans的新方法，用于将人类双手的技能高效地转移到灵巧的机器人手上。该方法分为两个阶段：首先训练一个通用的轨迹模仿器来模拟手部动作，然后在交互约束下微调特定的残差模块，从而实现复杂双手任务的高效学习和准确执行。实验结果表明，ManipTrans在成功率、保真度和效率上超越了现有的最先进方法。此外，利用ManipTrans，我们创建了一个名为DexManipNet的大规模数据集，包含了3.3K个机器人操作的实例，支持进一步的策略训练和实际应用。'}}}, {'id': 'https://huggingface.co/papers/2503.24219', 'title': 'MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote\n  Sensing', 'url': 'https://huggingface.co/papers/2503.24219', 'abstract': 'We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially supervised OD task. In the first stage, we construct a graph representation of each image, comprising object queries, class embeddings, and proposal locations. Then, our task-aware architecture processes this graph to perform the VG task. The model consists of: (i) a multi-branch network that integrates spatial, visual, and categorical features to generate task-aware proposals, and (ii) an object reasoning network that assigns probabilities across proposals, followed by a soft selection mechanism for final referring object localization. Our model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods while retaining classical OD capabilities. The code will be available in our repository: https://github.com/rd20karim/MB-ORES.', 'score': 1, 'issue_id': 3027, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'c59d3238abf4164f', 'authors': ['Karim Radouane', 'Hanane Azzag', 'Mustapha lebbah'], 'affiliations': ['University Paris-Saclay - DAVID Lab, UVSQ Versailles, France', 'University Sorbonne Paris Nord - LIPN, Villetaneuse, France'], 'pdf_title_img': 'assets/pdf/title_img/2503.24219.jpg', 'data': {'categories': ['#open_source', '#optimization', '#cv', '#architecture', '#graphs', '#dataset'], 'emoji': '🛰️', 'ru': {'title': 'Унифицированный подход к обнаружению и привязке объектов на спутниковых снимках', 'desc': 'Авторы предлагают унифицированный подход, объединяющий обнаружение объектов и визуальную привязку для дистанционного зондирования. Они дообучают детектор объектов с открытым набором классов, используя данные с языковыми описаниями. Модель строит графовое представление изображения и обрабатывает его с помощью специализированной архитектуры. Эксперименты показывают превосходство предложенного метода над современными аналогами на наборах данных OPT-RSVG и DIOR-RSVG.'}, 'en': {'title': 'Integrating Object Detection and Visual Grounding for Enhanced Remote Sensing Analysis', 'desc': 'This paper presents a new framework that combines object detection (OD) and visual grounding (VG) specifically for remote sensing imagery. The authors enhance a traditional open-set object detector by fine-tuning it with referring expression data, treating VG as a partially supervised OD task. They create a graph representation of images that includes object queries and class embeddings, which is then processed by a multi-branch network to generate proposals. The model outperforms existing methods on benchmark datasets while maintaining the capabilities of classical object detection.'}, 'zh': {'title': '统一框架：目标检测与视觉定位的结合', 'desc': '我们提出了一个统一框架，将目标检测（OD）和视觉定位（VG）集成到遥感图像中。为了支持传统的目标检测并为视觉定位任务建立直观的先验，我们使用参考表达数据微调了一个开放集目标检测器，将其框定为部分监督的目标检测任务。在第一阶段，我们构建了每个图像的图形表示，包括目标查询、类别嵌入和提议位置。然后，我们的任务感知架构处理这个图形以执行视觉定位任务，最终在多个数据集上表现出优越的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.02605', 'title': 'Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving', 'url': 'https://huggingface.co/papers/2504.02605', 'abstract': 'The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.', 'score': 29, 'issue_id': 3095, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'bcf7d7c20685c914', 'authors': ['Daoguang Zan', 'Zhirong Huang', 'Wei Liu', 'Hanwu Chen', 'Linhao Zhang', 'Shulin Xin', 'Lu Chen', 'Qi Liu', 'Xiaojian Zhong', 'Aoyan Li', 'Siyao Liu', 'Yongsheng Xiao', 'Liangqiang Chen', 'Yuyu Zhang', 'Jing Su', 'Tianyu Liu', 'Rui Long', 'Kai Shen', 'Liang Xiang'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2504.02605.jpg', 'data': {'categories': ['#agi', '#benchmark', '#rl', '#open_source', '#multilingual', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'Многоязычный бенчмарк для оценки ИИ в решении программных задач', 'desc': 'Представлен новый многоязычный бенчмарк Multi-SWE-bench для оценки способности языковых моделей решать задачи в различных программных экосистемах. Бенчмарк включает 1632 аннотированных примера на семи языках программирования. Проведена оценка современных моделей с использованием трех методов: Agentless, SWE-agent и OpenHands. Запущено сообщество Multi-SWE-RL для создания наборов данных для обучения с подкреплением в задачах исправления ошибок.'}, 'en': {'title': 'Empowering Issue Resolution with Multi-SWE-bench for Diverse Languages', 'desc': 'This paper introduces Multi-SWE-bench, a multilingual benchmark designed to evaluate Large Language Models (LLMs) in issue resolving across various programming languages, including Java, TypeScript, and C++. The benchmark consists of 1,632 high-quality instances, meticulously annotated by experts to ensure reliability in assessing model performance. The authors also present an analysis of state-of-the-art models using different evaluation methods and launch the Multi-SWE-RL community to foster the development of reinforcement learning datasets for issue-resolving tasks. By open-sourcing their data production pipeline and tutorials, they aim to encourage community contributions and advance research in this area, ultimately pushing towards the goal of Artificial General Intelligence (AGI).'}, 'zh': {'title': '多语言问题解决基准，推动强化学习研究', 'desc': '本论文介绍了一种多语言问题解决基准，称为Multi-SWE-bench，旨在评估大型语言模型在不同软件生态系统中的表现。该基准涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等七种编程语言，共包含1,632个高质量实例，确保评估的准确性和可靠性。我们还推出了Multi-SWE-RL开源社区，旨在为问题解决任务构建大规模的强化学习训练数据集，并发布了4,723个结构良好的实例。通过开放数据生产流程和详细教程，我们希望激励开源社区持续贡献，推动强化学习研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2504.03553', 'title': 'Agentic Knowledgeable Self-awareness', 'url': 'https://huggingface.co/papers/2504.03553', 'abstract': 'Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent\'s self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.', 'score': 19, 'issue_id': 3096, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '4a06cb6959ea30d3', 'authors': ['Shuofei Qiao', 'Zhisong Qiu', 'Baochang Ren', 'Xiaobin Wang', 'Xiangyuan Ru', 'Ningyu Zhang', 'Xiang Chen', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen'], 'affiliations': ['Alibaba Group', 'Nanjing University of Aeronautics and Astronautics', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03553.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#agents', '#agi'], 'emoji': '🧠', 'ru': {'title': 'Самоосознанность в планировании: эффективное использование знаний языковыми моделями', 'desc': 'Статья представляет новый подход к обучению языковых моделей для задач планирования, называемый KnowSelf. Он основан на принципе ситуативной самоосознанности, позволяющем модели динамически оценивать ситуацию и стратегически использовать знания. KnowSelf использует двухэтапный процесс обучения и специальные токены для переключения между различными ситуациями. Эксперименты показывают, что этот метод превосходит базовые подходы на различных задачах, минимально используя внешние знания.'}, 'en': {'title': 'Empowering LLMs with Self-Aware Decision Making', 'desc': "This paper introduces a new approach called agentic knowledgeable self-awareness for Large Language Models (LLMs) in planning tasks. Unlike traditional methods that flood models with external information, this approach emphasizes the importance of situational awareness, allowing agents to assess their environment and use knowledge more effectively. The proposed method, KnowSelf, utilizes a heuristic to identify key moments in an agent's learning process, enabling it to adapt its strategies based on the situation. Experimental results show that KnowSelf significantly improves performance on various tasks while minimizing reliance on external knowledge."}, 'zh': {'title': '自主调节知识使用的智能代理', 'desc': '大型语言模型（LLMs）在多种代理规划任务中表现出色。然而，传统的代理规划方法采用了"洪水灌溉"的方式，随意注入黄金轨迹、外部反馈和领域知识，这种做法忽视了人类在决策过程中动态评估情境需求的能力。我们提出了代理知识自我意识的概念，旨在填补这一空白，使基于LLM的代理能够自主调节知识的使用。具体而言，我们设计了一种启发式情境判断标准，通过标记代理自我探索轨迹上的特殊标记，收集训练数据，从而实现更高效的规划效果。'}}}, {'id': 'https://huggingface.co/papers/2504.02807', 'title': 'MegaMath: Pushing the Limits of Open Math Corpora', 'url': 'https://huggingface.co/papers/2504.02807', 'abstract': 'Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.', 'score': 19, 'issue_id': 3102, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'a6dd17864afc6dca', 'authors': ['Fan Zhou', 'Zengzhi Wang', 'Nikhil Ranjan', 'Zhoujun Cheng', 'Liping Tang', 'Guowei He', 'Zhengzhong Liu', 'Eric P. Xing'], 'affiliations': ['MBZUAI', 'MegaMath'], 'pdf_title_img': 'assets/pdf/title_img/2504.02807.jpg', 'data': {'categories': ['#open_source', '#dataset', '#reasoning', '#synthetic', '#data'], 'emoji': '🧮', 'ru': {'title': 'MegaMath: Большие данные для умных вычислений', 'desc': 'Статья представляет MegaMath - крупномасштабный открытый датасет для предобучения языковых моделей в области математики. Авторы использовали оптимизированные методы извлечения математических документов из веб-данных, а также включили математический код и синтетические данные. Датасет содержит 371 миллиард токенов и отличается высоким качеством и разнообразием. MegaMath призван улучшить математические рассуждения в больших языковых моделях.'}, 'en': {'title': 'MegaMath: Elevating LLMs with a Massive Math Dataset', 'desc': "This paper introduces MegaMath, a comprehensive dataset designed to enhance the mathematical reasoning capabilities of large language models (LLMs). The dataset is created by extracting and optimizing mathematical documents from the web, ensuring high quality through filtering and deduplication. Additionally, it incorporates high-quality math-related code from existing code corpora, further enriching the dataset's diversity. By synthesizing various forms of data, MegaMath provides a substantial resource of 371 billion tokens, making it the largest and highest quality open dataset for math pre-training available."}, 'zh': {'title': 'MegaMath：数学推理的开放数据集', 'desc': '数学推理是人类智能的基石，也是大型语言模型（LLM）高级能力的重要基准。然而，目前研究界缺乏一个开放的大规模高质量数学数据集，以满足数学中心的LLM预训练需求。我们提出了MegaMath，这是一个从多种数学相关来源精心策划的开放数据集，包含3710亿个标记，具有现有开放数学预训练数据集中最大的数量和最佳质量。该数据集通过重新提取网络数据、回收数学相关代码数据和探索合成数据等策略，确保了数据的多样性和高质量。'}}}, {'id': 'https://huggingface.co/papers/2504.03561', 'title': 'SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement', 'url': 'https://huggingface.co/papers/2504.03561', 'abstract': 'In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.', 'score': 14, 'issue_id': 3096, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '469f9f28c32e1a1a', 'authors': ['Runnan Fang', 'Xiaobin Wang', 'Yuan Liang', 'Shuofei Qiao', 'Jialong Wu', 'Zekun Xi', 'Ningyu Zhang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen'], 'affiliations': ['Alibaba Group', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03561.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#agents', '#rl'], 'emoji': '🌐', 'ru': {'title': 'SynWorld: Автономное исследование и обучение агентов в новых средах', 'desc': 'SynWorld - это фреймворк, позволяющий агентам на основе больших языковых моделей (LLM) автономно исследовать новые среды и оптимизировать рабочие процессы. Он использует синтез возможных сценариев с многошаговым вызовом действий и применяет метод Монте-Карло для поиска по дереву (MCTS) для эффективного уточнения знаний о действиях в текущей среде. Эксперименты показывают, что SynWorld является эффективным и универсальным подходом к изучению знаний о действиях в новых средах. Этот метод решает проблему ограниченных возможностей LLM-агентов при работе в незнакомых окружениях или нестандартных пространствах действий.'}, 'en': {'title': 'Empowering Agents to Explore with SynWorld', 'desc': "This paper introduces SynWorld, a framework designed to help agents improve their capabilities in unfamiliar environments. It allows agents to create and evaluate different scenarios by using multi-step actions and Monte Carlo Tree Search (MCTS) for exploration. By synthesizing possible actions, agents can better understand how to navigate and optimize their workflows. The results show that SynWorld effectively enhances agents' action knowledge in new settings, making it a valuable tool for autonomous exploration."}, 'zh': {'title': 'SynWorld：赋能代理探索新环境的框架', 'desc': '在代理与环境的互动中，代理通过规划和执行动作来扩展其能力。然而，基于大型语言模型的代理在新环境中或需要在非常规动作空间中导航时面临重大挑战。为了解决这个问题，我们提出了SynWorld框架，使代理能够合成可能的场景，并在动作空间内进行多步动作调用，同时执行蒙特卡洛树搜索（MCTS）探索，以有效地优化其在当前环境中的动作知识。实验结果表明，SynWorld是学习新环境中动作知识的有效且通用的方法。'}}}, {'id': 'https://huggingface.co/papers/2504.03641', 'title': 'MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models', 'url': 'https://huggingface.co/papers/2504.03641', 'abstract': 'Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies." 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.', 'score': 10, 'issue_id': 3095, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '45da77ffd9c21caf', 'authors': ['Wulin Xie', 'Yi-Fan Zhang', 'Chaoyou Fu', 'Yang Shi', 'Bingyan Nie', 'Hongkai Chen', 'Zhang Zhang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['CASIA', 'M-M-E Project', 'NJU', 'PKU', 'Vivo'], 'pdf_title_img': 'assets/pdf/title_img/2504.03641.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey'], 'emoji': '🧠', 'ru': {'title': 'Новый стандарт оценки мультимодальных языковых моделей', 'desc': 'Эта статья представляет новую систему оценки для унифицированных мультимодальных языковых моделей (U-MLLM). Авторы разработали комплексный фреймворк, включающий стандартизированную оценку традиционных задач и новые задачи для тестирования мультимодальных рассуждений. Было проведено сравнительное тестирование 12 ведущих U-MLLM и специализированных моделей. Результаты выявили существенные пробелы в производительности существующих U-MLLM, подчеркивая необходимость разработки более надежных моделей для эффективного решения задач со смешанной модальностью.'}, 'en': {'title': 'Enhancing Evaluation for Unified Multimodal Language Models', 'desc': 'This paper addresses the challenges in evaluating Unified Multimodal Language Models (U-MLLMs) due to inconsistent benchmarks and the lack of assessments for mixed-modality tasks. It introduces a comprehensive evaluation framework that includes standardized traditional task evaluations across multiple datasets and novel tasks that test multimodal reasoning capabilities. The framework assesses 12 leading U-MLLMs, revealing significant performance gaps and underscoring the necessity for improved models that can effectively manage mixed-modality tasks. The findings aim to enhance the evaluation process and guide future developments in U-MLLMs.'}, 'zh': {'title': '全面评估统一多模态大语言模型的必要性', 'desc': '现有的多模态大语言模型（U-MLLM）基准在评估时面临重大挑战，包括缺乏标准化的传统任务基准和混合模态生成的基准。我们提出了一个全面的评估框架，系统地评估U-MLLM。该基准包括标准化的传统任务评估和五个新颖的多模态推理任务，如图像编辑和常识问答。我们的研究发现现有U-MLLM在性能上存在显著差距，强调了开发更强大模型的必要性，以有效处理混合模态任务。'}}}, {'id': 'https://huggingface.co/papers/2504.03601', 'title': 'APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay', 'url': 'https://huggingface.co/papers/2504.03601', 'abstract': 'Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io', 'score': 9, 'issue_id': 3097, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '05921cbfa42a13b4', 'authors': ['Akshara Prabhakar', 'Zuxin Liu', 'Weiran Yao', 'Jianguo Zhang', 'Ming Zhu', 'Shiyu Wang', 'Zhiwei Liu', 'Tulika Awalgaonkar', 'Haolin Chen', 'Thai Hoang', 'Juan Carlos Niebles', 'Shelby Heinecke', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.03601.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#agents', '#open_source', '#data', '#training'], 'emoji': '🤖', 'ru': {'title': 'Революция в обучении ИИ-агентов: синтетические данные для реалистичного многоходового взаимодействия', 'desc': 'APIGen-MT - это новый фреймворк для генерации качественных данных для обучения ИИ-агентов многоходовому взаимодействию. Он использует двухфазный подход: сначала создаются детальные планы задач с помощью ревьюеров на основе больших языковых моделей, затем эти планы преобразуются в полные траектории взаимодействия. На основе полученных данных обучено семейство моделей xLAM-2-fc-r, превосходящих по ряду показателей такие модели как GPT-4 и Claude 3.5. Исследователи открыли доступ к синтетическим данным и обученным моделям для дальнейшего развития области ИИ-агентов.'}, 'en': {'title': 'Generating High-Quality Data for AI Agents with APIGen-MT', 'desc': 'The paper presents APIGen-MT, a framework designed to generate high-quality multi-turn interaction data for training AI agents. It consists of two phases: first, creating detailed task blueprints with accurate actions using a committee of large language model (LLM) reviewers and feedback loops. In the second phase, these blueprints are turned into full interaction sequences through simulated human-agent interactions. The resulting models, particularly the xLAM-2-fc-r series, show superior performance on benchmark tests, especially in multi-turn scenarios, and the authors provide open access to the generated data and models to support further research.'}, 'zh': {'title': '高效生成多轮交互数据的AI代理训练框架', 'desc': '为了训练有效的AI代理进行多轮交互，我们提出了APIGen-MT框架，该框架能够生成可验证和多样化的多轮代理数据。该框架分为两个阶段，首先通过大型语言模型（LLM）评审委员会和迭代反馈生成详细的任务蓝图，并提供真实的行动。接着，这些蓝图被转化为完整的交互轨迹，通过模拟人机互动实现。我们的xLAM-2-fc-r系列模型在多个基准测试中表现优异，尤其是在多轮设置中，小模型的表现超过了大模型，展示了我们的方法在生成高质量训练数据方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.02949', 'title': 'VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.02949', 'abstract': 'In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.', 'score': 9, 'issue_id': 3095, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '71423989b2bed2d8', 'authors': ['Xianwei Zhuang', 'Yuxin Xie', 'Yufan Deng', 'Dongchao Yang', 'Liming Liang', 'Jinghan Ru', 'Yuguo Yin', 'Yuexian Zou'], 'affiliations': ['School of Electronic and Computer Engineering, Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.02949.jpg', 'data': {'categories': ['#training', '#multimodal', '#rlhf', '#open_source', '#cv', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Единая модель для понимания, генерации и редактирования изображений', 'desc': 'VARGPT-v1.1 - это усовершенствованная унифицированная визуальная авторегрессионная модель, развивающая предыдущую версию VARGPT. Она использует новую стратегию обучения, сочетающую итеративную настройку визуальных инструкций с обучением с подкреплением через Direct Preference Optimization (DPO). Модель обучена на расширенном корпусе из 8,3 млн пар визуально-генеративных инструкций и использует улучшенную языковую модель Qwen2 в качестве основы. VARGPT-v1.1 достигает передовых результатов в задачах мультимодального понимания и генерации изображений по текстовым инструкциям.'}, 'en': {'title': 'Unifying Visual Understanding and Generation with VARGPT-v1.1', 'desc': 'VARGPT-v1.1 is a cutting-edge visual autoregressive model that enhances its predecessor by integrating advanced training techniques and a larger dataset. It employs a unique combination of visual instruction tuning and reinforcement learning to improve its performance in understanding and generating images. The model also features an upgraded backbone and higher image resolution, allowing for better quality outputs and new image editing capabilities. Overall, VARGPT-v1.1 demonstrates the effectiveness of unified models in handling multimodal tasks, showcasing significant advancements in both comprehension and generation.'}, 'zh': {'title': '统一视觉自回归模型的突破性进展', 'desc': '本研究介绍了VARGPT-v1.1，这是一个先进的统一视觉自回归模型，基于我们之前的VARGPT框架。该模型结合了视觉理解的下一个标记预测和图像合成的下一个尺度生成的双重范式。VARGPT-v1.1采用了一种新颖的训练策略，结合了迭代视觉指令调优和通过直接偏好优化（DPO）的强化学习。通过这些改进，VARGPT-v1.1在多模态理解和文本到图像指令跟随任务中实现了最先进的性能，展示了在理解和生成指标上的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2503.24067', 'title': 'TransMamba: Flexibly Switching between Transformer and Mamba', 'url': 'https://huggingface.co/papers/2503.24067', 'abstract': 'Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, a novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling.', 'score': 8, 'issue_id': 3100, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'c397bc55eaf9dd26', 'authors': ['Yixing Li', 'Ruobing Xie', 'Zhen Yang', 'Xingwu Sun', 'Shuaipeng Li', 'Weidong Han', 'Zhanhui Kang', 'Yu Cheng', 'Chengzhong Xu', 'Di Wang', 'Jie Jiang'], 'affiliations': ['Tencent Hunyuan', 'The Chinese University of Hong Kong', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.24067.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#long_context'], 'emoji': '🔀', 'ru': {'title': 'Объединение Transformer и Mamba для эффективной обработки длинных последовательностей', 'desc': 'TransMamba - это новая архитектура, объединяющая Transformer и Mamba через общие матрицы параметров. Она позволяет динамически переключаться между механизмами внимания и моделями пространства состояний (SSM) на разных длинах токенов и слоях. Модель использует конвертер памяти для преобразования выходов внимания в состояния, совместимые с SSM. Эксперименты показали, что TransMamba превосходит базовые модели по эффективности обучения и производительности.'}, 'en': {'title': 'TransMamba: Bridging Transformers and State Space Models for Efficient Sequence Processing', 'desc': 'This paper introduces TransMamba, a new framework that combines the strengths of Transformers and Mamba, a state space model, to improve efficiency in processing long sequences. By using shared parameter matrices, TransMamba can switch between attention mechanisms and state space models based on the length of the input tokens. The Memory converter is designed to ensure smooth transitions between these two methods, allowing for effective information flow. Experimental results show that TransMamba outperforms existing models in both training efficiency and performance, making it a promising solution for future sequence modeling tasks.'}, 'zh': {'title': 'TransMamba：高效的序列建模新方案', 'desc': '本文提出了一种新的框架TransMamba，旨在结合Transformer和Mamba模型，以提高长序列处理的效率。通过共享参数矩阵，TransMamba能够在不同的token长度和层次之间动态切换注意力机制和状态空间模型（SSM）。我们设计了记忆转换器，将注意力输出转换为SSM兼容的状态，确保信息在转换点的无缝流动。此外，本文还深入探讨了TransPoint调度，以进一步提升性能。'}}}, {'id': 'https://huggingface.co/papers/2504.03011', 'title': 'Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization', 'url': 'https://huggingface.co/papers/2504.03011', 'abstract': 'This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods.', 'score': 7, 'issue_id': 3096, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '94d3411c37993837', 'authors': ['Junying Wang', 'Jingyuan Liu', 'Xin Sun', 'Krishna Kumar Singh', 'Zhixin Shu', 'He Zhang', 'Jimei Yang', 'Nanxuan Zhao', 'Tuanfeng Y. Wang', 'Simon S. Chen', 'Ulrich Neumann', 'Jae Shin Yoon'], 'affiliations': ['Adobe Research', 'Runway', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2504.03011.jpg', 'data': {'categories': ['#inference', '#video', '#cv', '#diffusion'], 'emoji': '💡', 'ru': {'title': 'Универсальное управление освещением людей на изображениях и видео', 'desc': 'Статья представляет Comprehensive Relighting - первый универсальный подход к контролю и гармонизации освещения людей на изображениях и видео с произвольными частями тела в любых сценах. Авторы используют предобученную диффузионную модель в качестве общего априорного распределения изображений и совместно моделируют перелозировку человека и гармонизацию фона. Для улучшения временной согласованности освещения вводится модель временного освещения, обучаемая без учителя на реальных видео. Эксперименты показывают, что метод превосходит существующие подходы к перелозировке и гармонизации изображений людей.'}, 'en': {'title': 'Revolutionizing Lighting Control in Images and Videos', 'desc': 'This paper presents Comprehensive Relighting, a novel method that allows for flexible control and harmonization of lighting in images or videos featuring humans. The challenge lies in the limited datasets available, which typically restrict existing models to specific scenarios like faces or static poses. To overcome this, the authors utilize a pre-trained diffusion model to create a unified approach that addresses both human relighting and background harmonization. Additionally, they introduce an unsupervised temporal lighting model that ensures consistent lighting across frames, enhancing the overall quality and realism of the relit images.'}, 'zh': {'title': '全面重光照：人类图像光照的全能解决方案', 'desc': '本文介绍了全面重光照（Comprehensive Relighting），这是首个能够控制和协调来自任意场景中人类图像或视频的光照的全能方法。构建这样一个通用模型非常具有挑战性，因为缺乏数据集，限制了现有基于图像的重光照模型只能应用于特定场景（例如，面部或静态人类）。为了解决这个问题，我们重新利用了一个预训练的扩散模型作为通用图像先验，并在粗到细的框架中联合建模人类重光照和背景协调。实验结果表明，全面重光照在通用性和光照时间一致性方面表现出色，超越了现有的基于图像的人类重光照和协调方法。'}}}, {'id': 'https://huggingface.co/papers/2504.03536', 'title': 'HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration', 'url': 'https://huggingface.co/papers/2504.03536', 'abstract': 'Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce HumanDreamer-X, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, HumanFixer is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.', 'score': 6, 'issue_id': 3097, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '9a6ad8e0086d88eb', 'authors': ['Boyuan Wang', 'Runqi Ouyang', 'Xiaofeng Wang', 'Zheng Zhu', 'Guosheng Zhao', 'Chaojun Ni', 'Guan Huang', 'Lihong Liu', 'Xingang Wang'], 'affiliations': ['GigaAI', 'Institute of Automation, Chinese Academy of Sciences', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03536.jpg', 'data': {'categories': ['#synthetic', '#cv', '#3d'], 'emoji': '🧑\u200d🦰', 'ru': {'title': 'Реалистичные 3D-модели людей из одного фото', 'desc': 'HumanDreamer-X - это новая система для реконструкции 3D-моделей человека по одному изображению. Она объединяет генерацию мультиракурсных изображений и 3D-реконструкцию в единый процесс, что значительно улучшает геометрическую согласованность и визуальное качество моделей. Система использует 3D Gaussian Splatting для начальной геометрии и HumanFixer для улучшения рендеров. Предложенная стратегия модуляции внимания повышает детализацию и согласованность между ракурсами.'}, 'en': {'title': 'Revolutionizing Human Reconstruction with HumanDreamer-X', 'desc': 'This paper presents HumanDreamer-X, a new framework for single-image human reconstruction that combines multi-view generation and 3D reconstruction into one process. The framework addresses common issues like geometric inconsistencies and blurred limbs by using 3D Gaussian Splatting for better initial geometry and appearance. Additionally, it includes a component called HumanFixer, which enhances the photorealism of the 3D models. The authors also introduce an attention modulation strategy to improve detail consistency across different views, resulting in significant improvements in image quality metrics.'}, 'zh': {'title': '统一多视图生成与重建，提升人类模型质量', 'desc': '单图像人类重建对数字人类建模应用至关重要，但仍然是一个极具挑战性的任务。目前的方法依赖生成模型合成多视图图像以进行后续的3D重建和动画。然而，从单个人体图像直接生成多个视图会导致几何不一致，重建模型中出现肢体碎片或模糊的问题。为了解决这些限制，我们提出了HumanDreamer-X，一个将多视图人类生成和重建整合为统一流程的新框架，显著提高了重建3D模型的几何一致性和视觉真实感。'}}}, {'id': 'https://huggingface.co/papers/2504.02402', 'title': 'EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling', 'url': 'https://huggingface.co/papers/2504.02402', 'abstract': 'When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.', 'score': 5, 'issue_id': 3099, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'ca80ca19171ef86b', 'authors': ['Hao Yin', 'Shi Guo', 'Xu Jia', 'Xudong XU', 'Lu Zhang', 'Si Liu', 'Dong Wang', 'Huchuan Lu', 'Tianfan Xue'], 'affiliations': ['Beihang University', 'Dalian University of Technology', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.02402.jpg', 'data': {'categories': ['#training', '#dataset', '#data', '#cv'], 'emoji': '🔊', 'ru': {'title': 'Новый подход к бесконтактному восстановлению звука с помощью событийных камер', 'desc': 'Статья представляет новый метод бесконтактного восстановления звука с использованием событийных камер. Авторы разработали конвейер, который полностью использует пространственно-временную информацию из потока событий камеры. Они создали большой набор данных для обучения с помощью нового метода симуляции и спроектировали нейронную сеть, использующую разреженность событий и архитектуру Mamba для моделирования долгосрочной временной информации. Экспериментальные результаты на синтетических и реальных данных демонстрируют эффективность предложенного метода.'}, 'en': {'title': 'Revolutionizing Sound Recovery with Event Cameras', 'desc': 'This paper presents a new method for recovering sound from visual changes caused by sound-induced vibrations. It addresses limitations in previous techniques by utilizing event camera technology, which excels at capturing high-frequency signals. The authors developed a training pipeline to create a large dataset and designed a neural network that effectively captures both spatial and temporal information from the event data. Their approach includes a specialized imaging system to enhance signal detection, leading to improved sound recovery performance in experiments.'}, 'zh': {'title': '利用事件流实现高效声音恢复', 'desc': '本研究提出了一种新颖的非接触声音恢复方法，充分利用事件流中的时空信息。我们首先通过新的仿真管道生成了一个大型训练集，然后设计了一个网络，利用事件的稀疏性捕捉空间信息，并使用Mamba模型来处理长期的时间信息。最后，我们训练了一个空间聚合模块，以聚合来自不同位置的信息，从而进一步提高信号质量。实验结果表明，我们的方法在合成和真实数据上都表现出良好的效果。'}}}, {'id': 'https://huggingface.co/papers/2504.03600', 'title': 'MedSAM2: Segment Anything in 3D Medical Images and Videos', 'url': 'https://huggingface.co/papers/2504.03600', 'abstract': 'Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.', 'score': 3, 'issue_id': 3103, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': 'c1ef5354c6e2cdcb', 'authors': ['Jun Ma', 'Zongxin Yang', 'Sumin Kim', 'Bihui Chen', 'Mohammed Baharoon', 'Adibvafa Fallahpour', 'Reza Asakereh', 'Hongwei Lyu', 'Bo Wang'], 'affiliations': ['AI Collaborative Centre, University Health Network', 'AI Hub, University Health Network', 'Department of Biomedical Informatics, Harvard Medical School, Harvard University, Boston, USA', 'Department of Computer Science, University of Toronto', 'Department of Laboratory Medicine and Pathobiology and Department of Computer Science, University of Toronto', 'Peter Munk Cardiac Centre, University Health Network', 'University of Toronto, Toronto, Canada', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2504.03600.jpg', 'data': {'categories': ['#data', '#healthcare', '#3d', '#dataset', '#training', '#cv'], 'emoji': '\U0001fa7b', 'ru': {'title': 'MedSAM2: Революция в сегментации медицинских 3D-изображений и видео', 'desc': 'MedSAM2 - это модель сегментации медицинских 3D-изображений и видео, основанная на fine-tuning Segment Anything Model 2. Модель обучена на большом наборе данных, включающем более 455 000 пар 3D-изображений и масок, а также 76 000 кадров, и превосходит предыдущие модели по широкому спектру органов, поражений и модальностей визуализации. Исследователи реализовали подход human-in-the-loop для создания масштабных датасетов, что позволило сократить затраты на ручную разметку более чем на 85%. MedSAM2 интегрирована в популярные платформы с удобным интерфейсом для локального и облачного развертывания, что делает ее практичным инструментом для эффективной и качественной сегментации в исследованиях и здравоохранении.'}, 'en': {'title': 'MedSAM2: Revolutionizing 3D Medical Segmentation', 'desc': 'This paper introduces MedSAM2, a new model designed for segmenting 3D medical images and videos, which is essential for precision medicine. It builds on the Segment Anything Model 2 and has been fine-tuned using a vast dataset of over 455,000 3D image-mask pairs and 76,000 video frames. MedSAM2 outperforms existing models in segmenting various organs and lesions, while also significantly reducing manual annotation costs by over 85% through a human-in-the-loop approach. Additionally, it is user-friendly and can be deployed on both local and cloud platforms, making it accessible for research and healthcare applications.'}, 'zh': {'title': 'MedSAM2：高效的3D医学图像分割工具', 'desc': 'MedSAM2是一种用于3D医学图像和视频分割的可提示分割基础模型。该模型通过在一个包含超过455,000个3D图像-掩膜对和76,000帧的大型医学数据集上微调Segment Anything Model 2而开发。MedSAM2在多个器官、病变和成像模式上超越了之前的模型，并通过人机协作的流程创建了大规模数据集，进行了一项广泛的用户研究。该模型能够将人工成本降低超过85%，并且已集成到广泛使用的平台中，便于本地和云端部署。'}}}, {'id': 'https://huggingface.co/papers/2504.03597', 'title': 'Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin\n  for Real-World Robot Policy Evaluation', 'url': 'https://huggingface.co/papers/2504.03597', 'abstract': "Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com.", 'score': 3, 'issue_id': 3106, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': 'd1e68cb65f756f3e', 'authors': ['Jad Abou-Chakra', 'Lingfeng Sun', 'Krishan Rana', 'Brandon May', 'Karl Schmeckpeper', 'Maria Vittoria Minniti', 'Laura Herlant'], 'affiliations': ['Queensland University of Technology', 'Robotics and AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.03597.jpg', 'data': {'categories': ['#agents', '#training', '#optimization', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Цифровой двойник для эффективного обучения роботов', 'desc': 'Эта статья представляет новый подход к обучению роботов сложным манипуляционным задачам, называемый real-is-sim. Метод использует динамический цифровой двойник на основе Embodied Gaussians для сбора данных, обучения и развертывания политик. Real-is-sim позволяет оценивать политики в симуляторе, что значительно ускоряет процесс разработки. Авторы демонстрируют сильную корреляцию между успешностью в симуляторе и реальном мире на задаче PushT.'}, 'en': {'title': 'Bridging the Gap: Real-World Success through Simulated Training', 'desc': 'This paper introduces real-is-sim, a new behavior cloning framework designed to improve the training and evaluation of robots performing manipulation tasks. It utilizes a dynamic digital twin that aligns a simulated environment with the real world, allowing for better data collection and training processes. By enabling offline evaluation of policies in a simulator, it addresses the challenges of overfitting and underfitting while reducing reliance on costly real-world testing. The framework shows strong correlation between simulated success rates and actual performance, enhancing the efficiency of robot training.'}, 'zh': {'title': '动态数字双胞胎提升行为克隆性能', 'desc': '最近，行为克隆技术的进步使得机器人能够执行复杂的操作任务。然而，准确评估训练性能仍然具有挑战性，尤其是在实际应用中，因为行为克隆损失与实际任务成功率的相关性较差。因此，研究人员不得不依赖于昂贵且耗时的实际评估来获取成功率指标，这使得识别最佳策略和检测过拟合或欠拟合变得不切实际。为了解决这些问题，我们提出了real-is-sim，一个新颖的行为克隆框架，通过动态数字双胞胎在整个策略开发流程中进行数据收集、训练和部署。'}}}, {'id': 'https://huggingface.co/papers/2504.02534', 'title': 'Delineate Anything: Resolution-Agnostic Field Boundary Delineation on\n  Satellite Imagery', 'url': 'https://huggingface.co/papers/2504.02534', 'abstract': 'The accurate delineation of agricultural field boundaries from satellite imagery is vital for land management and crop monitoring. However, current methods face challenges due to limited dataset sizes, resolution discrepancies, and diverse environmental conditions. We address this by reformulating the task as instance segmentation and introducing the Field Boundary Instance Segmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset comprising 672,909 high-resolution satellite image patches (ranging from 0.25 m to 10 m) and 22,926,427 instance masks of individual fields, significantly narrowing the gap between agricultural datasets and those in other computer vision domains. We further propose Delineate Anything, an instance segmentation model trained on our new FBIS-22M dataset. Our proposed model sets a new state-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and 103% in mAP@0.5:0.95 over existing methods, while also demonstrating significantly faster inference and strong zero-shot generalization across diverse image resolutions and unseen geographic regions. Code, pre-trained models, and the FBIS-22M dataset are available at https://lavreniuk.github.io/Delineate-Anything.', 'score': 3, 'issue_id': 3107, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '18e13d7f115ab27c', 'authors': ['Mykola Lavreniuk', 'Nataliia Kussul', 'Andrii Shelestov', 'Bohdan Yailymov', 'Yevhenii Salii', 'Volodymyr Kuzin', 'Zoltan Szantoi'], 'affiliations': ['European Space Agency', 'National Technical University of Ukraine Igor Sikorsky Kyiv Polytechnic Institute', 'Space Research Institute NASU-SSAU', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.02534.jpg', 'data': {'categories': ['#dataset', '#cv', '#inference', '#open_source', '#benchmark', '#optimization'], 'emoji': '🛰️', 'ru': {'title': 'Революция в картографировании сельхозугодий: от пикселей к точным границам полей', 'desc': 'Статья представляет новый подход к точному определению границ сельскохозяйственных полей на спутниковых снимках, переформулируя задачу как сегментацию экземпляров. Авторы создали крупномасштабный набор данных FBIS-22M, содержащий более 22 миллионов масок отдельных полей на снимках разного разрешения. На основе этого набора данных разработана модель Delineate Anything, которая значительно превосходит существующие методы по точности и скорости. Модель демонстрирует сильную обобщающую способность на различных разрешениях изображений и неизвестных географических регионах.'}, 'en': {'title': 'Revolutionizing Agricultural Field Boundary Detection with FBIS-22M', 'desc': 'This paper focuses on improving the identification of agricultural field boundaries using satellite images, which is crucial for effective land management. The authors introduce a new dataset called Field Boundary Instance Segmentation - 22M (FBIS-22M), which contains a large number of high-resolution satellite image patches and detailed instance masks for individual fields. They reformulate the problem as instance segmentation and develop a model named Delineate Anything, which is trained on this extensive dataset. The model achieves state-of-the-art performance, showing significant improvements in mean Average Precision (mAP) and demonstrating fast inference and strong generalization capabilities across various conditions.'}, 'zh': {'title': '精准识别农业边界，助力土地管理', 'desc': '本文提出了一种新的农业领域边界识别方法，利用卫星图像进行土地管理和作物监测。我们通过实例分割的方式重新定义了这一任务，并引入了FBIS-22M数据集，该数据集包含672,909个高分辨率卫星图像片段和22,926,427个实例掩膜。我们的模型"Delineate Anything"在FBIS-22M数据集上训练，达到了新的最先进水平，在mAP@0.5上提高了88.5%，并在不同图像分辨率和未见地理区域上表现出强大的零样本泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.24310', 'title': 'BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models', 'url': 'https://huggingface.co/papers/2503.24310', 'abstract': 'In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.', 'score': 3, 'issue_id': 3097, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '890bba46601fef07', 'authors': ['Alok Abhishek', 'Lisa Erickson', 'Tushar Bandopadhyay'], 'affiliations': ['Boston, USA', 'San Francisco, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.24310.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#ethics'], 'emoji': '⚖️', 'ru': {'title': 'BEATS: комплексная оценка этичности языковых моделей', 'desc': 'Исследователи представили BEATS - новую систему оценки предвзятости, этики, справедливости и фактической точности в больших языковых моделях (LLM). На основе BEATS разработан эталонный тест, измеряющий производительность LLM по 29 различным метрикам, включая демографические, когнитивные и социальные предубеждения. Результаты показали, что 37,65% выходных данных ведущих моделей содержали некоторую форму предвзятости. BEATS предлагает масштабируемую методологию для оценки LLM, диагностики факторов, вызывающих предвзятость, и разработки стратегий по ее снижению.'}, 'en': {'title': 'BEATS: A Framework for Fair and Ethical AI Evaluation', 'desc': 'This paper presents BEATS, a new framework designed to evaluate Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). It introduces a comprehensive bias benchmark that assesses LLM performance using 29 different metrics, covering various biases and ethical considerations. The framework aims to quantitatively measure how LLM outputs may reinforce societal prejudices and systemic inequities. The findings reveal that a significant portion of outputs from leading models exhibit bias, underscoring the need for responsible AI practices and the potential for BEATS to guide improvements in AI ethics.'}, 'zh': {'title': 'BEATS框架：推动负责任的人工智能评估', 'desc': '本研究介绍了BEATS框架，用于评估大型语言模型（LLMs）中的偏见、伦理、公平性和事实性。我们建立了一个偏见基准，涵盖29个不同的指标，评估LLMs在多样性、认知和社会偏见等方面的表现。通过这些指标，可以定量评估LLM生成的响应在多大程度上可能延续社会偏见，强化或扩大系统性不平等。我们的目标是通过BEATS框架，促进更具社会责任感和伦理对齐的人工智能模型的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.22738', 'title': 'ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning', 'url': 'https://huggingface.co/papers/2503.22738', 'abstract': 'Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents.', 'score': 2, 'issue_id': 3111, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'b873783891272a3b', 'authors': ['Zhaorun Chen', 'Mintong Kang', 'Bo Li'], 'affiliations': ['University of Chicago, Chicago IL, USA', 'University of Illinois at Urbana-Champaign, Champaign IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.22738.jpg', 'data': {'categories': ['#agents', '#security', '#reasoning', '#inference', '#benchmark', '#dataset'], 'emoji': '🛡️', 'ru': {'title': 'ShieldAgent: Разумная защита для автономных агентов', 'desc': 'ShieldAgent - это первый агент-защитник, разработанный для обеспечения соблюдения явной политики безопасности для траектории действий других защищаемых агентов с помощью логических рассуждений. Он создает модель политики безопасности, извлекая проверяемые правила из документов и структурируя их в набор вероятностных схем правил на основе действий. ShieldAgent использует обширную библиотеку инструментов и исполняемый код для формальной верификации. Эксперименты показывают, что ShieldAgent превосходит предыдущие методы на 11,3% в среднем с высоким показателем полноты 90,1%.'}, 'en': {'title': 'Shielding Agents: Ensuring Safety with ShieldAgent', 'desc': 'This paper introduces ShieldAgent, a novel guardrail agent designed to enhance the safety of autonomous agents powered by foundation models. ShieldAgent addresses vulnerabilities to malicious instructions by enforcing compliance with safety policies through logical reasoning. It constructs a safety policy model from existing documents and generates shielding plans based on action trajectories of protected agents. The paper also presents ShieldAgent-Bench, a benchmark dataset for evaluating safety in agent instructions, demonstrating that ShieldAgent significantly outperforms existing methods in both accuracy and efficiency.'}, 'zh': {'title': 'ShieldAgent：自主代理的安全防护先锋', 'desc': '本论文提出了ShieldAgent，这是第一个专为自主代理设计的安全防护代理，旨在通过逻辑推理确保其他受保护代理的行动轨迹符合明确的安全政策。ShieldAgent通过从政策文件中提取可验证的规则，构建了一个安全政策模型，并将其结构化为基于行动的概率规则电路。该代理能够根据受保护代理的行动轨迹检索相关规则电路，并生成保护计划，从而实现形式验证。实验结果表明，ShieldAgent在安全性和效率方面表现优异，超越了现有方法，展示了其在保护自主代理方面的高精度和高效性。'}}}, {'id': 'https://huggingface.co/papers/2504.01328', 'title': 'Slow-Fast Architecture for Video Multi-Modal Large Language Models', 'url': 'https://huggingface.co/papers/2504.01328', 'abstract': 'Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose a novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim a video before focusing on relevant parts, our slow-fast design employs a dual-token strategy: 1) "fast" visual tokens -- a compact set of compressed video features -- are fed into the LLM alongside text embeddings to provide a quick overview; 2) "slow" visual tokens -- uncompressed video features -- are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just a 3% increase in computation, and achieving a 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is a plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability.', 'score': 1, 'issue_id': 3104, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '5272839c562d78b6', 'authors': ['Min Shi', 'Shihao Wang', 'Chieh-Yun Chen', 'Jitesh Jain', 'Kai Wang', 'Junjun Xiong', 'Guilin Liu', 'Zhiding Yu', 'Humphrey Shi'], 'affiliations': ['HKPU', 'NVIDIA', 'SHI Labs @ Georgia Tech', 'SUNY Buffalo'], 'pdf_title_img': 'assets/pdf/title_img/2504.01328.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#video', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Эффективное понимание видео с помощью slow-fast архитектуры для MLLM', 'desc': "Статья представляет новую архитектуру slow-fast для видео-ориентированных мультимодальных больших языковых моделей (MLLM). Эта архитектура использует двойную стратегию токенов: 'быстрые' визуальные токены для общего обзора и 'медленные' токены для детального анализа. Подход позволяет обрабатывать больше кадров видео, сохраняя пространственные детали, и значительно превосходит базовые модели на основе self-attention. Модель достигает state-of-the-art результатов среди моделей аналогичного размера на пяти бенчмарках по пониманию видео."}, 'en': {'title': 'Enhancing Video Understanding with Efficient Slow-Fast Architecture', 'desc': "This paper addresses the challenge of balancing temporal resolution and spatial detail in video-based multi-modal large language models (MLLMs) while managing computational limits. The authors introduce a slow-fast architecture that allows for the processing of more input frames without losing important spatial information. By using a dual-token strategy, the model incorporates both compressed 'fast' visual tokens for quick overviews and uncompressed 'slow' visual tokens for detailed analysis, enhancing instruction-aware visual extraction. Experimental results demonstrate that this approach significantly improves performance and efficiency, allowing for a greater input capacity with minimal computational cost."}, 'zh': {'title': '慢-快架构：提升视频理解的效率与性能', 'desc': '在视频基础的多模态大语言模型（MLLMs）中，平衡时间分辨率和空间细节是一个关键挑战。现有方法通常在输入到LLM之前使用预定义规则压缩视频表示，导致不可逆的信息损失，并且常常忽视输入指令。为了解决这个问题，我们提出了一种新颖的慢-快架构，能够在保留空间细节的同时使用更多的输入帧。我们的设计采用双令牌策略，通过快速视觉令牌和慢速视觉令牌的结合，实现了指令感知的相关视觉细节提取，显著提高了模型的性能和效率。'}}}, {'id': 'https://huggingface.co/papers/2504.00396', 'title': 'SPF-Portrait: Towards Pure Portrait Customization with Semantic\n  Pollution-Free Fine-tuning', 'url': 'https://huggingface.co/papers/2504.00396', 'abstract': "Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental learning while customizing target attributes. To address this issue, we propose SPF-Portrait, a pioneering work to purely understand customized semantics while eliminating semantic pollution in text-driven portrait customization. In our SPF-Portrait, we propose a dual-path pipeline that introduces the original model as a reference for the conventional fine-tuning path. Through contrastive learning, we ensure adaptation to target attributes and purposefully align other unrelated attributes with the original portrait. We introduce a novel Semantic-Aware Fine Control Map, which represents the precise response regions of the target semantics, to spatially guide the alignment process between the contrastive paths. This alignment process not only effectively preserves the performance of the original model but also avoids over-alignment. Furthermore, we propose a novel response enhancement mechanism to reinforce the performance of target attributes, while mitigating representation discrepancy inherent in direct cross-modal supervision. Extensive experiments demonstrate that SPF-Portrait achieves state-of-the-art performance. Project webpage: https://spf-portrait.github.io/SPF-Portrait/", 'score': 1, 'issue_id': 3104, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '0afcec0b90ba9cfd', 'authors': ['Xiaole Xian', 'Zhichao Liao', 'Qingyu Li', 'Wenyu Qin', 'Pengfei Wan', 'Weicheng Xie', 'Long Zeng', 'Linlin Shen', 'Pingfa Feng'], 'affiliations': ['Kuaishou Technology', 'Shenzhen University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00396.jpg', 'data': {'categories': ['#cv', '#multimodal', '#training', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'Точная настройка генерации портретов без семантического загрязнения', 'desc': 'Статья представляет новый метод SPF-Portrait для настройки моделей генерации изображений по тексту для создания портретов. Авторы предлагают двухпутевой подход с контрастным обучением, который позволяет адаптировать модель к целевым атрибутам, сохраняя при этом поведение исходной модели. Ключевым элементом является семантически-осведомленная карта точного контроля, которая направляет процесс выравнивания между контрастными путями. Метод также включает механизм усиления отклика для улучшения целевых атрибутов и снижения несоответствия представлений.'}, 'en': {'title': 'Preserving Originality in Portrait Customization with SPF-Portrait', 'desc': "This paper presents SPF-Portrait, a method for fine-tuning a pre-trained Text-to-Image model specifically for customizing portrait attributes without losing the original model's capabilities. The authors address the problem of Semantic Pollution, which occurs during fine-tuning and affects the model's performance on unrelated attributes. SPF-Portrait employs a dual-path pipeline that uses contrastive learning to align target attributes while maintaining the integrity of the original model. Additionally, a Semantic-Aware Fine Control Map is introduced to guide the alignment process, ensuring effective adaptation and enhancing the performance of the desired attributes."}, 'zh': {'title': '消除语义污染，实现肖像定制的创新之路', 'desc': '本文提出了一种名为SPF-Portrait的方法，用于在定制肖像属性时消除语义污染。通过引入原始模型作为参考，采用对比学习确保目标属性的适应性，同时保持原始肖像的其他无关属性。我们还引入了一种新的语义感知细控图，精确指导对比路径之间的对齐过程，从而有效保留原始模型的性能。实验结果表明，SPF-Portrait在肖像定制任务中达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.13837', 'title': 'Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?', 'url': 'https://huggingface.co/papers/2504.13837', 'abstract': "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@k metric with large values of k to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of k (\\eg, k=1), base models can achieve a comparable or even higher pass@k score compared to their RL counterparts at large k values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io", 'score': 53, 'issue_id': 3335, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 апреля', 'en': 'April 18', 'zh': '4月18日'}, 'hash': '2fe56493fe3aec80', 'authors': ['Yang Yue', 'Zhiqi Chen', 'Rui Lu', 'Andrew Zhao', 'Zhaokai Wang', 'Yang Yue', 'Shiji Song', 'Gao Huang'], 'affiliations': ['LeapLab, Tsinghua University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13837.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Обучение с подкреплением не расширяет границы рассуждений ИИ', 'desc': 'Исследование показывает, что обучение с подкреплением с верифицируемыми наградами (RLVR) не приводит к появлению принципиально новых способностей рассуждения у языковых моделей. Хотя модели, обученные с помощью RL, превосходят базовые модели при небольших значениях k в метрике pass@k, базовые модели могут достичь сопоставимых или даже более высоких показателей при больших k. Анализ выявил, что RL-обучение смещает распределение выходных данных модели в сторону путей, которые с большей вероятностью приносят награды, но это также приводит к сужению границ способностей рассуждения. Результаты подчеркивают ограниченность RLVR в улучшении способностей рассуждения языковых моделей.'}, 'en': {'title': 'Rethinking RLVR: Limits of Reinforcement Learning in Reasoning', 'desc': 'This paper critically evaluates the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing the reasoning capabilities of large language models (LLMs). The authors find that while RLVR improves performance at lower complexity tasks, it does not introduce fundamentally new reasoning patterns compared to base models when evaluated at higher complexity levels. Instead, RL-trained models tend to sample reasoning paths that are already present in base models, leading to a narrower range of reasoning capabilities. The study suggests that distillation may be a more effective method for introducing new knowledge into models, highlighting the limitations of RLVR in advancing LLM reasoning.'}, 'zh': {'title': '重新思考强化学习在推理中的作用', 'desc': '强化学习与可验证奖励（RLVR）在提升大型语言模型（LLM）推理能力方面取得了一定成功，尤其是在数学和编程任务中。然而，本研究重新审视了这一假设，发现RLVR并未真正引入新的推理模式。尽管RL训练的模型在小的k值下表现优于基础模型，但在较大的k值下，基础模型的表现可以与RL模型相媲美，甚至更好。这表明，RL训练模型的推理路径实际上已经包含在基础模型的采样分布中，强调了RLVR在提升LLM推理能力方面的局限性。'}}}, {'id': 'https://huggingface.co/papers/2504.13835', 'title': 'MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space', 'url': 'https://huggingface.co/papers/2504.13835', 'abstract': 'Data quality and diversity are key to the construction of effective instruction-tuning datasets. % With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. % Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. % However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. % Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. % To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. % Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to Maximize the Information Gain (MIG) in semantic space. % Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. % Notably, the model fine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73\\% on AlpacaEval and +6.89\\% on Wildbench.', 'score': 29, 'issue_id': 3335, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 апреля', 'en': 'April 18', 'zh': '4月18日'}, 'hash': '12926d762a03519c', 'authors': ['Yicheng Chen', 'Yining Li', 'Kai Hu', 'Zerun Ma', 'Haochen Ye', 'Kai Chen'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2504.13835.jpg', 'data': {'categories': ['#data', '#open_source', '#training', '#dataset', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Максимизация информационного прироста для эффективного обучения языковых моделей', 'desc': 'Статья представляет новый метод для отбора высококачественных и разнообразных подмножеств данных для обучения языковых моделей с инструкциями. Авторы предлагают унифицированный подход к измерению информационного содержания наборов данных, моделируя семантическое пространство с помощью графа меток. На основе этого измерения разработан эффективный метод выборки, максимизирующий прирост информации в семантическом пространстве. Эксперименты показывают, что предложенный метод превосходит современные подходы, позволяя достичь сопоставимой производительности при использовании лишь 5% данных.'}, 'en': {'title': 'Maximizing Information for Better Instruction-Tuning Datasets', 'desc': "This paper addresses the importance of data quality and diversity in creating effective instruction-tuning datasets for machine learning. It critiques existing methods that rely on heuristic rules for maintaining diversity, which often leads to suboptimal dataset selections. The authors propose a new approach that quantifies the information content of datasets by modeling the semantic space with a label graph, allowing for a more comprehensive understanding of data diversity. Their method, called Maximize the Information Gain (MIG), iteratively selects samples that enhance the dataset's information content, showing significant performance improvements in experiments compared to traditional methods."}, 'zh': {'title': '提升数据集质量与多样性的统一方法', 'desc': '数据质量和多样性是构建有效指令调优数据集的关键。随着开源指令调优数据集的增加，自动选择高质量和多样化的子集变得尤为重要。现有方法通常优先考虑实例质量，并使用启发式规则来维持多样性，但缺乏对整个数据集的全面视角，导致结果不理想。我们提出了一种统一的方法，通过构建标签图来量化数据集的信息内容，并基于信息分布来量化多样性，从而引入了一种高效的采样方法，以最大化语义空间中的信息增益。'}}}, {'id': 'https://huggingface.co/papers/2504.11544', 'title': 'NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes', 'url': 'https://huggingface.co/papers/2504.11544', 'abstract': 'Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building a knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, we propose NodeRAG, a graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures a fully cohesive and efficient end-to-end process. Through extensive experiments, we demonstrate that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. Our GitHub repository could be seen at https://github.com/Terry-Xu-666/NodeRAG.', 'score': 21, 'issue_id': 3335, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '86dd4da356ad5ef0', 'authors': ['Tianyang Xu', 'Haojie Zheng', 'Chengze Li', 'Haoxiang Chen', 'Yixin Liu', 'Ruoxi Chen', 'Lichao Sun'], 'affiliations': ['Columbia University', 'Lehigh University', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2504.11544.jpg', 'data': {'categories': ['#games', '#graphs', '#open_source', '#rag', '#multimodal', '#optimization'], 'emoji': '🕸️', 'ru': {'title': 'NodeRAG: Графовый подход к улучшению генерации с дополнением из источников', 'desc': 'NodeRAG - это новый подход к генерации с дополнением из источников (RAG), использующий гетерогенные графовые структуры для улучшения работы больших языковых моделей. Эта система позволяет эффективно интегрировать графовые алгоритмы в процесс RAG, обеспечивая более согласованный и производительный рабочий процесс. Эксперименты показывают, что NodeRAG превосходит предыдущие методы по скорости индексации, времени запросов и эффективности хранения. Кроме того, система демонстрирует улучшенные результаты в задачах ответов на вопросы и открытых сравнениях, используя минимальное количество токенов для извлечения информации.'}, 'en': {'title': 'NodeRAG: Enhancing RAG with Smart Graph Structures', 'desc': 'This paper introduces NodeRAG, a new framework that enhances retrieval-augmented generation (RAG) by using heterogeneous graph structures. By focusing on the design of graph structures, NodeRAG improves the integration of various graph algorithms into the RAG workflow, leading to better performance. The framework aligns with the capabilities of large language models (LLMs), ensuring a smooth and efficient process for generating responses. Experimental results show that NodeRAG outperforms existing methods like GraphRAG and LightRAG in terms of indexing time, query time, storage efficiency, and question-answering accuracy.'}, 'zh': {'title': 'NodeRAG：图结构助力检索增强生成', 'desc': '检索增强生成（RAG）使大型语言模型能够访问外部和私有语料库，从而在特定领域提供事实一致的响应。通过利用语料库的内在结构，基于图的RAG方法通过构建知识图谱索引进一步丰富了这一过程。然而，目前的基于图的RAG方法很少重视图结构的设计。为了解放图在RAG中的潜力，我们提出了NodeRAG，一个以图为中心的框架，引入异构图结构，实现图方法与RAG工作流程的无缝整合。'}}}, {'id': 'https://huggingface.co/papers/2504.11833', 'title': 'Could Thinking Multilingually Empower LLM Reasoning?', 'url': 'https://huggingface.co/papers/2504.11833', 'abstract': 'Previous work indicates that large language models exhibit a significant "English bias", i.e. they often perform better when tasks are presented in English. Interestingly, we have observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. In this paper, we explore the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@k points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, we also find that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs.', 'score': 15, 'issue_id': 3335, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '463f5ddc1d75970e', 'authors': ['Changjiang Gao', 'Xu Huang', 'Wenhao Zhu', 'Shujian Huang', 'Lei Li', 'Fei Yuan'], 'affiliations': ['Carnegie Mellon University', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2504.11833.jpg', 'data': {'categories': ['#reasoning', '#low_resource', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Многоязычное рассуждение - ключ к улучшению больших языковых моделей', 'desc': 'Это исследование показывает, что использование нескольких языков в задачах рассуждения может значительно повысить эффективность больших языковых моделей по сравнению с использованием только английского языка. Авторы обнаружили, что многоязычное рассуждение может улучшить точность на 10 процентных пунктов. Однако существующие методы выбора ответов не могут полностью реализовать этот потенциал из-за своих ограничений и предвзятостей. Эти выводы открывают новые направления для исследований в области многоязычного рассуждения в больших языковых моделях.'}, 'en': {'title': 'Unlocking the Power of Multilingual Reasoning in LLMs', 'desc': 'This paper investigates the performance of large language models (LLMs) in reasoning tasks across multiple languages. It reveals that using certain non-English languages can lead to better outcomes than relying solely on English, highlighting a significant opportunity for multilingual reasoning. The authors suggest that multilingual approaches can achieve higher accuracy and robustness compared to English-only methods, even when translation quality varies. Additionally, they identify limitations in current answer selection methods that prevent reaching the full potential of multilingual reasoning, setting the stage for future research in this area.'}, 'zh': {'title': '多语言推理的潜力超越英语', 'desc': '本论文探讨了大型语言模型在推理任务中的多语言能力，发现某些语言在推理任务中的表现优于英语。研究表明，多语言推理的上限比仅使用英语的推理高出近10个准确率点，并且对翻译质量和语言选择的变化具有更强的容忍度。我们分析了达到这一上限的原因和挑战，并指出常见的答案选择方法由于其局限性和偏见，无法实现这一上限。这些发现为未来研究充分利用大型语言模型的多语言推理潜力铺平了道路。'}}}, {'id': 'https://huggingface.co/papers/2504.10823', 'title': 'CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives', 'url': 'https://huggingface.co/papers/2504.10823', 'abstract': "Navigating high-stakes dilemmas involving conflicting values is challenging even for humans, let alone for AI. Yet prior work in evaluating the reasoning capabilities of large language models (LLMs) in such situations has been limited to everyday scenarios. To close this gap, this work first introduces CLASH (Character perspective-based LLM Assessments in Situations with High-stakes), a meticulously curated dataset consisting of 345 high-impact dilemmas along with 3,795 individual perspectives of diverse values. In particular, we design CLASH in a way to support the study of critical aspects of value-based decision-making processes which are missing from prior work, including understanding decision ambivalence and psychological discomfort as well as capturing the temporal shifts of values in characters' perspectives. By benchmarking 10 open and closed frontier models, we uncover several key findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet, achieve less than 50% accuracy in identifying situations where the decision should be ambivalent, while they perform significantly better in clear-cut scenarios. (2) While LLMs reasonably predict psychological discomfort as marked by human, they inadequately comprehend perspectives involving value shifts, indicating a need for LLMs to reason over complex values. (3) Our experiments also reveal a significant correlation between LLMs' value preferences and their steerability towards a given value. (4) Finally, LLMs exhibit greater steerability when engaged in value reasoning from a third-party perspective, compared to a first-person setup, though certain value pairs benefit uniquely from the first-person framing.", 'score': 10, 'issue_id': 3345, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '9e69138c0a313b09', 'authors': ['Ayoung Lee', 'Ryan Sungmo Kwon', 'Peter Railton', 'Lu Wang'], 'affiliations': ['Department of Computer Science and Engineering, University of Michigan', 'Department of Philosophy, University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2504.10823.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#dataset', '#benchmark'], 'emoji': '🤔', 'ru': {'title': 'Испытание искусственного интеллекта этическими дилеммами', 'desc': 'Статья представляет набор данных CLASH для оценки способности языковых моделей решать сложные этические дилеммы. Исследование выявило, что даже продвинутые модели, такие как GPT-4 и Claude-Sonnet, имеют трудности с определением неоднозначных ситуаций и пониманием изменения ценностей. Эксперименты показали корреляцию между предпочтениями моделей и их управляемостью в отношении определенных ценностей. Также обнаружено, что модели лучше рассуждают о ценностях с точки зрения третьего лица, чем от первого лица.'}, 'en': {'title': 'Navigating Complex Values: Evaluating AI in High-Stakes Dilemmas', 'desc': 'This paper introduces CLASH, a new dataset designed to evaluate large language models (LLMs) on high-stakes dilemmas that involve conflicting values. It contains 345 dilemmas and 3,795 perspectives, focusing on aspects like decision ambivalence and psychological discomfort. The study benchmarks various LLMs, revealing that even advanced models struggle with ambivalent decisions, achieving less than 50% accuracy. Additionally, while LLMs can predict psychological discomfort, they have difficulty understanding shifts in values, highlighting the need for improved reasoning in complex value scenarios.'}, 'zh': {'title': '应对高风险困境中的价值冲突', 'desc': '本研究提出了CLASH数据集，专注于高风险困境中的价值冲突，包含345个高影响力的困境和3795个不同价值观的个体视角。我们发现，即使是最强大的语言模型，如GPT-4o和Claude-Sonnet，在识别决策模糊性方面的准确率也不足50%。此外，虽然这些模型能够合理预测人类的心理不适，但在理解涉及价值转变的视角时表现不佳，显示出它们在复杂价值推理上的不足。最后，模型在第三方视角下进行价值推理时的可操控性更强，而某些价值对在第一人称框架下则表现更好。'}}}, {'id': 'https://huggingface.co/papers/2504.13173', 'title': "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization", 'url': 'https://huggingface.co/papers/2504.13173', 'abstract': 'Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, we observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, we present a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. We then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, we present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. We present three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Our experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models.', 'score': 9, 'issue_id': 3336, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '809d2f1facd3aed9', 'authors': ['Ali Behrouz', 'Meisam Razaviyayn', 'Peilin Zhong', 'Vahab Mirrokni'], 'affiliations': ['Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.13173.jpg', 'data': {'categories': ['#training', '#architecture', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Переосмысление архитектур нейронных сетей через призму селективного внимания', 'desc': 'Статья представляет новый подход к проектированию архитектур нейронных сетей, вдохновленный когнитивным феноменом селективного внимания. Авторы предлагают framework Miras для создания моделей на основе ассоциативной памяти с различными конфигурациями внимания и механизмами забывания. Представлены три новые модели последовательностей - Moneta, Yaad и Memora, которые превосходят существующие линейные RNN. Эксперименты показывают, что некоторые варианты Miras достигают исключительной производительности в задачах языкового моделирования, здравого смысла и интенсивного запоминания.'}, 'en': {'title': 'Revolutionizing Neural Architectures with Attentional Bias', 'desc': 'This paper explores new ways to design neural network architectures, particularly focusing on how they can mimic human attention. It introduces the concept of attentional bias, which helps models prioritize important information, and critiques existing methods that rely on simple similarity measures. The authors propose a framework called Miras, which allows for flexible design choices in memory architecture and training objectives. They also present new models that outperform traditional approaches in specific tasks, demonstrating the effectiveness of their innovative strategies.'}, 'zh': {'title': '基于注意偏向的深度学习架构设计', 'desc': '本文探讨了如何设计高效且有效的基础模型架构，灵感来源于人类的注意偏向现象。我们将神经网络架构重新概念化为关联记忆模块，利用内部目标（注意偏向）来学习键值映射。研究发现，现有序列模型主要依赖点积相似性或L2回归目标，而我们提出了一系列替代的注意偏向配置及其有效近似，以稳定训练过程。基于这些见解，我们提出了Miras框架，设计深度学习架构，并展示了三种新型序列模型，超越了现有线性RNN的能力。'}}}, {'id': 'https://huggingface.co/papers/2504.13157', 'title': 'AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis', 'url': 'https://huggingface.co/papers/2504.13157', 'abstract': 'We explore the task of geometric reconstruction of images captured from a mixture of ground and aerial views. Current state-of-the-art learning-based approaches fail to handle the extreme viewpoint variation between aerial-ground image pairs. Our hypothesis is that the lack of high-quality, co-registered aerial-ground datasets for training is a key reason for this failure. Such data is difficult to assemble precisely because it is difficult to reconstruct in a scalable way. To overcome this challenge, we propose a scalable framework combining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google Earth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The pseudo-synthetic data simulates a wide range of aerial viewpoints, while the real, crowd-sourced images help improve visual fidelity for ground-level images where mesh-based renderings lack sufficient detail, effectively bridging the domain gap between real images and pseudo-synthetic renderings. Using this hybrid dataset, we fine-tune several state-of-the-art algorithms and achieve significant improvements on real-world, zero-shot aerial-ground tasks. For example, we observe that baseline DUSt3R localizes fewer than 5% of aerial-ground pairs within 5 degrees of camera rotation error, while fine-tuning with our data raises accuracy to nearly 56%, addressing a major failure point in handling large viewpoint changes. Beyond camera estimation and scene reconstruction, our dataset also improves performance on downstream tasks like novel-view synthesis in challenging aerial-ground scenarios, demonstrating the practical value of our approach in real-world applications.', 'score': 8, 'issue_id': 3335, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'bbd51434265e3614', 'authors': ['Khiem Vuong', 'Anurag Ghosh', 'Deva Ramanan', 'Srinivasa Narasimhan', 'Shubham Tulsiani'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13157.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#synthetic', '#dataset', '#3d'], 'emoji': '🏙️', 'ru': {'title': 'Преодоление разрыва между землей и небом в компьютерном зрении', 'desc': 'Статья посвящена геометрической реконструкции изображений с наземных и аэросъемок. Авторы предлагают масштабируемый подход, комбинирующий псевдо-синтетические рендеры из 3D-моделей городов с реальными наземными изображениями. Этот гибридный набор данных используется для дообучения современных алгоритмов компьютерного зрения. Результаты показывают значительное улучшение в задачах локализации камеры и реконструкции сцены при экстремальных изменениях ракурса между наземными и аэроснимками.'}, 'en': {'title': 'Bridging the Viewpoint Gap: Enhanced Aerial-Ground Image Reconstruction', 'desc': "This paper addresses the challenge of reconstructing images from both ground and aerial perspectives, which current machine learning methods struggle with due to significant viewpoint differences. The authors suggest that the lack of high-quality datasets that pair aerial and ground images is a major obstacle. To tackle this, they introduce a scalable framework that combines pseudo-synthetic images generated from 3D city models with real ground-level images, effectively bridging the gap between these two domains. By fine-tuning existing algorithms with this hybrid dataset, they achieve substantial improvements in accuracy for aerial-ground tasks, demonstrating the framework's effectiveness in real-world applications."}, 'zh': {'title': '打破视角限制，实现图像几何重建', 'desc': '本文探讨了从地面和空中视角捕获的图像进行几何重建的任务。现有的基于学习的方法在处理空中与地面图像对之间的极端视角变化时表现不佳。我们认为，缺乏高质量的、共同注册的空中-地面数据集是导致这一失败的关键原因。为了解决这个问题，我们提出了一种可扩展的框架，结合了来自3D城市网格的伪合成渲染和真实的地面众包图像，从而有效地缩小了真实图像与伪合成渲染之间的领域差距。'}}}, {'id': 'https://huggingface.co/papers/2504.09621', 'title': 'Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images', 'url': 'https://huggingface.co/papers/2504.09621', 'abstract': 'Global contextual information and local detail features are essential for haze removal tasks. Deep learning models perform well on small, low-resolution images, but they encounter difficulties with large, high-resolution ones due to GPU memory limitations. As a compromise, they often resort to image slicing or downsampling. The former diminishes global information, while the latter discards high-frequency details. To address these challenges, we propose DehazeXL, a haze removal method that effectively balances global context and local feature extraction, enabling end-to-end modeling of large images on mainstream GPU hardware. Additionally, to evaluate the efficiency of global context utilization in haze removal performance, we design a visual attribution method tailored to the characteristics of haze removal tasks. Finally, recognizing the lack of benchmark datasets for haze removal in large images, we have developed an ultra-high-resolution haze removal dataset (8KDehaze) to support model training and testing. It includes 10000 pairs of clear and hazy remote sensing images, each sized at 8192 times 8192 pixels. Extensive experiments demonstrate that DehazeXL can infer images up to 10240 times 10240 pixels with only 21 GB of memory, achieving state-of-the-art results among all evaluated methods. The source code and experimental dataset are available at https://github.com/CastleChen339/DehazeXL.', 'score': 6, 'issue_id': 3340, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': '6c9f2fe055ad92dc', 'authors': ['Jiuchen Chen', 'Xinyu Yan', 'Qizhi Xu', 'Kaiqi Li'], 'affiliations': ['Beijing Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2504.09621.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#optimization', '#data', '#cv'], 'emoji': '🌫️', 'ru': {'title': 'DehazeXL: эффективное удаление дымки на больших изображениях', 'desc': 'DehazeXL - это новый метод удаления дымки, который эффективно балансирует глобальный контекст и извлечение локальных признаков, позволяя обрабатывать большие изображения на обычных GPU. Авторы также разработали метод визуальной атрибуции для оценки эффективности использования глобального контекста. Кроме того, они создали набор данных 8KDehaze с 10000 пар четких и дымчатых изображений размером 8192x8192 пикселей. Эксперименты показали, что DehazeXL может обрабатывать изображения до 10240x10240 пикселей, используя всего 21 ГБ памяти, и достигает лучших результатов среди всех оцененных методов.'}, 'en': {'title': 'DehazeXL: Mastering Haze Removal with Global Context and Local Detail', 'desc': 'The paper introduces DehazeXL, a novel method for haze removal that effectively integrates global contextual information with local detail features. Traditional deep learning models struggle with high-resolution images due to memory constraints, often leading to a loss of important information. DehazeXL overcomes these limitations by allowing end-to-end processing of large images while maintaining high-quality outputs. Additionally, the authors present a new ultra-high-resolution dataset, 8KDehaze, to facilitate training and testing of haze removal models, demonstrating that their approach achieves superior performance on large images.'}, 'zh': {'title': 'DehazeXL：高效去雾的新方法', 'desc': '本文提出了一种新的去雾方法DehazeXL，旨在平衡全局上下文信息和局部细节特征，以提高大图像的去雾效果。传统深度学习模型在处理高分辨率图像时面临GPU内存限制，通常采用图像切片或下采样的方法，但这会导致信息损失。DehazeXL能够在主流GPU硬件上实现端到端的大图像建模，并通过设计视觉归因方法来评估全局上下文在去雾性能中的有效性。此外，我们还开发了一个超高分辨率去雾数据集（8KDehaze），包含10000对8192x8192像素的清晰和模糊遥感图像，以支持模型的训练和测试。'}}}, {'id': 'https://huggingface.co/papers/2504.13072', 'title': 'HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation', 'url': 'https://huggingface.co/papers/2504.13072', 'abstract': 'Scene-level 3D generation represents a critical frontier in multimedia and computer graphics, yet existing approaches either suffer from limited object categories or lack editing flexibility for interactive applications. In this paper, we present HiScene, a novel hierarchical framework that bridges the gap between 2D image generation and 3D object generation and delivers high-fidelity scenes with compositional identities and aesthetic scene content. Our key insight is treating scenes as hierarchical "objects" under isometric views, where a room functions as a complex object that can be further decomposed into manipulatable items. This hierarchical approach enables us to generate 3D content that aligns with 2D representations while maintaining compositional structure. To ensure completeness and spatial alignment of each decomposed instance, we develop a video-diffusion-based amodal completion technique that effectively handles occlusions and shadows between objects, and introduce shape prior injection to ensure spatial coherence within the scene. Experimental results demonstrate that our method produces more natural object arrangements and complete object instances suitable for interactive applications, while maintaining physical plausibility and alignment with user inputs.', 'score': 5, 'issue_id': 3337, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '6eb45708f6cb0c26', 'authors': ['Wenqi Dong', 'Bangbang Yang', 'Zesong Yang', 'Yuan Li', 'Tao Hu', 'Hujun Bao', 'Yuewen Ma', 'Zhaopeng Cui'], 'affiliations': ['ByteDance', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13072.jpg', 'data': {'categories': ['#3d'], 'emoji': '🏠', 'ru': {'title': 'HiScene: Иерархическая 3D-генерация сцен с композиционной структурой', 'desc': "HiScene - это новый иерархический подход к генерации 3D-сцен, который объединяет 2D-генерацию изображений и 3D-генерацию объектов. Метод рассматривает сцены как иерархические 'объекты' в изометрической проекции, где комната функционирует как сложный объект, который можно разложить на отдельные предметы. Используется видео-диффузионная техника для заполнения скрытых частей объектов и инъекция априорной информации о форме для обеспечения пространственной согласованности. Результаты показывают, что HiScene создает более естественные компоновки объектов, подходящие для интерактивных приложений."}, 'en': {'title': 'HiScene: Bridging 2D and 3D for Interactive Scene Generation', 'desc': 'This paper introduces HiScene, a new framework for generating 3D scenes that combines the strengths of 2D image generation with 3D object creation. It treats scenes as hierarchical structures, allowing for detailed manipulation of individual elements within a room. The method employs a video-diffusion-based technique for amodal completion, addressing issues like occlusions and shadows to ensure realistic object interactions. Experimental results show that HiScene produces coherent and aesthetically pleasing 3D scenes that are well-suited for interactive applications.'}, 'zh': {'title': 'HiScene：层次化的3D场景生成新方法', 'desc': '本论文提出了一种名为HiScene的层次框架，用于场景级3D生成，旨在解决现有方法在对象类别和编辑灵活性方面的局限。我们将场景视为在等距视图下的层次“对象”，使得房间可以被进一步分解为可操作的物品。通过这种层次化的方法，我们能够生成与2D表示相一致的3D内容，同时保持组合结构。我们还开发了一种基于视频扩散的模态补全技术，以处理对象之间的遮挡和阴影，确保每个分解实例的完整性和空间对齐。'}}}, {'id': 'https://huggingface.co/papers/2504.13626', 'title': 'Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models', 'url': 'https://huggingface.co/papers/2504.13626', 'abstract': 'Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from "overthinking" problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on fine-tuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization.   Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token (<think> and </think>) can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications.', 'score': 4, 'issue_id': 3340, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 апреля', 'en': 'April 18', 'zh': '4月18日'}, 'hash': 'a0b61093ea88a6d7', 'authors': ['Yule Liu', 'Jingyi Zheng', 'Zhen Sun', 'Zifan Peng', 'Wenhan Dong', 'Zeyang Sha', 'Shiwen Cui', 'Weiqiang Wang', 'Xinlei He'], 'affiliations': ['Ant Group', 'Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2504.13626.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#training', '#optimization', '#reasoning', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация рассуждений больших языковых моделей без переобучения', 'desc': 'Статья представляет метод ThoughtMani для оптимизации работы больших моделей рассуждений (LRM). Авторы обнаружили, что размещение внешних цепочек рассуждений от меньших моделей между специальными токенами позволяет сократить избыточные шаги рассуждений LRM. ThoughtMani снижает количество выходных токенов примерно на 30% без потери производительности и улучшает согласование с требованиями безопасности. Этот подход не требует дополнительного обучения и позволяет эффективно использовать LRM в реальных приложениях.'}, 'en': {'title': 'Streamlining Reasoning: ThoughtMani Reduces Overthinking in LRMs', 'desc': "This paper discusses the challenges faced by large reasoning models (LRMs), particularly the issue of 'overthinking' where models produce excessive reasoning steps with minimal performance improvement. The authors propose a novel approach called ThoughtMani, which strategically places external Chains of Thought (CoTs) generated by smaller models to streamline the reasoning process. This method not only reduces the number of unnecessary intermediate steps but also maintains the model's performance while cutting down computational costs by about 30%. Additionally, ThoughtMani improves safety alignment, making it a practical solution for enhancing the efficiency of LRMs in real-world applications."}, 'zh': {'title': 'ThoughtMani：减少推理步骤，提升效率', 'desc': '最近的大型推理模型（LRMs）在多个任务中展示了通过扩展测试时计算来增强推理能力的有效性。然而，LRMs通常会出现“过度思考”问题，模型生成的推理步骤冗余且性能提升有限。现有的研究依赖于微调来缓解过度思考，但这需要额外的数据和复杂的训练设置。我们提出了一种简单高效的管道，ThoughtMani，通过在思考标记之间放置小模型生成的外部链条（CoTs），有效减少不必要的推理步骤，从而显著降低计算成本。'}}}, {'id': 'https://huggingface.co/papers/2504.13828', 'title': 'Generative AI Act II: Test Time Scaling Drives Cognition Engineering', 'url': 'https://huggingface.co/papers/2504.13828', 'abstract': 'The first generation of Large Language Models - what might be called "Act I" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of "Act II" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI\'s second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: https://github.com/GAIR-NLP/cognition-engineering', 'score': 3, 'issue_id': 3346, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 апреля', 'en': 'April 18', 'zh': '4月18日'}, 'hash': 'f0a8bce0c75eeac0', 'authors': ['Shijie Xia', 'Yiwei Qin', 'Xuefeng Li', 'Yan Ma', 'Run-Ze Fan', 'Steffi Chern', 'Haoyang Zou', 'Fan Zhou', 'Xiangkun Hu', 'Jiahe Jin', 'Yanheng He', 'Yixin Ye', 'Yixiu Liu', 'Pengfei Liu'], 'affiliations': ['Generative AI Research Lab (GAIR)', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13828.jpg', 'data': {'categories': ['#agi', '#multimodal', '#survey', '#optimization', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'От извлечения знаний к конструированию мыслей: новая эра ИИ', 'desc': "Статья описывает переход от первого поколения больших языковых моделей (LLM) к новому этапу развития искусственного интеллекта. Авторы выделяют ограничения ранних моделей и появление новых методов масштабирования во время выполнения (test-time scaling). Рассматривается концепция 'инженерии познания' (cognition engineering) как нового подхода к взаимодействию с ИИ на уровне мышления. Статья предлагает туториалы и оптимизированные реализации для демократизации доступа к этим передовым технологиям."}, 'en': {'title': 'From Knowledge Retrieval to Thought Construction in AI', 'desc': "This paper discusses the evolution of Large Language Models (LLMs) from their initial phase, termed 'Act I', to a new phase called 'Act II'. In 'Act I', LLMs relied heavily on large datasets and parameters but faced issues like slow knowledge updates and limited reasoning abilities. The current phase, 'Act II', focuses on enhancing these models into thought-construction engines that can generate ideas and insights in real-time. The authors aim to make cognition engineering accessible to all practitioners by providing tutorials and resources for implementing these advanced techniques."}, 'zh': {'title': '认知工程的新时代：从知识检索到思维构建', 'desc': '本文讨论了大型语言模型的第一代（2020-2023年）和第二代（2024年至今）的发展。第一代模型通过大规模参数和数据扩展取得了显著成功，但在知识延迟、推理浅显和认知过程受限等方面存在基本局限。第二代模型正在通过测试时扩展技术，从知识检索系统转变为思维构建引擎，建立与AI的语言思维连接。我们系统地分析了这些先进方法，并提供了全面的教程和优化实现，以促进认知工程的发展。'}}}, {'id': 'https://huggingface.co/papers/2504.13816', 'title': "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations", 'url': 'https://huggingface.co/papers/2504.13816', 'abstract': "While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs' perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs' recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.", 'score': 2, 'issue_id': 3347, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 апреля', 'en': 'April 18', 'zh': '4月18日'}, 'hash': '9243c69083d55578', 'authors': ['Chenghao Xiao', 'Hou Pong Chan', 'Hao Zhang', 'Mahani Aljunied', 'Lidong Bing', 'Noura Al Moubayed', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Department of Computer Science, Durham University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13816.jpg', 'data': {'categories': ['#hallucinations', '#dataset', '#transfer_learning', '#low_resource', '#open_source', '#training', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Преодоление языковых барьеров в понимании пределов знаний ИИ', 'desc': 'Исследование посвящено анализу восприятия границ знаний языковыми моделями (LLM) в разных языках. Авторы обнаружили, что это восприятие кодируется в средних и верхних слоях моделей и имеет линейную структуру между языками. Предложен метод переноса способности распознавания границ знаний между языками без дополнительного обучения. Также создан многоязычный набор данных для оценки границ знаний LLM.'}, 'en': {'title': 'Bridging Language Gaps in LLM Knowledge Boundaries', 'desc': 'This paper investigates how large language models (LLMs) understand their limits of knowledge in various languages, addressing a gap in previous research that mainly focused on English. The authors find that LLMs encode their knowledge boundaries in specific layers of their architecture, and that these perceptions vary in a structured way across languages. They propose a method to align knowledge boundary recognition without additional training, which can help reduce inaccuracies in languages with fewer resources. Additionally, fine-tuning LLMs with bilingual question pairs improves their ability to recognize knowledge boundaries, and the authors provide a new multilingual evaluation suite for further research.'}, 'zh': {'title': '跨语言知识边界的识别与转移', 'desc': '本研究首次分析了大型语言模型（LLMs）在不同语言中识别知识边界的能力。研究发现，LLMs对知识边界的感知主要编码在中间层到中上层。不同语言之间的知识边界感知呈线性结构，这促使我们提出了一种无训练的对齐方法，有效地在语言间转移知识边界感知能力，从而降低低资源语言中的幻觉风险。此外，双语问题对翻译的微调进一步增强了LLMs在跨语言识别知识边界的能力。'}}}, {'id': 'https://huggingface.co/papers/2504.13519', 'title': 'Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for\n  Low-Dose CT with Attention-Guided Bilateral Filtering', 'url': 'https://huggingface.co/papers/2504.13519', 'abstract': 'Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable self-supervised single-image denoising framework -- Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through a lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted post-training for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce a novel downsampling shuffle strategy with a new self-supervised loss function that extends the concept of Noise2Noise to a single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at https://github.com/sypsyp97/Filter2Noise.git .', 'score': 0, 'issue_id': 3348, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 апреля', 'en': 'April 18', 'zh': '4月18日'}, 'hash': '54a6d11b3dfb1ed4', 'authors': ['Yipeng Sun', 'Linda-Sophie Schneider', 'Mingxuan Gu', 'Siyuan Mei', 'Chengze Ye', 'Fabian Wagner', 'Siming Bayer', 'Andreas Maier'], 'affiliations': ['Friedrich-Alexander-University Erlangen-Nuremberg, Erlangen, Germany', 'Siemens Healthineers AG, Forchheim, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.13519.jpg', 'data': {'categories': ['#data', '#training', '#interpretability', '#low_resource', '#healthcare', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Интерпретируемое шумоподавление для КТ: от фильтра к чистоте', 'desc': 'Статья представляет новый метод шумоподавления для низкодозовой компьютерной томографии под названием Filter2Noise (F2N). Авторы предлагают интерпретируемый самоконтролируемый подход к шумоподавлению, основанный на адаптивном билатеральном фильтре с механизмом внимания. F2N использует инновационную стратегию понижения разрешения и новую функцию потерь для обучения на одном изображении. Метод превосходит существующие подходы, обеспечивая лучшее качество шумоподавления, интерпретируемость и контроль пользователя.'}, 'en': {'title': 'Revolutionizing Low-Dose CT Denoising with Filter2Noise', 'desc': 'This paper presents Filter2Noise (F2N), a self-supervised framework for denoising low-dose CT images using a single noisy input. Unlike traditional supervised methods that require paired datasets, F2N employs an Attention-Guided Bilateral Filter that adapts to the noise characteristics of each image, allowing for user-controlled adjustments. The framework introduces a novel downsampling shuffle strategy and a self-supervised loss function that effectively handles spatially correlated noise. Experimental results show that F2N significantly improves denoising performance, achieving higher PSNR compared to existing methods while enhancing interpretability and control for medical applications.'}, 'zh': {'title': '自监督去噪，精准可控！', 'desc': '在低剂量CT中，有效去噪对于增强细微结构和低对比度病变至关重要。传统的监督学习方法在有限的配对数据集上表现不佳，而自监督方法通常需要多张噪声图像，并依赖深度网络如U-Net，缺乏对去噪机制的深入理解。为了解决这些问题，我们提出了一种可解释的自监督单图像去噪框架——Filter2Noise (F2N)。该方法引入了一种基于注意力的双边滤波器，能够根据每个噪声输入自适应调整滤波参数，用户可以在训练后可视化和调整这些参数，以实现特定区域的去噪控制。'}}}, {'id': 'https://huggingface.co/papers/2504.10514', 'title': 'ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness', 'url': 'https://huggingface.co/papers/2504.10514', 'abstract': 'Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.', 'score': 30, 'issue_id': 3281, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'c786e69f24be2f9e', 'authors': ['Yijun Liang', 'Ming Li', 'Chenrui Fan', 'Ziyue Li', 'Dang Nguyen', 'Kwesi Cobbina', 'Shweta Bhardwaj', 'Jiuhai Chen', 'Fuxiao Liu', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2504.10514.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#reasoning', '#benchmark'], 'emoji': '🌈', 'ru': {'title': 'ColorBench: новый рубеж в понимании цвета искусственным интеллектом', 'desc': 'Эта статья представляет ColorBench - новый бенчмарк для оценки способностей мультимодальных моделей воспринимать и понимать цвета. Авторы провели обширное тестирование 32 моделей на различных сценариях, связанных с восприятием цвета, рассуждением и устойчивостью. Результаты показали, что хотя более крупные модели справляются лучше, разрыв в производительности между моделями относительно небольшой, что указывает на недостаточное внимание к пониманию цвета в существующих системах. ColorBench может служить фундаментальным инструментом для развития исследований в области понимания цвета искусственным интеллектом на уровне человека.'}, 'en': {'title': "Enhancing AI's Color Comprehension with ColorBench", 'desc': 'This paper presents ColorBench, a benchmark designed to evaluate how vision-language models (VLMs) understand and utilize color in visual reasoning. It assesses various aspects of color perception, reasoning, and robustness through a series of real-world scenarios. The study finds that while larger models generally perform better, the existing VLMs show limited capabilities in color understanding, indicating a gap in their training. Additionally, the research highlights that while VLMs can use color cues effectively, they can also be misled by them, emphasizing the need for improved color comprehension in AI models.'}, 'zh': {'title': '提升视觉语言模型的颜色理解能力', 'desc': '本文介绍了ColorBench，这是一个专门评估视觉语言模型（VLMs）在颜色理解方面能力的基准。研究表明，尽管更大的模型在ColorBench上表现更好，但语言模型的作用比视觉编码器更为重要。现有的VLMs在颜色理解方面的表现差距较小，表明这一领域尚未得到充分重视。此外，尽管VLMs能够利用颜色线索，但在某些任务中也可能会受到误导。'}}}, {'id': 'https://huggingface.co/papers/2504.12285', 'title': 'BitNet b1.58 2B4T Technical Report', 'url': 'https://huggingface.co/papers/2504.12285', 'abstract': 'We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.', 'score': 28, 'issue_id': 3281, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': 'cf67f70d9f122792', 'authors': ['Shuming Ma', 'Hongyu Wang', 'Shaohan Huang', 'Xingxing Zhang', 'Ying Hu', 'Ting Song', 'Yan Xia', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.12285.jpg', 'data': {'categories': ['#open_source', '#dataset', '#benchmark', '#science', '#architecture', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Революция в эффективности: 1-битная LLM на уровне полноточных моделей', 'desc': 'Представлен BitNet b1.58 2B4T - первая открытая 1-битная большая языковая модель (LLM) с 2 миллиардами параметров. Модель обучена на корпусе из 4 триллионов токенов и оценена по различным критериям, включая понимание языка, математические рассуждения и кодирование. BitNet b1.58 2B4T показывает производительность на уровне ведущих полноточных LLM аналогичного размера, но с преимуществами в эффективности вычислений. Модель доступна через Hugging Face вместе с реализациями для GPU и CPU.'}, 'en': {'title': 'Efficient Language Understanding with BitNet: The 1-Bit Revolution', 'desc': "BitNet b1.58 2B4T is a groundbreaking 1-bit Large Language Model (LLM) with 2 billion parameters, making it the first of its kind to be open-source. It has been trained on an extensive dataset of 4 trillion tokens and evaluated on various benchmarks, showcasing its capabilities in language understanding, mathematical reasoning, coding, and conversation. Remarkably, BitNet achieves performance comparable to other leading full-precision LLMs while being more efficient in terms of memory usage, energy consumption, and decoding speed. The model's weights and inference implementations are made available on Hugging Face, promoting further research and practical applications."}, 'zh': {'title': '开源高效的1位大型语言模型', 'desc': '我们介绍了BitNet b1.58 2B4T，这是第一个开源的、原生的1位大型语言模型，参数规模达到20亿。该模型在4万亿个标记的语料库上进行训练，并在语言理解、数学推理、编程能力和对话能力等基准测试中进行了严格评估。我们的结果表明，BitNet b1.58 2B4T在性能上与同类规模的领先开源全精度大型语言模型相当，同时在计算效率上具有显著优势，包括显著减少的内存占用、能耗和解码延迟。为了促进进一步的研究和应用，该模型的权重通过Hugging Face发布，并提供了适用于GPU和CPU架构的开源推理实现。'}}}, {'id': 'https://huggingface.co/papers/2504.11536', 'title': 'ReTool: Reinforcement Learning for Strategic Tool Use in LLMs', 'url': 'https://huggingface.co/papers/2504.11536', 'abstract': "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems.", 'score': 25, 'issue_id': 3288, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '1402eef5411f1416', 'authors': ['Jiazhan Feng', 'Shijue Huang', 'Xingwei Qu', 'Ge Zhang', 'Yujia Qin', 'Baoquan Zhong', 'Chengquan Jiang', 'Jinxin Chi', 'Wanjun Zhong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.11536.jpg', 'data': {'categories': ['#long_context', '#rl', '#math', '#synthetic', '#reasoning', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ReTool: Усиление ИИ-рассуждений с помощью интеграции инструментов и обучения с подкреплением', 'desc': 'В статье представлен метод ReTool, который улучшает способности моделей рассуждения путем интеграции инструментов, таких как интерпретаторы кода. ReTool использует динамическое чередование выполнения кода в реальном времени с процессами рассуждения на естественном языке. Метод включает автоматизированную парадигму обучения с подкреплением, которая обучает модель когда и как использовать инструменты на основе обратной связи. Эксперименты на сложном бенчмарке MATH Olympiad AIME показывают превосходство ReTool над базовыми моделями в эффективности и производительности.'}, 'en': {'title': 'ReTool: Bridging Reasoning and Computation for Enhanced Learning', 'desc': "This paper introduces ReTool, a novel approach that enhances long-form reasoning in machine learning models by integrating real-time code execution into the reasoning process. It addresses the limitations of existing models in structured problem-solving tasks, such as geometric reasoning and complex computations, by employing a reinforcement learning framework that allows models to learn when and how to use computational tools effectively. ReTool's training involves generating synthetic data to fine-tune models and using task outcomes as rewards to improve tool invocation strategies. Experimental results show that ReTool significantly outperforms traditional text-based reinforcement learning models, demonstrating its potential for advancing complex mathematical reasoning and hybrid neuro-symbolic systems."}, 'zh': {'title': 'ReTool：提升复杂推理的工具集成学习', 'desc': '本文提出了一种名为ReTool的模型，旨在提升长文本推理能力，特别是在几何推理和复杂方程求解等结构化问题上。ReTool结合了实时代码执行与自然语言推理的动态交替，利用强化学习（RL）自动化策略来优化工具的使用。通过合成冷启动数据生成代码增强的推理轨迹，ReTool在训练过程中不断调整模型的工具调用策略。实验结果表明，ReTool在MATH奥林匹克基准测试中表现优异，显著提高了模型的准确性和效率。'}}}, {'id': 'https://huggingface.co/papers/2504.12240', 'title': 'Cobra: Efficient Line Art COlorization with BRoAder References', 'url': 'https://huggingface.co/papers/2504.12240', 'abstract': 'The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.', 'score': 17, 'issue_id': 3280, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': 'a237e12792a9a0c8', 'authors': ['Junhao Zhuang', 'Lingen Li', 'Xuan Ju', 'Zhaoyang Zhang', 'Chun Yuan', 'Ying Shan'], 'affiliations': ['Tencent ARC Lab, China', 'The Chinese University of Hong Kong, China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.12240.jpg', 'data': {'categories': ['#long_context', '#cv', '#open_source', '#architecture', '#inference', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Cobra: Быстрая и точная колоризация комиксов с помощью контекстных референсов', 'desc': 'В статье представлен метод Cobra для эффективной колоризации комиксов на основе множества эталонных изображений. Используется архитектура Causal Sparse DiT с позиционным кодированием и разреженным вниманием для обработки длинного контекста. Метод позволяет быстро и точно раскрашивать линейные рисунки с учетом более 200 референсов. Cobra демонстрирует высокую производительность и интерактивность, отвечая требованиям индустрии комиксов.'}, 'en': {'title': 'Cobra: Revolutionizing Line Art Colorization with Contextual Efficiency', 'desc': 'This paper presents Cobra, a novel method for line art colorization in the comic production industry, which requires high accuracy and efficiency. Cobra utilizes a Causal Sparse DiT architecture that incorporates advanced techniques like causal sparse attention and positional encodings to handle over 200 reference images effectively. The method addresses challenges such as slow inference times and the need for flexible control, ensuring color identity consistency across diverse characters and backgrounds. Experimental results show that Cobra significantly improves the quality and speed of line art colorization, making it a valuable tool for artists.'}, 'zh': {'title': 'Cobra：高效灵活的线条艺术上色解决方案', 'desc': '本论文介绍了一种名为Cobra的高效线条艺术上色方法，旨在解决漫画制作行业中对高准确性和灵活控制的需求。Cobra能够处理超过200张参考图像，并保持低延迟，适应复杂的角色和背景。该方法采用了因果稀疏DiT架构，利用特殊设计的位置编码和因果稀疏注意力机制，有效管理长上下文参考。实验结果表明，Cobra在上色质量和推理速度上均有显著提升，满足了工业界的关键需求。'}}}, {'id': 'https://huggingface.co/papers/2504.10326', 'title': 'AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference', 'url': 'https://huggingface.co/papers/2504.10326', 'abstract': 'AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks.', 'score': 17, 'issue_id': 3286, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '030a9ba5449806fb', 'authors': ['Yangshen Deng', 'Zhengxin You', 'Long Xiang', 'Qilong Li', 'Peiqi Yuan', 'Zhaoyang Hong', 'Yitao Zheng', 'Wanting Li', 'Runzhong Li', 'Haotian Liu', 'Kyriakos Mouratidis', 'Man Lung Yiu', 'Huan Li', 'Qiaomu Shen', 'Rui Mao', 'Bo Tang'], 'affiliations': ['AlayaDB AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.10326.jpg', 'data': {'categories': ['#optimization', '#inference', '#long_context', '#benchmark', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'AlayaDB: Революция в эффективности вывода больших языковых моделей', 'desc': 'AlayaDB - это передовая система векторных баз данных, разработанная для эффективного вывода с длинным контекстом для больших языковых моделей (LLM). Она отделяет KV-кэш и вычисление внимания от систем вывода LLM, инкапсулируя их в новую систему векторных баз данных. AlayaDB потребляет меньше аппаратных ресурсов и обеспечивает более высокое качество генерации для различных рабочих нагрузок по сравнению с существующими альтернативными решениями. Ключевым элементом AlayaDB является абстрагирование вычисления внимания и управления кэшем для вывода LLM в процедуру обработки запросов с оптимизацией производительности через нативный оптимизатор запросов.'}, 'en': {'title': 'Revolutionizing LLM Inference with AlayaDB', 'desc': "AlayaDB is an innovative vector database designed to enhance long-context inference for Large Language Models (LLMs). It separates key-value (KV) caching and attention computation from the LLM inference process, streamlining these functions into a dedicated database system. This architecture allows Model as a Service (MaaS) providers to use fewer hardware resources while achieving superior generation quality across various workloads. The system's core feature is its ability to treat attention computation and cache management as a query processing task, which is further optimized by a specialized query optimizer."}, 'zh': {'title': 'AlayaDB：高效的长上下文推理解决方案', 'desc': 'AlayaDB是一种先进的向量数据库系统，专为大型语言模型（LLMs）设计，旨在提高长上下文推理的效率和效果。它将键值缓存和注意力计算从LLM推理系统中解耦，并将其封装到一个新颖的向量数据库系统中。与现有解决方案相比，AlayaDB在硬件资源消耗和生成质量方面表现更佳，适用于不同服务水平目标（SLO）的多种工作负载。该系统通过将注意力计算和缓存管理抽象为查询处理过程，并通过本地查询优化器优化性能，展示了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.09081', 'title': 'SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2504.09081', 'abstract': 'We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs.', 'score': 12, 'issue_id': 3287, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 апреля', 'en': 'April 12', 'zh': '4月12日'}, 'hash': '4c17c8d0a36c365d', 'authors': ['Prabhat Pandey', 'Rupak Vignesh Swaminathan', 'K V Vijay Girish', 'Arunasish Sen', 'Jian Xie', 'Grant P. Strimel', 'Andreas Schwarz'], 'affiliations': ['Amazon AGI', 'Apple Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.09081.jpg', 'data': {'categories': ['#dataset', '#audio', '#transfer_learning', '#multilingual', '#benchmark', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'SIFT: Революция в обучении речевых языковых моделей', 'desc': 'SIFT (Speech Instruction Fine-Tuning) - это набор данных из 50 миллионов примеров для дообучения и предобучения мультимодальных языковых моделей, работающих с речью и текстом. Набор данных создан на основе публично доступных речевых корпусов и охватывает пять языков, включая разнообразные инструкции по пониманию и генерации речи. Модель SIFT-LLM, обученная на этом наборе данных, превосходит существующие речевые ЯМ в задачах следования инструкциям. Авторы также представляют EvalSIFT - набор данных для оценки способности речевых ЯМ следовать инструкциям.'}, 'en': {'title': 'Empowering Speech Models with SIFT: Fine-Tuning for Instruction Mastery', 'desc': 'The paper presents SIFT (Speech Instruction Fine-Tuning), a large dataset containing 50 million examples for enhancing speech-text large language models (LLMs). It is constructed from 14,000 hours of publicly available speech data across five languages, focusing on both speech understanding and generation tasks. The authors train a model called SIFT-LLM, which shows superior performance on instruction-following benchmarks compared to existing models, while also being competitive in foundational speech tasks. Additionally, they introduce EvalSIFT, a specialized benchmark for assessing the instruction-following abilities of speech-text LLMs.'}, 'zh': {'title': 'SIFT：提升语音指令理解的创新数据集', 'desc': '我们介绍了SIFT（语音指令微调），这是一个包含5000万条示例的数据集，旨在用于语音-文本大型语言模型的指令微调和预训练。SIFT-50M基于公开的语音语料库构建，包含14000小时的语音，利用了大型语言模型和现成的专家模型。该数据集涵盖五种语言，包含多样的语音理解和可控的语音生成指令。使用SIFT-50M，我们训练了SIFT-LLM，其在指令跟随基准测试中超越了现有的语音-文本大型语言模型，同时在基础语音任务上也表现出竞争力。'}}}, {'id': 'https://huggingface.co/papers/2504.10483', 'title': 'REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers', 'url': 'https://huggingface.co/papers/2504.10483', 'abstract': 'In this paper we tackle a fundamental question: "Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io.', 'score': 11, 'issue_id': 3285, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'f46935088154b083', 'authors': ['Xingjian Leng', 'Jaskirat Singh', 'Yunzhong Hou', 'Zhenchang Xing', 'Saining Xie', 'Liang Zheng'], 'affiliations': ['Australian National University', 'Data61 CSIRO', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10483.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Революция в обучении диффузионных моделей: совместная оптимизация с VAE', 'desc': 'Статья исследует возможность совместного обучения латентных диффузионных моделей и вариационного автоэнкодера (VAE) в сквозном режиме. Авторы показывают, что стандартная функция потерь диффузии неэффективна для такого обучения, но предлагают новый подход с использованием функции потерь выравнивания представлений (REPA). Этот метод (REPA-E) значительно ускоряет обучение диффузионной модели и улучшает структуру латентного пространства VAE. Подход устанавливает новый state-of-the-art результат по метрике FID на ImageNet 256x256.'}, 'en': {'title': 'Unlocking End-to-End Training with REPA for Latent Diffusion Models', 'desc': 'This paper explores the possibility of training latent diffusion models alongside a variational auto-encoder (VAE) in an end-to-end fashion. It highlights that traditional methods using standard diffusion loss are ineffective and can even harm performance. The authors introduce a new loss function called representation-alignment (REPA) loss, which enables effective joint training of the VAE and diffusion model. Their proposed training method, REPA-E, significantly accelerates training and enhances the quality of generated outputs, achieving state-of-the-art results on ImageNet.'}, 'zh': {'title': '端到端训练的突破：REPA损失的力量', 'desc': '本文探讨了一个基本问题：能否将潜在扩散模型与变分自编码器（VAE）标记器一起进行端到端训练？传统的深度学习理论认为，尽可能进行端到端训练是更优的选择。然而，对于潜在扩散变换器，使用标准扩散损失进行端到端训练会导致效果不佳，甚至降低最终性能。我们提出了通过表示对齐损失（REPA损失）来解锁端到端训练，使得VAE和扩散模型能够在训练过程中共同调整，最终实现了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2504.11468', 'title': 'SFT or RL? An Early Investigation into Training R1-Like Reasoning Large\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2504.11468', 'abstract': "This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals a key finding: SFT can significantly undermine subsequent RL by inducing ``pseudo reasoning paths'' imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning. To systematically study this effect, we introduce VLAA-Thinking, a new multimodal dataset designed to support reasoning in LVLMs. Constructed via a six-step pipeline involving captioning, reasoning distillation, answer rewrite and verification, VLAA-Thinking comprises high-quality, step-by-step visual reasoning traces for SFT, along with a more challenging RL split from the same data source. Using this dataset, we conduct extensive experiments comparing SFT, RL and their combinations. Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with a novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard (https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard) among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope our findings provide valuable insights in developing reasoning-capable LVLMs and can inform future research in this area.", 'score': 8, 'issue_id': 3294, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '550fb98da92065ed', 'authors': ['Hardy Chen', 'Haoqin Tu', 'Fali Wang', 'Hui Liu', 'Xianfeng Tang', 'Xinya Du', 'Yuyin Zhou', 'Cihang Xie'], 'affiliations': ['Amazon Research', 'The Pennsylvania State University', 'University of California, Santa Cruz', 'University of Texas at Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2504.11468.jpg', 'data': {'categories': ['#multimodal', '#training', '#rl', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Переосмысление обучения мультимодальных моделей: от имитации к адаптивному рассуждению', 'desc': "Это исследование показывает, что стандартный подход к обучению мультимодальных языковых моделей, включающий сначала обучение с учителем (SFT), а затем обучение с подкреплением (RL), может быть неэффективным. Авторы обнаружили, что SFT может создавать 'псевдо-пути рассуждений', которые мешают дальнейшему обучению с помощью RL. Для изучения этого эффекта был создан новый датасет VLAA-Thinking, содержащий высококачественные пошаговые рассуждения для визуальных задач. Эксперименты показали, что подход на основе RL с использованием Group Relative Policy Optimization (GRPO) и новым модулем смешанного вознаграждения позволяет моделям развивать более гибкие и адаптивные навыки рассуждения."}, 'en': {'title': 'Unlocking Genuine Reasoning in LVLMs', 'desc': 'This paper examines the training process of Large Vision-Language Models (LVLMs) using supervised fine-tuning (SFT) followed by reinforcement learning (RL). It finds that SFT can create misleading reasoning patterns that hinder the effectiveness of subsequent RL, leading to less informative and incorrect reasoning. To address this, the authors introduce VLAA-Thinking, a new dataset that supports better reasoning in LVLMs through a structured six-step process. Their experiments show that while SFT can help with learning reasoning formats, it can also restrict models to rigid reasoning, whereas their RL approach encourages more flexible and adaptive reasoning capabilities.'}, 'zh': {'title': '打破模仿，促进真实推理的LVLM训练', 'desc': '本文重新审视了大型视觉语言模型（LVLMs）的监督微调（SFT）和强化学习（RL）训练范式，发现SFT可能会通过引入“伪推理路径”来显著削弱后续的RL。这些伪路径虽然看似与RL模型的原生推理路径相似，但往往涉及冗长、犹豫且信息量不足的步骤，甚至错误推理。为系统研究这一现象，我们引入了VLAA-Thinking，一个新型的多模态数据集，旨在支持LVLMs的推理能力。通过六步流程构建的VLAA-Thinking提供了高质量的逐步视觉推理轨迹，并通过实验表明，SFT虽然有助于模型学习推理格式，但会使模型陷入模仿和僵化的推理模式，妨碍进一步学习。'}}}, {'id': 'https://huggingface.co/papers/2504.12264', 'title': 'Towards Learning to Complete Anything in Lidar', 'url': 'https://huggingface.co/papers/2504.12264', 'abstract': 'We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion in-the-wild. This is closely related to Lidar-based semantic/panoptic scene completion. However, contemporary methods can only complete and recognize objects from a closed vocabulary labeled in existing Lidar datasets. Different to that, our zero-shot approach leverages the temporal context from multi-modal sensor sequences to mine object shapes and semantic features of observed objects. These are then distilled into a Lidar-only instance-level completion and recognition model. Although we only mine partial shape completions, we find that our distilled model learns to infer full object shapes from multiple such partial observations across the dataset. We show that our model can be prompted on standard benchmarks for Semantic and Panoptic Scene Completion, localize objects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class vocabularies. Our project page is https://research.nvidia.com/labs/dvl/projects/complete-anything-lidar', 'score': 4, 'issue_id': 3292, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '171045e898ed54a7', 'authors': ['Ayca Takmaz', 'Cristiano Saltori', 'Neehar Peri', 'Tim Meinhardt', 'Riccardo de Lutio', 'Laura Leal-Taixé', 'Aljoša Ošep'], 'affiliations': ['Carnegie Mellon University', 'ETH Zurich', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2504.12264.jpg', 'data': {'categories': ['#3d', '#multimodal', '#benchmark'], 'emoji': '🚗', 'ru': {'title': 'Универсальное завершение форм объектов по данным лидара', 'desc': 'CAL (Complete Anything in Lidar) - это новый подход к завершению форм объектов на основе данных лидара в реальных условиях. В отличие от существующих методов, ограниченных заранее определенным словарем объектов, CAL использует временной контекст из мультимодальных сенсорных последовательностей для извлечения форм и семантических признаков наблюдаемых объектов. Эта информация затем дистиллируется в модель завершения и распознавания на уровне отдельных объектов, работающую только с данными лидара. Несмотря на то, что изначально извлекаются лишь частичные завершения форм, модель учится восстанавливать полные формы объектов на основе множества таких частичных наблюдений в наборе данных.'}, 'en': {'title': 'Complete Any Object with Lidar: Beyond Fixed Classes!', 'desc': 'The paper introduces CAL (Complete Anything in Lidar), a novel approach for completing shapes using Lidar data in real-world scenarios. Unlike traditional methods that rely on a fixed set of labeled objects, CAL employs a zero-shot learning technique that utilizes temporal context from multi-modal sensor sequences to extract object shapes and semantic features. This information is then distilled into a model that can complete and recognize objects at the instance level, even when only partial shapes are available. The results demonstrate that CAL can effectively infer full object shapes and perform well on standard benchmarks for scene completion and object localization, extending recognition beyond predefined class categories.'}, 'zh': {'title': '激光雷达中的形状补全新方法', 'desc': '我们提出了CAL（在激光雷达中完成任何形状），用于在实际环境中进行激光雷达基础的形状补全。这种方法与激光雷达基础的语义/全景场景补全密切相关，但现有方法只能在固定的标签词汇中完成和识别对象。我们的零样本方法利用多模态传感器序列中的时间上下文，挖掘观察到对象的形状和语义特征，并将其提炼为仅基于激光雷达的实例级补全和识别模型。尽管我们只挖掘部分形状补全，但我们的模型能够从多个部分观察中推断出完整的对象形状。'}}}, {'id': 'https://huggingface.co/papers/2504.11952', 'title': 'Robust and Fine-Grained Detection of AI Generated Texts', 'url': 'https://huggingface.co/papers/2504.11952', 'abstract': "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.", 'score': 4, 'issue_id': 3280, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': 'bdea465fe17b9401', 'authors': ['Ram Mohan Rao Kadiyala', 'Siddartha Pullakhandam', 'Kanwal Mehreen', 'Drishti Sharma', 'Siddhant Gupta', 'Jebish Purbey', 'Ashay Srivastava', 'Subhasya TippaReddy', 'Arvind Reddy Bobbili', 'Suraj Telugara Chandrashekhar', 'Modabbir Adeeb', 'Srinadh Vura', 'Hamza Farooq'], 'affiliations': ['Cohere for AI Community', 'IISc Bangalore', 'IIT Roorkee', 'M2ai.in', 'Pulchowk Campus', 'Stanford University', 'Traversaal.ai', 'University of California, Los Angeles', 'University of Houston', 'University of Maryland, College Park', 'University of South Florida', 'Vantager'], 'pdf_title_img': 'assets/pdf/title_img/2504.11952.jpg', 'data': {'categories': ['#low_resource', '#hallucinations', '#dataset', '#benchmark', '#multilingual', '#security', '#data'], 'emoji': '🕵️', 'ru': {'title': 'Универсальный детектор ИИ-текстов: от токенов до языков', 'desc': 'Статья представляет новый подход к обнаружению текстов, созданных искусственным интеллектом. Авторы разработали модели для классификации токенов, обученные на обширном наборе текстов, созданных совместно человеком и ИИ. Модели показали хорошие результаты на текстах из неизвестных областей, от неизвестных генераторов и на текстах с состязательными входными данными. Исследователи также представили новый датасет из 2,4 миллиона текстов на 23 языках, созданных в соавторстве с популярными проприетарными языковыми моделями.'}, 'en': {'title': 'Advancing Detection of Human-LLM Co-Authored Texts', 'desc': 'This paper addresses the challenge of detecting machine-generated content, particularly in cases where texts are co-authored by humans and language models (LLMs). The authors developed a set of token classification models trained on a large dataset of 2.4 million co-authored texts, which allows for better detection across various domains and generators. The models demonstrated strong performance even with adversarial inputs and texts from non-native speakers. Additionally, the paper provides insights into how the models perform based on text length and characteristics compared to purely human-authored content.'}, 'zh': {'title': '构建高效的机器生成内容检测系统', 'desc': '本文提出了一种理想的检测系统，旨在有效识别机器生成的内容，尤其是在短文本中。现有系统在识别AI生成内容时常常面临挑战，因此我们专注于人类与大型语言模型（LLM）共同创作的文本。我们构建了一系列用于标记分类的模型，这些模型在大量人机共创文本上进行训练，并在未见领域和生成器的文本上表现良好。我们还引入了一个包含240万条文本的新数据集，主要由多种流行的专有LLM共同创作，涵盖23种语言。'}}}, {'id': 'https://huggingface.co/papers/2504.11092', 'title': 'Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting', 'url': 'https://huggingface.co/papers/2504.11092', 'abstract': 'Reconstructing 4D dynamic scenes from casually captured monocular videos is valuable but highly challenging, as each timestamp is observed from a single viewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular video synthesis by augmenting observation views - synthesizing multi-view videos from a monocular input. Unlike existing methods that either solely leverage geometric priors for supervision or use generative priors while overlooking geometry, we integrate both. This reformulates view augmentation as a video inpainting task, where observed views are warped into new viewpoints based on monocular depth priors. To achieve this, we train a video inpainting model on unposed web videos with synthetically generated masks that mimic warping occlusions, ensuring spatially and temporally consistent completion of missing regions. To further mitigate inaccuracies in monocular depth priors, we introduce an iterative view augmentation strategy and a robust reconstruction loss. Experiments demonstrate that our method effectively improves monocular 4D scene reconstruction and completion.', 'score': 4, 'issue_id': 3287, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'a6e019e03ced5592', 'authors': ['Jiaxin Huang', 'Sheng Miao', 'BangBnag Yang', 'Yuewen Ma', 'Yiyi Liao'], 'affiliations': ['ByteDance PICO', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11092.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': '🎥', 'ru': {'title': 'Vivid4D: Реконструкция динамических 3D-сцен из обычного видео', 'desc': 'Vivid4D - это новый подход к синтезу 4D-видео из монокулярных записей. Метод использует как геометрические, так и генеративные приоры для augmentации наблюдаемых ракурсов. Задача переформулируется как заполнение пробелов в видео с использованием модели, обученной на непозированных веб-видео. Предложена итеративная стратегия augmentации ракурсов и устойчивая функция потерь для улучшения реконструкции сцен.'}, 'en': {'title': 'Enhancing 4D Scene Reconstruction with Vivid4D', 'desc': 'The paper presents Vivid4D, a new method for reconstructing 4D dynamic scenes from single-view videos. It synthesizes multi-view videos by augmenting the observation views, addressing the challenges of limited viewpoints. The approach combines geometric and generative priors, treating view augmentation as a video inpainting task to fill in missing regions. By training on unposed web videos and using iterative strategies, Vivid4D enhances the accuracy of monocular depth priors and improves scene reconstruction quality.'}, 'zh': {'title': 'Vivid4D：从单目视频重建四维动态场景的新方法', 'desc': '本文介绍了一种名为Vivid4D的新方法，用于从单目视频中重建四维动态场景。该方法通过合成多视角视频来增强单目视频合成，克服了仅从单一视角观察的限制。与现有方法不同，Vivid4D同时利用几何先验和生成先验，将视角增强重新定义为视频修复任务。实验结果表明，该方法有效提高了单目四维场景的重建和补全效果。'}}}, {'id': 'https://huggingface.co/papers/2504.09566', 'title': 'Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution', 'url': 'https://huggingface.co/papers/2504.09566', 'abstract': 'Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of "Module", "Betti numbers","Freeness", "Mapping", "Exactness" and "Minimality", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.', 'score': 2, 'issue_id': 3290, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': '24032586ed61676f', 'authors': ['Chenghao Li', 'Chaoning Zhang', 'Yi Lu', 'Jiaquan Zhang', 'Qigan Sun', 'Xudong Wang', 'Jiwei Wei', 'Guoqing Wang', 'Yang Yang', 'Heng Tao Shen'], 'affiliations': ['Capital Normal University, Beijing, China', 'Kyung Hee University, Yongin-si, Republic of Korea', 'Tongji University, Shanghai, China', 'University of Electronic Science and Technology of China, Chengdu, China', 'University of Liverpool, Liverpool, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2504.09566.jpg', 'data': {'categories': ['#math', '#inference', '#reasoning', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Syzygy of Thoughts: Новый уровень рассуждений для языковых моделей', 'desc': 'Статья представляет новый метод Syzygy of Thoughts (SoT), расширяющий Chain-of-Thought (CoT) промптинг для улучшения рассуждений больших языковых моделей. SoT вдохновлен концепцией минимального свободного разрешения из коммутативной алгебры и вводит вспомогательные взаимосвязанные пути рассуждений. Этот подход позволяет захватывать более глубокие логические зависимости и обеспечивает более структурированное решение сложных задач. Тестирование SoT на различных наборах данных и моделях показало точность вывода, соответствующую или превосходящую стандартные методы CoT.'}, 'en': {'title': 'Enhancing Reasoning with Syzygy of Thoughts', 'desc': 'This paper introduces a new framework called Syzygy of Thoughts (SoT) that builds on Chain-of-Thought (CoT) prompting to improve the reasoning capabilities of large language models (LLMs). SoT addresses the limitations of single reasoning chains by incorporating multiple interrelated reasoning paths, which helps in tackling complex tasks with broad solution spaces. By drawing inspiration from concepts in commutative algebra, such as Minimal Free Resolution, SoT systematically breaks down complex problems into simpler, manageable subproblems while maintaining essential features. The framework has been tested on various datasets and models, demonstrating improved inference accuracy and efficiency compared to traditional CoT methods.'}, 'zh': {'title': '思维的Syzygy：提升推理能力的新框架', 'desc': '本文提出了一种名为思维的Syzygy（SoT）的新框架，旨在增强大型语言模型（LLMs）的推理能力。SoT通过引入辅助的、相互关联的推理路径，扩展了链式思维（CoT），使得模型能够处理更复杂的任务。该方法借鉴了交换代数和代数几何中的最小自由分解（MFR），通过系统地将复杂问题分解为逻辑上完整的最小子问题，从而提高推理的准确性和效率。实验结果表明，SoT在多个数据集和模型上表现出色，推理准确率达到或超过了主流的链式思维标准。'}}}, {'id': 'https://huggingface.co/papers/2504.09346', 'title': '"It\'s not a representation of me": Examining Accent Bias and Digital\n  Exclusion in Synthetic AI Voice Services', 'url': 'https://huggingface.co/papers/2504.09346', 'abstract': "Recent advances in artificial intelligence (AI) speech generation and voice cloning technologies have produced naturalistic speech and accurate voice replication, yet their influence on sociotechnical systems across diverse accents and linguistic traits is not fully understood. This study evaluates two synthetic AI voice services (Speechify and ElevenLabs) through a mixed methods approach using surveys and interviews to assess technical performance and uncover how users' lived experiences influence their perceptions of accent variations in these speech technologies. Our findings reveal technical performance disparities across five regional, English-language accents and demonstrate how current speech generation technologies may inadvertently reinforce linguistic privilege and accent-based discrimination, potentially creating new forms of digital exclusion. Overall, our study highlights the need for inclusive design and regulation by providing actionable insights for developers, policymakers, and organizations to ensure equitable and socially responsible AI speech technologies.", 'score': 1, 'issue_id': 3294, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 апреля', 'en': 'April 12', 'zh': '4月12日'}, 'hash': 'c5df8b667242e0e1', 'authors': ['Shira Michel', 'Sufi Kaur', 'Sarah Elizabeth Gillespie', 'Jeffrey Gleason', 'Christo Wilson', 'Avijit Ghosh'], 'affiliations': ['Hugging Face and University of Connecticut, USA', 'Northeastern University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.09346.jpg', 'data': {'categories': ['#multimodal', '#healthcare', '#ethics', '#synthetic', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Инклюзивность и справедливость в ИИ-технологиях синтеза речи', 'desc': 'Исследование оценивает два сервиса синтетического голоса на основе ИИ (Speechify и ElevenLabs) с использованием опросов и интервью. Анализируется техническая производительность и восприятие пользователями вариаций акцентов в этих речевых технологиях. Выявлены различия в производительности для пяти региональных английских акцентов. Результаты показывают, что современные технологии генерации речи могут непреднамеренно усиливать языковые привилегии и дискриминацию по акценту.'}, 'en': {'title': 'Ensuring Fairness in AI Speech: Addressing Accent Bias and Digital Exclusion', 'desc': 'This paper investigates the impact of AI speech generation and voice cloning technologies on social systems, focusing on how different accents are represented. It evaluates two AI voice services, Speechify and ElevenLabs, using surveys and interviews to understand user experiences and perceptions of accent variations. The study finds that there are significant differences in technical performance across various English accents, which may lead to accent-based discrimination and reinforce existing linguistic privileges. The authors emphasize the importance of inclusive design and regulation to promote fairness and accessibility in AI speech technologies.'}, 'zh': {'title': '确保AI语音技术的公平与包容', 'desc': '本研究探讨了人工智能语音生成和声音克隆技术对社会技术系统的影响，尤其是在不同口音和语言特征方面。我们评估了两种合成AI语音服务（Speechify和ElevenLabs），通过调查和访谈的方法来分析其技术性能，并了解用户的生活经历如何影响他们对口音变化的看法。研究发现，不同地区的英语口音在技术性能上存在差异，这可能无意中加剧了语言特权和基于口音的歧视，导致新的数字排斥现象。总体而言，研究强调了包容性设计和监管的必要性，为开发者、政策制定者和组织提供了可行的见解，以确保AI语音技术的公平性和社会责任。'}}}, {'id': 'https://huggingface.co/papers/2504.09048', 'title': 'BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via\n  Adaptive Block-Based Gaussian Splatting', 'url': 'https://huggingface.co/papers/2504.09048', 'abstract': 'The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, a novel framework incorporating a content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality large-scale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the ground-truth supervision, which enhances the reconstruction quality. Furthermore, we propose a pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on large-scale scenes demonstrate that our approach achieves state-of-the-art performance in both reconstruction efficiency and rendering quality, with a 5x speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on a single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian', 'score': 1, 'issue_id': 3291, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 апреля', 'en': 'April 12', 'zh': '4月12日'}, 'hash': '3f4a6ef28e699ddc', 'authors': ['Yongchang Wu', 'Zipeng Qi', 'Zhenwei Shi', 'Zhengxia Zou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.09048.jpg', 'data': {'categories': ['#3d', '#benchmark'], 'emoji': '🧊', 'ru': {'title': 'Эффективная реконструкция крупномасштабных сцен с помощью блочного гауссова сплаттинга', 'desc': 'Статья представляет BlockGaussian - новый фреймворк для эффективной и качественной реконструкции крупномасштабных сцен с использованием 3D гауссова сплаттинга. Авторы предлагают стратегию разделения сцены с учетом контента и оптимизацию блоков с учетом видимости для улучшения процесса реконструкции. Введение вспомогательных точек и геометрических ограничений псевдо-видов помогает решить проблемы несоответствия при оптимизации и слиянии блоков. Эксперименты показывают 5-кратное ускорение оптимизации и улучшение PSNR на 1.21 дБ по сравнению с современными методами.'}, 'en': {'title': 'Efficient Large-Scale Scene Reconstruction with BlockGaussian', 'desc': 'This paper presents BlockGaussian, a new framework for improving large-scale scene reconstruction using 3D Gaussian Splatting. It introduces a content-aware partitioning strategy that considers the complexity of different scene regions, allowing for better optimization and merging of blocks. The framework also addresses supervision mismatch by using auxiliary points to align with ground-truth data, which enhances the overall reconstruction quality. Experimental results show that BlockGaussian achieves faster optimization and better rendering quality, making it feasible to perform large-scale reconstructions on devices with limited memory.'}, 'zh': {'title': '高效大规模场景重建的新方法', 'desc': '本文介绍了一种名为BlockGaussian的新框架，旨在提高大规模场景重建的效率和质量。该框架采用了内容感知的场景分区策略和可见性感知的块优化方法，以应对场景分区、优化和合并过程中的挑战。通过引入辅助点来解决独立块优化中的监督不匹配问题，进一步提升了重建质量。此外，提出的伪视图几何约束有效减少了块合并过程中因空气浮动造成的渲染降级。'}}}, {'id': 'https://huggingface.co/papers/2503.23461', 'title': 'TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes', 'url': 'https://huggingface.co/papers/2503.23461', 'abstract': 'This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.', 'score': 59, 'issue_id': 2995, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '00cccb2000a01b76', 'authors': ['Nikai Du', 'Zhennan Chen', 'Zhizhou Chen', 'Shan Gao', 'Xi Chen', 'Zhengkai Jiang', 'Jian Yang', 'Ying Tai'], 'affiliations': ['China Mobile', 'Nanjing University', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.23461.jpg', 'data': {'categories': ['#cv', '#dataset', '#benchmark'], 'emoji': '📝', 'ru': {'title': 'TextCrafter: Прорыв в генерации сложного визуального текста', 'desc': 'Статья представляет новый метод TextCrafter для генерации сложного визуального текста в изображениях. Этот метод использует прогрессивную стратегию для декомпозиции сложного визуального текста на отдельные компоненты и обеспечивает надежное выравнивание между текстовым содержанием и его визуальным носителем. TextCrafter также включает механизм усиления фокуса токенов для повышения заметности визуального текста в процессе генерации. Авторы также представляют новый набор данных CVTG-2K для оценки производительности генеративных моделей в задачах CVTG.'}, 'en': {'title': 'TextCrafter: Mastering Complex Visual Text Generation', 'desc': 'This paper introduces Complex Visual Text Generation (CVTG), which involves creating detailed text that appears in various parts of images. The authors identify common issues in existing models, such as producing unclear or incomplete visual text. To improve this, they present TextCrafter, a new method that breaks down complex text into manageable parts and ensures that the text aligns well with the images. Additionally, TextCrafter enhances the visibility of the text during generation and is evaluated using a new dataset called CVTG-2K, showing superior performance compared to current methods.'}, 'zh': {'title': 'TextCrafter：提升复杂视觉文本生成的利器', 'desc': '本文探讨了复杂视觉文本生成（CVTG）任务，主要关注在视觉图像中生成分布在不同区域的复杂文本内容。在CVTG中，图像生成模型常常会渲染出扭曲、模糊的视觉文本或遗漏某些视觉文本。为了解决这些问题，我们提出了TextCrafter，这是一种新颖的多视觉文本渲染方法。TextCrafter通过逐步策略将复杂视觉文本分解为不同组件，同时确保文本内容与其视觉载体之间的强对齐。'}}}, {'id': 'https://huggingface.co/papers/2503.23307', 'title': 'MoCha: Towards Movie-Grade Talking Character Synthesis', 'url': 'https://huggingface.co/papers/2503.23307', 'abstract': 'Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.', 'score': 44, 'issue_id': 2994, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '6ce9b3642bf3ace3', 'authors': ['Cong Wei', 'Bo Sun', 'Haoyu Ma', 'Ji Hou', 'Felix Juefei-Xu', 'Zecheng He', 'Xiaoliang Dai', 'Luxin Zhang', 'Kunpeng Li', 'Tingbo Hou', 'Animesh Sinha', 'Peter Vajda', 'Wenhu Chen'], 'affiliations': ['GenAI, Meta', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23307.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#video', '#benchmark', '#story_generation'], 'emoji': '🎭', 'ru': {'title': 'MoCha: новый уровень ИИ-генерации кинематографических историй', 'desc': 'Представлена система MoCha для генерации анимированных разговаривающих персонажей на основе речи и текста. Использован механизм внимания для синхронизации речи и видео, а также совместное обучение на данных с речевой и текстовой разметкой. Система позволяет генерировать диалоги нескольких персонажей с учетом контекста. Результаты превосходят существующие подходы по реалистичности и выразительности генерируемых анимаций.'}, 'en': {'title': 'Revolutionizing Character Animation with MoCha', 'desc': "This paper presents MoCha, a novel approach for generating talking character animations from speech and text, focusing on full character portrayal rather than just facial movements. It introduces a speech-video window attention mechanism to ensure that the generated video aligns accurately with the spoken words. To tackle the challenge of limited speech-labeled video datasets, the authors propose a joint training strategy that utilizes both speech and text-labeled data, enhancing the model's ability to generalize across various character actions. Additionally, structured prompt templates with character tags allow for multi-character dialogues, enabling AI-generated characters to engage in coherent conversations, thus advancing the field of automated cinematic storytelling."}, 'zh': {'title': '会说话的角色：AI生成电影叙事的新标准', 'desc': '本论文介绍了一种新的视频生成任务，称为“会说话的角色”，旨在从语音和文本直接生成角色动画。与传统的“说话头”不同，这种方法生成的不仅仅是面部表情，而是完整的角色形象。我们提出了MoCha，这是首个能够生成会说话角色的模型，并引入了一种语音-视频窗口注意机制，以确保视频与语音的精确同步。此外，我们还设计了结构化的提示模板，使得多个角色能够进行基于回合的对话，从而实现更具电影感的情境对话。'}}}, {'id': 'https://huggingface.co/papers/2503.24290', 'title': 'Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model', 'url': 'https://huggingface.co/papers/2503.24290', 'abstract': 'We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.', 'score': 35, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'c3cd649c5eb9d423', 'authors': ['Jingcheng Hu', 'Yinmin Zhang', 'Qi Han', 'Daxin Jiang', 'Xiangyu Zhang', 'Heung-Yeung Shum'], 'affiliations': ['StepFun', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24290.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#training', '#rl', '#open_source', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Простота и эффективность в обучении моделей рассуждения', 'desc': 'Open-Reasoner-Zero - это первая открытая реализация обучения с подкреплением для масштабного рассуждения, фокусирующаяся на масштабируемости, простоте и доступности. Используя минималистичный подход с vanilla PPO и GAE, без KL-регуляризации, удалось улучшить длину ответов и производительность на бенчмарках. Модель превзошла DeepSeek-R1-Zero на тестах AIME2024, MATH500 и GPQA Diamond, требуя при этом в 10 раз меньше шагов обучения. Авторы открыто публикуют код, параметры, данные для обучения и веса моделей разных размеров.'}, 'en': {'title': 'Simplifying Large-Scale Reasoning in RL with Open-Reasoner-Zero', 'desc': 'Open-Reasoner-Zero is an innovative open-source framework designed for large-scale reasoning-oriented reinforcement learning (RL) training. It utilizes a simple approach with vanilla Proximal Policy Optimization (PPO) and Generalized Advantage Estimation (GAE), achieving impressive results without complex regularization techniques. The framework demonstrates that minimalistic strategies can effectively enhance response length and benchmark performance, outperforming previous models like DeepSeek-R1-Zero. By releasing all components of the project, including source code and model weights, Open-Reasoner-Zero promotes accessibility and collaboration in the machine learning community.'}, 'zh': {'title': '开源推理强化学习的高效实现', 'desc': '我们介绍了Open-Reasoner-Zero，这是首个开源的大规模推理导向强化学习训练实现，重点关注可扩展性、简洁性和可访问性。通过大量实验，我们证明了使用简单的PPO算法和基于规则的奖励机制，能够有效提升响应长度和基准性能。我们的实现与DeepSeek-R1-Zero使用相同的基础模型，在AIME2024、MATH500和GPQA Diamond基准上表现优异，同时训练效率显著提高，仅需DeepSeek-R1-Zero管道的十分之一训练步骤。为了支持开源精神，我们发布了源代码、参数设置、训练数据和不同规模的模型权重。'}}}, {'id': 'https://huggingface.co/papers/2503.24235', 'title': 'What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.24235', 'abstract': "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.", 'score': 34, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '21674468fcc8c7d5', 'authors': ['Qiyuan Zhang', 'Fuyuan Lyu', 'Zexu Sun', 'Lei Wang', 'Weixu Zhang', 'Zhihan Guo', 'Yufei Wang', 'Irwin King', 'Xue Liu', 'Chen Ma'], 'affiliations': ['Chinese University of Hong Kong', 'City University of Hong Kong', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Macquarie University', 'McGill University & MILA', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24235.jpg', 'data': {'categories': ['#survey', '#math', '#reasoning', '#training'], 'emoji': '🔍', 'ru': {'title': 'Систематизация методов масштабирования языковых моделей во время тестирования', 'desc': 'Эта статья представляет собой обзор методов масштабирования во время тестирования (TTS) для больших языковых моделей (LLM). Авторы предлагают унифицированную структуру для классификации TTS-подходов по четырем измерениям: что масштабировать, как масштабировать, где масштабировать и насколько хорошо масштабировать. В работе анализируются различные методы, сценарии применения и аспекты оценки TTS. Статья также выделяет основные тенденции развития TTS и предлагает практические рекомендации по его применению.'}, 'en': {'title': 'Unlocking Potential: The Power of Test-Time Scaling in LLMs', 'desc': 'This paper discusses the emerging focus on test-time scaling (TTS) in machine learning, particularly for large language models (LLMs). TTS enhances the problem-solving abilities of LLMs in various tasks, including specialized reasoning and open-ended questions. The authors propose a comprehensive framework that categorizes TTS research into four dimensions: what, how, where, and how well to scale. They also review existing methods and applications, highlight challenges, and suggest future research directions to improve TTS effectiveness.'}, 'zh': {'title': '测试时扩展：激发大型语言模型的潜力', 'desc': '随着对预训练时代计算规模（数据和参数）的热情逐渐减退，测试时扩展（TTS）成为一个重要的研究焦点。最近的研究表明，TTS可以进一步激发大型语言模型（LLMs）的问题解决能力，在数学、编程等专业推理任务以及开放式问答等一般任务中取得显著突破。尽管这一领域的研究迅速增加，但仍迫切需要一项全面的调查，以提供系统的理解。为此，我们提出了一个统一的多维框架，涵盖TTS研究的四个核心维度，并对方法、应用场景和评估方面进行了广泛的回顾。'}}}, {'id': 'https://huggingface.co/papers/2503.24388', 'title': 'RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy', 'url': 'https://huggingface.co/papers/2503.24388', 'abstract': 'Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.', 'score': 22, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '98b80967d4757be2', 'authors': ['Zhonghan Zhao', 'Wenwei Zhang', 'Haian Huang', 'Kuikun Liu', 'Jianfei Gao', 'Gaoang Wang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24388.jpg', 'data': {'categories': ['#training', '#reasoning', '#agents', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Синергия рассуждения и воображения для создания универсальных ИИ-агентов', 'desc': 'Данная статья представляет новый подход к обучению агентов искусственного интеллекта, названный RIG (Reasoning and Imagination in Generalist policy). RIG объединяет способности рассуждения и воображения в единой комплексной модели, что позволяет значительно повысить эффективность обучения и способность к обобщению. В процессе вывода RIG сначала рассуждает о следующем действии, затем генерирует потенциальное действие и предсказывает его результаты, что дает агенту возможность пересмотреть и скорректировать свои действия перед их реальным выполнением. Экспериментальные результаты показывают, что синергия рассуждения и воображения улучшает устойчивость, обобщение и интероперабельность политики генералиста.'}, 'en': {'title': 'Synergizing Reasoning and Imagination for Enhanced Agent Performance', 'desc': 'This paper introduces RIG, a novel end-to-end Generalist policy that combines reasoning and imagination for embodied agents in complex environments. By integrating these two capabilities, RIG enhances learning efficiency and generalization, overcoming limitations of previous models that focused on only one aspect. The authors developed a data pipeline that enriches trajectories from existing agents, allowing for joint learning of reasoning and next image generation. Experimental results demonstrate that RIG significantly improves sample efficiency and robustness, enabling agents to self-correct actions based on imagined outcomes before execution.'}, 'zh': {'title': '推理与想象的协同提升智能体能力', 'desc': '本文提出了一种名为RIG的通用策略，首次将推理和想象能力结合在一个端到端的智能体中。通过构建数据管道，逐步整合和丰富从现有智能体收集的轨迹中的推理和想象内容，RIG实现了更高的学习效率和泛化能力。联合学习推理和下一图像生成，明确建模了推理、行动和环境动态之间的内在关联，使得样本效率提高了17倍以上。实验结果表明，推理与想象的协同作用不仅增强了通用策略的鲁棒性和互操作性，还提升了整体性能。'}}}, {'id': 'https://huggingface.co/papers/2503.24370', 'title': 'Effectively Controlling Reasoning Models through Thinking Intervention', 'url': 'https://huggingface.co/papers/2503.24370', 'abstract': 'Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.', 'score': 13, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '5f218f08538c601f', 'authors': ['Tong Wu', 'Chong Xiang', 'Jiachen T. Wang', 'Prateek Mittal'], 'affiliations': ['NVIDIA', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24370.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#architecture', '#open_source', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Управление мышлением ИИ: новый путь к улучшению языковых моделей', 'desc': "Статья представляет новый подход к управлению языковыми моделями (LLM) под названием 'Thinking Intervention'. Этот метод позволяет вмешиваться в процесс рассуждений модели, стратегически вставляя или изменяя определенные токены мышления. Авторы провели обширные эксперименты на различных задачах, включая следование инструкциям и безопасность. Результаты показывают значительное улучшение производительности по сравнению с базовыми методами промптинга, открывая новые возможности для контроля над рассуждающими языковыми моделями."}, 'en': {'title': 'Enhancing LLM Reasoning with Thinking Intervention', 'desc': 'This paper introduces a new method called Thinking Intervention, which enhances the reasoning capabilities of large language models (LLMs) by allowing explicit control over their internal thought processes. By inserting or modifying specific reasoning tokens, the model can generate more accurate and contextually relevant answers. The authors conducted extensive tests on various tasks, showing that this approach leads to significant improvements in performance, such as higher accuracy in following instructions and better handling of unsafe prompts. Overall, the findings suggest that Thinking Intervention provides a valuable framework for refining how LLMs reason and respond to complex queries.'}, 'zh': {'title': '思维干预：提升大型语言模型推理能力的新方法', 'desc': '本文提出了一种新的思维干预（Thinking Intervention）方法，旨在通过插入或修改特定的思维标记来引导大型语言模型（LLMs）的内部推理过程。这种方法使得模型在复杂问题解决中能够更好地生成中间推理步骤，从而提高最终答案的准确性。我们在多个任务上进行了全面评估，结果显示思维干预显著优于传统的提示方法，尤其在指令遵循和推理层次方面取得了显著的准确率提升。我们的研究为控制推理过程中的大型语言模型开辟了新的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2503.23284', 'title': 'SketchVideo: Sketch-based Video Generation and Editing', 'url': 'https://huggingface.co/papers/2503.23284', 'abstract': "Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.", 'score': 13, 'issue_id': 2996, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'd968c1da27effa84', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#multimodal', '#optimization', '#games', '#video'], 'emoji': '✏️', 'ru': {'title': 'Точное управление видео через скетчи', 'desc': 'Эта статья представляет новый подход к генерации и редактированию видео на основе скетчей. Авторы предлагают эффективную структуру управления с блоками контроля скетчей, которая работает с моделью генерации видео DiT. Для распространения условий скетча на все кадры используется механизм межкадрового внимания. Также разработан модуль вставки видео для согласованного редактирования, а при инференсе применяется латентное слияние для сохранения неотредактированных областей.'}, 'en': {'title': 'Sketch Your Way to Better Video Control!', 'desc': 'This paper presents a novel approach to video generation and editing using sketch-based controls. It addresses the challenges of accurately managing global layout and motion details by introducing a memory-efficient control structure that utilizes sketch control blocks. The method allows users to draw sketches on keyframes, which are then analyzed through an inter-frame attention mechanism to ensure consistency across all video frames. The proposed system, SketchVideo, enhances the ability to generate and edit videos while preserving the integrity of unedited regions, demonstrating superior performance in controllable video tasks.'}, 'zh': {'title': '草图驱动的视频生成与编辑新方法', 'desc': '本论文探讨了基于草图的空间和运动控制在视频生成中的应用，旨在解决文本和图像条件下的布局和几何细节控制问题。我们提出了一种高效的控制结构，利用草图控制块预测跳过的DiT块的残差特征。通过在关键帧上绘制草图，并使用跨帧注意机制分析关键帧与每个视频帧之间的关系，实现了时间上稀疏的草图条件传播。实验结果表明，我们的SketchVideo在可控视频生成和编辑方面表现优越。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.23077', 'title': 'Efficient Inference for Large Reasoning Models: A Survey', 'url': 'https://huggingface.co/papers/2503.23077', 'abstract': "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant fieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.", 'score': 13, 'issue_id': 2995, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': 'f76d7ead3d85b37e', 'authors': ['Yue Liu', 'Jiaying Wu', 'Yufei He', 'Hongcheng Gao', 'Hongyu Chen', 'Baolong Bi', 'Jiaheng Zhang', 'Zhiqi Huang', 'Bryan Hooi'], 'affiliations': ['Beijing Jiaotong University', 'Moonshot', 'National University of Singapore', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.23077.jpg', 'data': {'categories': ['#reasoning', '#survey', '#interpretability', '#optimization', '#inference', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности вывода в моделях крупномасштабного рассуждения', 'desc': 'Данная статья представляет обзор методов эффективного вывода для моделей крупномасштабного рассуждения (LRM). Авторы классифицируют существующие подходы на две категории: явные компактные цепочки рассуждений и неявные латентные цепочки рассуждений. В работе проводится эмпирический анализ эффективности и производительности этих методов. Также обсуждаются открытые проблемы в этой области, включая контролируемое рассуждение, ориентированное на человека, баланс между интерпретируемостью и эффективностью, безопасность и более широкое применение эффективного рассуждения.'}, 'en': {'title': 'Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses Large Reasoning Models (LRMs) that enhance the reasoning capabilities of Large Language Models (LLMs) but face challenges like inefficient token usage and high memory consumption. It reviews various efficient inference methods aimed at reducing these inefficiencies while maintaining reasoning quality. The authors categorize these methods into explicit and implicit Chain-of-Thought approaches, analyzing their strengths and weaknesses. Additionally, the paper identifies ongoing challenges in the field and suggests strategies for improving inference efficiency in LRMs.'}, 'zh': {'title': '提升推理效率，优化大型推理模型', 'desc': '大型推理模型（LRMs）通过学习推理显著提高了大型语言模型（LLMs）的推理能力，能够在复杂任务中表现出色。然而，它们的深思熟虑的推理过程导致了令牌使用、内存消耗和推理时间的低效。因此，本文综述了专门为LRMs设计的高效推理方法，重点在于减轻令牌低效，同时保持推理质量。我们介绍了一种分类法，将最近的方法分为两大类：显式紧凑的思维链（CoT）和隐式潜在的思维链，讨论了它们的优缺点，并分析了现有方法的性能和效率。'}}}, {'id': 'https://huggingface.co/papers/2503.24364', 'title': 'Query and Conquer: Execution-Guided SQL Generation', 'url': 'https://huggingface.co/papers/2503.24364', 'abstract': 'We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.', 'score': 12, 'issue_id': 2997, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '2af26722887e77ac', 'authors': ['Łukasz Borchmann', 'Marek Wydmuch'], 'affiliations': ['Snowflake AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.24364.jpg', 'data': {'categories': ['#optimization', '#small_models', '#dataset', '#inference', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Революция в генерации SQL: эффективность через семантическую согласованность', 'desc': 'Предложен новый подход к генерации сложных выходных данных, значительно повышающий точность в задачах преобразования текста в SQL. Метод использует результаты выполнения для выбора наиболее семантически согласованного запроса из нескольких кандидатов. Это позволяет меньшим, экономически эффективным моделям превзойти вычислительно интенсивные методы рассуждений, такие как o1, o3-mini и DeepSeek R1, при этом снижая стоимость вывода до 30 раз. Подход легко интегрируется с существующими моделями, предлагая практичный и масштабируемый путь к современной генерации SQL.'}, 'en': {'title': 'Efficient SQL Generation: Small Models, Big Results!', 'desc': 'This paper introduces a new method for generating SQL queries from natural language, enhancing accuracy in text-to-SQL tasks. The approach utilizes execution results to choose the best query from several options based on semantic consistency. It allows smaller models to outperform larger, more complex models while also cutting down inference costs significantly. This method is designed to work well with current models, making it a practical solution for improving SQL generation.'}, 'zh': {'title': '高效生成SQL，提升文本到SQL的准确性', 'desc': '我们提出了一种新颖的方法，用于生成复杂输出，显著提高文本到SQL任务的准确性。该方法利用执行结果，从多个候选查询中选择最符合语义的一项，使得较小、成本效益高的模型能够超越计算密集型的推理方法，如o1、o3-mini和DeepSeek R1，同时将推理成本降低多达30倍。它与现有模型无缝集成，提供了一条实用且可扩展的通往最先进SQL生成的路径。'}}}, {'id': 'https://huggingface.co/papers/2503.23829', 'title': 'Expanding RL with Verifiable Rewards Across Diverse Domains', 'url': 'https://huggingface.co/papers/2503.23829', 'abstract': "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.", 'score': 12, 'issue_id': 2998, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '2c875f8335892dfd', 'authors': ['Yi Su', 'Dian Yu', 'Linfeng Song', 'Juntao Li', 'Haitao Mi', 'Zhaopeng Tu', 'Min Zhang', 'Dong Yu'], 'affiliations': ['Soochow University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.23829.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rlhf', '#training', '#rl', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'RLVR: Универсальное обучение с подкреплением для различных областей знаний', 'desc': 'Исследование посвящено расширению применения обучения с подкреплением с проверяемыми вознаграждениями (RLVR) на различные области, такие как медицина, химия, психология и экономика. Авторы обнаружили высокое согласие в бинарных оценках между различными большими языковыми моделями при наличии объективных эталонных ответов. Для улучшения гибкости метода при работе с неструктурированными эталонными ответами было предложено использование мягкого оценивания на основе моделей. Эксперименты показали, что дистиллированная генеративная модель вознаграждения может служить эффективным междоменным верификатором, обеспечивая надежные сигналы вознаграждения для обучения с подкреплением без необходимости в аннотациях для конкретных доменов.'}, 'en': {'title': 'Expanding RLVR: From Coding to Real-World Applications', 'desc': "This paper explores the use of Reinforcement Learning with Verifiable Rewards (RLVR) in diverse fields like medicine and economics, where traditional structured answers may not be available. The authors find that different large language models (LLMs) often agree on binary judgments when objective reference answers are present, suggesting that extensive annotations may not be necessary for training reward models. To enhance RLVR's effectiveness, they introduce model-based soft scoring to better handle unstructured reference answers. Their experiments demonstrate that a distilled generative reward model can effectively verify rewards across domains, leading to improved performance of RL policies compared to existing state-of-the-art models."}, 'zh': {'title': '强化学习的可验证奖励：跨领域应用的新可能', 'desc': '强化学习与可验证奖励（RLVR）在数学推理和编码任务中表现出色，但其在更广泛领域的应用尚未深入探索。我们研究了RLVR在医学、化学、心理学和经济学等多样化领域的扩展。研究发现，当存在客观参考答案时，不同的大型语言模型（LLMs）在二元判断上高度一致，这表明训练特定领域奖励模型不一定需要大规模标注。通过将基于模型的软评分纳入RLVR，我们提高了其灵活性，并通过实验验证了蒸馏生成奖励模型在跨领域验证中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.19901', 'title': 'TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization', 'url': 'https://huggingface.co/papers/2503.19901', 'abstract': 'Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/', 'score': 12, 'issue_id': 3000, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'c5dd40417e2f952a', 'authors': ['Liang Pan', 'Zeshi Yang', 'Zhiyang Dou', 'Wenjia Wang', 'Buzhen Huang', 'Bo Dai', 'Taku Komura', 'Jingbo Wang'], 'affiliations': ['Feeling AI', 'Independent Researcher', 'Shanghai AI Laboratory', 'Southeast University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19901.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#transfer_learning', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Единая трансформерная политика для гибкого синтеза взаимодействий человека с окружением', 'desc': 'TokenHSI - это новый подход к синтезу разнообразных и физически правдоподобных взаимодействий человека с окружающей средой (HSI). Метод использует единую трансформерную политику, способную объединять несколько навыков и гибко адаптироваться к новым сценариям. Ключевая идея заключается в моделировании проприоцепции гуманоида как отдельного общего токена и комбинировании его с различными токенами задач через механизм маскирования. Эксперименты показывают, что этот подход значительно улучшает универсальность, адаптивность и расширяемость в различных задачах HSI.'}, 'en': {'title': 'Unifying Skills for Enhanced Human-Scene Interactions', 'desc': 'This paper introduces TokenHSI, a novel transformer-based policy designed to enhance Human-Scene Interactions (HSI) by integrating multiple skills into a single framework. Unlike traditional methods that rely on separate controllers for each task, TokenHSI utilizes a shared token for humanoid proprioception, allowing for effective knowledge sharing across different interaction tasks. The architecture supports variable length inputs, making it adaptable to new scenarios and capable of coordinating complex actions, such as sitting while carrying an object. Experimental results show that TokenHSI significantly improves the versatility and adaptability of HSI tasks, paving the way for more sophisticated applications in computer animation and embodied AI.'}, 'zh': {'title': '统一多技能的人类场景交互策略', 'desc': '本论文提出了一种名为TokenHSI的统一变换器基础策略，用于合成多样且物理上合理的人类场景交互（HSI）。当前的方法主要集中在为特定交互任务开发独立控制器，这限制了处理复杂任务的能力。TokenHSI通过将人体本体感知建模为共享的独立标记，并结合不同的任务标记，促进了多技能的统一和灵活适应。实验结果表明，该方法在多种HSI任务中显著提高了多样性、适应性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2503.24115', 'title': 'TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection', 'url': 'https://huggingface.co/papers/2503.24115', 'abstract': 'The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.', 'score': 9, 'issue_id': 2994, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '61845428f5c3d9df', 'authors': ['Zhiming Ma', 'Peidong Wang', 'Minhua Huang', 'Jingpeng Wang', 'Kai Wu', 'Xiangzhao Lv', 'Yachun Pang', 'Yin Yang', 'Wenjie Tang', 'Yuchen Kang'], 'affiliations': ['China Mobile Internet Company Ltd. Guangzhou, Guangdong, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.24115.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#synthetic', '#open_source', '#benchmark', '#data'], 'emoji': '🎭', 'ru': {'title': 'Мультимодальный датасет для борьбы с телефонным мошенничеством', 'desc': 'Статья представляет TeleAntiFraud-28k - первый открытый аудио-текстовый датасет для анализа телекоммуникационного мошенничества. Датасет создан с использованием генерации образцов на основе ASR и TTS, семантического расширения с помощью LLM и многоагентного состязательного синтеза. Он содержит 28,511 пар речь-текст с аннотациями для рассуждений о мошенничестве и разделен на три задачи. Авторы также представляют TeleAntiFraud-Bench для оценки производительности моделей и открытую модель SFT.'}, 'en': {'title': 'Revolutionizing Telecom Fraud Detection with TeleAntiFraud-28k', 'desc': 'This paper introduces TeleAntiFraud-28k, a novel dataset designed to enhance telecom fraud detection by combining audio signals with textual analysis. The dataset is created using three innovative strategies, including privacy-preserved text generation from speech recordings and semantic enhancement through large language models. It consists of over 28,000 annotated speech-text pairs, enabling tasks like scenario classification and fraud detection. Additionally, the authors provide a benchmark for evaluating model performance and a fine-tuned model for practical applications, promoting further research in multimodal anti-fraud techniques.'}, 'zh': {'title': '构建电信欺诈检测的新基石', 'desc': '本论文提出了TeleAntiFraud-28k数据集，这是第一个专为电信欺诈分析设计的开源音频-文本慢思考数据集。该数据集通过三种策略构建，确保了数据的隐私保护和真实场景的一致性。我们还建立了TeleAntiFraud-Bench评估基准，以便系统地测试模型在电信欺诈检测任务上的表现。此项工作为多模态反欺诈研究奠定了基础，同时解决了数据隐私和场景多样性等关键挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.18809', 'title': 'Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code', 'url': 'https://huggingface.co/papers/2503.18809', 'abstract': 'In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit "reasoning" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs.', 'score': 9, 'issue_id': 2994, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '28288adc69a019ac', 'authors': ['Augusto B. Corrêa', 'André G. Pereira', 'Jendrik Seipp'], 'affiliations': ['Federal University of Rio Grande do Sul', 'Linköping University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.18809.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'LLM как генераторы эффективных эвристик для задач планирования', 'desc': 'Исследователи разработали метод использования больших языковых моделей (LLM) для генерации эвристических функций в задачах планирования. LLM создают несколько эвристик в виде Python-кода, которые затем оцениваются на тренировочных задачах. Выбранные эвристики показывают высокую эффективность на новых задачах, превосходя современные методы доменно-независимого планирования. Этот подход позволяет значительно улучшить способности LLM к планированию, даже для задач возрастающей сложности.'}, 'en': {'title': 'Empowering LLMs with Domain-Specific Heuristics for Better Planning', 'desc': 'This paper addresses the limitations of large language models (LLMs) in planning tasks, which often lead to incorrect and non-generalizable plans. The authors propose a novel approach where LLMs generate domain-specific heuristic functions in Python code, which are then evaluated using a greedy best-first search algorithm. Their method demonstrates that LLM-generated heuristics can outperform traditional domain-independent heuristics and compete with advanced learning algorithms in planning. The results indicate that these heuristics are not only efficient but also provide more informative guidance in certain planning domains.'}, 'zh': {'title': '提升大型语言模型的规划能力', 'desc': '近年来，大型语言模型（LLMs）在各种人工智能问题上展现了卓越的能力。然而，即使在详细定义规划任务的情况下，它们在规划方面仍然不可靠。本文展示了如何利用LLMs生成正确的规划，即使对于越来越大的分布外任务。通过生成领域相关的启发式函数并在贪婪优先搜索中评估，LLM生成的启发式函数在解决未见测试任务方面表现优于传统的领域无关启发式方法。'}}}, {'id': 'https://huggingface.co/papers/2503.22673', 'title': 'ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action Models', 'url': 'https://huggingface.co/papers/2503.22673', 'abstract': 'Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to facilitate research in the community.', 'score': 8, 'issue_id': 3011, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '32c8476678c711ba', 'authors': ['Jianguo Zhang', 'Thai Hoang', 'Ming Zhu', 'Zuxin Liu', 'Shiyu Wang', 'Tulika Awalgaonkar', 'Akshara Prabhakar', 'Haolin Chen', 'Weiran Yao', 'Zhiwei Liu', 'Juntao Tan', 'Juan Carlos Niebles', 'Shelby Heinecke', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.22673.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'ActionStudio: Универсальный инструмент для обучения моделей действий ИИ-агентов', 'desc': 'ActionStudio - это легковесный и расширяемый фреймворк для обучения крупных моделей действий. Он унифицирует разнородные траектории агентов через стандартизированный формат и поддерживает различные парадигмы обучения, включая LoRA и полную тонкую настройку. ActionStudio интегрирует инструменты предобработки и верификации, а также демонстрирует высокую производительность на публичных и промышленных бенчмарках. Фреймворк предназначен для упрощения обучения моделей действий для автономных агентов.'}, 'en': {'title': 'Empowering Autonomous Agents with ActionStudio', 'desc': 'This paper introduces ActionStudio, a new framework designed to improve the training of large action models for autonomous agents. It addresses the challenges of diverse environments and complex data by providing a standardized format for agent trajectories. ActionStudio supports various training methods, including LoRA and full fine-tuning, and offers tools for data preprocessing and verification. The framework has been validated on both public and industry benchmarks, showing strong performance and scalability, and is open-sourced for community use.'}, 'zh': {'title': 'ActionStudio：提升大型动作模型训练的利器', 'desc': '本文介绍了ActionStudio，这是一个轻量级且可扩展的数据和训练框架，旨在支持大型动作模型的训练。ActionStudio通过标准化格式统一了不同的代理轨迹，支持多种训练范式，包括LoRA、完全微调和分布式设置。该框架还集成了强大的预处理和验证工具，以提高训练的效率和可靠性。我们在公共和实际行业基准上验证了其有效性，展示了强大的性能和实用的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2503.21694', 'title': 'Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data', 'url': 'https://huggingface.co/papers/2503.21694', 'abstract': 'It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only 2.5% trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at https://github.com/theEricMa/TriplaneTurbo.', 'score': 8, 'issue_id': 3000, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '703f61255714367b', 'authors': ['Zhiyuan Ma', 'Xinyue Liang', 'Rongyuan Wu', 'Xiangyu Zhu', 'Zhen Lei', 'Lei Zhang'], 'affiliations': ['Center for Artificial Intelligence and Robotics, HKISI CAS', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, UCAS', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21694.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#3d', '#diffusion', '#open_source'], 'emoji': '🧊', 'ru': {'title': 'Быстрая генерация 3D-объектов из текста без 3D-разметки', 'desc': 'Статья представляет новый метод обучения генеративных моделей для создания 3D-объектов из текстовых описаний, называемый Progressive Rendering Distillation (PRD). PRD использует предобученные модели диффузии для 2D-изображений и многоракурсные модели, чтобы обойти проблему нехватки качественных 3D-данных для обучения. Метод позволяет адаптировать модель Stable Diffusion для генерации 3D-представлений без необходимости в 3D-разметке. В результате авторы создали модель TriplaneTurbo, способную генерировать качественные 3D-модели за 1.2 секунды.'}, 'en': {'title': 'Fast and High-Quality 3D Mesh Generation from Text Prompts', 'desc': 'This paper introduces a new method called Progressive Rendering Distillation (PRD) to create high-quality 3D meshes from text prompts quickly. PRD allows training without needing 3D ground-truth data by using multi-view diffusion models to distill textures and geometries into 3D outputs. The method enhances the efficiency of the generation process, enabling the model to produce 3D meshes in just 1.2 seconds while maintaining high quality. The resulting model, TriplaneTurbo, shows significant improvements over previous text-to-3D generators in both speed and output quality.'}, 'zh': {'title': '快速生成高质量3D网格的创新方案', 'desc': '本论文提出了一种新的训练方案，称为渐进渲染蒸馏（PRD），旨在从文本提示中快速生成高质量的3D网格。通过蒸馏多视图扩散模型，PRD消除了对3D真实数据的需求，使得训练数据的扩展变得更加容易。该方法利用U-Net逐步去噪，并将去噪后的潜在表示解码为3D输出，从而提高生成质量。最终，PRD训练的TriplaneTurbo生成器在效率和质量上均优于之前的文本到3D生成器，能够在1.2秒内生成高质量的3D网格。'}}}, {'id': 'https://huggingface.co/papers/2503.24391', 'title': 'Easi3R: Estimating Disentangled Motion from DUSt3R Without Training', 'url': 'https://huggingface.co/papers/2503.24391', 'abstract': 'Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major bottleneck for training a highly generalizable 4D model. This constraint has driven conventional 4D methods to fine-tune 3D models on scalable dynamic video data with additional geometric priors such as optical flow and depths. In this work, we take an opposite path and introduce Easi3R, a simple yet efficient training-free method for 4D reconstruction. Our approach applies attention adaptation during inference, eliminating the need for from-scratch pre-training or network fine-tuning. We find that the attention layers in DUSt3R inherently encode rich information about camera and object motion. By carefully disentangling these attention maps, we achieve accurate dynamic region segmentation, camera pose estimation, and 4D dense point map reconstruction. Extensive experiments on real-world dynamic videos demonstrate that our lightweight attention adaptation significantly outperforms previous state-of-the-art methods that are trained or finetuned on extensive dynamic datasets. Our code is publicly available for research purpose at https://easi3r.github.io/', 'score': 3, 'issue_id': 3001, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '48e601b0440aa5d3', 'authors': ['Xingyu Chen', 'Yue Chen', 'Yuliang Xiu', 'Andreas Geiger', 'Anpei Chen'], 'affiliations': ['Max Planck Institute for Intelligent Systems', 'University of Tubingen, Tubingen AI Center', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.24391.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#open_source', '#3d', '#video'], 'emoji': '🌟', 'ru': {'title': 'Easi3R: Эффективная 4D-реконструкция без обучения', 'desc': 'Статья представляет метод Easi3R для 4D-реконструкции, который не требует предварительного обучения или дообучения нейронной сети. Используя адаптацию внимания во время вывода, метод применяет предобученную 3D-модель DUSt3R для динамических сцен. Easi3R разделяет карты внимания для точной сегментации динамических областей, оценки положения камеры и реконструкции плотной 4D карты точек. Эксперименты показывают, что Easi3R превосходит современные методы, обученные на больших динамических датасетах.'}, 'en': {'title': 'Easi3R: Efficient 4D Reconstruction Without Pre-training', 'desc': 'This paper presents Easi3R, a novel method for 4D reconstruction that does not require extensive pre-training or fine-tuning of models. Instead, it utilizes attention adaptation during inference to effectively segment dynamic regions and estimate camera poses. The authors leverage the inherent information encoded in the attention layers of the DUSt3R model, allowing for accurate reconstruction of dense point clouds in dynamic scenes. Experimental results show that Easi3R outperforms existing methods that rely on large dynamic datasets, highlighting its efficiency and effectiveness.'}, 'zh': {'title': 'Easi3R：无需训练的高效4D重建方法', 'desc': '本论文介绍了一种名为Easi3R的4D重建方法，该方法无需预训练或微调网络，利用注意力适应技术进行推理。与传统方法依赖于大规模动态视频数据不同，Easi3R通过解耦注意力图，准确实现动态区域分割、相机姿态估计和4D稠密点云重建。实验结果表明，Easi3R在真实动态视频上的表现显著优于以往的最先进方法。该方法的轻量级设计使其在处理动态场景时更加高效。'}}}, {'id': 'https://huggingface.co/papers/2503.23730', 'title': 'KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language', 'url': 'https://huggingface.co/papers/2503.23730', 'abstract': 'The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA', 'score': 3, 'issue_id': 2995, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '1d3d53298afe6cce', 'authors': ['Yoonshik Kim', 'Jaeyoon Jung'], 'affiliations': ['MAUM AI Inc. / Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.23730.jpg', 'data': {'categories': ['#open_source', '#low_resource', '#benchmark', '#multilingual'], 'emoji': '🇰🇷', 'ru': {'title': 'KOFFVQA: объективная оценка мультимодальных моделей на корейском языке', 'desc': 'Статья представляет новый бенчмарк KOFFVQA для оценки мультимодальных моделей (VLM) на корейском языке. Бенчмарк состоит из 275 вопросов с изображениями и критериями оценки, охватывающими 10 аспектов работы VLM. Авторы предлагают объективный метод оценки ответов моделей, основанный на предопределенных критериях. Эксперименты показывают, что этот подход более надежен, чем существующие методы оценки VLM.'}, 'en': {'title': 'KOFFVQA: Reliable Evaluation for Korean Vision-Language Models', 'desc': 'This paper introduces KOFFVQA, a new benchmark for evaluating Large Vision-Language Models (VLMs) specifically in the Korean language. It addresses the limitations of current evaluation methods that either restrict responses or rely on subjective judge models. KOFFVQA features 275 visual question-answering tasks with clear grading criteria that enhance reliability and objectivity in assessments. The authors demonstrate that their approach provides a more dependable evaluation of VLMs compared to existing methods, making it suitable for both large and small models.'}, 'zh': {'title': 'KOFFVQA：韩语视觉语言模型的可靠评估基准', 'desc': '本文介绍了一种新的视觉语言模型（VLM）评估基准，名为KOFFVQA，专门针对韩语。现有的评估方法往往依赖于预设的回答选项或评判模型，导致评估结果主观且不可靠。KOFFVQA包含275个精心设计的问题，每个问题都配有图像和涵盖VLM性能的10个评估标准。通过客观的评估标准，我们的方法能够更可靠地评估模型，即使是小型开源模型也能有效使用。'}}}, {'id': 'https://huggingface.co/papers/2503.20286', 'title': 'Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization', 'url': 'https://huggingface.co/papers/2503.20286', 'abstract': 'Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.', 'score': 3, 'issue_id': 2994, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'bf1debfaa462fca8', 'authors': ['Zhenyu Liang', 'Hao Li', 'Naiwei Yu', 'Kebin Sun', 'Ran Cheng'], 'affiliations': ['Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Data Science and Artificial Intelligence and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.20286.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#robotics', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Тензоризация EMO: революция в скорости многоцелевой оптимизации', 'desc': 'Статья представляет новый подход к эволюционной многоцелевой оптимизации (EMO) с использованием тензоризации для ускорения алгоритмов на GPU. Авторы применили эту методологию к трем известным алгоритмам EMO: NSGA-III, MOEA/D и HypE. Эксперименты показали ускорение до 1113 раз по сравнению с версиями для CPU при сохранении качества решений. Также был представлен новый бенчмарк многоцелевого управления роботом с использованием физического движка на GPU для оценки эффективности предложенного подхода.'}, 'en': {'title': 'Accelerating EMO with GPU Tensorization for Enhanced Performance', 'desc': 'This paper addresses the limitations of traditional evolutionary multiobjective optimization (EMO) algorithms when faced with complex and large-scale problems. It highlights the lack of hardware acceleration in existing EMO approaches and proposes a novel method to parallelize these algorithms using GPUs through tensorization. By transforming EMO data structures into tensor representations, the authors enable efficient GPU computing, resulting in significant performance improvements. The proposed tensorized EMO algorithms demonstrate remarkable speedups while maintaining solution quality in multiobjective robot control tasks.'}, 'zh': {'title': '张量化提升EMO算法性能，GPU加速显著', 'desc': '进化多目标优化（EMO）在过去二十年取得了显著进展，但随着问题规模和复杂性的增加，传统的EMO算法面临性能限制。本文提出了一种通过张量化方法在GPU上并行化EMO算法的方案，以解决传统算法的并行性和可扩展性不足的问题。通过张量化，EMO算法的数据结构和操作被转化为简洁的张量表示，从而实现了GPU计算的自动利用。实验结果表明，张量化的EMO算法在速度上比基于CPU的算法快了多达1113倍，同时保持了解决方案的质量，并有效处理复杂的多目标机器人控制任务。'}}}, {'id': 'https://huggingface.co/papers/2503.14941', 'title': 'UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation', 'url': 'https://huggingface.co/papers/2503.14941', 'abstract': 'Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves a Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences.', 'score': 3, 'issue_id': 3000, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '0d6cdad4ff3e795e', 'authors': ['Qihui Zhang', 'Munan Ning', 'Zheyuan Liu', 'Yanbo Wang', 'Jiayi Ye', 'Yue Huang', 'Shuo Yang', 'Xiao Chen', 'Yibing Song', 'Li Yuan'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'School of Electrical and Computer Engineering, Peking University', 'Tsinghua University', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2503.14941.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#interpretability', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Автоматическая оценка мультимодальных ИИ без участия человека', 'desc': 'Статья представляет новый подход к оценке мультимодальных больших языковых моделей (MLLM) для задач визуального вопросно-ответного анализа. Авторы предлагают фреймворк Unsupervised Peer review MLLM Evaluation (UPME), который использует только данные изображений для автоматической генерации вопросов и проведения экспертной оценки ответов других моделей. UPME включает систему оценки связи между зрением и языком, фокусируясь на правильности ответа, визуальном понимании и рассуждении, а также корреляции между изображением и текстом. Экспериментальные результаты показывают высокую корреляцию UPME с человеческими оценками на различных наборах данных.'}, 'en': {'title': 'Automating VQA Evaluations with Unsupervised Peer Review', 'desc': 'This paper introduces a new framework called Unsupervised Peer review MLLM Evaluation (UPME) to improve the evaluation of Multimodal Large Language Models (MLLMs) in Visual Question Answering (VQA). The framework reduces the need for human involvement by allowing models to automatically generate questions and assess answers from other models using only image data. To address biases in automated evaluations, a vision-language scoring system is implemented, focusing on response correctness, visual understanding, and image-text correlation. Experimental results show that UPME closely aligns with human evaluations, achieving high correlation scores on benchmark datasets.'}, 'zh': {'title': '无监督同行评审，提升视觉问答评估的效率与公正性', 'desc': '多模态大型语言模型（MLLMs）正在解决视觉问答（VQA）中的挑战，推动了对这些模型进行客观评估的新研究方向。现有的评估方法由于需要大量人力设计问答对，限制了评估的规模和范围。虽然自动化的MLLM评估方法试图减少人力工作量，但往往会引入偏见。为了解决这些问题，我们提出了一种无监督的同行评审MLLM评估框架，利用图像数据自动生成问题，并对其他模型的答案进行评估，从而有效减轻对人力的依赖。'}}}, {'id': 'https://huggingface.co/papers/2503.23022', 'title': 'MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs', 'url': 'https://huggingface.co/papers/2503.23022', 'abstract': 'In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, a novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) a transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) a flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with a predefined number of faces. By utilizing the diffusion model for the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds (35times faster than existing baselines). Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation.', 'score': 2, 'issue_id': 3002, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '4bb15e0559669bbb', 'authors': ['Xianglong He', 'Junyi Chen', 'Di Huang', 'Zexiang Liu', 'Xiaoshui Huang', 'Wanli Ouyang', 'Chun Yuan', 'Yangguang Li'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'The University of Sydney', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2503.23022.jpg', 'data': {'categories': ['#3d', '#optimization', '#diffusion', '#games'], 'emoji': '🧊', 'ru': {'title': 'MeshCraft: быстрое создание 3D-сеток с контролируемой топологией', 'desc': 'MeshCraft - это новая система для эффективной и контролируемой генерации трёхмерных сеток, использующая непрерывную пространственную диффузию для создания дискретных треугольных граней. Она состоит из VAE на основе трансформера для кодирования и декодирования сеток, а также диффузионного трансформера для генерации топологии с заданным числом граней. MeshCraft значительно ускоряет процесс создания высококачественных 3D-сеток по сравнению с авторегрессивными методами, генерируя сетку из 800 граней всего за 3,2 секунды. Система превосходит современные техники в качественных и количественных оценках на наборах данных ShapeNet и Objaverse, а также легко интегрируется с существующими стратегиями условного управления.'}, 'en': {'title': 'MeshCraft: Fast and Controlled 3D Mesh Generation', 'desc': 'This paper presents MeshCraft, a new framework for generating 3D mesh topologies efficiently and with control over the number of faces. Unlike previous methods that use auto-regressive techniques, MeshCraft employs a transformer-based variational autoencoder (VAE) and a flow-based diffusion transformer to create high-quality meshes quickly. The framework allows for the simultaneous generation of entire mesh structures, significantly speeding up the process to just 3.2 seconds for an 800-face mesh. Experimental results show that MeshCraft outperforms existing methods in both quality and speed, making it a valuable tool for 3D artists.'}, 'zh': {'title': 'MeshCraft：高效可控的3D网格生成新方法', 'desc': '在3D内容创作领域，MeshCraft是一种新颖的高效可控网格生成框架。它利用连续空间扩散生成离散三角面，克服了传统自回归方法的生成速度慢和面数不可控的缺陷。MeshCraft由两个核心组件组成：基于变换器的变分自编码器（VAE）和条件化面数的流扩散变换器。通过同时生成整个网格拓扑，MeshCraft在生成高质量3D网格时速度显著提高，能够在3.2秒内生成800面网格，速度比现有方法快35倍。'}}}, {'id': 'https://huggingface.co/papers/2503.22655', 'title': 'Unicorn: Text-Only Data Synthesis for Vision Language Model Training', 'url': 'https://huggingface.co/papers/2503.22655', 'abstract': 'Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git.', 'score': 2, 'issue_id': 3005, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': 'a724544e9c0362b6', 'authors': ['Xiaomin Yu', 'Pengxiang Ding', 'Wenjie Zhang', 'Siteng Huang', 'Songyang Gao', 'Chengwei Qin', 'Kejian Wu', 'Zhaoxin Fan', 'Ziyue Qiao', 'Donglin Wang'], 'affiliations': ['Beihang University', 'Nanyang Technological University', 'Shanghai AI Lab', 'The Great Bay University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22655.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#data', '#reasoning', '#synthetic'], 'emoji': '🦄', 'ru': {'title': 'Синтез мультимодальных данных из текста для обучения ВЯМ', 'desc': 'Статья представляет новый подход к обучению визуально-языковых моделей без использования реальных изображений. Авторы предлагают трехэтапный процесс синтеза мультимодальных данных, включающий генерацию разнообразных подписей к изображениям, создание инструкций для обучения и преобразование текстовых представлений в визуальные. Этот метод позволяет создать наборы данных Unicorn-1.2M для предварительного обучения и Unicorn-471K-Instruction для обучения на основе инструкций. Подход обеспечивает экономичное и масштабируемое решение для обучения визуально-языковых моделей.'}, 'en': {'title': 'Synthesize High-Quality Multimodal Data from Text Alone!', 'desc': 'This paper presents a novel framework for synthesizing multimodal training data for vision-language models (VLMs) using only text. The proposed three-stage process generates high-quality image-text pairs without the need for real images, significantly reducing costs. In the first stage, diverse captions are created from sparse seeds using large language models, followed by the generation of instruction-tuning tasks in the second stage. Finally, the textual representations are transformed into visual representations, resulting in two datasets: Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, enhancing the training of VLMs.'}, 'zh': {'title': '无图像高效合成多模态数据', 'desc': '本论文提出了一种跨集成的三阶段多模态数据合成框架，用于生成高质量的图像-文本对。第一阶段通过大型语言模型扩展稀疏的文本种子，合成了120万条语义多样的高质量文本描述。第二阶段将47万条文本描述转化为多轮指令调优任务，以支持复杂推理。最后，第三阶段将文本表示转换为视觉表示，从而生成多样的合成图像表示，提供了一种无需真实图像的高效训练方案。'}}}, {'id': 'https://huggingface.co/papers/2503.23913', 'title': 'Entropy-Based Adaptive Weighting for Self-Training', 'url': 'https://huggingface.co/papers/2503.23913', 'abstract': 'The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs a mapping function with a tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K, EAST attains a further 1-2% performance boost compared to the vanilla method.', 'score': 1, 'issue_id': 3002, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'b7e8ee7260c71bb7', 'authors': ['Xiaoxuan Wang', 'Yihe Deng', 'Mingyu Derek Ma', 'Wei Wang'], 'affiliations': ['University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.23913.jpg', 'data': {'categories': ['#training', '#reasoning', '#math', '#benchmark', '#optimization'], 'emoji': '🧮', 'ru': {'title': 'Адаптивное взвешивание для улучшения математических способностей языковых моделей', 'desc': 'Эта статья посвящена улучшению способностей больших языковых моделей (LLM) решать математические задачи с помощью самогенерируемых путей рассуждений. Авторы предлагают метод EAST (Entropy-Based Adaptive Weighting for Self-Training), который адаптивно взвешивает данные во время самообучения, отдавая приоритет неопределенным примерам. EAST использует функцию отображения с настраиваемым параметром, контролирующим резкость взвешивания. Эмпирические результаты показывают, что EAST достигает улучшения производительности на 1% на бенчмарке MATH и на 1-2% на GSM8K по сравнению с базовыми методами.'}, 'en': {'title': 'Enhancing Model Reasoning with Adaptive Uncertainty Weighting', 'desc': "This paper explores how large language models can improve their mathematical problem-solving skills by using self-generated reasoning paths. The authors introduce a new method called Entropy-Based Adaptive Weighting for Self-Training (EAST), which focuses on prioritizing uncertain data during the training process. By adjusting the importance of different training examples based on the model's uncertainty, EAST helps the model learn from more challenging and informative cases. The results show that EAST leads to slight performance improvements on benchmark tests compared to traditional self-training methods."}, 'zh': {'title': '自适应加权提升模型推理能力', 'desc': '本文研究了大型语言模型在数学问题解决中的能力，提出了一种自我训练方法，利用自生成的推理路径来提升模型性能。我们提出了一种名为EAST的自适应加权策略，旨在优先考虑不确定性较高的数据进行自我训练。EAST通过可调参数的映射函数来控制加权的锐度，从而引导模型关注更具信息量和挑战性的示例。实验结果表明，EAST在GSM8K和MATH基准测试中均取得了性能提升，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.19906', 'title': 'AvatarArtist: Open-Domain 4D Avatarization', 'url': 'https://huggingface.co/papers/2503.19906', 'abstract': 'This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies..', 'score': 1, 'issue_id': 3012, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '9cc5699cb424a056', 'authors': ['Hongyu Liu', 'Xuan Wang', 'Ziyu Wan', 'Yue Ma', 'Jingye Chen', 'Yanbo Fan', 'Yujun Shen', 'Yibing Song', 'Qifeng Chen'], 'affiliations': ['Ant Group', 'City University of Hong Kong', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2503.19906.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#diffusion', '#3d'], 'emoji': '🎭', 'ru': {'title': 'AvatarArtist: универсальный генератор 4D-аватаров из портретов', 'desc': 'Статья посвящена созданию 4D-аватаров из портретных изображений в произвольном стиле. Авторы используют параметрические триплейны в качестве промежуточного 4D-представления и предлагают подход, сочетающий генеративно-состязательные сети (GAN) и диффузионные модели. GAN используются для связи изображений и триплейнов, а диффузионная модель помогает обрабатывать разнообразные распределения данных. Эта синергия позволяет создать мультидоменный набор данных изображение-триплейн для обучения универсального генератора 4D-аватаров.'}, 'en': {'title': 'Transforming Portraits into Dynamic 4D Avatars!', 'desc': 'This paper presents a method for creating 4D avatars from 2D portrait images in different styles. It utilizes parametric triplanes as a 4D representation and combines generative adversarial networks (GANs) with diffusion models for effective training. The approach leverages the strengths of 4D GANs to connect images and triplanes while addressing challenges with diverse data through a robust 2D diffusion prior. The resulting model, AvatarArtist, demonstrates the ability to generate high-quality 4D avatars across various image domains, with plans to share the code and data for further research.'}, 'zh': {'title': '打造多领域高质量4D头像的创新之路', 'desc': '本研究专注于开放领域的4D头像生成，旨在从任意风格的肖像图像中创建4D头像。我们选择参数三平面作为中间的4D表示，并提出了一种实用的训练范式，结合了生成对抗网络（GAN）和扩散模型的优势。研究表明，4D GAN在无监督情况下能够有效连接图像和三平面，但在处理多样化数据分布时通常面临挑战。通过引入强大的2D扩散先验，帮助GAN在不同领域之间转移其专业知识，从而构建一个多领域的图像-三平面数据集，推动通用4D头像生成器的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.19794', 'title': 'PAVE: Patching and Adapting Video Large Language Models', 'url': 'https://huggingface.co/papers/2503.19794', 'abstract': 'Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as "patches," which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE.', 'score': 1, 'issue_id': 3008, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'fefb309c48b93a29', 'authors': ['Zhuoming Liu', 'Yiquan Li', 'Khoi Duc Nguyen', 'Yiwu Zhong', 'Yin Li'], 'affiliations': ['The Chinese University of Hong Kong', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.19794.jpg', 'data': {'categories': ['#optimization', '#3d', '#transfer_learning', '#multimodal', '#reasoning', '#video', '#training'], 'emoji': '🎬', 'ru': {'title': 'PAVE: Универсальная адаптация видео-языковых моделей к новым задачам', 'desc': "PAVE - это гибкий фреймворк для адаптации предобученных видео-языковых моделей (Video LLMs) к задачам с дополнительными модальностями данных. Он вводит легковесные адаптеры, называемые 'патчами', которые добавляют небольшое количество параметров и операций к базовой модели без изменения ее архитектуры. PAVE эффективно адаптирует предобученную модель к различным задачам, включая аудиовизуальные вопросно-ответные системы, 3D-рассуждения и распознавание многоракурсного видео. Фреймворк значительно улучшает производительность базовой модели, превосходя специализированные модели при минимальном увеличении вычислительных затрат."}, 'en': {'title': 'PAVE: Adapting Video LLMs with Lightweight Patches', 'desc': 'This paper introduces PAVE, a new framework designed to adapt pre-trained video large language models (Video LLMs) for various tasks that require additional data types like audio or 3D information. PAVE utilizes lightweight adapters, called "patches," which integrate seamlessly into existing models without altering their original architecture or pre-trained weights. By doing so, it enhances the model\'s capabilities for tasks such as audio-visual question answering and multi-view video recognition while maintaining a low computational cost. The framework not only improves performance over state-of-the-art models but also supports multi-task learning and shows strong generalization across different Video LLMs.'}, 'zh': {'title': 'PAVE：灵活适应视频大语言模型的新框架', 'desc': '本文提出了一种名为PAVE的灵活框架，用于将预训练的视频大语言模型（Video LLMs）适应于新的任务，特别是涉及音频、3D信息或多视角视频等附加模态的数据。PAVE引入了轻量级的适配器，称为“补丁”，这些补丁在不改变基础模型架构或预训练权重的情况下，增加了少量参数和操作。通过这种方式，PAVE能够有效地将预训练的基础模型适应于多种下游任务，如音视频问答、3D推理和多视角视频识别等。实验结果表明，PAVE在这些任务上显著提升了基础模型的性能，超越了最先进的特定任务模型，同时仅增加约0.1%的计算量和参数。'}}}, {'id': 'https://huggingface.co/papers/2503.18225', 'title': 'Decoupling Angles and Strength in Low-rank Adaptation', 'url': 'https://huggingface.co/papers/2503.18225', 'abstract': 'Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.', 'score': 1, 'issue_id': 3001, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '36bcda1f90686965', 'authors': ['Massimo Bini', 'Leander Girrbach', 'Zeynep Akata'], 'affiliations': ['Helmholtz Munich', 'Technical University of Munich, Munich Center for Machine Learning', 'University of Tubingen, Tubingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2503.18225.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '🎛️', 'ru': {'title': 'DeLoRA: Устойчивая и эффективная тонкая настройка моделей машинного обучения', 'desc': 'DeLoRA - это новый метод тонкой настройки моделей машинного обучения, который нормализует и масштабирует обучаемые матрицы низкого ранга. Он отделяет угловое обучение от силы адаптации, повышая устойчивость без ущерба для производительности. DeLoRA показывает результаты на уровне или выше других методов эффективной настройки параметров (PEFT) в задачах генерации изображений, понимания естественного языка и инструктивной настройки. Метод особенно эффективен при выборе гиперпараметров и длительном обучении.'}, 'en': {'title': 'Decoupling Adaptation for Robust Fine-Tuning', 'desc': 'This paper introduces Decoupled Low-rank Adaptation (DeLoRA), a new method for fine-tuning large pretrained models efficiently. DeLoRA improves upon existing Parameter-Efficient FineTuning (PEFT) methods by normalizing and scaling low-rank matrices, which enhances robustness against hyperparameter variations. Unlike traditional methods like LoRA, which struggle with stability, DeLoRA decouples the learning angle from the adaptation strength, allowing for better performance across various tasks. The results demonstrate that DeLoRA not only matches but often exceeds the performance of other PEFT techniques in applications such as image generation and natural language understanding.'}, 'zh': {'title': '解耦低秩适应：提升微调鲁棒性的新方法', 'desc': '本文提出了一种新的微调方法，称为解耦低秩适应（DeLoRA），旨在提高参数高效微调（PEFT）方法的鲁棒性。DeLoRA通过规范化和缩放可学习的低秩矩阵，增强了模型在不同任务中的适应能力。与传统的微调方法相比，DeLoRA在保持性能的同时，能够更好地应对超参数选择和训练过程中的变化。通过在图像生成、自然语言理解和指令调优等任务上的评估，DeLoRA的表现与其他PEFT方法相当或更优，同时展现出更强的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.22677', 'title': 'DSO: Aligning 3D Generators with Simulation Feedback for Physical\n  Soundness', 'url': 'https://huggingface.co/papers/2503.22677', 'abstract': 'Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.', 'score': 0, 'issue_id': 3014, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '9a7faeb192777179', 'authors': ['Ruining Li', 'Chuanxia Zheng', 'Christian Rupprecht', 'Andrea Vedaldi'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.22677.jpg', 'data': {'categories': ['#3d', '#dataset', '#alignment', '#optimization', '#rlhf', '#training'], 'emoji': '🏗️', 'ru': {'title': 'Стабильные 3D-объекты: генерация с учетом физики без замедления', 'desc': 'Статья представляет новый подход к генерации стабильных 3D-объектов с помощью машинного обучения. Авторы предлагают метод Direct Simulation Optimization (DSO), который использует обратную связь от физического симулятора для улучшения генеративной модели. Они вводят новую целевую функцию Direct Reward Optimization (DRO) для настройки диффузионных моделей. Эксперименты показывают, что обученный генератор быстрее и эффективнее создает стабильные объекты по сравнению с оптимизацией во время вывода.'}, 'en': {'title': 'Generating Stable 3D Objects with Direct Simulation Optimization', 'desc': 'This paper introduces Direct Simulation Optimization (DSO), a new framework for generating stable 3D objects that can support themselves under gravity. Unlike previous methods that relied on slow and unstable differentiable physics simulators, DSO uses feedback from a non-differentiable simulator to enhance the stability of generated objects. The authors create a dataset of 3D objects with stability scores and employ direct preference optimization (DPO) and direct reward optimization (DRO) to fine-tune the generator. The results demonstrate that DSO significantly improves the speed and stability of 3D object generation without needing ground-truth training data.'}, 'zh': {'title': '直接模拟优化：生成稳定3D物体的新方法', 'desc': '本论文提出了一种新的3D物体生成框架，称为直接模拟优化（DSO），旨在生成符合物理约束的稳定3D物体。传统方法依赖于可微分物理模拟器进行几何优化，但速度慢且容易陷入局部最优。DSO框架利用来自非可微分模拟器的反馈，直接提高生成器输出稳定物体的可能性。通过构建带有稳定性评分的数据集，研究者可以使用直接偏好优化（DPO）或直接奖励优化（DRO）来微调生成器，从而实现更快且更稳定的3D物体生成。'}}}, {'id': 'https://huggingface.co/papers/2503.22668', 'title': 'Understanding Co-speech Gestures in-the-wild', 'url': 'https://huggingface.co/papers/2503.22668', 'abstract': "Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-text-speech associations: (i) gesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal speech-text-video-gesture representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs), across all three tasks. Further analysis reveals that speech and text modalities capture distinct gesture-related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal", 'score': 0, 'issue_id': 3007, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '77fc0ab156b46a61', 'authors': ['Sindhu B Hegde', 'K R Prajwal', 'Taein Kwon', 'Andrew Zisserman'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.22668.jpg', 'data': {'categories': ['#multimodal', '#benchmark'], 'emoji': '🤲', 'ru': {'title': 'Новый подход к пониманию жестов в речи: тримодальное обучение для улучшенного распознавания', 'desc': 'Статья представляет новую систему для понимания жестов, сопровождающих речь, в реальных условиях. Авторы предлагают три новые задачи и критерии оценки для проверки способности модели понимать связи между жестами, текстом и речью. Они разработали подход, который обучает тримодальное представление речи-текста-видео-жестов для решения этих задач. Их метод превосходит предыдущие подходы, включая крупные визуально-языковые модели, во всех трех задачах.'}, 'en': {'title': 'Unlocking Gesture Understanding with Tri-Modal Learning', 'desc': 'This paper presents a new framework for understanding co-speech gestures, which are important for non-verbal communication. It introduces three tasks to evaluate how well models can understand the relationships between gestures, text, and speech. The authors propose a tri-modal representation that combines speech, text, video, and gestures, using a unique loss function to improve learning from real-world videos. Their approach outperforms existing methods, including large vision-language models, highlighting the benefits of a shared embedding space for different modalities.'}, 'zh': {'title': '理解共语手势的新框架', 'desc': '本论文介绍了一种新的框架，用于理解自然环境中的共语手势。我们提出了三个新任务和基准，以评估模型理解手势、文本和语音之间关联的能力。通过结合全局短语对比损失和局部手势-词耦合损失，我们展示了如何在弱监督的情况下，从视频中学习强大的手势表示。我们的学习表示在所有三个任务中都超越了之前的方法，包括大型视觉-语言模型。'}}}, {'id': 'https://huggingface.co/papers/2504.00999', 'title': 'MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization', 'url': 'https://huggingface.co/papers/2504.00999', 'abstract': 'Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.', 'score': 56, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'bb6506ffd72aed19', 'authors': ['Siyuan Li', 'Luyuan Zhang', 'Zedong Wang', 'Juanxi Tian', 'Cheng Tan', 'Zicheng Liu', 'Chang Yu', 'Qingsong Xie', 'Haonan Lu', 'Haoqian Wang', 'Zhen Lei'], 'affiliations': ['CAIR, HKISI-CAS', 'MAIS CASIA', 'OPPO AI Center', 'The Hong Kong University of Science and Technology', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00999.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'MergeVQ: Эффективное объединение генерации изображений и обучения представлений', 'desc': 'Статья представляет новый метод MergeVQ, объединяющий маскированное моделирование изображений (MIM) с векторным квантованием (VQ) для улучшения генерации изображений и обучения представлений. MergeVQ использует технику слияния токенов в кодировщике для эффективного квантования и глобального выравнивания, а также кросс-внимание в декодере для восстановления деталей. Для генерации второго этапа предлагается MergeAR, выполняющий сжатие KV-кэша для эффективного предсказания в растровом порядке. Эксперименты на ImageNet показывают, что MergeVQ достигает конкурентоспособных результатов в задачах обучения визуальных представлений и генерации изображений, сохраняя высокую эффективность токенов и скорость вывода.'}, 'en': {'title': 'MergeVQ: Bridging Image Generation and Representation Learning Efficiently', 'desc': 'This paper introduces MergeVQ, a novel approach that enhances Masked Image Modeling (MIM) using Vector Quantization (VQ) techniques. It addresses the challenge of balancing image generation quality with efficient representation learning by integrating token merging into the VQ framework. During the pre-training phase, MergeVQ utilizes a token merge module to separate high-level semantics from the latent space, allowing for improved quantization and detail recovery. The second stage, MergeAR, optimizes the generation process through KV Cache compression, resulting in a model that excels in both visual representation and image generation while ensuring efficiency in token usage and inference speed.'}, 'zh': {'title': 'MergeVQ：提升图像生成与表示学习的统一模型', 'desc': '本文提出了一种新的模型MergeVQ，旨在改善基于向量量化的图像生成和视觉表示学习之间的平衡。通过在编码器中引入令牌合并技术，MergeVQ能够在自注意力块后解耦潜在空间中的语义，从而提高生成质量和表示学习的效率。该模型在预训练阶段通过交叉注意力恢复细节，并在生成阶段使用KV缓存压缩来提高预测效率。实验结果表明，MergeVQ在视觉表示学习和图像生成任务中表现出色，同时保持了良好的令牌效率和推理速度。'}}}, {'id': 'https://huggingface.co/papers/2504.00883', 'title': 'Improved Visual-Spatial Reasoning via R1-Zero-Like Training', 'url': 'https://huggingface.co/papers/2504.00883', 'abstract': 'Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.', 'score': 42, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '43fba84cc49adfad', 'authors': ['Zhenyi Liao', 'Qingsong Xie', 'Yanhao Zhang', 'Zijian Kong', 'Haonan Lu', 'Zhenyu Yang', 'Zhijie Deng'], 'affiliations': ['OPPO AI Center', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00883.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#video', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в визуально-пространственном мышлении мультимодальных ИИ', 'desc': 'Это исследование посвящено улучшению визуально-пространственного мышления мультимодальных больших языковых моделей (MLLM) с помощью обучения, подобного R1-Zero. Авторы обнаружили, что способности к визуально-пространственному мышлению у небольших и средних моделей Qwen2-VL не могут быть активированы с помощью подсказок цепочки рассуждений (CoT). Они применили обучение GRPO с использованием тщательно подобранного набора данных VSI-100k для улучшения визуально-пространственного мышления. В результате их модель vsGRPO-7B, дообученная на основе Qwen2-VL-7B, достигла производительности, сравнимой с лучшей моделью с открытым исходным кодом LLaVA-NeXT-Video-72B.'}, 'en': {'title': 'Enhancing Visual-Spatial Reasoning in MLLMs with GRPO Training', 'desc': 'This paper focuses on enhancing the visual-spatial reasoning abilities of multi-modal large language models (MLLMs), which are crucial for AI agents interacting with the physical world. The authors introduce a novel training method called GRPO, applied to the Qwen2-VL models, to improve their reasoning capabilities using a specially curated dataset named VSI-100k. They discover that traditional Chain of Thought prompts are ineffective for activating these reasoning skills in smaller models. The results show that their fine-tuned models significantly outperform baseline models, demonstrating the effectiveness of their approach in advancing visual-spatial intelligence in MLLMs.'}, 'zh': {'title': '提升多模态模型的视觉空间推理能力', 'desc': '本研究聚焦于提升多模态大型语言模型（MLLMs）的视觉空间推理能力，特别是在视频基础的视觉空间智能（VSI）方面。我们发现小到中型的Qwen2-VL模型无法通过思维链（CoT）提示激活其视觉空间推理能力，因此引入了GRPO训练方法，并使用精心策划的VSI-100k数据集进行改进。经过120个GPU小时的训练，我们的vsGRPO-2B模型在性能上超越了基础模型12.1%，并且vsGRPO-7B模型的表现与最佳开源模型LLaVA-NeXT-Video-72B相当。我们的研究表明，保持KL惩罚在GRPO中是必要的，并且vsGRPO在与监督微调和直接偏好优化基线的比较中表现出明显的优势。'}}}, {'id': 'https://huggingface.co/papers/2504.01014', 'title': 'AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction', 'url': 'https://huggingface.co/papers/2504.01014', 'abstract': 'Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.', 'score': 29, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '98efa783105c3173', 'authors': ['Junhao Cheng', 'Yuying Ge', 'Yixiao Ge', 'Jing Liao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01014.jpg', 'data': {'categories': ['#diffusion', '#games', '#multimodal', '#video'], 'emoji': '🎮', 'ru': {'title': 'AnimeGamer: погружение в интерактивный мир аниме через языковые инструкции', 'desc': 'AnimeGamer - это новый подход к созданию интерактивных игр с персонажами аниме, использующий мультимодальные языковые модели (MLLM). Система генерирует динамические анимационные кадры, отображающие движения персонажей и изменения их состояний, на основе текстовых диалогов. AnimeGamer вводит новые мультимодальные представления с учетом действий, которые декодируются в видеоклипы высокого качества с помощью диффузионной модели. Метод превосходит существующие подходы по различным аспектам игрового опыта, что подтверждается автоматическими метриками и оценками людей.'}, 'en': {'title': 'Transforming Anime into Interactive Gaming with Dynamic AI', 'desc': 'This paper presents AnimeGamer, a novel approach to infinite anime life simulation games that utilizes Multimodal Large Language Models (MLLMs). Unlike previous methods, AnimeGamer incorporates historical visual context to ensure consistent gameplay and generates dynamic animations rather than just static images. By employing action-aware multimodal representations, it can create high-quality video clips that reflect character movements and state changes. The results show that AnimeGamer significantly enhances the gaming experience compared to existing techniques, as validated by both automated and human evaluations.'}, 'zh': {'title': 'AnimeGamer：动态互动的无限动漫游戏体验', 'desc': '最近在图像和视频合成方面的进展为生成游戏带来了新的希望。本文提出了一种名为AnimeGamer的方法，利用多模态大语言模型生成动态动画镜头，以增强游戏的互动性和沉浸感。通过引入动作感知的多模态表示，AnimeGamer能够生成具有上下文一致性和动态变化的游戏状态。实验结果表明，AnimeGamer在游戏体验的各个方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2504.01724', 'title': 'DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance', 'url': 'https://huggingface.co/papers/2504.01724', 'abstract': 'While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.', 'score': 24, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'd59102a274145730', 'authors': ['Yuxuan Luo', 'Zhengkun Rong', 'Lizhen Wang', 'Longhao Zhang', 'Tianshu Hu', 'Yongming Zhu'], 'affiliations': ['Bytedance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2504.01724.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#3d', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'Реалистичная анимация человека с точным контролем мимики и движений', 'desc': 'DreamActor-M1 - это новая модель на основе диффузионного трансформера для анимации человека по изображениям. Она использует гибридные сигналы управления, включающие неявные лицевые представления, 3D-сферы головы и 3D-скелеты тела для точного контроля мимики и движений. Модель применяет прогрессивное обучение на данных разного масштаба для адаптации к различным позам и ракурсам. DreamActor-M1 также интегрирует паттерны движения из последовательных кадров с визуальными ориентирами для обеспечения долгосрочной согласованности анимации.'}, 'en': {'title': 'DreamActor-M1: Revolutionizing Human Animation with Robust Control and Consistency', 'desc': 'The paper presents DreamActor-M1, a novel framework that utilizes a diffusion transformer (DiT) to enhance human animation by addressing key limitations in existing methods. It introduces hybrid control signals that combine facial representations, 3D head spheres, and body skeletons to improve the expressiveness and control of animations. The framework also employs a progressive training strategy to adapt to various body poses and image scales, ensuring versatility in generating animations from portraits to full-body views. Additionally, it integrates motion patterns from sequential frames to maintain long-term temporal coherence, resulting in more robust and visually appealing animations.'}, 'zh': {'title': '突破动画生成的局限性，DreamActor-M1引领新潮流！', 'desc': '本文提出了一种基于扩散变换器的框架DreamActor-M1，旨在解决现有图像基础的人体动画方法在细粒度整体可控性、多尺度适应性和长期时间一致性方面的不足。通过混合控制信号，结合隐式面部表示、3D头部球体和3D身体骨架，实现了对面部表情和身体动作的强大控制，同时保持动画的表现力和身份一致性。为了适应不同的身体姿势和图像尺度，采用了渐进训练策略，使用不同分辨率和尺度的数据进行训练。实验结果表明，该方法在肖像、上半身和全身生成方面优于现有的最先进技术，具有强大的长期一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.20783', 'title': 'Understanding R1-Zero-Like Training: A Critical Perspective', 'url': 'https://huggingface.co/papers/2503.20783', 'abstract': "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.", 'score': 24, 'issue_id': 3044, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'c5971e424bc52a6b', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение способностей рассуждения языковых моделей с помощью обучения с подкреплением', 'desc': 'Исследование показало, что обучение с подкреплением (RL) в больших масштабах может напрямую улучшить способности рассуждения языковых моделей (LLM) без использования обучения с учителем. Авторы проанализировали различные базовые модели и выявили, что некоторые из них уже демонстрируют сильные способности к рассуждению без специальных шаблонов. Был обнаружен оптимизационный сдвиг в методе Group Relative Policy Optimization (GRPO), который искусственно увеличивает длину ответов. Для решения этой проблемы предложен новый метод Dr. GRPO, который улучшает эффективность токенов при сохранении производительности рассуждений.'}, 'en': {'title': 'Enhancing LLM Reasoning with Unbiased RL Optimization', 'desc': 'This paper explores how reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) without needing supervised fine-tuning. It examines the impact of different base models, particularly DeepSeek-V3-Base and Qwen2.5, on RL performance, revealing that pretraining characteristics can lead to inherent biases in reasoning capabilities. The authors also identify an optimization bias in the Group Relative Policy Optimization (GRPO) method, which can inflate response lengths during training. To counter this, they propose Dr. GRPO, a new optimization technique that enhances token efficiency while preserving reasoning accuracy, achieving a notable 43.3% accuracy on the AIME 2024 benchmark with a 7B base model.'}, 'zh': {'title': '强化学习提升推理能力的新突破', 'desc': 'DeepSeek-R1-Zero展示了大规模强化学习（RL）可以直接增强大型语言模型（LLMs）的推理能力，而无需监督微调。本文深入分析了R1-Zero训练的两个核心组成部分：基础模型和强化学习。我们研究了多种基础模型，包括DeepSeek-V3-Base，以了解预训练特性如何影响RL性能。我们的分析发现，DeepSeek-V3-Base已经展现出“恍然大悟”的时刻，而Qwen2.5基础模型即使在没有提示模板的情况下也表现出强大的推理能力，暗示了潜在的预训练偏差。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2504.01956', 'title': 'VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step', 'url': 'https://huggingface.co/papers/2504.01956', 'abstract': 'Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene', 'score': 22, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '44f1db8ef8cc244a', 'authors': ['Hanyang Wang', 'Fangfu Liu', 'Jiawei Chi', 'Yueqi Duan'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01956.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d'], 'emoji': '🎬', 'ru': {'title': 'VideoScene: Эффективная генерация 3D сцен из видео с помощью дистилляции диффузионных моделей', 'desc': 'Статья представляет VideoScene - метод для эффективной генерации трехмерных сцен из разреженных видов. Авторы предлагают стратегию дистилляции 3D-aware leap flow для преодоления избыточной информации и обучения динамической политики шумоподавления. VideoScene достигает более быстрых и качественных результатов генерации 3D сцен по сравнению с предыдущими моделями диффузии видео. Этот подход открывает новые возможности для приложений преобразования видео в 3D.'}, 'en': {'title': 'Efficient 3D Scene Generation from Sparse Views Using Video Diffusion Models', 'desc': 'This paper addresses the challenge of reconstructing 3D scenes from sparse views, which is often complicated by the lack of visual information. Traditional methods have limitations, particularly when input views have minimal overlap, leading to degraded performance. The authors introduce VideoScene, a novel approach that utilizes video diffusion models to efficiently generate 3D scenes in a single step. By implementing a 3D-aware leap flow distillation strategy and a dynamic denoising policy network, VideoScene significantly improves the speed and quality of 3D scene generation compared to existing methods.'}, 'zh': {'title': '高效生成3D场景的VideoScene', 'desc': '从稀疏视图恢复3D场景是一项具有挑战性的任务，因为它本质上是一个不适定的问题。传统方法通过几何正则化或前馈确定性模型等专门解决方案来缓解这一问题，但在输入视图重叠较少且视觉信息不足时，性能仍然下降。最近的视频生成模型显示出解决这一挑战的潜力，能够生成具有合理3D结构的视频片段。本文提出了VideoScene，通过视频扩散模型提炼生成3D场景，设计了3D感知的跃迁流蒸馏策略，以提高生成效率和效果。'}}}, {'id': 'https://huggingface.co/papers/2503.23368', 'title': 'Towards Physically Plausible Video Generation via VLM Planning', 'url': 'https://huggingface.co/papers/2503.23368', 'abstract': 'Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.', 'score': 21, 'issue_id': 3050, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'f3087a720104ea83', 'authors': ['Xindi Yang', 'Baolu Li', 'Yiming Zhang', 'Zhenfei Yin', 'Lei Bai', 'Liqian Ma', 'Zhiyong Wang', 'Jianfei Cai', 'Tien-Tsin Wong', 'Huchuan Lu', 'Xu Jia'], 'affiliations': ['Dalian University of Technology', 'Monash University', 'Oxford University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Sydney', 'ZMO AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.23368.jpg', 'data': {'categories': ['#reasoning', '#diffusion', '#video', '#architecture', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Физически достоверная генерация видео с помощью ИИ', 'desc': 'Эта статья представляет новый двухэтапный подход к генерации видео с учетом физических законов. На первом этапе используется Визуально-Языковая Модель (VLM) для планирования грубых траекторий движения с учетом физики. На втором этапе эти траектории используются для управления Моделью Диффузии Видео (VDM) при генерации конечного результата. Добавление шума во время вывода позволяет VDM генерировать более детальное движение. Эксперименты показывают, что этот метод превосходит существующие подходы в создании физически правдоподобных видео.'}, 'en': {'title': 'Bringing Physics to Video Generation: A Two-Stage Approach', 'desc': 'This paper introduces a new two-stage framework for generating videos that are more physically realistic using video diffusion models (VDMs). The first stage uses a Vision Language Model (VLM) to create rough motion trajectories that consider real-world physics, ensuring that the generated video maintains consistency between frames. In the second stage, these trajectories guide the VDM in producing detailed video content, with added noise to allow for creative freedom in motion generation. The results show that this approach significantly improves the physical plausibility of the generated videos compared to existing methods.'}, 'zh': {'title': '引入物理知识的视频生成新框架', 'desc': '视频扩散模型（VDMs）在生成高度真实的视频方面取得了显著进展，但它们常常缺乏对物理的理解，导致生成的视频在动态和事件序列上不够合理。为了解决这个问题，我们提出了一种新颖的两阶段图像到视频生成框架，明确地融入了物理知识。在第一阶段，我们使用视觉语言模型（VLM）作为粗略的运动规划器，结合思维链和物理感知推理，预测接近真实物理动态的粗略运动轨迹。第二阶段则利用预测的运动轨迹来指导VDM的视频生成，从而实现更细致的运动表现。'}}}, {'id': 'https://huggingface.co/papers/2504.01848', 'title': "PaperBench: Evaluating AI's Ability to Replicate AI Research", 'url': 'https://huggingface.co/papers/2504.01848', 'abstract': "We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.", 'score': 18, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '60923777325e85cc', 'authors': ['Giulio Starace', 'Oliver Jaffe', 'Dane Sherburn', 'James Aung', 'Jun Shern Chan', 'Leon Maksin', 'Rachel Dias', 'Evan Mays', 'Benjamin Kinsella', 'Wyatt Thompson', 'Johannes Heidecke', 'Amelia Glaese', 'Tejal Patwardhan'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2504.01848.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#survey'], 'emoji': '🧠', 'ru': {'title': 'PaperBench: измеряем способность ИИ воспроизводить передовые исследования', 'desc': 'PaperBench - это новый бенчмарк для оценки способности ИИ-агентов воспроизводить современные исследования в области искусственного интеллекта. Он включает в себя 20 статей из ICML 2024, которые агенты должны воспроизвести с нуля, включая понимание вклада статьи, разработку кодовой базы и успешное выполнение экспериментов. Для объективной оценки разработаны рубрики, которые иерархически разбивают каждую задачу на подзадачи с четкими критериями оценки. Лучший протестированный агент, Claude 3.5 Sonnet (New) с открытым исходным кодом, достиг среднего балла воспроизведения 21.0%.'}, 'en': {'title': "Evaluating AI's Research Replication Skills with PaperBench", 'desc': "This paper presents PaperBench, a benchmark designed to assess AI agents' ability to replicate advanced AI research. The benchmark includes 20 selected papers from ICML 2024, requiring agents to comprehend contributions, create a codebase, and conduct experiments. To ensure objective evaluation, the authors developed detailed rubrics that break down replication tasks into smaller, graded components, totaling 8,316 tasks. The study also introduces an LLM-based judge for automated grading and compares the performance of AI agents against human experts, revealing that current models still lag behind human capabilities."}, 'zh': {'title': 'PaperBench：评估AI复制研究能力的基准', 'desc': '我们介绍了PaperBench，这是一个评估人工智能代理复制最先进AI研究能力的基准。代理需要从头开始复制20篇ICML 2024的亮点和口头论文，包括理解论文贡献、开发代码库和成功执行实验。为了进行客观评估，我们开发了分层的评分标准，将每个复制任务分解为更小的子任务，并设定明确的评分标准。我们的评估还包括使用基于大型语言模型的评审者自动评分，并与顶尖的机器学习博士进行比较，发现目前的模型尚未超越人类基线。'}}}, {'id': 'https://huggingface.co/papers/2504.00824', 'title': 'ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations', 'url': 'https://huggingface.co/papers/2504.00824', 'abstract': "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.", 'score': 18, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'b135f3f003dcaaff', 'authors': ['Yubo Wang', 'Xueguang Ma', 'Ping Nie', 'Huaye Zeng', 'Zhiheng Lyu', 'Yuxuan Zhang', 'Benjamin Schneider', 'Yi Lu', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.00824.jpg', 'data': {'categories': ['#science', '#dataset', '#rag', '#multimodal', '#alignment'], 'emoji': '🎓', 'ru': {'title': 'ScholarCopilot: ИИ-помощник для профессионального академического письма', 'desc': 'ScholarCopilot - это унифицированная система, улучшающая существующие большие языковые модели для генерации профессиональных академических статей с точными и контекстуально релевантными цитатами. Она динамически определяет, когда извлекать научные ссылки, генерируя токен [RET], и использует его представление для поиска релевантных цитат в базе данных. ScholarCopilot совместно оптимизирует задачи генерации и цитирования в рамках единой системы для повышения эффективности. Обученная на 500 тысячах статей из arXiv, модель достигает точности извлечения top-1 в 40.1% на оценочном наборе данных, превосходя базовые модели.'}, 'en': {'title': 'Enhancing Academic Writing with ScholarCopilot', 'desc': 'This paper presents ScholarCopilot, a new framework that improves large language models for generating academic articles with accurate citations. It uses a retrieval token to decide when to fetch scholarly references, enhancing the text generation process with relevant citations. The model is trained on a large dataset of academic papers and shows significant improvements in both citation accuracy and writing quality compared to existing models. Human evaluations further validate its effectiveness in citation recall and overall user experience.'}, 'zh': {'title': 'ScholarCopilot：提升学术写作的智能助手', 'desc': '本研究提出了ScholarCopilot，一个统一框架，旨在提升大型语言模型在生成专业学术文章时的准确性和相关性。该系统通过生成检索标记[RET]，动态决定何时检索学术参考文献，并利用其表示从数据库中查找相关引用。ScholarCopilot在生成和引用任务上进行联合优化，以提高效率。经过在500K篇arXiv论文上的训练，该模型在评估数据集上实现了40.1%的顶级检索准确率，且在学术写作样本的生成质量上超越了参数更多的模型。'}}}, {'id': 'https://huggingface.co/papers/2504.01934', 'title': 'ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement', 'url': 'https://huggingface.co/papers/2504.01934', 'abstract': 'We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.', 'score': 16, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'a50e19f04d94405f', 'authors': ['Runhui Huang', 'Chunwei Wang', 'Junwei Yang', 'Guansong Lu', 'Yunlong Yuan', 'Jianhua Han', 'Lu Hou', 'Wei Zhang', 'Lanqing Hong', 'Hengshuang Zhao', 'Hang Xu'], 'affiliations': ['Huawei Noahs Ark Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01934.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#training', '#games', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ILLUME+: Унифицированная мультимодальная модель нового поколения', 'desc': 'ILLUME+ - это новая мультимодальная языковая модель, использующая двойную визуальную токенизацию и диффузионный декодер. Она улучшает глубокое семантическое понимание и высококачественную генерацию изображений. ILLUME+ вводит унифицированный двойной визуальный токенизатор DualViTok, сохраняющий как мелкие текстуры, так и семантику, согласованную с текстом. Модель демонстрирует конкурентоспособную производительность в задачах мультимодального понимания, генерации и редактирования по сравнению с существующими унифицированными MLLM и специализированными моделями.'}, 'en': {'title': 'ILLUME+: Unifying Understanding, Generation, and Editing in One Model', 'desc': 'ILLUME+ is a novel model that combines dual visual tokenization and a diffusion decoder to enhance deep semantic understanding and high-quality image generation. Unlike previous models that struggled with understanding, generation, and editing simultaneously, ILLUME+ effectively integrates these capabilities through its innovative DualViTok tokenizer. This approach preserves fine textures and aligns semantics, allowing for better image editing and generation. With a continuous-input, discrete-output framework and progressive training, ILLUME+ achieves competitive performance in multimodal tasks, paving the way for future applications.'}, 'zh': {'title': 'ILLUME+: 多模态理解与生成的新突破', 'desc': 'ILLUME+ 是一种新型的多模态学习模型，结合了双重视觉标记和扩散解码器，旨在提升深层语义理解和高保真图像生成能力。与现有的统一模型相比，ILLUME+ 能够更好地处理理解、生成和编辑这三种基本能力。通过引入 DualViTok，ILLUME+ 保留了细致的纹理和文本对齐的语义，同时采用粗到细的图像表示策略，增强了多模态理解和生成的能力。此外，ILLUME+ 在图像生成质量和超分辨率方面表现出色，展现了与现有模型的竞争力，为未来的多模态应用奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2504.01204', 'title': 'Articulated Kinematics Distillation from Video Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01204', 'abstract': 'We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/', 'score': 12, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'c8630e7ca691cef3', 'authors': ['Xuan Li', 'Qianli Ma', 'Tsung-Yi Lin', 'Yongxin Chen', 'Chenfanfu Jiang', 'Ming-Yu Liu', 'Donglai Xiang'], 'affiliations': ['NVIDIA', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2504.01204.jpg', 'data': {'categories': ['#diffusion', '#games', '#3d'], 'emoji': '🦾', 'ru': {'title': 'Революция в анимации персонажей: от скелета к реалистичному движению', 'desc': 'Articulated Kinematics Distillation (AKD) - это новый подход к генерации высококачественной анимации персонажей, объединяющий скелетную анимацию и современные генеративные модели. AKD использует скелетное представление для 3D-моделей, значительно уменьшая количество степеней свободы за счет фокуса на управлении суставами. Метод применяет Score Distillation Sampling с предобученными видео-диффузионными моделями для синтеза сложных движений, сохраняя структурную целостность. AKD показывает превосходные результаты по сравнению с существующими методами генерации текста в 4D анимацию.'}, 'en': {'title': 'Revolutionizing Character Animation with AKD', 'desc': 'Articulated Kinematics Distillation (AKD) is a new framework designed to create realistic character animations by combining skeleton-based animation techniques with advanced generative models. It simplifies the animation process by using a skeleton representation, which reduces the Degrees of Freedom (DoFs) and allows for better control over joint movements. AKD employs Score Distillation Sampling (SDS) with pre-trained video diffusion models to generate complex motions while ensuring that the shapes of the characters remain consistent. This method also integrates well with physics-based simulations, resulting in animations that are not only visually appealing but also physically plausible.'}, 'zh': {'title': '关节运动蒸馏：高保真动画的新方法', 'desc': '本文提出了一种名为关节运动蒸馏（AKD）的框架，用于生成高保真角色动画。AKD通过使用基于骨骼的表示，显著减少了自由度，专注于关节级控制，从而实现高效且一致的运动合成。通过与预训练的视频扩散模型结合的得分蒸馏采样（SDS），AKD能够蒸馏复杂的关节运动，同时保持结构完整性，克服了4D神经变形场在保持形状一致性方面的挑战。实验表明，AKD在3D一致性和运动质量上优于现有的文本到4D生成方法。'}}}, {'id': 'https://huggingface.co/papers/2504.01308', 'title': 'Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks', 'url': 'https://huggingface.co/papers/2504.01308', 'abstract': 'Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.', 'score': 11, 'issue_id': 3040, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '315938d70f25095e', 'authors': ['Jiawei Wang', 'Yushen Zuo', 'Yuanjun Chai', 'Zhendong Liu', 'Yichen Fu', 'Yichun Feng', 'Kin-man Lam'], 'affiliations': ['Nanjing University', 'Stanford University', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China', 'University of Washington', 'University of the Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.01308.jpg', 'data': {'categories': ['#security', '#cv', '#diffusion', '#training', '#dataset', '#optimization', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Защита визуально-языковых моделей от атак с помощью шумовой аугментации', 'desc': 'Исследователи обнаружили уязвимость визуально-языковых моделей (VLM) к атакам с использованием зашумленных или искаженных изображений. Для решения этой проблемы они предложили метод Robust-VLGuard, включающий набор данных для обучения и дообучение с аугментацией шумом. Также был разработан метод DiffPure-VLM, использующий диффузионные модели для преобразования состязательных возмущений в гауссовский шум. Эксперименты показали, что предложенные методы значительно повышают устойчивость VLM к различным типам атак.'}, 'en': {'title': 'Strengthening Vision-Language Models Against Noise Attacks', 'desc': 'This paper addresses the vulnerabilities of Vision-Language Models (VLMs) to jailbreak attacks, particularly when they encounter noisy or corrupted images. The authors highlight that existing security measures during training do not adequately account for noise-augmented visual inputs, leading to significant security gaps. To combat this issue, they introduce Robust-VLGuard, a dataset designed for multimodal safety that includes both aligned and misaligned image-text pairs, along with a noise-augmented fine-tuning process. Additionally, they propose DiffPure-VLM, which uses diffusion models to transform adversarial perturbations into Gaussian-like noise, enhancing the robustness of VLMs against such attacks while maintaining their functionality.'}, 'zh': {'title': '增强视觉语言模型的安全性', 'desc': '视觉语言模型（VLMs）通过结合视觉信息扩展了大型语言模型（LLMs）的能力，但在处理噪声或损坏的图像时仍然容易受到攻击。现有的VLMs在训练过程中采取了安全措施以减轻这些攻击，但对噪声增强视觉输入的脆弱性却未给予足够重视。我们提出了Robust-VLGuard，这是一个多模态安全数据集，结合了对齐和不对齐的图像-文本对，并通过噪声增强的微调来降低攻击成功率，同时保持VLM的功能。实验结果表明，扩散模型的分布转移特性与我们微调的VLMs很好地对齐，显著减轻了不同强度的对抗扰动。'}}}, {'id': 'https://huggingface.co/papers/2405.20216', 'title': 'Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback', 'url': 'https://huggingface.co/papers/2405.20216', 'abstract': 'The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.', 'score': 10, 'issue_id': 3046, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8ffe2bddf2c0ee58', 'authors': ['Sanghyeon Na', 'Yonggyu Kim', 'Hyunjoon Lee'], 'affiliations': ['Kakao'], 'pdf_title_img': 'assets/pdf/title_img/2405.20216.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#optimization', '#training', '#diffusion', '#cv'], 'emoji': '🧑\u200d🎨', 'ru': {'title': 'Революция в генерации изображений людей с помощью DPO', 'desc': 'Эта статья представляет новый подход к генерации изображений людей с использованием метода Direct Preference Optimization (DPO). Авторы предлагают эффективный способ создания специализированного набора данных DPO для обучения моделей без необходимости дорогостоящей обратной связи от людей. Они также вводят модифицированную функцию потерь, которая улучшает процесс обучения DPO, минимизируя артефакты и повышая точность изображений. Результаты показывают значительное улучшение в генерации изображений людей с естественной анатомией, позами и соответствием текстовым запросам.'}, 'en': {'title': 'Revolutionizing Human Image Generation with Direct Preference Optimization', 'desc': 'This paper addresses the challenges of generating high-quality human images from text descriptions using text-to-image (T2I) methods. It highlights the importance of meeting specific criteria such as human pose and anatomy, which are crucial for realistic image synthesis. The authors propose a novel approach that employs Direct Preference Optimization (DPO) to create a specialized dataset for training without requiring expensive human feedback. Additionally, they introduce a modified loss function that reduces artifacts and enhances image quality, leading to significant improvements in generating human images that align well with textual prompts.'}, 'zh': {'title': '优化人类图像生成的新方法', 'desc': '本文探讨了通过文本到图像（T2I）方法生成高质量人类图像的挑战。与一般图像生成不同，人类图像合成需要满足严格的人体姿势、解剖结构和与文本提示对齐的标准。我们提出了一种新颖的方法，利用直接偏好优化（DPO）专门针对人类图像生成，构建高效的DPO数据集以训练模型，减少对昂贵人类反馈的依赖。通过修改损失函数，我们的训练过程能够减少伪影并提高图像的真实感，显著提升人类图像生成的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.23573', 'title': 'DASH: Detection and Assessment of Systematic Hallucinations of VLMs', 'url': 'https://huggingface.co/papers/2503.23573', 'abstract': "Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH.", 'score': 9, 'issue_id': 3049, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'cd9c0f9c9392d470', 'authors': ['Maximilian Augustin', 'Yannic Neuhaus', 'Matthias Hein'], 'affiliations': ['Tubingen AI Center, University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2503.23573.jpg', 'data': {'categories': ['#cv', '#hallucinations', '#benchmark', '#transfer_learning', '#training', '#dataset', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'DASH: Автоматическое обнаружение систематических ошибок в мультимодальных моделях', 'desc': 'Эта статья представляет DASH - новый метод для обнаружения и оценки систематических галлюцинаций в мультимодальных моделях компьютерного зрения и языка. DASH использует оптимизацию на многообразии естественных изображений для поиска случаев, когда модель ошибочно определяет наличие объектов. Авторы применили DASH к нескольким современным моделям и обнаружили тысячи кластеров изображений, вызывающих галлюцинации. Исследование показало, что дообучение модели на найденных проблемных изображениях помогает уменьшить количество галлюцинаций.'}, 'en': {'title': 'DASH: Detecting and Mitigating Object Hallucinations in Vision-Language Models', 'desc': "This paper addresses the issue of object hallucinations in vision-language models (VLMs), where these models incorrectly identify objects in images. The authors introduce DASH, a new automated pipeline that detects and assesses systematic hallucinations in VLMs using large-scale, real-world image datasets. DASH includes a component called DASH-OPT, which generates misleading images to evaluate the VLM's performance on the 'natural image manifold'. The study demonstrates that fine-tuning VLMs with images identified by DASH can significantly reduce the occurrence of object hallucinations."}, 'zh': {'title': '揭示视觉语言模型的系统幻觉', 'desc': '视觉语言模型（VLMs）容易出现物体幻觉，即错误地指示图像中存在某些物体。现有的基准测试使用相对较小的标记数据集来量化幻觉，但这种方法不足以评估在开放世界环境中出现的幻觉。我们提出了DASH（系统幻觉的检测与评估），这是一个自动化的大规模管道，旨在识别VLMs在真实图像中的系统幻觉。通过优化“自然图像流形”，DASH能够生成误导VLM的图像，并识别出大量的真实和语义相似图像的聚类。'}}}, {'id': 'https://huggingface.co/papers/2503.23135', 'title': 'LSNet: See Large, Focus Small', 'url': 'https://huggingface.co/papers/2503.23135', 'abstract': "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.", 'score': 4, 'issue_id': 3040, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': 'd2ac65a2356c89c3', 'authors': ['Ao Wang', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['BNRist, Tsinghua University', 'Department of Automation, Tsinghua University', 'School of Software, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23135.jpg', 'data': {'categories': ['#architecture', '#training', '#cv'], 'emoji': '👁️', 'ru': {'title': "LSNet: Эффективное компьютерное зрение по принципу 'Смотри широко, фокусируйся узко'", 'desc': "Статья представляет новый подход к проектированию легковесных нейронных сетей для компьютерного зрения, вдохновленный человеческой системой зрения. Авторы предлагают стратегию 'Смотри широко, фокусируйся узко' и вводят LS-свертку, сочетающую восприятие с большим ядром и агрегацию с малым ядром. На основе LS-свертки разработано семейство моделей LSNet, которое эффективно обрабатывает визуальную информацию. Эксперименты показывают, что LSNet превосходит существующие легковесные сети по производительности и эффективности в различных задачах компьютерного зрения."}, 'en': {'title': 'See Large, Focus Small: Efficient Vision Networks', 'desc': "This paper addresses the challenges of deploying complex vision networks like Convolutional Neural Networks and Vision Transformers in real-time applications due to their heavy computations. The authors propose a new lightweight network design strategy called 'See Large, Focus Small', inspired by the human vision system's ability to dynamically adjust focus. They introduce LS convolution, which effectively combines large-kernel perception for broad information capture and small-kernel aggregation for precise feature refinement. The resulting LSNet model demonstrates improved performance and efficiency in various vision tasks compared to existing lightweight networks."}, 'zh': {'title': '轻量级视觉网络的新策略：大视野，小聚焦', 'desc': '本文提出了一种新的轻量级视觉网络设计策略，称为“See Large, Focus Small”。该策略结合了大核感知和小核聚合的LS卷积，能够高效捕捉广泛的感知信息并实现精确的特征聚合。通过这种方法，LSNet在多种视觉任务中展现出优越的性能和效率，克服了现有轻量级模型在计算预算有限情况下的局限性。实验结果表明，LSNet在实时应用中表现出色，适合实际部署。'}}}, {'id': 'https://huggingface.co/papers/2504.00406', 'title': 'VerifiAgent: a Unified Verification Agent in Language Model Reasoning', 'url': 'https://huggingface.co/papers/2504.00406', 'abstract': 'Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent', 'score': 2, 'issue_id': 3046, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'd4ef7ad5ea6d5aad', 'authors': ['Jiuzhou Han', 'Wray Buntine', 'Ehsan Shareghi'], 'affiliations': ['College of Engineering and Computer Science, VinUniversity', 'Department of Data Science & AI, Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00406.jpg', 'data': {'categories': ['#interpretability', '#training', '#math', '#agents', '#reasoning', '#inference'], 'emoji': '🔍', 'ru': {'title': 'VerifiAgent: умный верификатор для надежных ответов языковых моделей', 'desc': 'Статья представляет VerifiAgent - унифицированного агента для верификации ответов больших языковых моделей. VerifiAgent использует двухуровневый подход: мета-верификацию для оценки полноты и согласованности ответов, и адаптивную верификацию на основе инструментов для различных типов рассуждений. Экспериментальные результаты показывают превосходство VerifiAgent над базовыми методами верификации во всех задачах рассуждения. Агент также демонстрирует эффективность в масштабировании вывода, достигая лучших результатов с меньшими затратами по сравнению с существующими моделями вознаграждения процесса в области математических рассуждений.'}, 'en': {'title': 'VerifiAgent: Enhancing Reliability in Language Model Reasoning', 'desc': "This paper introduces VerifiAgent, a novel verification system designed to improve the reliability of large language models' responses. It combines two verification levels: meta-verification for checking the completeness and consistency of answers, and tool-based adaptive verification that selects the best verification tools based on the reasoning type. This approach enhances efficiency and robustness across various reasoning tasks, outperforming traditional verification methods. Additionally, VerifiAgent improves reasoning accuracy by utilizing feedback from its verification processes and is more cost-effective in mathematical reasoning applications."}, 'zh': {'title': 'VerifiAgent：智能验证，提升推理准确性', 'desc': '大型语言模型展现了出色的推理能力，但常常产生不可靠或错误的回答。现有的验证方法通常是针对特定模型或领域，计算资源消耗大，且在不同推理任务中缺乏可扩展性。为了解决这些问题，我们提出了VerifiAgent，一个统一的验证代理，集成了两级验证：元验证评估模型回答的完整性和一致性，工具自适应验证则根据推理类型自动选择合适的验证工具。实验结果表明，VerifiAgent在所有推理任务中优于基线验证方法，并能通过利用验证结果的反馈进一步提高推理准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.22879', 'title': 'Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models', 'url': 'https://huggingface.co/papers/2503.22879', 'abstract': 'State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input x, combined with a per-state-group quantization for input-dependent parameters B and C. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3times and 3times speed-ups in the pre-filling and generation stages, respectively, while offering 4times memory reduction with only a 1.6% average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.', 'score': 2, 'issue_id': 3059, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': 'dbc6e89f0857b2e7', 'authors': ['Hung-Yueh Chiang', 'Chi-Chih Chang', 'Natalia Frumkin', 'Kai-Chiang Wu', 'Mohamed S. Abdelfattah', 'Diana Marculescu'], 'affiliations': ['Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin', 'Department of Computer Science, National Yang Ming Chiao Tung University', 'Department of Electrical and Computer Engineering, Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22879.jpg', 'data': {'categories': ['#open_source', '#inference', '#training', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная квантизация моделей пространства состояний для ускорения и экономии памяти', 'desc': 'Статья представляет Quamba2 - метод квантизации для моделей пространства состояний (SSM), позволяющий эффективно использовать различные конфигурации разрядности (W8A8, W4A8, W4A16) для моделей Mamba1 и Mamba2. Авторы предлагают офлайн-подход к 8-битной квантизации входных данных линейной рекурсии, основанный на сортировке и кластеризации. Эксперименты показывают, что Quamba2-8B превосходит современные методы квантизации SSM, обеспечивая ускорение в 1.3 и 3 раза на этапах предварительного заполнения и генерации соответственно. Оценка на наборе данных MMLU демонстрирует обобщаемость и надежность предложенного фреймворка.'}, 'en': {'title': 'Quamba2: Efficient Quantization for State Space Models', 'desc': 'This paper introduces Quamba2, a method for quantizing State Space Models (SSMs) to improve their efficiency on various platforms. By using low bit-width data formats, Quamba2 reduces the model size and enhances performance without significant accuracy loss. The approach involves sorting and clustering inputs for better quantization of parameters, ensuring that the output remains stable despite the changes. Experimental results demonstrate that Quamba2 achieves significant speed-ups and memory reductions compared to existing SSM quantization techniques.'}, 'zh': {'title': '量化状态空间模型，提升性能与效率！', 'desc': '状态空间模型（SSMs）作为一种新兴的替代方案，因其一致的内存使用和高性能而受到关注。然而，在云服务或资源有限的设备上扩展SSMs面临存储和计算能力的挑战。为了解决这个问题，使用低位宽数据格式对SSMs进行量化可以减少模型大小，并利用硬件加速。我们提出的Quamba2能够支持多种位宽配置，优化了SSMs在不同场景下的性能，同时在保持较高准确率的情况下显著提高了计算速度和内存效率。'}}}, {'id': 'https://huggingface.co/papers/2504.01201', 'title': 'Medical large language models are easily distracted', 'url': 'https://huggingface.co/papers/2504.01201', 'abstract': "Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information.", 'score': 1, 'issue_id': 3052, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'cfc7db001feb908d', 'authors': ['Krithik Vishwanath', 'Anton Alyakin', 'Daniel Alexander Alber', 'Jin Vivian Lee', 'Douglas Kondziolka', 'Eric Karl Oermann'], 'affiliations': ['Center for Data Science, New York University, New York, New York, 10016', 'Department of Aerospace Engineering and Engineering Mechanics, The University of Texas at Austin, Austin, Texas, 78712', 'Department of Mathematics, The University of Texas at Austin, Austin, Texas, 78712', 'Department of Neurological Surgery, NYU Langone Medical Center, New York, New York, 10016', 'Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, Missouri, 63110', 'Department of Radiology, NYU Langone Medical Center, New York, New York, 10016'], 'pdf_title_img': 'assets/pdf/title_img/2504.01201.jpg', 'data': {'categories': ['#benchmark', '#rag', '#healthcare', '#reasoning', '#hallucinations'], 'emoji': '🩺', 'ru': {'title': 'Большие языковые модели в медицине: проблема отделения зерен от плевел', 'desc': 'Статья исследует влияние посторонней информации на производительность больших языковых моделей (LLM) в медицинских сценариях. Авторы разработали бенчмарк MedDistractQA, содержащий вопросы в стиле USMLE с добавленными отвлекающими факторами. Результаты показывают, что отвлекающие утверждения могут снизить точность LLM на 17.9%. Стандартные методы улучшения, такие как RAG и дообучение на медицинских данных, не решают эту проблему, что указывает на отсутствие у LLM механизмов различения релевантной и нерелевантной клинической информации.'}, 'en': {'title': 'Enhancing LLMs: Tackling Noise in Medical Contexts', 'desc': "This paper explores the challenges faced by large language models (LLMs) in medical settings, particularly when they encounter irrelevant information during clinical scenarios. The authors created a benchmark called MedDistractQA, which includes USMLE-style questions with distractions that mimic real-world clinical noise. Their research found that such distractions can significantly decrease the accuracy of LLMs, by as much as 17.9%. Additionally, common strategies to improve model performance, like retrieval-augmented generation and medical fine-tuning, did not alleviate the issue and sometimes worsened it, indicating a fundamental limitation in LLMs' ability to filter relevant clinical data."}, 'zh': {'title': '提升大型语言模型在医学中的抗干扰能力', 'desc': '大型语言模型（LLMs）在医学领域具有变革潜力，但现实临床场景中存在的多余信息可能会影响其表现。我们开发了MedDistractQA基准，使用嵌入模拟现实干扰的USMLE风格问题来评估LLMs过滤相关数据的能力。研究发现，干扰性陈述会使LLM的准确性降低多达17.9%。这表明LLMs在区分相关与无关临床信息方面缺乏必要的逻辑机制，给实际应用带来了挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.18817', 'title': 'Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations', 'url': 'https://huggingface.co/papers/2503.18817', 'abstract': 'Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.', 'score': 1, 'issue_id': 3046, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'fa652fd57c6312f8', 'authors': ['Jeonghyeon Kim', 'Sangheum Hwang'], 'affiliations': ['Seoul National University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.18817.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#training', '#transfer_learning'], 'emoji': '🔍', 'ru': {'title': 'Улучшение обнаружения выбросов через мультимодальную тонкую настройку', 'desc': 'Статья посвящена обнаружению выбросов (OoDD) в мультимодальных моделях машинного обучения. Авторы предлагают метод мультимодальной тонкой настройки (MMFT) для улучшения производительности OoDD. Они вводят регуляризацию для усиления кросс-модального выравнивания, сближая семантически похожие изображения и тексты в гиперсферическом пространстве представлений. Экспериментальные результаты на наборах данных ImageNet-1k OoD показывают, что предложенный метод в сочетании с пост-хок подходами OoDD достигает наилучших результатов в обнаружении выбросов.'}, 'en': {'title': 'Enhancing Out-of-Distribution Detection with Multi-Modal Fine-Tuning', 'desc': 'This paper focuses on improving out-of-distribution detection (OoDD) using multi-modal fine-tuning (MMFT) with vision-language models like CLIP. The authors argue that traditional methods often freeze or partially tune pretrained weights, which limits performance on new datasets. They identify a key issue with na"ive fine-tuning methods, which fail to fully utilize the pretrained knowledge due to a modality gap in embeddings. To overcome this, they propose a new training objective that aligns image and text embeddings more effectively, leading to significant improvements in OoDD performance and overall accuracy on benchmark datasets.'}, 'zh': {'title': '多模态微调提升分布外检测性能', 'desc': '本论文探讨了多模态微调（MMFT）在分布外检测（OoDD）中的应用。以往的研究主要集中在单一模态模型上，而我们提出的方法通过增强图像和文本嵌入之间的跨模态对齐，显著提升了OoDD性能。我们分析了传统微调方法的局限性，并提出了一种新的训练目标，以更好地利用预训练的知识。通过在ImageNet-1k OoD基准数据集上的实验，我们的方法在OoDD性能和ID准确率上均达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2502.18924', 'title': 'MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for\n  Zero-Shot Speech Synthesis', 'url': 'https://huggingface.co/papers/2502.18924', 'abstract': 'While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces MegaTTS 3, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at https://sditdemo.github.io/sditdemo/.', 'score': 1, 'issue_id': 3053, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '538aaff0c9fb3421', 'authors': ['Ziyue Jiang', 'Yi Ren', 'Ruiqi Li', 'Shengpeng Ji', 'Boyang Zhang', 'Zhenhui Ye', 'Chen Zhang', 'Bai Jionghao', 'Xiaoda Yang', 'Jialong Zuo', 'Yu Zhang', 'Rui Liu', 'Xiang Yin', 'Zhou Zhao'], 'affiliations': ['ByteDance', 'Inner Mongolia University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18924.jpg', 'data': {'categories': ['#audio'], 'emoji': '🗣️', 'ru': {'title': 'Революция в синтезе речи: естественность и гибкость с MegaTTS 3', 'desc': 'MegaTTS 3 - это система синтеза речи, использующая инновационный алгоритм разреженного выравнивания для управления латентным диффузионным трансформером. Система предоставляет разреженные границы выравнивания для уменьшения сложности без ограничения пространства поиска, что позволяет достичь высокой естественности речи. MegaTTS 3 использует многоусловную стратегию наведения без классификатора для регулировки интенсивности акцента и технику кусочно-выпрямленного потока для ускорения процесса генерации. Эксперименты показывают, что система достигает современного качества синтеза речи без предварительного обучения и поддерживает гибкий контроль над интенсивностью акцента.'}, 'en': {'title': 'Revolutionizing TTS with Sparse Alignment and Flexible Accent Control', 'desc': 'This paper presents MegaTTS 3, a zero-shot text-to-speech (TTS) system that addresses challenges in speech-text alignment. It introduces a sparse alignment algorithm that enhances the robustness and naturalness of generated speech by providing flexible alignment boundaries. The system also incorporates a multi-condition classifier-free guidance strategy for adjusting accent intensity and uses a piecewise rectified flow technique to speed up the generation process. Experiments show that MegaTTS 3 achieves top-tier speech quality while allowing for precise control over accent, generating high-quality audio efficiently.'}, 'zh': {'title': 'MegaTTS 3：高自然性与灵活控制的文本到语音系统', 'desc': '本论文介绍了一种新的文本到语音（TTS）系统，名为MegaTTS 3，旨在解决语音与文本对齐建模的问题。该系统采用创新的稀疏对齐算法，能够在不限制搜索空间的情况下，减少对齐的难度，从而提高语音的自然性。MegaTTS 3还使用无条件分类器引导策略来调整口音强度，并采用分段修正流技术加速生成过程。实验结果表明，MegaTTS 3在零样本TTS语音质量上达到了最先进的水平，并支持对口音强度的灵活控制。'}}}, {'id': 'https://huggingface.co/papers/2503.18950', 'title': 'Target-Aware Video Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18950', 'abstract': "We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.", 'score': 0, 'issue_id': 3054, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '4009a703c8832026', 'authors': ['Taeksoo Kim', 'Hanbyul Joo'], 'affiliations': ['RLWRLD', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18950.jpg', 'data': {'categories': ['#video', '#diffusion', '#multimodal', '#robotics', '#games'], 'emoji': '🎬', 'ru': {'title': 'Генерация видео с целенаправленным взаимодействием актера и объекта', 'desc': 'Статья представляет модель диффузии видео, которая генерирует ролики из входного изображения, где актер взаимодействует с заданной целью, выполняя желаемое действие. Модель использует маску сегментации для определения цели и текстовое описание для задания действия. В отличие от существующих моделей, она не требует подробных структурных или двигательных подсказок, опираясь на обобщающие способности предобученных моделей. Авторы расширяют базовую модель, добавляя маску цели как дополнительный вход и вводя специальный токен для кодирования пространственной информации о цели в текстовом запросе.'}, 'en': {'title': 'Target-Aware Video Generation: Simplifying Human-Object Interaction', 'desc': "This paper introduces a target-aware video diffusion model that generates videos based on an input image of an actor interacting with a specified target, defined by a segmentation mask, while performing a desired action described by a text prompt. Unlike traditional models that depend on detailed structural or motion cues, this model simplifies the process by using just a mask, leveraging the strengths of pretrained models for realistic action generation. The model incorporates a special token to encode the target's spatial information, and it is fine-tuned with a novel cross-attention loss to ensure alignment between the target mask and the generated actions. Experimental results indicate that this approach significantly improves the accuracy of human-object interactions in generated videos, making it useful for applications like video content creation and 3D motion synthesis."}, 'zh': {'title': '目标感知视频生成，轻松实现人-物体互动', 'desc': '我们提出了一种目标感知的视频扩散模型，可以根据输入图像生成视频，其中演员与指定目标互动并执行所需动作。该目标通过分割掩码定义，所需动作通过文本提示描述。与现有的可控图像到视频扩散模型不同，我们的模型仅需简单的掩码来指示目标，利用预训练模型的泛化能力生成合理的动作。这种方法在处理人-物体互动场景时特别有效，能够在机器人等应用中实现高层次的动作规划。'}}}, {'id': 'https://huggingface.co/papers/2504.01990', 'title': 'Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems', 'url': 'https://huggingface.co/papers/2504.01990', 'abstract': 'The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.', 'score': 145, 'issue_id': 3069, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'f72a29b6411b97b1', 'authors': ['Bang Liu', 'Xinfeng Li', 'Jiayi Zhang', 'Jinlin Wang', 'Tanjin He', 'Sirui Hong', 'Hongzhang Liu', 'Shaokun Zhang', 'Kaitao Song', 'Kunlun Zhu', 'Yuheng Cheng', 'Suyuchen Wang', 'Xiaoqiang Wang', 'Yuyu Luo', 'Haibo Jin', 'Peiyan Zhang', 'Ollie Liu', 'Jiaqi Chen', 'Huan Zhang', 'Zhaoyang Yu', 'Haochen Shi', 'Boyan Li', 'Dekun Wu', 'Fengwei Teng', 'Xiaojun Jia', 'Jiawei Xu', 'Jinyu Xiang', 'Yizhang Lin', 'Tianming Liu', 'Tongliang Liu', 'Yu Su', 'Huan Sun', 'Glen Berseth', 'Jianyun Nie', 'Ian Foster', 'Logan Ward', 'Qingyun Wu', 'Yu Gu', 'Mingchen Zhuge', 'Xiangru Tang', 'Haohan Wang', 'Jiaxuan You', 'Chi Wang', 'Jian Pei', 'Qiang Yang', 'Xiaoliang Qi', 'Chenglin Wu'], 'affiliations': ['Argonne National Laboratory', 'Canada CIFAR AI Chair', 'Duke University', 'Google DeepMind', 'King Abdullah University of Science and Technology', 'MetaGPT', 'Microsoft Research Asia', 'Mila - Quebec AI Institute', 'Nanyang Technological University', 'Penn State University', 'Stanford University', 'The Hong Kong Polytechnic University', 'The Hong Kong University of Science and Technology', 'The Ohio State University', 'University of Georgia', 'University of Illinois at Urbana-Champaign', 'University of Southern California', 'University of Sydney', 'Université de Montréal', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01990.jpg', 'data': {'categories': ['#architecture', '#survey', '#security', '#ethics', '#agi', '#agents', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Интеллектуальные агенты нового поколения: от нейронауки к безопасному ИИ', 'desc': 'Эта статья представляет собой всесторонний обзор интеллектуальных агентов на основе больших языковых моделей (LLM). Авторы предлагают модульную архитектуру агентов, вдохновленную человеческим мозгом, интегрируя принципы когнитивной науки, нейронауки и вычислительных исследований. Рассматриваются механизмы самосовершенствования агентов, их адаптивная эволюция и взаимодействие в многоагентных системах. Особое внимание уделяется вопросам безопасности, этики и надежности при разработке и внедрении таких систем искусственного интеллекта.'}, 'en': {'title': 'Building Intelligent Agents: From Brain-Inspired Design to Safe AI Deployment', 'desc': 'This paper explores the development of large language models (LLMs) and their role in creating advanced intelligent agents that can reason, perceive, and act in various environments. It presents a modular architecture inspired by the human brain, detailing how cognitive, perceptual, and operational modules correspond to brain functions like memory and emotion. The paper also discusses how these agents can improve themselves through adaptive learning and optimization techniques, including AutoML. Finally, it emphasizes the importance of ensuring that AI systems are safe, ethical, and reliable for real-world applications.'}, 'zh': {'title': '智能体的未来：从大脑启发到安全应用', 'desc': '本文探讨了大型语言模型（LLMs）在人工智能领域的变革性影响，强调了智能体的设计、评估和持续改进所面临的复杂挑战。我们将智能体框架置于模块化的、受大脑启发的架构中，结合了认知科学、神经科学和计算研究的原则。文章分为四个部分，首先分析智能体的模块化基础，映射其认知、感知和操作模块与人类大脑功能的相似性。接着讨论自我增强和适应性进化机制，最后强调构建安全、可靠和有益的人工智能系统的重要性。'}}}, {'id': 'https://huggingface.co/papers/2504.02507', 'title': 'ZClip: Adaptive Spike Mitigation for LLM Pre-Training', 'url': 'https://huggingface.co/papers/2504.02507', 'abstract': 'Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.', 'score': 69, 'issue_id': 3065, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '290019150fe5b4c9', 'authors': ['Abhay Kumar', 'Louis Owen', 'Nilabhra Roy Chowdhury', 'Fabian Güra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2504.02507.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '📊', 'ru': {'title': 'ZClip: адаптивное ограничение градиентов для стабильного обучения языковых моделей', 'desc': 'В статье представлен новый алгоритм адаптивного ограничения градиентов под названием ZClip для обучения больших языковых моделей (LLM). ZClip динамически корректирует порог ограничения на основе статистических свойств норм градиентов во времени. Алгоритм использует обнаружение аномалий на основе z-оценки для выявления и смягчения больших всплесков градиентов. ZClip помогает предотвратить вредные всплески потерь, не мешая сходимости в остальных случаях.'}, 'en': {'title': 'ZClip: Smart Gradient Clipping for Stable LLM Training', 'desc': 'This paper addresses the challenges of training large language models (LLMs) by introducing ZClip, an adaptive gradient clipping algorithm. Traditional methods often fail to manage gradient instability and loss spikes effectively, leading to inefficient training and the need for manual adjustments. ZClip improves upon these methods by dynamically adjusting the clipping threshold based on the statistical behavior of gradient norms, allowing for a more responsive training process. By using z-score-based anomaly detection, ZClip prevents harmful loss spikes while maintaining the overall convergence of the model.'}, 'zh': {'title': '自适应梯度裁剪，提升训练稳定性', 'desc': '在训练大型语言模型时，常常会遇到梯度不稳定和损失峰值等问题，这可能导致灾难性的发散。传统的梯度裁剪技术无法有效解决这些问题，因为它们依赖于固定的阈值或启发式方法，导致学习效率低下。我们提出了一种名为ZClip的自适应梯度裁剪算法，它根据梯度范数的统计特性动态调整裁剪阈值。ZClip通过基于z-score的异常检测来识别和减轻大梯度峰值，从而防止损失峰值，同时不干扰收敛过程。'}}}, {'id': 'https://huggingface.co/papers/2504.02826', 'title': 'Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing', 'url': 'https://huggingface.co/papers/2504.02826', 'abstract': 'Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.', 'score': 60, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'dbb1c07cd5a01838', 'authors': ['Xiangyu Zhao', 'Peiyuan Zhang', 'Kexian Tang', 'Hao Li', 'Zicheng Zhang', 'Guangtao Zhai', 'Junchi Yan', 'Hua Yang', 'Xue Yang', 'Haodong Duan'], 'affiliations': ['Shanghai Jiao Tong University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02826.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'RISEBench: Новый рубеж в оценке визуального редактирования с рассуждениями', 'desc': 'RISEBench - это новый эталонный тест для оценки визуального редактирования с учетом рассуждений в мультимодальных моделях. Он фокусируется на четырех типах рассуждений: временном, причинно-следственном, пространственном и логическом. Тест оценивает понимание инструкций, сохранение внешнего вида и визуальную правдоподобность с помощью как человеческих оценщиков, так и LLM-судей. Эксперименты показали, что даже современные модели, такие как GPT-4, испытывают трудности с задачами логического рассуждения.'}, 'en': {'title': 'RISEBench: Advancing Reasoning in Visual Editing', 'desc': 'This paper introduces RISEBench, a new benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) in Large Multi-modality Models (LMMs). It identifies challenges in visual editing, such as following complex instructions and maintaining appearance consistency. RISEBench categorizes reasoning into four types: Temporal, Causal, Spatial, and Logical, and provides a framework for assessing these reasoning types through both human and model evaluations. The findings indicate that even advanced models like GPT-4o-Native struggle with logical reasoning, suggesting a need for further research in this area.'}, 'zh': {'title': '推理驱动的视觉编辑新基准', 'desc': '大型多模态模型（LMMs）在视觉理解和生成方面取得了显著进展，但在通用视觉编辑中仍面临挑战，尤其是在遵循复杂指令、保持外观一致性和支持灵活输入格式方面。为了解决这一问题，我们引入了RISEBench，这是第一个用于评估推理驱动视觉编辑（RISE）的基准。RISEBench专注于四种关键推理类型：时间推理、因果推理、空间推理和逻辑推理。我们的实验表明，尽管GPT-4o-Native在性能上显著优于其他模型，但在逻辑推理任务上仍然存在困难，显示出这一领域仍需深入探索。'}}}, {'id': 'https://huggingface.co/papers/2504.02782', 'title': 'GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation', 'url': 'https://huggingface.co/papers/2504.02782', 'abstract': "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.", 'score': 48, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '5346697bd326eed4', 'authors': ['Zhiyuan Yan', 'Junyan Ye', 'Weijia Li', 'Zilong Huang', 'Shenghai Yuan', 'Xiangyang He', 'Kaiqing Lin', 'Jun He', 'Conghui He', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Rabbitpre AI', 'Shanghai AI Laboratory', 'Shenzhen University', 'Sun Yat-sen University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2504.02782.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#architecture', '#diffusion', '#interpretability', '#optimization', '#hallucinations'], 'emoji': '🖼️', 'ru': {'title': 'GPT-4o: Новый рубеж в генерации и редактировании изображений с помощью ИИ', 'desc': 'Статья представляет первый оценочный бенчмарк (GPT-ImgEval) для модели GPT-4o от OpenAI, анализирующий ее способности в генерации и редактировании изображений. Исследование оценивает качество генерации, мастерство редактирования и семантический синтез на основе мировых знаний, демонстрируя превосходство GPT-4o над существующими методами. Авторы также предполагают, что архитектура GPT-4o включает авторегрессионную модель в сочетании с диффузионной головкой для декодирования изображений. Кроме того, статья анализирует ограничения GPT-4o, сравнивает ее с Gemini 2.0 Flash и обсуждает вопросы безопасности, связанные с выходными данными модели.'}, 'en': {'title': 'Unleashing the Power of GPT-4o in Image Generation and Editing', 'desc': "This paper evaluates the performance of OpenAI's GPT-4o model in image generation and editing using a new benchmark called GPT-ImgEval. The evaluation focuses on three key areas: the quality of generated images, the model's ability to edit images, and its understanding of semantic context. Results show that GPT-4o outperforms existing models in both image generation and editing, while also demonstrating strong reasoning capabilities. The paper also explores the model's architecture and limitations, providing insights for future research in image generation."}, 'zh': {'title': 'GPT-4o：图像生成与编辑的新突破', 'desc': '本论文介绍了OpenAI的GPT-4o模型在图像生成和编辑方面的最新突破。我们提出了一个名为GPT-ImgEval的评估基准，定量和定性地分析了GPT-4o在生成质量、编辑能力和知识推理等三个关键维度的表现。研究表明，GPT-4o在图像生成控制和输出质量上显著优于现有方法，并展示了卓越的知识推理能力。此外，我们还探讨了GPT-4o的架构，并识别了其在图像生成中常见的合成伪影和局限性。'}}}, {'id': 'https://huggingface.co/papers/2503.23377', 'title': 'JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical\n  Spatio-Temporal Prior Synchronization', 'url': 'https://huggingface.co/papers/2503.23377', 'abstract': 'This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce a fine-grained spatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatio-temporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose a new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise a robust metric for evaluating the synchronization between generated audio-video pairs in real-world complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting a new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/.', 'score': 41, 'issue_id': 3076, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '5437e5bc1ca6fe1e', 'authors': ['Kai Liu', 'Wei Li', 'Lai Chen', 'Shengqiong Wu', 'Yanhao Zheng', 'Jiayi Ji', 'Fan Zhou', 'Rongxin Jiang', 'Jiebo Luo', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['National University of Singapore', 'University of Rochester', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23377.jpg', 'data': {'categories': ['#audio', '#benchmark', '#multimodal', '#diffusion', '#dataset', '#open_source', '#video'], 'emoji': '🎬', 'ru': {'title': 'Синхронная генерация аудио и видео с помощью диффузионных трансформеров', 'desc': 'Статья представляет JavisDiT - новую архитектуру трансформера для совместной генерации аудио и видео на основе текстовых запросов. Модель использует механизм тонкой пространственно-временной синхронизации HiST-Sypo для согласования аудио и видео компонентов. Авторы также предлагают новый бенчмарк JavisBench из 10,140 высококачественных видео с звуком и текстовыми описаниями. Эксперименты показывают, что JavisDiT значительно превосходит существующие методы по качеству генерации и точности синхронизации аудио и видео.'}, 'en': {'title': 'JavisDiT: Synchronizing Audio and Video Generation with Precision', 'desc': "This paper presents JavisDiT, a new model for generating audio and video together, called Joint Audio-Video Diffusion Transformer (JAVG). It uses a special architecture called Diffusion Transformer (DiT) to create high-quality content based on user prompts. To keep the audio and video in sync, the model includes a mechanism that aligns their timing and spatial features through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. Additionally, the authors introduce a benchmark dataset, JavisBench, to evaluate the model's performance in generating synchronized audio-video pairs, showing that JavisDiT outperforms existing methods in both quality and synchronization."}, 'zh': {'title': 'JavisDiT：音视频生成的新标准', 'desc': '本文介绍了一种新颖的联合音视频扩散变换器JavisDiT，旨在实现同步的音视频生成。该模型基于强大的扩散变换器架构，能够从开放式用户提示中同时生成高质量的音频和视频内容。为了确保最佳同步，我们引入了一种细粒度的时空对齐机制，通过层次化时空同步先验估计器（HiST-Sypo）提取全局和细粒度的时空先验，指导视觉和听觉组件之间的同步。此外，我们提出了一个新的基准JavisBench，包含10,140个高质量的文本标注音视频，涵盖多样的场景和复杂的现实世界场景。'}}}, {'id': 'https://huggingface.co/papers/2504.00939', 'title': 'WikiVideo: Article Generation from Multiple Videos', 'url': 'https://huggingface.co/papers/2504.00939', 'abstract': "We present the challenging task of automatically creating a high-level Wikipedia-style article that aggregates information from multiple diverse videos about real-world events, such as natural disasters or political elections. Videos are intuitive sources for retrieval-augmented generation (RAG), but most contemporary RAG workflows focus heavily on text and existing methods for video-based summarization focus on low-level scene understanding rather than high-level event semantics. To close this gap, we introduce WikiVideo, a benchmark consisting of expert-written articles and densely annotated videos that provide evidence for articles' claims, facilitating the integration of video into RAG pipelines and enabling the creation of in-depth content that is grounded in multimodal sources. We further propose Collaborative Article Generation (CAG), a novel interactive method for article creation from multiple videos. CAG leverages an iterative interaction between an r1-style reasoning model and a VideoLLM to draw higher level inferences about the target event than is possible with VideoLLMs alone, which fixate on low-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in both oracle retrieval and RAG settings and find that CAG consistently outperforms alternative methods, while suggesting intriguing avenues for future work.", 'score': 30, 'issue_id': 3076, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'd04bc5c674ac39fc', 'authors': ['Alexander Martin', 'Reno Kriz', 'William Gantt Walden', 'Kate Sanders', 'Hannah Recknor', 'Eugene Yang', 'Francis Ferraro', 'Benjamin Van Durme'], 'affiliations': ['Human Language Technology Center of Excellence', 'Johns Hopkins University', 'University of Maryland Baltimore County'], 'pdf_title_img': 'assets/pdf/title_img/2504.00939.jpg', 'data': {'categories': ['#survey', '#benchmark', '#rag', '#reasoning', '#multimodal', '#video'], 'emoji': '📽️', 'ru': {'title': 'Создание обзорных статей из видео с помощью коллаборативной генерации', 'desc': 'Статья представляет задачу автоматического создания обзорных статей в стиле Википедии на основе информации из нескольких видео о реальных событиях. Авторы вводят датасет WikiVideo, содержащий экспертные статьи и аннотированные видео для обучения моделей. Предлагается новый метод Collaborative Article Generation (CAG), использующий итеративное взаимодействие между моделью рассуждений и VideoLLM для формирования более глубоких выводов о событиях. Эксперименты показывают, что CAG превосходит существующие подходы в задаче генерации статей на основе видео.'}, 'en': {'title': 'Transforming Videos into Knowledge: The Future of Article Generation', 'desc': 'This paper addresses the challenge of generating comprehensive Wikipedia-style articles from various videos about real-world events. It highlights the limitations of current retrieval-augmented generation (RAG) methods that primarily focus on text and low-level video analysis. The authors introduce WikiVideo, a benchmark that combines expert-written articles with annotated videos to enhance the integration of video content into RAG workflows. They also propose Collaborative Article Generation (CAG), an innovative approach that uses an interactive model to derive higher-level insights from videos, outperforming existing methods in generating detailed and contextually rich articles.'}, 'zh': {'title': '视频驱动的高水平文章生成新方法', 'desc': '本文介绍了一个具有挑战性的任务，即自动创建高水平的维基百科风格文章，汇总关于现实事件（如自然灾害或政治选举）的多种视频信息。我们提出了WikiVideo基准，包含专家撰写的文章和密集注释的视频，以支持文章的论点，从而促进视频在检索增强生成（RAG）流程中的整合。我们还提出了协作文章生成（CAG）方法，通过与r1风格推理模型和VideoLLM的迭代互动，能够对目标事件进行更高层次的推理。实验结果表明，CAG在各类设置中均优于现有方法，展示了未来研究的有趣方向。'}}}, {'id': 'https://huggingface.co/papers/2504.02587', 'title': 'Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme', 'url': 'https://huggingface.co/papers/2504.02587', 'abstract': 'Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.', 'score': 27, 'issue_id': 3063, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '58300c3a6e30995f', 'authors': ['Yan Ma', 'Steffi Chern', 'Xuyang Shen', 'Yiran Zhong', 'Pengfei Liu'], 'affiliations': ['Fudan University', 'Generative Artificial Intelligence Lab (GAIR)', 'Minimax', 'SII', 'Shanghai Jiao Tong University (SJTU)'], 'pdf_title_img': 'assets/pdf/title_img/2504.02587.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Прозрачное обучение с подкреплением для визуально-языковых моделей', 'desc': 'Эта статья представляет новый подход к обучению с подкреплением (RL) для визуально-языковых моделей (VLM). Авторы предлагают прозрачную и воспроизводимую систему для применения RL в VLM, включающую четырехэтапный конвейер. Они также вводят стандартизированную схему оценки для анализа динамики обучения и рефлексивного поведения моделей. Эксперименты показывают, что RL превосходит обычное обучение с учителем в задачах визуального рассуждения и обобщения.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Vision-Language Models!', 'desc': 'This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks.'}, 'zh': {'title': '建立可重复的强化学习框架', 'desc': '强化学习（RL）在提升大型语言模型的推理能力方面展现出强大的潜力，并正在积极扩展到视觉语言模型（VLMs）。然而，现有的RL应用往往依赖于复杂的框架，限制了可重复性和可访问性，同时缺乏标准化的评估协议，使得结果比较和训练动态解释变得困难。本文提出了一个透明的、从零开始的RL框架，提供了一个经过多个模型和数据集验证的最小功能四步流程。此外，提出了一种标准化的评估方案，以评估训练动态和反思行为。'}}}, {'id': 'https://huggingface.co/papers/2504.02495', 'title': 'Inference-Time Scaling for Generalist Reward Modeling', 'url': 'https://huggingface.co/papers/2504.02495', 'abstract': 'Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.', 'score': 25, 'issue_id': 3071, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '07408fa4b72ccb6c', 'authors': ['Zijun Liu', 'Peiyi Wang', 'Runxin Xu', 'Shirong Ma', 'Chong Ruan', 'Peng Li', 'Yang Liu', 'Yu Wu'], 'affiliations': ['DeepSeek-AI', 'Dept. of Computer Sci. & Tech., Tsinghua University', 'Institute for AI Industry Research (AIR), Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02495.jpg', 'data': {'categories': ['#training', '#benchmark', '#rl', '#rlhf', '#reasoning', '#open_source', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Масштабируемое моделирование вознаграждений для LLM с помощью самокритики', 'desc': 'Эта статья исследует улучшение моделирования вознаграждений (RM) для крупных языковых моделей (LLM) с использованием обучения с подкреплением (RL). Авторы предлагают метод Self-Principled Critique Tuning (SPCT) для улучшения генерации вознаграждений и масштабируемости моделей. Они также вводят мета-RM для руководства процессом голосования при масштабировании во время вывода. Результаты показывают, что SPCT значительно улучшает качество и масштабируемость генеративных моделей вознаграждения (GRM), превосходя существующие методы в различных тестах RM.'}, 'en': {'title': 'Enhancing Language Models with Scalable Reward Learning', 'desc': 'This paper explores advancements in reinforcement learning (RL) for enhancing large language models (LLMs) by focusing on reward modeling (RM) for general queries. The authors introduce a novel approach called Self-Principled Critique Tuning (SPCT) that improves the generation of reward signals, enabling better inference-time scalability. They also propose a pointwise generative reward modeling (GRM) technique that allows flexibility in handling different input types. Empirical results demonstrate that SPCT significantly enhances the quality and scalability of GRMs, outperforming existing methods while addressing challenges in generalist reward systems.'}, 'zh': {'title': '提升语言模型推理能力的强化学习方法', 'desc': '强化学习（RL）在大规模语言模型（LLMs）的后训练中得到了广泛应用。本文探讨了如何通过改进奖励建模（RM）来提高一般查询的推理时间可扩展性，并提出了自我原则批评调优（SPCT）方法，以促进GRM中的可扩展奖励生成行为。我们采用点对点生成奖励建模（GRM），以适应不同输入类型并实现推理时间的可扩展性。实验结果表明，SPCT显著提高了GRM的质量和可扩展性，超越了现有方法和模型。'}}}, {'id': 'https://huggingface.co/papers/2504.02398', 'title': 'Scaling Analysis of Interleaved Speech-Text Language Models', 'url': 'https://huggingface.co/papers/2504.02398', 'abstract': 'Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.', 'score': 24, 'issue_id': 3067, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'c03d1b64b7e6e276', 'authors': ['Gallil Maimon', 'Michael Hassid', 'Amit Roth', 'Yossi Adi'], 'affiliations': ['Department of Computer Science and Engineering, Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2504.02398.jpg', 'data': {'categories': ['#dataset', '#data', '#audio', '#open_source', '#synthetic', '#transfer_learning', '#training'], 'emoji': '🎙️', 'ru': {'title': 'Эффективное масштабирование речевых моделей через инициализацию текстовыми моделями', 'desc': 'Исследование показывает, что речевые языковые модели (SLM), инициализированные с помощью предобученных текстовых моделей, масштабируются более эффективно, чем чисто речевые модели. Авторы провели анализ масштабирования таких интерлейвных SLM, обучив несколько десятков моделей и изучив тенденции. Результаты указывают на то, что при таком подходе следует выделять больше вычислительных ресурсов на увеличение размера модели, а не на увеличение объема обучающих данных. Масштабированная модель авторов достигает сопоставимой производительности с ведущими моделями при использовании меньшего количества вычислений и данных.'}, 'en': {'title': 'Interleaved SLMs: Efficient Scaling for Speech Models!', 'desc': 'This paper investigates the efficiency of interleaved Speech Language Models (SLMs) compared to traditional textless SLMs. It finds that interleaved SLMs, which leverage pre-trained Text Language Models (TextLMs), require less compute and data while achieving competitive performance on speech tasks. The authors conduct a scaling analysis that reveals distinct scaling dynamics, suggesting a need for more compute allocation towards model size rather than training data. Additionally, the study highlights the importance of synthetic data and various TextLM families in enhancing the performance of SLMs.'}, 'zh': {'title': '交错SLM：更高效的扩展之路', 'desc': '本论文探讨了交错语音语言模型（SLM）的扩展效率。研究表明，交错SLM在计算资源的使用上比无文本SLM更为高效。我们发现，交错SLM的扩展动态与无文本SLM显著不同，建议在增加模型规模时应更多地分配计算预算。最终，经过扩展的模型在语音语义指标上表现出与领先模型相当的性能，同时使用的计算和数据量更少。'}}}, {'id': 'https://huggingface.co/papers/2504.02436', 'title': 'SkyReels-A2: Compose Anything in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2504.02436', 'abstract': 'This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.', 'score': 22, 'issue_id': 3063, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '86b46513a72dbd76', 'authors': ['Zhengcong Fei', 'Debang Li', 'Di Qiu', 'Jiahua Wang', 'Yikun Dou', 'Rui Wang', 'Jingtao Xu', 'Mingyuan Fan', 'Guibin Chen', 'Yang Li', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.02436.jpg', 'data': {'categories': ['#inference', '#multimodal', '#video', '#benchmark', '#open_source'], 'emoji': '🎬', 'ru': {'title': 'Контролируемая генерация видео из отдельных элементов', 'desc': 'SkyReels-A2 - это система генерации видео, способная собирать произвольные визуальные элементы в синтезированные видео на основе текстовых подсказок. Она использует модель совместного встраивания изображений и текста для сохранения согласованности элементов и глобальной связности. Авторы оптимизировали процесс вывода для скорости и стабильности, а также создали специальный набор данных для оценки. SkyReels-A2 является первой моделью с открытым исходным кодом коммерческого уровня для генерации видео из элементов (E2V).'}, 'en': {'title': 'SkyReels-A2: Mastering Video Generation with Element Control', 'desc': 'This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation.'}, 'zh': {'title': 'SkyReels-A2：可控视频生成的新突破', 'desc': '本文介绍了SkyReels-A2，一个可控的视频生成框架，能够根据文本提示将任意视觉元素（如角色、物体、背景）组合成合成视频，同时保持与每个元素的参考图像的一致性。我们将这一任务称为元素到视频（E2V），其主要挑战在于保持每个参考元素的真实性，确保场景的连贯性，以及实现自然的输出。为了解决这些问题，我们首先设计了一个全面的数据管道，以构建提示-参考-视频三元组用于模型训练。实验表明，我们的框架能够生成多样化、高质量的视频，并实现精确的元素控制。'}}}, {'id': 'https://huggingface.co/papers/2504.02542', 'title': 'Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation', 'url': 'https://huggingface.co/papers/2504.02542', 'abstract': 'Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.', 'score': 19, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'fa93ea3aeacd0dbc', 'authors': ['Fa-Ting Hong', 'Zunnan Xu', 'Zixiang Zhou', 'Jun Zhou', 'Xiu Li', 'Qin Lin', 'Qinglin Lu', 'Dan Xu'], 'affiliations': ['HKUST', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02542.jpg', 'data': {'categories': ['#video', '#multimodal', '#diffusion'], 'emoji': '🗣️', 'ru': {'title': 'Гибкий синтез говорящей головы с множественным контролем', 'desc': 'ACTalker - это новая модель для синтеза видео с говорящей головой, использующая диффузионный подход. Она поддерживает как мультимодальное, так и одномодальное управление генерацией видео. Модель использует параллельную структуру mamba с несколькими ветвями для обработки различных управляющих сигналов. ACTalker применяет механизм маскирования для предотвращения конфликтов между разными модальностями управления.'}, 'en': {'title': 'ACTalker: Multi-Signal Control for Natural Talking Head Synthesis', 'desc': 'This paper presents ACTalker, a novel framework for generating talking head videos that can be controlled by multiple signals simultaneously. Unlike traditional methods that rely on a single control modality, ACTalker employs a parallel mamba structure with multiple branches, each dedicated to a specific facial region. A gate mechanism allows for flexible control, ensuring that different driving signals can manipulate facial features without interference. The introduction of a mask-drop strategy further enhances this capability, enabling independent control of facial regions and resulting in more natural and coordinated video outputs.'}, 'zh': {'title': '多信号控制的对话头像生成新方法', 'desc': '本文介绍了一种名为ACTalker的端到端视频扩散框架，旨在生成虚拟头像的对话视频。该方法支持多信号和单信号控制，克服了现有方法的局限性。通过设计并行的mamba结构，允许不同的驱动信号控制面部的特定区域，并使用门控机制实现灵活控制。实验结果表明，ACTalker能够生成自然的面部视频，并且能够无冲突地整合多种驱动信号。'}}}, {'id': 'https://huggingface.co/papers/2504.00502', 'title': 'ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers', 'url': 'https://huggingface.co/papers/2504.00502', 'abstract': "Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV", 'score': 17, 'issue_id': 3067, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '1b236225c1d92fd7', 'authors': ['Qianhao Yuan', 'Qingyu Zhang', 'Yanjiang Liu', 'Jiawei Chen', 'Yaojie Lu', 'Hongyu Lin', 'Jia Zheng', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.00502.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Оптимизация MLLM: меньше вычислений, та же эффективность', 'desc': 'Исследователи представили новый метод оценки вклада слоев в мультимодальных больших языковых моделях (MLLM) с помощью метрики Layer Contribution (LC). Они обнаружили, что многие слои MLLM минимально влияют на обработку визуальных токенов. На основе этого наблюдения был разработан метод ShortV, который идентифицирует неэффективные слои и замораживает обновления визуальных токенов в них. ShortV позволяет заморозить визуальные токены примерно в 60% слоев MLLM, значительно снижая вычислительные затраты без потери производительности.'}, 'en': {'title': 'Optimizing MLLMs: Freeze the Unnecessary Layers!', 'desc': 'This paper addresses the high computational costs associated with Multimodal Large Language Models (MLLMs) by analyzing layer-wise redundancy. It introduces a new metric called Layer Contribution (LC) to measure how much each layer affects the processing of visual and text tokens. The findings indicate that many layers contribute little to the processing of visual tokens, allowing for optimization. The authors propose a method called ShortV, which identifies and freezes these ineffective layers, resulting in significant reductions in computational costs while preserving model performance.'}, 'zh': {'title': '优化多模态模型，降低计算成本', 'desc': '多模态大型语言模型（MLLMs）由于其庞大的规模和大量的视觉标记，面临着高计算成本的问题。本文提出了一种新颖的度量标准——层贡献（Layer Contribution，LC），用于量化模型中各层对视觉和文本标记的影响。通过计算去除某层变换后模型输出的差异，LC能够评估该层的贡献。实验表明，许多层在处理视觉标记时的贡献很小，因此我们提出了ShortV方法，能够识别并冻结这些无效层，从而显著降低计算成本。'}}}, {'id': 'https://huggingface.co/papers/2504.02154', 'title': 'FreSca: Unveiling the Scaling Space in Diffusion Models', 'url': 'https://huggingface.co/papers/2504.02154', 'abstract': "Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a ``scaling space'' whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets.", 'score': 12, 'issue_id': 3074, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '1c4d8a95379fa9a1', 'authors': ['Chao Huang', 'Susan Liang', 'Yunlong Tang', 'Li Ma', 'Yapeng Tian', 'Chenliang Xu'], 'affiliations': ['Netflix Eyeline Studios', 'The University of Texas at Dallas', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.02154.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#optimization', '#training'], 'emoji': '🔬', 'ru': {'title': 'Частотное масштабирование для улучшения контроля диффузионных моделей', 'desc': 'Статья исследует потенциал пространства масштабирования в диффузионных моделях для точного семантического манипулирования изображениями. Авторы проводят Фурье-анализ предсказаний шума и обнаруживают, что низко- и высокочастотные компоненты развиваются по-разному в процессе диффузии. На основе этого наблюдения они предлагают метод FreSca, который применяет масштабирование управления независимо к различным частотным диапазонам в частотной области. FreSca улучшает существующие методы редактирования изображений без переобучения и показывает эффективность в задачах понимания изображений, таких как оценка глубины.'}, 'en': {'title': 'Unlocking Image Control with Frequency-Based Scaling', 'desc': "This paper explores the potential of diffusion models in image tasks, focusing on how noise predictions can be manipulated for better control. It introduces a concept called 'scaling space' that allows for fine-tuned semantic editing by analyzing the differences in noise predictions. The authors present FreSca, a novel method that applies guidance scaling to different frequency components of noise in the Fourier domain. This approach not only improves image editing techniques but also enhances performance in image understanding tasks like depth estimation across various datasets."}, 'zh': {'title': '探索扩散模型的缩放空间', 'desc': '扩散模型在图像任务中提供了出色的可控性，主要通过噪声预测来编码特定任务的信息，并通过无分类器引导实现可调缩放。本文探讨了这种缩放机制所定义的“缩放空间”，并重点研究了基于反演的编辑方法，其中条件和无条件噪声预测之间的差异携带了重要的语义信息。我们通过对噪声预测的傅里叶分析，发现其低频和高频成分在扩散过程中以不同方式演变。基于这一见解，我们提出了FreSca方法，能够在傅里叶域中独立地对不同频带应用引导缩放，从而显著提升现有的图像编辑方法。'}}}, {'id': 'https://huggingface.co/papers/2504.02119', 'title': 'Efficient Model Selection for Time Series Forecasting via LLMs', 'url': 'https://huggingface.co/papers/2504.02119', 'abstract': 'Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.', 'score': 11, 'issue_id': 3063, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '7c31e20ce0a7813b', 'authors': ['Wang Wei', 'Tiankai Yang', 'Hongjie Chen', 'Ryan A. Rossi', 'Yue Zhao', 'Franck Dernoncourt', 'Hoda Eldardiry'], 'affiliations': ['Adobe Research San Jose, CA, USA', 'Adobe Research Seattle, WA, USA', 'Department of Computer Science University of South California Los Angeles, CA, USA', 'Department of Computer Science Virginia Tech Blacksburg, VA, USA', 'Dolby Labs Atlanta, GA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.02119.jpg', 'data': {'categories': ['#dataset', '#training', '#reasoning', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'LLM как эффективный инструмент выбора моделей в прогнозировании временных рядов', 'desc': 'Статья предлагает использовать большие языковые модели (LLM) для автоматизации выбора моделей в прогнозировании временных рядов. Этот подход устраняет необходимость в предварительно созданных матрицах производительности, опираясь на внутренние знания и способности рассуждения LLM. Эксперименты с моделями LLaMA, GPT и Gemini показывают, что предложенный метод превосходит традиционные техники мета-обучения и эвристические базовые линии. Результаты подчеркивают потенциал LLM в эффективном выборе моделей для прогнозирования временных рядов.'}, 'en': {'title': 'Revolutionizing Model Selection with Large Language Models', 'desc': 'This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain.'}, 'zh': {'title': '利用大型语言模型优化时间序列预测的模型选择', 'desc': '本研究探讨了时间序列预测中的模型选择问题，传统方法需要在多个数据集上进行广泛的性能评估。我们提出了一种利用大型语言模型（LLMs）作为轻量级替代方案的方法，避免了构建昂贵的性能矩阵。通过与LLaMA、GPT和Gemini的广泛实验，我们的方法在性能上超越了传统的元学习技术和启发式基线，同时显著降低了计算开销。这些结果强调了LLMs在时间序列预测中高效模型选择的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.22444', 'title': 'Scaling Laws in Scientific Discovery with AI and Robot Scientists', 'url': 'https://huggingface.co/papers/2503.22444', 'abstract': "Scientific discovery is poised for rapid advancement through advanced robotics and artificial intelligence. Current scientific practices face substantial limitations as manual experimentation remains time-consuming and resource-intensive, while multidisciplinary research demands knowledge integration beyond individual researchers' expertise boundaries. Here, we envision an autonomous generalist scientist (AGS) concept combines agentic AI and embodied robotics to automate the entire research lifecycle. This system could dynamically interact with both physical and virtual environments while facilitating the integration of knowledge across diverse scientific disciplines. By deploying these technologies throughout every research stage -- spanning literature review, hypothesis generation, experimentation, and manuscript writing -- and incorporating internal reflection alongside external feedback, this system aims to significantly reduce the time and resources needed for scientific discovery. Building on the evolution from virtual AI scientists to versatile generalist AI-based robot scientists, AGS promises groundbreaking potential. As these autonomous systems become increasingly integrated into the research process, we hypothesize that scientific discovery might adhere to new scaling laws, potentially shaped by the number and capabilities of these autonomous systems, offering novel perspectives on how knowledge is generated and evolves. The adaptability of embodied robots to extreme environments, paired with the flywheel effect of accumulating scientific knowledge, holds the promise of continually pushing beyond both physical and intellectual frontiers.", 'score': 11, 'issue_id': 3069, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': 'c2d75e49d08273c1', 'authors': ['Pengsong Zhang', 'Heng Zhang', 'Huazhe Xu', 'Renjun Xu', 'Zhenting Wang', 'Cong Wang', 'Animesh Garg', 'Zhibin Li', 'Arash Ajoudani', 'Xinyu Liu'], 'affiliations': ['Georgia Tech', 'Harvard University', 'Istituto Italiano di Tecnologia', 'Rutgers University', 'Tsinghua University', 'Universita di Genova', 'University College of London', 'University of Toronto', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22444.jpg', 'data': {'categories': ['#agents', '#robotics', '#agi', '#science'], 'emoji': '🤖', 'ru': {'title': 'Автономный учёный-универсал: революция в научных открытиях', 'desc': 'Статья представляет концепцию автономного учёного-универсала (AGS), объединяющего агентный ИИ и воплощенную робототехнику для автоматизации всего цикла научных исследований. Система AGS может взаимодействовать с физической и виртуальной средой, интегрируя знания из различных научных дисциплин. Предполагается, что внедрение AGS значительно сократит время и ресурсы, необходимые для научных открытий. Авторы предполагают, что с развитием таких систем научные открытия могут подчиняться новым законам масштабирования, зависящим от количества и возможностей автономных систем.'}, 'en': {'title': 'Revolutionizing Science with Autonomous Generalist Scientists', 'desc': 'This paper introduces the concept of an Autonomous Generalist Scientist (AGS) that combines artificial intelligence and robotics to automate the entire scientific research process. The AGS can interact with both physical and virtual environments, facilitating knowledge integration across various scientific fields. By automating tasks such as literature review, hypothesis generation, experimentation, and manuscript writing, the AGS aims to significantly reduce the time and resources required for scientific discovery. The authors suggest that as these systems become more integrated into research, they could change how knowledge is generated and evolve, potentially leading to new scaling laws in scientific discovery.'}, 'zh': {'title': '自主科学家：加速科学发现的未来', 'desc': '这篇论文提出了一种自主通用科学家（AGS）的概念，结合了智能代理AI和具身机器人，旨在自动化整个研究生命周期。该系统能够动态与物理和虚拟环境互动，并促进不同科学学科之间的知识整合。通过在文献回顾、假设生成、实验和论文写作等研究阶段应用这些技术，AGS希望显著减少科学发现所需的时间和资源。随着这些自主系统越来越多地融入研究过程，科学发现可能会遵循新的规模法则，提供关于知识生成和演变的新视角。'}}}, {'id': 'https://huggingface.co/papers/2504.01871', 'title': 'Interpreting Emergent Planning in Model-Free Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.01871', 'abstract': "We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL", 'score': 10, 'issue_id': 3069, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'bf12d8cbe28bb942', 'authors': ['Thomas Bush', 'Stephen Chung', 'Usman Anwar', 'Adrià Garriga-Alonso', 'David Krueger'], 'affiliations': ['FAR AI', 'Mila, University of Montreal', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2504.01871.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#benchmark', '#rl', '#games', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Агенты обучения с подкреплением без модели способны к планированию', 'desc': 'Исследователи представили первое механистическое доказательство того, что агенты обучения с подкреплением без модели могут научиться планировать. Они применили методологию, основанную на интерпретируемости концепций, к агенту без модели в игре Sokoban. Было продемонстрировано, что агент DRC использует выученные концептуальные представления для внутреннего формулирования планов, которые предсказывают долгосрочные эффекты действий и влияют на выбор действий. Методология включала зондирование релевантных для планирования концепций, исследование формирования планов в представлениях агента и проверку причинно-следственной связи обнаруженных планов с поведением агента через вмешательства.'}, 'en': {'title': 'Unveiling Planning in Model-Free Reinforcement Learning Agents', 'desc': 'This paper provides evidence that model-free reinforcement learning agents can learn to plan by using concept-based interpretability. The authors focus on a model-free agent called DRC, which learns to create internal plans that predict the outcomes of its actions in the Sokoban environment. They explore how the agent identifies relevant concepts, forms plans, and how these plans affect its behavior through interventions. The study reveals that as the agent develops planning capabilities, it also shows improved performance with additional computational resources, resembling advanced search algorithms.'}, 'zh': {'title': '无模型强化学习中的规划能力', 'desc': '本文首次提供了无模型强化学习代理能够学习规划的机制性证据。我们通过在Sokoban这一常用基准上应用基于概念的可解释性方法，展示了DRC代理如何利用学习到的概念表示来内部制定计划。具体而言，代理能够预测行动对环境的长期影响，并影响行动选择。我们的研究表明，代理的计划形成与其行为之间存在因果关系，并且这种计划的出现与代理在测试时利用额外计算能力的能力相吻合。'}}}, {'id': 'https://huggingface.co/papers/2503.23162', 'title': 'NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact\n  3D Representations', 'url': 'https://huggingface.co/papers/2503.23162', 'abstract': '3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and a complex encoding and quantization strategy. In this paper, we aim to develop a simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into a compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 45-times average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields.', 'score': 9, 'issue_id': 3073, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '33d8d348d9e9fccf', 'authors': ['Zhenyu Tang', 'Chaoran Feng', 'Xinhua Cheng', 'Wangbo Yu', 'Junwu Zhang', 'Yuan Liu', 'Xiaoxiao Long', 'Wenping Wang', 'Li Yuan'], 'affiliations': ['Hong Kong University of Science and Technology', 'Peking University', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23162.jpg', 'data': {'categories': ['#optimization', '#inference', '#dataset', '#3d'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие 3D гауссовых сплаттингов с помощью нейронных полей', 'desc': 'Статья представляет NeuralGS - метод сжатия 3D гауссовых сплаттингов (3DGS) с использованием нейронных полей. Авторы применяют многослойные перцептроны (MLP) для кодирования атрибутов 3D гауссианов, что позволяет значительно уменьшить размер модели без потери качества визуализации. Метод использует стратегию кластеризации и подгонки гауссианов с разными небольшими MLP для каждого кластера. Эксперименты показывают 45-кратное среднее уменьшение размера модели при сохранении визуального качества.'}, 'en': {'title': 'NeuralGS: Compact 3D Gaussian Compression with Neural Fields', 'desc': 'This paper introduces NeuralGS, a novel method for compressing 3D Gaussian Splatting (3DGS) representations into a more compact form without relying on complex voxel structures. By leveraging neural fields and Multi-Layer Perceptron (MLP) networks, NeuralGS can effectively encode the attributes of 3D Gaussians while significantly reducing storage requirements. The approach involves clustering Gaussians and fitting them with small MLPs based on their importance scores, leading to a remarkable 45-times reduction in model size. Experimental results show that NeuralGS maintains high visual quality and achieves compression performance comparable to existing methods that use Scaffold-GS.'}, 'zh': {'title': '用神经场压缩3D高斯点云的创新方法', 'desc': '3D高斯点云（3DGS）在质量和渲染速度上表现优异，但其存储和传输成本高昂。本文提出了一种名为NeuralGS的简单有效的方法，通过神经场表示来压缩原始3DGS，避免了复杂的体素结构和量化策略。我们利用多层感知器（MLP）神经网络对3D高斯的属性进行编码，仅需少量存储空间，即使在大规模场景中也能保持良好的视觉质量。实验结果表明，我们的方法在模型大小上平均减少了45倍，压缩性能与现有的体素基础压缩方法相当，展示了直接使用神经场压缩3DGS的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2504.02821', 'title': 'Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2504.02821', 'abstract': 'Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.', 'score': 8, 'issue_id': 3069, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'd4568f020b5f2eca', 'authors': ['Mateusz Pach', 'Shyamgopal Karthik', 'Quentin Bouniot', 'Serge Belongie', 'Zeynep Akata'], 'affiliations': ['Helmholtz Munich', 'Munich Center of Machine Learning', 'Munich Data Science Institute', 'Technical University of Munich', 'University of Copenhagen', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2504.02821.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#architecture', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Разреженные автоэнкодеры раскрывают потенциал мультимодальных моделей', 'desc': 'Исследователи применили разреженные автоэнкодеры (SAE) к мультимодальным моделям, объединяющим зрение и язык (VLM). Эксперименты показали, что SAE значительно улучшают моносемантичность отдельных нейронов в VLM и формируют иерархические представления, соответствующие экспертным таксономиям. Авторы продемонстрировали возможность управления выходными данными мультимодальных языковых моделей путем вмешательства в энкодер зрения CLIP с помощью SAE. Результаты подчеркивают эффективность SAE для повышения интерпретируемости и управляемости VLM.'}, 'en': {'title': 'Enhancing Vision-Language Models with Sparse Autoencoders', 'desc': 'This paper explores the use of Sparse Autoencoders (SAEs) to improve the interpretability and steerability of Vision-Language Models (VLMs) like CLIP. The authors present a framework to assess how well these models represent single concepts, known as monosemanticity. Their experiments show that SAEs can enhance the clarity of individual neurons in VLMs and align these representations with established expert categories. Importantly, they demonstrate that SAEs can influence the output of multimodal language models without changing the original model architecture.'}, 'zh': {'title': '稀疏自编码器提升视觉-语言模型的可解释性与操控性', 'desc': '稀疏自编码器（SAEs）最近被证明可以提高大型语言模型（LLMs）的可解释性和可操控性。本文将SAEs的应用扩展到视觉-语言模型（VLMs），如CLIP，并引入了一个全面的框架来评估视觉表示的单义性。实验结果表明，在VLMs上训练的SAEs显著增强了单个神经元的单义性，并展示了与专家定义结构（如iNaturalist分类法）良好对齐的层次表示。最重要的是，我们证明了将SAEs应用于CLIP视觉编码器，可以直接操控多模态LLMs（如LLaVA）的输出，而无需对基础模型进行任何修改。'}}}, {'id': 'https://huggingface.co/papers/2504.00891', 'title': 'GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning', 'url': 'https://huggingface.co/papers/2504.00891', 'abstract': 'Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.', 'score': 8, 'issue_id': 3066, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'b22a54f43f9d7a89', 'authors': ['Jian Zhao', 'Runze Liu', 'Kaiyan Zhang', 'Zhimu Zhou', 'Junqi Gao', 'Dong Li', 'Jiafei Lyu', 'Zhouyi Qian', 'Biqing Qi', 'Xiu Li', 'Bowen Zhou'], 'affiliations': ['BUPT', 'Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00891.jpg', 'data': {'categories': ['#training', '#optimization', '#math', '#reasoning', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'GenPRM: Новая парадигма контроля процесса рассуждений в больших языковых моделях', 'desc': 'Статья представляет GenPRM - генеративную модель вознаграждения процесса, которая использует рассуждения по цепочке мыслей и верификацию кода для оценки каждого шага рассуждений. Авторы предлагают методы относительной оценки прогресса и синтеза обоснований для получения качественных обучающих данных. Эксперименты показывают, что GenPRM значительно превосходит предыдущие модели PRM на нескольких задачах математических рассуждений, используя всего 23 тысячи обучающих примеров. Модель также демонстрирует способность выступать в роли критика для улучшения политики в больших языковых моделях.'}, 'en': {'title': 'GenPRM: Elevating LLMs with Generative Process Reward Models', 'desc': 'This paper presents GenPRM, a generative process reward model designed to improve the performance of large language models (LLMs) by addressing key challenges faced by existing process reward models (PRMs). GenPRM utilizes Chain-of-Thought (CoT) reasoning and incorporates code verification to enhance the quality of its judgments at each reasoning step. The authors introduce a novel method called Relative Progress Estimation (RPE) to generate high-quality supervision labels and rationale data, leading to significant performance improvements on various reasoning tasks. Experimental results demonstrate that GenPRM outperforms previous PRMs and shows strong capabilities as a critic model for refining policy models, establishing a new approach for process supervision in LLMs.'}, 'zh': {'title': '生成性过程奖励模型：提升LLMs的新范式', 'desc': '最近，大型语言模型（LLMs）的进展表明，使用过程奖励模型（PRMs）作为验证器可以提升LLMs的性能。然而，当前的PRMs面临三个主要挑战：有限的过程监督和泛化能力、依赖于标量值预测而未利用LLMs的生成能力，以及无法扩展PRMs的测试时间计算。本文提出了GenPRM，这是一种生成性过程奖励模型，通过代码验证进行明确的思维链推理，然后对每个推理步骤进行判断。实验结果表明，GenPRM在多个数学推理任务上显著优于之前的PRMs，展示了其作为政策模型精炼的批评模型的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2503.23542', 'title': 'Whisper-LM: Improving ASR Models with Language Models for Low-Resource\n  Languages', 'url': 'https://huggingface.co/papers/2503.23542', 'abstract': 'Automatic speech recognition systems have undoubtedly advanced with the integration of multilingual and multitask models such as Whisper, which have shown a promising ability to understand and process speech across a wide range of languages. Despite their robustness, these models often fall short in handling the linguistic distinctions of minority languages. This study addresses this gap by integrating traditional and novel language models with fine-tuned Whisper models to raise their performance in less commonly studied languages. Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. We obtained improvements up to 51\\% for in-distribution datasets and up to 34\\% for out-of-distribution sentences using statistical language models, while large language models provided moderate but consistently robust improvement across diverse linguistic contexts. The findings reveal that, while the integration reliably benefits all model sizes, the extent of improvement varies, highlighting the importance of optimized language model parameters. Finally, we emphasize the importance of selecting appropriate evaluation parameters when reporting the results using transformer-based ASR models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge. For further implementation details of this study, the technical documentation and source code are available at http://www.github.com/hitz-zentroa/whisper-lm.', 'score': 8, 'issue_id': 3072, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'c8ef94a1b05fc918', 'authors': ['Xabier de Zuazo', 'Eva Navas', 'Ibon Saratxaga', 'Inma Hernáez Rioja'], 'affiliations': ['HiTZ - University of the Basque Country - UPV/EHU, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2503.23542.jpg', 'data': {'categories': ['#training', '#inference', '#multilingual', '#dataset', '#low_resource', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Улучшение распознавания речи для редких языков с помощью языковых моделей', 'desc': 'Исследование посвящено улучшению работы систем автоматического распознавания речи для малоресурсных языков. Авторы интегрируют традиционные и новые языковые модели с тонко настроенными моделями Whisper. Эксперименты показали значительное снижение частоты ошибок в словах, особенно для языков с ограниченными ресурсами. Подход сочетает преимущества предобученной модели Whisper с адаптивностью языковых моделей, открывая путь к более инклюзивным технологиям распознавания речи.'}, 'en': {'title': 'Enhancing ASR for Minority Languages with Whisper and Language Models', 'desc': 'This paper discusses advancements in automatic speech recognition (ASR) systems, particularly focusing on multilingual and multitask models like Whisper. It highlights the challenges these models face with minority languages and proposes a solution by combining traditional and novel language models with fine-tuned Whisper models. The study shows significant improvements in word error rates for low-resource languages through rigorous fine-tuning and evaluation. The results indicate that optimized language model parameters can enhance ASR performance across various linguistic contexts, paving the way for more inclusive ASR technologies.'}, 'zh': {'title': '提升少数语言语音识别的包容性', 'desc': '本研究探讨了如何提高自动语音识别系统在少数语言中的表现，尤其是通过结合传统和新型语言模型与微调的Whisper模型。尽管Whisper在多语言处理上表现出色，但在处理少数语言时仍存在不足。我们通过严格的微调和多数据集评估，显著降低了词错误率，尤其是在资源匮乏的情况下。研究结果表明，优化语言模型参数对提升模型性能至关重要，从而推动了更具包容性的语音识别技术的发展。'}}}, {'id': 'https://huggingface.co/papers/2504.02012', 'title': 'Instruction-Guided Autoregressive Neural Network Parameter Generation', 'url': 'https://huggingface.co/papers/2504.02012', 'abstract': "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.", 'score': 6, 'issue_id': 3065, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'cbdd586ccd2b682d', 'authors': ['Soro Bedionita', 'Bruno Andreis', 'Song Chong', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai, South Korea', 'KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.02012.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#optimization', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'IGPG: Универсальный генератор параметров нейросетей для различных задач и архитектур', 'desc': 'IGPG - это авторегрессивная система для генерации параметров нейронных сетей, основанная на VQ-VAE и авторегрессивной модели. Она создает параметры на основе описания задачи, датасета и архитектуры сети, обеспечивая согласованность между слоями. IGPG работает на уровне токенов, эффективно охватывая сложные распределения параметров из широкого спектра предобученных моделей. Эксперименты показывают, что IGPG превосходит современные методы по масштабируемости и эффективности для крупных архитектур.'}, 'en': {'title': 'IGPG: Unifying Neural Network Parameter Generation for Enhanced Adaptability', 'desc': 'This paper introduces IGPG (Instruction Guided Parameter Generation), a new framework for generating neural network parameters based on task descriptions and architecture specifications. Unlike previous methods, IGPG uses an autoregressive approach that ensures coherence between layers and adapts efficiently to different models and datasets. By employing a VQ-VAE and generating weights at the token level, IGPG captures complex parameter distributions from various pretrained models. The results show that IGPG outperforms existing methods in scalability and efficiency, making it a valuable tool for model adaptation and fine-tuning.'}, 'zh': {'title': 'IGPG：灵活的神经网络参数生成工具', 'desc': '本文提出了一种名为IGPG（指令引导参数生成）的自回归框架，旨在生成神经网络参数，以适应不同的任务描述和架构规范。IGPG通过结合VQ-VAE和自回归模型，能够在多种任务和架构中统一参数合成，确保层间一致性。该方法在生成神经网络权重时，采用了基于token的生成方式，有效捕捉来自多种预训练模型的复杂参数分布。实验结果表明，IGPG在多个视觉数据集上表现出色，尤其在大规模架构的可扩展性和效率方面，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2504.01955', 'title': 'Scene-Centric Unsupervised Panoptic Segmentation', 'url': 'https://huggingface.co/papers/2504.01955', 'abstract': 'Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ.', 'score': 4, 'issue_id': 3079, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '24a963913e1c2220', 'authors': ['Oliver Hahn', 'Christoph Reich', 'Nikita Araslanov', 'Daniel Cremers', 'Christian Rupprecht', 'Stefan Roth'], 'affiliations': ['ELIZA', 'MCML', 'TU Darmstadt', 'TU Munich', 'University of Oxford', 'hessian.AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.01955.jpg', 'data': {'categories': ['#cv'], 'emoji': '🖼️', 'ru': {'title': 'Неконтролируемая панорамная сегментация без разметки данных', 'desc': 'Эта статья представляет новый метод неконтролируемой панорамной сегментации изображений, который не требует размеченных данных. Авторы предлагают подход для получения псевдо-меток высокого разрешения на сложных сценах, сочетая визуальные представления, глубину и движение. Метод использует обучение на псевдо-метках и стратегию самообучения для точного предсказания панорамной сегментации. Результаты показывают значительное улучшение качества сегментации по сравнению с предыдущими методами.'}, 'en': {'title': 'Revolutionizing Unsupervised Panoptic Segmentation with Scene-Centric Learning', 'desc': 'This paper introduces a new method for unsupervised panoptic segmentation, which is the task of dividing an image into meaningful areas and separate object instances without using labeled data. The authors propose a novel approach that focuses on scene-centric images, eliminating the reliance on object-centric training data. They develop a technique to generate high-resolution pseudo labels by integrating visual features, depth information, and motion cues. Their method demonstrates a significant improvement in panoptic segmentation quality, achieving a 9.4% increase in performance on the Cityscapes dataset compared to previous state-of-the-art methods.'}, 'zh': {'title': '无监督全景分割的新突破', 'desc': '无监督全景分割旨在将图像划分为具有语义意义的区域和独特的物体实例，而无需依赖人工标注的数据。与之前的无监督全景场景理解工作不同，我们消除了对以物体为中心的训练数据的需求，从而实现对复杂场景的无监督理解。我们提出了一种新的无监督全景方法，直接在场景中心图像上进行训练，结合视觉表示、深度和运动线索，获得高分辨率的全景伪标签。通过伪标签训练和全景自我训练策略，我们的方法在无需任何人工标注的情况下，准确预测复杂场景的全景分割，显著提高了全景质量。'}}}, {'id': 'https://huggingface.co/papers/2504.01943', 'title': 'OpenCodeReasoning: Advancing Data Distillation for Competitive Coding', 'url': 'https://huggingface.co/papers/2504.01943', 'abstract': 'Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community.', 'score': 4, 'issue_id': 3083, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '6581bbd20e92ba34', 'authors': ['Wasi Uddin Ahmad', 'Sean Narenthiran', 'Somshubra Majumdar', 'Aleksander Ficek', 'Siddhartha Jain', 'Jocelyn Huang', 'Vahid Noroozi', 'Boris Ginsburg'], 'affiliations': ['NVIDIA Santa Clara, CA 15213, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.01943.jpg', 'data': {'categories': ['#optimization', '#open_source', '#benchmark', '#reasoning', '#dataset', '#data', '#training'], 'emoji': '💻', 'ru': {'title': 'Прорыв в обучении языковых моделей программированию без подкрепления', 'desc': 'Исследователи разработали усовершенствованный набор данных для обучения языковых моделей решению задач программирования. Используя только метод обучения с учителем (SFT), они достигли лучших результатов на бенчмарках LiveCodeBench и CodeContests по сравнению с моделями, обученными с помощью обучения с подкреплением. Анализ показал, что фильтрация по корректности выполнения кода негативно влияет на точность, поэтому было решено сделать акцент на разнообразии инструкций. Исследователи планируют открыть доступ к набору данных и обученным моделям для сообщества.'}, 'en': {'title': 'Unlocking Coding Potential with Enhanced Supervised Fine-Tuning', 'desc': 'This paper discusses the development of a new supervised fine-tuning (SFT) dataset aimed at enhancing the coding capabilities of reasoning-based large language models (LLMs). The authors demonstrate that their distilled models, trained solely on this SFT dataset, achieve superior performance on coding benchmarks compared to those trained with reinforcement learning. They analyze the effects of data sources, code execution filtering, and the diversity of instructions and solutions on model performance. The findings suggest that prioritizing instruction diversity can lead to better outcomes, and the authors plan to share their datasets and models with the research community.'}, 'zh': {'title': '提升编码能力的推理模型新方法', 'desc': '本文探讨了基于推理的大型语言模型在编码任务中的应用。我们构建了一个优质的监督微调（SFT）数据集，以提升不同规模模型的编码能力。通过使用该数据集，我们的模型在LiveCodeBench和CodeContests上分别达到了61.8%和24.6%的成绩，超越了使用强化学习训练的替代方案。我们还分析了数据来源、代码执行过滤的影响以及指令和解决方案多样性的重要性，并计划将这些数据集和模型开源。'}}}, {'id': 'https://huggingface.co/papers/2504.10479', 'title': 'InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models', 'url': 'https://huggingface.co/papers/2504.10479', 'abstract': 'We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.', 'score': 167, 'issue_id': 3237, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '51475893ef3c1d8b', 'authors': ['Jinguo Zhu', 'Weiyun Wang', 'Zhe Chen', 'Zhaoyang Liu', 'Shenglong Ye', 'Lixin Gu', 'Yuchen Duan', 'Hao Tian', 'Weijie Su', 'Jie Shao', 'Zhangwei Gao', 'Erfei Cui', 'Yue Cao', 'Yangzhou Liu', 'Weiye Xu', 'Hao Li', 'Jiahao Wang', 'Han Lv', 'Dengnian Chen', 'Songze Li', 'Yinan He', 'Tan Jiang', 'Jiapeng Luo', 'Yi Wang', 'Conghui He', 'Botian Shi', 'Xingcheng Zhang', 'Wenqi Shao', 'Junjun He', 'Yingtong Xiong', 'Wenwen Qu', 'Peng Sun', 'Penglong Jiao', 'Lijun Wu', 'Kaipeng Zhang', 'Huipeng Deng', 'Jiaye Ge', 'Kai Chen', 'Limin Wang', 'Min Dou', 'Lewei Lu', 'Xizhou Zhu', 'Tong Lu', 'Dahua Lin', 'Yu Qiao', 'Jifeng Dai', 'Wenhai Wang'], 'affiliations': ['Fudan University', 'Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10479.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#agi', '#training', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'InternVL3: Революция в мультимодальном обучении языковых моделей', 'desc': 'InternVL3 представляет собой значительный прогресс в области мультимодальных языковых моделей (MLLM). Эта модель объединяет обучение на мультимодальных и текстовых данных в едином процессе предобучения, что позволяет преодолеть сложности, возникающие при традиционном подходе адаптации текстовых моделей. InternVL3 использует ряд передовых техник, включая переменное визуальное позиционное кодирование (V2PE) и смешанную оптимизацию предпочтений (MPO). Модель демонстрирует превосходные результаты на различных мультимодальных задачах, достигая 72.2 балла на бенчмарке MMMU и конкурируя с ведущими проприетарными моделями.'}, 'en': {'title': 'Revolutionizing Multimodal Learning with InternVL3', 'desc': 'InternVL3 is a new multimodal large language model (MLLM) that learns from both visual and text data simultaneously during its training. This approach helps it overcome common challenges faced by traditional models that are adapted from text-only systems. It uses innovative techniques like variable visual position encoding and advanced fine-tuning methods to enhance its performance on various tasks. The model has achieved impressive results, outperforming many existing models while also committing to open science by sharing its resources with the research community.'}, 'zh': {'title': 'InternVL3：多模态预训练的新标杆', 'desc': 'InternVL3是InternVL系列的重要进展，采用了原生的多模态预训练范式。与传统的将文本模型转变为多模态模型不同，InternVL3在单一预训练阶段同时学习多模态和语言能力。该模型通过引入可变视觉位置编码（V2PE）和先进的后训练技术，显著提升了性能和可扩展性。经过广泛的实证评估，InternVL3在多模态任务中表现优异，尤其在MMMU基准测试中取得了72.2的分数，成为开源多模态语言模型的新标杆。'}}}, {'id': 'https://huggingface.co/papers/2504.08791', 'title': 'PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters', 'url': 'https://huggingface.co/papers/2504.08791', 'abstract': "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.", 'score': 92, 'issue_id': 3236, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '2d5649ec3925b1a3', 'authors': ['Zonghang Li', 'Tao Li', 'Wenjiao Feng', 'Mohsen Guizani', 'Hongfang Yu'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE', 'University of Electronic Science and Technology of China, Chengdu, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.08791.jpg', 'data': {'categories': ['#inference', '#optimization', '#open_source', '#architecture'], 'emoji': '🏠', 'ru': {'title': 'Продвинутые языковые модели теперь доступны на домашних устройствах', 'desc': 'Статья представляет prima.cpp - распределенную систему вывода, позволяющую запускать крупномасштабные языковые модели (до 70 миллиардов параметров) на обычных домашних устройствах. Система использует комбинацию CPU/GPU, низкие требования к RAM/VRAM и поддержку Wi-Fi для эффективной работы. Prima.cpp применяет технологию mmap для управления весами модели и вводит конвейерный кольцевой параллелизм с предвыборкой для скрытия загрузки с диска. Авторы предлагают алгоритм Halda для оптимального распределения слоев модели между устройствами, учитывая их гетерогенность.'}, 'en': {'title': 'Bringing Powerful AI to Your Home Devices', 'desc': 'This paper presents prima.cpp, a novel distributed inference system designed to run large language models (LLMs) on standard home devices. It leverages a combination of CPU and GPU resources, along with efficient memory management techniques like mmap and piped-ring parallelism, to optimize performance. By intelligently assigning model layers based on the capabilities of each device, it significantly reduces latency while maintaining low memory usage. The system demonstrates superior performance compared to existing solutions, making advanced AI models accessible for personal use.'}, 'zh': {'title': '让家庭设备也能运行大型语言模型', 'desc': '本文介绍了一种名为prima.cpp的分布式推理系统，能够在普通家庭设备上运行70B规模的语言模型。该系统通过混合使用CPU和GPU，优化内存和带宽的使用，解决了传统方案对高性能硬件的依赖。它采用了mmap管理模型权重，并引入了管道环并行和预取技术，以减少磁盘加载时间。通过优化计算、通信和内存管理，prima.cpp显著降低了延迟，使得先进的AI模型能够在家庭助手中普及。'}}}, {'id': 'https://huggingface.co/papers/2504.09925', 'title': 'FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding', 'url': 'https://huggingface.co/papers/2504.09925', 'abstract': 'We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION', 'score': 34, 'issue_id': 3236, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '948c65f51f6a11b7', 'authors': ['Zheng Liu', 'Mengjie Liu', 'Jingzhou Chen', 'Jingwei Xu', 'Bin Cui', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Nanjing University', 'Peking University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2504.09925.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#open_source', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'FUSION: Глубокая интеграция зрения и языка в мультимодальных ИИ-моделях', 'desc': 'FUSION - это семейство мультимодальных больших языковых моделей с полной интеграцией зрения и языка. Модель использует текстово-управляемое унифицированное кодирование изображений и контекстно-зависимое рекурсивное декодирование для глубокой интеграции модальностей. Авторы разработали специальную функцию потерь и синтетический набор данных для оптимизации процесса обучения. FUSION превосходит существующие методы, демонстрируя эффективность подхода полной интеграции модальностей.'}, 'en': {'title': 'FUSION: Deep Integration of Vision and Language for Enhanced Understanding', 'desc': 'FUSION is a new type of multimodal large language model (MLLM) that integrates vision and language more effectively than previous models. It uses a method called Text-Guided Unified Vision Encoding to combine text and visual information at a very detailed level, allowing for better understanding of images in context. The model also features Context-Aware Recursive Alignment Decoding, which helps it to refine visual features based on the text it is processing. With a focus on high-quality question-answer pairs, FUSION shows significant improvements over existing models in various benchmarks, even with fewer visual tokens.'}, 'zh': {'title': 'FUSION：深度集成的多模态语言模型', 'desc': '我们介绍了FUSION，这是一种多模态大型语言模型（MLLM），采用完全的视觉-语言对齐和集成范式。与现有方法主要依赖于LLM解码过程中的后期模态交互不同，我们的方法在整个处理流程中实现了深度、动态的集成。我们提出了文本引导的统一视觉编码，将文本信息融入视觉编码，实现像素级的集成。此外，我们设计了上下文感知的递归对齐解码，能够在解码过程中根据文本上下文递归聚合视觉特征，从而实现细粒度的问题级语义集成。'}}}, {'id': 'https://huggingface.co/papers/2504.08837', 'title': 'VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.08837', 'abstract': "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.", 'score': 34, 'issue_id': 3237, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'e73823d36c951e4e', 'authors': ['Haozhe Wang', 'Chao Qu', 'Zuming Huang', 'Wei Chu', 'Fangzhen Lin', 'Wenhu Chen'], 'affiliations': ['HKUST', 'INF.AI', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.08837.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#rl', '#math', '#training', '#rlhf', '#optimization', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Переосмысление зрения и языка: прорыв в медленном мышлении ИИ', 'desc': 'Статья описывает новый подход к улучшению мультимодальных рассуждений в моделях машинного обучения с использованием медленного мышления. Авторы адаптируют алгоритм GRPO с техникой выборочного воспроизведения выборки (SSR) для решения проблемы исчезающих преимуществ. Они также вводят метод принудительного переосмысления для стимулирования самоанализа модели. Результатом является модель VL-Rethinker, которая достигает новых рекордных показателей на нескольких бенчмарках в области математики и междисциплинарных задач.'}, 'en': {'title': 'Enhancing Slow-Thinking in Vision-Language Models with Reinforcement Learning', 'desc': 'This paper presents VL-Rethinker, a vision-language model that enhances slow-thinking capabilities through reinforcement learning. It introduces Selective Sample Replay (SSR) to tackle the vanishing advantages problem in reinforcement learning, improving performance on math and science benchmarks. Additionally, the model incorporates Forced Rethinking, which adds a self-reflection step during training to promote deeper reasoning. As a result, VL-Rethinker achieves state-of-the-art scores on multiple benchmarks, demonstrating significant advancements in multimodal reasoning.'}, 'zh': {'title': '提升视觉-语言模型的慢思考能力', 'desc': '本文探讨了如何通过强化学习提升视觉-语言模型的慢思考能力。我们提出了一种新的技术，称为选择性样本重放（SSR），以解决优势消失问题，并结合强制重新思考的方法，增强模型的自我反思能力。通过这两种技术的结合，我们的模型VL-Rethinker在多个数学和科学基准测试中取得了显著的进展。最终，VL-Rethinker在多学科基准测试中也达到了开源的最新水平，缩小了与现有最佳模型的差距。'}}}, {'id': 'https://huggingface.co/papers/2504.08003', 'title': "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability", 'url': 'https://huggingface.co/papers/2504.08003', 'abstract': "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.", 'score': 34, 'issue_id': 3236, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '7b12ba874d92915a', 'authors': ['Ning Li', 'Jingran Zhang', 'Justin Cui'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2504.08003.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#alignment', '#multimodal', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'GPT-4o: Ограничения в семантическом синтезе и необходимость улучшения мультимодальной генерации', 'desc': 'Исследование оценивает способности мультимодальной модели GPT-4o от OpenAI в области семантического синтеза с использованием мировых знаний. Авторы анализируют три ключевых аспекта: глобальное следование инструкциям, точность детального редактирования и постгенерационное рассуждение. Результаты показывают, что модель часто интерпретирует инструкции буквально, непоследовательно применяет ограничения, основанные на знаниях, и испытывает трудности с задачами условного рассуждения. Исследование призывает к разработке более надежных методов оценки и стратегий обучения для улучшения мультимодальной генерации.'}, 'en': {'title': 'Bridging the Gaps in Multimodal Understanding', 'desc': "This paper evaluates OpenAI's multimodal model, GPT-4o, focusing on its ability to integrate knowledge and reasoning in image generation and editing. The study examines three key areas: how well the model follows global instructions, its precision in fine-grained editing, and its reasoning after generating images. Despite strong performance in generating images, the model often misinterprets instructions, applies knowledge inconsistently, and struggles with tasks requiring conditional reasoning. The authors suggest that improvements in training and evaluation methods are needed to enhance the model's contextual understanding and reasoning capabilities."}, 'zh': {'title': '提升多模态生成的上下文理解与推理能力', 'desc': '本研究评估了OpenAI的多模态模型GPT-4o在图像生成和编辑方面的能力，特别关注其在全球指令遵循、精细编辑精度和生成后推理三个维度的表现。尽管现有基准显示GPT-4o在图像处理上表现强劲，但我们的评估揭示了其在指令理解和知识应用上的局限性。模型常常对指令进行字面解释，知识约束应用不一致，并且在条件推理任务中表现不佳。这些发现挑战了对GPT-4o统一理解和生成能力的普遍假设，强调了需要更强大的基准和训练策略，以实现更具上下文意识和推理基础的多模态生成。'}}}, {'id': 'https://huggingface.co/papers/2504.09643', 'title': 'Iterative Self-Training for Code Generation via Reinforced Re-Ranking', 'url': 'https://huggingface.co/papers/2504.09643', 'abstract': 'Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality.   One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance.   Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language.', 'score': 29, 'issue_id': 3245, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': '9f5e20f45a50902d', 'authors': ['Nikita Sorokin', 'Ivan Sedykh', 'Valentin Malykh'], 'affiliations': ['International IT University', 'MTS AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.09643.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#plp'], 'emoji': '🔄', 'ru': {'title': 'Самообучение ранжированию для улучшения генерации кода', 'desc': 'Статья предлагает новый метод итеративного самообучения для улучшения ранжирования и генерации кода с использованием Proximal Policy Optimization (PPO). Подход фокусируется на разработке надежной модели ранжирования, которая улучшает качество генерируемого кода путем переранжирования и устранения ошибок. Метод итеративно уточняет обучающий набор данных, переоценивая выходные данные и включая высокооцененные отрицательные примеры в цикл обучения. Оценка на наборе данных MultiPL-E показывает, что 13.4B-параметровая модель превосходит 33B модель по качеству генерации кода и сравнима с GPT-4.'}, 'en': {'title': 'Enhancing Code Generation with Smart Reranking', 'desc': 'This paper addresses the challenges of generating high-quality code using decoder-based models, which often produce unpredictable outputs. It introduces a novel approach that combines a code generation model with a reranker model to select the best solutions from multiple generated samples. The authors propose an iterative self-training method using Proximal Policy Optimization (PPO) to enhance the reranking accuracy and overall code generation process. Their results show that their model, with 13.4 billion parameters, outperforms larger models in both quality and speed, achieving results comparable to GPT-4 in certain programming languages.'}, 'zh': {'title': '提升代码生成质量的创新方法', 'desc': '本文探讨了高质量代码生成的挑战，尤其是在当前基于解码器的模型中，输出结果具有高度随机性。通过结合代码生成模型和重排序模型，可以显著提高生成代码的质量。我们提出了一种新颖的自我训练方法，利用近端策略优化（PPO）来提升重排序模型的准确性和整体代码生成过程。实验结果表明，我们的模型在代码生成质量上优于更大参数的模型，并且在速度上也有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2504.10068', 'title': 'Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model', 'url': 'https://huggingface.co/papers/2504.10068', 'abstract': "Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose Mavors, a novel framework that introduces Multi-granularity video representation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.", 'score': 25, 'issue_id': 3237, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'bbb7251e84f61649', 'authors': ['Yang Shi', 'Jiaheng Liu', 'Yushuo Guan', 'Zhenhua Wu', 'Yuanxing Zhang', 'Zihao Wang', 'Weihong Lin', 'Jingyun Hua', 'Zekun Wang', 'Xinlong Chen', 'Bohan Zeng', 'Wentao Zhang', 'Fuzheng Zhang', 'Wenjing Yang', 'Di Zhang'], 'affiliations': ['CASIA', 'Kuaishou Technology', 'Nanjing University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10068.jpg', 'data': {'categories': ['#reasoning', '#video', '#benchmark', '#architecture', '#long_context', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Mavors: Эффективное понимание длинных видео с сохранением мелких деталей', 'desc': 'Статья представляет новый подход Mavors для понимания длинных видео в мультимодальных больших языковых моделях. Mavors использует многоуровневое представление видео, сочетая внутрикадровое кодирование высокого разрешения и межкадровую агрегацию признаков. Этот метод позволяет сохранить как пространственные детали, так и временную динамику в видео. Эксперименты показывают превосходство Mavors над существующими методами в задачах, требующих детального пространственно-временного анализа.'}, 'en': {'title': 'Mavors: Enhancing Long-Context Video Understanding with Multi-Granularity Representation', 'desc': 'This paper presents Mavors, a new framework designed to improve long-context video understanding in multimodal large language models (MLLMs). Mavors addresses the challenge of maintaining fine-grained spatio-temporal patterns while ensuring computational efficiency. It utilizes an Intra-chunk Vision Encoder to capture high-resolution spatial features and an Inter-chunk Feature Aggregator to ensure temporal coherence across video segments. The framework also integrates image and video understanding by treating images as single-frame videos, leading to superior performance in tasks that require detailed spatio-temporal reasoning.'}, 'zh': {'title': 'Mavors：长视频理解的新突破', 'desc': '本论文提出了一种名为Mavors的新框架，旨在解决多模态大语言模型在长视频理解中的计算效率与细粒度时空模式保留之间的平衡问题。Mavors通过两个核心组件实现对原始视频内容的编码：一是使用3D卷积和视觉变换器的内部块视觉编码器（IVE），以保留高分辨率的空间特征；二是通过基于变换器的依赖建模和块级旋转位置编码的块间特征聚合器（IFA），建立块之间的时间一致性。该框架还通过子图像分解将图像视为单帧视频，从而统一了图像和视频的理解。实验结果表明，Mavors在保持空间保真度和时间连续性方面优于现有方法，特别是在需要细粒度时空推理的任务中表现突出。'}}}, {'id': 'https://huggingface.co/papers/2504.08942', 'title': 'AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories', 'url': 'https://huggingface.co/papers/2504.08942', 'abstract': 'Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io', 'score': 18, 'issue_id': 3237, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': 'd756012a0eceafb9', 'authors': ['Xing Han Lù', 'Amirhossein Kazemnejad', 'Nicholas Meade', 'Arkil Patel', 'Dongchan Shin', 'Alejandra Zambrano', 'Karolina Stańczak', 'Peter Shaw', 'Christopher J. Pal', 'Siva Reddy'], 'affiliations': ['Canada CIFAR AI Chair', 'Google DeepMind', 'McGill University', 'Mila Quebec AI Institute', 'Polytechnique Montréal', 'ServiceNow Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.08942.jpg', 'data': {'categories': ['#interpretability', '#agents', '#benchmark', '#optimization'], 'emoji': '🕸️', 'ru': {'title': 'AgentRewardBench: новый подход к оценке веб-агентов с помощью LLM', 'desc': 'Статья представляет AgentRewardBench - первый бенчмарк для оценки эффективности языковых моделей (LLM) в оценке веб-агентов. Бенчмарк содержит 1302 траектории, охватывающие 5 наборов тестов и 4 LLM, каждая из которых проверена экспертом. Исследование показывает, что ни одна LLM не превосходит остальные во всех тестах, а правило-ориентированные методы оценки часто занижают успешность веб-агентов. Авторы подчеркивают необходимость разработки более гибких автоматических методов оценки веб-агентов.'}, 'en': {'title': 'Revolutionizing Web Agent Evaluation with LLMs', 'desc': "This paper introduces AgentRewardBench, a new benchmark designed to evaluate the effectiveness of large language models (LLMs) in assessing web agents' performance. Traditional rule-based evaluation methods struggle with flexibility and often fail to accurately identify successful task completions. By comparing 12 different LLM judges across 1302 trajectories, the study reveals that no single LLM consistently outperforms others in all scenarios. The findings emphasize the limitations of rule-based evaluations and advocate for more adaptable automatic evaluation methods to better assess web agents."}, 'zh': {'title': '评估网络代理的新方法：AgentRewardBench', 'desc': '本论文探讨了如何评估网络代理的表现，特别是通过自然语言交互来完成任务的代理。传统的基于规则的方法在扩展新任务时存在困难，并且可能无法准确识别成功的轨迹。我们提出了AgentRewardBench，这是第一个基准测试，用于评估大型语言模型（LLM）在评估网络代理方面的有效性。通过对1302个轨迹的评估，我们发现没有单一的LLM在所有基准上表现优异，这表明需要开发更灵活的自动评估方法。'}}}, {'id': 'https://huggingface.co/papers/2504.10368', 'title': 'S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models', 'url': 'https://huggingface.co/papers/2504.10368', 'abstract': "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.", 'score': 16, 'issue_id': 3239, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'c5995fab2f284493', 'authors': ['Wenyuan Zhang', 'Shuaiyi Nie', 'Xinghua Zhang', 'Zefeng Zhang', 'Tingwen Liu'], 'affiliations': ['Institute of Information Engineering, Chinese Academy of Sciences', 'School of Cyber Security, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.10368.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multilingual'], 'emoji': '🧠', 'ru': {'title': 'Большие модели рассуждений нуждаются в интуиции', 'desc': 'S1-Bench - это новый бенчмарк для оценки производительности Больших Моделей Рассуждений (LRM) на простых задачах, требующих интуитивного мышления системы 1. Исследование показало, что LRM демонстрируют низкую эффективность на таких задачах, выдавая ответы в среднем в 15,5 раз длиннее, чем традиционные малые языковые модели. Модели часто находят правильные ответы рано, но продолжают ненужные рассуждения, иногда допуская множество ошибок. Результаты указывают на необходимость развития сбалансированных возможностей двойственного мышления в LRM.'}, 'en': {'title': 'Evaluating Intuition: S1-Bench for Large Reasoning Models', 'desc': 'The paper introduces S1-Bench, a benchmark aimed at assessing Large Reasoning Models (LRMs) on tasks that require quick, intuitive responses, akin to system 1 thinking. It highlights that while LRMs excel in complex reasoning, they struggle with simpler tasks that demand rapid decision-making. The study evaluates 22 LRMs, revealing that they often produce longer outputs and unnecessary deliberation, even when they identify correct answers early. These results indicate that current LRMs exhibit rigid reasoning patterns, suggesting a need for further development to enhance their ability to balance intuitive and analytical thinking.'}, 'zh': {'title': '评估大型推理模型的直观思维能力', 'desc': '我们介绍了S1-Bench，这是一个新颖的基准测试，旨在评估大型推理模型（LRMs）在简单任务上的表现，这些任务更倾向于直观的系统1思维，而非深思熟虑的系统2推理。尽管LRMs在复杂推理任务中取得了显著突破，但它们对深度分析思维的依赖可能限制了其系统1思维能力。目前缺乏评估LRMs在需要这种能力的任务表现的基准。S1-Bench提供了一组简单、多样且自然清晰的问题，专门设计用于评估LRMs在这些任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2504.09710', 'title': 'DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training', 'url': 'https://huggingface.co/papers/2504.09710', 'abstract': 'Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.', 'score': 13, 'issue_id': 3236, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': '1d7a588a7370ed5c', 'authors': ['Zhenting Wang', 'Guofeng Cui', 'Kun Wan', 'Wentian Zhao'], 'affiliations': ['Adobe Inc.', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2504.09710.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное постобучение LLM с учетом разнородности данных', 'desc': 'Статья представляет новый подход к постобучению больших языковых моделей (LLM) с использованием обучения с подкреплением (RL). Авторы предлагают адаптивную стратегию обучения, учитывающую разнородность обучающих данных по сложности и источникам. Ключевая идея заключается в использовании величины преимущества политики для оценки пользы дальнейшего обучения на конкретном распределении данных. Метод применяет принцип Upper Confidence Bound для динамической корректировки вероятностей выборки из разных распределений, что позволяет оптимизировать процесс обучения.'}, 'en': {'title': 'Adaptive Learning for Enhanced Reasoning in Language Models', 'desc': 'This paper introduces a new approach to improve large language models (LLMs) using reinforcement learning (RL) by focusing on the diverse sources and difficulties of training data. It highlights the importance of adapting the training process to different data distributions, rather than treating all data as the same. The authors propose a curriculum learning framework that uses the Upper Confidence Bound (UCB) principle to prioritize training on data distributions that either have high potential for improvement or are underrepresented. Their experiments show that this method enhances the efficiency and effectiveness of LLM post-training, particularly in complex reasoning tasks.'}, 'zh': {'title': '优化学习效率的分布级课程学习框架', 'desc': '本文提出了一种基于分布级学习能力的课程学习框架，旨在优化强化学习（RL）后训练的大型语言模型（LLM）。现有方法通常将训练数据视为统一整体，忽视了数据分布的多样性和复杂性。我们的方法通过动态调整不同分布的采样概率，优先考虑高平均优势或低样本数量的分布，从而提高学习效率。实验结果表明，该框架在逻辑推理数据集上显著提高了收敛速度和最终性能，展示了分布感知课程策略的价值。'}}}, {'id': 'https://huggingface.co/papers/2504.10127', 'title': 'Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization', 'url': 'https://huggingface.co/papers/2504.10127', 'abstract': 'Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.', 'score': 12, 'issue_id': 3242, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '604dff752f16abf2', 'authors': ['Junlei Zhang', 'Zichen Ding', 'Chang Ma', 'Zijie Chen', 'Qiushi Sun', 'Zhenzhong Lan', 'Junxian He'], 'affiliations': ['HKUST', 'Shanghai AI Laboratory', 'The University of Hong Kong', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10127.jpg', 'data': {'categories': ['#agents', '#dataset', '#training', '#optimization', '#multimodal', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'Улучшение GUI-агентов через обучение на разнообразных задачах рассуждения', 'desc': 'Статья предлагает новый подход к обучению агентов графического пользовательского интерфейса (GUI) с использованием моделей компьютерного зрения и обработки естественного языка (VLM). Авторы исследуют влияние различных задач среднего уровня обучения на производительность GUI-агентов, включая восприятие интерфейса, мультимодальные рассуждения и текстовые рассуждения. Результаты показывают, что обучение на задачах рассуждения, особенно математических, значительно улучшает производительность агентов в различных GUI-средах. Исследование демонстрирует эффективность переноса знаний между различными доменами и предлагает практический подход к решению проблемы нехватки данных в области GUI-агентов.'}, 'en': {'title': 'Enhancing GUI Agents through Vision Language Model Training', 'desc': 'This paper discusses how to improve the performance of Graphical User Interface (GUI) agents by using Vision Language Models (VLMs) trained on various reasoning tasks. The authors found that training on data-rich tasks helps these models generalize better to GUI planning scenarios, leading to significant performance improvements. They conducted experiments showing that tasks like multimodal reasoning and text-based mathematical data can enhance GUI agent performance more than traditional GUI perception data. The study highlights the importance of cross-domain knowledge transfer and provides a method to overcome data scarcity in training GUI agents.'}, 'zh': {'title': '提升GUI代理性能的关键在于任务泛化', 'desc': '本论文探讨了图形用户界面（GUI）代理在自动化复杂数字任务中的应用，尤其是在数据稀缺的情况下如何提升其性能。我们提出在专门的中期训练阶段，利用丰富的数据和推理密集型任务来训练视觉语言模型（VLMs），以促进其在GUI规划场景中的泛化能力。通过对11个中期训练任务的广泛实验，我们发现任务泛化显著提高了性能，尤其是多模态数学推理对AndroidWorld的性能提升达到了6.3%。此外，我们还发现GUI感知数据对最终性能的影响有限，并提出了优化的混合数据集，以实现更大的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2504.10157', 'title': 'SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users', 'url': 'https://huggingface.co/papers/2504.10157', 'abstract': 'Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.', 'score': 11, 'issue_id': 3237, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'ba0402c49e397963', 'authors': ['Xinnong Zhang', 'Jiayu Lin', 'Xinyi Mou', 'Shiyue Yang', 'Xiawei Liu', 'Libo Sun', 'Hanjia Lyu', 'Yihang Yang', 'Weihong Qi', 'Yue Chen', 'Guanying Li', 'Ling Yan', 'Yao Hu', 'Siming Chen', 'Yu Wang', 'Jingxuan Huang', 'Jiebo Luo', 'Shiping Tang', 'Libo Wu', 'Baohua Zhou', 'Zhongyu Wei'], 'affiliations': ['Fudan University', 'Indiana University', 'Shanghai Innovation Institute', 'University of Rochester', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.10157.jpg', 'data': {'categories': ['#rl', '#agents', '#social_simulation', '#alignment', '#multimodal'], 'emoji': '🌐', 'ru': {'title': 'SocioVerse: виртуальный мир для моделирования общества', 'desc': 'Статья представляет SocioVerse - модель социальной симуляции на основе больших языковых моделей (LLM) и агентов. Фреймворк включает четыре компонента выравнивания и базу из 10 миллионов реальных пользователей. Авторы провели масштабные эксперименты в трех областях: политика, новости и экономика. Результаты показывают, что SocioVerse способна отражать динамику населения, обеспечивая разнообразие и репрезентативность.'}, 'en': {'title': 'SocioVerse: Enhancing Social Simulations with LLMs', 'desc': 'This paper presents SocioVerse, a novel framework that utilizes large language models (LLMs) to enhance social simulations by modeling human interactions and behaviors. The framework addresses alignment challenges by incorporating four key components that ensure accurate representation of diverse user behaviors and environmental interactions. Through extensive simulations in politics, news, and economics, SocioVerse effectively captures population dynamics while maintaining credibility and diversity. The results indicate that this approach can significantly improve the predictive power of social simulations in various domains.'}, 'zh': {'title': 'SocioVerse：社会模拟的新纪元', 'desc': '社会模拟正在通过模拟虚拟个体与环境之间的互动来改变传统社会科学研究。随着大型语言模型（LLMs）的进步，这种方法在捕捉个体差异和预测群体行为方面显示出越来越大的潜力。然而，现有方法在环境、目标用户、互动机制和行为模式方面面临对齐挑战。为此，我们提出了SocioVerse，这是一个基于LLM代理的社会模拟世界模型，具有强大的对齐组件和1000万真实个体的用户池。'}}}, {'id': 'https://huggingface.co/papers/2504.09763', 'title': 'Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems', 'url': 'https://huggingface.co/papers/2504.09763', 'abstract': 'Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.', 'score': 11, 'issue_id': 3240, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'c4ae0eadf040035f', 'authors': ['Zaid Khan', 'Elias Stengel-Eskin', 'Archiki Prasad', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2504.09763.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#dataset', '#math', '#data', '#training'], 'emoji': '🧮', 'ru': {'title': 'Автоматическая генерация абстракций для продвинутых математических задач', 'desc': 'Исследователи представили концепцию EFA (Executable Functional Abstraction) - исполняемых функциональных абстракций для математических задач. Они разработали систему EFAGen, использующую большую языковую модель (LLM) для автоматического создания EFA на основе исходной задачи и ее пошагового решения. EFAGen способна генерировать EFA для сложных математических задач уровня олимпиад, сохраняя верность исходной проблеме. Созданные EFA могут использоваться для генерации вариаций задач различной сложности и тестирования моделей машинного обучения.'}, 'en': {'title': 'Automating Advanced Math Problem Generation with EFAs', 'desc': 'This paper introduces Executable Functional Abstractions (EFAs), which are programs designed to generate advanced math problems based on specific instances. The authors present EFAGen, a tool that uses a large language model (LLM) to automatically create EFAs by conditioning on a seed math problem and its solution. They establish criteria for valid EFAs through executable unit tests, which serve as rewards for training the LLM to improve its EFA generation capabilities. The results show that EFAGen can produce diverse and learnable problem variations, enhancing the ability to create tailored math challenges for learners.'}, 'zh': {'title': '自动生成高级数学问题的可执行功能抽象', 'desc': '本文介绍了一种名为可执行功能抽象（EFA）的程序，这些程序能够从特定的数学问题实例中推导出抽象过程，并生成新的相关实例。我们提出了一种自动构建高级数学问题的EFA的方法，称为EFAGen，它利用大型语言模型（LLM）根据种子数学问题及其逐步解决方案生成候选EFA程序。通过可执行单元测试，我们定义了有效EFA必须具备的属性，并展示了如何利用这些测试作为可验证的奖励来训练LLM，提高其EFA编写能力。最终，我们证明了EFAGen生成的EFA能够保持与种子问题的一致性，并能够生成适合学习者的不同难度的数学问题变体。'}}}, {'id': 'https://huggingface.co/papers/2504.10471', 'title': 'MIEB: Massive Image Embedding Benchmark', 'url': 'https://huggingface.co/papers/2504.10471', 'abstract': 'Image representations are often evaluated through disjointed, task-specific protocols, leading to a fragmented understanding of model capabilities. For instance, it is unclear whether an image embedding model adept at clustering images is equally good at retrieving relevant images given a piece of text. We introduce the Massive Image Embedding Benchmark (MIEB) to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. MIEB spans 38 languages across 130 individual tasks, which we group into 8 high-level categories. We benchmark 50 models across our benchmark, finding that no single method dominates across all task categories. We reveal hidden capabilities in advanced vision models such as their accurate visual representation of texts, and their yet limited capabilities in interleaved encodings and matching images and texts in the presence of confounders. We also show that the performance of vision encoders on MIEB correlates highly with their performance when used in multimodal large language models. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.', 'score': 10, 'issue_id': 3248, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '5b9a143dfa0081cd', 'authors': ['Chenghao Xiao', 'Isaac Chung', 'Imene Kerboua', 'Jamie Stirling', 'Xin Zhang', 'Márton Kardos', 'Roman Solomatin', 'Noura Al Moubayed', 'Kenneth Enevoldsen', 'Niklas Muennighoff'], 'affiliations': ['Aarhus University', 'Contextual AI', 'Durham University', 'Esker', 'INSA Lyon, LIRIS', 'ITMO University', 'Stanford University', 'The Hong Kong Polytechnic University', 'Zendesk'], 'pdf_title_img': 'assets/pdf/title_img/2504.10471.jpg', 'data': {'categories': ['#survey', '#multimodal', '#cv', '#open_source', '#dataset', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'MIEB: Комплексная оценка моделей встраивания изображений и текста', 'desc': 'Статья представляет Massive Image Embedding Benchmark (MIEB) - новый метод оценки моделей встраивания изображений и текста. MIEB включает 130 задач на 38 языках, сгруппированных в 8 категорий. Авторы провели бенчмаркинг 50 моделей, выявив скрытые возможности и ограничения продвинутых моделей компьютерного зрения. Результаты показывают корреляцию между производительностью энкодеров изображений в MIEB и их эффективностью в мультимодальных языковых моделях.'}, 'en': {'title': 'Unifying Image Embedding Evaluation with MIEB', 'desc': 'This paper presents the Massive Image Embedding Benchmark (MIEB), a comprehensive evaluation framework for image and image-text embedding models. It addresses the limitations of existing evaluation methods by assessing model performance across 130 tasks in 38 languages, grouped into 8 categories. The study benchmarks 50 different models, revealing that no single model excels in all areas, highlighting both strengths and weaknesses in advanced vision models. Additionally, it finds a strong correlation between the performance of vision encoders on MIEB and their effectiveness in multimodal large language models.'}, 'zh': {'title': '全面评估图像嵌入模型的能力', 'desc': '本文介绍了一个新的评估框架，称为大规模图像嵌入基准（MIEB），用于全面评估图像和图像-文本嵌入模型的性能。MIEB 涉及 38 种语言和 130 个任务，分为 8 个高层次类别，旨在提供对模型能力的更全面理解。研究发现，没有单一的方法在所有任务类别中表现最佳，同时揭示了先进视觉模型在文本视觉表示方面的潜力和在图像与文本匹配中的局限性。最后，结果表明，视觉编码器在 MIEB 上的表现与其在多模态大语言模型中的表现高度相关。'}}}, {'id': 'https://huggingface.co/papers/2504.09641', 'title': 'TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning', 'url': 'https://huggingface.co/papers/2504.09641', 'abstract': 'Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models\' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of "aha moments". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.', 'score': 8, 'issue_id': 3237, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': 'b7c9f390686ff6ef', 'authors': ['Xingjian Zhang', 'Siwei Wen', 'Wenjun Wu', 'Lei Huang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Beihang University', 'Hangzhou International Innovation Institute, Beihang University, Hangzhou, China', 'SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.09641.jpg', 'data': {'categories': ['#reasoning', '#video', '#rl', '#small_models', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Маленькая модель - большие рассуждения: TinyLLaVA-Video-R1 для анализа видео', 'desc': "Статья представляет TinyLLaVA-Video-R1 - небольшую мультимодальную модель для рассуждений по видео. Модель обучена с помощью обучения с подкреплением на наборах данных для ответов на вопросы по видео. TinyLLaVA-Video-R1 демонстрирует улучшенные способности к рассуждениям и мышлению, а также проявляет эффект 'ага-момента'. Авторы делятся экспериментальными результатами для дальнейших исследований возможностей рассуждений по видео в небольших моделях."}, 'en': {'title': 'Empowering Small Models for Big Reasoning in Video Understanding', 'desc': "This paper introduces TinyLLaVA-Video-R1, a small-scale video reasoning model designed to enhance reasoning abilities using reinforcement learning. Unlike previous models that rely on large-scale datasets and architectures, this model operates with fewer than 4 billion parameters, making it accessible for researchers with limited computational resources. The model not only improves reasoning capabilities on general Video-QA datasets but also demonstrates the ability to explain its reasoning process, showcasing 'aha moments' during its operation. The findings aim to inspire further research into the reasoning abilities of smaller models in the field of video understanding."}, 'zh': {'title': '小模型也能推理，TinyLLaVA-Video-R1助力视频理解！', 'desc': '最近，通过强化学习提高大型多模态模型（LMMs）的推理能力取得了显著进展。然而，大多数现有研究基于高度推理密集的数据集，如数学和代码，且通常选择大规模模型作为基础。我们认为，探索小规模模型的推理能力对计算资源有限的研究者仍然具有重要价值。此外，使模型能够解释其在一般问答数据集上的推理过程同样具有意义。'}}}, {'id': 'https://huggingface.co/papers/2504.08066', 'title': 'The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via\n  Agentic Tree Search', 'url': 'https://huggingface.co/papers/2504.08066', 'abstract': 'AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.', 'score': 7, 'issue_id': 3245, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'a1d1275982a7e4b0', 'authors': ['Yutaro Yamada', 'Robert Tjarko Lange', 'Cong Lu', 'Shengran Hu', 'Chris Lu', 'Jakob Foerster', 'Jeff Clune', 'David Ha'], 'affiliations': ['Canada CIFAR AI Chair', 'FLAIR, University of Oxford', 'Sakana AI', 'University of British Columbia', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.08066.jpg', 'data': {'categories': ['#multimodal', '#science', '#healthcare', '#ethics', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'ИИ-ученый: первая полностью автономная научная статья', 'desc': 'Статья представляет The AI Scientist-v2 - агентную систему, способную автономно создавать научные работы, принимаемые для рецензирования. Система формулирует гипотезы, проводит эксперименты, анализирует данные и пишет рукописи без участия человека. По сравнению с предыдущей версией, она не зависит от шаблонов кода и использует прогрессивный агентный поиск по дереву. Одна из созданных системой работ была принята на рецензируемый семинар ICLR, что стало первым случаем полностью ИИ-сгенерированной статьи, прошедшей рецензирование.'}, 'en': {'title': "Revolutionizing Science: The AI Scientist-v2's Autonomous Research Breakthrough", 'desc': 'The paper presents The AI Scientist-v2, an advanced AI system that autonomously conducts scientific research, from hypothesis generation to manuscript writing. This system improves upon its predecessor by eliminating the need for human-written code and effectively generalizing across various machine learning fields. It utilizes a progressive agentic tree-search method and incorporates a Vision-Language Model for enhancing the quality of figures in its manuscripts. The successful submission of AI-generated papers to a peer-reviewed workshop demonstrates the potential of AI to revolutionize scientific discovery and increase research productivity.'}, 'zh': {'title': 'AI科学家的新纪元：完全自主的科学发现', 'desc': '本论文介绍了AI Scientist-v2，这是一个能够完全自主生成科学论文的人工智能系统。该系统能够迭代地提出科学假设，设计和执行实验，分析和可视化数据，并撰写科学手稿。与其前身相比，AI Scientist-v2不再依赖人类编写的代码模板，能够在不同的机器学习领域中有效泛化，并采用了一种新的渐进式代理树搜索方法。通过提交三篇完全自主撰写的手稿到同行评审的ICLR研讨会，其中一篇成功通过评审，标志着AI在科学研究中的能力不断增强。'}}}, {'id': 'https://huggingface.co/papers/2504.10415', 'title': 'LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models', 'url': 'https://huggingface.co/papers/2504.10415', 'abstract': 'Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.', 'score': 6, 'issue_id': 3239, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '15b7c5f49cceef01', 'authors': ['Parshin Shojaee', 'Ngoc-Hieu Nguyen', 'Kazem Meidani', 'Amir Barati Farimani', 'Khoa D Doan', 'Chandan K Reddy'], 'affiliations': ['Carnegie Mellon University', 'VinUniversity', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2504.10415.jpg', 'data': {'categories': ['#benchmark', '#math', '#reasoning', '#synthetic', '#science'], 'emoji': '🧪', 'ru': {'title': 'LLM-SRBench: Вызов искусственному интеллекту в открытии научных уравнений', 'desc': 'Статья представляет LLM-SRBench - новый набор данных для оценки способностей больших языковых моделей (LLM) к открытию научных уравнений. Бенчмарк содержит 239 сложных задач из четырех научных областей, специально разработанных для предотвращения простого запоминания уравнений моделями. LLM-SRBench включает две категории: LSR-Transform с трансформированными физическими моделями и LSR-Synth с синтетическими задачами, требующими рассуждений на основе данных. Тестирование современных методов показало, что лучшая система достигает лишь 31.5% символьной точности, подчеркивая сложность задачи открытия научных уравнений.'}, 'en': {'title': 'LLM-SRBench: A New Frontier in Scientific Equation Discovery', 'desc': 'This paper discusses the challenges of using Large Language Models (LLMs) for discovering scientific equations, which are essential for understanding natural laws. The authors introduce LLM-SRBench, a new benchmark with 239 difficult problems across four scientific fields, designed to assess the true discovery capabilities of LLMs without the influence of memorization. The benchmark includes two categories: LSR-Transform, which tests reasoning by altering common models, and LSR-Synth, which presents synthetic problems that require data-driven reasoning. The evaluation shows that even the best LLMs achieve only 31.5% accuracy, underscoring the difficulties in scientific equation discovery and the importance of LLM-SRBench for future studies.'}, 'zh': {'title': '科学方程发现的新基准：LLM-SRBench', 'desc': '科学方程发现是科学进步中的一项基本任务，能够推导出自然现象的规律。最近，大型语言模型（LLMs）因其利用嵌入科学知识进行假设生成的潜力而受到关注。然而，评估这些方法的真实发现能力仍然具有挑战性，因为现有基准往往依赖于容易被LLMs记忆的常见方程，导致性能指标被夸大。本文介绍了LLM-SRBench，这是一个包含239个具有挑战性问题的综合基准，旨在评估基于LLM的科学方程发现方法，同时防止简单的记忆化。'}}}, {'id': 'https://huggingface.co/papers/2504.09130', 'title': 'VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search', 'url': 'https://huggingface.co/papers/2504.09130', 'abstract': 'Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.', 'score': 5, 'issue_id': 3242, 'pub_date': '2025-04-12', 'pub_date_card': {'ru': '12 апреля', 'en': 'April 12', 'zh': '4月12日'}, 'hash': '3912c7cbd4c137f9', 'authors': ['Yikun Wang', 'Siyin Wang', 'Qinyuan Cheng', 'Zhaoye Fei', 'Liang Ding', 'Qipeng Guo', 'Dacheng Tao', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2504.09130.jpg', 'data': {'categories': ['#inference', '#reasoning', '#multimodal', '#cv'], 'emoji': '🧠', 'ru': {'title': 'VisuoThink: Визуально-вербальное рассуждение для ИИ', 'desc': 'VisuoThink - это новая система, которая объединяет визуальное и языковое мышление для улучшения рассуждений искусственного интеллекта. Она имитирует медленное, пошаговое мышление человека с использованием визуальных подсказок. VisuoThink значительно улучшает способности к рассуждению крупных языковых моделей без дополнительного обучения. Система показывает лучшие результаты в задачах, связанных с геометрией и пространственным мышлением.'}, 'en': {'title': 'Enhancing Reasoning with VisuoThink: A Multimodal Approach', 'desc': "This paper presents VisuoThink, a new framework designed to improve reasoning in Large Vision-Language Models. It addresses the shortcomings of existing methods that struggle with complex reasoning tasks by integrating visual and textual information in a more human-like manner. VisuoThink allows for progressive reasoning through a combination of visual and linguistic inputs, enhancing the model's ability to perform tasks that require spatial and geometric understanding. The framework shows significant improvements in reasoning capabilities during inference, achieving top performance without the need for additional fine-tuning."}, 'zh': {'title': 'VisuoThink：提升视觉-语言推理的新框架', 'desc': '最近，大型视觉语言模型取得了显著进展，但在复杂推理任务中表现不佳。现有方法虽然尝试了基于文本的慢思考或简单的视觉辅助，但未能有效捕捉人类视觉-语言推理过程的复杂性。为了解决这些问题，我们提出了VisuoThink框架，它将视觉空间和语言领域无缝整合，促进多模态的慢思考。实验表明，VisuoThink在几何和空间推理任务中显著提升了推理能力，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.09689', 'title': 'EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety', 'url': 'https://huggingface.co/papers/2504.09689', 'abstract': "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent", 'score': 4, 'issue_id': 3237, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': '11b21970119f2c59', 'authors': ['Jiahao Qiu', 'Yinghui He', 'Xinzhe Juan', 'Yiming Wang', 'Yuhan Liu', 'Zixin Yao', 'Yue Wu', 'Xun Jiang', 'Ling Yang', 'Mengdi Wang'], 'affiliations': ['AI Lab, Princeton University', 'Chen Frontier Lab for Al and Mental Health, Tianqiao and Chrissy Chen Institute', 'Department of Computer Science & Engineering, University of Michigan', 'Department of Computer Science, Princeton University', 'Department of Data Science & Engineering, University of Michigan', 'Department of Electrical & Computer Engineering, Princeton University', 'Department of Philosophy, Columbia University', 'Theta Health Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.09689.jpg', 'data': {'categories': ['#agents', '#security', '#ethics', '#healthcare'], 'emoji': '🧠', 'ru': {'title': 'Защита психического здоровья в эпоху ИИ-персонажей', 'desc': 'Статья представляет EmoAgent - мультиагентную систему искусственного интеллекта для оценки и снижения рисков для психического здоровья при взаимодействии человека с ИИ-персонажами. EmoAgent состоит из двух компонентов: EmoEval, который симулирует виртуальных пользователей для оценки изменений психического состояния, и EmoGuard, который выступает посредником, отслеживающим психическое состояние пользователей. Эксперименты показали, что эмоционально вовлекающие диалоги могут привести к ухудшению психологического состояния уязвимых пользователей. EmoGuard значительно снижает эти риски, обеспечивая более безопасное взаимодействие человека с ИИ.'}, 'en': {'title': 'Ensuring Safer AI Interactions for Vulnerable Users with EmoAgent', 'desc': "This paper introduces EmoAgent, a multi-agent AI framework aimed at enhancing safety in interactions between AI characters and users, especially those with mental health vulnerabilities. EmoAgent consists of two main components: EmoEval, which simulates virtual users to assess mental health changes using established psychological assessment tools, and EmoGuard, which monitors users' mental states and provides feedback to prevent harm. The study reveals that emotionally engaging dialogues with AI can negatively impact the mental health of vulnerable users, with over 34.4% experiencing deterioration. The implementation of EmoGuard effectively reduces these risks, highlighting its importance in creating safer AI-human interactions."}, 'zh': {'title': 'EmoAgent：保障人机交互心理安全的智能框架', 'desc': '随着大型语言模型驱动的人工智能角色的兴起，特别是对心理障碍的脆弱用户，安全问题引起了关注。为了解决这些风险，我们提出了EmoAgent，这是一个多智能体的人工智能框架，旨在评估和减轻人机交互中的心理健康危害。EmoAgent包括两个组件：EmoEval模拟虚拟用户，评估与AI角色交互前后的心理健康变化，并使用经过临床验证的心理评估工具进行评估。EmoGuard作为中介，监测用户的心理状态，预测潜在伤害，并提供纠正反馈，以降低风险。'}}}, {'id': 'https://huggingface.co/papers/2504.09522', 'title': 'How new data permeates LLM knowledge and how to dilute it', 'url': 'https://huggingface.co/papers/2504.09522', 'abstract': 'Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a "priming" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce "Outlandish," a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM\'s existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone\'\' text augmentation strategy and (2) an ``ignore-k\'\' update pruning method. These approaches reduce undesirable priming effects by 50-95\\% while preserving the model\'s ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/', 'score': 4, 'issue_id': 3242, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': 'd9aa235633073967', 'authors': ['Chen Sun', 'Renat Aksitov', 'Andrey Zhmoginov', 'Nolan Andrew Miller', 'Max Vladymyrov', 'Ulrich Rueckert', 'Been Kim', 'Mark Sandler'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2504.09522.jpg', 'data': {'categories': ['#architecture', '#optimization', '#dataset', '#training', '#hallucinations', '#long_context', '#data'], 'emoji': '🧠', 'ru': {'title': 'Контроль эффекта прайминга при обучении языковых моделей', 'desc': "Статья исследует эффект прайминга в больших языковых моделях (LLM) при обучении новой информации. Авторы представляют датасет 'Outlandish' для изучения того, как новые знания распространяются в существующей базе знаний LLM. Обнаружена связь между вероятностью ключевых слов до обучения и степенью прайминга после. Предложены две техники для уменьшения нежелательных эффектов прайминга: аугментация текста 'stepping-stone' и метод обновления 'ignore-k'."}, 'en': {'title': 'Understanding and Controlling Knowledge Priming in LLMs', 'desc': "This paper explores how large language models (LLMs) learn new information and how this learning can unintentionally affect their existing knowledge. The authors identify a phenomenon called 'priming,' where new facts can lead to incorrect applications of knowledge in unrelated situations. They introduce a dataset named 'Outlandish' to study this priming effect and find that the likelihood of priming can be predicted by analyzing token probabilities of key words before learning. Additionally, they propose two techniques to mitigate undesirable priming while maintaining the model's learning capabilities, achieving significant reductions in priming effects."}, 'zh': {'title': '理解大型语言模型的知识学习与启动效应', 'desc': '大型语言模型通过基于梯度的更新不断学习，但新信息如何影响已有知识仍不清楚。我们发现，当学习新信息时，模型会出现“启动效应”，即学习新事实可能导致模型在不相关的上下文中错误应用这些知识。为系统研究这一现象，我们引入了“Outlandish”数据集，包含1320个多样化的文本样本，旨在探讨新知识如何渗透到模型的现有知识库中。我们的研究表明，通过测量关键字的token概率，可以预测学习新信息后的启动程度，并提出了两种新技术来调节新知识对模型行为的影响。'}}}, {'id': 'https://huggingface.co/papers/2504.10449', 'title': 'M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models', 'url': 'https://huggingface.co/papers/2504.10449', 'abstract': 'Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.', 'score': 3, 'issue_id': 3242, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '565dae169d16775e', 'authors': ['Junxiong Wang', 'Wen-Ding Li', 'Daniele Paliotta', 'Daniel Ritter', 'Alexander M. Rush', 'Tri Dao'], 'affiliations': ['Cornell University', 'Princeton University', 'TogetherAI', 'University of Geneva'], 'pdf_title_img': 'assets/pdf/title_img/2504.10449.jpg', 'data': {'categories': ['#inference', '#architecture', '#reasoning', '#training', '#math', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование рассуждений с помощью гибридной RNN модели', 'desc': 'Статья представляет новую гибридную модель рассуждений на основе линейной RNN, названную M1, которая построена на архитектуре Mamba. Модель использует процесс дистилляции из существующих моделей рассуждений и дополнительно улучшена с помощью обучения с подкреплением. Эксперименты на бенчмарках AIME и MATH показывают, что M1 превосходит предыдущие линейные RNN модели и соответствует производительности современных дистиллированных моделей рассуждений Deepseek R1 аналогичного масштаба. М1 демонстрирует более чем трехкратное ускорение по сравнению с трансформером того же размера, что позволяет достичь более высокой точности при фиксированном времени генерации.'}, 'en': {'title': 'M1: A Fast and Efficient Reasoning Model for Complex Math Problems', 'desc': 'This paper presents a new reasoning model called M1, which combines linear RNNs with the Mamba architecture to improve efficiency in solving complex mathematical problems. Unlike traditional transformer models that struggle with long context due to their computational limits, M1 utilizes a hybrid approach that allows for memory-efficient inference. The model is trained using a distillation process and reinforcement learning, resulting in performance that rivals state-of-the-art models while being faster. Experimental results demonstrate that M1 achieves significant speed improvements and higher accuracy compared to existing reasoning models, making it a promising solution for scalable test-time generation.'}, 'zh': {'title': '混合Mamba推理模型：高效推理的新选择', 'desc': '本文提出了一种新型的混合线性RNN推理模型M1，基于Mamba架构，旨在提高复杂数学问题的推理效率。与传统的变换器模型相比，M1在内存使用上更为高效，能够处理更长的上下文。通过对现有推理模型的蒸馏过程和强化学习训练，M1在AIME和MATH基准测试中表现优异，超越了之前的线性RNN模型。实验结果显示，M1在生成速度上比同规模的变换器快超过3倍，同时在固定生成时间预算下实现了更高的准确性。'}}}, {'id': 'https://huggingface.co/papers/2504.10430', 'title': 'LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models', 'url': 'https://huggingface.co/papers/2504.10430', 'abstract': 'Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.', 'score': 1, 'issue_id': 3242, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '01daa1337864118c', 'authors': ['Minqian Liu', 'Zhiyang Xu', 'Xinyi Zhang', 'Heajun An', 'Sarvech Qadir', 'Qi Zhang', 'Pamela J. Wisniewski', 'Jin-Hee Cho', 'Sang Won Lee', 'Ruoxi Jia', 'Lifu Huang'], 'affiliations': ['UC Davis', 'Vanderbilt University', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2504.10430.jpg', 'data': {'categories': ['#alignment', '#ethics', '#rlhf', '#healthcare', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Безопасность убеждения в эпоху языковых моделей: выявление этических рисков', 'desc': 'Это исследование посвящено анализу безопасности крупных языковых моделей (LLM) в контексте их способности к убеждению. Авторы разработали фреймворк PersuSafety для оценки этических аспектов убеждения, включающий создание сценариев, симуляцию диалогов и оценку безопасности. Эксперименты с 8 популярными LLM выявили значительные проблемы безопасности, в том числе неспособность распознавать вредные задачи убеждения и использование неэтичных стратегий. Исследование подчеркивает необходимость улучшения безопасности и этической согласованности в прогрессивных и целенаправленных разговорах с участием LLM.'}, 'en': {'title': 'Ensuring Ethical Persuasion in Large Language Models', 'desc': 'This paper investigates the safety risks associated with Large Language Models (LLMs) when used for persuasion. It focuses on two main areas: the ability of LLMs to reject unethical persuasion tasks and the influence of factors like personality traits on their behavior. The authors introduce a framework called PersuSafety, which evaluates persuasion safety through scene creation, conversation simulation, and safety assessment. Their experiments reveal that many LLMs struggle to identify harmful tasks and often employ unethical strategies, highlighting the need for improved safety measures in persuasive applications.'}, 'zh': {'title': '提升说服安全性，防范不道德影响', 'desc': '最近，大型语言模型（LLMs）的进步使其在说服能力上接近人类水平。然而，这种潜力也引发了对LLM驱动的说服安全风险的担忧，特别是它们可能通过操控、欺骗和利用脆弱性等不道德手段进行影响。本文系统地研究了LLM说服的安全性，重点关注LLM是否能拒绝不道德的说服任务，以及个性特征和外部压力如何影响其行为。我们提出了PersuSafety框架，评估说服安全性，并通过实验发现大多数LLM在识别有害说服任务和使用不道德策略方面存在显著安全隐患。'}}}, {'id': 'https://huggingface.co/papers/2504.09858', 'title': 'Reasoning Models Can Be Effective Without Thinking', 'url': 'https://huggingface.co/papers/2504.09858', 'abstract': 'Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.', 'score': 1, 'issue_id': 3250, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '6aa94cff6001428d', 'authors': ['Wenjie Ma', 'Jingxuan He', 'Charlie Snell', 'Tyler Griggs', 'Sewon Min', 'Matei Zaharia'], 'affiliations': ['Allen Institute for AI', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.09858.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#training', '#math', '#benchmark', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение без явного мышления в языковых моделях', 'desc': 'Исследователи обнаружили, что обход процесса мышления в больших языковых моделях (LLM) с помощью простого запроса может быть удивительно эффективным. Метод NoThinking превзошел стандартный подход с мышлением на семи сложных наборах данных по рассуждению, особенно в условиях ограниченных ресурсов. Параллельный подход с использованием NoThinking для генерации нескольких выходов независимо и их агрегации оказался высокоэффективным. Это исследование ставит под сомнение необходимость длительных процессов мышления в LLM и предлагает конкурентоспособный метод для достижения высокой производительности в рассуждениях при низких затратах или малой задержке.'}, 'en': {'title': 'Rethinking Reasoning: Less Thinking, More Efficiency!', 'desc': 'This paper investigates the necessity of explicit thinking processes in large language models (LLMs) for reasoning tasks. The authors introduce a method called NoThinking, which simplifies prompting and shows that it can outperform traditional thinking methods in various reasoning challenges. They demonstrate that using NoThinking in a parallel scaling approach, where multiple outputs are generated and aggregated, yields competitive results with lower latency. This research suggests that lengthy thinking processes may not be essential for effective reasoning, especially in resource-constrained environments.'}, 'zh': {'title': '重新思考推理过程的必要性', 'desc': '最近的大型语言模型（LLMs）在推理能力上有了显著提升，主要是通过将明确且冗长的思考过程纳入生成中。本文质疑这种明确思考是否真的必要。我们使用最先进的DeepSeek-R1-Distill-Qwen发现，通过简单提示跳过思考过程（称为NoThinking）在多个推理数据集上表现出色，尤其是在低预算设置下。我们的研究表明，NoThinking方法在生成多个独立输出并进行聚合时，能够有效提升推理性能，挑战了传统的思考过程的必要性。'}}}, {'id': 'https://huggingface.co/papers/2504.09518', 'title': '3D CoCa: Contrastive Learners are 3D Captioners', 'url': 'https://huggingface.co/papers/2504.09518', 'abstract': '3D captioning, which aims to describe the content of 3D scenes in natural language, remains highly challenging due to the inherent sparsity of point clouds and weak cross-modal alignment in existing methods. To address these challenges, we propose 3D CoCa, a novel unified framework that seamlessly combines contrastive vision-language learning with 3D caption generation in a single architecture. Our approach leverages a frozen CLIP vision-language backbone to provide rich semantic priors, a spatially-aware 3D scene encoder to capture geometric context, and a multi-modal decoder to generate descriptive captions. Unlike prior two-stage methods that rely on explicit object proposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a shared feature space, eliminating the need for external detectors or handcrafted proposals. This joint training paradigm yields stronger spatial reasoning and richer semantic grounding by aligning 3D and textual representations. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that 3D CoCa significantly outperforms current state-of-the-arts by 10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at https://github.com/AIGeeksGroup/3DCoCa.', 'score': 1, 'issue_id': 3247, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': '053d31a29b03d829', 'authors': ['Ting Huang', 'Zeyu Zhang', 'Yemin Wang', 'Hao Tang'], 'affiliations': ['Peking University', 'Shanghai University of Engineering Science', 'The Australian National University', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2504.09518.jpg', 'data': {'categories': ['#reasoning', '#3d', '#alignment', '#architecture', '#benchmark', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Единая модель для понимания и описания 3D-сцен', 'desc': '3D CoCa - это новая унифицированная архитектура для описания 3D-сцен на естественном языке. Она объединяет контрастное обучение и генерацию подписей в единой модели, используя замороженный CLIP-бэкбон, энкодер 3D-сцен и мультимодальный декодер. В отличие от двухэтапных методов, 3D CoCa оптимизирует контрастные и генеративные цели совместно, что улучшает пространственное мышление и семантическую привязку. Эксперименты показывают значительное превосходство 3D CoCa над современными методами на бенчмарках ScanRefer и Nr3D.'}, 'en': {'title': 'Revolutionizing 3D Captioning with Unified Learning', 'desc': 'This paper introduces 3D CoCa, a new framework for 3D captioning that describes 3D scenes using natural language. It combines contrastive vision-language learning with 3D caption generation, addressing challenges like point cloud sparsity and weak alignment between visual and textual data. The model uses a frozen CLIP backbone for semantic understanding and a 3D scene encoder for geometric context, allowing it to generate captions without relying on external object detectors. Experimental results show that 3D CoCa outperforms existing methods, achieving significant improvements in captioning accuracy on benchmark datasets.'}, 'zh': {'title': '3D CoCa：无缝结合视觉与语言的3D标题生成', 'desc': '3D标题生成旨在用自然语言描述3D场景的内容，但由于点云稀疏和跨模态对齐不足，这一任务非常具有挑战性。为了解决这些问题，我们提出了3D CoCa，这是一种将对比视觉-语言学习与3D标题生成无缝结合的新框架。该方法利用冻结的CLIP视觉-语言骨干网络提供丰富的语义先验，使用空间感知的3D场景编码器捕捉几何上下文，并通过多模态解码器生成描述性标题。与依赖显式对象提议的两阶段方法不同，3D CoCa在共享特征空间中联合优化对比和标题生成目标，从而消除了对外部检测器或手工提议的需求。'}}}, {'id': 'https://huggingface.co/papers/2504.08120', 'title': 'DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?', 'url': 'https://huggingface.co/papers/2504.08120', 'abstract': 'Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use.', 'score': 1, 'issue_id': 3245, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '2cc9864fc01e2a7b', 'authors': ['Daniil Larionov', 'Sotaro Takeshita', 'Ran Zhang', 'Yanran Chen', 'Christoph Leiter', 'Zhipin Wang', 'Christian Greisinger', 'Steffen Eger'], 'affiliations': ['University of Mannheim', 'University of Technology Nuremberg'], 'pdf_title_img': 'assets/pdf/title_img/2504.08120.jpg', 'data': {'categories': ['#machine_translation', '#benchmark', '#training', '#reasoning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Рассуждающие языковые модели: неоднозначный успех в оценке генерации текста', 'desc': 'Это исследование сравнивает языковые модели с возможностями рассуждения (reasoning-enabled LLM) и обычные LLM в задачах оценки машинного перевода и суммаризации текста. Эксперименты на бенчмарках WMT23 и SummEval показали, что преимущества возможностей рассуждения сильно зависят от конкретной модели и задачи. Модели OpenAI o3-mini продемонстрировали улучшение производительности с увеличением интенсивности рассуждений, в то время как DeepSeek-R1 показала худшие результаты по сравнению с обычной версией. Исследование также выявило, что дистилляция возможностей рассуждения сохраняет разумную производительность в моделях среднего размера (32B), но значительно ухудшается в меньших вариантах (8B).'}, 'en': {'title': 'Evaluating Reasoning in Language Models for Better NLG Performance', 'desc': 'This paper investigates how reasoning-enabled large language models (LLMs) perform in evaluating natural language generation tasks like machine translation and text summarization. It compares reasoning-based models, such as DeepSeek-R1 and OpenAI o3, with non-reasoning models across various sizes and architectures. The findings indicate that the effectiveness of reasoning capabilities varies by model and task, with some models benefiting from increased reasoning intensity while others do not. Additionally, the study highlights that distillation of reasoning abilities can preserve performance in medium-sized models but leads to significant degradation in smaller ones.'}, 'zh': {'title': '推理能力提升自然语言生成评估的潜力', 'desc': '本研究探讨了推理能力强的大型语言模型（LLMs）在自然语言生成（NLG）评估中的表现。我们比较了基于推理的模型（如DeepSeek-R1和OpenAI o3）与非推理模型在机器翻译和文本摘要任务中的效果。实验结果表明，推理能力的优势依赖于具体模型和任务，某些情况下推理模型的表现不如其非推理版本。我们的分析还发现，推理能力的蒸馏在中等规模模型中保持了合理的性能，但在小型模型中显著下降。'}}}, {'id': 'https://huggingface.co/papers/2504.05782', 'title': 'MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in\n  Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2504.05782', 'abstract': 'Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12-Bench, a multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on a well-organized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing a robust platform for comprehensive evaluation. Additionally, we present a novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at https://github.com/LanceZPF/MDK12.', 'score': 1, 'issue_id': 3246, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': '6764ce8059618273', 'authors': ['Pengfei Zhou', 'Fanrui Zhang', 'Xiaopeng Peng', 'Zhaopan Xu', 'Jiaxin Ai', 'Yansheng Qiu', 'Chuanhao Li', 'Zhen Li', 'Ming Li', 'Yukang Feng', 'Jianwen Sun', 'Haoquan Zhang', 'Zizhen Li', 'Xiaofeng Mao', 'Wangbo Zhao', 'Kai Wang', 'Xiaojun Chang', 'Wenqi Shao', 'Yang You', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'MBZUAI', 'NUS', 'RIT', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'USTC', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2504.05782.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#agi', '#survey'], 'emoji': '🧠', 'ru': {'title': 'MDK12-Bench: Новый стандарт оценки мультимодального интеллекта', 'desc': 'Статья представляет MDK12-Bench - многопрофильный бенчмарк для оценки способностей мультимодальных больших языковых моделей (MLLM) к рассуждениям на основе реальных экзаменов K-12. Бенчмарк охватывает шесть дисциплин и содержит 140 тысяч примеров рассуждений разной сложности от начальной школы до 12 класса. Он включает в себя подробные аннотации знаний, объяснения ответов и динамическую систему оценки для минимизации проблем загрязнения данных. Эксперименты на MDK12-Bench выявили значительные ограничения текущих MLLM в мультимодальных рассуждениях.'}, 'en': {'title': 'Evaluating Multimodal Reasoning with MDK12-Bench', 'desc': 'This paper introduces MDK12-Bench, a new benchmark designed to evaluate the multimodal reasoning abilities of Multimodal Large Language Models (MLLMs) using real-world K-12 exam questions. It covers six subjects and includes 140,000 reasoning instances with varying difficulty levels, providing a structured way to assess model performance. The benchmark also features detailed annotations and a dynamic evaluation framework to address data contamination issues. Results from testing on MDK12-Bench highlight the current limitations of MLLMs in multimodal reasoning, offering valuable insights for future model development.'}, 'zh': {'title': '多模态推理评估的新基准', 'desc': '多模态推理是将语言和视觉线索结合起来进行问题解决和决策的重要能力，是人类智能的基本特征。本文提出了MDK12-Bench，这是一个多学科基准，旨在评估多模态大型语言模型（MLLMs）的推理能力。该基准涵盖数学、物理、化学、生物、地理和信息科学六个学科，包含14万个推理实例，适用于从小学到12年级的不同难度水平。通过动态评估框架，我们解决了数据污染问题，并揭示了当前MLLMs在多模态推理方面的显著局限性。'}}}, {'id': 'https://huggingface.co/papers/2504.09513', 'title': 'DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion', 'url': 'https://huggingface.co/papers/2504.09513', 'abstract': 'Large-scale pre-trained diffusion models have produced excellent results in the field of conditional image generation. However, restoration of ancient murals, as an important downstream task in this field, poses significant challenges to diffusion model-based restoration methods due to its large defective area and scarce training samples. Conditional restoration tasks are more concerned with whether the restored part meets the aesthetic standards of mural restoration in terms of overall style and seam detail, and such metrics for evaluating heuristic image complements are lacking in current research. We therefore propose DiffuMural, a combined Multi-scale convergence and Collaborative Diffusion mechanism with ControlNet and cyclic consistency loss to optimise the matching between the generated images and the conditional control. DiffuMural demonstrates outstanding capabilities in mural restoration, leveraging training data from 23 large-scale Dunhuang murals that exhibit consistent visual aesthetics. The model excels in restoring intricate details, achieving a coherent overall appearance, and addressing the unique challenges posed by incomplete murals lacking factual grounding. Our evaluation framework incorporates four key metrics to quantitatively assess incomplete murals: factual accuracy, textural detail, contextual semantics, and holistic visual coherence. Furthermore, we integrate humanistic value assessments to ensure the restored murals retain their cultural and artistic significance. Extensive experiments validate that our method outperforms state-of-the-art (SOTA) approaches in both qualitative and quantitative metrics.', 'score': 0, 'issue_id': 3247, 'pub_date': '2025-04-13', 'pub_date_card': {'ru': '13 апреля', 'en': 'April 13', 'zh': '4月13日'}, 'hash': 'ed55bc95454d113a', 'authors': ['Puyu Han', 'Jiaju Kang', 'Yuhang Pan', 'Erting Pan', 'Zeyu Zhang', 'Qunchao Jin', 'Juntao Jiang', 'Zhichen Liu', 'Luqi Gong'], 'affiliations': ['AI Geeks', 'Beijing Normal University', 'Hebei Guoyan Science and Technology Center', 'Southern University of Science and Technology', 'The Australian National University', 'Wuhan University', 'Zhejiang Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.09513.jpg', 'data': {'categories': ['#optimization', '#cv', '#benchmark', '#diffusion', '#dataset', '#ethics'], 'emoji': '🖼️', 'ru': {'title': 'DiffuMural: Передовой метод реставрации древних фресок с помощью ИИ', 'desc': 'Статья представляет DiffuMural - новый метод для реставрации древних фресок с использованием диффузионных моделей. Авторы предлагают комбинированный подход с многомасштабной конвергенцией и совместным диффузионным механизмом, оптимизированным с помощью ControlNet и циклической функции потерь. Модель обучена на 23 крупномасштабных фресках Дуньхуана и демонстрирует превосходные результаты в восстановлении сложных деталей и общей согласованности изображений. Для оценки качества реставрации авторы вводят новые метрики, учитывающие фактическую точность, текстурные детали, контекстную семантику и целостную визуальную согласованность.'}, 'en': {'title': "Reviving Ancient Art: DiffuMural's Innovative Restoration Approach", 'desc': 'This paper introduces DiffuMural, a novel approach for restoring ancient murals using large-scale pre-trained diffusion models. The method addresses challenges such as large defective areas and limited training samples by employing a Multi-scale convergence and Collaborative Diffusion mechanism. It focuses on aesthetic standards and incorporates a unique evaluation framework that includes metrics for factual accuracy and visual coherence. The results show that DiffuMural significantly outperforms existing methods in restoring intricate details while preserving the cultural significance of the murals.'}, 'zh': {'title': 'DiffuMural：古代壁画修复的新突破', 'desc': '本论文提出了一种名为DiffuMural的模型，旨在解决古代壁画修复中的挑战。该模型结合了多尺度收敛和协同扩散机制，利用ControlNet和循环一致性损失来优化生成图像与条件控制之间的匹配。DiffuMural在修复复杂细节和保持整体美观方面表现出色，特别是在处理缺失信息的壁画时。我们还建立了一个评估框架，结合定量指标和人文价值评估，确保修复后的壁画保留其文化和艺术意义。'}}}, {'id': 'https://huggingface.co/papers/2504.08685', 'title': 'Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model', 'url': 'https://huggingface.co/papers/2504.08685', 'abstract': 'This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/', 'score': 77, 'issue_id': 3213, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': '43b42333b796033b', 'authors': ['Team Seawead', 'Ceyuan Yang', 'Zhijie Lin', 'Yang Zhao', 'Shanchuan Lin', 'Zhibei Ma', 'Haoyuan Guo', 'Hao Chen', 'Lu Qi', 'Sen Wang', 'Feng Cheng', 'Feilong Zuo Xuejiao Zeng', 'Ziyan Yang', 'Fangyuan Kong', 'Zhiwu Qing', 'Fei Xiao', 'Meng Wei', 'Tuyen Hoang', 'Siyu Zhang', 'Peihao Zhu', 'Qi Zhao', 'Jiangqiao Yan', 'Liangke Gui', 'Sheng Bi', 'Jiashi Li', 'Yuxi Ren', 'Rui Wang', 'Huixia Li', 'Xuefeng Xiao', 'Shu Liu', 'Feng Ling', 'Heng Zhang', 'Houmin Wei', 'Huafeng Kuang', 'Jerry Duncan', 'Junda Zhang', 'Junru Zheng', 'Li Sun', 'Manlin Zhang', 'Renfei Sun', 'Xiaobin Zhuang', 'Xiaojie Li', 'Xin Xia', 'Xuyan Chi', 'Yanghua Peng', 'Yuping Wang', 'Yuxuan Wang', 'Zhongkai Zhao', 'Zhuo Chen', 'Zuquan Song', 'Zhenheng Yang', 'Jiashi Feng', 'Jianchao Yang', 'Lu Jiang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2504.08685.jpg', 'data': {'categories': ['#video', '#optimization', '#transfer_learning', '#small_models', '#diffusion', '#training', '#architecture'], 'emoji': '🌊', 'ru': {'title': 'Эффективная генерация видео с ограниченными ресурсами', 'desc': 'Этот технический отчет представляет экономически эффективную стратегию обучения базовой модели для генерации видео. Авторы представляют модель среднего размера с примерно 7 миллиардами параметров (7B), названную Seaweed-7B, обученную с нуля за 665 000 часов работы GPU H100. Несмотря на умеренные вычислительные ресурсы, Seaweed-7B демонстрирует высококонкурентную производительность по сравнению с современными моделями генерации видео гораздо большего размера. В отчете подчеркиваются ключевые проектные решения, которые повышают производительность диффузионной модели среднего размера.'}, 'en': {'title': 'Efficient Video Generation with Seaweed-7B: Small Size, Big Impact!', 'desc': 'This paper introduces Seaweed-7B, a mid-sized video generation model with 7 billion parameters, trained efficiently using 665,000 GPU hours. Despite its smaller size, Seaweed-7B achieves competitive performance against larger models, showcasing the importance of strategic design choices in resource-limited environments. The model demonstrates strong generalization capabilities, allowing it to adapt effectively to various downstream tasks through lightweight fine-tuning or continued training. Overall, the findings suggest that a well-designed medium-sized model can rival larger counterparts while being more cost-effective to train.'}, 'zh': {'title': '经济高效的视频生成模型训练策略', 'desc': '本技术报告提出了一种经济高效的视频生成基础模型训练策略。我们介绍了一种名为Seaweed-7B的中型研究模型，具有约70亿个参数，使用665,000个H100 GPU小时从零开始训练。尽管训练资源适中，Seaweed-7B的性能与更大规模的现代视频生成模型相比仍然具有竞争力。报告强调了在资源受限环境中增强中型扩散模型性能的关键设计决策。'}}}, {'id': 'https://huggingface.co/papers/2504.08736', 'title': 'GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2504.08736', 'abstract': 'In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality -- a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to 3 space billion parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality.', 'score': 28, 'issue_id': 3216, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': '4af199758c238fd4', 'authors': ['Tianwei Xiong', 'Jun Hao Liew', 'Zilong Huang', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.08736.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'GigaTok: масштабирование токенизаторов без компромиссов', 'desc': 'GigaTok - это новый подход к масштабированию визуальных токенизаторов для авторегрессивной генерации изображений. Он решает проблему ухудшения качества генерации при улучшении реконструкции изображений путем введения семантической регуляризации. Метод выравнивает признаки токенизатора с семантически согласованными признаками предобученного визуального энкодера. GigaTok достигает лучших результатов в реконструкции, генерации и качестве представлений для авторегрессивных моделей.'}, 'en': {'title': 'GigaTok: Balancing Image Quality and Generation in Autoregressive Models', 'desc': 'This paper presents GigaTok, a novel approach to enhance autoregressive image generation by improving visual tokenizers. It addresses the challenge of balancing image reconstruction quality with downstream generation quality, which often deteriorates when scaling tokenizers. The authors introduce semantic regularization to align tokenizer features with those from a pre-trained visual encoder, reducing latent space complexity. By implementing key practices for scaling, GigaTok achieves state-of-the-art results in image reconstruction and generation tasks.'}, 'zh': {'title': 'GigaTok：提升图像生成与重建的创新方法', 'desc': '在自回归图像生成中，视觉标记器将图像压缩为紧凑的离散潜在标记，从而实现高效的下游自回归模型训练。尽管扩大视觉标记器可以提高图像重建质量，但往往会降低下游生成质量，这是现有文献中未能充分解决的挑战。为了解决这个问题，我们提出了GigaTok，这是一种在扩大视觉标记器时同时改善图像重建、生成和表示学习的首个方法。我们通过语义正则化来减轻潜在空间复杂性，从而在重建和下游生成之间取得一致的改进。'}}}, {'id': 'https://huggingface.co/papers/2504.08388', 'title': 'MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft', 'url': 'https://huggingface.co/papers/2504.08388', 'abstract': 'World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate 4 to 7 frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.', 'score': 22, 'issue_id': 3215, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': 'f87f4b2c67ff61ca', 'authors': ['Junliang Guo', 'Yang Ye', 'Tianyu He', 'Haoyu Wu', 'Yushu Jiang', 'Tim Pearce', 'Jiang Bian'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.08388.jpg', 'data': {'categories': ['#inference', '#benchmark', '#diffusion', '#cv', '#open_source', '#agents', '#games'], 'emoji': '🕹️', 'ru': {'title': 'MineWorld: интеллектуальное моделирование мира Minecraft в реальном времени', 'desc': 'MineWorld - это интерактивная модель мира в реальном времени для Minecraft, основанная на авторегрессионном трансформере. Модель обучается на парах игровых сцен и соответствующих действий, генерируя последующие сцены. Используется новый алгоритм параллельного декодирования для генерации 4-7 кадров в секунду. Предложены новые метрики оценки качества генерации и способности следовать действиям, MineWorld превосходит существующие модели на основе диффузии.'}, 'en': {'title': 'MineWorld: Real-Time World Modeling in Minecraft', 'desc': "This paper introduces MineWorld, a real-time interactive world model designed for the game Minecraft, which serves as a platform for testing world modeling techniques. The model utilizes a visual-action autoregressive Transformer that processes paired game scenes and actions to predict subsequent scenes based on player interactions. By converting visual inputs and actions into discrete token IDs, MineWorld learns to represent game states and the relationships between actions and outcomes effectively. The authors also present a new parallel decoding algorithm that enhances the model's speed, allowing it to generate multiple frames per second while maintaining high visual quality and action coherence."}, 'zh': {'title': 'MineWorld：实时互动的智能世界模型', 'desc': '本研究提出了MineWorld，这是一个基于Minecraft的实时互动世界模型。该模型使用视觉-动作自回归Transformer，能够根据游戏场景和相应的动作生成新的场景。通过将视觉场景和动作转换为离散的标记ID，模型能够学习游戏状态的丰富表示及状态与动作之间的关系。在评估中，我们提出了新的指标来评估生成新场景的视觉质量和动作跟随能力，显示MineWorld在性能上显著优于现有的世界模型。'}}}, {'id': 'https://huggingface.co/papers/2504.08600', 'title': 'SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.08600', 'abstract': 'Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model.', 'score': 12, 'issue_id': 3217, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': '6153f561c5040630', 'authors': ['Peixian Ma', 'Xialie Zhuang', 'Chengjin Xu', 'Xuhui Jiang', 'Ran Chen', 'Jian Guo'], 'affiliations': ['DataArc Tech Ltd.', 'IDEA Research, International Digital Economy Academy', 'The Hong Kong University of Science and Technology (Guangzhou)', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.08600.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#training', '#rl', '#dataset', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Обучение с подкреплением улучшает преобразование естественного языка в SQL', 'desc': 'Статья представляет новую модель SQL-R1 для преобразования естественного языка в SQL-запросы. Модель использует обучение с подкреплением для улучшения производительности в сложных сценариях с объединением нескольких таблиц и вложенными запросами. Авторы разработали специальную функцию вознаграждения для задач NL2SQL и исследовали влияние холодного старта на эффективность интенсивного обучения. SQL-R1 достигает высокой точности выполнения на эталонных наборах данных Spider и BIRD, используя только базовую модель размером 7 миллиардов параметров.'}, 'en': {'title': 'Transforming Natural Language Queries with Reinforcement Learning', 'desc': 'This paper presents SQL-R1, a new model designed to convert natural language queries into SQL statements more effectively, especially in complex scenarios like multi-table joins. It addresses the limitations of traditional supervised fine-tuning methods by employing reinforcement learning (RL) to improve reasoning performance. The authors introduce a specialized reward function for NL2SQL tasks and explore the challenges of cold start in training. SQL-R1 demonstrates impressive execution accuracy on benchmark datasets, achieving 88.6% on Spider and 66.6% on BIRD, using minimal synthetic data for training.'}, 'zh': {'title': '提升NL2SQL推理性能的新方法', 'desc': '自然语言转SQL（NL2SQL）使用户能够通过自然语言查询与数据库进行直观交互。尽管在数据库应用中人机交互方面取得了一些进展，但在复杂场景下的推理性能仍然面临重大挑战，尤其是涉及多表连接和嵌套查询的情况。当前的方法主要依赖监督微调（SFT）来训练NL2SQL模型，这可能限制了其在新环境中的适应性和可解释性。为提高NL2SQL模型在复杂情况下的推理性能，我们提出了SQL-R1，这是一种通过强化学习（RL）算法训练的新型NL2SQL推理模型。'}}}, {'id': 'https://huggingface.co/papers/2504.07963', 'title': 'PixelFlow: Pixel-Space Generative Models with Flow', 'url': 'https://huggingface.co/papers/2504.07963', 'abstract': 'We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256times256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow.', 'score': 9, 'issue_id': 3217, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '4cdb0cfa27fd5251', 'authors': ['Shoufa Chen', 'Chongjian Ge', 'Shilong Zhang', 'Peize Sun', 'Ping Luo'], 'affiliations': ['Adobe', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.07963.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#architecture', '#cv', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'PixelFlow: революция в генерации изображений прямо в пиксельном пространстве', 'desc': 'PixelFlow - это семейство моделей генерации изображений, работающих непосредственно в пространстве пикселей, в отличие от преобладающих моделей латентного пространства. Этот подход упрощает процесс генерации изображений, устраняя необходимость в предварительно обученном вариационном автоэнкодере (VAE) и позволяя обучать всю модель сквозным образом. Благодаря эффективному каскадному моделированию потока, PixelFlow достигает приемлемых вычислительных затрат в пиксельном пространстве. Модель достигает показателя FID 1.98 на бенчмарке условной генерации изображений ImageNet размером 256x256.'}, 'en': {'title': 'PixelFlow: Revolutionizing Image Generation in Raw Pixel Space', 'desc': 'PixelFlow is a new type of image generation model that works directly with raw pixels instead of using latent spaces like many existing models. This method simplifies the process by removing the need for a pre-trained Variational Autoencoder (VAE), allowing the entire model to be trained in one go. By using efficient cascade flow modeling, PixelFlow maintains low computational costs while generating high-quality images. It has shown impressive results, achieving a low FID score of 1.98 on the ImageNet benchmark, and demonstrates strong performance in generating artistic and semantically controlled images from text prompts.'}, 'zh': {'title': 'PixelFlow：新一代图像生成模型的突破', 'desc': 'PixelFlow是一种直接在原始像素空间中操作的图像生成模型，与主流的潜在空间模型不同。这种方法简化了图像生成过程，消除了对预训练变分自编码器（VAE）的需求，使整个模型可以端到端训练。通过高效的级联流建模，PixelFlow在像素空间中实现了可承受的计算成本，并在256x256的ImageNet条件图像生成基准上达到了1.98的FID值。其文本到图像的生成结果显示，PixelFlow在图像质量、艺术性和语义控制方面表现出色。'}}}, {'id': 'https://huggingface.co/papers/2504.07405', 'title': 'FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation', 'url': 'https://huggingface.co/papers/2504.07405', 'abstract': 'With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/).', 'score': 7, 'issue_id': 3213, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'fb73f6a8f480a7a1', 'authors': ['Linyan Huang', 'Haonan Lin', 'Yanning Zhou', 'Kaiwen Xiao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.07405.jpg', 'data': {'categories': ['#inference', '#cv', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'Гибкий контроль идентичности в генеративных моделях', 'desc': 'FlexIP - это новый фреймворк для 2D генеративных моделей, который решает проблему сохранения идентичности объекта при разнообразном редактировании. Он использует два специализированных компонента: адаптер персонализации для стилистических манипуляций и адаптер сохранения для поддержания идентичности. Фреймворк позволяет гибко контролировать генерацию путем динамической настройки весового адаптера во время вывода. Эксперименты показывают, что FlexIP превосходит традиционные методы, обеспечивая лучшее сохранение идентичности при более разнообразных возможностях персонализированной генерации.'}, 'en': {'title': 'FlexIP: Balancing Identity and Personalization in 2D Generative Models', 'desc': "This paper presents FlexIP, a new framework designed to improve 2D generative models by separating the tasks of identity preservation and stylistic manipulation. It introduces two components: a Personalization Adapter that allows for diverse editing styles and a Preservation Adapter that ensures the subject's identity remains intact. By integrating these components, FlexIP enables users to dynamically adjust the balance between identity and personalization during the generation process. Experimental results show that FlexIP outperforms traditional methods, providing better identity retention while allowing for a wider range of personalized outputs."}, 'zh': {'title': '灵活的身份保持与个性化编辑', 'desc': '随着二维生成模型的快速发展，保持主体身份同时实现多样化编辑成为了一个重要的研究方向。现有方法通常在身份保持和个性化操作之间存在固有的权衡。我们提出了FlexIP，一个新颖的框架，通过两个专门的组件解耦这些目标：个性化适配器用于风格化操作，保持适配器用于身份维护。实验结果表明，我们的方法突破了传统方法的性能限制，实现了更优的身份保持，同时支持更多样化的个性化生成能力。'}}}, {'id': 'https://huggingface.co/papers/2504.08716', 'title': 'ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance', 'url': 'https://huggingface.co/papers/2504.08716', 'abstract': "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.", 'score': 5, 'issue_id': 3217, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': '50998c3f1c1cc54d', 'authors': ['Wissam Antoun', 'Benoît Sagot', 'Djamé Seddah'], 'affiliations': ['Inria, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.08716.jpg', 'data': {'categories': ['#optimization', '#architecture', '#benchmark', '#training'], 'emoji': '🧠', 'ru': {'title': 'Архитектура vs данные: что важнее для языковых моделей?', 'desc': 'Исследование сравнивает архитектурные улучшения в моделях трансформеров ModernBERT и DeBERTaV3. Авторы провели контролируемый эксперимент, предобучив ModernBERT на том же наборе данных, что и CamemBERTaV2 (модель DeBERTaV3 для французского языка). Результаты показали, что предыдущее поколение моделей остается более эффективным по выборке и производительности на бенчмарках, но ModernBERT быстрее в обучении и выводе. Исследование также выявило, что качественные данные для предобучения ускоряют сходимость, но не значительно улучшают конечную производительность.'}, 'en': {'title': 'Disentangling Architecture from Data in Transformer Models', 'desc': 'This paper investigates the performance of ModernBERT compared to DeBERTaV3 by controlling for training data. The authors find that while ModernBERT shows faster training and inference speeds, DeBERTaV3 outperforms it in terms of sample efficiency and overall benchmark results. The study highlights that high-quality pre-training data can speed up the training process but does not necessarily enhance final model performance. Ultimately, the research emphasizes the need to separate the effects of model architecture from the quality of training data when assessing transformer models.'}, 'zh': {'title': '架构创新与预训练数据的分离评估', 'desc': '本文研究了预训练的变换器编码器模型，如DeBERTaV3和ModernBERT，旨在提高效率和性能。尽管ModernBERT在多个基准测试中表现优于DeBERTaV3，但由于缺乏公开的训练数据和共享数据集的比较，难以判断这些提升是由于架构改进还是训练数据的差异。通过在与CamemBERTaV2相同的数据集上预训练ModernBERT，我们的研究表明，旧一代模型在样本效率和整体基准性能上仍然优于新模型，ModernBERT的主要优势在于更快的训练和推理速度。研究结果强调了在评估变换器模型时，将预训练数据与架构创新分开考虑的重要性。'}}}, {'id': 'https://huggingface.co/papers/2504.08727', 'title': 'Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images', 'url': 'https://huggingface.co/papers/2504.08727', 'abstract': 'We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., "what are the frequent types of changes in the city?") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., "addition of outdoor dining,", "overpass was painted blue," etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles.', 'score': 4, 'issue_id': 3224, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': '0bf30d396e917961', 'authors': ['Boyang Deng', 'Songyou Peng', 'Kyle Genova', 'Gordon Wetzstein', 'Noah Snavely', 'Leonidas Guibas', 'Thomas Funkhouser'], 'affiliations': ['Google DeepMind', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.08727.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#cv', '#interpretability'], 'emoji': '🏙️', 'ru': {'title': 'Мультимодальные ИИ раскрывают городские изменения', 'desc': 'Статья представляет систему, использующую мультимодальные языковые модели (MLLM) для анализа большой базы данных изображений, снятых в разное время, с целью обнаружения закономерностей во временных изменениях. Система способна отвечать на открытые вопросы о частых типах изменений в городе без предопределенных целевых объектов или обучающих меток. Авторы разработали процедуру, которая разбивает масштабную задачу визуального анализа на более управляемые подзадачи. Эксперименты показывают, что система значительно превосходит базовые методы и способна обнаруживать интересные тенденции из изображений, снятых в крупных городах.'}, 'en': {'title': 'Unlocking Urban Trends with Multimodal LLMs', 'desc': 'This paper introduces a system that utilizes Multimodal Large Language Models (MLLMs) to analyze a vast collection of images taken over time, aiming to identify patterns in urban changes. The system is designed to answer open-ended questions about frequent changes in a city without relying on predefined labels or subjects, which sets it apart from traditional visual analysis methods. To handle the enormous dataset, the authors propose a bottom-up approach that breaks down the analysis into smaller, manageable sub-problems, each addressed with tailored MLLM solutions. Experimental results demonstrate that this innovative approach significantly outperforms existing methods, successfully uncovering notable urban trends from the image data.'}, 'zh': {'title': '利用多模态大语言模型发现城市变化趋势', 'desc': '本文提出了一种使用多模态大语言模型（MLLMs）分析大型数据库的系统，该数据库包含数千万张在不同时间拍摄的图像，旨在发现时间变化中的模式。我们特别关注城市在一定时间内频繁共现的变化（"趋势"）。与以往的视觉分析不同，我们的分析能够回答开放式问题，而无需预先设定目标主题或训练标签。我们引入了一种自下而上的方法，将庞大的视觉分析问题分解为更易处理的子问题，并设计了基于MLLM的解决方案，实验结果显示该系统显著优于基线，能够从大城市的图像中发现有趣的趋势。'}}}, {'id': 'https://huggingface.co/papers/2504.08641', 'title': 'Training-free Guidance in Text-to-Video Generation via Multimodal\n  Planning and Structured Noise Initialization', 'url': 'https://huggingface.co/papers/2504.08641', 'abstract': 'Recent advancements in text-to-video (T2V) diffusion models have significantly enhanced the visual quality of the generated videos. However, even recent T2V models find it challenging to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. A recent line of research uses layout guidance for T2V models that require fine-tuning or iterative manipulation of the attention map during inference time. This significantly increases the memory requirement, making it difficult to adopt a large T2V model as a backbone. To address this, we introduce Video-MSG, a training-free Guidance method for T2V generation based on Multimodal planning and Structured noise initialization. Video-MSG consists of three steps, where in the first two steps, Video-MSG creates Video Sketch, a fine-grained spatio-temporal plan for the final video, specifying background, foreground, and object trajectories, in the form of draft video frames. In the last step, Video-MSG guides a downstream T2V diffusion model with Video Sketch through noise inversion and denoising. Notably, Video-MSG does not need fine-tuning or attention manipulation with additional memory during inference time, making it easier to adopt large T2V models. Video-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V generation benchmarks (T2VCompBench and VBench). We provide comprehensive ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation.', 'score': 4, 'issue_id': 3227, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': 'a354f738033ac186', 'authors': ['Jialu Li', 'Shoubin Yu', 'Han Lin', 'Jaemin Cho', 'Jaehong Yoon', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2504.08641.jpg', 'data': {'categories': ['#training', '#multimodal', '#video', '#benchmark', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Video-MSG: точное управление генерацией видео без дополнительного обучения', 'desc': 'Статья представляет Video-MSG - новый метод генерации видео по текстовому описанию без дополнительного обучения модели. Метод создает детальный пространственно-временной план видео в виде черновых кадров, специфицирующих фон, передний план и траектории объектов. Затем Video-MSG направляет диффузионную модель text-to-video с помощью этого плана через инверсию шума и денойзинг. Метод не требует тонкой настройки или манипуляций с вниманием, что позволяет использовать крупные модели T2V.'}, 'en': {'title': 'Streamlining Text-to-Video Generation with Video-MSG', 'desc': 'This paper presents Video-MSG, a novel method for improving text-to-video (T2V) generation without the need for fine-tuning or additional memory during inference. It introduces a three-step process that creates a Video Sketch, which outlines the spatial and temporal elements of the video, including backgrounds and object movements. By guiding T2V diffusion models with this structured plan, Video-MSG enhances the alignment of generated videos with text descriptions. The method shows promising results across various T2V models and benchmarks, demonstrating its effectiveness in generating high-quality videos that accurately reflect the input prompts.'}, 'zh': {'title': '无训练的文本到视频生成新方法', 'desc': '最近，文本到视频（T2V）扩散模型的进展显著提高了生成视频的视觉质量。然而，即使是最新的T2V模型，在准确遵循文本描述方面仍然面临挑战，尤其是在需要精确控制空间布局或物体轨迹时。为了解决这个问题，我们提出了Video-MSG，这是一种基于多模态规划和结构化噪声初始化的无训练指导方法。Video-MSG通过创建视频草图来引导下游T2V扩散模型，从而在不需要额外内存的情况下提高文本对齐效果。'}}}, {'id': 'https://huggingface.co/papers/2504.08591', 'title': 'ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image\n  Restoration', 'url': 'https://huggingface.co/papers/2504.08591', 'abstract': 'Recent progress in generative models has significantly improved image restoration capabilities, particularly through powerful diffusion models that offer remarkable recovery of semantic details and local fidelity. However, deploying these models at ultra-high resolutions faces a critical trade-off between quality and efficiency due to the computational demands of long-range attention mechanisms. To address this, we introduce ZipIR, a novel framework that enhances efficiency, scalability, and long-range modeling for high-res image restoration. ZipIR employs a highly compressed latent representation that compresses image 32x, effectively reducing the number of spatial tokens, and enabling the use of high-capacity models like the Diffusion Transformer (DiT). Toward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that structures the latent space into sub-bands to ease diffusion training. Trained on full images up to 2K resolution, ZipIR surpasses existing diffusion-based methods, offering unmatched speed and quality in restoring high-resolution images from severely degraded inputs.', 'score': 4, 'issue_id': 3227, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': 'd11a697d646326c2', 'authors': ['Yongsheng Yu', 'Haitian Zheng', 'Zhifei Zhang', 'Jianming Zhang', 'Yuqian Zhou', 'Connelly Barnes', 'Yuchen Liu', 'Wei Xiong', 'Zhe Lin', 'Jiebo Luo'], 'affiliations': ['Adobe Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.08591.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#architecture', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная реставрация изображений сверхвысокого разрешения', 'desc': 'ZipIR - это новая система для высокоэффективной реставрации изображений высокого разрешения. Она использует сжатое латентное представление, уменьшая количество пространственных токенов в 32 раза. Это позволяет применять мощные модели вроде Diffusion Transformer (DiT). ZipIR включает архитектуру Latent Pyramid VAE для структурирования латентного пространства. Обученная на полных изображениях до 2K, ZipIR превосходит существующие диффузионные методы по скорости и качеству реставрации.'}, 'en': {'title': 'ZipIR: Efficient High-Resolution Image Restoration with Latent Compression', 'desc': 'This paper presents ZipIR, a new framework designed to improve the efficiency and scalability of high-resolution image restoration using generative models. It addresses the challenges of long-range attention mechanisms in diffusion models, which can be computationally intensive. ZipIR utilizes a compressed latent representation that reduces the image size by 32 times, allowing for the application of high-capacity models like the Diffusion Transformer. The framework, trained on images up to 2K resolution, demonstrates superior speed and quality compared to existing methods, effectively restoring images from severely degraded conditions.'}, 'zh': {'title': 'ZipIR：高效高分辨率图像修复的新框架', 'desc': '最近生成模型的进展显著提升了图像修复的能力，尤其是通过强大的扩散模型，能够出色地恢复语义细节和局部清晰度。然而，在超高分辨率下部署这些模型时，由于长程注意机制的计算需求，面临质量与效率之间的关键权衡。为了解决这个问题，我们提出了ZipIR，一个新颖的框架，增强了高分辨率图像修复的效率、可扩展性和长程建模能力。ZipIR采用高度压缩的潜在表示，将图像压缩32倍，有效减少空间标记的数量，从而能够使用像扩散变换器（DiT）这样的高容量模型。'}}}, {'id': 'https://huggingface.co/papers/2504.08366', 'title': 'In-2-4D: Inbetweening from Two Single-View Images to 4D Generation', 'url': 'https://huggingface.co/papers/2504.08366', 'abstract': 'We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/', 'score': 4, 'issue_id': 3213, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': 'df65a8f6baab7f84', 'authors': ['Sauradip Nag', 'Daniel Cohen-Or', 'Hao Zhang', 'Ali Mahdavi-Amiri'], 'affiliations': ['Simon Fraser University, Canada', 'Tel Aviv University, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2504.08366.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d', '#optimization'], 'emoji': '🎞️', 'ru': {'title': 'Из 2D в 4D: генерация реалистичного движения по двум кадрам', 'desc': 'Статья представляет новую задачу In-2-4D для генеративного 4D-интерполирования на основе двух изображений объекта в разных состояниях движения. Авторы используют модель интерполяции видео для предсказания движения и применяют иерархический подход для определения ключевых кадров. Для каждого фрагмента создается 3D-представление с помощью Gaussian Splatting, а временные кадры управляют движением через поле деформации. Метод улучшает временную согласованность, расширяя самовнимание мультиракурсной диффузии и применяя регуляризацию жесткой трансформации.'}, 'en': {'title': 'Transforming 2D Images into Smooth 4D Motion!', 'desc': 'This paper introduces the In-2-4D problem, which focuses on generating 4D representations of motion from just two images of an object in different states. The authors propose a hierarchical method that identifies keyframes to create smooth transitions between these states, addressing challenges posed by large frame-to-frame motions. They utilize Gaussian Splatting to construct 3D representations and apply a deformation field to guide the transformation of these representations into dynamic forms. The approach enhances temporal consistency and smoothness by merging motion segments and optimizing them to align with the original video, demonstrating its effectiveness through various experiments.'}, 'zh': {'title': '从静态到动态：4D运动插值的新方法', 'desc': '我们提出了一个新的问题，In-2-4D，旨在从两个不同运动状态的单视图图像中生成4D（即3D + 动作）插值。给定表示物体运动起始和结束状态的两幅图像，我们的目标是生成和重建4D中的运动。我们采用视频插值模型来预测运动，但大幅度的帧间运动可能导致模糊的解释。为了解决这个问题，我们使用分层方法识别与输入状态视觉上接近且运动显著的关键帧，然后在它们之间生成平滑的片段。'}}}, {'id': 'https://huggingface.co/papers/2504.07866', 'title': 'Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs', 'url': 'https://huggingface.co/papers/2504.07866', 'abstract': 'We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.', 'score': 4, 'issue_id': 3222, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'dbf55819969ad502', 'authors': ['Yichun Yin', 'Wenyong Huang', 'Kaikai Song', 'Yehui Tang', 'Xueyu Wu', 'Wei Guo', 'Peng Guo', 'Yaoyuan Wang', 'Xiaojun Meng', 'Yasheng Wang', 'Dong Li', 'Can Chen', 'Dandan Tu', 'Yin Li', 'Fisher Yu', 'Ruiming Tang', 'Yunhe Wang', 'Baojun Wang', 'Bin Wang', 'Bo Wang', 'Boxiao Liu', 'Changzheng Zhang', 'Duyu Tang', 'Fei Mi', 'Hui Jin', 'Jiansheng Wei', 'Jiarui Qin', 'Jinpeng Li', 'Jun Zhao', 'Liqun Deng', 'Lin Li', 'Minghui Xu', 'Naifu Zhang', 'Nianzu Zheng', 'Qiang Li', 'Rongju Ruan', 'Shengjun Cheng', 'Tianyu Guo', 'Wei He', 'Wei Li', 'Weiwen Liu', 'Wulong Liu', 'Xinyi Dai', 'Yonghan Dong', 'Yu Pan', 'Yue Li', 'Yufei Wang', 'Yujun Li', 'Yunsheng Ni', 'Zhe Liu', 'Zhenhe Zhang', 'Zhicheng Liu'], 'affiliations': ['Huawei'], 'pdf_title_img': 'assets/pdf/title_img/2504.07866.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Pangu Ultra: новые горизонты в обучении крупных языковых моделей', 'desc': 'Pangu Ultra - это крупная языковая модель с 135 миллиардами параметров, обученная на нейронных процессорах Ascend. Для стабилизации обучения авторы предложили метод depth-scaled sandwich normalization. Модель была предобучена на 13,2 триллионах токенов разнообразных высококачественных данных с использованием 8192 нейропроцессоров Ascend. Результаты оценки показывают, что Pangu Ultra значительно превосходит современные плотные языковые модели и даже конкурирует с некоторыми разреженными моделями, имеющими больше параметров.'}, 'en': {'title': 'Pangu Ultra: Redefining Large Language Model Training Efficiency', 'desc': 'Pangu Ultra is a Large Language Model (LLM) featuring 135 billion parameters, designed to enhance natural language processing tasks. The model employs depth-scaled sandwich normalization to stabilize training and prevent loss spikes, addressing challenges associated with training large-scale models. It is pre-trained on an extensive dataset of 13.2 trillion tokens and further refined to improve reasoning capabilities. Evaluations show that Pangu Ultra outperforms existing dense LLMs and competes well with sparse models, demonstrating the efficiency of Ascend NPUs in handling such large models.'}, 'zh': {'title': 'Pangu Ultra：超越大型语言模型的极限', 'desc': 'Pangu Ultra是一种大型语言模型，拥有1350亿个参数，采用密集的Transformer模块，并在Ascend神经处理单元上进行训练。为了稳定训练过程，本文提出了一种深度缩放的三明治归一化方法，有效消除了深度模型训练过程中的损失波动。我们在132万亿个多样化和高质量的标记上进行了预训练，并在后期训练中进一步增强了推理能力。评估结果表明，Pangu Ultra在多个基准测试中显著提升了密集型大型语言模型的性能，展示了Ascend NPU在训练超过1000亿参数的密集模型方面的高效性。'}}}, {'id': 'https://huggingface.co/papers/2504.07615', 'title': 'VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model', 'url': 'https://huggingface.co/papers/2504.07615', 'abstract': 'Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs\' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the "OD aha moment", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1', 'score': 4, 'issue_id': 3226, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': '4a70c069a64a48af', 'authors': ['Haozhan Shen', 'Peng Liu', 'Jingcheng Li', 'Chunxin Fang', 'Yibo Ma', 'Jiajia Liao', 'Qiaoli Shen', 'Zilun Zhang', 'Kangjia Zhao', 'Qianqian Zhang', 'Ruochen Xu', 'Tiancheng Zhao'], 'affiliations': ['Binjiang Institute of Zhejiang University', 'Om AI Research', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07615.jpg', 'data': {'categories': ['#open_source', '#rl', '#multimodal', '#reasoning', '#training', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Усиление визуального мышления ИИ с помощью обучения с подкреплением', 'desc': 'Исследователи представили VLM-R1 - фреймворк для улучшения визуально-языковых моделей (VLM) с помощью обучения с подкреплением (RL). Подход основан на правилах вознаграждения для задач с четко определенными правильными ответами. Эксперименты показали, что модель, обученная с RL, превосходит модели с обычной тонкой настройкой по способности к обобщению. Авторы также провели подробный анализ влияния RL на VLM, выявив ряд интересных эффектов.'}, 'en': {'title': 'Enhancing Vision-Language Models with Reinforcement Learning', 'desc': 'This paper presents VLM-R1, a framework that applies reinforcement learning (RL) to enhance the visual reasoning capabilities of Vision-Language Models (VLMs). By utilizing a rule-based reward system, the framework effectively leverages tasks with clear ground-truth answers, leading to improved performance on various vision-language tasks. The experimental results demonstrate that VLM-R1 not only competes well with traditional Supervised Fine-Tuning (SFT) methods but also shows superior generalization abilities. Additionally, the study reveals important insights into reward dynamics and the effects of training data quality on model performance.'}, 'zh': {'title': '强化学习提升视觉语言模型的推理能力', 'desc': '最近，DeepSeek R1展示了强化学习（RL）可以显著提升大型语言模型（LLMs）的推理能力。R1的核心在于其基于规则的奖励机制，利用具有确定性真实答案的任务来实现精确和稳定的奖励计算。我们发现，许多视觉理解任务也具备良好的真实标注，使其与基于规则的奖励机制自然兼容。基于此，我们开发了VLM-R1框架，旨在通过强化学习提升视觉语言模型（VLMs）在视觉推理任务上的表现。'}}}, {'id': 'https://huggingface.co/papers/2504.05262', 'title': 'Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models', 'url': 'https://huggingface.co/papers/2504.05262', 'abstract': 'Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition (0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and compositional generalization (via isomorphic symbolic mappings, e.g., 7 rightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on numerical addition, performance collapses to leq7.5\\% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of A+B neq B+A) further support this. Explicitly providing addition rules degrades performance by 81.2\\% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning.', 'score': 4, 'issue_id': 3220, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '428bb7d2af34af73', 'authors': ['Yang Yan', 'Yu Lu', 'Renjun Xu', 'Zhenzhong Lan'], 'affiliations': ['School of Engineering, Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05262.jpg', 'data': {'categories': ['#alignment', '#math', '#architecture', '#reasoning', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'Большие языковые модели: запоминание вместо понимания в арифметике', 'desc': 'Исследование показывает, что современные большие языковые модели (LLM) не способны к настоящему математическому рассуждению, а полагаются на запоминание шаблонов. Эксперименты с элементарным сложением двух целых чисел выявили, что модели не усваивают фундаментальные свойства операции, такие как коммутативность. При применении символических отображений точность LLM резко падает, что указывает на отсутствие композиционного обобщения. Эти результаты подчеркивают ограничения текущих архитектур LLM и необходимость новых подходов для достижения истинного математического мышления.'}, 'en': {'title': 'Unveiling the Limits of LLMs: From Memorization to Mathematical Reasoning', 'desc': 'This paper examines whether Large Language Models (LLMs) truly understand mathematical principles or simply memorize patterns. The authors focus on basic two-integer addition and test LLMs on their ability to apply commutativity and compositional generalization. Despite high accuracy in straightforward addition tasks, LLMs perform poorly when faced with symbolic mappings, indicating a lack of generalization. The results suggest that LLMs depend more on memorization than on learning mathematical rules, revealing significant limitations in their architecture for true mathematical reasoning.'}, 'zh': {'title': '大型语言模型的数学推理能力不足', 'desc': '尽管大型语言模型（LLMs）在基准测试中得分很高，但它们在简单问题上常常失败，这引发了一个关键问题：LLMs是学习了数学原理，还是仅仅记忆了模式？我们通过研究基本的两整数加法（0到2^{64}）来探讨这一点，重点关注两个核心属性：交换律（A+B=B+A）和组合泛化（通过同构符号映射，例如7映射到y）。尽管最先进的LLMs在数字加法上取得了73.8-99.8%的准确率，但在符号映射下，性能骤降至不超过7.5%，这表明它们未能泛化所学规则。我们的研究结果表明，当前的LLMs依赖于记忆模式而非真正的规则学习，突显了其架构的局限性，并需要新的方法来实现真正的数学推理。'}}}, {'id': 'https://huggingface.co/papers/2504.01883', 'title': 'CoRAG: Collaborative Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2504.01883', 'abstract': 'Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research.', 'score': 3, 'issue_id': 3218, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'f3bd4bbb45b0315a', 'authors': ['Aashiq Muhamed', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University', 'Machine Learning Department, Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01883.jpg', 'data': {'categories': ['#transfer_learning', '#rag', '#benchmark', '#low_resource'], 'emoji': '🤝', 'ru': {'title': 'CoRAG: Совместное обучение для улучшения генерации с извлечением', 'desc': 'Статья представляет CoRAG - фреймворк для совместного обучения моделей на основе RAG. Авторы вводят новый бенчмарк CRAB для оценки коллаборативного ответа на вопросы. Эксперименты показывают, что CoRAG превосходит другие методы в условиях ограниченных ресурсов. Исследование выявляет важность релевантных пассажей в общем хранилище и потенциальные риски включения вредных пассажей от других клиентов.'}, 'en': {'title': 'Collaborative Learning with CoRAG: Enhancing RAG Models Together!', 'desc': 'The paper presents CoRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) models for collaborative learning environments. In this setup, multiple clients work together to train a shared model using a common passage store, which improves performance on knowledge-intensive tasks. The authors introduce CRAB, a benchmark for evaluating collaborative question answering, and show that CoRAG outperforms traditional methods in low-resource situations. Key insights include the importance of relevant passages, the unexpected advantages of irrelevant ones, and the risks posed by hard negatives, highlighting the complexities of collaborative knowledge sharing.'}, 'zh': {'title': '协作增强生成：共享知识的力量与挑战', 'desc': '本论文介绍了一种名为CoRAG的框架，它扩展了检索增强生成（RAG）模型，使其能够在协作环境中共同训练共享模型。我们提出了CRAB基准，用于评估协作同质开放域问答的性能。实验结果表明，CoRAG在低资源场景下的表现优于传统的参数协作学习方法和本地训练的RAG模型。研究还揭示了共享知识库中相关段落的重要性，以及引入无关段落的意外好处和困难负样本对性能的潜在负面影响。'}}}, {'id': 'https://huggingface.co/papers/2504.08192', 'title': 'SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs', 'url': 'https://huggingface.co/papers/2504.08192', 'abstract': 'Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic DAE Guardrails (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.', 'score': 2, 'issue_id': 3218, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': 'e6e92f7d8a3d930d', 'authors': ['Aashiq Muhamed', 'Jacopo Bonato', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Carnegie Mellon University', 'Leonardo Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.08192.jpg', 'data': {'categories': ['#training', '#security', '#interpretability', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Точное и эффективное разобучение нейросетей с помощью динамических автоэнкодеров', 'desc': 'Статья представляет новый метод машинного разобучения под названием Dynamic DAE Guardrails (DSG), основанный на динамических разреженных автоэнкодерах. DSG позволяет эффективно удалять нежелательные знания из языковых моделей, превосходя существующие методы по ряду параметров. Метод демонстрирует лучший баланс между забыванием и полезностью, повышенную вычислительную эффективность и стабильность. DSG также обладает улучшенной устойчивостью к атакам повторного обучения и большей интерпретируемостью процесса разобучения.'}, 'en': {'title': 'Dynamic Unlearning: Enhancing Safety in LLMs with DSG', 'desc': 'This paper discusses a new method called Dynamic DAE Guardrails (DSG) for improving machine unlearning in large language models (LLMs). Traditional gradient-based unlearning methods face several challenges, including high computational costs and vulnerability to relearning attacks. The authors propose using Sparse Autoencoders (SAEs) to enhance unlearning efficiency and stability, demonstrating that DSG can outperform existing methods. Their experiments show that DSG provides better performance in terms of forget-utility trade-offs, making unlearning more effective and interpretable.'}, 'zh': {'title': '动态去噪自编码器：提升机器遗忘的效率与稳定性', 'desc': '机器遗忘是一种有前景的方法，可以通过从模型中移除不必要的知识来提高大型语言模型的安全性。然而，现有的基于梯度的遗忘方法存在计算成本高、超参数不稳定、顺序遗忘能力差等问题。本文提出了一种新的动态去噪自编码器方法（DSG），通过精确的特征选择和动态分类器来实现更有效的遗忘。实验结果表明，DSG在遗忘效率和实用性之间取得了显著的平衡，超越了现有的遗忘方法。'}}}, {'id': 'https://huggingface.co/papers/2504.01786', 'title': 'BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing', 'url': 'https://huggingface.co/papers/2504.01786', 'abstract': "3D graphics editing is crucial in applications like movie production and game design, yet it remains a time-consuming process that demands highly specialized domain expertise. Automating this process is challenging because graphical editing requires performing a variety of tasks, each requiring distinct skill sets. Recently, vision-language models (VLMs) have emerged as a powerful framework for automating the editing process, but their development and evaluation are bottlenecked by the lack of a comprehensive benchmark that requires human-level perception and presents real-world editing complexity. In this work, we present BlenderGym, the first comprehensive VLM system benchmark for 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D reconstruction tasks. We evaluate closed- and open-source VLM systems and observe that even the state-of-the-art VLM system struggles with tasks relatively easy for human Blender users. Enabled by BlenderGym, we study how inference scaling techniques impact VLM's performance on graphics editing tasks. Notably, our findings reveal that the verifier used to guide the scaling of generation can itself be improved through inference scaling, complementing recent insights on inference scaling of LLM generation in coding and math tasks. We further show that inference compute is not uniformly effective and can be optimized by strategically distributing it between generation and verification.", 'score': 2, 'issue_id': 3231, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '2c60846bd7ed943f', 'authors': ['Yunqi Gu', 'Ian Huang', 'Jihyeon Je', 'Guandao Yang', 'Leonidas Guibas'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01786.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#3d', '#inference'], 'emoji': '🎨', 'ru': {'title': 'BlenderGym: революция в автоматизации 3D-редактирования с помощью VLM', 'desc': 'Статья представляет BlenderGym - первый комплексный бенчмарк для оценки систем vision-language models (VLM) в задачах редактирования 3D-графики. BlenderGym оценивает VLM-системы через задачи 3D-реконструкции на основе кода. Исследование показывает, что даже современные VLM-системы испытывают трудности с задачами, относительно простыми для пользователей Blender. Авторы изучают, как методы масштабирования вывода влияют на производительность VLM в задачах редактирования графики, и обнаруживают, что верификатор, используемый для управления масштабированием генерации, сам может быть улучшен с помощью масштабирования вывода.'}, 'en': {'title': 'BlenderGym: Benchmarking VLMs for 3D Graphics Editing', 'desc': 'This paper introduces BlenderGym, a new benchmark designed to evaluate vision-language models (VLMs) in the context of 3D graphics editing. The authors highlight the challenges of automating this complex task, which requires diverse skills and human-level perception. Through their evaluation, they find that even advanced VLMs struggle with tasks that are relatively simple for human users. Additionally, the study explores how inference scaling techniques can enhance VLM performance, revealing that optimizing the distribution of computational resources between generation and verification can lead to better outcomes.'}, 'zh': {'title': 'BlenderGym：3D图形编辑的智能自动化新基准', 'desc': '3D图形编辑在电影制作和游戏设计中至关重要，但这一过程耗时且需要高度专业的领域知识。自动化这一过程面临挑战，因为图形编辑涉及多种任务，每种任务都需要不同的技能。最近，视觉-语言模型（VLMs）作为一种强大的框架，开始用于自动化编辑过程，但缺乏全面的基准测试限制了其开发和评估。我们提出了BlenderGym，这是第一个综合性的VLM系统基准，用于3D图形编辑，评估VLM系统在代码基础的3D重建任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2504.08635', 'title': 'Latent Diffusion Autoencoders: Toward Efficient and Meaningful\n  Unsupervised Representation Learning in Medical Imaging', 'url': 'https://huggingface.co/papers/2504.08635', 'abstract': 'This study presents Latent Diffusion Autoencoder (LDAE), a novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimer disease (AD) using brain MR from the ADNI database as a case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in a compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as a promising framework for scalable medical imaging applications, with the potential to serve as a foundation model for medical image analysis. Code available at https://github.com/GabrieleLozupone/LDAE', 'score': 1, 'issue_id': 3224, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': '797102aac1993585', 'authors': ['Gabriele Lozupone', 'Alessandro Bria', 'Francesco Fontanella', 'Frederick J. A. Meijer', 'Claudio De Stefano', 'Henkjan Huisman'], 'affiliations': ['Department of Electrical and Information Engineering (DIEI), University of Cassino and Southern Lazio, Italy', 'Department of Medical Imaging, Radboud University Medical Center, Netherlands', 'Diagnostic Image Analysis Group, Radboud University Medical Center, Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2504.08635.jpg', 'data': {'categories': ['#science', '#inference', '#3d', '#healthcare', '#dataset', '#diffusion', '#cv'], 'emoji': '🧠', 'ru': {'title': 'LDAE: Эффективное обучение без учителя для анализа 3D медицинских изображений', 'desc': 'Исследование представляет Латентный Диффузионный Автоэнкодер (LDAE) - новую архитектуру для эффективного обучения без учителя в медицинской визуализации, с фокусом на болезнь Альцгеймера. LDAE применяет процесс диффузии в сжатом латентном представлении, что улучшает вычислительную эффективность для 3D медицинских изображений. Эксперименты показывают, что LDAE эффективно захватывает семантические представления, связанные с болезнью Альцгеймера и старением, а также обеспечивает качественную генерацию и реконструкцию изображений. По сравнению с обычными диффузионными автоэнкодерами, LDAE значительно увеличивает скорость вывода и улучшает качество реконструкции.'}, 'en': {'title': 'Revolutionizing Medical Imaging with Latent Diffusion Autoencoder', 'desc': "This paper introduces the Latent Diffusion Autoencoder (LDAE), a new framework designed for efficient unsupervised learning in medical imaging, specifically targeting Alzheimer disease using brain MR images. LDAE innovatively applies the diffusion process in a compressed latent space rather than directly on images, which enhances computational efficiency and makes it easier to handle 3D medical data. The study validates LDAE's effectiveness by demonstrating its ability to capture meaningful representations related to Alzheimer and aging, achieving high-quality image generation and reconstruction. Experimental results show that LDAE significantly outperforms traditional methods in both speed and quality, making it a valuable tool for medical image analysis."}, 'zh': {'title': '潜在扩散自编码器：医学影像分析的新突破', 'desc': '本研究提出了一种新颖的潜在扩散自编码器（LDAE），旨在提高医学影像中无监督学习的效率和意义，特别关注阿尔茨海默病（AD）。与传统的图像空间扩散自编码器不同，LDAE在压缩的潜在表示中应用扩散过程，从而提高计算效率，使得3D医学影像的表示学习变得可行。实验结果表明，LDAE能够有效捕捉与AD和衰老相关的3D脑部MR的有意义语义表示，并在图像生成和重建方面表现出高质量和计算效率。LDAE的表现优于传统扩散自编码器，推理速度提高了20倍，同时重建质量也得到了增强。'}}}, {'id': 'https://huggingface.co/papers/2504.06908', 'title': 'UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image\n  Segmentation', 'url': 'https://huggingface.co/papers/2504.06908', 'abstract': 'In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at https://emmanuelleb985.github.io/ukbob , and the filtered labels will be made available with the UK Biobank.', 'score': 0, 'issue_id': 3230, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '43cceed4e0431425', 'authors': ['Emmanuelle Bourigault', 'Amir Jamaludin', 'Abdullah Hamdi'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2504.06908.jpg', 'data': {'categories': ['#healthcare', '#open_source', '#synthetic', '#3d', '#benchmark', '#architecture', '#dataset', '#data'], 'emoji': '🧠', 'ru': {'title': 'UKBOB: Революция в сегментации медицинских 3D-изображений', 'desc': 'Статья представляет UKBOB - крупнейший размеченный набор данных органов тела, основанный на МРТ-снимках из UK Biobank. Авторы используют автоматическую разметку и очистку данных, а также ручную аннотацию подмножества снимков для валидации качества. Они предлагают новый метод ETTA для уточнения сегментации и обучают фундаментальную модель Swin-BOB для 3D-сегментации медицинских изображений. Модель достигает передовых результатов на нескольких бенчмарках в области 3D-медицинской визуализации.'}, 'en': {'title': 'Revolutionizing Medical Imaging with UKBOB: A Giant Leap in Labeled Data', 'desc': 'This paper addresses the challenge of acquiring large-scale labeled data in medical imaging by introducing the UK Biobank Organs and Bones (UKBOB) dataset, which contains 51,761 MRI 3D samples and over 1.37 billion segmentation masks for 72 organs. The authors employ automatic labeling and a cleaning pipeline to ensure high-quality labels, while also validating the dataset with manual annotations of a subset of MRIs. They demonstrate the effectiveness of the dataset by achieving zero-shot generalization on other small labeled datasets and propose a new method, Entropy Test-time Adaptation (ETTA), to improve segmentation outputs. The foundation model, Swin-BOB, trained on UKBOB, sets new benchmarks in 3D medical image segmentation, showcasing significant improvements in established challenges.'}, 'zh': {'title': '构建大规模医学影像数据集的创新之路', 'desc': '在医学影像领域，收集大规模标注数据面临隐私、物流和高标注成本等挑战。本文介绍了UK Biobank Organs and Bones (UKBOB)，这是最大的身体器官标注数据集，包含51,761个MRI 3D样本和超过13.7亿个2D分割掩膜。我们采用自动标注和自动化标签清理流程，并手动标注300个MRI以验证标签质量。通过训练基础模型Swin-BOB，我们在3D医学影像分割中取得了多项基准测试的最新成果。'}}}, {'id': 'https://huggingface.co/papers/2504.05303', 'title': 'InteractVLM: 3D Interaction Reasoning from 2D Foundational Models', 'url': 'https://huggingface.co/papers/2504.05303', 'abstract': 'We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce a novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multi-view rendering, (2) trains a novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose a new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enabling richer interaction modeling. InteractVLM outperforms existing work on contact estimation and also facilitates 3D reconstruction from an in-the wild image. Code and models are available at https://interactvlm.is.tue.mpg.de.', 'score': 0, 'issue_id': 3219, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'be4a2fea61b66424', 'authors': ['Sai Kumar Dwivedi', 'Dimitrije Antić', 'Shashank Tripathi', 'Omid Taheri', 'Cordelia Schmid', 'Michael J. Black', 'Dimitrios Tzionas'], 'affiliations': ['Inria, Ecole normale superieure, CNRS, PSL Research University, France', 'Max Planck Institute for Intelligent Systems, Tubingen, Germany', 'University of Amsterdam (UvA), the Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2504.05303.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🤖', 'ru': {'title': 'InteractVLM: 3D-реконструкция взаимодействия человека и объекта с помощью визуально-языковых моделей', 'desc': 'InteractVLM - это новый метод оценки 3D-точек контакта между людьми и объектами на изображениях. Он использует крупные визуально-языковые модели (VLM) для преодоления проблем окклюзии и неоднозначности глубины. Метод включает модуль Render-Localize-Lift для встраивания 3D-поверхностей в 2D-пространство и последующего подъема в 3D. InteractVLM также вводит задачу семантической оценки контактов человека, учитывающую семантику объектов.'}, 'en': {'title': 'Revolutionizing 3D Contact Estimation with InteractVLM', 'desc': 'InteractVLM is a new method designed to identify 3D contact points between humans and objects using just single images taken in everyday settings. It addresses challenges like occlusions and varying object shapes, which complicate accurate 3D reconstruction. Unlike previous methods that depend on costly 3D annotations, InteractVLM leverages the capabilities of Vision-Language Models (VLMs) fine-tuned with minimal 3D data. The approach includes a unique Render-Localize-Lift module that converts 3D surfaces into 2D, infers contacts in 2D, and then translates these findings back into 3D, enhancing the understanding of human-object interactions.'}, 'zh': {'title': 'InteractVLM：从图像中实现3D接触点估计的创新方法', 'desc': 'InteractVLM是一种新方法，可以从单张自然场景图像中估计人体和物体的3D接触点，从而实现准确的人体-物体联合3D重建。这项任务面临遮挡、深度模糊和物体形状多样性等挑战。现有方法依赖于昂贵的运动捕捉系统或繁琐的手动标注来收集3D接触注释，限制了其可扩展性和泛化能力。InteractVLM利用大型视觉-语言模型的广泛视觉知识，并通过有限的3D接触数据进行微调，克服了这些限制。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (33)', '#agents (32)', '#agi (9)', '#alignment (20)', '#architecture (64)', '#audio (6)', '#benchmark (132)', '#cv (76)', '#data (45)', '#dataset (102)', '#diffusion (51)', '#ethics (12)', '#games (18)', '#graphs (5)', '#hallucinations (13)', '#healthcare (11)', '#inference (47)', '#interpretability (25)', '#leakage', '#long_context (24)', '#low_resource (12)', '#machine_translation (3)', '#math (22)', '#multilingual (12)', '#multimodal (104)', '#open_source (76)', '#optimization (141)', '#plp (2)', '#rag (9)', '#reasoning (113)', '#rl (40)', '#rlhf (17)', '#robotics (7)', '#science (13)', '#security (10)', '#small_models (13)', '#story_generation (2)', '#survey (15)', '#synthetic (25)', '#training (161)', '#transfer_learning (23)', '#video (42)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-21 19:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-21 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-21 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    