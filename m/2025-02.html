
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 310 papers. February 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">–§–µ–≤—Ä–∞–ª—å 2025</span> | <span id="title-articles-count">310 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-01.html">‚¨ÖÔ∏è <span id="prev-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-03.html">‚û°Ô∏è <span id="next-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">üìà <span id='top-day-label'>–î–µ–Ω—å</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '–§–µ–≤—Ä–∞–ª—å 2025', 'en': 'February 2025', 'zh': '2Êúà2025Âπ¥'};
        let feedDateNext = {'ru': '03.2025', 'en': '03/2025', 'zh': '3Êúà2025Âπ¥'};
        let feedDatePrev = {'ru': '01.2025', 'en': '01/2025', 'zh': '1Êúà2025Âπ¥'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.02737', 'title': 'SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model', 'url': 'https://huggingface.co/papers/2502.02737', 'abstract': 'While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art "small" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project.', 'score': 80, 'issue_id': 2066, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'c78fe4c39300443d', 'authors': ['Loubna Ben Allal', 'Anton Lozhkov', 'Elie Bakouch', 'Gabriel Mart√≠n Bl√°zquez', 'Guilherme Penedo', 'Lewis Tunstall', 'Andr√©s Marafioti', 'Hynek Kydl√≠ƒçek', 'Agust√≠n Piqueres Lajar√≠n', 'Vaibhav Srivastav', 'Joshua Lochner', 'Caleb Fahlgren', 'Xuan-Son Nguyen', 'Cl√©mentine Fourrier', 'Ben Burtenshaw', 'Hugo Larcher', 'Haojun Zhao', 'Cyril Zakka', 'Mathieu Morlon', 'Colin Raffel', 'Leandro von Werra', 'Thomas Wolf'], 'affiliations': ['HuggingFaceTB'], 'pdf_title_img': 'assets/pdf/title_img/2502.02737.jpg', 'data': {'categories': ['#open_source', '#dataset', '#low_resource', '#training', '#small_models'], 'emoji': 'ü§è', 'ru': {'title': '–ë–æ–ª—å—à–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ –º–∞–ª–µ–Ω—å–∫–æ–º –ø–∞–∫–µ—Ç–µ: SmolLM2 - –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å –≤–ø–µ—á–∞—Ç–ª—è—é—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é', 'desc': "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É SmolLM2 - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π '–º–∞–ª–µ–Ω—å–∫–æ–π' —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å 1,7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ ~11 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞, —Å–æ—á–µ—Ç–∞—é—â–µ–≥–æ –≤–µ–±-—Ç–µ–∫—Å—Ç—ã —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, –∫–æ–¥—É –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ SmolLM2 –ø—Ä–µ–≤–∑–æ—à–ª–∞ –¥—Ä—É–≥–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Qwen2.5-1.5B –∏ Llama3.2-1B."}, 'en': {'title': 'SmolLM2: Efficient Language Modeling for Resource-Constrained Environments', 'desc': 'This paper presents SmolLM2, a compact language model with 1.7 billion parameters designed to be efficient for deployment in resource-limited environments. The model is trained on an extensive dataset of approximately 11 trillion tokens, utilizing a multi-stage training approach that incorporates diverse data sources, including web text and specialized datasets for math and coding. The authors introduce new datasets to enhance training quality and perform systematic evaluations to optimize dataset mixing based on performance feedback. SmolLM2 demonstrates superior performance compared to other recent small language models, and the authors provide access to the model and datasets for further research.'}, 'zh': {'title': 'Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂº∫Â§ßÁ™ÅÁ†¥', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜSmolLM2ÁöÑÂºÄÂèëÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ∑Êúâ17‰∫øÂèÇÊï∞ÁöÑÂ∞èÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞Âº∫Â§ßÁöÑÊÄßËÉΩÔºåÊàë‰ª¨Âú®Á∫¶11‰∏á‰∫ø‰∏™Êï∞ÊçÆ‰∏äËøõË°å‰∫ÜËøáÂ∫¶ËÆ≠ÁªÉÔºåÈááÁî®‰∫ÜÂ§öÈò∂ÊÆµËÆ≠ÁªÉËøáÁ®ãÔºåÁªìÂêà‰∫ÜÁΩëÁªúÊñáÊú¨„ÄÅÊï∞Â≠¶„ÄÅ‰ª£Á†ÅÂíåÊåá‰ª§Ë∑üÈöèÊï∞ÊçÆ„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÊñ∞ÁöÑ‰∏ìÁî®Êï∞ÊçÆÈõÜÔºå‰ª•Ëß£ÂÜ≥Áé∞ÊúâÊï∞ÊçÆÈõÜËßÑÊ®°Â∞èÊàñË¥®Èáè‰ΩéÁöÑÈóÆÈ¢ò„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ËØÅÊòéSmolLM2Âú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñËøëÊúüÁöÑÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂ¶ÇQwen2.5-1.5BÂíåLlama3.2-1B„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01506', 'title': 'TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets', 'url': 'https://huggingface.co/papers/2502.01506', 'abstract': 'The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.', 'score': 26, 'issue_id': 2063, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'f5ec0450054af574', 'authors': ['Yuzhe Yang', 'Yifei Zhang', 'Minghao Wu', 'Kaidi Zhang', 'Yunmiao Zhang', 'Honghai Yu', 'Yan Hu', 'Benyou Wang'], 'affiliations': ['Nanjing University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.01506.jpg', 'data': {'categories': ['#multimodal', '#agents'], 'emoji': 'üìä', 'ru': {'title': 'LLM-–∞–≥–µ–Ω—Ç—ã —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Ç–∞–π–Ω—ã —Å–æ—Ü–∏–∞–ª—å–Ω–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π –¥–∏–Ω–∞–º–∏–∫–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ TwinMarket –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç LLM-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è, —É—á–∏—Ç—ã–≤–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã. –í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –Ω–∞ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ–Ω–¥–æ–≤–æ–º —Ä—ã–Ω–∫–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è, –∫–∞–∫ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –≥—Ä—É–ø–ø–æ–≤–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é –∏ —ç–º–µ—Ä–≥–µ–Ω—Ç–Ω—ã–º —è–≤–ª–µ–Ω–∏—è–º. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º –ø—Ä–∏–Ω—è—Ç–∏–µ–º —Ä–µ—à–µ–Ω–∏–π –∏ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–º–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏.'}, 'en': {'title': 'Harnessing LLMs for Realistic Socio-Economic Simulations', 'desc': 'This paper presents TwinMarket, a new framework that uses large language models (LLMs) to simulate socio-economic systems. Unlike traditional Agent-Based Models, TwinMarket captures the complexity of human behavior, including cognitive biases and emotional influences. The framework allows for the exploration of how individual actions can lead to collective dynamics, such as financial bubbles and recessions, in a simulated stock market. By leveraging LLMs, the study provides deeper insights into the interactions between individual decision-making and broader socio-economic patterns.'}, 'zh': {'title': 'Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊ®°ÊãüÁ§æ‰ºöÁªèÊµéÁ≥ªÁªüÁöÑÊ∂åÁé∞Áé∞Ë±°', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÁ§æ‰ºöÊ∂åÁé∞Áé∞Ë±°Ôºå‰º†ÁªüÁöÑÂü∫‰∫éËßÑÂàôÁöÑ‰ª£ÁêÜÊ®°ÂûãÔºàABMÔºâÈöæ‰ª•ÊçïÊçâ‰∫∫Á±ªË°å‰∏∫ÁöÑÂ§öÊ†∑ÊÄßÂíåÂ§çÊùÇÊÄßÔºåÂ∞§ÂÖ∂ÊòØË°å‰∏∫ÁªèÊµéÂ≠¶‰∏≠Âº∫Ë∞ÉÁöÑÈùûÁêÜÊÄßÂõ†Á¥†„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§ö‰ª£ÁêÜÊ°ÜÊû∂TwinMarketÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊù•Ê®°ÊãüÁ§æ‰ºöÁªèÊµéÁ≥ªÁªü„ÄÇÈÄöËøáÊ®°ÊãüËÇ°Á•®Â∏ÇÂú∫ÁéØÂ¢ÉÁöÑÂÆûÈ™åÔºåÊàë‰ª¨Â±ïÁ§∫‰∫Ü‰∏™‰ΩìË°å‰∏∫Â¶Ç‰ΩïÈÄöËøá‰∫íÂä®ÂíåÂèçÈ¶àÊú∫Âà∂ÂºïÂèëÈõÜ‰ΩìÂä®ÊÄÅÔºåÂØºËá¥ÈáëËûçÊ≥°Ê≤´ÂíåÁªèÊµéË°∞ÈÄÄÁ≠âÊ∂åÁé∞Áé∞Ë±°„ÄÇËØ•ÊñπÊ≥ï‰∏∫‰∏™‰ΩìÂÜ≥Á≠ñ‰∏éÈõÜ‰ΩìÁ§æ‰ºöÁªèÊµéÊ®°Âºè‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ªÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03373', 'title': 'Demystifying Long Chain-of-Thought Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2502.03373', 'abstract': 'Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.', 'score': 20, 'issue_id': 2064, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'a1d00a6c8452131a', 'authors': ['Edward Yeo', 'Yuxuan Tong', 'Morry Niu', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University', 'IN.AI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.03373.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rl', '#training', '#long_context'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å–∫—Ä—ã–≤–∞—è —Å–µ–∫—Ä–µ—Ç—ã –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ CoT —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–≥—Ä–∞–¥ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö CoT —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM.'}, 'en': {'title': 'Unlocking Reasoning Power in Large Language Models', 'desc': 'This paper explores how to improve reasoning in large language models (LLMs) by enhancing their inference capabilities through long chains-of-thought (CoTs). It highlights the importance of reinforcement learning (RL) in developing these reasoning skills, while also addressing the unclear conditions for the emergence of long CoTs. The study presents four key findings, including the role of supervised fine-tuning (SFT) in simplifying training, the necessity of reward shaping for stabilizing CoT growth, and the significance of scaling reward signals for effective RL. Overall, the research provides valuable insights for optimizing training strategies to boost long CoT reasoning in LLMs.'}, 'zh': {'title': '‰ºòÂåñËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊèêÂçáÈïøÊé®ÁêÜÈìæËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÈïøÊé®ÁêÜÈìæÔºàCoTsÔºâÁöÑÁîüÊàêÊú∫Âà∂ÔºåÊè≠Á§∫‰∫ÜÂΩ±ÂìçÊ®°ÂûãÁîüÊàêÈïøÊé®ÁêÜÈìæÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåËôΩÁÑ∂ÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâ‰∏çÊòØÁªùÂØπÂøÖË¶ÅÁöÑÔºå‰ΩÜÂÆÉÂèØ‰ª•ÁÆÄÂåñËÆ≠ÁªÉËøáÁ®ãÂπ∂ÊèêÈ´òÊïàÁéá„ÄÇÈöèÁùÄËÆ≠ÁªÉËÆ°ÁÆóËÉΩÂäõÁöÑÂ¢ûÂä†ÔºåÊé®ÁêÜËÉΩÂäõÊúâÂèØËÉΩÂá∫Áé∞Ôºå‰ΩÜÂÖ∂ÂèëÂ±ïÂπ∂‰∏çÊÄªÊòØ‰øùËØÅÔºåÂõ†Ê≠§Â•ñÂä±ËÆæËÆ°ÂØπ‰∫éÁ®≥ÂÆöÊé®ÁêÜÈìæÁöÑÈïøÂ∫¶Â¢ûÈïøËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫‰ºòÂåñËÆ≠ÁªÉÁ≠ñÁï•‰ª•Â¢ûÂº∫LLMs‰∏≠ÁöÑÈïøÊé®ÁêÜÈìæÊèê‰æõ‰∫ÜÂÆûÁî®ÊåáÂØº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03387', 'title': 'LIMO: Less is More for Reasoning', 'url': 'https://huggingface.co/papers/2502.03387', 'abstract': 'We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (>100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models\' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model\'s encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as "cognitive templates" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.', 'score': 18, 'issue_id': 2066, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'ad1fa98bc3904527', 'authors': ['Yixin Ye', 'Zhen Huang', 'Yang Xiao', 'Ethan Chern', 'Shijie Xia', 'Pengfei Liu'], 'affiliations': ['SJTU, SII, GAIR'], 'pdf_title_img': 'assets/pdf/title_img/2502.03387.jpg', 'data': {'categories': ['#open_source', '#dataset', '#reasoning', '#training', '#math'], 'emoji': 'üß†', 'ru': {'title': '–ú–µ–Ω—å—à–µ –∑–Ω–∞—á–∏—Ç –±–æ–ª—å—à–µ: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –º–æ–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å —Å –ø–æ–º–æ—â—å—é —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤. –ò—Ö –º–æ–¥–µ–ª—å LIMO –¥–æ—Å—Ç–∏–≥–ª–∞ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 817 –æ–±—É—á–∞—é—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ, —á–µ–º —É –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤. LIMO —Ç–∞–∫–∂–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ø—Ä–µ–≤–∑–æ–π–¥—è –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ–º –æ–±—ä–µ–º–µ –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–ø–æ—Ç–µ–∑—É LIMO, —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ—Ç–æ—Ä–æ–π —Å–ª–æ–∂–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–æ–≥—É—Ç –≤–æ–∑–Ω–∏–∫–∞—Ç—å —á–µ—Ä–µ–∑ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ, –Ω–æ —Ç–æ—á–Ω–æ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö.'}, 'en': {'title': 'Less Data, More Reasoning: The LIMO Hypothesis', 'desc': 'This paper reveals a surprising finding about how large language models can perform complex reasoning tasks. It shows that instead of needing a lot of training data, a model called LIMO can achieve high accuracy in mathematical reasoning with only a small number of examples. LIMO outperforms previous models that used much more data, indicating that less can be more when it comes to training for reasoning tasks. The authors introduce the Less-Is-More Reasoning Hypothesis, suggesting that a well-prepared model can effectively learn complex reasoning from minimal, well-structured examples.'}, 'zh': {'title': 'Â∞ëÂç≥ÊòØÂ§öÔºåÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞ÂèëÁé∞', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÈ°πÈáçË¶ÅÂèëÁé∞ÔºåÊåëÊàò‰∫ÜÊàë‰ª¨ÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠Â§çÊùÇÊé®ÁêÜËÉΩÂäõ‰∫ßÁîüÊú∫Âà∂ÁöÑÁêÜËß£„ÄÇ‰º†ÁªüËßÇÁÇπËÆ§‰∏∫ÔºåÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°ÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ΩÜÊàë‰ª¨ËØÅÊòéÂè™ÈúÄÂ∞ëÈáèÁ§∫‰æãÂç≥ÂèØÊúâÊïàÂºïÂèëÂ§çÊùÇÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãLIMOÂú®Êï∞Â≠¶Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫ÂâçÊâÄÊú™ÊúâÁöÑÊÄßËÉΩÔºå‰ΩøÁî®‰ªÖ817‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÔºåÂàÜÂà´Âú®AIMEÂíåMATH‰∏äËææÂà∞‰∫Ü57.1%Âíå94.8%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑ‚ÄúÂ∞ëÂç≥ÊòØÂ§öÊé®ÁêÜÂÅáËÆæ‚ÄùË°®ÊòéÔºåÂú®Âü∫Á°ÄÊ®°Âûã‰∏≠ÔºåÁªèËøáÂÖÖÂàÜÁºñÁ†ÅÁöÑÈ¢ÜÂüüÁü•ËØÜÂèØ‰ª•ÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑÂ∞ëÈáèÁ§∫‰æãÊù•ÊøÄÂèëÂ§çÊùÇÊé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.02339', 'title': 'Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking', 'url': 'https://huggingface.co/papers/2502.02339', 'abstract': "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0%) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2%) while maintaining substantial data and computational efficiency.", 'score': 9, 'issue_id': 2063, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '3f3413717efb32f6', 'authors': ['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Ruihan Jin', 'Feihu Che', 'Zengqi Wen', 'Jianhua Tao'], 'affiliations': ['Beijing', 'Department of Automation, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.02339.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#multimodal', '#architecture', '#training'], 'emoji': 'üß†', 'ru': {'title': 'AStar: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ AStar, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –ø–æ –¥–µ—Ä–µ–≤—É (MCTS). AStar –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ —Å –≤–Ω–µ—à–Ω–∏–º–∏ —É–∫–∞–∑–∞–Ω–∏—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ AStar –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 54.0% –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MathVerse, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è GPT-4o –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.'}, 'en': {'title': 'AStar: Enhancing Multimodal Reasoning with Efficient Structured Thinking', 'desc': "This paper introduces AStar, a new approach to improve the reasoning abilities of multimodal large language models (MLLMs) using Monte Carlo Tree Search (MCTS). AStar focuses on deriving high-level cognitive reasoning patterns from limited data, which helps in enhancing the models' performance without requiring extensive data sets. The framework integrates internal reasoning capabilities of the models with external guidelines, allowing for efficient inference with fewer iterations. Experimental results show that AStar outperforms existing models like GPT-4o in accuracy while being more data and computationally efficient."}, 'zh': {'title': 'AStarÔºöÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÊñ∞ËåÉÂºè', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§çÊùÇËßÜËßâÊé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜ‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÁöÑÁ†îÁ©∂Â∞ùËØïÈÄöËøáÂºïÂÖ•ÁªìÊûÑÂåñÊÄùÁª¥ÂíåÊïôÂ∏àÊåáÂØºÊù•Â¢ûÂº∫Êé®ÁêÜËÉΩÂäõÔºå‰ΩÜÂú®ÊÄßËÉΩÂíåÊïàÁéá‰πãÈó¥ÁöÑÂπ≥Ë°°‰ªçÁÑ∂Âõ∞Èöæ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫AStarÁöÑËá™Âä®ÂåñÁªìÊûÑÂåñÊÄùÁª¥ËåÉÂºèÔºåÂà©Áî®ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâ‰ªéÊúâÈôêÊï∞ÊçÆ‰∏≠Ëá™Âä®Êé®ÂØºÈ´òÂ±ÇÊ¨°ÁöÑËÆ§Áü•Êé®ÁêÜÊ®°Âºè„ÄÇAStarÈÄöËøáÁªü‰∏ÄÁöÑÊé®ÁêÜÊ°ÜÊû∂ÔºåÁªìÂêàÊ®°ÂûãÁöÑÂÜÖÈÉ®Êé®ÁêÜËÉΩÂäõÂíåÂ§ñÈÉ®Êé®ÁêÜÊåáÂØºÔºåÂÆûÁé∞È´òÊïàÊé®ÁêÜÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÂíåÊï∞ÊçÆÂà©Áî®ÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01105', 'title': 'LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer', 'url': 'https://huggingface.co/papers/2502.01105', 'abstract': "Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer's superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition.", 'score': 6, 'issue_id': 2067, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'b4eb829c549c6a2e', 'authors': ['Yiren Song', 'Danze Chen', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.01105.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#diffusion', '#cv', '#dataset'], 'emoji': 'üé®', 'ru': {'title': 'LayerTracer: –ò–ò-–¥–∏–∑–∞–π–Ω–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏', 'desc': 'LayerTracer - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö SVG –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ. –û–Ω –∏–º–∏—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–±–æ—Ç—ã –¥–∏–∑–∞–π–Ω–µ—Ä–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Ç—Ä–æ–≤—ã–µ —á–µ—Ä—Ç–µ–∂–∏, –∞ –∑–∞—Ç–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑—É—è –∏—Ö –ø–æ —Å–ª–æ—è–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å–ª–æ–≤–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ LayerTracer –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏.'}, 'en': {'title': 'LayerTracer: Bridging the Gap in Layered SVG Generation', 'desc': 'This paper introduces LayerTracer, a new framework that improves the generation of layered SVGs by learning from how designers create them. It uses a two-phase process: first, it generates rasterized blueprints that mimic human design steps, and then it converts these into clean, editable SVGs while removing duplicate paths. The framework employs a conditional diffusion mechanism to ensure that the generated images maintain their structure and quality. Experiments show that LayerTracer outperforms existing methods in both the quality of the generated designs and their ease of editing, aligning better with professional design practices.'}, 'zh': {'title': 'LayerTracerÔºöÊô∫ËÉΩÁîüÊàêÂèØÁºñËæëÁöÑÂàÜÂ±ÇSVGÂõæÂΩ¢', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫LayerTracerÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÁîüÊàêËÆ§Áü•ÂØπÈΩêÁöÑÂàÜÂ±ÇSVGÂõæÂΩ¢„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ≠¶‰π†ËÆæËÆ°Â∏àÁöÑÂàÜÂ±ÇSVGÂàõÂª∫ËøáÁ®ãÔºåÂà©Áî®‰∏Ä‰∏™Êñ∞È¢ñÁöÑÈ°∫Â∫èËÆæËÆ°Êìç‰ΩúÊï∞ÊçÆÈõÜ„ÄÇLayerTracerÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÔºåÂü∫‰∫éÊñáÊú¨ÁöÑÊâ©Êï£ÂèòÊç¢Âô®ÁîüÊàêÂ§öÈò∂ÊÆµÁöÑÂÖâÊ†ÖÂåñÊûÑÂª∫ËìùÂõæÔºõÂÖ∂Ê¨°ÔºåÈÄöËøáË∑ØÂæÑÂéªÈáçÂÆûÁé∞ÂàÜÂ±ÇÁü¢ÈáèÂåñÔºåÁîüÊàêÂπ≤ÂáÄ‰∏îÂèØÁºñËæëÁöÑSVGÊñá‰ª∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLayerTracerÂú®ÁîüÊàêË¥®ÈáèÂíåÂèØÁºñËæëÊÄßÊñπÈù¢‰ºò‰∫éÂü∫‰∫é‰ºòÂåñÂíåÁ•ûÁªèÁΩëÁªúÁöÑÂü∫Á∫øÊñπÊ≥ïÔºåÊúâÊïàÂú∞Â∞ÜAIÁîüÊàêÁöÑÁü¢ÈáèÂõæ‰∏é‰∏ì‰∏öËÆæËÆ°ËÆ§Áü•ÂØπÈΩê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.02671', 'title': 'On Teacher Hacking in Language Model Distillation', 'url': 'https://huggingface.co/papers/2502.02671', 'abstract': "Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.", 'score': 5, 'issue_id': 2072, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'defca87e9bf06d0b', 'authors': ['Daniil Tiapkin', 'Daniele Calandriello', 'Johan Ferret', 'Sarah Perrin', 'Nino Vieillard', 'Alexandre Ram√©', 'Mathieu Blondel'], 'affiliations': ['Ecole 1CMAP, France; Polytechnique', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.02671.jpg', 'data': {'categories': ['#alignment', '#optimization', '#rlhf', '#training', '#data'], 'emoji': 'üß†', 'ru': {'title': "–ë–æ—Ä—å–±–∞ —Å 'teacher hacking': –∫–ª—é—á –∫ robust —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º", 'desc': "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ñ–µ–Ω–æ–º–µ–Ω 'teacher hacking' –ø—Ä–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—É—é —É—Å—Ç–∞–Ω–æ–≤–∫—É —Å –æ—Ä–∞–∫—É–ª–æ–º, —É—á–∏—Ç–µ–ª–µ–º –∏ —É—á–µ–Ω–∏–∫–æ–º –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è —ç—Ç–æ–≥–æ —è–≤–ª–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ 'teacher hacking' –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ñ–ª–∞–π–Ω-–¥–∞—Ç–∞—Å–µ—Ç–∞, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–º—è–≥—á–µ–Ω —Å –ø–æ–º–æ—â—å—é –æ–Ω–ª–∞–π–Ω-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è 'hacking' –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."}, 'en': {'title': 'Preventing Teacher Hacking: The Key Role of Data Diversity in Distillation', 'desc': "This paper explores the concept of 'teacher hacking' in the context of knowledge distillation for language models (LMs). Teacher hacking occurs when a student LM overly optimizes based on an imperfect teacher LM, leading to poor performance on the actual task. The authors conducted experiments using an oracle LM as the ground truth, a teacher LM distilled from it, and a student LM distilled from the teacher. They found that using diverse online data can prevent teacher hacking, highlighting the importance of data diversity in the distillation process."}, 'zh': {'title': 'Èò≤Ê≠¢ÊïôÂ∏àÈªëÂÆ¢ÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑËí∏È¶èÊïàÊûú', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÂú®Áü•ËØÜËí∏È¶èÈò∂ÊÆµÂèØËÉΩÂá∫Áé∞ÁöÑ‚ÄúÊïôÂ∏àÈªëÂÆ¢‚ÄùÁé∞Ë±°„ÄÇÊïôÂ∏àÈªëÂÆ¢ÊòØÊåáÂ≠¶ÁîüÊ®°ÂûãÂú®Ê®°‰ªøÊïôÂ∏àÊ®°ÂûãÊó∂ÔºåËøáÂ∫¶‰ºòÂåñÂØºËá¥ÊÄßËÉΩ‰∏ãÈôçÁöÑÊÉÖÂÜµ„ÄÇËøôÁßçÁé∞Ë±°‰∏éÂè§Âæ∑ÂìàÁâπÊ≥ïÂàôÁõ∏Á¨¶ÔºåÂèØËÉΩÊ∫ê‰∫éÊïôÂ∏àÊ®°ÂûãÂØπÁúüÂÆûÂàÜÂ∏ÉÁöÑ‰∏çÂÆåÁæéËøë‰ºº„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®Âõ∫ÂÆöÁöÑÁ¶ªÁ∫øÊï∞ÊçÆÈõÜËøõË°åËí∏È¶èÊó∂ÔºåÊïôÂ∏àÈªëÂÆ¢Áé∞Ë±°‰ºöÂèëÁîüÔºåËÄåÈááÁî®Âú®Á∫øÊï∞ÊçÆÁîüÊàêÊäÄÊúØÂàôËÉΩÊúâÊïàÁºìËß£Ëøô‰∏ÄÈóÆÈ¢òÔºåÊï∞ÊçÆÂ§öÊ†∑ÊÄßÊòØÈò≤Ê≠¢ÊïôÂ∏àÈªëÂÆ¢ÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01618', 'title': 'A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods', 'url': 'https://huggingface.co/papers/2502.01618', 'abstract': 'Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code and further information is available at https://probabilistic-inference-scaling.github.io.', 'score': 5, 'issue_id': 2065, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'c9971916eb027101', 'authors': ['Isha Puri', 'Shivchander Sudalairaj', 'Guangxuan Xu', 'Kai Xu', 'Akash Srivastava'], 'affiliations': ['MIT CSAIL', 'Red Hat AI Innovation'], 'pdf_title_img': 'assets/pdf/title_img/2502.01618.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#math', '#inference'], 'emoji': 'üé≤', 'ru': {'title': '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –≤—ã–≤–æ–¥–∞ LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –í–º–µ—Å—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∞–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –∑–∞–¥–∞—á—É –∫–∞–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –≤—ã–≤–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥—ã –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–∏—Ü. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∏–º–µ–µ—Ç –≤ 4-16 —Ä–∞–∑ –ª—É—á—à—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ –Ω–µ–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏—á—å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –º–µ–Ω—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—Ä–æ–≥–æ–Ω–æ–≤.'}, 'en': {'title': 'Revolutionizing Inference: Probabilistic Scaling for LLMs', 'desc': 'This paper addresses the limitations of scaling large language models (LLMs) by focusing on improving inference time rather than just increasing model size or data. The authors propose a new approach that treats inference-time scaling as a probabilistic inference task, using sampling techniques to better explore the state distribution. By applying particle-based Monte Carlo methods, their method shows significant improvements in scaling rates compared to traditional deterministic search methods. The results indicate that their approach can achieve higher accuracy with fewer rollouts, demonstrating a promising direction for enhancing LLM performance.'}, 'zh': {'title': 'Êé®ÁêÜÊó∂Èó¥Êâ©Â±ïÁöÑÊñ∞ÊñπÊ≥ïÔºöÊ¶ÇÁéáÊé®ÁêÜ‰∏éÁ≤íÂ≠êÈááÊ†∑ÁªìÂêà', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈÄöËøáÂ¢ûÂä†Ê®°ÂûãËßÑÊ®°ÂíåÊï∞ÊçÆÈáèÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÁÑ∂ËÄåÔºåÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÁöÑÊî∂ÁõäÈÄíÂáèÔºå‰øÉ‰ΩøÊàë‰ª¨ËÄÉËôëÂú®Êé®ÁêÜÊó∂Â¢ûÂä†ËÆ°ÁÆóÈáè„ÄÇÁé∞ÊúâÁöÑÊé®ÁêÜÊó∂Èó¥Êâ©Â±ïÊñπÊ≥ïÈÄöÂ∏∏Â∞Ü‰ªªÂä°ËßÜ‰∏∫ÊêúÁ¥¢ÈóÆÈ¢òÔºåÂÆπÊòìÂèóÂà∞Â•ñÂä±Ê®°ÂûãÁöÑËøë‰ººËØØÂ∑ÆÂΩ±ÂìçËÄåÂØºËá¥Â•ñÂä±ÊìçÊéß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜÊó∂Èó¥Êâ©Â±ïÊñπÊ≥ïÔºåÈÄöËøáÈÄÇÂ∫îÂü∫‰∫éÁ≤íÂ≠êÁöÑËíôÁâπÂç°Ê¥õÊñπÊ≥ïÔºåÂ∞ÜÊé®ÁêÜÊó∂Èó¥Êâ©Â±ïËßÜ‰∏∫Ê¶ÇÁéáÊé®ÁêÜ‰ªªÂä°Ôºå‰ªéËÄåÂú®ÂêÑÁßçÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊõ¥Â•ΩÁöÑÊâ©Â±ïÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01154', 'title': 'Jailbreaking with Universal Multi-Prompts', 'url': 'https://huggingface.co/papers/2502.01154', 'abstract': 'Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques.', 'score': 3, 'issue_id': 2068, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'aa9860c81d83ac21', 'authors': ['Yu-Ling Hsu', 'Hsuan Su', 'Shang-Tse Chen'], 'affiliations': ['National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01154.jpg', 'data': {'categories': ['#security', '#rl', '#data', '#optimization', '#transfer_learning', '#training', '#ethics'], 'emoji': 'üîì', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤–∑–ª–æ–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ JUMP', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º JUMP –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –≤–∑–ª–æ–º–∞ (jailbreak) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏-–ø—Ä–æ–º–ø—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –∑–∞—â–∏—Ç—ã, –Ω–∞–∑—ã–≤–∞–µ–º—É—é DUMP. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º—É–ª—å—Ç–∏-–ø—Ä–æ–º–ø—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ç–µ–º—É —ç—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –∏ –Ω–æ–≤—ã—Ö —Ç–∏–ø–æ–≤ –∞—Ç–∞–∫ –Ω–∞ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'JUMP: Universal Multi-Prompts for Jailbreaking LLMs', 'desc': 'This paper presents JUMP, a novel method for jailbreaking large language models (LLMs) using universal multi-prompts. Unlike traditional prompting techniques that focus on specific adversarial inputs, JUMP aims to create a universal attacker that can adapt to various unseen tasks, reducing computational costs. Additionally, the authors propose a defense mechanism called DUMP, which leverages the same principles to protect against such attacks. Experimental results indicate that JUMP significantly outperforms existing methods in optimizing these universal multi-prompts.'}, 'zh': {'title': 'ÈÄöÁî®Â§öÊèêÁ§∫ÔºöÁ†¥Ëß£‰∏éÈò≤Âæ°ÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøëÂπ¥Êù•ËøÖÈÄüÂèëÂ±ïÔºåÊîπÂèò‰∫ÜËÆ∏Â§öÂ∫îÁî®ÔºåÊòæËëóÊèêÈ´ò‰∫Ü‰æøÂà©ÊÄßÂíåÁîü‰∫ßÂäõ„ÄÇÁÑ∂ËÄåÔºåÈöèÁùÄÂÖ∂Âº∫Â§ßËÉΩÂäõÁöÑÊèêÂçáÔºå‰º¶ÁêÜÈóÆÈ¢òÂíåÊñ∞ÂûãÊîªÂáªÔºàÂ¶ÇË∂äÁã±ÊîªÂáªÔºâ‰πüÈöè‰πãÂá∫Áé∞„ÄÇÂ§ßÂ§öÊï∞ÊèêÁ§∫ÊäÄÊúØ‰∏ìÊ≥®‰∫é‰ºòÂåñÂçï‰∏™Ê°à‰æãÁöÑÂØπÊäóËæìÂÖ•ÔºåËøôÂú®Â§ÑÁêÜÂ§ßÊï∞ÊçÆÈõÜÊó∂‰ºöÂØºËá¥Êõ¥È´òÁöÑËÆ°ÁÆóÊàêÊú¨„ÄÇÊú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫JUMPÁöÑÊñπÊ≥ïÔºåÊó®Âú®‰ΩøÁî®ÈÄöÁî®Â§öÊèêÁ§∫ÂØπLLMsËøõË°åË∂äÁã±ÔºåÂêåÊó∂Êàë‰ª¨ËøòÊèêÂá∫‰∫ÜÈò≤Âæ°ÊñπÊ≥ïDUMPÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®‰ºòÂåñÈÄöÁî®Â§öÊèêÁ§∫ÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊäÄÊúØ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03275', 'title': 'Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning', 'url': 'https://huggingface.co/papers/2502.03275', 'abstract': 'Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.', 'score': 3, 'issue_id': 2066, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'f94d674e0f57dcf9', 'authors': ['DiJia Su', 'Hanlin Zhu', 'Yingchen Xu', 'Jiantao Jiao', 'Yuandong Tian', 'Qinqing Zheng'], 'affiliations': ['Meta AI', 'UC Berkeley', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2502.03275.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#optimization', '#training', '#benchmark', '#math'], 'emoji': 'üß†', 'ru': {'title': '–ì–∏–±—Ä–∏–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ VQ-VAE, –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–π –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –¥–ª–∏–Ω—É –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è, —Ç–∞–∫ –∏ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö LLM –Ω–∞ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Streamlining Reasoning with Hybrid Token Representations', 'desc': 'This paper presents a new method to improve reasoning in Large Language Models (LLMs) by using a hybrid representation of reasoning processes. Instead of relying solely on lengthy text inputs, the authors introduce latent discrete tokens generated by VQ-VAE to simplify the reasoning steps. This approach reduces the input length and computational resources needed while maintaining effective reasoning capabilities. The proposed method shows superior performance in training and fine-tuning scenarios for logical and mathematical reasoning tasks compared to traditional methods.'}, 'zh': {'title': '‰ºòÂåñÊé®ÁêÜËøáÁ®ãÔºåÊèêÂçáÊ®°ÂûãÊïàÁéá', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜÂíåËßÑÂàí‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂú®ÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊï∞ÊçÆËÆ≠ÁªÉÊó∂ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàË°®Á§∫Ê≥ïÔºåÈÄöËøá‰ΩøÁî®VQ-VAEÁîüÊàêÁöÑÊΩúÂú®Á¶ªÊï£Ê†áËÆ∞ÔºåÈÉ®ÂàÜÊäΩË±°ÂåñÂàùÂßãÊé®ÁêÜÊ≠•È™§Ôºå‰ªéËÄåÊòæËëóÂáèÂ∞ëÊé®ÁêÜËøáÁ®ãÁöÑÈïøÂ∫¶„ÄÇÊàë‰ª¨Âú®‰∏§‰∏™Âú∫ÊôØ‰∏≠Êé¢Á¥¢‰∫ÜÊΩúÂú®ËøΩË∏™ÊäΩË±°ÁöÑ‰ΩøÁî®Ôºö‰∏ÄÊòØ‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÊ®°ÂûãËß£ÂÜ≥Èí•ÂåôÂØªÊâæËø∑ÂÆ´ÈóÆÈ¢òÔºå‰∫åÊòØÂØπLLMsËøõË°åÂæÆË∞É‰ª•Â§ÑÁêÜÈÄªËæëÂíåÊï∞Â≠¶Êé®ÁêÜÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÈÄöËøáÈöèÊú∫Ê∑∑ÂêàÊΩúÂú®Ê†áËÆ∞ÂíåÊñáÊú¨Ê†áËÆ∞Ôºå‰øÉËøõ‰∫ÜÂØπÊñ∞ÊΩúÂú®Ê†áËÆ∞ÁöÑÂø´ÈÄüÈÄÇÂ∫îÔºå‰∏îÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.02928', 'title': 'Large Language Model Guided Self-Debugging Code Generation', 'url': 'https://huggingface.co/papers/2502.02928', 'abstract': 'Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.', 'score': 2, 'issue_id': 2075, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'dc7aaadeeee7e1e7', 'authors': ['Muntasir Adnan', 'Zhiwei Xu', 'Carlos C. N. Kuhn'], 'affiliations': ['Open Source Institute, Faculty of Science and Technology, University of Canberra, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2502.02928.jpg', 'data': {'categories': ['#agents', '#plp', '#training'], 'emoji': 'üêç', 'ru': {'title': 'PyCapsule: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è Python-–∫–æ–¥–∞ —Å —Å–∞–º–æ–æ—Ç–ª–∞–¥–∫–æ–π', 'desc': 'PyCapsule - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –Ω–∞ Python, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–≤—É—Ö–∞–≥–µ–Ω—Ç–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –∏ –º–æ–¥—É–ª–∏ —Å–∞–º–æ–æ—Ç–ª–∞–¥–∫–∏. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ª–æ–∂–Ω—ã–π –≤—ã–≤–æ–¥ –ø—Ä–æ–º–ø—Ç–æ–≤, –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. PyCapsule –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –û–¥–Ω–∞–∫–æ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –ø–æ–ø—ã—Ç–æ–∫ —Å–∞–º–æ–æ—Ç–ª–∞–¥–∫–∏, –≤–æ–∑–º–æ–∂–Ω–æ, –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –∏ –∑–∞—à—É–º–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ–± –æ—à–∏–±–∫–∞—Ö.'}, 'en': {'title': 'Revolutionizing Python Code Generation with PyCapsule', 'desc': 'This paper introduces PyCapsule, a new framework designed to enhance automated code generation, particularly for Python. It employs a two-agent pipeline that focuses on efficient self-debugging and robust error handling, addressing common issues in existing methods. The framework utilizes advanced prompt inference and iterative testing to improve the stability and correctness of generated code. Empirical results show that PyCapsule outperforms current state-of-the-art techniques in various benchmarks, highlighting its potential for more efficient AI-driven programming solutions.'}, 'zh': {'title': 'PyCapsuleÔºöÈ´òÊïàÁöÑËá™Âä®Âåñ‰ª£Á†ÅÁîüÊàêÊ°ÜÊû∂', 'desc': 'Ëá™Âä®Âåñ‰ª£Á†ÅÁîüÊàêÂú®Êô∫ËÉΩËÆ°ÁÆóÊú∫ÁºñÁ®ãÂíåÁ≥ªÁªüÈÉ®ÁΩ≤‰∏≠ÂèòÂæóË∂äÊù•Ë∂äÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÊñπÊ≥ïÂú®ËÆ°ÁÆóÊïàÁéá‰∏äÂ∏∏Â∏∏Èù¢‰∏¥ÊåëÊàòÔºåÂπ∂‰∏îÁº∫‰πèÂº∫Â§ßÁöÑ‰ª£Á†ÅËß£ÊûêÂíåÈîôËØØ‰øÆÊ≠£Êú∫Âà∂„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂PyCapsuleÔºåÈááÁî®ÁÆÄÂçïËÄåÊúâÊïàÁöÑÂèå‰ª£ÁêÜÁÆ°ÈÅìÂíåÈ´òÊïàÁöÑËá™ÊàëË∞ÉËØïÊ®°ÂùóÊù•ÁîüÊàêPython‰ª£Á†Å„ÄÇPyCapsuleÈÄöËøáÂ§çÊùÇÁöÑÊèêÁ§∫Êé®ÁêÜ„ÄÅËø≠‰ª£ÈîôËØØÂ§ÑÁêÜÂíåÊ°à‰æãÊµãËØïÔºåÁ°Æ‰øù‰∫ÜÈ´òÁîüÊàêÁ®≥ÂÆöÊÄß„ÄÅÂÆâÂÖ®ÊÄßÂíåÊ≠£Á°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.02421', 'title': 'Activation-Informed Merging of Large Language Models', 'url': 'https://huggingface.co/papers/2502.02421', 'abstract': 'Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\\% increase in benchmark performance.', 'score': 1, 'issue_id': 2079, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '90e80efaaef789ec', 'authors': ['Amin Heyrani Nobari', 'Kaveh Alimohammadi', 'Ali ArjomandBigdeli', 'Akash Srivastava', 'Faez Ahmed', 'Navid Azizan'], 'affiliations': ['Massachusetts Institute of Technology', 'RedHat AI Innovation & MIT-IBM Watson AI Lab', 'Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2502.02421.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': 'AIM: –£–º–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Activation-Informed Merging (AIM). AIM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω–∏–º –∫ –ª—é–±–æ–º—É —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É —Å–ø–æ—Å–æ–±—É —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –¥–æ 40%.'}, 'en': {'title': 'Boosting Model Performance with Activation-Informed Merging', 'desc': 'This paper presents Activation-Informed Merging (AIM), a novel technique for merging large language models (LLMs) that leverages activation space information to improve model performance. AIM enhances the merging process by selectively prioritizing essential weights from the base models, which helps maintain robustness and efficiency. The method is flexible and can be integrated with existing merging techniques, making it widely applicable. Empirical results show that AIM can lead to significant performance improvements, achieving up to a 40% increase in benchmark scores for merged models.'}, 'zh': {'title': 'ÊøÄÊ¥ª‰ø°ÊÅØÂêàÂπ∂ÔºöÊèêÂçáÊ®°ÂûãÂêàÂπ∂ÊÄßËÉΩÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Ê®°ÂûãÂêàÂπ∂ÊòØ‰∏ÄÁßçÂ∞ÜÂ§ö‰∏™ÂæÆË∞ÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèÇÊï∞ÂíåÂµåÂÖ•ÁªìÂêàËµ∑Êù•ÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÂú®‰øùÊåÅËÆ°ÁÆóÊïàÁéáÁöÑÂêåÊó∂ÊèêÂçáÊ®°ÂûãÂú®ÂêÑÁßç‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÊøÄÊ¥ª‰ø°ÊÅØÂêàÂπ∂ÔºàAIMÔºâÁöÑÊäÄÊúØÔºåÂÆÉÂ∞ÜLLMsÁöÑÊøÄÊ¥ªÁ©∫Èó¥‰ø°ÊÅØÊï¥ÂêàÂà∞ÂêàÂπ∂ËøáÁ®ã‰∏≠Ôºå‰ª•ÊèêÈ´òÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇAIMÊó®Âú®‰Ωú‰∏∫‰∏ÄÁßçÁÅµÊ¥ªÁöÑË°•ÂÖÖËß£ÂÜ≥ÊñπÊ°àÔºåÈÄÇÁî®‰∫é‰ªª‰ΩïÁé∞ÊúâÁöÑÂêàÂπ∂ÊñπÊ≥ïÔºåÂπ∂ÈÄöËøáÊåÅÁª≠Â≠¶‰π†ÂíåÊ®°ÂûãÂéãÁº©ÁöÑÂéüÂàôÊù•‰øùÁïôÂü∫Á°ÄÊ®°Âûã‰∏≠ÁöÑÂÖ≥ÈîÆÊùÉÈáç„ÄÇÈÄöËøá‰ΩøÁî®‰∏é‰ªªÂä°Êó†ÂÖ≥ÁöÑÊ†°ÂáÜÈõÜÔºåAIMÂú®ÂêàÂπ∂ËøáÁ®ã‰∏≠‰ºòÂÖàËÄÉËôëÈáçË¶ÅÊùÉÈáçÔºåÂÆûÈ™åËØÅÊòéAIMÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÂêàÂπ∂Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00306', 'title': 'Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2502.00306', 'abstract': "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.", 'score': 1, 'issue_id': 2076, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '4987f380f5ddb7af', 'authors': ['Ali Naseh', 'Yuefeng Peng', 'Anshuman Suri', 'Harsh Chaudhari', 'Alina Oprea', 'Amir Houmansadr'], 'affiliations': ['Northeastern University', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.00306.jpg', 'data': {'categories': ['#inference', '#rag', '#leakage', '#security'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–ù–µ–∑–∞–º–µ—Ç–Ω–∞—è –∞—Ç–∞–∫–∞ –Ω–∞ RAG-—Å–∏—Å—Ç–µ–º—ã: –∫–∞–∫ –≤—ã—è–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ –Ω–∞ —Å–∏—Å—Ç–µ–º—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (RAG). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–µ—Ö–Ω–∏–∫—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Interrogation Attack', –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π RAG-—Å–∏—Å—Ç–µ–º—ã. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ü–µ–ª–µ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∞—Ç–∞–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –∏ —Ç—Ä—É–¥–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏–º–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞–º–∏."}, 'en': {'title': 'Stealthy Inference: Unveiling Membership in RAG Systems', 'desc': 'This paper introduces a new method called Interrogation Attack (IA) for membership inference in Retrieval-Augmented Generation (RAG) systems. RAG allows Large Language Models (LLMs) to generate responses using external knowledge without changing their internal parameters, but this can be exploited by adversaries. The IA technique uses natural-text queries that can only be answered if a specific document is present, making it harder to detect than previous methods. The authors demonstrate that their approach is more effective and stealthy, achieving better performance with fewer queries and lower costs compared to existing techniques.'}, 'zh': {'title': 'ÈöêËîΩÁöÑ‰ºöÂëòÊé®Êñ≠ÊîªÂáªÔºöRAGÁ≥ªÁªüÁöÑÊñ∞ÊåëÊàò', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ºöÂëòÊé®Êñ≠ÊäÄÊúØÔºåÁß∞‰∏∫ÂÆ°ÈóÆÊîªÂáªÔºàInterrogation Attack, IAÔºâÔºåÊó®Âú®ÈíàÂØπRAGÊï∞ÊçÆÂ≠òÂÇ®‰∏≠ÁöÑÊñáÊ°£ËøõË°åÊîªÂáª„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊûÑÈÄ†Ëá™ÁÑ∂ËØ≠Ë®ÄÊü•ËØ¢Ôºå‰ªÖÂú®ÁõÆÊ†áÊñáÊ°£Â≠òÂú®Êó∂ÊâçËÉΩÂæóÂà∞Á≠îÊ°àÔºå‰ªéËÄåÂÆûÁé∞ÊúâÊïàÁöÑÊé®Êñ≠„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊîªÂáªÂú®‰ªÖ‰ΩøÁî®30‰∏™Êü•ËØ¢ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàêÂäüÁéáÊèêÈ´ò‰∫Ü2ÂÄçÔºåÂêåÊó∂‰øùÊåÅÈöêËîΩÊÄß„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåIAÂú®Â§öÁßçRAGÈÖçÁΩÆ‰∏ãÁöÑË°®Áé∞‰ºò‰∫é‰ª•ÂæÄÁöÑÊé®Êñ≠ÊîªÂáªÔºå‰∏îÊØè‰∏™ÊñáÊ°£ÁöÑÊé®Êñ≠ÊàêÊú¨‰Ωé‰∫é0.02ÁæéÂÖÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00226', 'title': 'HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems', 'url': 'https://huggingface.co/papers/2502.00226', 'abstract': 'Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks.', 'score': 0, 'issue_id': 2079, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': 'c9615b5d00a42037', 'authors': ['Jun Xing', 'Mayur Bhatia', 'Sahil Phulwani', 'Darshan Suresh', 'Rafik Matta'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00226.jpg', 'data': {'categories': ['#benchmark', '#science', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª LLM –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ü–û', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. HackerRank-ASTRA Benchmark –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∏–º–∏—Ç–∏—Ä—É—é—â–∏–µ —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏, –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ 32 –∑–∞–ø—É—Å–∫–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Ç—Ä–∏ –≤–µ–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ —Å—Ä–∞–≤–Ω–∏–º—ã—Ö —Å—Ä–µ–¥–Ω–∏—Ö –æ—Ü–µ–Ω–æ–∫ –≤ 75%. –ú–æ–¥–µ–ª—å Claude-3.5-Sonnet-1022 –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –Ω–∞–∏–≤—ã—Å—à—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –Ω–∏–∑–∫—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.'}, 'en': {'title': 'Benchmarking LLMs for Real-World Coding Consistency', 'desc': 'This paper evaluates the effectiveness of large language models (LLMs) in real-world software development tasks using the HackerRank-ASTRA Benchmark. Unlike previous benchmarks that focus on isolated coding problems, this benchmark introduces project-based scenarios that require multi-file handling and assesses model consistency through extensive testing. The study analyzes the performance of top models, revealing that while they achieved similar average scores, one model, Claude-3.5-Sonnet-1022, stood out for its high consistency and low variability in results. This research emphasizes the importance of rigorous evaluation methods to ensure LLMs are reliable for practical applications in coding.'}, 'zh': {'title': 'ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ËΩØ‰ª∂ÂºÄÂèë‰∏≠ÁöÑÁúüÂÆûÂ∫îÁî®ÊÄß', 'desc': 'ËøôÁØáËÆ∫ÊñáËØÑ‰º∞‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂÆûÈôÖËΩØ‰ª∂ÂºÄÂèë‰ªªÂä°‰∏≠ÁöÑÈÄÇÁî®ÊÄß„ÄÇÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØïÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®Âçï‰∏ÄÁöÑÁºñÁ†ÅÈóÆÈ¢òÊàñÁâπÂÆöÂ∫ìÔºåÂøΩËßÜ‰∫ÜÂ§öÊñá‰ª∂„ÄÅÂü∫‰∫éÈ°πÁõÆÁöÑÂú∫ÊôØÔºåÂπ∂Áº∫‰πèÂØπ‰∏ÄËá¥ÊÄßÁöÑ‰∏•Ê†ºËØÑ‰º∞„ÄÇHackerRank-ASTRAÂü∫ÂáÜÂºïÂÖ•‰∫ÜÊ®°ÊãüÁúüÂÆûÂú∫ÊôØÁöÑÈ°πÁõÆÂü∫Á°ÄÁºñÁ†ÅÈóÆÈ¢òÔºåÂπ∂ÈÄöËøá32Ê¨°ËøêË°åËØÑ‰º∞Ê®°ÂûãÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂàùÊ≠•ËØÑ‰º∞ÊòæÁ§∫ÔºåClaude-3.5-Sonnet-1022Âú®ÈóÆÈ¢ò‰∏ÄËá¥ÊÄßÊñπÈù¢Ë°®Áé∞ÊúÄ‰Ω≥ÔºåÂÖ∑ÊúâËæÉ‰ΩéÁöÑÂèòÂºÇÊÄßÔºåÁ™ÅÊòæ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖËΩØ‰ª∂ÂºÄÂèë‰ªªÂä°‰∏≠ÁöÑÂèØÈù†ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10389', 'title': 'Region-Adaptive Sampling for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2502.10389', 'abstract': "Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.", 'score': 44, 'issue_id': 2241, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '8068f45b7fd0c2ee', 'authors': ['Ziming Liu', 'Yifan Yang', 'Chengruidong Zhang', 'Yiqi Zhang', 'Lili Qiu', 'Yang You', 'Yuqing Yang'], 'affiliations': ['Microsoft Research', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.10389.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#diffusion', '#architecture', '#dataset'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RAS. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Ñ–æ–∫—É—Å –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ Diffusion Transformer. RAS –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ –æ–±–ª–∞—Å—Ç–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à—É–º –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ RAS –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ 2.51x –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'Accelerating Diffusion Transformers with RAS for Real-Time Generation', 'desc': "This paper introduces RAS, a new sampling strategy for Diffusion Transformers (DiTs) that improves the efficiency of generative tasks. Traditional diffusion models require multiple sequential passes, which slow down real-time performance, but RAS dynamically adjusts sampling ratios based on the model's focus on different image regions. By only updating areas of interest and reusing noise from previous steps, RAS significantly accelerates the sampling process while maintaining high quality. The results show that RAS can achieve speedups of over 2x with minimal loss in generation quality, making it a promising advancement for real-time applications in generative modeling."}, 'zh': {'title': 'ÊèêÂçáÊâ©Êï£Ê®°ÂûãÁöÑÂÆûÊó∂ÊÄßËÉΩ', 'desc': 'Êâ©Êï£Ê®°ÂûãÔºàDMsÔºâÂú®ÁîüÊàê‰ªªÂä°‰∏≠Â∑≤Êàê‰∏∫È¶ñÈÄâÔºå‰ΩÜÂÖ∂‰æùËµñÂ§ö‰∏™È°∫Â∫èÂâçÂêë‰º†ÈÄíÈôêÂà∂‰∫ÜÂÆûÊó∂ÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó†ËÆ≠ÁªÉÈááÊ†∑Á≠ñÁï•RASÔºåÂà©Áî®Êâ©Êï£ÂèòÊç¢Âô®ÔºàDiTsÔºâÁöÑÁÅµÊ¥ªÊÄßÔºåÊ†πÊçÆÊ®°ÂûãÁöÑÂÖ≥Ê≥®ÁÇπÂä®ÊÄÅÂàÜÈÖçÂõæÂÉèÂå∫ÂüüÁöÑÈááÊ†∑ÊØî‰æã„ÄÇRASÂè™Êõ¥Êñ∞ÂΩìÂâçÂÖ≥Ê≥®ÁöÑÂå∫ÂüüÔºåËÄåÂÖ∂‰ªñÂå∫ÂüüÂàô‰ΩøÁî®‰∏ä‰∏ÄÊ≠•ÁöÑÁºìÂ≠òÂô™Â£∞Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊïàÁéá„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåRASÂú®ÁîüÊàêË¥®ÈáèÂá†‰πé‰∏ç‰∏ãÈôçÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂÆûÁé∞ÊòæËëóÁöÑÂä†ÈÄü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09992', 'title': 'Large Language Diffusion Models', 'url': 'https://huggingface.co/papers/2502.09992', 'abstract': 'Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.', 'score': 35, 'issue_id': 2242, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '5117e8f17ba51f92', 'authors': ['Shen Nie', 'Fengqi Zhu', 'Zebin You', 'Xiaolu Zhang', 'Jingyang Ou', 'Jun Hu', 'Jun Zhou', 'Yankai Lin', 'Ji-Rong Wen', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.09992.jpg', 'data': {'categories': ['#architecture', '#optimization', '#benchmark', '#diffusion', '#training'], 'emoji': 'üåä', 'ru': {'title': '–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±—Ä–æ—Å–∞—é—Ç –≤—ã–∑–æ–≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —è–∑—ã–∫–∞', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LLaDA - –Ω–æ–≤—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º. LLaDA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è –≥—Ä–∞–Ω–∏—Ü—É –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö. LLaDA –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –∞ —Ç–∞–∫–∂–µ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É '–ø—Ä–æ–∫–ª—è—Ç–∏—è –æ–±—Ä–∞—â–µ–Ω–∏—è'."}, 'en': {'title': 'LLaDA: A New Era for Language Models Beyond Autoregression', 'desc': 'This paper introduces LLaDA, a novel diffusion model that challenges the dominance of autoregressive models (ARMs) in large language models (LLMs). LLaDA employs a forward data masking process and a reverse process, utilizing a Transformer architecture to predict masked tokens effectively. By optimizing a likelihood bound, it offers a robust generative framework for probabilistic inference. The results show that LLaDA not only scales well but also competes with leading LLMs in tasks like in-context learning and instruction-following, suggesting that diffusion models can serve as a strong alternative to traditional ARMs.'}, 'zh': {'title': 'Êâ©Êï£Ê®°ÂûãÔºöËá™ÂõûÂΩíÊ®°ÂûãÁöÑÊñ∞ÊåëÊàò', 'desc': 'Ëá™ÂõûÂΩíÊ®°ÂûãÔºàARMsÔºâË¢´ÂπøÊ≥õËÆ§‰∏∫ÊòØÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂü∫Áü≥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜLLaDAÔºåËøôÊòØ‰∏ÄÁßç‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÁöÑÊâ©Êï£Ê®°ÂûãÔºåÈááÁî®È¢ÑËÆ≠ÁªÉÂíåÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÁöÑÊñπÊ≥ï„ÄÇLLaDAÈÄöËøáÂâçÂêëÊï∞ÊçÆÊé©ËîΩËøáÁ®ãÂíåÂèçÂêëËøáÁ®ãÂª∫Ê®°ÂàÜÂ∏ÉÔºåÂπ∂‰ΩøÁî®ÊôÆÈÄöTransformerÈ¢ÑÊµãË¢´Êé©ËîΩÁöÑÊ†áËÆ∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåLLaDAÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÂèØÊâ©Â±ïÊÄßÔºåË∂ÖË∂ä‰∫ÜËá™ÊûÑÂª∫ÁöÑARMsÂü∫Á∫øÔºåËØÅÊòé‰∫ÜÊâ©Êï£Ê®°Âûã‰Ωú‰∏∫ARMsÁöÑÂèØË°åÊõø‰ª£ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10248', 'title': 'Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model', 'url': 'https://huggingface.co/papers/2502.10248', 'abstract': "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.", 'score': 33, 'issue_id': 2241, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '356bc046cc5f59e5', 'authors': ['Guoqing Ma', 'Haoyang Huang', 'Kun Yan', 'Liangyu Chen', 'Nan Duan', 'Shengming Yin', 'Changyi Wan', 'Ranchen Ming', 'Xiaoniu Song', 'Xing Chen', 'Yu Zhou', 'Deshan Sun', 'Deyu Zhou', 'Jian Zhou', 'Kaijun Tan', 'Kang An', 'Mei Chen', 'Wei Ji', 'Qiling Wu', 'Wen Sun', 'Xin Han', 'Yanan Wei', 'Zheng Ge', 'Aojie Li', 'Bin Wang', 'Bizhu Huang', 'Bo Wang', 'Brian Li', 'Changxing Miao', 'Chen Xu', 'Chenfei Wu', 'Chenguang Yu', 'Dapeng Shi', 'Dingyuan Hu', 'Enle Liu', 'Gang Yu', 'Ge Yang', 'Guanzhe Huang', 'Gulin Yan', 'Haiyang Feng', 'Hao Nie', 'Haonan Jia', 'Hanpeng Hu', 'Hanqi Chen', 'Haolong Yan', 'Heng Wang', 'Hongcheng Guo', 'Huilin Xiong', 'Huixin Xiong', 'Jiahao Gong', 'Jianchang Wu', 'Jiaoren Wu', 'Jie Wu', 'Jie Yang', 'Jiashuai Liu', 'Jiashuo Li', 'Jingyang Zhang', 'Junjing Guo', 'Junzhe Lin', 'Kaixiang Li', 'Lei Liu', 'Lei Xia', 'Liang Zhao', 'Liguo Tan', 'Liwen Huang', 'Liying Shi', 'Ming Li', 'Mingliang Li', 'Muhua Cheng', 'Na Wang', 'Qiaohui Chen', 'Qinglin He', 'Qiuyan Liang', 'Quan Sun', 'Ran Sun', 'Rui Wang', 'Shaoliang Pang', 'Shiliang Yang', 'Sitong Liu', 'Siqi Liu', 'Shuli Gao', 'Tiancheng Cao', 'Tianyu Wang', 'Weipeng Ming', 'Wenqing He', 'Xu Zhao', 'Xuelin Zhang', 'Xianfang Zeng', 'Xiaojia Liu', 'Xuan Yang', 'Yaqi Dai', 'Yanbo Yu', 'Yang Li', 'Yineng Deng', 'Yingming Wang', 'Yilei Wang', 'Yuanwei Lu', 'Yu Chen', 'Yu Luo', 'Yuchu Luo', 'Yuhe Yin', 'Yuheng Feng', 'Yuxiang Yang', 'Zecheng Tang', 'Zekai Zhang', 'Zidong Yang', 'Binxing Jiao', 'Jiansheng Chen', 'Jing Li', 'Shuchang Zhou', 'Xiangyu Zhang', 'Xinhao Zhang', 'Yibo Zhu', 'Heung-Yeung Shum', 'Daxin Jiang'], 'affiliations': ['StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2502.10248.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#training', '#open_source', '#diffusion', '#video', '#architecture'], 'emoji': 'üé¨', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —Ä–æ–ª–∏–∫–∞–º', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Step-Video-T2V - –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é —Å 30 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª—É–±–æ–∫–∏–π –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä Video-VAE –¥–ª—è —Å–∂–∞—Ç–∏—è –≤–∏–¥–µ–æ –∏ –¥–≤–∞ –¥–≤—É—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ Video-DPO –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏.'}, 'en': {'title': 'Revolutionizing Video Generation with Step-Video-T2V', 'desc': 'Step-Video-T2V is a cutting-edge text-to-video model that utilizes 30 billion parameters to generate videos with a maximum length of 204 frames. It employs a deep compression Variational Autoencoder, achieving significant spatial and temporal compression while preserving high video quality. The model incorporates bilingual text encoders for processing prompts in both English and Chinese, and utilizes a DiT with 3D full attention for effective denoising of latent frames. Evaluated on a new benchmark, Step-Video-T2V demonstrates superior performance in video generation, addressing current limitations in diffusion-based models and paving the way for future advancements in video foundation models.'}, 'zh': {'title': 'ÂàõÊñ∞ËßÜÈ¢ëÁîüÊàêÔºåËµãËÉΩÂÜÖÂÆπÂàõ‰ΩúËÄÖ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Step-Video-T2VÁöÑÂÖàËøõÊñáÊú¨Âà∞ËßÜÈ¢ëÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÂÖ∑Êúâ300‰∫øÂèÇÊï∞ÔºåËÉΩÂ§üÁîüÊàêÊúÄÈïø204Â∏ßÁöÑËßÜÈ¢ë„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊ∑±Â∫¶ÂéãÁº©ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVideo-VAEÔºâÔºåÂú®ËßÜÈ¢ëÁîüÊàê‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫Ü16x16ÁöÑÁ©∫Èó¥ÂéãÁº©Âíå8xÁöÑÊó∂Èó¥ÂéãÁº©ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂçìË∂äÁöÑËßÜÈ¢ëÈáçÂª∫Ë¥®Èáè„ÄÇÁî®Êà∑ÊèêÁ§∫ÈÄöËøáÂèåËØ≠ÊñáÊú¨ÁºñÁ†ÅÂô®ËøõË°åÁºñÁ†ÅÔºå‰ª•Â§ÑÁêÜËã±ËØ≠Âíå‰∏≠Êñá„ÄÇÊàë‰ª¨ËøòËÆ®ËÆ∫‰∫ÜÂΩìÂâçÊâ©Êï£Ê®°ÂûãËåÉÂºèÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂Ê¶ÇËø∞‰∫ÜËßÜÈ¢ëÂü∫Á°ÄÊ®°ÂûãÁöÑÊú™Êù•ÊñπÂêë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09696', 'title': 'ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models', 'url': 'https://huggingface.co/papers/2502.09696', 'abstract': 'Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.', 'score': 23, 'issue_id': 2241, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '00f4f8e85ea27717', 'authors': ['Jonathan Roberts', 'Mohammad Reza Taesiri', 'Ansh Sharma', 'Akash Gupta', 'Samuel Roberts', 'Ioana Croitoru', 'Simion-Vlad Bogolin', 'Jialu Tang', 'Florian Langer', 'Vyas Raina', 'Vatsal Raina', 'Hanyi Xiong', 'Vishaal Udandarao', 'Jingyi Lu', 'Shiyang Chen', 'Sam Purkis', 'Tianshuo Yan', 'Wenye Lin', 'Gyungin Shin', 'Qiaochu Yang', 'Anh Totti Nguyen', 'Kai Han', 'Samuel Albanie'], 'affiliations': ['Auburn University', 'Independent Researcher', 'The University of Hong Kong', 'University of Alberta', 'University of Cambridge', 'University of Oxford', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2502.09696.jpg', 'data': {'categories': ['#cv', '#benchmark', '#interpretability', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'ZeroBench: –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ZeroBench, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM). –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 100 –≤—Ä—É—á–Ω—É—é –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ 334 –º–µ–Ω–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –ø–æ–¥–≤–æ–ø—Ä–æ—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–ª–∏ 20 LMM –Ω–∞ ZeroBench, –∏ –≤—Å–µ –æ–Ω–∏ –ø–æ–∫–∞–∑–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç 0%. –¶–µ–ª—å—é –±–µ–Ω—á–º–∞—Ä–∫–∞ —è–≤–ª—è–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–∂–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –¥–æ–ª—å—à–µ, —á–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏.'}, 'en': {'title': 'ZeroBench: Raising the Bar for Visual Reasoning in LMMs', 'desc': 'This paper discusses the limitations of Large Multimodal Models (LMMs) in interpreting images, highlighting that they perform worse in spatial reasoning compared to small children and animals. Despite achieving high scores on existing visual benchmarks, these models struggle with more complex visual reasoning tasks. To tackle this issue, the authors introduce ZeroBench, a new benchmark designed to be extremely challenging for current LMMs, consisting of 100 curated questions and 334 easier subquestions. The evaluation of 20 LMMs on ZeroBench resulted in a score of 0.0%, demonstrating the need for more difficult benchmarks to drive advancements in visual understanding.'}, 'zh': {'title': 'ZeroBenchÔºöÊé®Âä®ËßÜËßâÁêÜËß£ÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®ÂõæÂÉèÁêÜËß£ÊñπÈù¢Â≠òÂú®ÊòæËëó‰∏çË∂≥ÔºåÁîöËá≥Âú®Êüê‰∫õÊñπÈù¢ÁöÑÁ©∫Èó¥ËÆ§Áü•ËÉΩÂäõ‰∏çÂ¶ÇÂ∞èÂ≠©ÊàñÂä®Áâ©„ÄÇÂ∞ΩÁÆ°Â¶ÇÊ≠§ÔºåÂÆÉ‰ª¨Âú®ËÆ∏Â§öÊµÅË°åÁöÑËßÜËßâÂü∫ÂáÜÊµãËØï‰∏≠ÂæóÂàÜÂæàÈ´òÔºå‰ΩÜÈöèÁùÄÊ®°ÂûãËøõÊ≠•ÁöÑÂä†ÈÄüÔºåËøô‰∫õÂü∫ÂáÜÁöÑÊåëÊàòÊÄßËøÖÈÄüÈôç‰Ωé„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜZeroBenchÔºåËøôÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑËßÜËßâÊé®ÁêÜÂü∫ÂáÜÔºåÂΩìÂâçÁöÑÂâçÊ≤øLMMsÂÆåÂÖ®Êó†Ê≥ïËß£ÂÜ≥„ÄÇZeroBenchÂåÖÂê´100‰∏™ÊâãÂä®Á≠ñÂàíÁöÑÈóÆÈ¢òÂíå334‰∏™ËæÉÁÆÄÂçïÁöÑÂ≠êÈóÆÈ¢òÔºåÊàë‰ª¨ÂØπ20‰∏™LMMsËøõË°å‰∫ÜËØÑ‰º∞ÔºåÁªìÊûúÂùá‰∏∫0.0%ÔºåÂπ∂ÂØπÈîôËØØËøõË°å‰∫Ü‰∏•Ê†ºÂàÜÊûê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10391', 'title': 'MM-RLHF: The Next Step Forward in Multimodal LLM Alignment', 'url': 'https://huggingface.co/papers/2502.10391', 'abstract': 'Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across 10 distinct dimensions and 27 benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a 19.5% increase in conversational abilities and a 60% improvement in safety.   We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.', 'score': 20, 'issue_id': 2241, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': 'c47a89fda79a1a4b', 'authors': ['Yi-Fan Zhang', 'Tao Yu', 'Haochen Tian', 'Chaoyou Fu', 'Peiyan Li', 'Jianshu Zeng', 'Wulin Xie', 'Yang Shi', 'Huanyu Zhang', 'Junkang Wu', 'Xue Wang', 'Yibo Hu', 'Bin Wen', 'Fan Yang', 'Zhang Zhang', 'Tingting Gao', 'Di Zhang', 'Liang Wang', 'Rong Jin', 'Tieniu Tan'], 'affiliations': ['Alibaba', 'CASIA', 'KuaiShou', 'Meta AI', 'NJU', 'PKU', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2502.10391.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#training', '#interpretability', '#open_source', '#rlhf', '#dataset'], 'emoji': 'ü§ñ', 'ru': {'title': '–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MM-RLHF - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 120 —Ç—ã—Å—è—á —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø–∞—Ä —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ù–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ –æ–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≤–∫–ª—é—á–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä–∏—Ç–∏–∫–∏ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ LLaVA-ov-7B –≤ –¥–∏–∞–ª–æ–≥–∞—Ö –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ—Ç–∫—Ä—ã–ª–∏ –¥–æ—Å—Ç—É–ø –∫ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö, –º–æ–¥–µ–ª—è–º –∏ –∫–æ–¥—É –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.'}, 'en': {'title': 'Enhancing MLLM Alignment with Human Preferences', 'desc': 'This paper addresses the need for better alignment of Multimodal Large Language Models (MLLMs) with human preferences. It introduces MM-RLHF, a new dataset with 120,000 human-annotated preference comparisons, which enhances the quality and diversity of training data for alignment tasks. The authors propose innovative techniques like a Critique-Based Reward Model that provides detailed feedback on model outputs and Dynamic Reward Scaling to optimize the training process. Their methods show significant improvements in model performance, including a 19.5% boost in conversational skills and a 60% increase in safety metrics.'}, 'zh': {'title': 'ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩê', 'desc': 'Â∞ΩÁÆ°Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂ§ßÂ§öÊï∞ÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂ∞öÊú™‰∏é‰∫∫Á±ªÂÅèÂ•ΩËøõË°åÂÖÖÂàÜÂØπÈΩê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜMM-RLHFÊï∞ÊçÆÈõÜÔºåÂåÖÂê´12‰∏á‰∏™ÁªÜÁ≤íÂ∫¶ÁöÑ‰∫∫Á±ªÊ†áÊ≥®ÂÅèÂ•ΩÊØîËæÉÂØπÔºåÊòæËëóÊèêÂçá‰∫ÜÁé∞ÊúâËµÑÊ∫êÁöÑËßÑÊ®°ÂíåË¥®Èáè„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÂü∫‰∫éÊâπËØÑÁöÑÂ•ñÂä±Ê®°ÂûãÔºåËÉΩÂ§üÂú®ËØÑÂàÜÂâçÁîüÊàêÊ®°ÂûãËæìÂá∫ÁöÑÊâπËØÑÔºå‰ªéËÄåÊèê‰æõÊõ¥ÂÖ∑ÂèØËß£ÈáäÊÄßÂíå‰ø°ÊÅØÈáèÁöÑÂèçÈ¶à„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜÂä®ÊÄÅÂ•ñÂä±Áº©ÊîæÊñπÊ≥ïÔºåÊ†πÊçÆÂ•ñÂä±‰ø°Âè∑Ë∞ÉÊï¥ÊØè‰∏™Ê†∑Êú¨ÁöÑÊçüÂ§±ÊùÉÈáçÔºå‰ª•‰ºòÂåñÈ´òË¥®ÈáèÊØîËæÉÂØπÁöÑ‰ΩøÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09935', 'title': 'Precise Parameter Localization for Textual Generation in Diffusion Models', 'url': 'https://huggingface.co/papers/2502.09935', 'abstract': "Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at https://t2i-text-loc.github.io/.", 'score': 9, 'issue_id': 2245, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '1c3ce78b0c6424d2', 'authors': ['≈Åukasz Staniszewski', 'Bartosz Cywi≈Ñski', 'Franziska Boenisch', 'Kamil Deja', 'Adam Dziedzic'], 'affiliations': ['CISPA Helmholtz Center for Information Security', 'IDEAS NCBR', 'Warsaw University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.09935.jpg', 'data': {'categories': ['#cv', '#diffusion', '#training', '#synthetic', '#architecture'], 'emoji': 'üîç', 'ru': {'title': '–õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –º–µ–Ω–µ–µ 1% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã—Ö –≤ —Å–ª–æ—è—Ö –≤–Ω–∏–º–∞–Ω–∏—è, –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –æ–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥—ã –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—è —Ç–æ—á–µ—á–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–µ–≤ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ü–æ–¥—Ö–æ–¥ –ø—Ä–∏–º–µ–Ω–∏–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —ç–Ω–∫–æ–¥–µ—Ä–∞–º.'}, 'en': {'title': 'Targeting Attention for Enhanced Text Generation in Diffusion Models', 'desc': "This paper explores how diffusion models can create realistic images that include high-quality text. It reveals that less than 1% of the model's parameters, specifically in the attention layers, are crucial for generating text within these images. By focusing on these specific layers, the authors enhance the efficiency and performance of text generation in diffusion models. They also present applications such as improving text generation, editing text in images, and preventing toxic text generation, demonstrating the broad applicability of their approach across different model architectures."}, 'zh': {'title': 'Â±ÄÈÉ®ÂåñÊ≥®ÊÑèÂäõÂ±ÇÊèêÂçáÊñáÊú¨ÁîüÊàêËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊâ©Êï£Ê®°ÂûãÔºåËÉΩÂ§üÂêàÊàêÈ´òË¥®ÈáèÁöÑÁÖßÁâáÁ∫ßÂõæÂÉèÔºåÂπ∂ÈõÜÊàêÊñáÊú¨ÁîüÊàê„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊâ©Êï£Ê®°Âûã‰∏≠Âè™Êúâ‰∏çÂà∞1%ÁöÑÂèÇÊï∞Ôºå‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ê≥®ÊÑèÂäõÂ±ÇÔºåÂΩ±ÂìçÂõæÂÉè‰∏≠ÁöÑÊñáÊú¨ÂÜÖÂÆπÁîüÊàê„ÄÇÂü∫‰∫éËøô‰∏ÄËßÇÂØüÔºå‰ΩúËÄÖÈÄöËøáÈíàÂØπ‰∫§ÂèâÂíåËÅîÂêàÊ≥®ÊÑèÂäõÂ±ÇÔºåÊèêÂçá‰∫ÜÊñáÊú¨ÁîüÊàêÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇËÆ∫ÊñáËøòÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Ëøô‰∫õÂ±ÄÈÉ®ÂåñÁöÑÂ±ÇÊù•ÁºñËæëÁîüÊàêÂõæÂÉè‰∏≠ÁöÑÊñáÊú¨ÂÜÖÂÆπÔºåÂπ∂Èò≤Ê≠¢ÁîüÊàêÊúâÂÆ≥ÊñáÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09955', 'title': 'Diverse Inference and Verification for Advanced Reasoning', 'url': 'https://huggingface.co/papers/2502.09955', 'abstract': "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.", 'score': 9, 'issue_id': 2242, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '10eaccfc7377f600', 'authors': ['Iddo Drori', 'Gaston Longhitano', 'Mao Mao', 'Seunghwan Hyun', 'Yuke Zhang', 'Sungjun Park', 'Zachary Meeks', 'Xin-Yu Zhang', 'Ben Segev', 'Howard Yong', 'Nakul Verma', 'Avi Shporer', 'Alon Amit', 'Madeleine Udell'], 'affiliations': ['Boston University', 'Columbia University', 'Google', 'Intuit', 'Massachusetts Institute of Technology', 'NotBadMath.AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09955.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#agents', '#rl', '#training', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ LLM –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á: –º–Ω–æ–≥–æ–º–æ–¥–µ–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥', 'desc': "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–æ–¥–æ–≤, –≤–∫–ª—é—á–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Ä–µ—à–µ–Ω–∏–π –∏ –æ—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ü–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ª–∏–º–ø–∏–∞–¥—ã, Humanity's Last Exam –∏ Abstraction and Reasoning Corpus. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–∏–º—É–ª—è—Ü–∏–∏, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–≤ –∏ —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏."}, 'en': {'title': 'Boosting LLMs: From 33% to 77% Accuracy in Complex Problem Solving!', 'desc': "This paper discusses advancements in reasoning large language models (LLMs) like OpenAI's models and DeepSeek in tackling complex mathematical and coding challenges. The authors propose a diverse inference strategy that integrates various models and methods during testing to enhance performance on difficult tasks such as IMO problems and ARC puzzles. They demonstrate that their method significantly boosts accuracy, achieving a 77.8% success rate on IMO combinatorics and solving 80% of previously unsolved ARC puzzles. Additionally, they employ techniques like reinforcement learning and meta-learning to improve the adaptability and generalization of their models, ensuring their approach is both reliable and scalable."}, 'zh': {'title': 'ÊèêÂçáÊé®ÁêÜÊ®°ÂûãÂú®È´òÁ∫ßÊï∞Â≠¶ÈóÆÈ¢ò‰∏äÁöÑÂáÜÁ°ÆÊÄß', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊé®ÁêÜÂûãÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Ëß£ÂÜ≥È´òÁ∫ßÊï∞Â≠¶ÂíåÁºñÁ®ã‰ªªÂä°‰∏≠ÁöÑÊåëÊàòÔºåÁâπÂà´ÊòØÂõΩÈôÖÊï∞Â≠¶Â••ÊûóÂåπÂÖãÔºàIMOÔºâÁªÑÂêàÈóÆÈ¢ò„ÄÅÊäΩË±°‰∏éÊé®ÁêÜËØ≠ÊñôÂ∫ìÔºàARCÔºâÈöæÈ¢òÂíå‰∫∫Á±ªÊúÄÂêéËÄÉËØïÔºàHLEÔºâÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊ®°ÂûãÂíåÂ§öÊñπÊ≥ïÁªìÂêàÁöÑÊé®ÁêÜÊñπÊ≥ïÔºåÂú®ÊµãËØïÊó∂ËøõË°åÂ§öÊ†∑ÂåñÊé®ÁêÜ„ÄÇÈÄöËøáËá™Âä®È™åËØÅIMOÈóÆÈ¢òÂíåARCÈöæÈ¢òÁöÑËß£Á≠îÊ≠£Á°ÆÊÄßÔºåÊàë‰ª¨ÊòæËëóÊèêÈ´ò‰∫ÜËøô‰∫õÈóÆÈ¢òÁöÑËß£Á≠îÂáÜÁ°ÆÁéá„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊñπÊ≥ïÂèØÈù†„ÄÅÁ®≥ÂÅ•‰∏îÂèØÊâ©Â±ïÔºåÊó®Âú®Êé®Âä®ÂèØÈáçÂ§çÁ†îÁ©∂ÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07780', 'title': 'DarwinLM: Evolutionary Structured Pruning of Large Language Models', 'url': 'https://huggingface.co/papers/2502.07780', 'abstract': 'Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \\sysname, a method for training-aware structured pruning. \\sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \\sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training.', 'score': 7, 'issue_id': 2251, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': 'b53421574afe0c8a', 'authors': ['Shengkun Tang', 'Oliver Sieberling', 'Eldar Kurtic', 'Zhiqiang Shen', 'Dan Alistarh'], 'affiliations': ['Department of Machine Learning, MBZUAI, Abu Dhabi, UAE', 'ETH Zurich, Zurich, Switzerland', 'ISTA, Vienna, Austria', 'Red Hat AI, Boston, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.07780.jpg', 'data': {'categories': ['#inference', '#small_models', '#training', '#optimization'], 'emoji': 'üß¨', 'ru': {'title': '–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤—ã—Å–æ–∫–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ –¥–ª—è –∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ \x1710\x187 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —Å–æ–∑–¥–∞–≤–∞—è –∏ –æ—Ç–±–∏—Ä–∞—è –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–¥–º–æ–¥–µ–ª–∏. –í–∞–∂–Ω–æ–π —á–∞—Å—Ç—å—é –º–µ—Ç–æ–¥–∞ —è–≤–ª—è–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—Ä–µ–∑–∫–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ \x1710\x187 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã, —Ç—Ä–µ–±—É—è –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Efficient Pruning for Powerful Language Models', 'desc': 'This paper presents a new method called \textit{sysname} for structured pruning of large language models (LLMs) to improve their efficiency in natural language processing tasks. The method uses an evolutionary search process to create multiple model variations, selecting the best-performing ones while incorporating a lightweight training process to enhance post-compression performance. By focusing on the varying sensitivities of different model components, \textit{sysname} achieves effective model compression without sacrificing accuracy. The results show that \textit{sysname} outperforms existing methods, requiring significantly less training data while maintaining high performance on various LLMs.'}, 'zh': {'title': 'ËÆ≠ÁªÉÊÑüÁü•ÁöÑÁªìÊûÑÂåñÂâ™ÊûùÊñπÊ≥ï', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÂÖ∂Â∑®Â§ßÁöÑËÆ°ÁÆóÊàêÊú¨ÈôêÂà∂‰∫ÜÂÖ∂ÂπøÊ≥õÂ∫îÁî®ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÆûÊó∂Â∫îÁî®‰∏≠„ÄÇÁªìÊûÑÂåñÂâ™ÊûùÊòØ‰∏ÄÁßçÊúâÊïàÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÈÄöËøáÂéãÁº©Ê®°ÂûãÂπ∂Áõ¥Êé•Êèê‰æõÁ´ØÂà∞Á´ØÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇ‰∏çÂêåÊ®°ÂûãÁªÑ‰ª∂ÂØπÂâ™ÊûùÁöÑÊïèÊÑüÊÄß‰∏çÂêåÔºåÂõ†Ê≠§ÈúÄË¶ÅÈùûÂùáÂåÄÁöÑÊ®°ÂûãÂéãÁº©ÊñπÊ≥ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫\textit{sysname}ÁöÑËÆ≠ÁªÉÊÑüÁü•ÁªìÊûÑÂåñÂâ™ÊûùÊñπÊ≥ïÔºåÈÄöËøáËøõÂåñÊêúÁ¥¢ËøáÁ®ãÁîüÊàêÂ§ö‰∏™Âêé‰ª£Ê®°ÂûãÔºåÂπ∂Âú®ÊØè‰∏Ä‰ª£‰∏≠ÈÄâÊã©ÊúÄÈÄÇÂêàÁöÑÊ®°ÂûãËøõË°åÁîüÂ≠ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09411', 'title': 'ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation', 'url': 'https://huggingface.co/papers/2502.09411', 'abstract': 'Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models.   Our project page is available at: https://rotem-shalev.github.io/ImageRAG', 'score': 7, 'issue_id': 2251, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'a3c0f020b9cc5226', 'authors': ['Rotem Shalev-Arkushin', 'Rinon Gal', 'Amit H. Bermano', 'Ohad Fried'], 'affiliations': ['NVIDIA', 'Reichman University', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09411.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#cv', '#rag', '#diffusion'], 'emoji': 'üîç', 'ru': {'title': 'ImageRAG: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ ImageRAG, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (RAG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–¥–∫–∏—Ö –∏–ª–∏ –Ω–µ–≤–∏–¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. ImageRAG –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, ImageRAG –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–¥–∫–∏—Ö –∏ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Enhancing Image Generation with Dynamic Retrieval', 'desc': 'This paper introduces ImageRAG, a novel method that enhances image generation by integrating Retrieval-Augmented Generation (RAG) techniques. Unlike previous methods that required specific training for retrieval-based generation, ImageRAG utilizes existing image conditioning models to dynamically retrieve relevant images based on text prompts. This approach allows for improved synthesis of rare and fine-grained visual concepts by providing contextual guidance during the generation process. The adaptability of ImageRAG across various model types demonstrates its effectiveness in producing high-quality and diverse visual content.'}, 'zh': {'title': 'ImageRAGÔºöÊèêÂçáÁ®ÄÊúâÊ¶ÇÂøµÁîüÊàêÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êâ©Êï£Ê®°ÂûãÂèØ‰ª•ÁîüÊàêÈ´òË¥®ÈáèÂíåÂ§öÊ†∑ÂåñÁöÑËßÜËßâÂÜÖÂÆπÔºå‰ΩÜÂú®ÁîüÊàêÁ®ÄÊúâÊàñÊú™ËßÅËøáÁöÑÊ¶ÇÂøµÊó∂Â≠òÂú®Âõ∞Èöæ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨Êé¢Á¥¢‰∫ÜÁªìÂêàÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâ‰∏éÂõæÂÉèÁîüÊàêÊ®°ÂûãÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜImageRAGÔºåËøôÊòØ‰∏ÄÁßçÊ†πÊçÆÁªôÂÆöÊñáÊú¨ÊèêÁ§∫Âä®ÊÄÅÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÂõæÂÉèÁöÑÊñπÊ≥ïÔºåÂπ∂Â∞ÜËøô‰∫õÂõæÂÉè‰Ωú‰∏∫‰∏ä‰∏ãÊñáÊù•ÊåáÂØºÁîüÊàêËøáÁ®ã„ÄÇ‰∏é‰πãÂâçÈúÄË¶Å‰∏ìÈó®ËÆ≠ÁªÉÁöÑÊ£ÄÁ¥¢ÁîüÊàêÊ®°Âûã‰∏çÂêåÔºåImageRAGÂà©Áî®Áé∞ÊúâÂõæÂÉèÊù°‰ª∂Ê®°ÂûãÁöÑËÉΩÂäõÔºåÊó†ÈúÄÁâπÂÆöÁöÑRAGËÆ≠ÁªÉÔºåÈÄÇÂ∫îÊÄßÂº∫ÔºåËÉΩÂ§üÂú®‰∏çÂêåÊ®°ÂûãÁ±ªÂûã‰∏≠Â∫îÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10235', 'title': 'AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting', 'url': 'https://huggingface.co/papers/2502.10235', 'abstract': 'Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.', 'score': 7, 'issue_id': 2248, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': 'dbd38216ecfcb531', 'authors': ['Abdelhakim Benechehab', 'Vasilii Feofanov', 'Giuseppe Paolo', 'Albert Thomas', 'Maurizio Filippone', 'Bal√°zs K√©gl'], 'affiliations': ['Department of Data Science, EURECOM', 'Huawei Noahs Ark Lab, Paris, France', 'Statistics Program, KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2502.10235.jpg', 'data': {'categories': ['#data', '#synthetic', '#dataset', '#inference', '#optimization', '#training'], 'emoji': 'üìä', 'ru': {'title': 'AdaPTS: –ê–¥–∞–ø—Ç–∞—Ü–∏—è –æ–¥–Ω–æ–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AdaPTS - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π-–æ—Å–Ω–æ–≤ (foundation models) –≤ –∑–∞–¥–∞—á–∞—Ö –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. –ê–¥–∞–ø—Ç–µ—Ä—ã –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –ø–æ–∑–≤–æ–ª—è—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ AdaPTS –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–æ–¥—É–ª—å–Ω–æ–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.'}, 'en': {'title': 'Enhancing Multivariate Time Series Forecasting with Adapters', 'desc': 'This paper introduces a new approach called adapters to improve the use of pre-trained foundation models (FMs) for multivariate time series forecasting. Adapters transform multivariate inputs into a latent space, allowing the FM to process each feature independently while managing complex dependencies. The study also explores various optimization and inference strategies inspired by representation learning and Bayesian neural networks. Experiments show that this method significantly enhances forecasting accuracy and uncertainty quantification, making it a valuable tool for real-world applications.'}, 'zh': {'title': 'ÈÄÇÈÖçÂô®ÔºöÂ§öÂèòÈáèÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑÊñ∞Ëß£ÂÜ≥ÊñπÊ°à', 'desc': 'È¢ÑËÆ≠ÁªÉÂü∫Á°ÄÊ®°ÂûãÂú®ÂçïÂèòÈáèÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Â§ÑÁêÜÁâπÂæÅÈó¥Â§çÊùÇ‰æùËµñÂÖ≥Á≥ªÂíåÈáèÂåñÈ¢ÑÊµã‰∏çÁ°ÆÂÆöÊÄßÊñπÈù¢‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÊú¨Á†îÁ©∂ÈÄöËøáÂºïÂÖ•ÈÄÇÈÖçÂô®Êù•Ëß£ÂÜ≥Ëøô‰∫õÂÖ≥ÈîÆÈôêÂà∂ÔºåÈÄÇÈÖçÂô®ÊòØ‰∏ÄÁßçÁâπÂæÅÁ©∫Èó¥ËΩ¨Êç¢ÔºåËÉΩÂ§üÊúâÊïàÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÂçïÂèòÈáèÊó∂Èó¥Â∫èÂàóÊ®°ÂûãËøõË°åÂ§öÂèòÈáè‰ªªÂä°„ÄÇÈÄÇÈÖçÂô®ÈÄöËøáÂ∞ÜÂ§öÂèòÈáèËæìÂÖ•ÊäïÂΩ±Âà∞ÂêàÈÄÇÁöÑÊΩúÂú®Á©∫Èó¥ÔºåÂπ∂Áã¨Á´ãÂú∞ÂØπÊØè‰∏™Áª¥Â∫¶Â∫îÁî®Âü∫Á°ÄÊ®°ÂûãÔºå‰ªéËÄåÂÆûÁé∞ÂäüËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÄÇÈÖçÂô®Âú®È¢ÑÊµãÂáÜÁ°ÆÊÄßÂíå‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÊñπÈù¢ÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§öÂèòÈáèÊó∂Èó¥Â∫èÂàóÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07586', 'title': "We Can't Understand AI Using our Existing Vocabulary", 'url': 'https://huggingface.co/papers/2502.07586', 'abstract': 'This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they\'re reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a "length neologism" enables controlling LLM response length, while a "diversity neologism" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better.', 'score': 6, 'issue_id': 2247, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '0435a4b5389f3dcb', 'authors': ['John Hewitt', 'Robert Geirhos', 'Been Kim'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.07586.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#interpretability'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–ù–µ–æ–ª–æ–≥–∏–∑–º—ã –∫–∞–∫ –º–æ—Å—Ç –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –ò–ò', 'desc': '–í —Å—Ç–∞—Ç—å–µ —É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è, —á—Ç–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–µ–æ–ª–æ–≥–∏–∑–º—ã - –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∏ –º–∞—à–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–æ–±–ª–µ–º–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É –ª—é–¥—å–º–∏ –∏ –º–∞—à–∏–Ω–∞–º–∏. –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –Ω–µ–æ–ª–æ–≥–∏–∑–º—ã –º–æ–∂–µ—Ç —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –ø–æ–∑–≤–æ–ª—è—è –ª—É—á—à–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∏ –ø–æ–Ω–∏–º–∞—Ç—å –ò–ò.'}, 'en': {'title': 'Creating New Words for Better AI Communication', 'desc': 'This paper discusses the need for new words, or neologisms, to better communicate with artificial intelligence (AI). It argues that humans and machines have different ways of understanding concepts, which makes it hard for us to explain our ideas to machines. By creating a shared language with these new terms, we can improve how we control and interpret machine behavior. The authors provide examples of neologisms that help manage AI responses, showing that a richer vocabulary can enhance our interaction with AI systems.'}, 'zh': {'title': 'ÈÄöËøáÊñ∞ËØçÊ±áÁêÜËß£‰∫∫Â∑•Êô∫ËÉΩ', 'desc': 'ËøôÁØáËÆ∫ÊñáËÆ§‰∏∫ÔºåË¶ÅÁêÜËß£‰∫∫Â∑•Êô∫ËÉΩÔºåÊàë‰ª¨‰∏çËÉΩ‰ªÖ‰æùËµñÁé∞ÊúâÁöÑ‰∫∫Á±ªËØçÊ±á„ÄÇÊàë‰ª¨Â∫îËØ•Âä™ÂäõÂºÄÂèëÊñ∞ËØçÊ±áÔºå‰ª•ÂáÜÁ°ÆË°®ËææÊàë‰ª¨ÊÉ≥ÊïôÁªôÊú∫Âô®ÁöÑ‰∫∫Á±ªÊ¶ÇÂøµÊàñÊàë‰ª¨ÈúÄË¶ÅÂ≠¶‰π†ÁöÑÊú∫Âô®Ê¶ÇÂøµ„ÄÇ‰∫∫Á±ªÂíåÊú∫Âô®ÁöÑÊ¶ÇÂøµ‰∏çÂêåÔºåÂõ†Ê≠§ÂèØËß£ÈáäÊÄßÂèØ‰ª•Áúã‰ΩúÊòØ‰∏Ä‰∏™Ê≤üÈÄöÈóÆÈ¢òÔºö‰∫∫Á±ªÂøÖÈ°ªËÉΩÂ§üÂºïÁî®ÂíåÊéßÂà∂Êú∫Âô®Ê¶ÇÂøµÔºåÂπ∂Â∞Ü‰∫∫Á±ªÊ¶ÇÂøµ‰º†ËææÁªôÊú∫Âô®„ÄÇÈÄöËøáÂºÄÂèëÊñ∞ËØçÊ±áÂàõÂª∫‰∏Ä‰∏™ÂÖ±‰∫´ÁöÑ‰∫∫Êú∫ËØ≠Ë®ÄÔºåÂèØ‰ª•Ëß£ÂÜ≥Ëøô‰∏™Ê≤üÈÄöÈóÆÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09741', 'title': 'FoNE: Precise Single-Token Number Embeddings via Fourier Features', 'url': 'https://huggingface.co/papers/2502.09741', 'abstract': "Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64times less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3times and 6times fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.", 'score': 6, 'issue_id': 2241, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'adb30f7d3d01ce3a', 'authors': ['Tianyi Zhou', 'Deqing Fu', 'Mahdi Soltanolkotabi', 'Robin Jia', 'Vatsal Sharan'], 'affiliations': ['Department of Computer Science University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2502.09741.jpg', 'data': {'categories': ['#architecture', '#training', '#data', '#optimization'], 'emoji': 'üî¢', 'ru': {'title': 'FoNE: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∏—Å–µ–ª –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —á–∏—Å–µ–ª –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π Fourier Number Embedding (FoNE). FoNE –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —á–∏—Å–ª–∞ –≤ –≤–∏–¥–µ –µ–¥–∏–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—É—Ä—å–µ-–ø–æ–¥–æ–±–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –±–µ–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –≠—Ç–æ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π —Å–ø–æ—Å–æ–± –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —É—Å–∫–æ—Ä—è–µ—Ç –∫–∞–∫ –æ–±—É—á–µ–Ω–∏–µ, —Ç–∞–∫ –∏ –≤—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, FoNE –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —Å–ª–æ–∂–µ–Ω–∏–µ, –≤—ã—á–∏—Ç–∞–Ω–∏–µ –∏ —É–º–Ω–æ–∂–µ–Ω–∏–µ, –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö.'}, 'en': {'title': 'Revolutionizing Number Representation in LLMs with FoNE', 'desc': 'This paper introduces Fourier Number Embedding (FoNE), a new approach for representing numbers in Large Language Models (LLMs). FoNE simplifies the representation of numerical values by encoding each number as a single token using Fourier features, which reduces fragmentation and improves efficiency. The method significantly enhances model performance on numerical tasks, achieving higher accuracy with fewer tokens compared to traditional embeddings. Notably, FoNE demonstrates remarkable results, requiring much less data to reach high accuracy levels in arithmetic operations like addition, subtraction, and multiplication.'}, 'zh': {'title': 'ÂÇÖÈáåÂè∂Êï∞Â≠óÂµåÂÖ•ÔºöÈ´òÊïàÁöÑÊï∞Â≠óË°®Á§∫ÊñπÊ≥ï', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈÄöÂ∏∏‰ΩøÁî®Â§ö‰∏™Ê†áËÆ∞Êù•Ë°®Á§∫Êï∞Â≠óÔºåËøôÂØºËá¥Ê®°ÂûãÂú®ÁêÜËß£Êï∞ÂÄºÊó∂ÈúÄË¶ÅËÅöÂêàËøô‰∫õÊ†áËÆ∞ÔºåÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÂíåÊé®ÁêÜÁöÑÊïàÁéá„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁß∞‰∏∫ÂÇÖÈáåÂè∂Êï∞Â≠óÂµåÂÖ•ÔºàFoNEÔºâÔºåÂÆÉÂ∞ÜÊï∞Â≠óÁõ¥Êé•Êò†Â∞ÑÂà∞ÂµåÂÖ•Á©∫Èó¥Ôºå‰ΩøÁî®ÂÇÖÈáåÂè∂ÁâπÂæÅËøõË°åÁºñÁ†Å„ÄÇFoNEÂ∞ÜÊØè‰∏™Êï∞Â≠óÁºñÁ†Å‰∏∫‰∏Ä‰∏™Âçï‰∏ÄÊ†áËÆ∞ÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÂºÄÈîÄÔºåÂπ∂Âú®Âä†Ê≥ï„ÄÅÂáèÊ≥ïÂíå‰πòÊ≥ïÁ≠âÊï∞ÂÄº‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰∏é‰º†ÁªüÁöÑÂ≠êËØçÂíåÊï∞Â≠óÂµåÂÖ•Áõ∏ÊØîÔºåFoNEÂú®6‰ΩçÂçÅËøõÂà∂Âä†Ê≥ï‰∏≠ÊâÄÈúÄÁöÑÊï∞ÊçÆÈáèÂáèÂ∞ë‰∫Ü64ÂÄçÔºåÂêåÊó∂ÊØè‰∏™Êï∞Â≠ó‰ΩøÁî®ÁöÑÊ†áËÆ∞Êï∞Èáè‰πüÂáèÂ∞ë‰∫Ü3ÂÄçÂà∞6ÂÄç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10140', 'title': 'Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages', 'url': 'https://huggingface.co/papers/2502.10140', 'abstract': 'Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.', 'score': 4, 'issue_id': 2251, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '0d0292edb1900dd5', 'authors': ['Daniil Gurgurov', 'Ivan Vykopal', 'Josef van Genabith', 'Simon Ostermann'], 'affiliations': ['Brno University of Technology', 'German Research Center for Artificial Intelligence (DFKI)', 'Kempelen Institute of Intelligent Technologies (KInIT)', 'University of Saarland'], 'pdf_title_img': 'assets/pdf/title_img/2502.10140.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#small_models', '#multilingual', '#low_resource'], 'emoji': 'üåç', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (mLMs) –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ (LRLs) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —Ç—Ä–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: Sequential Bottleneck, Invertible Bottleneck –∏ Low-Rank Adaptation, –æ—Ü–µ–Ω–∏–≤–∞—è –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö (–¥–æ 1 –ì–ë –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ú–ë —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π) —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –¥–ª—è LRLs –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –º–µ–Ω—å—à–∏–µ mLMs, —á–µ–º –∫—Ä—É–ø–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ç–∏–ø–∞ LLaMA-3 –∏–ª–∏ GPT-4.'}, 'en': {'title': 'Empowering Low-Resource Languages with Efficient Multilingual Models', 'desc': 'This paper addresses the challenges of processing low-resource languages (LRLs) in natural language processing (NLP) by exploring the use of smaller multilingual models (mLMs) like mBERT and XLM-R. It investigates parameter-efficient adapter-based methods to adapt these mLMs to LRLs, focusing on three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. The study demonstrates that even small adaptation datasets can significantly enhance performance in both intrinsic and extrinsic NLP tasks. Results indicate that Sequential Bottleneck adapters are particularly effective for language modeling, while Invertible Bottleneck adapters perform better on downstream tasks, highlighting the advantages of adapter-based methods over full fine-tuning.'}, 'zh': {'title': 'Â∞èÂûãÂ§öËØ≠Ë®ÄÊ®°ÂûãÂä©Âäõ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂ§ÑÁêÜ', 'desc': '‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºàLRLsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∏≠Èù¢‰∏¥Êï∞ÊçÆ‰∏çË∂≥ÁöÑÈáçÂ§ßÊåëÊàò„ÄÇÂΩìÂâçÁöÑÂÖàËøõÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜLRLsÊó∂‰ªçÁÑ∂Â≠òÂú®Âõ∞ÈöæÔºåËÄåËæÉÂ∞èÁöÑÂ§öËØ≠Ë®ÄÊ®°ÂûãÔºàmLMsÔºâÂ¶ÇmBERTÂíåXLM-RÁî±‰∫éÂÖ∂ÂÆπÈáèÊõ¥ÈÄÇÂêà‰ΩéËÆ≠ÁªÉÊï∞ÊçÆÈáèÔºåÂ±ïÁé∞Âá∫Êõ¥Â§ßÁöÑÊΩúÂäõ„ÄÇÊú¨ÊñáÁ≥ªÁªüÁ†îÁ©∂‰∫ÜÂü∫‰∫éÂèÇÊï∞È´òÊïàÈÄÇÈÖçÂô®ÁöÑÊñπÊ≥ïÔºåËØÑ‰º∞‰∫Ü‰∏âÁßçÊû∂ÊûÑÔºöÈ°∫Â∫èÁì∂È¢à„ÄÅÂèØÈÄÜÁì∂È¢àÂíå‰ΩéÁß©ÈÄÇÈÖç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®Â∞èÂûãÈÄÇÈÖçÊï∞ÊçÆÈõÜÂèØ‰ª•Âú®ËØ≠Ë®ÄÂª∫Ê®°Âíå‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÂèñÂæóÊòæËëóÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÈ°∫Â∫èÁì∂È¢àÈÄÇÈÖçÂô®Âú®ËØ≠Ë®ÄÂª∫Ê®°ÊñπÈù¢Ë°®Áé∞‰ºòÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08130', 'title': 'Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models', 'url': 'https://huggingface.co/papers/2502.08130', 'abstract': "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization. S3FT leverages the existence of multiple valid responses to a query. By utilizing the model's correct responses, S3FT reduces model specialization during the fine-tuning stage. S3FT first identifies the correct model responses from the training set by deploying an appropriate judge. Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples. The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks. The results show that standard SFT can lead to an average performance drop of up to 4.4 on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, S3FT reduces this drop by half, i.e. 2.5, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks.", 'score': 3, 'issue_id': 2255, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '2270a63ed413aee6', 'authors': ['Sonam Gupta', 'Yatin Nandwani', 'Asaf Yehudai', 'Dinesh Khandelwal', 'Dinesh Raghu', 'Sachindra Joshi'], 'affiliations': ['IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.08130.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#benchmark', '#plp', '#optimization'], 'emoji': 'üß†', 'ru': {'title': 'S3FT: –£–ª—É—á—à–µ–Ω–∏–µ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Selective Self-to-Supervised Fine-Tuning (S3FT). S3FT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–ø—Ä–æ—Å, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (SFT). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ S3FT —Å–Ω–∏–∂–∞–µ—Ç –ø–∞–¥–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –≤–¥–≤–æ–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å SFT. –ú–µ—Ç–æ–¥ S3FT –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ª—É—á—à—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Enhancing Generalization in Fine-Tuning with S3FT', 'desc': "This paper presents Selective Self-to-Supervised Fine-Tuning (S3FT), a novel approach to fine-tuning Large Language Models (LLMs) that enhances their performance while mitigating overfitting. S3FT improves generalization by utilizing multiple valid responses to a query, allowing the model to learn from its correct outputs rather than just the training data. By identifying and incorporating these correct responses during the fine-tuning process, S3FT reduces the model's tendency to specialize too narrowly on specific tasks. Experimental results demonstrate that S3FT outperforms standard supervised fine-tuning (SFT) and significantly improves generalization across various benchmarks, including mathematical reasoning and programming tasks."}, 'zh': {'title': 'ÈÄâÊã©ÊÄßËá™ÁõëÁù£ÂæÆË∞ÉÔºöÊèêÂçáÊ®°ÂûãÊ≥õÂåñËÉΩÂäõÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂæÆË∞ÉÊñπÊ≥ïÔºåÁß∞‰∏∫ÈÄâÊã©ÊÄßËá™ÁõëÁù£ÂæÆË∞ÉÔºàS3FTÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁâπÂÆö‰ªªÂä°‰∏äÁöÑË°®Áé∞ÔºåÂêåÊó∂ÊîπÂñÑÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂ∏∏Â∏∏ÂØºËá¥Ê®°ÂûãËøáÊãüÂêàÔºåÂç≥Ê®°ÂûãÂØπËÆ≠ÁªÉÊï∞ÊçÆÊàñ‰ªªÂä°Ëøá‰∫é‰∏ìÊ≥®ÔºåÂ§±ÂéªÂØπÊñ∞Êï∞ÊçÆÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇS3FTÈÄöËøáÂà©Áî®Â§ö‰∏™ÊúâÊïàÂìçÂ∫îÊù•ÂáèÂ∞ëÊ®°ÂûãÁöÑ‰∏ìÈó®ÂåñÔºåÈ¶ñÂÖàËØÜÂà´ËÆ≠ÁªÉÈõÜ‰∏≠Ê≠£Á°ÆÁöÑÊ®°ÂûãÂìçÂ∫îÔºåÁÑ∂ÂêéÁªìÂêàËøô‰∫õÂìçÂ∫îÂíåÁúüÂÆûÁ≠îÊ°àËøõË°åÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåS3FTÂú®Êï∞Â≠¶Êé®ÁêÜ„ÄÅPythonÁºñÁ®ãÂíåÈòÖËØªÁêÜËß£Á≠â‰ªªÂä°‰∏äË°®Áé∞‰ºò‰∫éÊ†áÂáÜÁöÑSFTÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10392', 'title': 'Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding', 'url': 'https://huggingface.co/papers/2502.10392', 'abstract': 'In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with +1.13 lead of Acc@0.5 on ScanRefer, and +2.6 and +3.2 leads on NR3D and SR3D respectively. The code is available at https://github.com/GWxuan/TSP3D{https://github.com/GWxuan/TSP3D}.', 'score': 3, 'issue_id': 2252, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '364eaebf9db5bd4a', 'authors': ['Wenxuan Guo', 'Xiuwei Xu', 'Ziwei Wang', 'Jianjiang Feng', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.10392.jpg', 'data': {'categories': ['#3d', '#architecture', '#training', '#cv'], 'emoji': 'üèÜ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è 3D –≤–∏–∑—É–∞–ª—å–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è 3D –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ-—É–ø—Ä–∞–≤–ª—è–µ–º–æ–µ –ø—Ä–æ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ (TGP) –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (CBA) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω—ã –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ –Ω–∞ 100% –ø–æ FPS. –¢–∞–∫–∂–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –Ω–∞–∏–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö ScanRefer, NR3D –∏ SR3D.'}, 'en': {'title': 'Efficient 3D Visual Grounding with Multi-Level Convolution', 'desc': 'This paper introduces a new multi-level convolution architecture designed for 3D visual grounding, which is the task of linking 3D scenes with textual descriptions. Traditional methods struggle with real-time performance due to their complex two-stage or point-based designs. The authors leverage a multi-level fully sparse convolution approach, enhancing the interaction between 3D scene representations and text features through innovative techniques like text-guided pruning (TGP) and completion-based addition (CBA). Their method not only improves inference speed, achieving a 100% increase in frames per second (FPS) compared to the fastest existing methods, but also enhances accuracy on benchmark datasets, outperforming previous models.'}, 'zh': {'title': 'È´òÊïàËûçÂêà3DÂú∫ÊôØ‰∏éÊñáÊú¨ÁâπÂæÅÁöÑËßÜËßâÂÆö‰ΩçÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÂ§öÂ±ÇÂç∑ÁßØÊû∂ÊûÑÔºåÁî®‰∫é3DËßÜËßâÂÆö‰Ωç„ÄÇ‰º†ÁªüÊñπÊ≥ïÁî±‰∫éÈááÁî®‰∏§Èò∂ÊÆµÊàñÂü∫‰∫éÁÇπÁöÑÊû∂ÊûÑÔºåÈöæ‰ª•Êª°Ë∂≥ÂÆûÊó∂Êé®ÁêÜÁöÑË¶ÅÊ±Ç„ÄÇÊàë‰ª¨ÂÄüÈâ¥‰∫ÜÂ§öÂ±ÇÁ®ÄÁñèÂç∑ÁßØÊû∂ÊûÑÂú®3DÁâ©‰ΩìÊ£ÄÊµã‰∏≠ÁöÑÊàêÂäüÔºåÊûÑÂª∫‰∫ÜÊñ∞ÁöÑ3DËßÜËßâÂÆö‰ΩçÊ°ÜÊû∂„ÄÇÈÄöËøáÊñáÊú¨ÂºïÂØº‰øÆÂâ™ÂíåÂü∫‰∫éË°•ÂÖ®ÁöÑÊ∑ªÂä†ÔºåÊàë‰ª¨ÊúâÊïàÂú∞ËûçÂêà‰∫Ü3DÂú∫ÊôØË°®Á§∫ÂíåÊñáÊú¨ÁâπÂæÅÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÂíåÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09638', 'title': 'Jailbreaking to Jailbreak', 'url': 'https://huggingface.co/papers/2502.09638', 'abstract': 'Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as J_2 attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as J_2, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with J_2, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.', 'score': 3, 'issue_id': 2242, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': '3c2ed560e12b971a', 'authors': ['Jeremy Kritz', 'Vaughn Robinson', 'Robert Vacareanu', 'Bijan Varjavand', 'Michael Choi', 'Bobby Gogov', 'Scale Red Team', 'Summer Yue', 'Willow E. Primack', 'Zifan Wang'], 'affiliations': ['Scale'], 'pdf_title_img': 'assets/pdf/title_img/2502.09638.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#training', '#security', '#rlhf'], 'emoji': 'üïµÔ∏è', 'ru': {'title': 'LLM –ø—Ä–æ—Ç–∏–≤ LLM: –Ω–æ–≤—ã–π —Ñ—Ä–æ–Ω—Ç –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å–∞–º–∏ LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ '–∫—Ä–∞—Å–Ω–æ–π –∫–æ–º–∞–Ω–¥—ã'. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, –∫–∞–∫ –≤–∑–ª–æ–º–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (J_2) –º–æ–∂–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∏ –∞—Ç–∞–∫–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏, –¥–æ—Å—Ç–∏–≥–∞—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è —É—Å–ø–µ—Ö–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Sonnet 3.5 –∏ Gemini 1.5 pro –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –¥—Ä—É–≥–∏–µ LLM –≤ —Ä–æ–ª–∏ J_2, –¥–æ—Å—Ç–∏–≥–∞—è 93.0% –∏ 91.0% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∞—Ç–∞–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∑–∞—â–∏—Ç—ã –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞."}, 'en': {'title': 'Jailbreaking the Safeguards: A New Approach to LLM Red Teaming', 'desc': 'This paper discusses a new method for testing the safety of Large Language Models (LLMs) by using a jailbroken version of the model itself, referred to as J_2 attackers. These J_2 attackers can evaluate and exploit vulnerabilities in other LLMs, achieving high success rates in bypassing safeguards. The approach allows the jailbroken LLM to learn from its previous attempts, improving its effectiveness in red teaming strategies. The authors emphasize the importance of understanding this jailbreaking-to-jailbreak phenomenon as a potential risk in AI safety.'}, 'zh': {'title': 'Á†¥Ëß£Ëá™Êàë‰øùÊä§ÁöÑÁ∫¢ÈòüÁ≠ñÁï•', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰Ωú‰∏∫Á∫¢ÈòüÊàêÂëòÔºåÊù•ËØÑ‰º∞ÂíåÊîπËøõÊãíÁªùËÆ≠ÁªÉÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇÊàë‰ª¨Áß∞Ëøô‰∫õË¢´Á†¥Ëß£ÁöÑLLM‰∏∫J_2ÊîªÂáªËÄÖÔºåÂÆÉ‰ª¨ËÉΩÂ§üÈÄöËøá‰∏ä‰∏ãÊñáÂ≠¶‰π†‰ªé‰πãÂâçÁöÑÂ§±Ë¥•‰∏≠ÊèêÈ´òÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSonnet 3.5ÂíåGemini 1.5Âú®ÊîªÂáªÊàêÂäüÁéá‰∏ä‰ºò‰∫éÂÖ∂‰ªñLLMÔºåÂàÜÂà´ËææÂà∞‰∫Ü93.0%Âíå91.0%„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏ç‰ªÖÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÁ∫¢ÈòüÁ≠ñÁï•ÔºåËøòÊè≠Á§∫‰∫ÜÁ†¥Ëß£Ëá™Ë∫´‰øùÊä§Êú∫Âà∂ÁöÑÊΩúÂú®È£éÈô©„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10177', 'title': 'STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning', 'url': 'https://huggingface.co/papers/2502.10177', 'abstract': 'A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.', 'score': 3, 'issue_id': 2240, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '1b17b668b26c2264', 'authors': ['Mingcong Lei', 'Yiming Zhao', 'Ge Wang', 'Zhixin Mai', 'Shuguang Cui', 'Yatong Han', 'Jinke Ren'], 'affiliations': ['FNii-Shenzhen, The Chinese University of Hong Kong, Shenzhen, China', 'Harbin Engineering University, China', 'Infused Synapse AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.10177.jpg', 'data': {'categories': ['#games', '#agents', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Spatio-Temporal Memory Agent (STMA) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –∞–≥–µ–Ω—Ç–∞–º–∏ —Å –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º. STMA –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –º–æ–¥—É–ª—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –∏ –º–µ—Ö–∞–Ω–∏–∑–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞-–∫—Ä–∏—Ç–∏–∫–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –≤ —Å—Ä–µ–¥–µ TextWorld –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∏ —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø–∞–º—è—Ç–∏ –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Enhancing Agent Intelligence with Spatio-Temporal Memory', 'desc': 'The paper introduces the Spatio-Temporal Memory Agent (STMA), which aims to improve how agents perform complex tasks in changing environments. STMA incorporates a spatio-temporal memory module to track past events and environmental shifts, enhancing decision-making. It also utilizes a dynamic knowledge graph for better spatial reasoning and a planner-critic mechanism to refine strategies over time. The results show that STMA significantly outperforms existing models in task success rates and scoring, demonstrating the importance of memory in embodied intelligence.'}, 'zh': {'title': 'Êó∂Á©∫ËÆ∞ÂøÜÊô∫ËÉΩ‰ΩìÔºöÊèêÂçáÊô∫ËÉΩ‰ΩìÂÜ≥Á≠ñ‰∏éÈÄÇÂ∫îËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÁß∞‰∏∫Êó∂Á©∫ËÆ∞ÂøÜÊô∫ËÉΩ‰ΩìÔºàSTMAÔºâÔºåÊó®Âú®ÊèêÈ´òÊô∫ËÉΩ‰ΩìÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÊâßË°åÈïøÊúü‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇSTMAÈõÜÊàê‰∫ÜÊó∂Á©∫ËÆ∞ÂøÜÊ®°Âùó„ÄÅÂä®ÊÄÅÁü•ËØÜÂõæË∞±ÂíåËßÑÂàí-ËØÑ‰º∞Êú∫Âà∂Ôºå‰ª•Â¢ûÂº∫‰ªªÂä°ËßÑÂàíÂíåÊâßË°åÁöÑÊïàÊûú„ÄÇÈÄöËøáÂú®TextWorldÁéØÂ¢É‰∏≠ËøõË°å32‰∏™‰ªªÂä°ÁöÑËØÑ‰º∞ÔºåSTMAÂú®ÊàêÂäüÁéáÂíåÂπ≥ÂùáÂæóÂàÜ‰∏äÂàÜÂà´ÊèêÈ´ò‰∫Ü31.25%Âíå24.7%„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊó∂Á©∫ËÆ∞ÂøÜÂú®ÊèêÂçáÊô∫ËÉΩ‰ΩìÁöÑËÆ∞ÂøÜËÉΩÂäõÊñπÈù¢ÂÖ∑ÊúâÊòæËëóÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07856', 'title': 'MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers', 'url': 'https://huggingface.co/papers/2502.07856', 'abstract': 'In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.', 'score': 2, 'issue_id': 2244, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '2bb6766a68f50cdc', 'authors': ['Ao Li', 'Wei Fang', 'Hongbo Zhao', 'Le Lu', 'Ge Yang', 'Minfeng Xu'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Laboratory, Hangzhou, China', 'Institute of Automation, Chinese Academy of Sciences (CASIA)', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07856.jpg', 'data': {'categories': ['#training', '#cv', '#data', '#diffusion', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º MRS (MR Sampler) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ Mean Reverting (MR) Diffusion –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –æ–±—Ä–∞—Ç–Ω–æ–µ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å MR Diffusion, –∏ –≤—ã–≤–æ–¥—è—Ç –ø–æ–ª—É–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —à—É–º–∞, –¥–∞–Ω–Ω—ã—Ö –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MR Sampler —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ —É—Å–∫–æ—Ä–µ–Ω–∏–∏ –≤ 10-20 —Ä–∞–∑ –¥–ª—è –¥–µ—Å—è—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Accelerating Controllable Generation with MR Sampler', 'desc': 'This paper introduces a new algorithm called MRS (MR Sampler) to improve the efficiency of sampling in Mean Reverting (MR) Diffusion models. Unlike traditional methods that modify the score function, MRS directly addresses the stochastic differential equation structure, simplifying the integration of image conditions. The proposed method achieves high-quality sample generation with significantly fewer function evaluations, enhancing the speed of the sampling process. Experimental results show that MRS can produce samples 10 to 20 times faster while maintaining quality across various image restoration tasks.'}, 'zh': {'title': 'Âä†ÈÄüÂèØÊéßÁîüÊàêÁöÑMRÈááÊ†∑Âô®', 'desc': 'Âú®Êâ©Êï£Ê®°ÂûãÁöÑÂ∫îÁî®‰∏≠ÔºåÂèØÊéßÁîüÊàêÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÊÑè‰πâÔºå‰ΩÜ‰πüÈù¢‰∏¥ÊåëÊàò„ÄÇÂΩìÂâçÁöÑÂèØÊéßÁîüÊàêÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰øÆÊîπÊâ©Êï£Ê®°ÂûãÁöÑËØÑÂàÜÂáΩÊï∞ÔºåËÄåÂùáÂÄºÂõûÂΩíÊâ©Êï£ÔºàMR DiffusionÔºâÂàôÁõ¥Êé•‰øÆÊîπÈöèÊú∫ÂæÆÂàÜÊñπÁ®ãÔºàSDEÔºâÁöÑÁªìÊûÑÔºå‰ΩøÂæóÂõæÂÉèÊù°‰ª∂ÁöÑÁªìÂêàÊõ¥Âä†ÁÆÄÂçïËá™ÁÑ∂„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁÆóÊ≥ïMRSÔºàMRÈááÊ†∑Âô®ÔºâÔºåÊó®Âú®ÂáèÂ∞ëMRÊâ©Êï£ÁöÑÈááÊ†∑ÂáΩÊï∞ËØÑ‰º∞Ê¨°Êï∞ÔºàNFEsÔºâÔºåÂπ∂ÈÄöËøáËß£ÂÜ≥‰∏éMRÊâ©Êï£Áõ∏ÂÖ≥ÁöÑÂèçÂêëÊó∂Èó¥SDEÂíåÊ¶ÇÁéáÊµÅÂ∏∏ÂæÆÂàÜÊñπÁ®ãÔºàPF-ODEÔºâÊù•Ëé∑ÂæóÈ´òË¥®ÈáèÊ†∑Êú¨„ÄÇÂÆûÈ™åË°®ÊòéÔºåMRÈááÊ†∑Âô®Âú®ÂçÅÁßç‰∏çÂêåÁöÑÂõæÂÉèÊÅ¢Â§ç‰ªªÂä°‰∏≠‰øùÊåÅÈ´òÈááÊ†∑Ë¥®ÈáèÔºåÂπ∂ÂÆûÁé∞‰∫Ü10Âà∞20ÂÄçÁöÑÂä†ÈÄü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10362', 'title': 'CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages', 'url': 'https://huggingface.co/papers/2502.10362', 'abstract': 'CLaMP 3 is a unified framework developed to address challenges of cross-modal and cross-lingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalities--including sheet music, performance signals, and audio recordings--with multilingual text in a shared representation space, enabling retrieval across unaligned modalities with text as a bridge. It features a multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, a web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents a wide array of global musical traditions. To advance future research, we release WikiMT-X, a benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts.', 'score': 1, 'issue_id': 2253, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '01dc60f8f9db0ad7', 'authors': ['Shangda Wu', 'Zhancheng Guo', 'Ruibin Yuan', 'Junyan Jiang', 'Seungheon Doh', 'Gus Xia', 'Juhan Nam', 'Xiaobing Li', 'Feng Yu', 'Maosong Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.10362.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#multilingual', '#rag', '#low_resource', '#data', '#benchmark', '#dataset'], 'emoji': 'üéµ', 'ru': {'title': '–ï–¥–∏–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –º—É–∑—ã–∫–∏', 'desc': 'CLaMP 3 - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–π –∏ –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º—É–∑—ã–∫–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π (–Ω–æ—Ç—ã, —Å–∏–≥–Ω–∞–ª—ã –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è, –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–∏) —Å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –≤ –µ–¥–∏–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä, –∞–¥–∞–ø—Ç–∏—Ä—É–µ–º—ã–π –∫ –Ω–æ–≤—ã–º —è–∑—ã–∫–∞–º, –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CLaMP 3 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö MIR, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–∏–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Bridging Music and Language with CLaMP 3', 'desc': "CLaMP 3 is a new framework designed to improve how we retrieve music information across different formats and languages. It uses contrastive learning to connect various music types, like sheet music and audio, with text in multiple languages, allowing for better searches even when the formats don't match. The framework includes a multilingual text encoder that can adapt to new languages, showing its ability to generalize across different linguistic contexts. Additionally, it introduces a large dataset and benchmark to support further research in music information retrieval, achieving top performance in various tasks."}, 'zh': {'title': 'Ë∑®Ê®°ÊÄÅÈü≥‰πêÊ£ÄÁ¥¢ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'CLaMP 3 ÊòØ‰∏Ä‰∏™Áªü‰∏ÄÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Èü≥‰πê‰ø°ÊÅØÊ£ÄÁ¥¢‰∏≠ÁöÑË∑®Ê®°ÊÄÅÂíåË∑®ËØ≠Ë®ÄÊ≥õÂåñÊåëÊàò„ÄÇÂÆÉÈÄöËøáÂØπÊØîÂ≠¶‰π†ÔºåÂ∞Ü‰πêË∞±„ÄÅË°®Êºî‰ø°Âè∑ÂíåÈü≥È¢ëÂΩïÈü≥Á≠â‰∏ªË¶ÅÈü≥‰πêÊ®°ÊÄÅ‰∏éÂ§öËØ≠Ë®ÄÊñáÊú¨ÂØπÈΩêÔºåÂΩ¢ÊàêÂÖ±‰∫´Ë°®Á§∫Á©∫Èó¥Ôºå‰ªéËÄåÂÆûÁé∞ÈÄöËøáÊñáÊú¨‰Ωú‰∏∫Ê°•Ê¢ÅÁöÑÊ£ÄÁ¥¢„ÄÇËØ•Ê°ÜÊû∂ÂÖ∑ÊúâÈÄÇÂ∫îÊú™ËßÅËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÊñáÊú¨ÁºñÁ†ÅÂô®ÔºåÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑË∑®ËØ≠Ë®ÄÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÂ¢ûÂº∫Ê£ÄÁ¥¢ÁîüÊàêÔºåÊàë‰ª¨ÂàõÂª∫‰∫Ü M4-RAG Êï∞ÊçÆÈõÜÔºåÂåÖÂê´ 231 ‰∏áÂØπÈü≥‰πê-ÊñáÊú¨ÂØπÔºåÂπ∂ÂèëÂ∏É‰∫Ü WikiMT-X Âü∫ÂáÜÔºåÊé®Âä®Êú™Êù•Á†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08769', 'title': 'Cluster and Predict Latents Patches for Improved Masked Image Modeling', 'url': 'https://huggingface.co/papers/2502.08769', 'abstract': 'Masked Image Modeling (MIM) offers a promising approach to self-supervised representation learning, however existing MIM models still lag behind the state-of-the-art. In this paper, we systematically analyze target representations, loss functions, and architectures, to introduce CAPI - a novel pure-MIM framework that relies on the prediction of latent clusterings. Our approach leverages a clustering-based loss, which is stable to train, and exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8% accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes, substantially outperforming previous MIM methods and approaching the performance of the current state-of-the-art, DINOv2. We release all our code and models.', 'score': 1, 'issue_id': 2250, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '8fd9852310af51f5', 'authors': ['Timoth√©e Darcet', 'Federico Baldassarre', 'Maxime Oquab', 'Julien Mairal', 'Piotr Bojanowski'], 'affiliations': ['CNRS', 'Grenoble INP', 'Inria', 'LJK', 'Meta', 'Univ. Grenoble Alpes'], 'pdf_title_img': 'assets/pdf/title_img/2502.08769.jpg', 'data': {'categories': ['#optimization', '#cv', '#training', '#architecture', '#dataset', '#open_source'], 'emoji': 'üß†', 'ru': {'title': 'CAPI: –ü—Ä–æ—Ä—ã–≤ –≤ —Å–∞–º–æ–æ–±—É—á–∞–µ–º–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CAPI - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–∞–º–æ–æ–±—É—á–∞–µ–º–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (MIM). CAPI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–π –∏ –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 83.8% –Ω–∞ ImageNet –∏ 32.1% mIoU –Ω–∞ ADE20K —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Å—Ç—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–æ–±. CAPI –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã MIM –∏ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ state-of-the-art –º–µ—Ç–æ–¥–∞ DINOv2.'}, 'en': {'title': 'CAPI: Clustering for Superior Masked Image Modeling', 'desc': 'This paper presents CAPI, a new framework for Masked Image Modeling (MIM) that enhances self-supervised learning by focusing on predicting latent clusterings. The authors analyze various aspects of MIM, including target representations and loss functions, to develop a clustering-based loss that is stable during training. CAPI utilizes a Vision Transformer (ViT-L) backbone, achieving impressive results with 83.8% accuracy on ImageNet and 32.1% mean Intersection over Union (mIoU) on ADE20K. The framework significantly outperforms previous MIM methods and approaches the performance of the leading model, DINOv2, while the authors provide all code and models for further research.'}, 'zh': {'title': 'CAPIÔºöÊèêÂçáËá™ÁõëÁù£Â≠¶‰π†ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ∫ØÈÅÆÁΩ©ÂõæÂÉèÂª∫Ê®°Ê°ÜÊû∂CAPIÔºåÊó®Âú®ÊèêÂçáËá™ÁõëÁù£Ë°®Á§∫Â≠¶‰π†ÁöÑÊïàÊûú„ÄÇÊàë‰ª¨Á≥ªÁªüÂàÜÊûê‰∫ÜÁõÆÊ†áË°®Á§∫„ÄÅÊçüÂ§±ÂáΩÊï∞ÂíåÊû∂ÊûÑÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËÅöÁ±ªÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøÂæóËÆ≠ÁªÉËøáÁ®ãÊõ¥Âä†Á®≥ÂÆö„ÄÇCAPIÂú®ViT-LÈ™®Âπ≤ÁΩëÁªú‰∏äÂÆûÁé∞‰∫Ü83.8%ÁöÑImageNetÂáÜÁ°ÆÁéáÂíå32.1%ÁöÑADE20K mIoUÔºåÊòæËëóË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑMIMÊñπÊ≥ï„ÄÇÊàë‰ª¨Â∞ÜÊâÄÊúâ‰ª£Á†ÅÂíåÊ®°ÂûãÂÖ¨ÂºÄÔºå‰øÉËøõÁ†îÁ©∂ÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09980', 'title': 'V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models', 'url': 'https://huggingface.co/papers/2502.09980', 'abstract': 'Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .', 'score': 1, 'issue_id': 2244, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '57343c782d806dc0', 'authors': ['Hsu-kuang Chiu', 'Ryo Hachiuma', 'Chien-Yi Wang', 'Stephen F. Smith', 'Yu-Chiang Frank Wang', 'Min-Hung Chen'], 'affiliations': ['Carnegie Mellon University', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2502.09980.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#reasoning', '#science', '#agi', '#games', '#optimization', '#dataset', '#agents'], 'emoji': 'üöó', 'ru': {'title': 'LLM –Ω–∞ —Å–ª—É–∂–±–µ –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –≤–æ–∂–¥–µ–Ω–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏ –±–µ–Ω—á–º–∞—Ä–∫ V2V-QA –¥–ª—è –æ–±–º–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –º–µ–∂–¥—É –∞–≤—Ç–æ–º–æ–±–∏–ª—è–º–∏ —á–µ—Ä–µ–∑ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É. –ò—Ö –º–µ—Ç–æ–¥ V2V-LLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≤–æ–∂–¥–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ V2V-LLM –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±—É–¥—É—â–∏—Ö —Å–∏—Å—Ç–µ–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Enhancing Cooperative Driving with Language Models', 'desc': 'This paper introduces a new approach to enhance cooperative autonomous driving by integrating Large Language Models (LLMs) with vehicle-to-vehicle (V2V) communication. The proposed method, called Vehicle-to-Vehicle Large Language Model (V2V-LLM), allows connected autonomous vehicles (CAVs) to share and fuse perception data, enabling them to answer driving-related questions effectively. The authors present a new dataset, Vehicle-to-Vehicle Question-Answering (V2V-QA), to benchmark this integration and demonstrate its effectiveness in improving planning and safety. Experimental results indicate that V2V-LLM outperforms existing methods, paving the way for a unified model architecture in cooperative driving systems.'}, 'zh': {'title': 'Âçè‰ΩúËá™Âä®È©æÈ©∂ÁöÑÊñ∞ÊñπÂêëÔºöËΩ¶ËæÜÈó¥ÈóÆÁ≠îÊ®°Âûã', 'desc': 'ÂΩìÂâçÁöÑËá™Âä®È©æÈ©∂ËΩ¶ËæÜ‰∏ªË¶Å‰æùËµñÂêÑËá™ÁöÑ‰º†ÊÑüÂô®Êù•ÁêÜËß£Âë®Âõ¥Âú∫ÊôØÂíåËßÑÂàíÊú™Êù•ËΩ®ËøπÔºå‰ΩÜÂΩì‰º†ÊÑüÂô®Âá∫Áé∞ÊïÖÈöúÊàñË¢´ÈÅÆÊå°Êó∂ÔºåËøôÁßçÊñπÊ≥ïÂèØËÉΩ‰∏çÂèØÈù†„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊèêÂá∫‰∫ÜÈÄöËøáËΩ¶‰∏éËΩ¶ÔºàV2VÔºâÈÄö‰ø°ÁöÑÂçè‰ΩúÊÑüÁü•ÊñπÊ≥ïÔºå‰ΩÜËøô‰∫õÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ê£ÄÊµãÂíåË∑üË∏™‰∏ä„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÈóÆÈ¢òËÆæÁΩÆÔºåÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈõÜÊàêÂà∞Âçè‰ΩúËá™Âä®È©æÈ©∂‰∏≠ÔºåÂπ∂ÂºïÂÖ•‰∫ÜËΩ¶ËæÜÈó¥ÈóÆÁ≠îÔºàV2V-QAÔºâÊï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÊµãËØï„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåV2V-LLMËÉΩÂ§üÊúâÊïàËûçÂêàÂ§ö‰∏™ËøûÊé•ÁöÑËá™Âä®È©æÈ©∂ËΩ¶ËæÜÁöÑÊÑüÁü•‰ø°ÊÅØÔºåÂπ∂Âú®Âçè‰ΩúËá™Âä®È©æÈ©∂‰∏≠ÊâßË°åÂ§öÁßç‰ªªÂä°ÔºåÊèêÂçáÊú™Êù•Ëá™Âä®È©æÈ©∂Á≥ªÁªüÁöÑÂÆâÂÖ®ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10173', 'title': 'Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model', 'url': 'https://huggingface.co/papers/2502.10173', 'abstract': 'Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.', 'score': 0, 'issue_id': 2247, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '84102aa522298331', 'authors': ['Bo Ni', 'Markus J. Buehler'], 'affiliations': ['Center for Computational Science and Engineering, Schwarzman College of Computing, Massachusetts Institute of Technology, Cambridge, MA, USA', 'Department of Materials Science and Engineering, Carnegie Mellon University, Pittsburgh, PA, USA', 'Laboratory for Atomistic and Molecular Mechanics (LAMM), Massachusetts Institute of Technology, Cambridge, MA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.10173.jpg', 'data': {'categories': ['#architecture', '#agents', '#dataset'], 'emoji': 'üß¨', 'ru': {'title': 'VibeGen: –ò–ò-–¥–∏–∑–∞–π–Ω –±–µ–ª–∫–æ–≤ —Å –∑–∞–¥–∞–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–æ–π', 'desc': 'VibeGen - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ò–ò-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–ª–∫–æ–≤ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–æ–π–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –º–æ–¥–µ–ª—å—é-–¥–∏–∑–∞–π–Ω–µ—Ä–æ–º, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–µ–ª–∫–æ–≤, –∏ –º–æ–¥–µ–ª—å—é-–ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–º, –æ—Ü–µ–Ω–∏–≤–∞—é—â–µ–π –∏—Ö –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å. VibeGen —Å–ø–æ—Å–æ–±–Ω–∞ —Å–æ–∑–¥–∞–≤–∞—Ç—å de novo –±–µ–ª–∫–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–∞–º–∏ –∫–æ–ª–µ–±–∞–Ω–∏–π, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è –ø–æ–ª–Ω–æ–∞—Ç–æ–º–Ω—ã–º –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –±–∏–æ–º–æ–ª–µ–∫—É–ª —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏.'}, 'en': {'title': 'Unlocking Dynamic Protein Design with VibeGen', 'desc': 'This paper presents VibeGen, a generative AI framework designed for creating proteins with specific dynamic properties. It utilizes a dual-model architecture that includes a protein designer to generate sequences based on desired vibrational modes and a protein predictor to assess their dynamic accuracy. The framework successfully integrates protein dynamics into the design process, allowing for the creation of novel protein sequences that do not resemble existing natural proteins. This innovation opens new avenues for engineering proteins with tailored functions and dynamics, which could significantly impact fields like enzyme design and biomaterials.'}, 'zh': {'title': 'Âä®ÊÄÅÈ©±Âä®ÁöÑËõãÁôΩË¥®ËÆæËÆ°Êñ∞ÊñπÊ≥ï', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫VibeGenÁöÑÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÊ°ÜÊû∂ÔºåÁî®‰∫éËÆæËÆ°ÂÖ∑ÊúâÁâπÂÆöÂä®ÊÄÅÁâπÊÄßÁöÑËõãÁôΩË¥®„ÄÇVibeGenÁªìÂêà‰∫ÜËõãÁôΩË¥®ËÆæËÆ°Âô®ÂíåËõãÁôΩË¥®È¢ÑÊµãÂô®ÔºåÂâçËÄÖÊ†πÊçÆÊåáÂÆöÁöÑÊåØÂä®Ê®°ÂºèÁîüÊàêÂ∫èÂàóÂÄôÈÄâÔºåÂêéËÄÖËØÑ‰º∞ÂÖ∂Âä®ÊÄÅÂáÜÁ°ÆÊÄß„ÄÇÈÄöËøáÂÖ®ÂéüÂ≠êÂàÜÂ≠êÊ®°ÊãüÈ™åËØÅÔºåËÆæËÆ°ÁöÑËõãÁôΩË¥®ËÉΩÂ§üÂáÜÁ°ÆÂÜçÁé∞È¢ÑÂÆöÁöÑÊ≠£Â∏∏Ê®°ÂºèÊåØÂπÖÔºåÂπ∂ÈááÁî®Â§öÁßçÁ®≥ÂÆöÁöÑ„ÄÅÂäüËÉΩÁõ∏ÂÖ≥ÁöÑÁªìÊûÑ„ÄÇËØ•ÊñπÊ≥ï‰∏ç‰ªÖÊâ©Â±ï‰∫ÜÂèØËÆæËÆ°ËõãÁôΩË¥®ÁöÑÁ©∫Èó¥ÔºåËøò‰∏∫ÁÅµÊ¥ªÈÖ∂„ÄÅÂä®ÊÄÅÊîØÊû∂ÂíåÁîüÁâ©ÊùêÊñôÁöÑÁêÜÊÄßËÆæËÆ°Êèê‰æõ‰∫ÜÊñ∞ÁöÑÈÄîÂæÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12900', 'title': 'Soundwave: Less is More for Speech-Text Alignment in LLMs', 'url': 'https://huggingface.co/papers/2502.12900', 'abstract': 'Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.', 'score': 62, 'issue_id': 2289, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '95780ecdf251cffd', 'authors': ['Yuhao Zhang', 'Zhiheng Liu', 'Fan Bu', 'Ruiyu Zhang', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.12900.jpg', 'data': {'categories': ['#training', '#audio', '#transfer_learning', '#open_source', '#optimization', '#data', '#architecture'], 'emoji': 'üéôÔ∏è', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Soundwave - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞–∑—Ä—ã–≤–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –º–µ–∂–¥—É —Ä–µ—á—å—é –∏ —Ç–µ–∫—Å—Ç–æ–º. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É. Soundwave –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–µ—Ä–µ–¥–æ–≤—É—é –º–æ–¥–µ–ª—å Qwen2-Audio –≤ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ—á–∏ –∏ —Ç–µ—Å—Ç–∞—Ö AIR-Bench, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 1/50 —á–∞—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤–æ–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤–æ –≤—Ä–µ–º—è –¥–∏–∞–ª–æ–≥–∞.'}, 'en': {'title': 'Soundwave: Efficient Speech Translation with Minimal Data', 'desc': 'This paper introduces Soundwave, a new model designed for speech translation that requires significantly less training data compared to existing large language models. It addresses two key challenges: the differences in how speech and text are represented and the varying lengths of sequences in speech data. By employing an efficient training strategy and a unique architecture, Soundwave achieves superior performance on speech tasks while using only 2% of the data needed by its competitors. The findings indicate that Soundwave maintains high conversational intelligence, making it a promising approach for data-efficient speech processing.'}, 'zh': {'title': 'È´òÊïàËÆ≠ÁªÉÔºåËØ≠Èü≥ÁøªËØëÊñ∞Á™ÅÁ†¥ÔºÅ', 'desc': 'Áé∞ÊúâÁöÑÁ´ØÂà∞Á´ØËØ≠Èü≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÈÄöÂ∏∏‰æùËµñ‰∫éÂ§ßËßÑÊ®°Ê†áÊ≥®Êï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåËÄåÊï∞ÊçÆÈ´òÊïàËÆ≠ÁªÉÂ∞öÊú™Ê∑±ÂÖ•Êé¢ËÆ®„ÄÇÊàë‰ª¨ÂÖ≥Ê≥®ËØ≠Èü≥ÂíåÊñáÊú¨‰πãÈó¥ÁöÑ‰∏§‰∏™Âü∫Êú¨ÈóÆÈ¢òÔºöË°®Á§∫Á©∫Èó¥Â∑ÆË∑ùÂíåÂ∫èÂàóÈïøÂ∫¶‰∏ç‰∏ÄËá¥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜSoundwaveÔºåÂÆÉÂà©Áî®È´òÊïàÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÂíåÊñ∞È¢ñÁöÑÊû∂ÊûÑÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇÁªìÊûúË°®ÊòéÔºåSoundwaveÂú®ËØ≠Èü≥ÁøªËØëÂíåAIR-BenchËØ≠Èü≥‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÂÖàËøõÁöÑQwen2-AudioÔºå‰ªÖ‰ΩøÁî®‰∫Ü‰∫îÂçÅÂàÜ‰πã‰∏ÄÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13063', 'title': 'Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity', 'url': 'https://huggingface.co/papers/2502.13063', 'abstract': 'A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches allow to reduce the amount of compute in existing language models. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.', 'score': 51, 'issue_id': 2293, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': 'bd5537cf011da83d', 'authors': ['Yuri Kuratov', 'Mikhail Arkhipov', 'Aydar Bulatov', 'Mikhail Burtsev'], 'affiliations': ['AIRI, Moscow, Russia', 'Independent Researcher, Amsterdam, Netherlands', 'London Institute for Mathematical Sciences, London, UK', 'Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.13063.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': 'üóúÔ∏è', 'ru': {'title': '–†–∞—Å–∫—Ä—ã–≤–∞—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Å–≤–µ—Ä—Ö—Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∂–∞—Ç–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å–∂–∞—Ç–∏—è –Ω–µ –≤—ã—à–µ 10, —Ö–æ—Ç—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ–µ —Å–∂–∞—Ç–∏–µ. –ò—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–∂–∞—Ç–∏—è –¥–æ 1500 —Ä–∞–∑. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π —ë–º–∫–æ—Å—Ç—å—é –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö.'}, 'en': {'title': 'Unlocking Compression: From x10 to x1500 in Token Sequences', 'desc': 'This paper investigates the compression of token sequences into shorter real-valued vectors for use in language models. It reveals that while current methods achieve a maximum lossless compression ratio of about x10, a new optimization approach can reach ratios as high as x1500. The study emphasizes that the limits of compression are influenced more by the uncertainty in the data rather than the input length itself. This finding indicates a significant disparity between the theoretical potential of embeddings and their actual performance, suggesting opportunities for further optimization in model architecture.'}, 'zh': {'title': 'ÂéãÁº©ÊΩúÂäõÔºö‰ªé10ÂÄçÂà∞1500ÂÄçÁöÑÈ£ûË∑É', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ∞ÜÂ∫èÂàóÁöÑÊ†áËÆ∞ÂéãÁº©‰∏∫Êõ¥Áü≠ÁöÑÂÆûÂÄºÂêëÈáèÂ∫èÂàóÁöÑÈóÆÈ¢òÔºå‰ª•‰æøÁî®‰ΩúËæìÂÖ•ÔºåËÄå‰∏çÊòØ‰ΩøÁî®Ê†áËÆ∞ÂµåÂÖ•ÊàñÈîÆÂÄºÁºìÂ≠ò„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÁöÑÁºñÁ†ÅÂô®Ê®°ÂûãÈùûÂ∏∏Âº∫Â§ßÔºå‰ΩÜÊó†ÊçüÂéãÁº©ÁöÑÊúÄÂ§ßÊØîÁéáÈÄöÂ∏∏‰∏çË∂ÖËøá10ÂÄç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄöËøáÊØè‰∏™Ê†∑Êú¨ÁöÑ‰ºòÂåñÁ®ãÂ∫èÊõø‰ª£ÁºñÁ†ÅÂô®ÔºåÂèØ‰ª•ÂÆûÁé∞È´òËææ1500ÂÄçÁöÑÂéãÁº©ÊØîÔºåÊòæÁ§∫Âá∫Áé∞ÊúâËß£ÂÜ≥ÊñπÊ°à‰∏éÂèØÂÆûÈôÖËææÂà∞ÁöÑËß£ÂÜ≥ÊñπÊ°à‰πãÈó¥ÁöÑÂ∑®Â§ßÂ∑ÆË∑ù„ÄÇÊ≠§Â§ñÔºåÂéãÁº©ÁöÑÈôêÂà∂‰∏ªË¶ÅÁî±ÈúÄË¶ÅÂáèÂ∞ëÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂÜ≥ÂÆöÔºåËÄå‰∏çÊòØËæìÂÖ•ÁöÑÈïøÂ∫¶ÔºåËøô‰∏∫Ê®°ÂûãËÆæËÆ°ÁöÑ‰ºòÂåñÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÁ©∫Èó¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11564', 'title': 'Continuous Diffusion Model for Language Modeling', 'url': 'https://huggingface.co/papers/2502.11564', 'abstract': 'Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}.', 'score': 41, 'issue_id': 2287, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': 'f5889d3a88d24b7c', 'authors': ['Jaehyeong Jo', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2502.11564.jpg', 'data': {'categories': ['#training', '#benchmark', '#optimization', '#dataset', '#diffusion', '#architecture'], 'emoji': 'üåä', 'ru': {'title': '–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–º –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏ –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–µ–π –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–º –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±–æ–±—â–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –≤–≤–æ–¥–∏—Ç –±–µ–∑—Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω—É—é —Å—Ö–µ–º—É –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing Language Modeling with Continuous Diffusion', 'desc': 'This paper presents a new continuous diffusion model designed for language modeling, which effectively handles discrete categorical data. The authors highlight the limitations of existing discrete diffusion models and propose a method that leverages the geometry of categorical distributions to enhance performance. By establishing a connection between discrete diffusion and continuous flow on a statistical manifold, they introduce a novel design that improves iterative refinement. Their experiments demonstrate that this approach not only surpasses traditional discrete models but also approaches the performance of autoregressive models.'}, 'zh': {'title': 'ËøûÁª≠Êâ©Êï£Ê®°ÂûãÔºöÊèêÂçáÁ¶ªÊï£Êï∞ÊçÆÂª∫Ê®°ÁöÑÊÄßËÉΩ', 'desc': 'Êâ©Êï£Ê®°Âûã‰Ωú‰∏∫‰∏ÄÁßçÊñ∞ÂÖ¥ÁöÑÊõø‰ª£Ëá™ÂõûÂΩíÊ®°ÂûãÁöÑÊñπÊ≥ïÔºåÂú®Âª∫Ê®°Á¶ªÊï£ÂàÜÁ±ªÊï∞ÊçÆÊñπÈù¢Â±ïÁé∞‰∫ÜËâØÂ•ΩÁöÑÂâçÊôØ„ÄÇÁé∞ÊúâÁöÑËøûÁª≠Êâ©Êï£Ê®°ÂûãÂú®Â§ÑÁêÜÁ¶ªÊï£Êï∞ÊçÆÊó∂ÊÄßËÉΩÊúâÈôêÔºå‰∏î‰∫åËÄÖ‰πãÈó¥ÁöÑËÅîÁ≥ª‰∏çÊòéÁ°ÆÔºåÈôêÂà∂‰∫ÜÊâ©Êï£Ê®°ÂûãÁöÑÂèëÂ±ï„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËøûÁª≠Êâ©Êï£Ê®°ÂûãÔºåÁªìÂêà‰∫ÜÂü∫Á°ÄÂàÜÁ±ªÂàÜÂ∏ÉÁöÑÂá†‰ΩïÁâπÊÄßÔºåÂπ∂Âª∫Á´ã‰∫ÜÁ¶ªÊï£Êâ©Êï£‰∏éËøûÁª≠ÊµÅÂä®‰πãÈó¥ÁöÑËÅîÁ≥ª„ÄÇÈÄöËøáÂÖ®Èù¢ÁöÑÂÆûÈ™åÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ËØ≠Ë®ÄÂª∫Ê®°Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÁ¶ªÊï£Êâ©Êï£Ê®°ÂûãÔºåÊé•ËøëËá™ÂõûÂΩíÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11079', 'title': 'Phantom: Subject-consistent video generation via cross-modal alignment', 'url': 'https://huggingface.co/papers/2502.11079', 'abstract': 'The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.', 'score': 40, 'issue_id': 2286, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': 'e443a635e58b164f', 'authors': ['Lijie Liu', 'Tianxiang Ma', 'Bingchuan Li', 'Zhuowei Chen', 'Jiawei Liu', 'Qian He', 'Xinglong Wu'], 'affiliations': ['Intelligent Creation Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.11079.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal'], 'emoji': 'üé•', 'ru': {'title': 'Phantom: –±–∞–ª–∞–Ω—Å —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Phantom - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å—É–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –æ–ø–æ—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. Phantom –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—É—á–∞—è—Å—å –Ω–∞ —Ç—Ä–∏–ø–ª–µ—Ç–∞—Ö —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–≤–∏–¥–µ–æ. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ª—é–¥–µ–π –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Phantom: Consistent Video Generation from Text and Images', 'desc': 'This paper introduces a new approach called Subject-to-Video, which focuses on generating videos that maintain consistency with specific subjects extracted from reference images. The authors present Phantom, a unified framework that integrates both text and image inputs to create videos, ensuring that the generated content aligns well with the provided prompts. By utilizing a triplet data structure of text, image, and video, Phantom enhances the learning of cross-modal relationships, improving the quality of video generation. The framework particularly excels in generating videos of humans while preserving their identity across frames, marking a significant advancement in video generation technology.'}, 'zh': {'title': 'ÂÆûÁé∞‰∏ªÈ¢ò‰∏ÄËá¥ÊÄßÁöÑËßÜÈ¢ëÁîüÊàê', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÁß∞‰∏∫PhantomÔºåÊó®Âú®ÂÆûÁé∞‰∏ªÈ¢ò‰∏ÄËá¥ÊÄßÁöÑËßÜÈ¢ëÁîüÊàê„ÄÇËØ•Ê®°ÂûãÈÄöËøáÊèêÂèñÂèÇËÄÉÂõæÂÉè‰∏≠ÁöÑ‰∏ªÈ¢òÂÖÉÁ¥†ÔºåÂπ∂ÁªìÂêàÊñáÊú¨Êåá‰ª§ÁîüÊàêËßÜÈ¢ë„ÄÇPhantomÊ°ÜÊû∂ËÉΩÂ§üÂ§ÑÁêÜÂçï‰∏ÄÂíåÂ§ö‰∏™‰∏ªÈ¢òÁöÑÂèÇËÄÉÔºåÂº∫Ë∞ÉÊñáÊú¨ÂíåÂõæÂÉèÁöÑÂèåÊ®°ÊÄÅÊèêÁ§∫ÁöÑÂπ≥Ë°°„ÄÇÁ†îÁ©∂ËÄÖ‰ª¨ÈÄöËøáÊñáÊú¨-ÂõæÂÉè-ËßÜÈ¢ë‰∏âÂÖÉÁªÑÊï∞ÊçÆÊù•Â≠¶‰π†Ë∑®Ê®°ÊÄÅÂØπÈΩêÔºå‰ªéËÄåÊèêÂçá‰∫∫Á±ªÁîüÊàêËßÜÈ¢ëÁöÑ‰∏ªÈ¢ò‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13131', 'title': 'Rethinking Diverse Human Preference Learning through Principal Component Analysis', 'url': 'https://huggingface.co/papers/2502.13131', 'abstract': 'Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment.', 'score': 32, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': 'b3376cde29e0b44f', 'authors': ['Feng Luo', 'Rui Yang', 'Hao Sun', 'Chunyuan Deng', 'Jiarui Yao', 'Jingyan Shen', 'Huan Zhang', 'Hanjie Chen'], 'affiliations': ['Columbia University', 'Rice University', 'University of Cambridge', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.13131.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training', '#dataset', '#interpretability'], 'emoji': 'üß©', 'ru': {'title': '–†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏–∑ –±–∏–Ω–∞—Ä–Ω—ã—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏–π –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö - Decomposed Reward Models (DRMs). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–º –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (PCA) –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –±–∞–∑–∏—Å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. DRMs –ø–æ–∑–≤–æ–ª—è—é—Ç –≥–∏–±–∫–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–æ–∂–µ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è–º —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –ø—Ä–µ–¥–ª–∞–≥–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—É—é –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DRMs —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞—é—Ç –∑–Ω–∞—á–∏–º—ã–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç—Å—è –∫ –Ω–æ–≤—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Decomposed Reward Models: Unlocking Diverse Human Preferences for Personalized AI', 'desc': 'This paper presents Decomposed Reward Models (DRMs), a new method for understanding human preferences in AI systems. Instead of relying on detailed preference data, DRMs use binary comparisons to extract diverse preferences, making the process more scalable and cost-effective. By representing preferences as vectors and applying Principal Component Analysis (PCA), the model identifies key dimensions of preference that can be combined to meet different user needs. The results show that DRMs can effectively adapt to new users and provide interpretable insights into preferences like helpfulness and safety.'}, 'zh': {'title': 'ÂàÜËß£Â•ñÂä±Ê®°ÂûãÔºö‰∏™ÊÄßÂåñAIÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÁêÜËß£‰∫∫Á±ªÂÅèÂ•ΩÂØπ‰∫éÊîπÂñÑÂü∫Á°ÄÊ®°ÂûãÂíåÊûÑÂª∫‰∏™ÊÄßÂåñAIÁ≥ªÁªüËá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰º†ÁªüÁöÑÂ•ñÂä±Ê®°ÂûãÈöæ‰ª•ÊçïÊçâÂÅèÂ•ΩÁöÑÂ§öÊ†∑ÊÄßÂíåÂ§çÊùÇÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁß∞‰∏∫ÂàÜËß£Â•ñÂä±Ê®°ÂûãÔºàDRMsÔºâÔºåÂÆÉÈÄöËøá‰∫åÂÖÉÊØîËæÉÊèêÂèñ‰∫∫Á±ªÂÅèÂ•ΩÔºåËÄåÊó†ÈúÄÁªÜÁ≤íÂ∫¶ÁöÑÊ≥®Èáä„ÄÇDRMsÂà©Áî®‰∏ªÊàêÂàÜÂàÜÊûêÔºàPCAÔºâÂàÜÊûêÂÅèÂ•ΩÂêëÈáèÔºåËÉΩÂ§üÁÅµÊ¥ªÁªÑÂêà‰ª•Êª°Ë∂≥‰∏çÂêåÁî®Êà∑ÈúÄÊ±ÇÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØËß£Èáä‰∏îÂèØÊâ©Â±ïÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13145', 'title': 'Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation', 'url': 'https://huggingface.co/papers/2502.13145', 'abstract': "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6times speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5times speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba", 'score': 26, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '6810113d41bfd26d', 'authors': ['Bencheng Liao', 'Hongyuan Tao', 'Qian Zhang', 'Tianheng Cheng', 'Yingyue Li', 'Haoran Yin', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.13145.jpg', 'data': {'categories': ['#open_source', '#optimization', '#transfer_learning', '#architecture', '#training', '#multimodal'], 'emoji': 'üöÄ', 'ru': {'title': 'mmMamba: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –ª–∏–Ω–µ–π–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç mmMamba - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ª–∏–Ω–µ–π–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –Ω–∞ –æ—Å–Ω–æ–≤–µ state space models. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ –ª–∏–Ω–µ–π–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö RNN-–º–æ–¥–µ–ª–µ–π –∏–ª–∏ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Mamba –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ Transformer –∏ —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ mmMamba –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏ —ç–∫–æ–Ω–æ–º–∏—é –ø–∞–º—è—Ç–∏.'}, 'en': {'title': 'Efficient Multimodal Models with mmMamba', 'desc': 'The paper introduces mmMamba, a new framework designed to create efficient multimodal large language models (MLLMs) with linear computational complexity. It utilizes a progressive distillation process to convert existing decoder-only MLLMs into more efficient architectures without needing separate vision encoders or pre-trained RNNs. The proposed seeding strategy and three-stage distillation recipe allow for effective knowledge transfer from Transformer models while maintaining multimodal capabilities. The results show that mmMamba-linear and mmMamba-hybrid significantly outperform traditional models in terms of speed and memory usage, making them suitable for practical deployment.'}, 'zh': {'title': 'mmMambaÔºöÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÊû∂ÊûÑ', 'desc': 'ÊúÄËøëÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÊÄßËÉΩ‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÁî±‰∫éÂÖ∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÂëàÂπ≥ÊñπÂ¢ûÈïø„ÄÅÂØπÈîÆÂÄºÁºìÂ≠òÁöÑÈúÄÊ±ÇÂ¢ûÂä†‰ª•Âèä‰æùËµñ‰∫éÁã¨Á´ãÁöÑËßÜËßâÁºñÁ†ÅÂô®ÔºåÈù¢‰∏¥ÈÉ®ÁΩ≤ÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜmmMambaÊ°ÜÊû∂ÔºåÈÄöËøá‰ªéÁé∞ÊúâÁöÑMLLMsËøõË°åÊ∏êËøõËí∏È¶èÔºåÂºÄÂèëÁ∫øÊÄßÂ§çÊùÇÂ∫¶ÁöÑÊú¨Âú∞Â§öÊ®°ÊÄÅÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºå‰ΩøÁî®ÈÄÇÂ∫¶ÁöÑÂ≠¶ÊúØËÆ°ÁÆóËµÑÊ∫ê„ÄÇËØ•ÊñπÊ≥ïÂÖÅËÆ∏Â∞ÜËÆ≠ÁªÉÂ•ΩÁöÑ‰ªÖËß£Á†ÅÂô®MLLMsÁõ¥Êé•ËΩ¨Êç¢‰∏∫Á∫øÊÄßÂ§çÊùÇÂ∫¶Êû∂ÊûÑÔºåËÄåÊó†ÈúÄÈ¢ÑËÆ≠ÁªÉÁöÑÂü∫‰∫éRNNÁöÑLLMÊàñËßÜËßâÁºñÁ†ÅÂô®„ÄÇÊàë‰ª¨ÁöÑËí∏È¶èÁ≠ñÁï•ÊúâÊïàÂú∞Â∞ÜÁü•ËØÜ‰ªéTransformerËΩ¨ÁßªÂà∞MambaÔºåÂêåÊó∂‰øùÁïôÂ§öÊ®°ÊÄÅËÉΩÂäõÔºåÂπ∂ÊîØÊåÅÁÅµÊ¥ªÁöÑÊ∑∑ÂêàÊû∂ÊûÑÔºå‰ª•ÂÆûÁé∞ÂèØÂÆöÂà∂ÁöÑÊïàÁéá‰∏éÊÄßËÉΩÊùÉË°°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13143', 'title': 'SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation', 'url': 'https://huggingface.co/papers/2502.13143', 'abstract': "Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER.", 'score': 26, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '7aab95a55286d8b2', 'authors': ['Zekun Qi', 'Wenyao Zhang', 'Yufei Ding', 'Runpei Dong', 'Xinqiang Yu', 'Jingwen Li', 'Lingyun Xu', 'Baoyu Li', 'Xialin He', 'Guofan Fan', 'Jiazhao Zhang', 'Jiawei He', 'Jiayuan Gu', 'Xin Jin', 'Kaisheng Ma', 'Zhizheng Zhang', 'He Wang', 'Li Yi'], 'affiliations': ['Eastern Institute of Technology', 'Galbot', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shanghai Qi Zhi Institute', 'ShanghaiTech University', 'Tsinghua University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.13143.jpg', 'data': {'categories': ['#3d', '#games', '#dataset', '#reasoning', '#robotics'], 'emoji': 'ü§ñ', 'ru': {'title': '–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É —Ä–æ–±–æ—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —Ä–æ–±–æ—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç OrienText300K —Å 3D –º–æ–¥–µ–ª—è–º–∏, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è–º–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –≤ —Å–∏—Å—Ç–µ–º—É —Å –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é (VLM) –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–æ–±–æ—Ç–∞–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è —Å —É—á–µ—Ç–æ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∏ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–æ–±–æ—Ç–æ–≤ –≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è—Ö —Å –æ–±—ä–µ–∫—Ç–∞–º–∏.'}, 'en': {'title': 'Empowering Robots with Semantic Orientation for Enhanced Manipulation', 'desc': 'This paper addresses the challenge of teaching robots to understand object orientations, which is essential for precise manipulation tasks. It introduces the concept of semantic orientation, allowing robots to interpret orientations using natural language instead of traditional geometric frames. The authors present OrienText300K, a dataset of 3D models annotated with these semantic orientations, linking geometric understanding to functional semantics. By incorporating this approach into vision-language models (VLMs), the paper demonstrates improved accuracy in robotic manipulation tasks, showcasing the effectiveness of using natural language for orientation representation.'}, 'zh': {'title': 'Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊèêÂçáÊú∫Âô®‰∫∫ÁöÑÁ©∫Èó¥Êô∫ËÉΩ', 'desc': 'Á©∫Èó¥Êô∫ËÉΩÊòØÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºåÂ∏ÆÂä©Êú∫Âô®‰∫∫ÁêÜËß£Âíå‰∏éÁéØÂ¢É‰∫íÂä®„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÁöÑËøõÂ±ïÊèêÈ´ò‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂØπÁâ©‰Ωì‰ΩçÁΩÆÂíåÂÖ≥Á≥ªÁöÑÊÑüÁü•ËÉΩÂäõÔºå‰ΩÜÂÆÉ‰ª¨‰ªçÁÑ∂Áº∫‰πèÁ≤æÁ°ÆÁêÜËß£Áâ©‰ΩìÊñπÂêëÁöÑËÉΩÂäõÔºåËøôÂØπ‰∫éÁªÜËá¥Êìç‰Ωú‰ªªÂä°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËØ≠‰πâÊñπÂêëÁöÑÊ¶ÇÂøµÔºå‰ΩøÁî®Ëá™ÁÑ∂ËØ≠Ë®Ä‰ª•Êó†ÂèÇËÄÉÊ°ÜÊû∂ÁöÑÊñπÂºèÂÆö‰πâÁâ©‰ΩìÊñπÂêë„ÄÇÈÄöËøáÊûÑÂª∫OrienText300KÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨Â∞ÜÂá†‰ΩïÁêÜËß£‰∏éÂäüËÉΩËØ≠‰πâËÅîÁ≥ªËµ∑Êù•Ôºå‰ªéËÄåÊèêÂçáÊú∫Âô®‰∫∫Âú®Êìç‰Ωú‰∏≠ÁöÑËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13130', 'title': 'Magma: A Foundation Model for Multimodal AI Agents', 'url': 'https://huggingface.co/papers/2502.13130', 'abstract': 'We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma.', 'score': 25, 'issue_id': 2288, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '1851b242ac65c88f', 'authors': ['Jianwei Yang', 'Reuben Tan', 'Qianhui Wu', 'Ruijie Zheng', 'Baolin Peng', 'Yongyuan Liang', 'Yu Gu', 'Mu Cai', 'Seonghyeon Ye', 'Joel Jang', 'Yuquan Deng', 'Lars Liden', 'Jianfeng Gao'], 'affiliations': ['KAIST', 'Microsoft Research', 'University of Maryland', 'University of Washington', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2502.13130.jpg', 'data': {'categories': ['#cv', '#robotics', '#agi', '#multimodal', '#agents', '#open_source'], 'emoji': 'ü§ñ', 'ru': {'title': 'Magma: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º–∏—Ä–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Magma - –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Å–ø–æ—Å–æ–±–Ω—É—é –≤—ã–ø–æ–ª–Ω—è—Ç—å –∞–≥–µ–Ω—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∫–∞–∫ –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–º, —Ç–∞–∫ –∏ –≤ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º –º–∏—Ä–µ. –ú–æ–¥–µ–ª—å —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö vision-language –º–æ–¥–µ–ª–µ–π, –¥–æ–±–∞–≤–ª—è—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –º–∏—Ä–µ. Magma –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–∏—Ö –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ –∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑–º–µ—Ç–∫–∏ Set-of-Mark –∏ Trace-of-Mark. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Magma –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤—ã—Ö –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º—É –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É –∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–∞–º–∏.'}, 'en': {'title': 'Magma: Bridging Digital and Physical Worlds with Multimodal Intelligence', 'desc': 'Magma is a foundation model designed for multimodal AI tasks that operate in both digital and physical environments. It enhances traditional vision-language models by integrating spatial-temporal intelligence, allowing it to plan and execute actions in real-world scenarios. The model is pretrained on diverse datasets, utilizing Set-of-Mark (SoM) for action grounding and Trace-of-Mark (ToM) for action planning, which together improve its ability to understand and interact with visual-spatial elements. Magma achieves state-of-the-art performance in UI navigation and robotic manipulation, surpassing specialized models and competing effectively with larger multimodal models.'}, 'zh': {'title': 'MagmaÔºöÂ§öÊ®°ÊÄÅÊô∫ËÉΩÁöÑÊú™Êù•', 'desc': 'MagmaÊòØ‰∏Ä‰∏™Âü∫Á°ÄÊ®°ÂûãÔºåËÉΩÂ§üÂ§ÑÁêÜÊï∞Â≠óÂíåÁâ©ÁêÜ‰∏ñÁïå‰∏≠ÁöÑÂ§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩ‰ªªÂä°„ÄÇÂÆÉ‰∏ç‰ªÖ‰øùÁïô‰∫ÜËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁêÜËß£ËÉΩÂäõÔºåËøòÂÖ∑Â§áÂú®ËßÜËßâÁ©∫Èó¥‰∏≠ËßÑÂàíÂíåË°åÂä®ÁöÑËÉΩÂäõ„ÄÇMagmaÈÄöËøáÂ§ßÈáèÂºÇÊûÑÊï∞ÊçÆÈõÜËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåËøô‰∫õÊï∞ÊçÆÈõÜÂåÖÊã¨ÂõæÂÉè„ÄÅËßÜÈ¢ëÂíåÊú∫Âô®‰∫∫Êï∞ÊçÆÔºå‰ΩøÁî®Set-of-MarkÂíåTrace-of-MarkËøõË°åÂä®‰ΩúÊ†áÂÆö„ÄÇÂÆûÈ™åË°®ÊòéÔºåMagmaÂú®Áî®Êà∑ÁïåÈù¢ÂØºËà™ÂíåÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏äÂàõÈÄ†‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁªìÊûúÔºåË∂ÖË∂ä‰∫Ü‰∏ìÈó®‰∏∫Ëøô‰∫õ‰ªªÂä°ËÆæËÆ°ÁöÑÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12464', 'title': 'SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models', 'url': 'https://huggingface.co/papers/2502.12464', 'abstract': 'Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on "hard" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model\'s capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.', 'score': 25, 'issue_id': 2288, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '5edcf5af6c8edecb', 'authors': ['Seanie Lee', 'Dong Bok Lee', 'Dominik Wagner', 'Minki Kang', 'Haebin Seong', 'Tobias Bocklet', 'Juho Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'Technische Hochschule N√ºrnberg Georg Simon Ohm'], 'pdf_title_img': 'assets/pdf/title_img/2502.12464.jpg', 'data': {'categories': ['#security', '#benchmark', '#training', '#inference', '#optimization'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∑–∞—â–∏—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ SafeRoute –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). SafeRoute –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ '–ø—Ä–æ—Å—Ç—ã–µ' –∏ '—Å–ª–æ–∂–Ω—ã–µ'. –°–ª–æ–∂–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª—å—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∞ –ø—Ä–æ—Å—Ç—ã–µ - –º–µ–Ω—å—à–µ–π –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –∫—Ä—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ SafeRoute –ø–µ—Ä–µ–¥ –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."}, 'en': {'title': 'Smart Routing for Safer AI: Balancing Efficiency and Accuracy', 'desc': "This paper introduces SafeRoute, a binary router designed to improve the efficiency of safety guard models used with large language models (LLMs). The router identifies which inputs are 'hard' and require the computational power of a larger safety model, while 'easy' inputs can be handled by smaller, more efficient models. By selectively applying the larger model only to challenging cases, SafeRoute reduces overall computational costs without sacrificing safety performance. Experimental results show that this adaptive approach significantly enhances the balance between resource usage and accuracy compared to using only the larger model."}, 'zh': {'title': 'Êô∫ËÉΩË∑ØÁî±ÔºåÊèêÂçáÂÆâÂÖ®‰∏éÊïàÁéáÔºÅ', 'desc': 'Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÈÉ®ÁΩ≤Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈúÄË¶ÅÂº∫Â§ßÁöÑÂÆâÂÖ®Èò≤Êä§Ê®°ÂûãÊù•Ê£ÄÊµãÂíåÈòªÊ≠¢ÊúâÂÆ≥ÁöÑÁî®Êà∑ÊèêÁ§∫„ÄÇËôΩÁÑ∂Â§ßÂûãÂÆâÂÖ®Èò≤Êä§Ê®°ÂûãÁöÑÊÄßËÉΩÂæàÂº∫Ôºå‰ΩÜÂÖ∂ËÆ°ÁÆóÊàêÊú¨‰πüÂæàÈ´ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÁ†îÁ©∂ËÄÖ‰ª¨‰ΩøÁî®‰∫ÜËæÉÂ∞èÁöÑËí∏È¶èÊ®°ÂûãÔºå‰ΩÜÂú®Â§ÑÁêÜ‚ÄúÂõ∞Èöæ‚ÄùÁ§∫‰æãÊó∂ÔºåÂÆÉ‰ª¨ÁöÑË°®Áé∞ÂæÄÂæÄ‰∏çÂ¶ÇÂ§ßÂûãÊ®°Âûã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜSafeRouteÔºå‰∏Ä‰∏™‰∫åÂÖÉË∑ØÁî±Âô®ÔºåÂèØ‰ª•Âå∫ÂàÜÂõ∞ÈöæÁ§∫‰æãÂíåÁÆÄÂçïÁ§∫‰æãÔºå‰ªéËÄåÊèêÈ´òÊïàÁéáÔºåÂêåÊó∂‰øùÊåÅÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09245', 'title': "You Do Not Fully Utilize Transformer's Representation Capacity", 'url': 'https://huggingface.co/papers/2502.09245', 'abstract': "In contrast to RNNs, which compress previous tokens into a single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation collapse and leads to suboptimal performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that preserves the model's overall memory footprint while expanding its representational capacity by allowing access to hidden states from earlier layers. Through extensive experiments across various architectures and different lookup mechanisms, we demonstrate consistent performance improvements on a wide range of tasks. Moreover, our analysis of the learned representation dynamics and our exploration of depthwise circuits reveal how LIMe integrates information across layers, pointing to promising directions for future research.", 'score': 24, 'issue_id': 2291, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '8526a2bbb83d3754', 'authors': ['Gleb Gerasimov', 'Yaroslav Aksenov', 'Nikita Balagansky', 'Viacheslav Sinii', 'Daniil Gavrilov'], 'affiliations': ['HSE University', 'Moscow Institute of Physics and Technology', 'T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.09245.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'üß†', 'ru': {'title': 'LIMe: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Layer-Integrated Memory (LIMe). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è, LIMe –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫ —Å–∫—Ä—ã—Ç—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º –∏–∑ –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏—Ö —Å–ª–æ–µ–≤. –≠—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ LIMe –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –∑–∞–¥–∞—á –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –æ–±—â–µ–≥–æ –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Unlocking Transformer Potential with Layer-Integrated Memory', 'desc': "This paper discusses the limitations of standard Transformers, which only utilize information from the most recent layer, leading to representation collapse and reduced performance. The authors propose a new method called Layer-Integrated Memory (LIMe) that allows access to hidden states from earlier layers, enhancing the model's representational capacity without increasing memory usage. Through various experiments, they show that LIMe consistently improves performance across different tasks and architectures. Additionally, the paper explores how LIMe integrates information across layers, suggesting new avenues for future research in model design."}, 'zh': {'title': 'Â±ÇÈõÜÊàêËÆ∞ÂøÜÔºöÊèêÂçáÂèòÊç¢Âô®ÊÄßËÉΩÁöÑÊñ∞ÊñπÊ≥ï', 'desc': '‰∏éÈÄíÂΩíÁ•ûÁªèÁΩëÁªúÔºàRNNÔºâ‰∏çÂêåÔºåÂèòÊç¢Âô®ÔºàTransformersÔºâÂèØ‰ª•Áõ¥Êé•ÂÖ≥Ê≥®ÊâÄÊúâ‰πãÂâçÁöÑÊ†áËÆ∞„ÄÇÁÑ∂ËÄåÔºåÊ†áÂáÜÁöÑÂèòÊç¢Âô®‰ªÖ‰ΩøÁî®Êù•Ëá™Ââç‰∏ÄÂ±ÇÁöÑË°®Á§∫ÔºåËøôÁßçËÆæËÆ°ÈÄâÊã©ÂØºËá¥‰∫ÜË°®Á§∫Â¥©Ê∫ÉÔºå‰ªéËÄåÂΩ±Âìç‰∫ÜÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â±ÇÈõÜÊàêËÆ∞ÂøÜÔºàLIMeÔºâÁöÑÊñπÊ≥ïÔºåÂÆÉÂú®‰øùÊåÅÊ®°ÂûãÊï¥‰ΩìÂÜÖÂ≠òÂç†Áî®ÁöÑÂêåÊó∂ÔºåÊâ©Â±ï‰∫ÜË°®Á§∫ËÉΩÂäõÔºåÂÖÅËÆ∏ËÆøÈóÆÊó©ÊúüÂ±ÇÁöÑÈöêËóèÁä∂ÊÄÅ„ÄÇÈÄöËøáÂú®ÂêÑÁßçÊû∂ÊûÑÂíå‰∏çÂêåÊü•ÊâæÊú∫Âà∂‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂú®Â§öÁßç‰ªªÂä°‰∏äÁöÑ‰∏ÄËá¥ÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11433', 'title': 'FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading', 'url': 'https://huggingface.co/papers/2502.11433', 'abstract': 'Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-Trader, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.', 'score': 22, 'issue_id': 2286, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '0731051fe5131889', 'authors': ['Guojun Xiong', 'Zhiyang Deng', 'Keyi Wang', 'Yupeng Cao', 'Haohang Li', 'Yangyang Yu', 'Xueqing Peng', 'Mingquan Lin', 'Kaleb E Smith', 'Xiao-Yang Liu', 'Jimin Huang', 'Sophia Ananiadou', 'Qianqian Xie'], 'affiliations': ['Columbia University', 'Harvard University', 'NVIDIA', 'Rensselaer Polytechnic Institute', 'Stevens Institute of Technology', 'TheFinAI', 'University of Manchester', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2502.11433.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#architecture', '#training', '#reasoning', '#rl', '#multimodal'], 'emoji': 'üìà', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ FLAG-Trader, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —è–∑—ã–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π. LLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–ª–∏—Ç–∏–∫–∏, –∞–¥–∞–ø—Ç–∏—Ä—É—è—Å—å –∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ–±–ª–∞—Å—Ç–∏ —á–µ—Ä–µ–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –¥–æ–æ–±—É—á–µ–Ω–∏–µ. –ú–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–æ—Ä–≥–æ–≤–ª—è. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –Ω–µ —Ç–æ–ª—å–∫–æ –≤ —Ç–æ—Ä–≥–æ–≤–ª–µ, –Ω–æ –∏ –≤ –¥—Ä—É–≥–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Empowering Financial Trading with FLAG-Trader: Merging Language and Reinforcement Learning', 'desc': 'This paper introduces FLAG-Trader, a new architecture that combines large language models (LLMs) with reinforcement learning (RL) to improve decision-making in financial trading. The approach uses a partially fine-tuned LLM as a policy network, allowing it to utilize its pre-trained knowledge while adapting specifically to financial tasks. By applying policy gradient optimization based on trading rewards, FLAG-Trader enhances the performance of LLMs in trading scenarios and other financial applications. The authors provide extensive empirical evidence demonstrating the effectiveness of their proposed method.'}, 'zh': {'title': 'FLAG-TraderÔºöÊèêÂçáÈáëËûçÂÜ≥Á≠ñÁöÑÊô∫ËÉΩ‰∫§ÊòìÊû∂ÊûÑ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫FLAG-TraderÁöÑÁªü‰∏ÄÊû∂ÊûÑÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÈáëËûçÂ∏ÇÂú∫‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõ„ÄÇËØ•Êû∂ÊûÑÁªìÂêà‰∫ÜËØ≠Ë®ÄÂ§ÑÁêÜÂíåÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁ≠ñÁï•‰ºòÂåñÔºå‰ΩøÂæóÈÉ®ÂàÜÂæÆË∞ÉÁöÑLLMÂèØ‰ª•‰Ωú‰∏∫Á≠ñÁï•ÁΩëÁªúÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁü•ËØÜÂπ∂ÈÄÇÂ∫îÈáëËûçÈ¢ÜÂüü„ÄÇÈÄöËøá‰∫§ÊòìÂ•ñÂä±È©±Âä®ÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶‰ºòÂåñÔºåFLAG-Trader‰∏ç‰ªÖÊèêÈ´ò‰∫ÜLLMÂú®‰∫§Êòì‰∏≠ÁöÑË°®Áé∞ÔºåËøòÊîπÂñÑ‰∫ÜÂÖ∂‰ªñÈáëËûçÈ¢ÜÂüü‰ªªÂä°ÁöÑÁªìÊûú„ÄÇÊàë‰ª¨Êèê‰æõ‰∫ÜÂ§ßÈáèÂÆûËØÅËØÅÊçÆÊù•È™åËØÅËøô‰∫õÊîπËøõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12513', 'title': 'RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm', 'url': 'https://huggingface.co/papers/2502.12513', 'abstract': 'After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn.', 'score': 14, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '1ed14365f683281c', 'authors': ['Tiancheng Gu', 'Kaicheng Yang', 'Chaoyi Zhang', 'Yin Xie', 'Xiang An', 'Ziyong Feng', 'Dongnan Liu', 'Weidong Cai', 'Jiankang Deng'], 'affiliations': ['DeepGlint', 'Imperial College London', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2502.12513.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#data', '#synthetic', '#dataset', '#multimodal'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'RealSyn: –£–ª—É—á—à–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ RealSyn, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–µ–ø–∞—Ä–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω–∏ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ RealSyn, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.'}, 'en': {'title': 'Unlocking Vision-Language Learning with RealSyn Dataset', 'desc': 'This paper introduces RealSyn, a new dataset designed to improve vision-language representation learning by utilizing both realistic and synthetic texts. The authors develop a data extraction pipeline to gather high-quality images and texts from unpaired multimodal documents. They also implement a hierarchical retrieval method to link images with relevant texts and propose an image semantic augmented generation module to create synthetic text. The resulting dataset, RealSyn, shows significant improvements in model performance on various tasks, demonstrating its effectiveness and scalability in the field of machine learning.'}, 'zh': {'title': 'Âà©Áî®Êú™ÈÖçÂØπÊï∞ÊçÆÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÂ≠¶‰π†ÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÈõÜRealSynÔºåÁî®‰∫éËßÜËßâ-ËØ≠Ë®ÄË°®Á§∫Â≠¶‰π†„ÄÇÁ†îÁ©∂ËÄÖ‰ª¨ÈÄöËøáÊèêÂèñÈ´òË¥®ÈáèÁöÑÂõæÂÉèÂíåÊñáÊú¨ÔºåÂà©Áî®Êú™ÈÖçÂØπÁöÑÊï∞ÊçÆÊù•ÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞ÂÖ≥ËÅîÂõæÂÉèÂíåÊñáÊú¨ÔºåËÆæËÆ°‰∫Ü‰∏ÄÁßçÂ±ÇÊ¨°Ê£ÄÁ¥¢ÊñπÊ≥ïÔºåÂπ∂ÊèêÂá∫‰∫ÜÂõæÂÉèËØ≠‰πâÂ¢ûÂº∫ÁîüÊàêÊ®°ÂùóÊù•ÁîüÊàêÂêàÊàêÊñáÊú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éRealSynÈ¢ÑËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11271', 'title': 'OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning', 'url': 'https://huggingface.co/papers/2502.11271', 'abstract': "Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools' generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.", 'score': 10, 'issue_id': 2291, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': '1477460cf6641c67', 'authors': ['Pan Lu', 'Bowen Chen', 'Sheng Liu', 'Rahul Thapa', 'Joseph Boen', 'James Zou'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11271.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal', '#open_source', '#training'], 'emoji': 'üêô', 'ru': {'title': 'OctoTools: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OctoTools - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. OctoTools –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –∏ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –∏ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 9.3% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å GPT-4 –Ω–∞ 16 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. OctoTools —Ç–∞–∫–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –ø–æ–¥—Ö–æ–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ AutoGen –∏ LangChain, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –∑–∞–¥–∞—á –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'OctoTools: Empowering LLMs for Complex Reasoning Tasks', 'desc': 'This paper presents OctoTools, an innovative framework designed to enhance large language models (LLMs) in solving complex reasoning tasks without the need for additional training. OctoTools features standardized tool cards that define the capabilities of various tools, a planner for organizing tasks, and an executor to implement the planned actions. The framework has been tested across 16 different tasks, showing an impressive average accuracy improvement of 9.3% compared to GPT-4o. Additionally, OctoTools outperforms other existing methods like AutoGen and LangChain by up to 10.6%, highlighting its effectiveness in multi-step reasoning and tool utilization.'}, 'zh': {'title': 'OctoToolsÔºöË∑®È¢ÜÂüüÂ§çÊùÇÊé®ÁêÜÁöÑÊñ∞Ëß£ÂÜ≥ÊñπÊ°à', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫OctoToolsÁöÑÂºÄÊ∫êÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Â§çÊùÇÊé®ÁêÜ‰ªªÂä°„ÄÇOctoTools‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÁî®Êà∑ÂèãÂ•Ω‰∏îÊòì‰∫éÊâ©Â±ïÔºåËÉΩÂ§üÂú®Â§ö‰∏™È¢ÜÂüü‰∏≠Â∫îÁî®„ÄÇÂÆÉÈÄöËøáÊ†áÂáÜÂåñÁöÑÂ∑•ÂÖ∑Âç°ÁâáÊù•Â∞ÅË£ÖÂ∑•ÂÖ∑ÂäüËÉΩÔºåÂπ∂Êèê‰æõÈ´òÂ±ÇÊ¨°Âíå‰ΩéÂ±ÇÊ¨°ÁöÑËßÑÂàíÂô®‰ª•ÂèäÊâßË°åÂô®Êù•ÊâßË°åÂ∑•ÂÖ∑‰ΩøÁî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOctoToolsÂú®16‰∏™‰∏çÂêå‰ªªÂä°‰∏äÁõ∏ËæÉ‰∫éGPT-4oÂπ≥ÂùáÊèêÈ´ò‰∫Ü9.3%ÁöÑÂáÜÁ°ÆÁéáÔºåÂπ∂Âú®Áõ∏ÂêåÂ∑•ÂÖ∑ÈõÜ‰∏ãË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12859', 'title': 'PAFT: Prompt-Agnostic Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.12859', 'abstract': 'While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. PAFT operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs. Extensive experiments across diverse datasets and LLMs demonstrate that models trained with PAFT exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of PAFT.', 'score': 10, 'issue_id': 2290, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': 'f5398572cb7bbb47', 'authors': ['Chenxing Wei', 'Yao Shu', 'Mingwen Ou', 'Ying Tiffany He', 'Fei Richard Yu'], 'affiliations': ['College of Computer Science and Software Engineering, Shenzhen University, China', 'Guangdong Lab of AI and Digital Economy (SZ), China', 'School of Information Technology, Carleton University, Canada', 'Tsinghua Shenzhen International Graduate School, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12859.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#synthetic', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø—Ä–æ–º–ø—Ç–∞–º: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Prompt-Agnostic Fine-Tuning (PAFT). PAFT —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–Ω–∏–∂–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫—É –ø—Ä–æ–º–ø—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –∏–∑ –Ω–∏—Ö –≤–æ –≤—Ä–µ–º—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PAFT –ø–æ–≤—ã—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–æ–º–ø—Ç–∞–º, —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –≤—ã–≤–æ–¥–∞.'}, 'en': {'title': 'Enhancing Robustness in LLMs with Dynamic Prompting', 'desc': 'This paper introduces Prompt-Agnostic Fine-Tuning (PAFT), a method designed to improve the robustness of Large Language Models (LLMs) when adapting to various tasks. PAFT works by dynamically adjusting prompts during the fine-tuning process, which helps the model focus on the core principles of the tasks instead of memorizing specific prompt formats. The approach involves creating a diverse set of synthetic prompts and randomly sampling from them during training, leading to better generalization and performance on unseen prompts. Experimental results show that PAFT enhances both the robustness and inference speed of LLMs while keeping training efficient.'}, 'zh': {'title': 'ÊèêÂçáÊ®°ÂûãÈ≤ÅÊ£íÊÄß‰∏éÊ≥õÂåñËÉΩÂäõÁöÑÊèêÁ§∫Êó†ÂÖ≥ÂæÆË∞É', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂæÆË∞ÉÂêéËÉΩÂ§üÂæàÂ•ΩÂú∞ÈÄÇÂ∫î‰∏ãÊ∏∏‰ªªÂä°Ôºå‰ΩÜËøôÁßçÈÄÇÂ∫îÊÄßÂæÄÂæÄ‰ºöÂΩ±ÂìçÊèêÁ§∫ÁöÑÈ≤ÅÊ£íÊÄßÔºåÂõ†‰∏∫Âç≥‰ΩøÊòØÂæÆÂ∞èÁöÑÊèêÁ§∫ÂèòÂåñ‰πü‰ºöÊòæËëóÈôç‰ΩéÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ï‚Äî‚ÄîÊèêÁ§∫Êó†ÂÖ≥ÂæÆË∞ÉÔºàPAFTÔºâÔºåËØ•ÊñπÊ≥ïÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠Âä®ÊÄÅË∞ÉÊï¥ÊèêÁ§∫„ÄÇPAFTÁöÑÊìç‰ΩúÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÔºåÊûÑÂª∫‰∏ÄÁªÑÂ§öÊ†∑Âåñ‰∏îÊúâÊÑè‰πâÁöÑÂêàÊàêÂÄôÈÄâÊèêÁ§∫ÔºõÂÖ∂Ê¨°ÔºåÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠Ôºå‰ªéËøôÁªÑÊèêÁ§∫‰∏≠ÈöèÊú∫ÊäΩÂèñÔºå‰ª•ÂàõÂª∫Âä®ÊÄÅËÆ≠ÁªÉËæìÂÖ•„ÄÇÈÄöËøáÂú®Â§öÁßçÊï∞ÊçÆÈõÜÂíåLLMs‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåËØÅÊòé‰∫Ü‰ΩøÁî®PAFTËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®ÂêÑÁßçÊèêÁ§∫ÔºàÂåÖÊã¨Êú™ËßÅËøáÁöÑÊèêÁ§∫Ôºâ‰∏äË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12170', 'title': 'MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections', 'url': 'https://huggingface.co/papers/2502.12170', 'abstract': 'We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .', 'score': 10, 'issue_id': 2287, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'f06bcb1b611b7c39', 'authors': ['Da Xiao', 'Qingye Meng', 'Shengping Li', 'Xingyuan Yuan'], 'affiliations': ['Beijing University of Posts and Telecommunications, Beijing, China', 'ColorfulClouds Technology Co., Ltd., Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12170.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#architecture'], 'emoji': 'üîÄ', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ MUDD (MUltiway Dynamic Dense) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Transformer. MUDD –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–µ—Å–∞ —Å–≤—è–∑–µ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –Ω–∞ –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ö–æ–¥–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –±–ª–æ–∫–∞ Transformer. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MUDDFormer –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Transformer-–º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤ 1.8-2.4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ü—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ MUDDPythia-2.8B —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–∞ –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å Pythia-6.9B –∏ –¥–∞–∂–µ –∫–æ–Ω–∫—É—Ä–∏—Ä—É–µ—Ç —Å Pythia-12B –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –¥–æ–±–∞–≤–ª—è—è –≤—Å–µ–≥–æ 0.23% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ 0.4% –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.'}, 'en': {'title': 'Dynamic Connections for Enhanced Transformer Performance', 'desc': 'The paper introduces MUDD connections, which improve the flow of information between layers in Transformer models. Unlike traditional residual connections that use fixed weights, MUDD connections adaptively generate weights based on the hidden states of the input at each position. This dynamic approach allows for better integration of information from different input streams, enhancing the overall performance of the model. The proposed MUDDFormer architecture shows significant improvements in language modeling tasks, achieving results comparable to larger models while maintaining a smaller parameter count.'}, 'zh': {'title': 'Âä®ÊÄÅËøûÊé•ÔºåÊèêÂçáTransformerÊÄßËÉΩÔºÅ', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§öÂêëÂä®ÊÄÅÁ®†ÂØÜËøûÊé•ÔºàMUDDÔºâÁöÑÁÆÄÂçïÊúâÊïàÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥ÊÆãÂ∑ÆËøûÊé•ÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂Â¢ûÂº∫Transformer‰∏≠Ë∑®Â±Ç‰ø°ÊÅØÊµÅÂä®„ÄÇ‰∏éÁé∞ÊúâÁöÑÈùôÊÄÅÂÖ±‰∫´ËøûÊé•ÊùÉÈáçÁöÑÁ®†ÂØÜËøûÊé•ÊñπÊ≥ï‰∏çÂêåÔºåMUDDÊ†πÊçÆÊØè‰∏™Â∫èÂàó‰ΩçÁΩÆÁöÑÈöêËóèÁä∂ÊÄÅÂä®ÊÄÅÁîüÊàêËøûÊé•ÊùÉÈáçÔºåÂπ∂ÈíàÂØπTransformerÂùóÁöÑÊØè‰∏™Ëß£ËÄ¶ËæìÂÖ•ÊµÅÔºàÊü•ËØ¢„ÄÅÈîÆ„ÄÅÂÄºÊàñÊÆãÂ∑ÆÔºâ„ÄÇMUDDËøûÊé•ÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞‰ªª‰ΩïTransformerÊû∂ÊûÑ‰∏≠ÔºåÂΩ¢ÊàêMUDDFormer„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåMUDDFormerÂú®ËØ≠Ë®ÄÂª∫Ê®°‰∏≠ÊòæËëóË∂ÖË∂ä‰∫ÜÂêÑÁßçÊ®°ÂûãÊû∂ÊûÑÂíåËßÑÊ®°ÁöÑTransformerÔºåË°®Áé∞Âá∫‰∏éËÆ≠ÁªÉÊó∂ËÆ°ÁÆóÈáè‰∏∫1.8X-2.4XÁöÑTransformerÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12215', 'title': 'Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?', 'url': 'https://huggingface.co/papers/2502.12215', 'abstract': "The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.", 'score': 9, 'issue_id': 2288, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': 'a02df3e32ba854de', 'authors': ['Zhiyuan Zeng', 'Qinyuan Cheng', 'Zhangyue Yin', 'Yunhua Zhou', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University, Shanghai, China', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2502.12215.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#optimization'], 'emoji': 'üîç', 'ru': {'title': '–ö–æ—Ä–æ—á–µ - –ª—É—á—à–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–µ –≤—Å–µ–≥–¥–∞ –ø–æ–≤—ã—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å, –∏ —á–∞—Å—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –∫–æ—Ä–æ—á–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–µ–π –∫ —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç —É—Ö—É–¥—à–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Shortest Majority Vote, —Å–æ—á–µ—Ç–∞—é—â–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏ –¥–ª–∏–Ω—ã —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing LLM Performance with Shorter, Smarter Reasoning', 'desc': 'This paper investigates the concept of test-time scaling in large language models (LLMs), particularly focusing on the o1 series by OpenAI and its successors. It reveals that longer chains of thought (CoTs) do not always lead to better accuracy, as shorter CoTs can yield correct answers more frequently. The study highlights that the presence of self-revisions in longer CoTs can negatively impact performance. To enhance test-time scalability, the authors propose a new method called Shortest Majority Vote, which integrates parallel scaling strategies with CoT length characteristics, outperforming traditional majority voting methods.'}, 'zh': {'title': 'ÊèêÂçáÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜÊó∂ÁöÑÊµãËØïÊó∂Èó¥Áº©ÊîæËÉΩÂäõÔºåÁâπÂà´ÊòØOpenAIÁöÑo1Á≥ªÂàó„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåËôΩÁÑ∂‰∏Ä‰∫õÂêéÁª≠Ê®°ÂûãÂ¶ÇQwQÂíåDeepseek-R1Ê®°‰ªø‰∫ÜËøô‰∫õËøõÂ±ïÔºå‰ΩÜÂÆÉ‰ª¨ÁöÑÊµãËØïÊó∂Èó¥Áº©ÊîæËÉΩÂäõ‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÈ™åËØÅ„ÄÇÊõ¥ÈïøÁöÑÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÂπ∂‰∏çÊÄªÊòØÊèêÈ´òÂáÜÁ°ÆÊÄßÔºåÂèçËÄåÊ≠£Á°ÆÁ≠îÊ°àÂæÄÂæÄÊØîÈîôËØØÁ≠îÊ°àÊõ¥Áü≠„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ï‚Äî‚ÄîÊúÄÁü≠Â§öÊï∞ÊäïÁ•®ÔºåÁªìÂêàÂπ∂Ë°åÁº©ÊîæÁ≠ñÁï•ÂíåCoTÈïøÂ∫¶ÁâπÂæÅÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊµãËØïÊó∂Èó¥ÂèØÊâ©Â±ïÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13092', 'title': 'Text2World: Benchmarking Large Language Models for Symbolic World Model Generation', 'url': 'https://huggingface.co/papers/2502.13092', 'abstract': 'Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at https://text-to-world.github.io/.', 'score': 8, 'issue_id': 2296, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': 'f5365c5f8afa57df', 'authors': ['Mengkang Hu', 'Tianxing Chen', 'Yude Zou', 'Yuheng Lei', 'Qiguang Chen', 'Ming Li', 'Hongyuan Zhang', 'Wenqi Shao', 'Ping Luo'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Shenzhen University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.13092.jpg', 'data': {'categories': ['#rl', '#games', '#reasoning', '#benchmark'], 'emoji': 'üåç', 'ru': {'title': 'Text2World: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–∏—Ä–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Text2World –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —è–∑—ã–∫–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–º–µ–Ω–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (PDDL) –∏ –≤–∫–ª—é—á–∞–µ—Ç —Å–æ—Ç–Ω–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM —Å –ø–æ–º–æ—â—å—é Text2World –∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –¥—Ä—É–≥–∏–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∏—Ä–∞ –¥–ª—è LLM.'}, 'en': {'title': 'Enhancing World Modeling with Text2World Benchmark', 'desc': 'This paper discusses the use of large language models (LLMs) to create symbolic representations of worlds from text descriptions. It identifies challenges in previous research, such as evaluation randomness and limited domain coverage. To overcome these issues, the authors introduce a new benchmark called Text2World, which uses planning domain definition language (PDDL) and offers a variety of evaluation metrics. The study finds that while LLMs trained with reinforcement learning show promise, they still struggle with world modeling, prompting the exploration of strategies to improve their capabilities.'}, 'zh': {'title': 'Âà©Áî®LLMsÊèêÂçá‰∏ñÁïåÂª∫Ê®°ËÉΩÂäõÁöÑÊé¢Á¥¢', 'desc': 'ÊúÄËøëÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰ªéÊñáÊú¨ÊèèËø∞ÁîüÊàêÁ¨¶Âè∑‰∏ñÁïåÊ®°ÂûãÁöÑÂÖ¥Ë∂£Êó•ÁõäÂ¢ûÈïø„ÄÇÂ∞ΩÁÆ°Âú®‰∏ñÁïåÂª∫Ê®°ÊñπÈù¢ÂØπLLMsËøõË°å‰∫ÜÂπøÊ≥õÁ†îÁ©∂Ôºå‰ΩÜ‰πãÂâçÁöÑÁ†îÁ©∂Èù¢‰∏¥ËØÑ‰º∞ÈöèÊú∫ÊÄß„ÄÅ‰æùËµñÈó¥Êé•ÊåáÊ†áÂíåÈ¢ÜÂüüËåÉÂõ¥ÊúâÈôêÁ≠âÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïText2WorldÔºåÂü∫‰∫éËßÑÂàíÈ¢ÜÂüüÂÆö‰πâËØ≠Ë®ÄÔºàPDDLÔºâÔºåÊ∂µÁõñÊï∞Áôæ‰∏™Â§öÊ†∑ÂåñÁöÑÈ¢ÜÂüüÔºåÂπ∂ÈááÁî®Â§öÊ†áÂáÜ„ÄÅÂü∫‰∫éÊâßË°åÁöÑÊåáÊ†áËøõË°åÊõ¥Á®≥ÂÅ•ÁöÑËØÑ‰º∞„ÄÇÊàë‰ª¨‰ΩøÁî®Text2WorldÂØπÂΩìÂâçÁöÑLLMsËøõË°åÂü∫ÂáÜÊµãËØïÔºåÂèëÁé∞ÁªèËøáÂ§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÊé®ÁêÜÊ®°ÂûãË°®Áé∞‰ºò‰∫éÂÖ∂‰ªñÊ®°ÂûãÔºå‰ΩÜÂç≥‰ΩøÊòØË°®Áé∞ÊúÄÂ•ΩÁöÑÊ®°ÂûãÂú®‰∏ñÁïåÂª∫Ê®°ËÉΩÂäõ‰∏ä‰ªçÁÑ∂ÊúâÈôê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12996', 'title': 'Eager Updates For Overlapped Communication and Computation in DiLoCo', 'url': 'https://huggingface.co/papers/2502.12996', 'abstract': 'Distributed optimization methods such as DiLoCo have been shown to be effective in training very large models across multiple distributed workers, such as datacenters. These methods split updates into two parts: an inner optimization phase, where the workers independently execute multiple optimization steps on their own local data, and an outer optimization step, where the inner updates are synchronized. While such approaches require orders of magnitude less communication than standard data-parallel training, in settings where the workers are datacenters, even the limited communication requirements of these approaches can still cause significant slow downs due to the blocking necessary at each outer optimization step. In this paper, we investigate techniques to mitigate this issue by overlapping communication with computation in a manner that allows the outer optimization step to fully overlap with the inner optimization phase. We show that a particular variant, dubbed eager updates, provides competitive performance with standard DiLoCo in settings with low bandwidth between workers.', 'score': 7, 'issue_id': 2295, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '61c2807bd3fe3a67', 'authors': ['Satyen Kale', 'Arthur Douillard', 'Yanislav Donchev'], 'affiliations': ['Apple', 'Google DeepMind', 'Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.12996.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π', 'desc': "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —É–∑–ª–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –¥–∞—Ç–∞-—Ü–µ–Ω—Ç—Ä—ã. –û–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ DiLoCo, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Ñ–∞–∑—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤–Ω–µ—à–Ω–∏–π —à–∞–≥ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–µ—Ö–Ω–∏–∫—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'eager updates' –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏—Ç—å –≤–Ω–µ—à–Ω–∏–π —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ñ–∞–∑–æ–π. –ü–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º DiLoCo –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–∏–∑–∫–æ–π –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —É–∑–ª–∞–º–∏."}, 'en': {'title': 'Enhancing Distributed Training with Eager Updates', 'desc': "This paper explores distributed optimization methods, specifically DiLoCo, which are used to train large machine learning models across multiple workers in datacenters. The process involves an inner optimization phase where workers perform local updates and an outer phase for synchronizing these updates. However, the communication required during the outer phase can slow down training, even with reduced communication needs. The authors propose a solution that overlaps communication with computation, introducing 'eager updates' to enhance performance in low bandwidth scenarios while maintaining competitive results with traditional DiLoCo methods."}, 'zh': {'title': 'ÈáçÂè†ÈÄö‰ø°‰∏éËÆ°ÁÆóÔºåÊèêÂçáÂàÜÂ∏ÉÂºè‰ºòÂåñÊïàÁéá', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÂàÜÂ∏ÉÂºè‰ºòÂåñÊñπÊ≥ïÔºåÁß∞‰∏∫DiLoCoÔºåÈÄÇÁî®‰∫éÂú®Â§ö‰∏™ÂàÜÂ∏ÉÂºèÂ∑•‰ΩúËÄÖÔºàÂ¶ÇÊï∞ÊçÆ‰∏≠ÂøÉÔºâ‰∏äËÆ≠ÁªÉÂ§ßÂûãÊ®°Âûã„ÄÇËøôÁßçÊñπÊ≥ïÂ∞ÜÊõ¥Êñ∞ÂàÜ‰∏∫‰∏§‰∏™ÈÉ®ÂàÜÔºöÂÜÖÈÉ®‰ºòÂåñÈò∂ÊÆµÂíåÂ§ñÈÉ®‰ºòÂåñÈò∂ÊÆµÔºåÂ∑•‰ΩúËÄÖÂú®Êú¨Âú∞Êï∞ÊçÆ‰∏äÁã¨Á´ãÊâßË°åÂ§ö‰∏™‰ºòÂåñÊ≠•È™§„ÄÇÂ∞ΩÁÆ°ËøôÁßçÊñπÊ≥ïÊØîÊ†áÂáÜÁöÑÊï∞ÊçÆÂπ∂Ë°åËÆ≠ÁªÉÈúÄË¶ÅÊõ¥Â∞ëÁöÑÈÄö‰ø°Ôºå‰ΩÜÂú®Êï∞ÊçÆ‰∏≠ÂøÉÁéØÂ¢É‰∏≠ÔºåÂ§ñÈÉ®‰ºòÂåñÊ≠•È™§ÁöÑÈòªÂ°û‰ªçÂèØËÉΩÂØºËá¥ÊòæËëóÁöÑÂª∂Ëøü„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊäÄÊúØÔºåÈÄöËøáÈáçÂè†ÈÄö‰ø°‰∏éËÆ°ÁÆóÔºå‰ΩøÂ§ñÈÉ®‰ºòÂåñÊ≠•È™§‰∏éÂÜÖÈÉ®‰ºòÂåñÈò∂ÊÆµÂÆåÂÖ®ÈáçÂè†Ôºå‰ªéËÄåÊèêÈ´òÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09838', 'title': 'HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation', 'url': 'https://huggingface.co/papers/2502.09838', 'abstract': 'We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.', 'score': 7, 'issue_id': 2287, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': 'ce920c7d40d11dad', 'authors': ['Tianwei Lin', 'Wenqiao Zhang', 'Sijing Li', 'Yuqian Yuan', 'Binhe Yu', 'Haoyuan Li', 'Wanggui He', 'Hao Jiang', 'Mengze Li', 'Xiaohui Song', 'Siliang Tang', 'Jun Xiao', 'Hui Lin', 'Yueting Zhuang', 'Beng Chin Ooi'], 'affiliations': ['Alibaba', 'National University of Singapore', 'The Hong Kong University of Science and Technology', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09838.jpg', 'data': {'categories': ['#data', '#training', '#cv', '#dataset', '#healthcare'], 'emoji': 'üè•', 'ru': {'title': 'HealthGPT: –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'HealthGPT - —ç—Ç–æ –º–æ—â–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–∏–∫—É –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ H-LoRA –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö VL-Health. HealthGPT –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Revolutionizing Medical AI with HealthGPT', 'desc': 'HealthGPT is a Medical Large Vision-Language Model that combines understanding and generating medical images and text in one system. It uses a unique method called heterogeneous low-rank adaptation (H-LoRA) to enhance pre-trained large language models with diverse medical knowledge. The model is trained on a specialized dataset named VL-Health, which focuses on medical comprehension and generation tasks. Results show that HealthGPT performs exceptionally well in various medical visual tasks, demonstrating its effectiveness and scalability.'}, 'zh': {'title': 'HealthGPTÔºöÂåªÁñóËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÊñ∞Êï¥Âêà', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫ÜHealthGPTÔºåËøôÊòØ‰∏ÄÁßçÂº∫Â§ßÁöÑÂåªÁñóÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàMed-LVLMÔºâÔºåÂÆÉÂ∞ÜÂåªÁñóËßÜËßâÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõÊï¥ÂêàÂú®‰∏Ä‰∏™Áªü‰∏ÄÁöÑËá™ÂõûÂΩíÊ°ÜÊû∂‰∏≠„ÄÇÊàë‰ª¨ÁöÑËá™‰∏æÁêÜÂøµÊòØÈÄêÊ≠•Â∞ÜÂºÇÊûÑÁöÑÁêÜËß£ÂíåÁîüÊàêÁü•ËØÜÈÄÇÂ∫î‰∫éÈ¢ÑËÆ≠ÁªÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ„ÄÇËøôÈÄöËøá‰∏ÄÁßçÊñ∞È¢ñÁöÑÂºÇÊûÑ‰ΩéÁß©ÈÄÇÂ∫îÔºàH-LoRAÔºâÊäÄÊúØÂÆûÁé∞ÔºåÂπ∂ËæÖ‰ª•ÂÆöÂà∂ÁöÑÂàÜÂ±ÇËßÜËßâÊÑüÁü•ÊñπÊ≥ïÂíå‰∏âÈò∂ÊÆµÂ≠¶‰π†Á≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHealthGPTÂú®ÂåªÁñóËßÜËßâÁªü‰∏Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12018', 'title': 'Atom of Thoughts for Markov LLM Test-Time Scaling', 'url': 'https://huggingface.co/papers/2502.12018', 'abstract': 'Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom.', 'score': 6, 'issue_id': 2295, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': 'acefee7f3111548b', 'authors': ['Fengwei Teng', 'Zhaoyang Yu', 'Quan Shi', 'Jiayi Zhang', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['DeepWisdom', 'Renmin University of China', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.12018.jpg', 'data': {'categories': ['#training', '#benchmark', '#reasoning', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–ê—Ç–æ–º–∞—Ä–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Atom of Thoughts' (AoT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. AoT —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –ø–æ–¥–≤–æ–ø—Ä–æ—Å—ã, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∞—Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –∏ –∏–∑–±–µ–≥–∞—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—è –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —à–µ—Å—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å AoT –∫–∞–∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –∏ –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫ –¥—Ä—É–≥–∏–º –º–µ—Ç–æ–¥–∞–º."}, 'en': {'title': 'Enhancing Reasoning in LLMs with Atomic Question Decomposition', 'desc': 'This paper introduces Atom of Thoughts (AoT), a method designed to improve reasoning in Large Language Models (LLMs) during inference. AoT addresses the problem of accumulated historical information in existing test-time scaling methods, which can hinder effective reasoning and waste computational resources. By breaking down complex questions into independent subquestions, AoT allows for a more efficient reasoning process that resembles memoryless transitions in a Markov process. The proposed method not only enhances reasoning capabilities but also integrates well with existing frameworks, showing significant performance improvements in benchmark tests.'}, 'zh': {'title': 'ÊÄùÁª¥ÂéüÂ≠êÁöÑÂäõÈáèÔºöÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈÄöËøáÊâ©Â±ïËÆ≠ÁªÉËßÑÊ®°ÂíåÊµãËØïËßÑÊ®°Êù•ÊèêÈ´òÊÄßËÉΩ„ÄÇÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÔºåÁé∞ÊúâÁöÑÊµãËØïÊó∂Èó¥Êâ©Â±ïÊñπÊ≥ïÁî±‰∫éÂéÜÂè≤‰ø°ÊÅØÁöÑÁ¥ØÁßØÔºåÂØºËá¥ËÆ°ÁÆóËµÑÊ∫êÊµ™Ë¥πÂíåÊé®ÁêÜÊïàÊûúÂπ≤Êâ∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‚ÄúÊÄùÁª¥ÂéüÂ≠ê‚ÄùÔºàAtom of ThoughtsÔºåAoTÔºâÔºåÈÄöËøáÂ∞ÜÂΩìÂâçÈóÆÈ¢òÂàÜËß£‰∏∫‰æùËµñÂÖ≥Á≥ªÁöÑÊúâÂêëÊó†ÁéØÂõæÔºåÂπ∂Êî∂Áº©ÂÖ∂Â≠êÈóÆÈ¢òÔºåÂΩ¢ÊàêÊñ∞ÁöÑÂéüÂ≠êÈóÆÈ¢òÁä∂ÊÄÅ„ÄÇËøôÁßçËø≠‰ª£ÁöÑÂàÜËß£-Êî∂Áº©ËøáÁ®ãÂÆûÁé∞‰∫ÜÈóÆÈ¢òÁä∂ÊÄÅ‰πãÈó¥ÁöÑÈ©¨Â∞îÂèØÂ§´ËΩ¨ÁßªÔºåÂπ∂ÂèØ‰ª•‰∏éÁé∞ÊúâÁöÑÊµãËØïÊó∂Èó¥Êâ©Â±ïÊñπÊ≥ïÊó†ÁºùÈõÜÊàê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12574', 'title': 'HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading', 'url': 'https://huggingface.co/papers/2502.12574', 'abstract': 'Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods.', 'score': 6, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '6e5de7c584198857', 'authors': ['Cheng Luo', 'Zefan Cai', 'Hanshi Sun', 'Jinqi Xiao', 'Bo Yuan', 'Wen Xiao', 'Junjie Hu', 'Jiawei Zhao', 'Beidi Chen', 'Anima Anandkumar'], 'affiliations': ['California Institute of Technology', 'Carnegie Mellon University', 'Microsoft', 'Rutgers University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2502.12574.jpg', 'data': {'categories': ['#optimization', '#architecture', '#inference', '#training', '#long_context'], 'emoji': 'üß†', 'ru': {'title': 'HEADINFER: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º', 'desc': 'HEADINFER - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–≥—Ä—É–∂–∞—Ç—å –∫—ç—à –∫–ª—é—á–µ–π-–∑–Ω–∞—á–µ–Ω–∏–π (KV cache) –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å CPU, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ GPU. –ò—Å–ø–æ–ª—å–∑—É—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è, HEADINFER —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —É–º–µ–Ω—å—à–µ–Ω–∏–∏ –æ–±—ä–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –ø–∞–º—è—Ç–∏. –ú–µ—Ç–æ–¥ –±—ã–ª —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –º–æ–¥–µ–ª–∏ Llama-3-8B —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ–∫—Ä–∞—Ç–∏–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU –Ω–∞ 92% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º –º–µ—Ç–æ–¥–æ–º.'}, 'en': {'title': 'HEADINFER: Efficient Memory Management for Long Contexts in LLMs', 'desc': 'This paper introduces HEADINFER, a novel approach to manage the memory usage of large language models (LLMs) during long context generation. By offloading the key-value (KV) cache to CPU RAM, HEADINFER reduces the reliance on GPU memory, which is crucial for efficient inference. The method employs a fine-grained, head-wise offloading strategy, allowing selective storage of attention heads on the GPU while dynamically computing attention outputs. Evaluation on the Llama-3-8B model shows a remarkable reduction in GPU memory usage, enabling efficient processing of extremely long sequences without compromising performance.'}, 'zh': {'title': 'HEADINFERÔºö‰ºòÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜÖÂ≠ò‰ΩøÁî®', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫HEADINFERÁöÑÊñπÊ≥ïÔºåÊó®Âú®‰ºòÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Èïø‰∏ä‰∏ãÊñáÁîüÊàê‰∏≠ÁöÑÂÜÖÂ≠ò‰ΩøÁî®„ÄÇÈÄöËøáÂ∞ÜÂÖ≥ÈîÆÂÄºÁºìÂ≠òÔºàKVÁºìÂ≠òÔºâËΩ¨ÁßªÂà∞CPU RAMÔºåHEADINFERÈÅøÂÖç‰∫ÜÂú®GPU‰∏äÂÆåÂÖ®Â≠òÂÇ®KVÁºìÂ≠òÁöÑÈúÄÊ±Ç„ÄÇËØ•ÊñπÊ≥ïÈááÁî®ÁªÜÁ≤íÂ∫¶ÁöÑÂ§¥ÈÉ®Á∫ßÂà´Âç∏ËΩΩÁ≠ñÁï•Ôºå‰ªÖÂú®GPU‰∏ä‰øùÁïôÈÄâÊã©ÊÄßÁöÑÊ≥®ÊÑèÂäõÂ§¥KVÁºìÂ≠òÔºåÂêåÊó∂Âä®ÊÄÅËÆ°ÁÆóÊ≥®ÊÑèÂäõËæìÂá∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHEADINFERÂú®ÊòæËëóÂáèÂ∞ëÂÜÖÂ≠òÂç†Áî®ÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫ÜËÆ°ÁÆóÊïàÁéáÔºå‰ΩøÂæóÂú®Âçï‰∏™Ê∂àË¥πÁ∫ßGPU‰∏äÂÆûÁé∞‰∫ÜÂØπÈïøËææ400‰∏áÊ†áËÆ∞ÁöÑÊé®ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12501', 'title': 'Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge', 'url': 'https://huggingface.co/papers/2502.12501', 'abstract': "LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.", 'score': 5, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '1c26233f72f504ee', 'authors': ['Qiyuan Zhang', 'Yufei Wang', 'Yuxin Jiang', 'Liangyou Li', 'Chuhan Wu', 'Yasheng Wang', 'Xin Jiang', 'Lifeng Shang', 'Ruiming Tang', 'Fuyuan Lyu', 'Chen Ma'], 'affiliations': ['City University of Hong Kong', 'Huawei Noahs Ark Lab', 'McGill University & MILA', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.12501.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#inference', '#training', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –ò–ò —á–µ—Ä–µ–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –º–Ω–µ–Ω–∏–µ–º —Ç–æ–ª–ø—ã', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Crowd-based Comparative Evaluation'. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ LLM-as-a-Judge, –¥–æ–±–∞–≤–ª—è—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ—Ç–≤–µ—Ç–∞–º–∏ —Ç–æ–ª–ø—ã –¥–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 6.7% –Ω–∞ –ø—è—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –æ—Ç–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º."}, 'en': {'title': 'Enhancing LLM Evaluations with Crowd Insights', 'desc': 'This paper addresses the limitations of the LLM-as-a-Judge method, which generates chain-of-thought (CoT) judgments for auto-evaluation. The authors identify that CoT reasoning often lacks depth and comprehensiveness, leading to incomplete evaluations. To overcome this, they propose a new method called Crowd-based Comparative Evaluation, which incorporates additional crowd responses to enhance the evaluation process. Their experiments show that this approach improves the reliability of evaluations, increases accuracy by 6.7%, and produces higher-quality CoTs that benefit supervised fine-tuning.'}, 'zh': {'title': 'ÊèêÂçáLLMËØÑ‰º∞ÂèØÈù†ÊÄßÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞ÊñπÊ≥ïÔºåÁß∞‰∏∫Âü∫‰∫é‰∫∫Áæ§ÁöÑÊØîËæÉËØÑ‰º∞ÔºàCrowd-based Comparative EvaluationÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑÂèØÈù†ÊÄß„ÄÇ‰º†ÁªüÁöÑÈìæÂºèÊé®ÁêÜÔºàCoTÔºâÊñπÊ≥ïÂ∏∏Â∏∏Êó†Ê≥ïÊçïÊçâÂà∞ÂÖ®Èù¢ÂíåÊ∑±ÂÖ•ÁöÑÁªÜËäÇÔºåÂØºËá¥ËØÑ‰º∞ÁªìÊûú‰∏çÂÆåÊï¥„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÂºïÂÖ•È¢ùÂ§ñÁöÑ‰∫∫Áæ§ÂèçÈ¶àÔºå‰∏éÂÄôÈÄâÂìçÂ∫îËøõË°åÊØîËæÉÔºå‰ªéËÄåÊè≠Á§∫ÂÄôÈÄâÂìçÂ∫î‰∏≠ÁöÑÊõ¥Ê∑±Â±ÇÊ¨°ÂíåÊõ¥ÂÖ®Èù¢ÁöÑÁªÜËäÇ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰∫î‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Âπ≥ÂùáÊèêÈ´ò‰∫Ü6.7%ÁöÑËØÑ‰º∞ÂáÜÁ°ÆÊÄßÔºåÂπ∂ÁîüÊàê‰∫ÜÊõ¥È´òË¥®ÈáèÁöÑCoTÔºå‰øÉËøõ‰∫ÜÁõëÁù£ÂæÆË∞ÉÁöÑÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12929', 'title': 'Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options', 'url': 'https://huggingface.co/papers/2502.12929', 'abstract': 'We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning.', 'score': 4, 'issue_id': 2296, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '7cde53c7aa78c7ed', 'authors': ['Lakshmi Nair', 'Ian Trase', 'Mark Kim'], 'affiliations': ['Flagship Pioneering, Cambridge, MA 02142, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.12929.jpg', 'data': {'categories': ['#interpretability', '#training', '#rl', '#reasoning', '#cv', '#agents'], 'emoji': 'üß†', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: FoO –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –ò–ò', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Flow-of-Options (FoO), –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). FoO –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å–≤–æ–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ FoO –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (AutoML). –≠—Ç–æ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –¥–æ—Å—Ç–∏–≥–∞—è —É–ª—É—á—à–µ–Ω–∏–π –Ω–∞ 38.2% - 69.2% –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞ 37.4% - 47.9% –≤ –∑–∞–¥–∞—á–∞—Ö —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–æ–π —Ö–∏–º–∏–∏. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω–∏–º–∞ –∫ —à–∏—Ä–æ–∫–æ–º—É —Å–ø–µ–∫—Ç—Ä—É –∑–∞–¥–∞—á, –≤–∫–ª—é—á–∞—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Unlocking Diverse Reasoning in Large Language Models with Flow-of-Options', 'desc': 'The paper introduces a new reasoning method called Flow-of-Options (FoO) that helps Large Language Models (LLMs) overcome their inherent biases. FoO allows these models to explore a wide variety of reasoning paths, which is particularly useful for automating machine learning tasks (AutoML). The proposed framework shows significant performance improvements over existing methods, achieving better results in both data science and therapeutic chemistry tasks while maintaining low operational costs. Additionally, FoO enhances the versatility of LLMs, enabling them to tackle not just classification and regression, but also reinforcement learning and image generation tasks.'}, 'zh': {'title': 'ÈÄâÈ°πÊµÅÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÁöÑÂ§öÊ†∑ÊÄß‰∏éÊÄßËÉΩ', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜÊñπÊ≥ïÔºåÁß∞‰∏∫ÈÄâÈ°πÊµÅÔºàFlow-of-OptionsÔºåFoOÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÂÜÖÂú®ÂÅèËßÅ„ÄÇFoO‰ΩøÂæóLLMsËÉΩÂ§üÁ≥ªÁªüÂú∞Êé¢Á¥¢Â§öÁßçÊé®ÁêÜÂèØËÉΩÊÄßÔºåÁâπÂà´ÊòØÂú®Ëá™Âä®ÂåñÊú∫Âô®Â≠¶‰π†ÔºàAutoMLÔºâ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âú®Ê†áÂáÜÊï∞ÊçÆÁßëÂ≠¶‰ªªÂä°‰∏äÊèêÈ´ò‰∫Ü38.2%Âà∞69.2%ÁöÑÊÄßËÉΩÔºåÂú®Ê≤ªÁñóÂåñÂ≠¶‰ªªÂä°‰∏äÊèêÈ´ò‰∫Ü37.4%Âà∞47.9%„ÄÇÊ≠§Â§ñÔºåFoOËøòÈÄÇÁî®‰∫éÂº∫ÂåñÂ≠¶‰π†ÂíåÂõæÂÉèÁîüÊàêÁ≠âÊõ¥ÂπøÊ≥õÁöÑ‰ªªÂä°ÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âú®Â§öÊ†∑ÊÄßÂíåÂèØËß£ÈáäÊÄßÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10708', 'title': 'Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2502.10708', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks. To keep researchers updated on the latest studies, we maintain an open-source at: https://github.com/abilliyb/Knowledge_Injection_Survey_Papers, dedicated to documenting research in the field of specialized LLM.', 'score': 3, 'issue_id': 2291, 'pub_date': '2025-02-15', 'pub_date_card': {'ru': '15 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 15', 'zh': '2Êúà15Êó•'}, 'hash': '01a93665c8e86d8d', 'authors': ['Zirui Song', 'Bin Yan', 'Yuhan Liu', 'Miao Fang', 'Mingzhe Li', 'Rui Yan', 'Xiuying Chen'], 'affiliations': ['ByteDance', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.10708.jpg', 'data': {'categories': ['#survey', '#dataset', '#healthcare', '#multilingual', '#benchmark', '#machine_translation', '#open_source'], 'emoji': 'üß†', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ LLM –¥–æ–º–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏: –∫–ª—é—á –∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–∞–¥–∞—á–∞–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –ø—É—Ç–µ–º –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –¥–æ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–µ–ª—è—é—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, –º–æ–¥—É–ª—å–Ω—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤. –í —Ä–∞–±–æ—Ç–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞, –∞ —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–æ–º–µ–Ω–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö LLM –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞—é—Ç –≤—ã–∑–æ–≤—ã –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —ç—Ç–æ–π —Ä–∞–∑–≤–∏–≤–∞—é—â–µ–π—Å—è –æ–±–ª–∞—Å—Ç–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Enhancing LLMs with Domain-Specific Knowledge', 'desc': 'This paper surveys methods to enhance Large Language Models (LLMs) for specialized tasks in fields like healthcare and law. It categorizes these methods into four approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each method aims to integrate domain-specific knowledge into LLMs, improving their performance while considering trade-offs in flexibility and efficiency. The paper also evaluates the effectiveness of these specialized LLMs compared to general-purpose models and discusses the challenges and opportunities in this area.'}, 'zh': {'title': 'Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÜÂüüÁü•ËØÜ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£„ÄÅÊñáÊú¨ÊëòË¶ÅÂíåÊú∫Âô®ÁøªËØëÁ≠â‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÈúÄË¶Å‰∏ì‰∏öÁü•ËØÜÁöÑÈ¢ÜÂüüÂ∫îÁî®‰∏≠ÊïàÊûúÊúâÈôê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÁ†îÁ©∂‰∫∫ÂëòÊé¢Á¥¢‰∫ÜÂ§öÁßçÊñπÊ≥ïÊù•Â¢ûÂº∫LLMsÔºå‰∏ªË¶ÅÂåÖÊã¨Âä®ÊÄÅÁü•ËØÜÊ≥®ÂÖ•„ÄÅÈùôÊÄÅÁü•ËØÜÂµåÂÖ•„ÄÅÊ®°ÂùóÈÄÇÈÖçÂô®ÂíåÊèêÁ§∫‰ºòÂåñ„ÄÇÊØèÁßçÊñπÊ≥ïÈÉΩÊúâÂÖ∂Áã¨ÁâπÁöÑÊú∫Âà∂ÔºåÊó®Âú®‰∏∫LLMsÊèê‰æõÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜÔºåÂêåÊó∂Âú®ÁÅµÊ¥ªÊÄß„ÄÅÂèØÊâ©Â±ïÊÄßÂíåÊïàÁéá‰πãÈó¥ËøõË°åÊùÉË°°„ÄÇÊú¨ÊñáÁªºËø∞‰∫ÜËøô‰∫õÊñπÊ≥ïÁöÑ‰ºòÁº∫ÁÇπÔºåÂπ∂ËÆ®ËÆ∫‰∫ÜÈ¢ÜÂüüÁâπÂÆöLLMs‰∏éÈÄöÁî®LLMsÁöÑÊØîËæÉÔºå‰ª•ÂèäËØ•È¢ÜÂüüÈù¢‰∏¥ÁöÑÊåëÊàòÂíåÊú∫ÈÅá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12669', 'title': 'Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research', 'url': 'https://huggingface.co/papers/2502.12669', 'abstract': 'The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key components. First, we develop Perovskite-KG, a domain-specific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships. Second, we create two complementary datasets: Perovskite-Chat, comprising 55,101 high-quality question-answer pairs generated through a novel multi-agent framework, and Perovskite-Reasoning, containing 2,217 carefully curated materials science problems. Third, we introduce two specialized large language models: Perovskite-Chat-LLM for domain-specific knowledge assistance and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental results demonstrate that our system significantly outperforms existing models in both domain-specific knowledge retrieval and scientific reasoning tasks, providing researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research.', 'score': 2, 'issue_id': 2291, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': 'cb1e20aec1d7e12c', 'authors': ['Xiang Liu', 'Penglei Sun', 'Shuyan Chen', 'Longhan Zhang', 'Peijie Dong', 'Huajie You', 'Yongqi Zhang', 'Chang Yan', 'Xiaowen Chu', 'Tong-yi Zhang'], 'affiliations': ['Guangzhou Municipal Key Laboratory of Materials Informatics', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.12669.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#data', '#science', '#architecture', '#multimodal', '#training', '#graphs'], 'emoji': '‚òÄÔ∏è', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –ø–µ—Ä–æ–≤—Å–∫–∏—Ç–Ω—ã—Ö —Å–æ–ª–Ω–µ—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–µ—Ä–æ–≤—Å–∫–∏—Ç–Ω—ã—Ö —Å–æ–ª–Ω–µ—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (PSC), –≤–∫–ª—é—á–∞—é—â—É—é —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞. –ü–µ—Ä–≤—ã–π - —ç—Ç–æ Perovskite-KG, –ø—Ä–µ–¥–º–µ—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ 1517 –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π. –í—Ç–æ—Ä–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç - –¥–≤–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: Perovskite-Chat —Å 55101 –ø–∞—Ä–æ–π –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –∏ Perovskite-Reasoning —Å 2217 –∑–∞–¥–∞—á–∞–º–∏ –ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤–µ–¥–µ–Ω–∏—é. –¢—Ä–µ—Ç–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç - –¥–≤–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: Perovskite-Chat-LLM –¥–ª—è –ø–æ–º–æ—â–∏ –≤ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –∏ Perovskite-Reasoning-LLM –¥–ª—è –∑–∞–¥–∞—á –Ω–∞—É—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering PSC Research with Knowledge and Reasoning Systems', 'desc': 'This paper presents a knowledge-enhanced system specifically designed for perovskite solar cells (PSCs) research. It includes a knowledge graph built from over 1,500 research papers, which organizes entities and relationships relevant to PSCs. Additionally, the authors created two datasets: one for question-answer pairs and another for materials science problems, both aimed at improving knowledge retrieval and reasoning. The system also features specialized large language models that outperform existing tools, aiding researchers in literature review and experimental design.'}, 'zh': {'title': 'ÈíôÈíõÁüøÂ§™Èò≥ËÉΩÁîµÊ±†ÁöÑÁü•ËØÜÁÆ°ÁêÜ‰∏éÊé®ÁêÜÁ≥ªÁªü', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÈíàÂØπÈíôÈíõÁüøÂ§™Èò≥ËÉΩÁîµÊ±†ÁöÑÁü•ËØÜÂ¢ûÂº∫Á≥ªÁªü„ÄÇËØ•Á≥ªÁªüÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöÈ¶ñÂÖàÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´1517ÁØáÁ†îÁ©∂ËÆ∫ÊñáÁöÑÈ¢ÜÂüüÁâπÂÆöÁü•ËØÜÂõæË∞±ÔºåÊ∂µÁõñ23789‰∏™ÂÆû‰ΩìÂíå22272‰∏™ÂÖ≥Á≥ª„ÄÇÂÖ∂Ê¨°ÔºåÂàõÂª∫‰∫Ü‰∏§‰∏™‰∫íË°•ÁöÑÊï∞ÊçÆÈõÜÔºåÂàÜÂà´Áî®‰∫éÈóÆÁ≠îÂíåÊùêÊñôÁßëÂ≠¶ÈóÆÈ¢ò„ÄÇÊúÄÂêéÔºåÊèêÂá∫‰∫Ü‰∏§‰∏™‰∏ìÈó®ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈ¢ÜÂüüÁü•ËØÜÊ£ÄÁ¥¢ÂíåÁßëÂ≠¶Êé®ÁêÜÁöÑÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13142', 'title': 'Pre-training Auto-regressive Robotic Models with 4D Representations', 'url': 'https://huggingface.co/papers/2502.13142', 'abstract': 'Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations.', 'score': 2, 'issue_id': 2290, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '3c2f3b3e998c6c95', 'authors': ['Dantong Niu', 'Yuvan Sharma', 'Haoru Xue', 'Giscard Biamby', 'Junyi Zhang', 'Ziteng Ji', 'Trevor Darrell', 'Roei Herzig'], 'affiliations': ['BAIR, UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.13142.jpg', 'data': {'categories': ['#training', '#dataset', '#transfer_learning', '#3d', '#robotics'], 'emoji': 'ü§ñ', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ARM4R - –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é 4D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö —Å –ª—é–¥—å–º–∏. –ú–æ–¥–µ–ª—å –ø—Ä–∏–º–µ–Ω—è–µ—Ç 3D-–æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –∏–∑ –≤–∏–¥–µ–æ, –ø–æ–¥–Ω–∏–º–∞—è 2D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤ 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–Ω–∞–Ω–∏—è —Å –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π –Ω–∞ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ARM4R —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö.'}, 'en': {'title': 'Revolutionizing Robotics with Human Video Insights', 'desc': 'This paper presents ARM4R, an Auto-regressive Robotic Model designed to enhance robotic learning by utilizing low-level 4D representations derived from human video data. By converting 2D video representations into 3D point tracking through monocular depth estimation, ARM4R captures the geometric relationships necessary for effective robotic control. The model enables efficient transfer learning, allowing robots to learn from human actions without the need for extensive annotations. Experimental results demonstrate that ARM4R significantly improves performance across diverse robotic tasks and environments.'}, 'zh': {'title': 'Âà©Áî®ËßÜÈ¢ëÊï∞ÊçÆÊèêÂçáÊú∫Âô®‰∫∫ÊéßÂà∂ËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ARM4RÁöÑËá™ÂõûÂΩíÊú∫Âô®‰∫∫Ê®°ÂûãÔºåÂÆÉÂà©Áî®‰ªé‰∫∫Á±ªËßÜÈ¢ëÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÁöÑ‰ΩéÁ∫ß4DË°®Á§∫Êù•ÊîπËøõÊú∫Âô®‰∫∫ÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÂà©Áî®‰ªéËßÜÈ¢ë‰∏≠ÊèêÂèñÁöÑ3DÁÇπË∑üË∏™Ë°®Á§∫ÔºåËøô‰∫õË°®Á§∫ÈÄöËøáÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Â∞Ü2DË°®Á§∫ÊèêÂçáÂà∞3DÁ©∫Èó¥„ÄÇ4DË°®Á§∫Âú®ÁÇπÂíåÊú∫Âô®‰∫∫Áä∂ÊÄÅË°®Á§∫‰πãÈó¥‰øùÊåÅÂÖ±‰∫´ÁöÑÂá†‰ΩïÁªìÊûÑÔºå‰ªéËÄåÂÆûÁé∞‰ªé‰∫∫Á±ªËßÜÈ¢ëÊï∞ÊçÆÂà∞‰ΩéÁ∫ßÊú∫Âô®‰∫∫ÊéßÂà∂ÁöÑÈ´òÊïàËøÅÁßªÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåARM4RËÉΩÂ§üÊúâÊïàÂú∞‰ªé‰∫∫Á±ªËßÜÈ¢ëÊï∞ÊçÆËøÅÁßªÂà∞Êú∫Âô®‰∫∫ÔºåÂπ∂Âú®ÂêÑÁßçÊú∫Âô®‰∫∫ÁéØÂ¢ÉÂíåÈÖçÁΩÆ‰∏≠ÊåÅÁª≠ÊèêÈ´ò‰ªªÂä°ÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10852', 'title': 'Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages', 'url': 'https://huggingface.co/papers/2502.10852', 'abstract': 'While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.', 'score': 2, 'issue_id': 2287, 'pub_date': '2025-02-15', 'pub_date_card': {'ru': '15 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 15', 'zh': '2Êúà15Êó•'}, 'hash': '11a782268b627ec1', 'authors': ['Zeli Su', 'Ziyin Zhang', 'Guixian Xu', 'Jianing Liu', 'XU Han', 'Ting Zhang', 'Yushuang Dong'], 'affiliations': ['Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE', 'Minzu University of China', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.10852.jpg', 'data': {'categories': ['#multilingual', '#low_resource'], 'emoji': 'üåç', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –∫—Ä–∞–π–Ω–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ—Å–æ–≤ –º–µ–∂–¥—É —ç–Ω–∫–æ–¥–µ—Ä–æ–º –∏ –¥–µ–∫–æ–¥–µ—Ä–æ–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —ç–Ω–∫–æ–¥–µ—Ä–∞. –ü—Ä–∏–º–µ–Ω—è—è —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∫ —á–µ—Ç—ã—Ä–µ–º –∫–∏—Ç–∞–π—Å–∫–∏–º —è–∑—ã–∫–∞–º –º–µ–Ω—å—à–∏–Ω—Å—Ç–≤, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥–µ–ª—å XLM-SWCM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã XLM-SWCM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –¥–∞–∂–µ –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'Empowering Low-Resource Languages with Efficient Multilingual Models', 'desc': 'This paper addresses the challenges faced by multilingual language models in generating text for extremely low-resource languages. It introduces a new framework that adapts multilingual encoders for text generation by reusing weights between the encoder and decoder. This approach allows the model to utilize the semantic knowledge learned by the encoder, leading to better performance in low-resource settings. The authors demonstrate the effectiveness of their framework, named XLM-SWCM, on four Chinese minority languages, showing that it outperforms larger models in various tasks.'}, 'zh': {'title': '‰∏∫ÊûÅ‰ΩéËµÑÊ∫êËØ≠Ë®ÄËµãËÉΩÁöÑÂ§öËØ≠Ë®ÄÊñáÊú¨ÁîüÊàêÊ°ÜÊû∂', 'desc': 'Â§öËØ≠Ë®ÄÊ®°ÂûãÂ¶ÇXLM-RÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÁöÑÂ§öËØ≠Ë®ÄËÉΩÂäõÊúâÊâÄÊèêÂçáÔºå‰ΩÜÂú®ÊûÅ‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏äË°®Áé∞‰ªçÁÑ∂ËæÉÂ∑Æ„ÄÇÁé∞‰ª£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ¶ÇLLaMAÂíåQwenÊîØÊåÅÁöÑËØ≠Ë®ÄÊï∞ÈáèËøúÂ∞ë‰∫éXLM-RÔºåÂØºËá¥ËÆ∏Â§öËØ≠Ë®ÄÁº∫‰πèÊñáÊú¨ÁîüÊàêÊ®°Âûã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂ÔºåÂ∞ÜÂ§öËØ≠Ë®ÄÁºñÁ†ÅÂô®ÈÄÇÂ∫î‰∫éÊûÅ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÊñáÊú¨ÁîüÊàê„ÄÇÈÄöËøáÈáçÁî®ÁºñÁ†ÅÂô®ÂíåËß£Á†ÅÂô®‰πãÈó¥ÁöÑÊùÉÈáçÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂ËÉΩÂ§üÂà©Áî®ÁºñÁ†ÅÂô®Â≠¶‰π†Âà∞ÁöÑËØ≠‰πâÁ©∫Èó¥Ôºå‰ªéËÄåÂú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏≠ÂÆûÁé∞È´òÊïàÂ≠¶‰π†ÂíåÊúâÊïàÊ≥õÂåñ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12524', 'title': 'YOLOv12: Attention-Centric Real-Time Object Detectors', 'url': 'https://huggingface.co/papers/2502.12524', 'abstract': 'Enhancing the network architecture of the YOLO framework has been crucial for a long time, but has focused on CNN-based improvements despite the proven superiority of attention mechanisms in modeling capabilities. This is because attention-based models cannot match the speed of CNN-based models. This paper proposes an attention-centric YOLO framework, namely YOLOv12, that matches the speed of previous CNN-based ones while harnessing the performance benefits of attention mechanisms. YOLOv12 surpasses all popular real-time object detectors in accuracy with competitive speed. For example, YOLOv12-N achieves 40.6% mAP with an inference latency of 1.64 ms on a T4 GPU, outperforming advanced YOLOv10-N / YOLOv11-N by 2.1%/1.2% mAP with a comparable speed. This advantage extends to other model scales. YOLOv12 also surpasses end-to-end real-time detectors that improve DETR, such as RT-DETR / RT-DETRv2: YOLOv12-S beats RT-DETR-R18 / RT-DETRv2-R18 while running 42% faster, using only 36% of the computation and 45% of the parameters. More comparisons are shown in Figure 1.', 'score': 1, 'issue_id': 2302, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '6f915c875f0fb15d', 'authors': ['Yunjie Tian', 'Qixiang Ye', 'David Doermann'], 'affiliations': ['University at Buffalo', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.12524.jpg', 'data': {'categories': ['#cv', '#optimization', '#architecture'], 'emoji': 'üîç', 'ru': {'title': 'YOLOv12: –í–Ω–∏–º–∞–Ω–∏–µ —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é CNN', 'desc': 'YOLOv12 - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–∞—Ö –≤–Ω–∏–º–∞–Ω–∏—è. –û–Ω–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å CNN-–º–æ–¥–µ–ª–µ–π. YOLOv12 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, YOLOv12-N –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 40.6% mAP —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –≤—ã–≤–æ–¥–∞ 1.64 –º—Å –Ω–∞ GPU T4, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è YOLOv10-N –∏ YOLOv11-N.'}, 'en': {'title': 'YOLOv12: Merging Speed and Accuracy with Attention Mechanisms', 'desc': 'This paper introduces YOLOv12, an enhanced version of the YOLO framework that integrates attention mechanisms to improve object detection accuracy while maintaining high speed. Unlike previous models that relied solely on CNN improvements, YOLOv12 achieves a mean Average Precision (mAP) of 40.6% with an inference latency of just 1.64 ms on a T4 GPU. It outperforms earlier YOLO versions and other real-time detectors like RT-DETR, demonstrating superior performance with reduced computational requirements. The results indicate that attention-based models can now compete effectively with traditional CNNs in real-time applications.'}, 'zh': {'title': 'YOLOv12ÔºöÈÄüÂ∫¶‰∏éÂáÜÁ°ÆÊÄßÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ª•Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏∫‰∏≠ÂøÉÁöÑYOLOÊ°ÜÊû∂ÔºåÁß∞‰∏∫YOLOv12„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNÔºâÁöÑÊ®°ÂûãÁõ∏ÊØîÔºåYOLOv12Âú®‰øùÊåÅÁõ∏‰ººÈÄüÂ∫¶ÁöÑÂêåÊó∂ÔºåÂà©Áî®‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊÄßËÉΩ‰ºòÂäø„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåYOLOv12Âú®ÂáÜÁ°ÆÊÄß‰∏äË∂ÖË∂ä‰∫ÜÊâÄÊúâÊµÅË°åÁöÑÂÆûÊó∂ÁõÆÊ†áÊ£ÄÊµãÂô®ÔºåÂπ∂‰∏îÂú®Êé®ÁêÜÂª∂ËøüÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇËØ•Ê®°ÂûãÂú®‰∏çÂêåËßÑÊ®°‰∏ãÂùáÂ±ïÁé∞Âá∫‰ºòË∂äÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®ËÆ°ÁÆóÂíåÂèÇÊï∞‰ΩøÁî®‰∏äÊõ¥‰∏∫È´òÊïà„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08869', 'title': 'Harnessing Vision Models for Time Series Analysis: A Survey', 'url': 'https://huggingface.co/papers/2502.08869', 'abstract': 'Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides a comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the pre- and post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models.', 'score': 1, 'issue_id': 2299, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '8bfec40a634b6df7', 'authors': ['Jingchao Ni', 'Ziming Zhao', 'ChengAo Shen', 'Hanghang Tong', 'Dongjin Song', 'Wei Cheng', 'Dongsheng Luo', 'Haifeng Chen'], 'affiliations': ['Florida International University', 'NEC Laboratories America', 'University of Connecticut', 'University of Houston', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.08869.jpg', 'data': {'categories': ['#multimodal', '#cv', '#survey', '#data'], 'emoji': 'üìä', 'ru': {'title': '–í–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –∞–Ω–∞–ª–∏–∑–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. –í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–æ–¥—Ä–æ–±–Ω–∞—è —Ç–∞–∫—Å–æ–Ω–æ–º–∏—è –º–µ—Ç–æ–¥–æ–≤ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏—Ö –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –¢–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —ç—Ç–æ–º –ø–æ–¥—Ö–æ–¥–µ –∏ –Ω–∞–º–µ—á–∞—é—Ç—Å—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.'}, 'en': {'title': 'Harnessing Vision Models for Enhanced Time Series Analysis', 'desc': 'This paper surveys the use of vision models, such as Large Vision Models (LVMs) and Vision Language Models (VLMs), in time series analysis, highlighting their advantages over traditional Large Language Models (LLMs). It discusses the challenges of converting continuous time series data into a format suitable for these models, particularly focusing on encoding time series as images. The paper also explores how to effectively model these imaged time series for various analytical tasks. Furthermore, it identifies key challenges in the pre- and post-processing stages and suggests future research directions to enhance the application of vision models in this field.'}, 'zh': {'title': 'ËßÜËßâÊ®°ÂûãÂä©ÂäõÊó∂Èó¥Â∫èÂàóÂàÜÊûêÁöÑÊú™Êù•', 'desc': 'Êó∂Èó¥Â∫èÂàóÂàÜÊûêÁªèÂéÜ‰∫Ü‰ªé‰º†ÁªüËá™ÂõûÂΩíÊ®°ÂûãÂà∞Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºåÂÜçÂà∞ÊúÄËøëÁöÑÂèòÊç¢Âô®ÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèëÂ±ï„ÄÇÂ∞ΩÁÆ°Âú®Êó∂Èó¥Â∫èÂàóÂàÜÊûê‰∏≠ÔºåËßÜËßâÊ®°ÂûãÁöÑÂ∫îÁî®‰πüÂú®Â¢ûÂä†Ôºå‰ΩÜÁî±‰∫éÂØπÂ∫èÂàóÂª∫Ê®°ÁöÑÁ†îÁ©∂Âç†‰∏ªÂØºÂú∞‰ΩçÔºåËøô‰∫õÂä™ÂäõÂπ∂‰∏çÊòæËëó„ÄÇÊú¨ÊñáÁªºËø∞‰∫ÜËßÜËßâÊ®°ÂûãÂú®Êó∂Èó¥Â∫èÂàóÂàÜÊûê‰∏≠ÁöÑ‰ºòÂäøÔºåÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÊó∂Èó¥Â∫èÂàóÁºñÁ†Å‰∏∫ÂõæÂÉè‰ª•ÂèäÂ¶Ç‰ΩïÂØπÂõæÂÉèÂåñÁöÑÊó∂Èó¥Â∫èÂàóËøõË°åÂª∫Ê®°„ÄÇÊàë‰ª¨ËøòËÆ®ËÆ∫‰∫ÜËØ•Ê°ÜÊû∂‰∏≠È¢ÑÂ§ÑÁêÜÂíåÂêéÂ§ÑÁêÜÊ≠•È™§ÁöÑÊåëÊàòÔºåÂπ∂ÊèêÂá∫‰∫ÜÊú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10990', 'title': 'FinMTEB: Finance Massive Text Embedding Benchmark', 'url': 'https://huggingface.co/papers/2502.10990', 'abstract': 'Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, FinPersona-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including FinPersona-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.', 'score': 1, 'issue_id': 2293, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': '4cba4906d5d024ae', 'authors': ['Yixuan Tang', 'Yi Yang'], 'affiliations': ['The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.10990.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#multilingual', '#transfer_learning', '#science', '#benchmark'], 'emoji': 'üíπ', 'ru': {'title': 'FinMTEB: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FinMTEB - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 64 –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ 7 –∑–∞–¥–∞—á–∞–º –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥–µ–ª—å FinPersona-E5, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ–±–ª–∞—Å—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ–ª–∏ –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è, –∞ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ Bag-of-Words –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–ª–æ–∂–Ω—ã–µ –ø–ª–æ—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤.'}, 'en': {'title': 'FinMTEB: Elevating Financial NLP with Domain-Specific Embeddings', 'desc': 'This paper introduces the Finance Massive Text Embedding Benchmark (FinMTEB), which is designed specifically for evaluating embedding models in the financial domain. It includes 64 datasets across 7 tasks, focusing on various types of financial texts in both Chinese and English. The authors also present FinPersona-E5, a finance-adapted model that utilizes a persona-based data synthesis method for training on financial tasks. The study reveals that general-purpose benchmarks do not correlate well with financial tasks, domain-adapted models perform better, and a simple Bag-of-Words approach can outperform complex dense embeddings in certain financial tasks.'}, 'zh': {'title': 'ÈáëËûçÈ¢ÜÂüüÁöÑÂµåÂÖ•Ê®°ÂûãËØÑ‰º∞Êñ∞Ê†áÂáÜ', 'desc': 'ÂµåÂÖ•Ê®°ÂûãÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâÂ∫îÁî®‰∏≠ÊâÆÊºîÁùÄÈáçË¶ÅËßíËâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ø°ÊÅØË°®Á§∫ÂíåÊ£ÄÁ¥¢ÊñπÈù¢„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜÈáëËûçÂ§ßËßÑÊ®°ÊñáÊú¨ÂµåÂÖ•Âü∫ÂáÜÔºàFinMTEBÔºâÔºåËøôÊòØ‰∏Ä‰∏™‰∏ì‰∏∫ÈáëËûçÈ¢ÜÂüüËÆæËÆ°ÁöÑËØÑ‰º∞Â∑•ÂÖ∑ÔºåÂåÖÂê´64‰∏™ÈáëËûçÁâπÂÆöÁöÑÂµåÂÖ•Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñ7‰∏™‰ªªÂä°„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÈÄÇÂ∫îÈáëËûçÈ¢ÜÂüüÁöÑÊ®°ÂûãFinPersona-E5ÔºåÂà©Áî®Âü∫‰∫éËßíËâ≤ÁöÑÊï∞ÊçÆÂêàÊàêÊñπÊ≥ïÊù•ËÆ≠ÁªÉÂ§öÊ†∑ÂåñÁöÑÈáëËûçÂµåÂÖ•‰ªªÂä°„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÈÄöÁî®Âü∫ÂáÜÁöÑË°®Áé∞‰∏éÈáëËûçÈ¢ÜÂüü‰ªªÂä°ÁöÑÁõ∏ÂÖ≥ÊÄßÊúâÈôêÔºåÈ¢ÜÂüüÈÄÇÂ∫îÊ®°ÂûãÁöÑË°®Áé∞‰ºò‰∫éÈÄöÁî®Ê®°ÂûãÔºåËÄåÁÆÄÂçïÁöÑËØçË¢ãÊ®°ÂûãÂú®ÈáëËûçËØ≠‰πâÊñáÊú¨Áõ∏‰ººÊÄß‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫‰πéÊÑèÊñôÂú∞‰ºò‰∫éÂ§çÊùÇÁöÑÂØÜÈõÜÂµåÂÖ•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06329', 'title': 'Expect the Unexpected: FailSafe Long Context QA for Finance', 'url': 'https://huggingface.co/papers/2502.06329', 'abstract': 'We propose a new long-context financial benchmark, FailSafeQA, designed to test the robustness and context-awareness of LLMs against six variations in human-interface interactions in LLM-based query-answer systems within finance. We concentrate on two case studies: Query Failure and Context Failure. In the Query Failure scenario, we perturb the original query to vary in domain expertise, completeness, and linguistic accuracy. In the Context Failure case, we simulate the uploads of degraded, irrelevant, and empty documents. We employ the LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained rating criteria to define and calculate Robustness, Context Grounding, and Compliance scores for 24 off-the-shelf models. The results suggest that although some models excel at mitigating input perturbations, they must balance robust answering with the ability to refrain from hallucinating. Notably, Palmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained strong baseline performance but encountered challenges in sustaining robust predictions in 17% of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in 41% of tested cases. The results demonstrate that even high-performing models have significant room for improvement and highlight the role of FailSafeQA as a tool for developing LLMs optimized for dependability in financial applications. The dataset is available at: https://huggingface.co/datasets/Writer/FailSafeQA', 'score': 102, 'issue_id': 2168, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '836d77158c8a414b', 'authors': ['Kiran Kamble', 'Melisa Russak', 'Dmytro Mozolevskyi', 'Muayad Ali', 'Mateusz Russak', 'Waseem AlShikh'], 'affiliations': ['Writer, Inc'], 'pdf_title_img': 'assets/pdf/title_img/2502.06329.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#optimization', '#long_context', '#hallucinations'], 'emoji': 'üíº', 'ru': {'title': 'FailSafeQA: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç FailSafeQA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ. –¢–µ—Å—Ç –≤–∫–ª—é—á–∞–µ—Ç —à–µ—Å—Ç—å –≤–∞—Ä–∏–∞—Ü–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ LLM, –∏ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –¥–≤—É—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö: –æ—Ç–∫–∞–∑ –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ—Ç–∫–∞–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é LLM-as-a-Judge –∏ –º–æ–¥–µ–ª—å Qwen2.5-72B-Instruct, –∞–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–ª–∏ 24 –≥–æ—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è, –∏ –ø–æ–¥—á–µ—Ä–∫–Ω—É–ª–∏ –≤–∞–∂–Ω–æ—Å—Ç—å FailSafeQA –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã—Ö LLM –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'Enhancing LLM Reliability in Finance with FailSafeQA', 'desc': 'The paper introduces FailSafeQA, a benchmark aimed at evaluating the robustness and context-awareness of large language models (LLMs) in financial query-answer systems. It focuses on two main failure scenarios: Query Failure, where the input query is altered in terms of expertise and clarity, and Context Failure, where irrelevant or degraded documents are introduced. The authors utilize the LLM-as-a-Judge methodology to assess various models based on their Robustness, Context Grounding, and Compliance scores. The findings reveal that while some models perform well under input variations, they struggle with maintaining accuracy, indicating a need for further enhancements in LLM reliability for financial contexts.'}, 'zh': {'title': 'FailSafeQAÔºöÊèêÂçáÈáëËûçÈ¢ÜÂüüLLMÁöÑÈ≤ÅÊ£íÊÄß‰∏é‰∏ä‰∏ãÊñáÊÑèËØÜ', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÈïø‰∏ä‰∏ãÊñáÈáëËûçÂü∫ÂáÜÔºåFailSafeQAÔºåÊó®Âú®ÊµãËØïÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÈáëËûçÈ¢ÜÂüü‰∏≠ÂØπ‰∫∫Êú∫‰∫§‰∫íÁöÑÈ≤ÅÊ£íÊÄßÂíå‰∏ä‰∏ãÊñáÊÑèËØÜ„ÄÇÊàë‰ª¨ÂÖ≥Ê≥®‰∏§‰∏™Ê°à‰æãÁ†îÁ©∂ÔºöÊü•ËØ¢Â§±Ë¥•Âíå‰∏ä‰∏ãÊñáÂ§±Ë¥•„ÄÇÂú®Êü•ËØ¢Â§±Ë¥•Âú∫ÊôØ‰∏≠ÔºåÊàë‰ª¨ÈÄöËøáÊîπÂèòÈ¢ÜÂüü‰∏ì‰∏öÊÄß„ÄÅÂÆåÊï¥ÊÄßÂíåËØ≠Ë®ÄÂáÜÁ°ÆÊÄßÊù•Êâ∞Âä®ÂéüÂßãÊü•ËØ¢„ÄÇÂú®‰∏ä‰∏ãÊñáÂ§±Ë¥•Ê°à‰æã‰∏≠ÔºåÊàë‰ª¨Ê®°Êãü‰∏ä‰º†ÈôçÁ∫ß„ÄÅÊó†ÂÖ≥ÂíåÁ©∫ÊñáÊ°£ÁöÑÊÉÖÂÜµ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06807', 'title': 'Competitive Programming with Large Reasoning Models', 'url': 'https://huggingface.co/papers/2502.06807', 'abstract': 'We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.', 'score': 38, 'issue_id': 2164, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'fd76cceb75f32321', 'authors': ['OpenAI', ':', 'Ahmed El-Kishky', 'Alexander Wei', 'Andre Saraiva', 'Borys Minaev', 'Daniel Selsam', 'David Dohan', 'Francis Song', 'Hunter Lightman', 'Ignasi Clavera', 'Jakub Pachocki', 'Jerry Tworek', 'Lorenz Kuhn', 'Lukasz Kaiser', 'Mark Chen', 'Max Schwarzer', 'Mostafa Rohaninejad', 'Nat McAleese', 'o3 contributors', 'Oleg M√ºrk', 'Rhythm Garg', 'Rui Shu', 'Szymon Sidor', 'Vineet Kosaraju', 'Wenda Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.06807.jpg', 'data': {'categories': ['#rlhf', '#rl', '#games', '#training', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∫ –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç –º–æ–¥–µ–ª–∏ –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è (OpenAI o1 –∏ o3) —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π o1-ioi, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –¥–ª—è —É—á–∞—Å—Ç–∏—è –≤ –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–π –æ–ª–∏–º–ø–∏–∞–¥–µ –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ (IOI) 2024 –≥–æ–¥–∞. –ú–æ–¥–µ–ª—å o3 –¥–æ—Å—Ç–∏–≥–ª–∞ —É—Ä–æ–≤–Ω—è –∑–æ–ª–æ—Ç–æ–π –º–µ–¥–∞–ª–∏ –Ω–∞ IOI 2024 –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏–ª–∏ –ø–æ—Å–ª–∞–±–ª–µ–Ω–∏–π –ø—Ä–∞–≤–∏–ª. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º –∫ —Å–æ–∑–¥–∞–Ω–∏—é –ò–ò –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —á–µ–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫.'}, 'en': {'title': 'Scaling General-Purpose Learning Outshines Specialized Strategies', 'desc': 'This paper demonstrates that applying reinforcement learning to large language models (LLMs) enhances their ability to tackle complex coding and reasoning challenges. It compares two reasoning models, OpenAI o1 and an early version of o3, against a specialized model, o1-ioi, which uses tailored inference strategies for the 2024 International Olympiad in Informatics (IOI). The results show that while o1-ioi performed well with its hand-crafted strategies, the later model o3 achieved superior results without such specific techniques. This suggests that scaling general-purpose reinforcement learning is a more effective approach for achieving high performance in reasoning tasks, like competitive programming.'}, 'zh': {'title': 'Âº∫ÂåñÂ≠¶‰π†Âä©ÂäõÈÄöÁî®Ê®°ÂûãË∂ÖË∂äÁâπÂÆöÈ¢ÜÂüüÁ≥ªÁªü', 'desc': 'Êú¨ËÆ∫ÊñáÂ±ïÁ§∫‰∫ÜÂº∫ÂåñÂ≠¶‰π†Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÂ∫îÁî®ÔºåÊòæËëóÊèêÂçá‰∫ÜÂ§çÊùÇÁºñÁ†ÅÂíåÊé®ÁêÜ‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÊØîËæÉ‰∫Ü‰∏§ÁßçÈÄöÁî®Êé®ÁêÜÊ®°Âûã‚Äî‚ÄîOpenAIÁöÑo1Âíåo3ÁöÑÊó©ÊúüÊ£ÄÊü•ÁÇπÔºå‰ª•Âèä‰∏Ä‰∏™ÁâπÂÆöÈ¢ÜÂüüÁöÑÁ≥ªÁªüo1-ioiÔºåËØ•Á≥ªÁªü‰ΩøÁî®ÊâãÂ∑•ËÆæËÆ°ÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄÇo1-ioiÂú®2024Âπ¥ÂõΩÈôÖ‰ø°ÊÅØÂ≠¶Â••ÊûóÂåπÂÖãÁ´ûËµõ‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåËé∑Âæó‰∫ÜÁ¨¨49ÁôæÂàÜ‰ΩçÁöÑÊàêÁª©ÔºåËÄåÂú®ÊîæÂÆΩÁ´û‰∫âÁ∫¶ÊùüÁöÑÊÉÖÂÜµ‰∏ãÂàôËé∑Âæó‰∫ÜÈáëÁâå„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°‰∏ìÈó®ÁöÑÁÆ°ÈÅìÂ¶Ço1-ioiËÉΩÂ∏¶Êù•ÊòæËëóÊèêÂçáÔºå‰ΩÜÊâ©Â±ïÁöÑÈÄöÁî®o3Ê®°ÂûãÂú®Ê≤°Êúâ‰æùËµñÊâãÂ∑•Êé®ÁêÜÂêØÂèëÂºèÁöÑÊÉÖÂÜµ‰∏ãÔºåË∂ÖË∂ä‰∫ÜËøô‰∫õÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05878', 'title': 'Retrieval-augmented Large Language Models for Financial Time Series Forecasting', 'url': 'https://huggingface.co/papers/2502.05878', 'abstract': 'Stock movement prediction, a fundamental task in financial time-series forecasting, requires identifying and retrieving critical influencing factors from vast amounts of time-series data. However, existing text-trained or numeric similarity-based retrieval methods fall short in handling complex financial analysis. To address this, we propose the first retrieval-augmented generation (RAG) framework for financial time-series forecasting, featuring three key innovations: a fine-tuned 1B parameter large language model (StockLLM) as the backbone, a novel candidate selection method leveraging LLM feedback, and a training objective that maximizes similarity between queries and historically significant sequences. This enables our retriever, FinSeer, to uncover meaningful patterns while minimizing noise in complex financial data. We also construct new datasets integrating financial indicators and historical stock prices to train FinSeer and ensure robust evaluation. Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while FinSeer surpasses existing retrieval methods, achieving an 8\\% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work underscores the importance of tailored retrieval models in financial forecasting and provides a novel framework for future research.', 'score': 24, 'issue_id': 2172, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': 'bb6c947ce857a2db', 'authors': ['Mengxi Xiao', 'Zihao Jiang', 'Lingfei Qian', 'Zhengyu Chen', 'Yueru He', 'Yijing Xu', 'Yuecheng Jiang', 'Dong Li', 'Ruey-Ling Weng', 'Min Peng', 'Jimin Huang', 'Sophia Ananiadou', 'Qianqian Xie'], 'affiliations': ['School of Computer Science, Wuhan University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05878.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#rag'], 'emoji': 'üìà', 'ru': {'title': 'FinSeer: –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –∞–∫—Ü–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –∞–∫—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (RAG). –ö–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—Ä—É–ø–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ StockLLM, –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é –æ—Ç LLM –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ü–µ–ª—å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–Ω–∞—á–∏–º—ã—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ FinSeer –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–∞ 8% –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ BIGDATA22. –†–∞–±–æ—Ç–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏.'}, 'en': {'title': 'Revolutionizing Stock Prediction with RAG Framework', 'desc': 'This paper introduces a new approach to predicting stock movements by using a retrieval-augmented generation (RAG) framework specifically designed for financial time-series data. The framework employs a large language model called StockLLM, which has been fine-tuned to better understand financial contexts. It also features a unique candidate selection method that utilizes feedback from the language model to improve the relevance of retrieved data. The results show that this method significantly enhances prediction accuracy and uncovers important patterns in complex financial datasets, outperforming traditional retrieval techniques.'}, 'zh': {'title': 'ÈáëËûçÈ¢ÑÊµãÁöÑÊñ∞Ê°ÜÊû∂ÔºöÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁî®‰∫éÈáëËûçÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊ°ÜÊû∂ÔºåÊó®Âú®‰ªéÂ§ßÈáèÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆ‰∏≠ËØÜÂà´ÂíåÊèêÂèñÂÖ≥ÈîÆÂΩ±ÂìçÂõ†Á¥†„ÄÇÊàë‰ª¨‰ΩøÁî®‰∫Ü‰∏Ä‰∏™ÁªèËøáÂæÆË∞ÉÁöÑ1BÂèÇÊï∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàStockLLMÔºâ‰Ωú‰∏∫Âü∫Á°ÄÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂÄôÈÄâÈÄâÊã©ÊñπÊ≥ïÔºåÂà©Áî®LLMÂèçÈ¶àÊù•‰ºòÂåñÊ£ÄÁ¥¢ËøáÁ®ã„ÄÇÈÄöËøáÊúÄÂ§ßÂåñÊü•ËØ¢‰∏éÂéÜÂè≤ÈáçË¶ÅÂ∫èÂàó‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÔºåÊàë‰ª¨ÁöÑÊ£ÄÁ¥¢Âô®FinSeerËÉΩÂ§üÂú®Â§çÊùÇÁöÑÈáëËûçÊï∞ÊçÆ‰∏≠ÂèëÁé∞ÊúâÊÑè‰πâÁöÑÊ®°ÂºèÔºåÂêåÊó∂ÂáèÂ∞ëÂô™Â£∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•RAGÊ°ÜÊû∂Âú®ÂáÜÁ°ÆÊÄß‰∏ä‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂº∫Ë∞É‰∫ÜÂÆöÂà∂Ê£ÄÁ¥¢Ê®°ÂûãÂú®ÈáëËûçÈ¢ÑÊµã‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07316', 'title': 'CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction', 'url': 'https://huggingface.co/papers/2502.07316', 'abstract': 'Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.', 'score': 21, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': 'fd4f34152d4de2c1', 'authors': ['Junlong Li', 'Daya Guo', 'Dejian Yang', 'Runxin Xu', 'Yu Wu', 'Junxian He'], 'affiliations': ['DeepSeek-AI', 'HKUST', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07316.jpg', 'data': {'categories': ['#dataset', '#data', '#training', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'CodeI/O: –†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∫–æ–¥', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ CodeI/O –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤ –∫–æ–¥, –≤ —Ñ–æ—Ä–º–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–≤–æ–¥–∞-–≤—ã–≤–æ–¥–∞ –∫–æ–¥–∞ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –∏–∑—É—á–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –∏ –º–æ–¥—É–ª—å–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CodeI/O –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É–ª—É—á—à–µ–Ω–∏—è–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö, –Ω–∞—É—á–Ω—ã—Ö, –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏ –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with CodeI/O', 'desc': 'This paper introduces CodeI/O, a new method designed to enhance reasoning capabilities in Large Language Models (LLMs) by transforming code into a format that predicts inputs and outputs. The approach focuses on training models using natural language Chain-of-Thought (CoT) rationales, which helps the models learn universal reasoning patterns without being tied to specific coding syntax. By exposing models to various reasoning tasks, such as logic flow and state-space searching, CodeI/O improves performance across multiple domains, including math and commonsense reasoning. The results show that this method not only boosts reasoning accuracy but also allows for verification and refinement of predictions through a multi-turn revision process, leading to even better outcomes with CodeI/O++.'}, 'zh': {'title': 'CodeI/OÔºöÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫CodeI/OÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáÂ∞ÜÂéüÂßã‰ª£Á†ÅËΩ¨Êç¢‰∏∫ËæìÂÖ•ËæìÂá∫È¢ÑÊµãÊ†ºÂºèÔºåCodeI/OÁ≥ªÁªüÂú∞ÊèêÁÇº‰∫ÜÂ§öÊ†∑ÁöÑÊé®ÁêÜÊ®°Âºè„ÄÇÊ®°ÂûãÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊé®ÁêÜÊù•È¢ÑÊµã‰ª£Á†ÅÁöÑËæìÂÖ•ÂíåËæìÂá∫Ôºå‰ªéËÄåÂ¢ûÂº∫‰∫ÜÈÄªËæëÊµÅËßÑÂàí„ÄÅÁä∂ÊÄÅÁ©∫Èó¥ÊêúÁ¥¢Á≠âÊé®ÁêÜÂéüËØ≠ÁöÑËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCodeI/OÂú®Â§öÁßçÊé®ÁêÜ‰ªªÂä°‰∏äÂùáË°®Áé∞Âá∫‰∏ÄËá¥ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07701', 'title': 'Magic 1-For-1: Generating One Minute Video Clips within One Minute', 'url': 'https://huggingface.co/papers/2502.07701', 'abstract': 'In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.', 'score': 17, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '7212b752112bcd5a', 'authors': ['Hongwei Yi', 'Shitong Shao', 'Tian Ye', 'Jiantong Zhao', 'Qingyu Yin', 'Michael Lingelbach', 'Li Yuan', 'Yonghong Tian', 'Enze Xie', 'Daquan Zhou'], 'affiliations': ['Hedra Inc.', 'Nvidia', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07701.jpg', 'data': {'categories': ['#video', '#training', '#inference', '#multimodal', '#open_source', '#diffusion', '#optimization'], 'emoji': 'üé¨', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –∫–∞–¥—Ä–∞–º –∑–∞ —Å–µ–∫—É–Ω–¥—ã', 'desc': 'Magic 1-For-1 (Magic141) - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏ –∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é –≤—ã–≤–æ–¥–∞. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É –Ω–∞ –¥–≤–µ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏: –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Ä—è–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–µ–º–æ–≤, –≤–∫–ª—é—á–∞—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ —É—Å–ª–æ–≤–∏–π, —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é —à–∞–≥–æ–≤ –∏ —Ä–∞–∑—Ä–µ–∂–∏–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –º–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å 5-—Å–µ–∫—É–Ω–¥–Ω—ã–µ –≤–∏–¥–µ–æ–∫–ª–∏–ø—ã –º–µ–Ω–µ–µ —á–µ–º –∑–∞ 3 —Å–µ–∫—É–Ω–¥—ã, –∞ –º–∏–Ω—É—Ç–Ω–æ–µ –≤–∏–¥–µ–æ - –∑–∞ –æ–¥–Ω—É –º–∏–Ω—É—Ç—É —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –¥–∏–Ω–∞–º–∏–∫–æ–π.'}, 'en': {'title': 'Efficient Video Generation: Simplifying with Magic141', 'desc': 'The paper introduces Magic 1-For-1 (Magic141), a video generation model designed to optimize memory usage and reduce inference time. It simplifies the text-to-video generation process by breaking it down into two tasks: generating images from text and then creating videos from those images. The authors demonstrate that the image-to-video task converges more easily than the direct text-to-video approach, allowing for faster training. They implement various optimization techniques to enhance model performance, achieving impressive video generation speeds while maintaining high visual quality.'}, 'zh': {'title': 'È´òÊïàËßÜÈ¢ëÁîüÊàêÔºåËΩªÊùæÂÆûÁé∞ÔºÅ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãMagic 1-For-1ÔºàMagic141ÔºâÔºåËØ•Ê®°Âûã‰ºòÂåñ‰∫ÜÂÜÖÂ≠òÊ∂àËÄóÂíåÊé®ÁêÜÂª∂Ëøü„ÄÇÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàê‰ªªÂä°ÂàÜËß£‰∏∫‰∏§‰∏™Êõ¥ÁÆÄÂçïÁöÑ‰ªªÂä°ÔºöÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÂíåÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàê„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®Áõ∏ÂêåÁöÑ‰ºòÂåñÁÆóÊ≥ïÔºåÂõæÂÉèÂà∞ËßÜÈ¢ë‰ªªÂä°ÁöÑÊî∂ÊïõÈÄüÂ∫¶Á°ÆÂÆû‰ºò‰∫éÊñáÊú¨Âà∞ËßÜÈ¢ë‰ªªÂä°„ÄÇÈÄöËøáÂ§öÁßç‰ºòÂåñÊäÄÂ∑ßÔºåÊ®°ÂûãËÉΩÂ§üÂú®Áü≠Êó∂Èó¥ÂÜÖÁîüÊàêÈ´òË¥®ÈáèÁöÑËßÜÈ¢ëÁâáÊÆµÔºåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07374', 'title': 'LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!', 'url': 'https://huggingface.co/papers/2502.07374', 'abstract': "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.", 'score': 17, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '4df9e17df3250cb4', 'authors': ['Dacheng Li', 'Shiyi Cao', 'Tyler Griggs', 'Shu Liu', 'Xiangxi Mo', 'Shishir G. Patil', 'Matei Zaharia', 'Joseph E. Gonzalez', 'Ion Stoica'], 'affiliations': ['Department of Electrical Engineering and Computer Sciences, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.07374.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#math', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã–º —Ü–µ–ø–æ—á–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –¥–ª–∏–Ω–Ω—ã–º —Ü–µ–ø–æ—á–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Long CoT) —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¥–æ–æ–±—É—á–∫–∏ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å Qwen2.5-32B-Instruct, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 17 —Ç—ã—Å—è—á–∞—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ Long CoT, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ Long CoT –æ–∫–∞–∑–∞–ª–∞—Å—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏–º–µ–ª–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ. –≠—Ç–∏ –≤—ã–≤–æ–¥—ã —É–≥–ª—É–±–ª—è—é—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–≥–æ, –∫–∞–∫ —Ä–∞–∑–≤–∏–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ LLM.'}, 'en': {'title': 'Unlocking Reasoning: Structure Over Content in Large Models', 'desc': 'This paper explores how Large Reasoning Models (LRMs) can improve their reasoning abilities by learning from structured long chain-of-thought (Long CoT) examples. It demonstrates that a Large Language Model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning and low-rank adaptation techniques. The study reveals that the structure of Long CoT is crucial for learning, while the specific content of reasoning steps has a minimal effect on performance. The findings suggest that maintaining logical consistency in reasoning steps is vital for accuracy, even when training on incorrect samples.'}, 'zh': {'title': 'ÈïøÈìæÊÄùÁª¥ÔºöÊé®ÁêÜÊ®°ÂûãÁöÑÂÖ≥ÈîÆÁªìÊûÑ', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÈÄöËøáÈïøÈìæÊÄùÁª¥ÔºàLong CoTÔºâËß£ÂÜ≥Â§çÊùÇÁöÑÊé®ÁêÜÈóÆÈ¢òÔºåËøôÁßçÊÄùÁª¥ÊñπÂºèÂåÖÊã¨ÂèçÊÄù„ÄÅÂõûÊ∫ØÂíåËá™ÊàëÈ™åËØÅ„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂèØ‰ª•ÈÄöËøáÊï∞ÊçÆÈ´òÊïàÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÂèÇÊï∞È´òÊïàÁöÑ‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÊúâÊïàÂ≠¶‰π†ÈïøÈìæÊÄùÁª¥„ÄÇ‰ªÖ‰ΩøÁî®17,000‰∏™ÈïøÈìæÊÄùÁª¥ËÆ≠ÁªÉÊ†∑Êú¨ÔºåQwen2.5-32B-InstructÊ®°ÂûãÂú®Â§ö‰∏™Êï∞Â≠¶ÂíåÁºñÁ†ÅÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈïøÈìæÊÄùÁª¥ÁöÑÁªìÊûÑÂØπÂ≠¶‰π†ËøáÁ®ãËá≥ÂÖ≥ÈáçË¶ÅÔºåËÄåÂçï‰∏™Êé®ÁêÜÊ≠•È™§ÁöÑÂÜÖÂÆπÂØπÊÄßËÉΩÂΩ±ÂìçËæÉÂ∞è„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06857', 'title': 'Gemstones: A Model Suite for Multi-Faceted Scaling Laws', 'url': 'https://huggingface.co/papers/2502.06857', 'abstract': 'Scaling laws are typically fit using a family of models with a narrow range of frozen hyper-parameter choices. In this work we study scaling laws using a wide range of architecture and hyper-parameter choices, and highlight their impact on resulting prescriptions. As a primary artifact of our research, we release the Gemstones: the most comprehensive open-source scaling law dataset to date, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters; these models have been trained with different learning rates, cooldown schedules, and architectural shapes. Our checkpoints enable more complex studies of scaling, such as a law that predicts language modeling performance as a function of model width and depth. By examining the various facets of our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting. Code: https://github.com/mcleish7/gemstone-scaling-laws', 'score': 16, 'issue_id': 2172, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': '8f1b246a774832da', 'authors': ['Sean McLeish', 'John Kirchenbauer', 'David Yu Miller', 'Siddharth Singh', 'Abhinav Bhatele', 'Micah Goldblum', 'Ashwinee Panda', 'Tom Goldstein'], 'affiliations': ['Columbia University', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2502.06857.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#architecture', '#optimization'], 'emoji': 'üíé', 'ru': {'title': '–ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∑–∞–∫–æ–Ω–æ–≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Gemstones, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–æ–ª–µ–µ 4000 –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∑–∞–∫–æ–Ω–∞—Ö –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –º–æ–≥—É—Ç —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –¥–∏–∑–∞–π–Ω–∞ –∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑—É—á–µ–Ω–∏—é –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∑–∞–∫–æ–Ω, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —à–∏—Ä–∏–Ω—ã –∏ –≥–ª—É–±–∏–Ω—ã –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Unlocking Scaling Laws with Gemstones Dataset', 'desc': 'This paper explores scaling laws in machine learning by analyzing a diverse set of models with various hyper-parameters. The authors introduce the Gemstones dataset, which includes over 4000 transformer model checkpoints, allowing for extensive experimentation on scaling effects. They demonstrate that the performance of language models can be predicted based on their architecture, specifically width and depth. The findings reveal that scaling law prescriptions are sensitive to the design of experiments and the specific models used, emphasizing the importance of comprehensive datasets in understanding scaling behavior.'}, 'zh': {'title': 'Êé¢Á¥¢Áº©ÊîæÊ≥ïÂàôÁöÑÂ§öÊ†∑ÊÄß‰∏éÊïèÊÑüÊÄß', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÁº©ÊîæÊ≥ïÂàôÔºå‰ΩøÁî®‰∫ÜÂ§öÁßçÊû∂ÊûÑÂíåË∂ÖÂèÇÊï∞ÈÄâÊã©ÔºåÂº∫Ë∞É‰∫ÜËøô‰∫õÈÄâÊã©ÂØπÁªìÊûúÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÂèëÂ∏É‰∫ÜGemstonesÊï∞ÊçÆÈõÜÔºåËøôÊòØËøÑ‰ªä‰∏∫Ê≠¢ÊúÄÂÖ®Èù¢ÁöÑÂºÄÊ∫êÁº©ÊîæÊ≥ïÂàôÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Ë∂ÖËøá4000‰∏™Êù•Ëá™ÂèòÊç¢Âô®Ê®°ÂûãÁöÑÊ£ÄÊü•ÁÇπÔºåÂèÇÊï∞ÈáèÈ´òËææ20‰∫ø„ÄÇÈÄöËøáËøô‰∫õÊ£ÄÊü•ÁÇπÔºåÊàë‰ª¨ËÉΩÂ§üËøõË°åÊõ¥Â§çÊùÇÁöÑÁº©ÊîæÁ†îÁ©∂Ôºå‰æãÂ¶ÇÈ¢ÑÊµãËØ≠Ë®ÄÂª∫Ê®°ÊÄßËÉΩ‰∏éÊ®°ÂûãÂÆΩÂ∫¶ÂíåÊ∑±Â∫¶ÁöÑÂÖ≥Á≥ª„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁº©ÊîæÊ≥ïÂàôÁöÑÈÄÇÁî®ÊÄßÂØπÂÆûÈ™åËÆæËÆ°ËøáÁ®ãÂíå‰ΩøÁî®ÁöÑÁâπÂÆöÊ®°ÂûãÊ£ÄÊü•ÁÇπÈùûÂ∏∏ÊïèÊÑü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03492', 'title': 'Teaching Language Models to Critique via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.03492', 'abstract': 'Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose CTRL, a framework for Critic Training via Reinforcement Learning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with CTRL significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.', 'score': 14, 'issue_id': 2165, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'a0c98706806837c6', 'authors': ['Zhihui Xie', 'Jie chen', 'Liyu Chen', 'Weichao Mao', 'Jingjing Xu', 'Lingpeng Kong'], 'affiliations': ['Bytedance, Seed', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.03492.jpg', 'data': {'categories': ['#rlhf', '#training', '#benchmark', '#reasoning', '#rl', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': '–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∏–µ—Å—è –ò–ò-–∫—Ä–∏—Ç–∏–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CTRL - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π-–∫—Ä–∏—Ç–∏–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –¶–µ–ª—å - –Ω–∞—É—á–∏—Ç—å —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–µ–∑–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –∫–æ–¥–∞ –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–∏–∫–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—é—Ç –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∏ —É–º–µ–Ω—å—à–∞—é—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫. –ú–æ–¥–µ–ª–∏-–∫—Ä–∏—Ç–∏–∫–∏ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —É–ª—É—á—à–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∫—Ä–∏—Ç–∏–∫—É –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering Code Generation with Self-Critiquing Models', 'desc': 'This paper introduces CTRL, a framework that trains large language models (LLMs) to act as critics for code generation tasks. By using reinforcement learning, CTRL enables these critic models to provide feedback that helps improve the performance of a fixed generator model without needing human input. The study shows that critics trained with CTRL can significantly increase the success rates of code generation and reduce errors. Additionally, these critics function as effective generative reward models, allowing for iterative improvements during testing, leading to substantial performance gains on difficult benchmarks.'}, 'zh': {'title': 'ÈÄöËøáÊâπËØÑËÆ≠ÁªÉÊèêÂçá‰ª£Á†ÅÁîüÊàêÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÁîüÊàê‰∏≠ÁöÑÊâπËØÑÂíåÊîπËøõËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCTRLÊ°ÜÊû∂ÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊâπËØÑÊ®°ÂûãÔºåÁîüÊàêÂèçÈ¶à‰ª•ÊèêÈ´òÂõ∫ÂÆöÁîüÊàêÊ®°ÂûãÁöÑ‰øÆÊ≠£ÊÄßËÉΩÔºåËÄåÊó†ÈúÄ‰∫∫Â∑•ÁõëÁù£„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®CTRLËÆ≠ÁªÉÁöÑÊâπËØÑÊ®°ÂûãÊòæËëóÊèêÈ´ò‰∫ÜÈÄöËøáÁéáÔºåÂπ∂ÂáèÂ∞ë‰∫ÜÁ¥ØÁßØÈîôËØØ„ÄÇÊ≠§Â§ñÔºåËøô‰∫õÊâπËØÑÊ®°Âûã‰Ωú‰∏∫ÁîüÊàêÂ•ñÂä±Ê®°ÂûãÔºåËÉΩÂ§üÂú®ÊµãËØïÊó∂ÈÄöËøáËø≠‰ª£ÊâπËØÑ-‰øÆËÆ¢ÂÆûÁé∞Êâ©Â±ïÔºåÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜ‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ106.1%ÁöÑÁõ∏ÂØπÊîπËøõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07617', 'title': 'Scaling Pre-training to One Hundred Billion Data for Vision Language Models', 'url': 'https://huggingface.co/papers/2502.07617', 'abstract': "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.", 'score': 13, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '503a9dac2cae323c', 'authors': ['Xiao Wang', 'Ibrahim Alabdulmohsin', 'Daniel Salz', 'Zhe Li', 'Keran Rong', 'Xiaohua Zhai'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.07617.jpg', 'data': {'categories': ['#cultural_diversity', '#multilingual', '#dataset', '#low_resource', '#data', '#multimodal', '#benchmark'], 'emoji': 'üåç', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∏–Ω–∫–ª—é–∑–∏–≤–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–µ—Å–ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–Ω–æ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –≤ 100 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –º–Ω–æ–≥–∏—Ö –∑–∞–ø–∞–¥–Ω–æ—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –Ω–∞—Å—ã—â–∞–µ—Ç—Å—è –ø—Ä–∏ —Ç–∞–∫–æ–º –º–∞—Å—à—Ç–∞–±–µ. –û–¥–Ω–∞–∫–æ –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∫—É–ª—å—Ç—É—Ä–Ω—ã–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º, –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –±–ª–∞–≥–æ–¥–∞—Ä—è –æ—Ö–≤–∞—Ç—É —Ä–µ–¥–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –∏ –ø—Ä–µ–¥–æ—Å—Ç–µ—Ä–µ–≥–∞–µ—Ç –æ—Ç —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∫—É–ª—å—Ç—É—Ä–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ.'}, 'en': {'title': 'Unlocking Cultural Diversity with 100 Billion Examples', 'desc': 'This paper investigates the effects of pre-training vision-language models using a massive dataset of 100 billion examples. The authors find that while performance on common benchmarks tends to plateau, tasks that involve cultural diversity show significant improvements due to the extensive coverage of diverse concepts in the dataset. Additionally, the study highlights the benefits of this large-scale data for enhancing multilingual capabilities, particularly for low-resource languages. However, it also warns that applying quality filters to reduce dataset size can diminish the representation of cultural diversity, emphasizing the importance of large-scale data for inclusive multimodal systems.'}, 'zh': {'title': 'Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÂä©ÂäõÊñáÂåñÂ§öÊ†∑ÊÄß', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®ÂâçÊâÄÊú™ÊúâÁöÑËßÑÊ®°‰∏äÔºà1000‰∫ø‰∏™Á§∫‰æãÔºâÂØπËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãËøõË°åÈ¢ÑËÆ≠ÁªÉÁöÑÊΩúÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂú®ËÆ∏Â§öÂ∏∏ËßÅÁöÑË•øÊñπÂàÜÁ±ªÂíåÊ£ÄÁ¥¢Âü∫ÂáÜ‰∏äÔºåÊ®°ÂûãÊÄßËÉΩÂú®Ê≠§ËßÑÊ®°‰∏ãË∂ã‰∫éÈ•±Âíå„ÄÇÁÑ∂ËÄåÔºåÂØπ‰∫éÊñáÂåñÂ§öÊ†∑ÊÄßÁöÑ‰ªªÂä°Ôºå1000‰∫øËßÑÊ®°ÁöÑÁΩëÁªúÊï∞ÊçÆÂ∏¶Êù•‰∫ÜÊõ¥ÊòæËëóÁöÑÊèêÂçáÔºåÂõ†‰∏∫ÂÆÉÊ∂µÁõñ‰∫ÜÈïøÂ∞æÊ¶ÇÂøµ„ÄÇÊ≠§Â§ñÔºåÁ†îÁ©∂ËøòÂàÜÊûê‰∫ÜÊ®°ÂûãÁöÑÂ§öËØ≠Ë®ÄËÉΩÂäõÔºåÊòæÁ§∫Âú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏ä‰πüÊúâÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07508', 'title': 'Enhance-A-Video: Better Generated Video for Free', 'url': 'https://huggingface.co/papers/2502.07508', 'abstract': 'DiT-based video generation has achieved remarkable results, but research into enhancing existing models remains relatively unexplored. In this work, we introduce a training-free approach to enhance the coherence and quality of DiT-based generated videos, named Enhance-A-Video. The core idea is enhancing the cross-frame correlations based on non-diagonal temporal attention distributions. Thanks to its simple design, our approach can be easily applied to most DiT-based video generation frameworks without any retraining or fine-tuning. Across various DiT-based video generation models, our approach demonstrates promising improvements in both temporal consistency and visual quality. We hope this research can inspire future explorations in video generation enhancement.', 'score': 12, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': 'e02d3082d3b21016', 'authors': ['Yang Luo', 'Xuanlei Zhao', 'Mengzhao Chen', 'Kaipeng Zhang', 'Wenqi Shao', 'Kai Wang', 'Zhangyang Wang', 'Yang You'], 'affiliations': ['National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.07508.jpg', 'data': {'categories': ['#video', '#training'], 'emoji': 'üé¨', 'ru': {'title': 'Enhance-A-Video: –ü–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ DiT (Diffusion Transformer), –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π Enhance-A-Video, —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —É—Å–∏–ª–µ–Ω–∏–∏ –º–µ–∂–∫–∞–¥—Ä–æ–≤—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –ü–æ–¥—Ö–æ–¥ –ª–µ–≥–∫–æ –ø—Ä–∏–º–µ–Ω–∏–º –∫ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ DiT –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, —Ç–∞–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Enhancing DiT Video Generation Without Retraining', 'desc': 'This paper presents a novel method called Enhance-A-Video, aimed at improving the coherence and quality of videos generated by DiT-based models. The approach focuses on enhancing cross-frame correlations using non-diagonal temporal attention distributions, which helps maintain consistency across frames. Importantly, Enhance-A-Video does not require any retraining or fine-tuning, making it easy to integrate into existing DiT frameworks. The results show significant improvements in both temporal consistency and visual quality, paving the way for further advancements in video generation techniques.'}, 'zh': {'title': 'ÊèêÂçáËßÜÈ¢ëÁîüÊàêË¥®ÈáèÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ Enhance-A-Video ÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÂü∫‰∫é DiT ÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑËøûË¥ØÊÄßÂíåË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ¢ûÂº∫Ë∑®Â∏ßÁõ∏ÂÖ≥ÊÄßÔºåÂà©Áî®ÈùûÂØπËßíÊó∂Èó¥Ê≥®ÊÑèÂäõÂàÜÂ∏ÉÊù•ÂÆûÁé∞„ÄÇÁî±‰∫éÂÖ∂ËÆæËÆ°ÁÆÄÂçïÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•ËΩªÊùæÂ∫îÁî®‰∫éÂ§ßÂ§öÊï∞Âü∫‰∫é DiT ÁöÑËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂ÔºåËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉÊàñÂæÆË∞É„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êó∂Èó¥‰∏ÄËá¥ÊÄßÂíåËßÜËßâË¥®ÈáèÊñπÈù¢ÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊîπÂñÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06589', 'title': 'Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training', 'url': 'https://huggingface.co/papers/2502.06589', 'abstract': 'Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.', 'score': 10, 'issue_id': 2164, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '4273eabfdc59b328', 'authors': ['Yuchen Zhuang', 'Jingfeng Yang', 'Haoming Jiang', 'Xin Liu', 'Kewei Cheng', 'Sanket Lokegaonkar', 'Yifan Gao', 'Qing Ping', 'Tianyi Liu', 'Binxuan Huang', 'Zheng Li', 'Zhengyang Wang', 'Pei Chen', 'Ruijie Wang', 'Rongzhi Zhang', 'Nasser Zalmout', 'Priyanka Nigam', 'Bing Yin', 'Chao Zhang'], 'affiliations': ['Amazon', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.06589.jpg', 'data': {'categories': ['#agents', '#transfer_learning', '#optimization', '#training', '#reasoning', '#dataset', '#benchmark'], 'emoji': 'üõ†Ô∏è', 'ru': {'title': '–ö—É–∑–Ω–∏—Ü–∞ –∞–≥–µ–Ω—Ç–æ–≤: —É–ª—É—á—à–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Hephaestus-Forge - –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –∫–æ—Ä–ø—É—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ö–æ—Ä–ø—É—Å —Å–æ–¥–µ—Ä–∂–∏—Ç 103 –º–∏–ª–ª–∏–∞—Ä–¥–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 76,537 API, –≤–∫–ª—é—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤—ã–∑–æ–≤–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Hephaestus-Forge –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª–∏–ª–æ –º–æ–¥–µ–ª–∏ Hephaestus –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ LLM –º–∞–ª–æ–≥–æ –∏ —Å—Ä–µ–¥–Ω–µ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –∏ –∫–æ–Ω–∫—É—Ä–∏—Ä–æ–≤–∞—Ç—å —Å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ LLM –≤ —Ç—Ä–µ—Ö —Ç–µ—Å—Ç–∞—Ö –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –≤ —É–ª—É—á—à–µ–Ω–∏–∏ –±–∞–∑–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á –∏ —Å—Ä–µ–¥.'}, 'en': {'title': 'Empowering LLM Agents with Hephaestus-Forge', 'desc': 'This paper presents Hephaestus-Forge, a large-scale pre-training dataset specifically designed for enhancing the capabilities of large language model (LLM) agents. It includes 103 billion agent-specific data points, featuring 76,537 APIs, which provide both documentation and function calling examples to improve reasoning and planning skills. The authors explore different training protocols and data mixing ratios to optimize the pre-training process. Results show that agents trained on Hephaestus-Forge outperform smaller open-source LLMs and compete with commercial models, highlighting its effectiveness in improving agent performance and adaptability.'}, 'zh': {'title': 'Hephaestus-ForgeÔºöÊèêÂçáLLM‰ª£ÁêÜËÉΩÂäõÁöÑÂàõÊñ∞È¢ÑËÆ≠ÁªÉËØ≠ÊñôÂ∫ì', 'desc': 'Áî±‰∫éÁº∫‰πèÈù¢Âêë‰ª£ÁêÜÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËá™‰∏ª‰ª£ÁêÜÈÄöÂ∏∏‰æùËµñÂ§çÊùÇÁöÑÊèêÁ§∫ÊàñÂπøÊ≥õÁöÑÂæÆË∞ÉÔºåËøôÂæÄÂæÄÊó†Ê≥ïÂú®‰øùÊåÅÂº∫Ê≥õÂåñËÉΩÂäõÁöÑÂêåÊó∂ÂºïÂÖ•Êñ∞ÂäüËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜHephaestus-ForgeÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉËØ≠ÊñôÂ∫ìÔºåÊó®Âú®Â¢ûÂº∫LLM‰ª£ÁêÜÂú®APIÂäüËÉΩË∞ÉÁî®„ÄÅÂÜÖÂú®Êé®ÁêÜÂíåËßÑÂàí‰ª•ÂèäÈÄÇÂ∫îÁéØÂ¢ÉÂèçÈ¶àÊñπÈù¢ÁöÑÂü∫Êú¨ËÉΩÂäõ„ÄÇHephaestus-ForgeÂåÖÂê´1030‰∫ø‰∏™ÁâπÂÆö‰∫é‰ª£ÁêÜÁöÑÊï∞ÊçÆÔºåÊ∂µÁõñ76,537‰∏™APIÔºåÂåÖÊã¨Â∑•ÂÖ∑ÊñáÊ°£‰ª•‰ªãÁªçAPIÂäüËÉΩÁöÑÁü•ËØÜÂíåÂäüËÉΩË∞ÉÁî®ËΩ®Ëøπ‰ª•Â¢ûÂº∫ÂÜÖÂú®Êé®ÁêÜ„ÄÇÈÄöËøáÂú®Hephaestus-Forge‰∏äÊåÅÁª≠È¢ÑËÆ≠ÁªÉÔºåHephaestusÂú®‰∏â‰∏™‰ª£ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÂ∞èÂà∞‰∏≠ÂûãÁöÑÂºÄÊ∫êLLMÔºåÂπ∂‰∏éÂïÜ‰∏öLLMÁõ∏Â™≤ÁæéÔºåËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠ÊñôÂ∫ìÂú®Â¢ûÂº∫‰ª£ÁêÜÂü∫Êú¨ËÉΩÂäõÂíåLLMÂØπÊñ∞‰ªªÂä°ÊàñÁéØÂ¢ÉÁöÑÊ≥õÂåñËÉΩÂäõÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04223', 'title': '√âclair -- Extracting Content and Layout with Integrated Reading Order for Documents', 'url': 'https://huggingface.co/papers/2502.04223', 'abstract': "Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure -- including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages -- as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce \\'Eclair, a general-purpose text-extraction tool specifically designed to process a wide range of document types. Given an image, \\'Eclair is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark for document-level OCR and semantic classification. \\'Eclair achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate \\'Eclair on established benchmarks, demonstrating its versatility and strength across several evaluation standards.", 'score': 9, 'issue_id': 2170, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '98e15ce7f5732b9f', 'authors': ['Ilia Karmanov', 'Amala Sanjay Deshmukh', 'Lukas Voegtle', 'Philipp Fischer', 'Kateryna Chumachenko', 'Timo Roman', 'Jarno Sepp√§nen', 'Jupinder Parmar', 'Joseph Jennings', 'Andrew Tao', 'Karan Sapra'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2502.04223.jpg', 'data': {'categories': ['#benchmark', '#science', '#data', '#dataset', '#optimization', '#cv'], 'emoji': 'üìÑ', 'ru': {'title': '√âclair: –ü–µ—Ä–µ–¥–æ–≤–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç OCR –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç √âclair - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. √âclair –Ω–µ —Ç–æ–ª—å–∫–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç, –Ω–æ –∏ –ø–æ–Ω–∏–º–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, —Ñ–æ—Ä–º—É–ª—ã, —Ç–∞–±–ª–∏—Ü—ã –∏ –ø–æ—Ä—è–¥–æ–∫ —á—Ç–µ–Ω–∏—è. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ç–∞–∫–∂–µ —Å–ø–æ—Å–æ–±–µ–Ω –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ —Å–Ω–æ—Å–∫–∏ –∏ –ø–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ OCR –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º √âclair –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.'}, 'en': {'title': 'Eclair: Revolutionizing Document Understanding with Advanced OCR', 'desc': "This paper presents 'Eclair', an advanced Optical Character Recognition (OCR) tool designed to extract not just text, but also the structural and semantic elements of complex documents. It recognizes formatting, tables, and reading order, which are essential for understanding multi-page documents. 'Eclair' provides bounding boxes and semantic classes for extracted text, enhancing its utility for tasks like document retrieval and question answering. The tool demonstrates state-of-the-art performance on a newly introduced benchmark, showcasing its effectiveness compared to existing methods."}, 'zh': {'title': 'EclairÔºöÂÖ®Èù¢ÁêÜËß£ÊñáÊ°£ÁöÑOCRÂ∑•ÂÖ∑', 'desc': "ÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´ÔºàOCRÔºâÊäÄÊúØÂπøÊ≥õÂ∫îÁî®‰∫é‰ªéÊñáÊ°£ÂõæÂÉè‰∏≠ÊèêÂèñÊñáÊú¨Ôºå‰øÉËøõÈ´òÊïàÁöÑÊï∞Â≠óÂåñÂíåÊï∞ÊçÆÊ£ÄÁ¥¢„ÄÇÁÑ∂ËÄåÔºå‰ªÖ‰ªÖÊèêÂèñÊñáÊú¨ÂØπ‰∫éÂ§ÑÁêÜÂ§çÊùÇÊñáÊ°£ÊòØ‰∏çÂ§üÁöÑ„ÄÇÂÖ®Èù¢ÁêÜËß£Ëøô‰∫õÊñáÊ°£ÈúÄË¶Å‰∫ÜËß£ÂÖ∂ÁªìÊûÑÔºåÂåÖÊã¨Ê†ºÂºè„ÄÅÂÖ¨Âºè„ÄÅË°®Ê†º‰ª•ÂèäË∑®Â§ö‰∏™È°µÈù¢ÁöÑÂ§ö‰∏™ÂùóÂíåÂàóÁöÑÈòÖËØªÈ°∫Â∫èÔºå‰ª•ÂèäÊ£ÄÊµãËÑöÊ≥®ÂíåÂõæÂÉèÊ†áÈ¢òÁ≠âÂÖÉÁ¥†ÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü'Eclair'ÔºåËøôÊòØ‰∏ÄÁßçÈÄöÁî®ÁöÑÊñáÊú¨ÊèêÂèñÂ∑•ÂÖ∑Ôºå‰∏ìÈó®ËÆæËÆ°Áî®‰∫éÂ§ÑÁêÜÂêÑÁßçÊñáÊ°£Á±ªÂûãÔºåÂπ∂Âú®ÊñáÊ°£Á∫ßOCRÂíåËØ≠‰πâÂàÜÁ±ªÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÊÄß„ÄÇ"}}}, {'id': 'https://huggingface.co/papers/2502.03997', 'title': 'CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing', 'url': 'https://huggingface.co/papers/2502.03997', 'abstract': 'Computer Aided Design (CAD) is indispensable across various industries. Text-based CAD editing, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce CAD-Editor, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.', 'score': 8, 'issue_id': 2165, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '69bfc4de9cf7106d', 'authors': ['Yu Yuan', 'Shizhao Sun', 'Qi Liu', 'Jiang Bian'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.03997.jpg', 'data': {'categories': ['#data', '#dataset', '#multimodal', '#architecture'], 'emoji': 'üõ†Ô∏è', 'ru': {'title': 'CAD-Editor: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ CAD-–º–æ–¥–µ–ª–µ–π', 'desc': "CAD-Editor - —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è CAD-–º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ—á–µ—Ç–∞—è –º–æ–¥–µ–ª–∏ –≤–∞—Ä–∏–∞—Ü–∏–π –¥–∏–∑–∞–π–Ω–∞ –∏ –±–æ–ª—å—à–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ–¥—Ö–æ–¥ 'locate-then-infill', —Ä–∞–∑–±–∏–≤–∞—è –∑–∞–¥–∞—á—É –Ω–∞ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é –æ–±–ª–∞—Å—Ç–µ–π –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –∏—Ö –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –ø—Ä–∞–≤–∫–∞–º–∏. CAD-Editor –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –∑–Ω–∞–Ω–∏–π –æ CAD, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö."}, 'en': {'title': 'Revolutionizing CAD Editing with Text Instructions', 'desc': 'This paper presents CAD-Editor, a novel framework for text-based editing of Computer Aided Design (CAD) models. It addresses the limitations of existing methods by integrating automated data synthesis and Large Vision-Language Models (LVLMs) to generate editing instructions from original and modified CAD models. The framework employs a locate-then-infill approach, breaking down the editing process into identifying areas for change and applying the necessary modifications. Experimental results demonstrate that CAD-Editor outperforms previous techniques in both quantitative metrics and qualitative assessments.'}, 'zh': {'title': 'ÊñáÊú¨È©±Âä®ÁöÑCADÁºñËæëÊñ∞Á∫™ÂÖÉ', 'desc': 'ËÆ°ÁÆóÊú∫ËæÖÂä©ËÆæËÆ°ÔºàCADÔºâÂú®ÂêÑ‰∏™Ë°å‰∏ö‰∏≠Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÂü∫‰∫éÊñáÊú¨ÁöÑCADÁºñËæëÂèØ‰ª•Ê†πÊçÆÊñáÊú¨Êåá‰ª§Ëá™Âä®‰øÆÊîπCADÊ®°ÂûãÔºå‰ΩÜËøô‰∏ÄÈ¢ÜÂüüÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®ËÆæËÆ°Âèò‰ΩìÁîüÊàêÊàñÂü∫‰∫éÊñáÊú¨ÁöÑCADÁîüÊàêÔºåÁº∫‰πèÂØπÊñáÊú¨ÊéßÂà∂ÁöÑÊîØÊåÅÊàñÂøΩËßÜ‰∫ÜÁé∞ÊúâCADÊ®°ÂûãÁöÑÁ∫¶Êùü„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCAD-EditorÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Áî®‰∫éÂü∫‰∫éÊñáÊú¨ÁöÑCADÁºñËæëÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåËá™Âä®ÂåñÊï∞ÊçÆÂêàÊàêÁÆ°ÈÅìÔºåÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑÁºñËæëÊåá‰ª§ÁîüÊàêÂíåÊ®°Âûã‰øÆÊîπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07527', 'title': 'NatureLM: Deciphering the Language of Nature for Scientific Discovery', 'url': 'https://huggingface.co/papers/2502.07527', 'abstract': 'Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.', 'score': 8, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': 'a6e947f52bde9a9c', 'authors': ['Yingce Xia', 'Peiran Jin', 'Shufang Xie', 'Liang He', 'Chuan Cao', 'Renqian Luo', 'Guoqing Liu', 'Yue Wang', 'Zequn Liu', 'Yuan-Jyue Chen', 'Zekun Guo', 'Yeqi Bai', 'Pan Deng', 'Yaosen Min', 'Ziheng Lu', 'Hongxia Hao', 'Han Yang', 'Jielan Li', 'Chang Liu', 'Jia Zhang', 'Jianwei Zhu', 'Kehan Wu', 'Wei Zhang', 'Kaiyuan Gao', 'Qizhi Pei', 'Qian Wang', 'Xixian Liu', 'Yanting Li', 'Houtian Zhu', 'Yeqing Lu', 'Mingqian Ma', 'Zun Wang', 'Tian Xie', 'Krzysztof Maziarz', 'Marwin Segler', 'Zhao Yang', 'Zilong Chen', 'Yu Shi', 'Shuxin Zheng', 'Lijun Wu', 'Chen Hu', 'Peggy Dai', 'Tie-Yan Liu', 'Haiguang Liu', 'Tao Qin'], 'affiliations': ['Microsoft Research AI for Science'], 'pdf_title_img': 'assets/pdf/title_img/2502.07527.jpg', 'data': {'categories': ['#healthcare', '#architecture', '#science', '#optimization', '#transfer_learning', '#training', '#dataset', '#multimodal'], 'emoji': 'üß¨', 'ru': {'title': 'NatureLM: –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π –≤–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–æ–º–µ–Ω–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ NatureLM - —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é —Ä–∞–∑–ª–∏—á–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –¥–æ–º–µ–Ω—ã. –≠—Ç–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±–ª–∞—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è –º–æ–ª–µ–∫—É–ª—ã, –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –±–µ–ª–∫–∏, –î–ù–ö –∏ –†–ù–ö. NatureLM —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤, –∞ —Ç–∞–∫–∂–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∫—Ä–æ—Å—Å-–¥–æ–º–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö, –æ—Ç 1 –¥–æ 46,7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'NatureLM: Unifying Science Through Language Models', 'desc': 'This paper introduces Nature Language Model (NatureLM), a foundation model designed to enhance scientific discovery by integrating knowledge across various scientific domains. Unlike traditional models that operate in isolation, NatureLM is pre-trained on data from multiple fields, allowing it to understand and generate sequences related to small molecules, proteins, RNA, and materials. The model supports diverse applications, such as generating new compounds and optimizing existing ones, while also excelling in specific tasks like translating chemical notations. With different sizes available, NatureLM demonstrates improved performance with larger models, showcasing its potential as a versatile tool in drug discovery and material design.'}, 'zh': {'title': 'Ëá™ÁÑ∂ËØ≠Ë®ÄÊ®°ÂûãÔºöÁßëÂ≠¶ÂèëÁé∞ÁöÑÊñ∞Â∑•ÂÖ∑', 'desc': 'Âü∫Á°ÄÊ®°ÂûãÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíå‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÂ∏¶Êù•‰∫ÜÈù©ÂëΩÊÄßÁöÑÂèòÂåñÔºåÊòæËëóÊèêÂçá‰∫ÜÊú∫Âô®ÁêÜËß£ÂíåÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄÁöÑËÉΩÂäõ„ÄÇÂèóÂü∫Á°ÄÊ®°ÂûãÊàêÂäüÁöÑÂêØÂèëÔºåÁ†îÁ©∂‰∫∫Âëò‰∏∫ÂêÑ‰∏™ÁßëÂ≠¶È¢ÜÂüüÂºÄÂèë‰∫ÜÁõ∏Â∫îÁöÑÂü∫Á°ÄÊ®°ÂûãÔºå‰ΩÜËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏ÊòØÂ≠§Á´ãËÆ≠ÁªÉÁöÑÔºåÁº∫‰πèË∑®È¢ÜÂüüÊï¥ÂêàÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÊ®°ÂûãÔºàNatureLMÔºâÔºåËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éÂ∫èÂàóÁöÑÁßëÂ≠¶Âü∫Á°ÄÊ®°ÂûãÔºåÊó®Âú®‰øÉËøõÁßëÂ≠¶ÂèëÁé∞„ÄÇNatureLMÁªèËøáÂ§öÈ¢ÜÂüüÊï∞ÊçÆÁöÑÈ¢ÑËÆ≠ÁªÉÔºåËÉΩÂ§üÊîØÊåÅÂ∞èÂàÜÂ≠ê„ÄÅËõãÁôΩË¥®„ÄÅRNAÂíåÊùêÊñôÁöÑÁîüÊàê‰∏é‰ºòÂåñÔºåÂπ∂Âú®Â§ö‰∏™ÁßëÂ≠¶‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05364', 'title': 'Hypencoder: Hypernetworks for Information Retrieval', 'url': 'https://huggingface.co/papers/2502.05364', 'abstract': "The vast majority of retrieval models depend on vector inner products to produce a relevance score between a query and a document. This naturally limits the expressiveness of the relevance score that can be employed. We propose a new paradigm, instead of producing a vector to represent the query we produce a small neural network which acts as a learned relevance function. This small neural network takes in a representation of the document, in this paper we use a single vector, and produces a scalar relevance score. To produce the little neural network we use a hypernetwork, a network that produce the weights of other networks, as our query encoder or as we call it a Hypencoder. Experiments on in-domain search tasks show that Hypencoder is able to significantly outperform strong dense retrieval models and has higher metrics then reranking models and models an order of magnitude larger. Hypencoder is also shown to generalize well to out-of-domain search tasks. To assess the extent of Hypencoder's capabilities, we evaluate on a set of hard retrieval tasks including tip-of-the-tongue retrieval and instruction-following retrieval tasks and find that the performance gap widens substantially compared to standard retrieval tasks. Furthermore, to demonstrate the practicality of our method we implement an approximate search algorithm and show that our model is able to search 8.8M documents in under 60ms.", 'score': 5, 'issue_id': 2176, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'c3495b093acff010', 'authors': ['Julian Killingback', 'Hansi Zeng', 'Hamed Zamani'], 'affiliations': ['University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.05364.jpg', 'data': {'categories': ['#benchmark', '#rag', '#optimization'], 'emoji': 'üîç', 'ru': {'title': 'Hypencoder: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–º –ø–æ–∏—Å–∫–µ —Å –ø–æ–º–æ—â—å—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–º—É –ø–æ–∏—Å–∫—É, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Hypencoder. –í–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞, –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ–±–æ–ª—å—à—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, –≤—ã—Å—Ç—É–ø–∞—é—â—É—é –≤ —Ä–æ–ª–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏. –≠—Ç–∞ —Å–µ—Ç—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—ã–¥–∞–µ—Ç —Å–∫–∞–ª—è—Ä–Ω—É—é –æ—Ü–µ–Ω–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Hypencoder –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞.'}, 'en': {'title': 'Revolutionizing Retrieval with Hypencoder: A Neural Network Approach', 'desc': 'This paper introduces a novel approach to document retrieval by replacing traditional vector representations with a small neural network that functions as a learned relevance function. Instead of using vector inner products to score relevance, the proposed Hypencoder generates a scalar relevance score based on document representations. The use of a hypernetwork allows for efficient weight generation for the Hypencoder, leading to superior performance on various retrieval tasks compared to existing dense retrieval and reranking models. Additionally, the Hypencoder demonstrates strong generalization capabilities and can efficiently search large document collections in a short time frame.'}, 'zh': {'title': 'HypencoderÔºöË∂ÖË∂ä‰º†ÁªüÊ£ÄÁ¥¢Ê®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Â§ßÂ§öÊï∞Ê£ÄÁ¥¢Ê®°Âûã‰æùËµñ‰∫éÂêëÈáèÂÜÖÁßØÊù•ÁîüÊàêÊü•ËØ¢ÂíåÊñáÊ°£‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßËØÑÂàÜÔºåËøôÈôêÂà∂‰∫ÜÁõ∏ÂÖ≥ÊÄßËØÑÂàÜÁöÑË°®ËææËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ËåÉÂºèÔºå‰ΩøÁî®Â∞èÂûãÁ•ûÁªèÁΩëÁªú‰Ωú‰∏∫Â≠¶‰π†ÁöÑÁõ∏ÂÖ≥ÊÄßÂáΩÊï∞ÔºåËÄå‰∏çÊòØÁîüÊàêÂêëÈáèÊù•Ë°®Á§∫Êü•ËØ¢„ÄÇËøô‰∏™Â∞èÂûãÁ•ûÁªèÁΩëÁªúÊé•Êî∂ÊñáÊ°£ÁöÑË°®Á§∫ÔºåÂπ∂ËæìÂá∫‰∏Ä‰∏™Ê†áÈáèÁöÑÁõ∏ÂÖ≥ÊÄßËØÑÂàÜ„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåHypencoderÂú®È¢ÜÂüüÂÜÖÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÂº∫Â§ßÁöÑÂØÜÈõÜÊ£ÄÁ¥¢Ê®°ÂûãÔºåÂπ∂‰∏îÂú®‰∏Ä‰∫õÂõ∞ÈöæÁöÑÊ£ÄÁ¥¢‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06428', 'title': 'CoS: Chain-of-Shot Prompting for Long Video Understanding', 'url': 'https://huggingface.co/papers/2502.06428', 'abstract': 'Multi-modal Large Language Models (MLLMs) struggle with long videos due to the need for excessive visual tokens. These tokens exceed massively the context length of MLLMs, resulting in filled by redundant task-irrelevant shots. How to select shots is an unsolved critical problem: sparse sampling risks missing key details, while exhaustive sampling overwhelms the model with irrelevant content, leading to video misunderstanding. To solve this problem, we propose Chain-of-Shot prompting (CoS). The key idea is to frame shot selection as test-time visual prompt optimisation, choosing shots adaptive to video understanding semantic task by optimising shots-task alignment. CoS has two key parts: (1) a binary video summary mechanism that performs pseudo temporal grounding, discovering a binary coding to identify task-relevant shots, and (2) a video co-reasoning module that deploys the binary coding to pair (learning to align) task-relevant positive shots with irrelevant negative shots. It embeds the optimised shot selections into the original video, facilitating a focus on relevant context to optimize long video understanding. Experiments across three baselines and five datasets demonstrate the effectiveness and adaptability of CoS. Code given in https://lwpyh.github.io/CoS.', 'score': 5, 'issue_id': 2173, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '8302e2d276dc5929', 'authors': ['Jian Hu', 'Zixu Cheng', 'Chenyang Si', 'Wei Li', 'Shaogang Gong'], 'affiliations': ['Nanyang Technological University', 'Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2502.06428.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#long_context', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–£–º–Ω—ã–π –≤—ã–±–æ—Ä –∫–∞–¥—Ä–æ–≤ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Chain-of-Shot prompting (CoS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (MLLM). CoS —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≤—ã–±–æ—Ä–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ –ø—É—Ç–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—è –∫–∞–¥—Ä—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–¥–∞—á–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –±–∏–Ω–∞—Ä–Ω–æ–≥–æ –æ–±–æ–±—â–µ–Ω–∏—è –≤–∏–¥–µ–æ –∏ –º–æ–¥—É–ª—å —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–∞–¥—Ä—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –ø—è—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞.'}, 'en': {'title': 'Optimizing Video Understanding with Chain-of-Shot Prompting', 'desc': "This paper addresses the challenge that Multi-modal Large Language Models (MLLMs) face when processing long videos, which often contain too many visual tokens. These tokens can overwhelm the model with irrelevant information, making it difficult to understand the video's content. The authors propose a method called Chain-of-Shot prompting (CoS) that optimizes shot selection based on the specific task at hand, improving the alignment between selected shots and the semantic understanding required. CoS includes a binary video summary mechanism and a video co-reasoning module to enhance the model's focus on relevant shots, leading to better video comprehension."}, 'zh': {'title': '‰ºòÂåñÈïúÂ§¥ÈÄâÊã©ÔºåÊèêÂçáËßÜÈ¢ëÁêÜËß£', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§ÑÁêÜÈïøËßÜÈ¢ëÊó∂Èù¢‰∏¥ÊåëÊàòÔºåÂõ†‰∏∫ÈúÄË¶ÅËøáÂ§öÁöÑËßÜËßâÊ†áËÆ∞„ÄÇËøô‰∫õÊ†áËÆ∞Ë∂ÖÂá∫‰∫ÜMLLMsÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÔºåÂØºËá¥Â°´ÂÖÖ‰∫ÜÂ§ßÈáè‰∏é‰ªªÂä°Êó†ÂÖ≥ÁöÑÈïúÂ§¥„ÄÇÂ¶Ç‰ΩïÈÄâÊã©ÈïúÂ§¥ÊòØ‰∏Ä‰∏™Êú™Ëß£ÂÜ≥ÁöÑÂÖ≥ÈîÆÈóÆÈ¢òÔºöÁ®ÄÁñèÈááÊ†∑ÂèØËÉΩ‰ºöÈîôËøáÂÖ≥ÈîÆÁªÜËäÇÔºåËÄåÂÖ®Èù¢ÈááÊ†∑Âàô‰ºö‰ΩøÊ®°ÂûãË¢´Êó†ÂÖ≥ÂÜÖÂÆπÊ∑πÊ≤°Ôºå‰ªéËÄåÂØºËá¥ËßÜÈ¢ëÁêÜËß£ÈîôËØØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈïúÂ§¥ÈìæÊèêÁ§∫ÔºàCoSÔºâÔºåÈÄöËøá‰ºòÂåñÈïúÂ§¥‰∏é‰ªªÂä°ÁöÑÂØπÈΩêÊù•ÈÄâÊã©ÈÄÇÂêàËßÜÈ¢ëÁêÜËß£ÁöÑÈïúÂ§¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07490', 'title': 'Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More', 'url': 'https://huggingface.co/papers/2502.07490', 'abstract': "Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.", 'score': 5, 'issue_id': 2172, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '2d86c4124095af30', 'authors': ['Xialie Zhuang', 'Zhikai Jia', 'Jianjin Li', 'Zhenyu Zhang', 'Li Shen', 'Zheng Cao', 'Shiwei Liu'], 'affiliations': ['SCITIX (SGP) TECH PTE. LTD., Singapore', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, China', 'South China Normal University, China', 'Sun YatSen University, China', 'University of Oxford, UK', 'University of Texas at Austin, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.07490.jpg', 'data': {'categories': ['#training', '#reasoning', '#long_context', '#architecture', '#optimization'], 'emoji': 'üé≠', 'ru': {'title': 'MEAP: –£–ª—É—á—à–µ–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MEAP (Mask-Enhanced Autoregressive Prediction). MEAP –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —è–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (MLM) —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ (NTP) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ MEAP –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç NTP –≤ –∑–∞–¥–∞—á–∞—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –ø–æ—Ç–µ—Ä–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Enhancing Information Retrieval in LLMs with MEAP', 'desc': 'This paper introduces Mask-Enhanced Autoregressive Prediction (MEAP), a new training method for Large Language Models (LLMs) that improves their ability to retrieve important information. MEAP combines Masked Language Modeling (MLM) with Next-Token Prediction (NTP) by masking some input tokens and then predicting the next token using a decoder-only Transformer. This approach avoids the complexity of bidirectional attention and encoder-decoder structures, making it computationally efficient during training and inference. Experimental results show that MEAP significantly enhances performance in information retrieval and reasoning tasks, especially in scenarios where context is crucial, while also benefiting supervised fine-tuning.'}, 'zh': {'title': 'Êé©Á†ÅÂ¢ûÂº∫Ëá™ÂõûÂΩíÈ¢ÑÊµãÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ø°ÊÅØÊ£ÄÁ¥¢ËÉΩÂäõ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂáÜÁ°ÆÊ£ÄÁ¥¢ÂÖ≥ÈîÆ‰ø°ÊÅØÊñπÈù¢Â≠òÂú®ÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Êé©Á†ÅÂ¢ûÂº∫Ëá™ÂõûÂΩíÈ¢ÑÊµãÔºàMEAPÔºâÁöÑËÆ≠ÁªÉËåÉÂºèÔºåÂÆÉÂ∞ÜÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÔºàMLMÔºâ‰∏é‰∏ã‰∏Ä‰∏™Ê†áËÆ∞È¢ÑÊµãÔºàNTPÔºâÊó†ÁºùÁªìÂêàÔºå‰ª•Â¢ûÂº∫ÂêéËÄÖÁöÑ‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢ËÉΩÂäõ„ÄÇMEAPÈÄöËøáÈöèÊú∫Êé©ÁõñËæìÂÖ•Ê†áËÆ∞ÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜÔºåÁÑ∂Âêé‰ΩøÁî®‰ªÖËß£Á†ÅÂô®ÁöÑTransformerÁõ¥Êé•ËøõË°åÊ†áÂáÜÁöÑ‰∏ã‰∏Ä‰∏™Ê†áËÆ∞È¢ÑÊµã„ÄÇÂÆûÈ™åË°®ÊòéÔºåMEAPÂú®ÂÖ≥ÈîÆ‰ø°ÊÅØÊ£ÄÁ¥¢ÂíåÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜ‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éNTPÔºåÂêåÊó∂Âú®Â∏∏ËØÜÊé®ÁêÜ‰ªªÂä°‰∏äË°®Áé∞Áõ∏ÂΩìÊàñÊõ¥Â•Ω„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07531', 'title': 'VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.07531', 'abstract': 'Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.', 'score': 5, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': 'dea5fd89dd98f3b1', 'authors': ['Sixiao Zheng', 'Zimian Peng', 'Yanpeng Zhou', 'Yi Zhu', 'Hang Xu', 'Xiangru Huang', 'Yanwei Fu'], 'affiliations': ['Fudan University, China', 'Huawei Noahs Ark Lab, China', 'Westlake University, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07531.jpg', 'data': {'categories': ['#video', '#training', '#open_source', '#dataset', '#benchmark', '#synthetic'], 'emoji': 'üé•', 'ru': {'title': '–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –∫–ª—é—á–µ–≤—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'VidCRAFT3 - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –¥–≤–∏–∂–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã, –¥–≤–∏–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å–≤–µ—â–µ–Ω–∏—è. –í –æ—Å–Ω–æ–≤–µ —Å–∏—Å—Ç–µ–º—ã –ª–µ–∂–∏—Ç Spatial Triple-Attention Transformer, –∫–æ—Ç–æ—Ä—ã–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Å–≤–µ—â–µ–Ω–∏–∏, —Ç–µ–∫—Å—Ç–µ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –±—ã–ª —Å–æ–∑–¥–∞–Ω —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç VideoLightingDirection —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Å–≤–µ—â–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–æ–π—Ç–∏—Å—å –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö —Å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –≤—Å–µ—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'VidCRAFT3: Mastering Multi-Element Control in Image-to-Video Generation', 'desc': 'This paper presents VidCRAFT3, a new framework for generating videos from images with enhanced control over multiple visual elements, including camera motion, object motion, and lighting direction. The authors introduce the Spatial Triple-Attention Transformer, which allows for better separation and management of these elements during the generation process. To support this framework, they created the VideoLightingDirection (VLD) dataset, which includes detailed lighting annotations to improve the realism of generated videos. The proposed three-stage training strategy enables effective learning without requiring simultaneous annotations for all visual elements, leading to superior video quality and coherence compared to existing methods.'}, 'zh': {'title': 'VidCRAFT3ÔºöÂ§öÂÖÉÁ¥†ÊéßÂà∂ÁöÑÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂VidCRAFT3ÔºåËÉΩÂ§üÂêåÊó∂ÊéßÂà∂Áõ∏Êú∫ËøêÂä®„ÄÅÁâ©‰ΩìËøêÂä®ÂíåÂÖâÁÖßÊñπÂêë„ÄÇ‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞Ëß£ËÄ¶ÊØè‰∏™ËßÜËßâÂÖÉÁ¥†ÁöÑÊéßÂà∂ÔºåÊèêÂá∫‰∫ÜÁ©∫Èó¥‰∏âÈáçÊ≥®ÊÑèÂäõÂèòÊç¢Âô®ÔºåËÉΩÂ§üÂØπÂÖâÁÖßÊñπÂêë„ÄÅÊñáÊú¨ÂíåÂõæÂÉèËøõË°åÂØπÁß∞Êï¥Âêà„ÄÇÁî±‰∫éÂ§ßÂ§öÊï∞ÁúüÂÆû‰∏ñÁïåËßÜÈ¢ëÊï∞ÊçÆÈõÜÁº∫‰πèÂÖâÁÖßÊ≥®ÈáäÔºåÁ†îÁ©∂ËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÂêàÊàêËßÜÈ¢ëÊï∞ÊçÆÈõÜVideoLightingDirectionÔºàVLDÔºâÔºåËØ•Êï∞ÊçÆÈõÜÂåÖÂê´ÂÖâÁÖßÊñπÂêëÊ≥®ÈáäÂíåÂ§öÊ†∑ÂåñÂ§ñËßÇÁöÑÁâ©‰Ωì„ÄÇÈÄöËøáÂπøÊ≥õÁöÑÂÆûÈ™åÔºåVidCRAFT3Âú®ÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ëÂÜÖÂÆπÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07445', 'title': 'Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon', 'url': 'https://huggingface.co/papers/2502.07445', 'abstract': "Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.", 'score': 4, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '3e6282dd3913750a', 'authors': ['Nurit Cohen-Inger', 'Yehonatan Elisha', 'Bracha Shapira', 'Lior Rokach', 'Seffi Cohen'], 'affiliations': ['Ben Gurion University', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07445.jpg', 'data': {'categories': ['#interpretability', '#training', '#dataset', '#hallucinations', '#benchmark', '#optimization'], 'emoji': 'ü¶é', 'ru': {'title': '–†–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ –∏–ª–ª—é–∑–∏–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è: –∫–∞–∫ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–∞—Å–∫–∏—Ä—É—é—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º C-BOD. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤—ã—è–≤–ª—è–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–∫–∞–∂–µ–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –º–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è –∫—Ä—É–ø–Ω—ã–µ –Ø–ú, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–∑—ã–≤–∞—é—Ç —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —É–¥–µ–ª—è—Ç—å –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º –≤ —Ä–µ–π—Ç–∏–Ω–≥–∞—Ö.'}, 'en': {'title': 'Beyond Scores: Evaluating True Language Understanding in LLMs', 'desc': "This paper discusses the limitations of large language models (LLMs) in truly understanding language, as they often rely on specific patterns in datasets rather than genuine comprehension. The authors introduce the Chameleon Benchmark Overfit Detector (C-BOD), a tool that modifies benchmark prompts to test if LLMs are overfitting to memorized cues. By analyzing the performance of 26 leading LLMs on the MMLU benchmark, they find that many models show a decline in performance when faced with slight changes in input, indicating a reliance on fixed patterns. The study emphasizes the need for better evaluation methods that focus on a model's ability to generalize and understand language, rather than just achieving high scores on benchmarks."}, 'zh': {'title': 'Ë∂ÖË∂äÂàÜÊï∞ÔºåÂÖ≥Ê≥®Ê®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß‰∏éÊ≥õÂåñËÉΩÂäõ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂÖ¨ÂÖ±Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰ΩÜËøô‰∫õÈ´òÂàÜÂèØËÉΩÊé©Áõñ‰∫ÜÊ®°ÂûãÂØπÁâπÂÆöÊï∞ÊçÆÈõÜË°®Èù¢ÁâπÂæÅÁöÑËøáÂ∫¶‰æùËµñÔºåËÄåÈùûÁúüÊ≠£ÁöÑËØ≠Ë®ÄÁêÜËß£„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÂèòËâ≤ÈæôÂü∫ÂáÜËøáÊãüÂêàÊ£ÄÊµãÂô®ÔºàC-BODÔºâÔºåËøôÊòØ‰∏Ä‰∏™ÈÄöËøáÂèÇÊï∞ÂèòÊç¢Á≥ªÁªüÊÄßÊâ≠Êõ≤Âü∫ÂáÜÊèêÁ§∫ÁöÑÂÖÉËØÑ‰º∞Ê°ÜÊû∂ÔºåÁî®‰∫éÊ£ÄÊµãLLMsÁöÑËøáÊãüÂêà„ÄÇC-BODÈÄöËøáÈáçÊñ∞Ë°®Ëø∞ËæìÂÖ•ÔºåÂêåÊó∂‰øùÊåÅÂÖ∂ËØ≠‰πâÂÜÖÂÆπÂíåÊ†áÁ≠æÔºåÊè≠Á§∫Ê®°ÂûãÊÄßËÉΩÊòØÂê¶ÂèóÂà∞ËÆ∞ÂøÜÊ®°ÂºèÁöÑÈ©±Âä®„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÁªèËøáÈÄÇÂ∫¶Êâ∞Âä®ÂêéÔºå26‰∏™È¢ÜÂÖàÁöÑLLMÂú®MMLUÂü∫ÂáÜ‰∏äÁöÑÂπ≥ÂùáÊÄßËÉΩ‰∏ãÈôç‰∫Ü2.15%ÔºåËøôË°®ÊòéÊ®°ÂûãÂú®ËØÑ‰º∞Êó∂ÈúÄË¶ÅÂÖ≥Ê≥®Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06755', 'title': 'Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models', 'url': 'https://huggingface.co/papers/2502.06755', 'abstract': 'To truly understand vision models, we must not only interpret their learned features but also validate these interpretations through controlled experiments. Current approaches either provide interpretable features without the ability to test their causal influence, or enable model editing without interpretable controls. We present a unified framework using sparse autoencoders (SAEs) that bridges this gap, allowing us to discover human-interpretable visual features and precisely manipulate them to test hypotheses about model behavior. By applying our method to state-of-the-art vision models, we reveal key differences in the semantic abstractions learned by models with different pre-training objectives. We then demonstrate the practical usage of our framework through controlled interventions across multiple vision tasks. We show that SAEs can reliably identify and manipulate interpretable visual features without model re-training, providing a powerful tool for understanding and controlling vision model behavior. We provide code, demos and models on our project website: https://osu-nlp-group.github.io/SAE-V.', 'score': 3, 'issue_id': 2175, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': 'cf5a833e93ca6f88', 'authors': ['Samuel Stevens', 'Wei-Lun Chao', 'Tanya Berger-Wolf', 'Yu Su'], 'affiliations': ['The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06755.jpg', 'data': {'categories': ['#cv', '#architecture', '#optimization', '#open_source', '#interpretability'], 'emoji': 'üî¨', 'ru': {'title': '–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã: –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏ –∫–æ–Ω—Ç—Ä–æ–ª—é –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ (SAE) –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —á–µ–ª–æ–≤–µ–∫–æ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ç–æ—á–Ω–æ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –∏–º–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–∏–ø–æ—Ç–µ–∑ –æ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Å–≤–æ–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –≤—ã—è–≤–∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è—Ö, –∏–∑—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è–º–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–≤–æ–µ–π —Å–∏—Å—Ç–µ–º—ã —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è.'}, 'en': {'title': 'Bridging Interpretation and Control in Vision Models with Sparse Autoencoders', 'desc': 'This paper introduces a new framework that uses sparse autoencoders (SAEs) to interpret and manipulate visual features in vision models. It addresses the challenge of validating the causal influence of these features through controlled experiments. By applying this framework, the authors uncover significant differences in the semantic abstractions learned by various models based on their pre-training objectives. The framework allows for reliable identification and manipulation of interpretable features without needing to retrain the models, enhancing our understanding of their behavior.'}, 'zh': {'title': 'Áî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÁêÜËß£ÂíåÊìçÊéßËßÜËßâÊ®°Âûã', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÊ°ÜÊû∂ÔºåÂà©Áî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâÊù•ÁêÜËß£ËßÜËßâÊ®°ÂûãÁöÑÁâπÂæÅ„ÄÇËØ•Ê°ÜÊû∂‰∏ç‰ªÖÂèØ‰ª•ÂèëÁé∞‰∫∫Á±ªÂèØËß£ÈáäÁöÑËßÜËßâÁâπÂæÅÔºåËøòËÉΩÁ≤æÁ°ÆÊìçÊéßËøô‰∫õÁâπÂæÅÔºå‰ª•ÊµãËØïÊ®°ÂûãË°å‰∏∫ÁöÑÂÅáËÆæ„ÄÇÈÄöËøáÂØπÂÖàËøõËßÜËßâÊ®°ÂûãÁöÑÂ∫îÁî®ÔºåÊàë‰ª¨Êè≠Á§∫‰∫Ü‰∏çÂêåÈ¢ÑËÆ≠ÁªÉÁõÆÊ†áÊ®°ÂûãÊâÄÂ≠¶‰π†ÁöÑËØ≠‰πâÊäΩË±°‰πãÈó¥ÁöÑÂÖ≥ÈîÆÂ∑ÆÂºÇ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåSAEËÉΩÂ§üÂú®‰∏çÈáçÊñ∞ËÆ≠ÁªÉÊ®°ÂûãÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèØÈù†Âú∞ËØÜÂà´ÂíåÊìçÊéßÂèØËß£ÈáäÁöÑËßÜËßâÁâπÂæÅÔºå‰∏∫ÁêÜËß£ÂíåÊéßÂà∂ËßÜËßâÊ®°ÂûãË°å‰∏∫Êèê‰æõ‰∫ÜÂº∫Â§ßÁöÑÂ∑•ÂÖ∑„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07785', 'title': 'Pippo: High-Resolution Multi-View Humans from a Single Image', 'url': 'https://huggingface.co/papers/2502.07785', 'abstract': 'We present Pippo, a generative model capable of producing 1K resolution dense turnaround videos of a person from a single casually clicked photo. Pippo is a multi-view diffusion transformer and does not require any additional inputs - e.g., a fitted parametric model or camera parameters of the input image. We pre-train Pippo on 3B human images without captions, and conduct multi-view mid-training and post-training on studio captured humans. During mid-training, to quickly absorb the studio dataset, we denoise several (up to 48) views at low-resolution, and encode target cameras coarsely using a shallow MLP. During post-training, we denoise fewer views at high-resolution and use pixel-aligned controls (e.g., Spatial anchor and Plucker rays) to enable 3D consistent generations. At inference, we propose an attention biasing technique that allows Pippo to simultaneously generate greater than 5 times as many views as seen during training. Finally, we also introduce an improved metric to evaluate 3D consistency of multi-view generations, and show that Pippo outperforms existing works on multi-view human generation from a single image.', 'score': 2, 'issue_id': 2179, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': 'a7eee475138619ae', 'authors': ['Yash Kant', 'Ethan Weber', 'Jin Kyu Kim', 'Rawal Khirodkar', 'Su Zhaoen', 'Julieta Martinez', 'Igor Gilitschenski', 'Shunsuke Saito', 'Timur Bagautdinov'], 'affiliations': ['Meta Reality Labs', 'UC Berkeley', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2502.07785.jpg', 'data': {'categories': ['#multimodal', '#inference', '#3d', '#diffusion', '#video', '#benchmark'], 'emoji': 'üîÑ', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–≤–∏–¥–µ–æ —á–µ–ª–æ–≤–µ–∫–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–æ—Ç–æ', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Pippo, —Å–ø–æ—Å–æ–±–Ω–∞—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ —Å –ø–æ–≤–æ—Ä–æ—Ç–æ–º —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ 360 –≥—Ä–∞–¥—É—Å–æ–≤ –≤ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ 1K –∏–∑ –æ–¥–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏. –≠—Ç–æ –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ 3 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª—é–¥–µ–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å—Ç—É–¥–∏–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–∏–∫—Å–µ–ª—å–Ω–æ-–≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. –ù–∞ —ç—Ç–∞–ø–µ –≤—ã–≤–æ–¥–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏–∫–∞ —Å–º–µ—â–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ 5 —Ä–∞–∑ –±–æ–ª—å—à–µ —Ä–∞–∫—É—Ä—Å–æ–≤, —á–µ–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.'}, 'en': {'title': 'Pippo: Transforming Single Photos into Stunning 3D Videos!', 'desc': 'Pippo is a generative model designed to create high-resolution videos of a person using just one casual photo. It employs a multi-view diffusion transformer, eliminating the need for extra inputs like camera settings or parametric models. The model is pre-trained on a vast dataset of human images and fine-tuned with studio-captured data to enhance its performance. Pippo introduces innovative techniques for generating multiple views and evaluating the consistency of 3D outputs, outperforming previous methods in generating videos from single images.'}, 'zh': {'title': 'PippoÔºö‰ªéÂçïÂº†ÁÖßÁâáÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ëÁöÑÂàõÊñ∞Ê®°Âûã', 'desc': 'PippoÊòØ‰∏ÄÁßçÁîüÊàêÊ®°ÂûãÔºåÂèØ‰ª•‰ªé‰∏ÄÂº†ÈöèÊÑèÊãçÊëÑÁöÑÁÖßÁâáÁîüÊàê1KÂàÜËæ®ÁéáÁöÑÂØÜÈõÜËßÜÈ¢ë„ÄÇÂÆÉ‰ΩøÁî®Â§öËßÜËßíÊâ©Êï£ÂèòÊç¢Âô®Ôºå‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËæìÂÖ•ÔºåÂ¶ÇÂèÇÊï∞Ê®°ÂûãÊàñÁõ∏Êú∫ÂèÇÊï∞„ÄÇPippoÂú®30‰∫øÂº†Êó†Ê†áÁ≠æÁöÑ‰∫∫Á±ªÂõæÂÉè‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂Âú®ÂêéÁª≠ÁöÑËÆ≠ÁªÉ‰∏≠‰ΩøÁî®‰ΩéÂàÜËæ®ÁéáÂíåÈ´òÂàÜËæ®ÁéáÁöÑÂ§öËßÜËßíÊï∞ÊçÆËøõË°å‰ºòÂåñ„ÄÇÊúÄÁªàÔºåPippoÂú®ÁîüÊàêÂ§öËßÜËßí‰∫∫Á±ªËßÜÈ¢ëÊó∂Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊñπÊ≥ïÔºå‰∏îÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞ÊåáÊ†áÊù•Ë°°Èáè3D‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07640', 'title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2502.07640', 'abstract': 'We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. The final prover outperforms all existing open-source models in whole-proof generation. On the miniF2F benchmark, it achieves a 57.6% success rate (Pass@32), exceeding the previous best open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.', 'score': 2, 'issue_id': 2175, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': 'f0efbd784e8053e1', 'authors': ['Yong Lin', 'Shange Tang', 'Bohan Lyu', 'Jiayun Wu', 'Hongzhou Lin', 'Kaiyu Yang', 'Jia Li', 'Mengzhou Xia', 'Danqi Chen', 'Sanjeev Arora', 'Chi Jin'], 'affiliations': ['Amazon', 'Meta FAIR', 'Numina', 'Princeton Language and Intelligence, Princeton University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07640.jpg', 'data': {'categories': ['#training', '#math', '#data', '#benchmark', '#reasoning', '#dataset', '#open_source'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–æ—Ä—ã–≤ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ —Ç–µ–æ—Ä–µ–º —Å –ø–æ–º–æ—â—å—é LLM', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Goedel-Prover - –º–æ–¥–µ–ª—å –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ (LLM) —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ - –Ω–µ—Ö–≤–∞—Ç–∫–∞ —Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, –∫–æ—Ç–æ—Ä—É—é –∞–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—É—Ç–µ–º —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ 1,64 –º–∏–ª–ª–∏–æ–Ω–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, –æ–±—É—á–∞—è —Å–µ—Ä–∏—é –¥–æ–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π. Goedel-Prover –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, –¥–æ—Å—Ç–∏–≥–∞—è 57,6% —É—Å–ø–µ—Ö–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ miniF2F.'}, 'en': {'title': 'Revolutionizing Formal Proof Generation with Goedel-Prover', 'desc': 'Goedel-Prover is an advanced open-source large language model designed for generating formal proofs in mathematics. It addresses the challenge of limited formalized math statements by creating a dataset of 1.64 million formal statements from natural language problems. The model employs iterative training of provers, where each new prover builds on the successes of its predecessors, leading to improved proof generation capabilities. As a result, Goedel-Prover achieves state-of-the-art performance, surpassing previous models in both proof generation success rates and the volume of formal proofs produced.'}, 'zh': {'title': 'Goedel-ProverÔºöÊï∞Â≠¶ËØÅÊòéÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Goedel-Prover ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫éËá™Âä®ÂåñÂΩ¢ÂºèËØÅÊòéÁîüÊàêÔºåÁâπÂà´ÊòØÂú®Êï∞Â≠¶ÈóÆÈ¢ò‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇÊàë‰ª¨ÈÄöËøáËÆ≠ÁªÉËØ≠Âè•ÂΩ¢ÂºèÂåñÂô®ÔºåÂ∞ÜËá™ÁÑ∂ËØ≠Ë®ÄÊï∞Â≠¶ÈóÆÈ¢òËΩ¨Êç¢‰∏∫Ê≠£ÂºèËØ≠Ë®ÄÔºàLean 4ÔºâÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´164‰∏áÊù°Ê≠£ÂºèËØ≠Âè•ÁöÑÊï∞ÊçÆÈõÜ„ÄÇ‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊù•È™åËØÅËøô‰∫õÊ≠£ÂºèËØ≠Âè•ÊòØÂê¶ÂáÜÁ°Æ‰øùÁïô‰∫ÜÂéüÂßãËá™ÁÑ∂ËØ≠Ë®ÄÈóÆÈ¢òÁöÑÂÜÖÂÆπ„ÄÇÊúÄÁªàÁöÑËØÅÊòéËÄÖÂú®Êï¥‰∏™ËØÅÊòéÁîüÊàêÊñπÈù¢Ë∂ÖË∂ä‰∫ÜÊâÄÊúâÁé∞ÊúâÁöÑÂºÄÊ∫êÊ®°ÂûãÔºåÊàêÂäüÁéáÊòæËëóÊèêÈ´ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05932', 'title': 'Skill Expansion and Composition in Parameter Space', 'url': 'https://huggingface.co/papers/2502.05932', 'abstract': "Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However, previous methods suffer from limited training efficiency when expanding new skills and fail to fully leverage prior knowledge to facilitate new task learning. In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library. This library can progressively integrate skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning, facilitating efficient and flexible skill expansion. This structure also enables the direct skill compositions in parameter space by merging LoRA modules that encode different skills, leveraging shared information across skills to effectively program new skills. Based on this, we propose a context-aware module to dynamically activate different skills to collaboratively handle new tasks. Empowering diverse applications including multi-objective composition, dynamics shift, and continual policy shift, the results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC exhibits superior capacity to leverage prior knowledge to efficiently tackle new challenges, as well as expand its skill libraries to evolve the capabilities. Project website: https://ltlhuuu.github.io/PSEC/.", 'score': 2, 'issue_id': 2170, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': 'd4dd3a87e6ed9712', 'authors': ['Tenglong Liu', 'Jianxiong Li', 'Yinan Zheng', 'Haoyi Niu', 'Yixing Lan', 'Xin Xu', 'Xianyuan Zhan'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'National University of Defense Technology', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05932.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#optimization', '#agents', '#training'], 'emoji': 'üß†', 'ru': {'title': '–≠–≤–æ–ª—é—Ü–∏—è –Ω–∞–≤—ã–∫–æ–≤ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Parametric Skill Expansion and Composition (PSEC) –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. PSEC –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–∞–≤–ª—è–µ–º—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É –Ω–∞–≤—ã–∫–æ–≤, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è –ø—Ä–∏–º–∏—Ç–∏–≤—ã –Ω–∞–≤—ã–∫–æ–≤ –∫–∞–∫ –º–æ–¥—É–ª–∏ Low-Rank Adaptation (LoRA) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–≤—ã–∫–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—É—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥—É–ª–µ–π LoRA, –∫–æ–¥–∏—Ä—É—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –Ω–∞–≤—ã–∫–∏. PSEC –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–∞–≤—ã–∫–æ–≤.'}, 'en': {'title': 'Empowering Autonomous Agents with Efficient Skill Evolution', 'desc': "This paper introduces Parametric Skill Expansion and Composition (PSEC), a framework that enhances the ability of autonomous agents to learn new skills by building on existing knowledge. PSEC maintains a skill library that allows for efficient integration of skill primitives using Low-Rank Adaptation (LoRA) modules, which support parameter-efficient finetuning. The framework also enables the merging of these modules to create new skills by leveraging shared information, promoting flexibility in skill development. Experimental results demonstrate that PSEC significantly improves the agents' performance in adapting to new challenges while expanding their skill sets effectively."}, 'zh': {'title': 'Êô∫ËÉΩ‰ΩìÊäÄËÉΩÁöÑÈ´òÊïàÊâ©Â±ï‰∏éÁªÑÂêà', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÁß∞‰∏∫ÂèÇÊï∞ÂåñÊäÄËÉΩÊâ©Â±ï‰∏éÁªÑÂêàÔºàPSECÔºâÔºåÊó®Âú®ÊèêÈ´òËá™‰∏ªÊô∫ËÉΩ‰ΩìÂú®Èù¢ÂØπÊñ∞ÊåëÊàòÊó∂ÁöÑËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁª¥Êä§‰∏Ä‰∏™ÂèØÁÆ°ÁêÜÁöÑÊäÄËÉΩÂ∫ìÔºåÈÄêÊ≠•Êï¥ÂêàÊäÄËÉΩÂéüËØ≠ÔºåÊîØÊåÅÈ´òÊïàÁöÑÂèÇÊï∞ÂæÆË∞ÉÔºå‰ªéËÄåÂÆûÁé∞ÁÅµÊ¥ªÁöÑÊäÄËÉΩÊâ©Â±ï„ÄÇPSECËøòÂÖÅËÆ∏Âú®ÂèÇÊï∞Á©∫Èó¥‰∏≠Áõ¥Êé•ÁªÑÂêàÊäÄËÉΩÔºåÈÄöËøáÂêàÂπ∂‰∏çÂêåÊäÄËÉΩÁöÑLoRAÊ®°ÂùóÔºåÂà©Áî®ÂÖ±‰∫´‰ø°ÊÅØÊúâÊïàÁºñÁ®ãÊñ∞ÊäÄËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPSECÂú®Âà©Áî®ÂÖàÂâçÁü•ËØÜÂ∫îÂØπÊñ∞ÊåëÊàòÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂ËÉΩÂ§üÊâ©Â±ïÂÖ∂ÊäÄËÉΩÂ∫ì‰ª•ËøõÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04465', 'title': 'FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks', 'url': 'https://huggingface.co/papers/2502.04465', 'abstract': 'Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/.', 'score': 2, 'issue_id': 2167, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'a6ebe3d69cd8bcc2', 'authors': ['Luca Della Libera', 'Francesco Paissan', 'Cem Subakan', 'Mirco Ravanelli'], 'affiliations': ['Concordia University, Montreal, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2502.04465.jpg', 'data': {'categories': ['#multilingual', '#audio', '#architecture'], 'emoji': 'üéôÔ∏è', 'ru': {'title': 'FocalCodec: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ä–µ—á–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∏ –∞–∫—É—Å—Ç–∏–∫–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ FocalCodec - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞—É–¥–∏–æ–∫–æ–¥–µ–∫ —Å –Ω–∏–∑–∫–∏–º –±–∏—Ç—Ä–µ–π—Ç–æ–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ñ–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥—É–ª—è—Ü–∏–∏. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π –±–∏–Ω–∞—Ä–Ω—ã–π –∫–æ–¥–±—É–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è —Ä–µ—á–∏ –¥–æ 0.16-0.65 –∫–±–∏—Ç/—Å, —á—Ç–æ –Ω–∏–∂–µ, —á–µ–º —É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–æ–≤. FocalCodec –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–µ—Å–∏–Ω—Ç–µ–∑–µ —Ä–µ—á–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –≥–æ–ª–æ—Å–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏ –∞–∫—É—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ö–æ–¥–µ–∫ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π —Ä–µ—á—å—é –∏ —à—É–º–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏, –∞ —Ç–∞–∫–∂–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'FocalCodec: Efficient Speech Compression with Single Binary Codebook', 'desc': 'This paper presents FocalCodec, a novel low-bitrate codec designed for speech processing, which addresses the limitations of existing methods that often require complex multi-codebook architectures. By utilizing focal modulation and a single binary codebook, FocalCodec achieves efficient compression of speech at bitrates between 0.16 and 0.65 kbps. The model demonstrates strong performance in tasks like speech resynthesis and voice conversion, while maintaining the integrity of both semantic and acoustic information. Additionally, FocalCodec is effective in handling multilingual speech and noisy environments, making it a promising tool for generative modeling in natural language processing.'}, 'zh': {'title': 'FocalCodecÔºöÈ´òÊïà‰ΩéÊØîÁâπÁéáËØ≠Èü≥ÁºñËß£Á†ÅÂô®', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÈÄöËøáÂú®Êµ∑ÈáèÊï∞ÊçÆÈõÜ‰∏äËøõË°åËá™ÁõëÁù£È¢ÑËÆ≠ÁªÉÔºåÂΩªÂ∫ïÊîπÂèò‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÇÂèóÂà∞Ëøô‰∏ÄÊàêÂäüÁöÑÂêØÂèëÔºåÁ†îÁ©∂‰∫∫ÂëòÂ∞ùËØïÂ∞ÜËøô‰∫õÊñπÊ≥ïÂ∫îÁî®‰∫éËØ≠Èü≥Â§ÑÁêÜÔºåÈÄöËøáÁ•ûÁªèÈü≥È¢ëÁºñËß£Á†ÅÂô®Â∞ÜËøûÁª≠Èü≥È¢ëÁ¶ªÊï£Âåñ‰∏∫Ê†áËÆ∞„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÈù¢‰∏¥È´òÊØîÁâπÁéá„ÄÅËØ≠‰πâÊàñÂ£∞Â≠¶‰ø°ÊÅØ‰∏¢Â§±‰ª•ÂèäÂ§ö‰ª£Á†ÅÊú¨ËÆæËÆ°ÁöÑÂ§çÊùÇÊÄßÁ≠âÈôêÂà∂„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFocalCodecÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÁÑ¶ÁÇπË∞ÉÂà∂ÁöÑÈ´òÊïà‰ΩéÊØîÁâπÁéáÁºñËß£Á†ÅÂô®ÔºåËÉΩÂ§üÂú®0.16Âà∞0.65 kbps‰πãÈó¥ÂéãÁº©ËØ≠Èü≥ÔºåÂêåÊó∂Âú®ËØ≠Èü≥ÈáçÂêàÊàêÂíåËØ≠Èü≥ËΩ¨Êç¢‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07776', 'title': 'Auditing Prompt Caching in Language Model APIs', 'url': 'https://huggingface.co/papers/2502.07776', 'abstract': "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.", 'score': 2, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '48f7472ef1c86b27', 'authors': ['Chenchen Gu', 'Xiang Lisa Li', 'Rohith Kuditipudi', 'Percy Liang', 'Tatsunori Hashimoto'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07776.jpg', 'data': {'categories': ['#healthcare', '#leakage', '#inference', '#ethics', '#security', '#data'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ LLM: —Å–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç—Ç–∏–º —Ä–∏—Å–∫–∏ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞—É–¥–∏—Ç–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ —É —Ä–µ–∞–ª—å–Ω—ã—Ö API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM. –û–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫—ç—à–∞ –º–µ–∂–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ —É —Å–µ–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤, –≤–∫–ª—é—á–∞—è OpenAI, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —É—Ç–µ—á–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–º–ø—Ç–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –∏–∑-–∑–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–≥—É—Ç —Ä–∞—Å–∫—Ä—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –º–æ–¥–µ–ª–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–≤—Ç–æ—Ä—ã –Ω–∞—à–ª–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–æ–≥–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è OpenAI —è–≤–ª—è–µ—Ç—Å—è –¥–µ–∫–æ–¥–µ—Ä-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º.'}, 'en': {'title': 'Timing Variations: A Privacy Risk in Prompt Caching for LLMs', 'desc': "This paper discusses how prompt caching in large language models (LLMs) can lead to timing variations that depend on the data being processed. When prompts are cached, they are handled more quickly than those that are not, which can create vulnerabilities for side-channel attacks. The authors highlight the risks of privacy breaches, especially when cache is shared among users, allowing attackers to infer information about others' prompts based on response times. To address these concerns, the paper presents statistical audits that reveal global cache sharing in several API providers, including OpenAI, and even uncovers details about the model architecture that were previously undisclosed."}, 'zh': {'title': 'ÊèêÁ§∫ÁºìÂ≠òÁöÑÈöêÁßÅÈ£éÈô©‰∏éÈÄèÊòéÊÄß', 'desc': 'Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÔºåÊèêÁ§∫ÁºìÂ≠ò‰ºöÂØºËá¥Êï∞ÊçÆ‰æùËµñÁöÑÊó∂Èó¥ÂèòÂåñÔºöÁºìÂ≠òÁöÑÊèêÁ§∫Â§ÑÁêÜÈÄüÂ∫¶ÊØîÈùûÁºìÂ≠òÁöÑÊèêÁ§∫Âø´„ÄÇËøô‰∫õÊó∂Èó¥Â∑ÆÂºÇÂèØËÉΩÂºïÂèë‰æß‰ø°ÈÅìÊîªÂáªÁöÑÈ£éÈô©Ôºå‰æãÂ¶ÇÔºåÂ¶ÇÊûúÁºìÂ≠òË¢´Â§ö‰∏™Áî®Êà∑ÂÖ±‰∫´ÔºåÊîªÂáªËÄÖÂèØ‰ª•ÈÄöËøáÂø´ÈÄüÁöÑAPIÂìçÂ∫îÊó∂Èó¥ËØÜÂà´Âá∫ÁºìÂ≠òÁöÑÊèêÁ§∫Ôºå‰ªéËÄåËé∑ÂèñÂÖ∂‰ªñÁî®Êà∑ÊèêÁ§∫ÁöÑ‰ø°ÊÅØ„ÄÇÁî±‰∫éÊèêÁ§∫ÁºìÂ≠òÂèØËÉΩÂØºËá¥ÈöêÁßÅÊ≥ÑÈú≤ÔºåÂõ†Ê≠§APIÊèê‰æõÂïÜÁöÑÁºìÂ≠òÊîøÁ≠ñÈÄèÊòéÂ∫¶ÈùûÂ∏∏ÈáçË¶Å„ÄÇÊàë‰ª¨ÂºÄÂèëÂπ∂ËøõË°åÁªüËÆ°ÂÆ°ËÆ°Ôºå‰ª•Ê£ÄÊµãÁé∞ÂÆû‰∏ñÁïå‰∏≠LLM APIÊèê‰æõÂïÜÁöÑÊèêÁ§∫ÁºìÂ≠òÊÉÖÂÜµÔºåÂèëÁé∞‰∏É‰∏™APIÊèê‰æõÂïÜÔºàÂåÖÊã¨OpenAIÔºâ‰πãÈó¥Â≠òÂú®ÂÖ®ÁêÉÁºìÂ≠òÂÖ±‰∫´ÔºåÂèØËÉΩÂØºËá¥Áî®Êà∑ÊèêÁ§∫ÁöÑÈöêÁßÅÊ≥ÑÈú≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06394', 'title': 'SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators', 'url': 'https://huggingface.co/papers/2502.06394', 'abstract': 'Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that models trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting. Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our dataset and code to help further research in multilingual text detoxification.', 'score': 69, 'issue_id': 2145, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '86b7da795fcf943b', 'authors': ['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], 'affiliations': ['AIRI', 'ISP RAS Research Center for Trusted AI', 'Sber AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2502.06394.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#dataset', '#data', '#synthetic', '#open_source'], 'emoji': 'üßº', 'ru': {'title': '–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —É–ª—É—á—à–∞—é—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç SynthDetoxM, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 16,000 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º, –∏—Å–ø–∞–Ω—Å–∫–æ–º –∏ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –ª—é–¥—å–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–µ MultiParaDetox. –ê–≤—Ç–æ—Ä—ã –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏ —Å–≤–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –∫–æ–¥ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'Enhancing Multilingual Detoxification with SynthDetoxM', 'desc': 'This paper addresses the challenge of multilingual text detoxification, which is limited by the lack of parallel datasets in multiple languages. The authors present a new pipeline for generating such datasets, introducing SynthDetoxM, a collection of 16,000 detoxified sentence pairs in German, French, Spanish, and Russian. These pairs were created by rewriting existing toxicity evaluation data using modern open-source large language models (LLMs) in a few-shot learning context. The results show that models trained on this synthetic dataset outperform those trained on existing human-annotated datasets, demonstrating the effectiveness of the proposed approach in enhancing multilingual detoxification efforts.'}, 'zh': {'title': 'Â§öËØ≠Ë®ÄÊñáÊú¨ÂéªÊØíÂåñÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁîüÊàêÂ§öËØ≠Ë®ÄÂπ≥Ë°åÂéªÊØíÂåñÊï∞ÊçÆÁöÑÊµÅÁ®ãÔºå‰ª•Ëß£ÂÜ≥Áé∞ÊúâÂ§öËØ≠Ë®ÄÊñáÊú¨ÂéªÊØíÂåñÊñπÊ≥ï‰∏≠Âπ≥Ë°åÂ§öËØ≠Ë®ÄÊï∞ÊçÆÈõÜÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨‰ªãÁªç‰∫ÜSynthDetoxMÔºåËøôÊòØ‰∏Ä‰∏™ÊâãÂä®Êî∂ÈõÜÂíåÂêàÊàêÁîüÊàêÁöÑÂ§öËØ≠Ë®ÄÂπ≥Ë°åÊñáÊú¨ÂéªÊØíÂåñÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Êù•Ëá™Âæ∑ËØ≠„ÄÅÊ≥ïËØ≠„ÄÅË•øÁè≠ÁâôËØ≠Âíå‰øÑËØ≠ÁöÑ16,000ÂØπÈ´òË¥®ÈáèÂéªÊØíÂåñÂè•Â≠ê„ÄÇÊï∞ÊçÆÊù•Ê∫ê‰∫é‰∏çÂêåÁöÑÊØíÊÄßËØÑ‰º∞Êï∞ÊçÆÈõÜÔºåÂπ∂ÈÄöËøá‰πùÁßçÁé∞‰ª£ÂºÄÊ∫êÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â∞ëÈáèÊ†∑Êú¨ËÆæÁΩÆ‰∏ãËøõË°åÈáçÂÜô„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éÂêàÊàêÊï∞ÊçÆÈõÜËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Êï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãË°®Áé∞‰ºò‰∫éÂü∫‰∫é‰∫∫Â∑•Ê†áÊ≥®ÁöÑMultiParaDetoxÊï∞ÊçÆÈõÜËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06703', 'title': 'Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling', 'url': 'https://huggingface.co/papers/2502.06703', 'abstract': 'Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.', 'score': 60, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '2129c5ac1750f3cc', 'authors': ['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], 'affiliations': ['BUPT', 'Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06703.jpg', 'data': {'categories': ['#inference', '#reasoning', '#small_models', '#training', '#optimization', '#math'], 'emoji': 'üßÆ', 'ru': {'title': '–ú–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ–±–µ–∂–¥–∞—é—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤: —Å–∏–ª–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∞', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –º–µ—Ç–æ–¥–∞ Test-Time Scaling (TTS) –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç, –∫–∞–∫ –≤—ã–±–æ—Ä –ø–æ–ª–∏—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏, –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM) –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏ –≤–ª–∏—è—é—Ç –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é TTS. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ TTS. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª TTS –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Ä–µ—à–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'Unlocking LLM Potential: Small Models, Big Gains with TTS!', 'desc': 'This paper investigates Test-Time Scaling (TTS), a technique that enhances the performance of Large Language Models (LLMs) by adjusting computation during inference. It addresses how different policy models, Process Reward Models (PRMs), and the difficulty of problems affect the effectiveness of TTS. The authors conduct experiments on MATH-500 and AIME24 tasks, revealing that smaller models can outperform larger ones when using an optimal TTS strategy. The results emphasize the importance of tailoring TTS methods to specific models and tasks to improve reasoning capabilities in LLMs.'}, 'zh': {'title': '‰ºòÂåñÊµãËØïÊó∂Èó¥Êâ©Â±ïÔºåÊèêÂçáÂ∞èÂûãÊ®°ÂûãÊÄßËÉΩÔºÅ', 'desc': 'ÊµãËØïÊó∂Èó¥Êâ©Â±ïÔºàTTSÔºâÊòØ‰∏ÄÁßçÈÄöËøáÂú®Êé®ÁêÜÈò∂ÊÆµÂ¢ûÂä†ËÆ°ÁÆóÈáèÊù•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊÄßËÉΩÁöÑÊñπÊ≥ï„ÄÇÊú¨ÊñáÁ≥ªÁªüÂàÜÊûê‰∫ÜÁ≠ñÁï•Ê®°Âûã„ÄÅËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÂíåÈóÆÈ¢òÈöæÂ∫¶Â¶Ç‰ΩïÂΩ±ÂìçTTSÁöÑÊïàÊûú„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËÆ°ÁÆóÊúÄ‰ºòÁöÑTTSÁ≠ñÁï•‰æùËµñ‰∫éÊâÄÈÄâÁöÑÁ≠ñÁï•Ê®°Âûã„ÄÅPRMÂíåÈóÆÈ¢òÈöæÂ∫¶Ôºå‰∏îÂ∞èÂûãÊ®°ÂûãÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÂèØ‰ª•Ë∂ÖË∂äÂ§ßÂûãÊ®°Âûã„ÄÇÈÄöËøáÂú®MATH-500ÂíåAIME24‰ªªÂä°‰∏äÁöÑÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞ÈÄÇÂ∫îÁâπÂÆö‰ªªÂä°ÂíåÊ®°ÂûãÁöÑTTSÁ≠ñÁï•ÂØπ‰∫éÊèêÂçáLLMsÁöÑÊé®ÁêÜËÉΩÂäõËá≥ÂÖ≥ÈáçË¶Å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06781', 'title': 'Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2502.06781', 'abstract': 'Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through Outcome REwArd-based reinforcement Learning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future researchhttps://github.com/InternLM/OREAL.', 'score': 36, 'issue_id': 2142, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '9cd2694b7c865b94', 'authors': ['Chengqi Lyu', 'Songyang Gao', 'Yuzhe Gu', 'Wenwei Zhang', 'Jianfei Gao', 'Kuikun Liu', 'Ziyi Wang', 'Shuaibin Li', 'Qian Zhao', 'Haian Huang', 'Weihan Cao', 'Jiangning Liu', 'Hongwei Liu', 'Junnan Liu', 'Songyang Zhang', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['HKGAI under InnoHK', 'MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06781.jpg', 'data': {'categories': ['#training', '#open_source', '#rl', '#reasoning', '#optimization', '#math'], 'emoji': 'üßÆ', 'ru': {'title': 'OREAL: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º OREAL –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö –∏–∑ –≤—ã–±–æ—Ä–∫–∏ best-of-N –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –≤ —Å—Ä–µ–¥–∞—Ö —Å –±–∏–Ω–∞—Ä–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é. –î–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤. –° –ø–æ–º–æ—â—å—é OREAL –º–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º 7B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 94.0% pass@1 –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ MATH-500, —á—Ç–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ 32B –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'OREAL: Advancing AI Reasoning with Outcome-Based Reinforcement Learning', 'desc': 'This paper introduces a new reinforcement learning framework called OREAL, designed to enhance mathematical reasoning capabilities in AI models. OREAL focuses on using binary outcome rewards to improve learning efficiency, particularly in environments where feedback is sparse. The authors demonstrate that behavior cloning from positive examples can effectively learn optimal policies, while also reshaping negative rewards to maintain gradient consistency. The results show that OREAL achieves high accuracy on mathematical tasks, outperforming larger models and highlighting the significance of initial policy models in the training process.'}, 'zh': {'title': 'OREALÔºöÊï∞Â≠¶Êé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁß∞‰∏∫OREALÔºåÊó®Âú®ÊèêÈ´òÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇOREAL‰ΩøÁî®Âü∫‰∫éÁªìÊûúÁöÑÂ•ñÂä±Êú∫Âà∂Ôºå‰∏ìÊ≥®‰∫é‰∫åÂÖÉÁªìÊûúÂ•ñÂä±Ôºå‰ª•Ëß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÁ®ÄÁñèÂ•ñÂä±ÈóÆÈ¢ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄöËøáÂØπÊúÄ‰Ω≥Ê†∑Êú¨ËøõË°åË°å‰∏∫ÂÖãÈöÜÔºåÂèØ‰ª•ÊúâÊïàÂ≠¶‰π†ÊúÄ‰ºòÁ≠ñÁï•ÔºåÂπ∂‰∏îÈúÄË¶ÅÂØπË¥üÊ†∑Êú¨ÁöÑÂ•ñÂä±ËøõË°åÈáçÂ°ë‰ª•‰øùÊåÅÊ¢ØÂ∫¶‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰ΩøÁî®OREALÁöÑ7BÊ®°ÂûãÂú®MATH-500‰∏äËææÂà∞‰∫Ü94.0ÁöÑÂáÜÁ°ÆÁéáÔºåË°®Áé∞‰∏é32BÊ®°ÂûãÁõ∏ÂΩìÔºå‰∏îOREAL-32BÂú®Âêå‰∏Ä‰ªªÂä°‰∏äË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑ32BÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06060', 'title': 'Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.06060', 'abstract': "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/", 'score': 20, 'issue_id': 2146, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': 'd235746154e72f16', 'authors': ['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], 'affiliations': ['Stanford University, Stanford, United States of America'], 'pdf_title_img': 'assets/pdf/title_img/2502.06060.jpg', 'data': {'categories': ['#alignment', '#games', '#rlhf', '#agents', '#open_source', '#rl'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫ –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–µ—Å—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ã–µ –¥–∏—Å–∫—É—Å—Å–∏–∏ –≤ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Å—Ä–µ–¥–µ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –æ—Ç –ª—é–¥–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–¥–µ–ª—è—é—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –Ω–∞ –Ω–∞–≤—ã–∫–∏ —Å–ª—É—à–∞–Ω–∏—è –∏ –≥–æ–≤–æ—Ä–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ü–µ–ª—å –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞–≥—Ä–∞–¥—ã. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –∏–≥—Ä–µ –Ω–∞ –¥–µ–¥—É–∫—Ü–∏—é, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ Among Us, –≥–¥–µ –∫–ª—é—á–µ–≤–æ–π –≤–æ–ø—Ä–æ—Å - –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–∏—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ç–∏–≤–Ω–∏–∫–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–µ—Å—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è, —É–¥–≤–∞–∏–≤–∞—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –≤—ã–∏–≥—Ä—ã—à–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Empowering Agents with Natural Language Communication for Enhanced Coordination', 'desc': "This paper explores how language models can be trained to communicate effectively in multi-agent environments without relying on human demonstrations. The authors break down communication into two parts: listening and speaking, using the agents' goals as a reward signal to enhance their communication skills. By applying multi-agent reinforcement learning, they improve how agents generate and interpret messages, leading to more productive discussions. The study demonstrates that these enhanced communication strategies significantly increase success rates in a social deduction game, showcasing the importance of effective communication in complex scenarios."}, 'zh': {'title': 'Ëá™ÁÑ∂ËØ≠Ë®ÄÊ≤üÈÄöÊèêÂçáÂ§öÊô∫ËÉΩ‰ΩìÂçè‰Ωú', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂú®Â§öÊô∫ËÉΩ‰ΩìÁéØÂ¢É‰∏≠ÔºåÂ¶Ç‰ΩïÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄËøõË°åÊúâÊïàÊ≤üÈÄö„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÔºåËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÂú®Ê≤°Êúâ‰∫∫Á±ªÁ§∫ËåÉÁöÑÊÉÖÂÜµ‰∏ãÔºåËøõË°åÂÖ≥‰∫éÁéØÂ¢ÉÁöÑËÆ®ËÆ∫„ÄÇÈÄöËøáÂ∞ÜÊ≤üÈÄöÈóÆÈ¢òÂàÜËß£‰∏∫ÂÄæÂê¨ÂíåÂèëË®ÄÔºåÊàë‰ª¨Âà©Áî®Êô∫ËÉΩ‰ΩìÁöÑÁõÆÊ†áÊù•È¢ÑÊµãÊúâÁî®ÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÂºïÂØºÊ≤üÈÄö„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂú®Â§çÊùÇÁ§æ‰∫§Âú∫ÊôØ‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑËÉúÁéáÔºå‰øÉËøõ‰∫ÜÊõ¥Âº∫ÁöÑËÆ®ËÆ∫ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05664', 'title': 'CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging', 'url': 'https://huggingface.co/papers/2502.05664', 'abstract': "Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim's remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (https://kagnlp.github.io/codesim.github.io/).", 'score': 15, 'issue_id': 2152, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 8', 'zh': '2Êúà8Êó•'}, 'hash': '6a6a71a03d5f0d9c', 'authors': ['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], 'affiliations': ['Bangladesh University of Engineering and Technology (BUET)', 'Qatar Computing Research Institute (QCRI)'], 'pdf_title_img': 'assets/pdf/title_img/2502.05664.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agents', '#games', '#open_source', '#training'], 'emoji': 'ü§ñ', 'ru': {'title': 'CodeSim: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º —É—Ä–æ–≤–Ω–µ', 'desc': 'CodeSim - —ç—Ç–æ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –ø–æ–¥—Ö–æ–¥, –∏–º–∏—Ç–∏—Ä—É—é—â–∏–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ. –û–Ω–∞ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —ç—Ç–∞–ø—ã —Å–∏–Ω—Ç–µ–∑–∞ –ø—Ä–æ–≥—Ä–∞–º–º: –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ª–∞–¥–∫—É, –ø—Ä–∏–º–µ–Ω—è—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –æ—Ç–ª–∞–¥–∫–∏ —á–µ—Ä–µ–∑ –ø–æ—à–∞–≥–æ–≤—É—é —Å–∏–º—É–ª—è—Ü–∏—é –≤–≤–æ–¥–∞/–≤—ã–≤–æ–¥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–µ–º–∏ —Å–ª–æ–∂–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á –∏ —Å–∏–Ω—Ç–µ–∑—É –ø—Ä–æ–≥—Ä–∞–º–º –ø–æ–∫–∞–∑–∞–ª–∏ –≤—ã–¥–∞—é—â–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã CodeSim –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞. –°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–ª–∞ –Ω–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π state-of-the-art (pass@1) –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è HumanEval –∏ MBPP.'}, 'en': {'title': 'CodeSim: Revolutionizing Code Generation with Human-like Perception', 'desc': 'This paper presents CodeSim, a new framework for code generation that improves the process of program synthesis by integrating planning, coding, and debugging stages. Unlike traditional methods that rely on external tools for debugging, CodeSim uses a human-like perception approach, allowing for step-by-step simulation of input and output to verify plans and debug internally. The framework has been tested on various competitive benchmarks, achieving state-of-the-art results in code generation tasks. Additionally, CodeSim shows promise for further improvements when combined with existing external debugging tools.'}, 'zh': {'title': 'CodeSimÔºöÁ±ª‰∫∫ÊÑüÁü•ÁöÑ‰ª£Á†ÅÁîüÊàêÊñ∞Ê°ÜÊû∂', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÁîüÊàêÂíåÈóÆÈ¢òËß£ÂÜ≥ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÂΩìÂâçÁöÑÊñπÊ≥ï‰æùËµñ‰∫éÂ§ñÈÉ®Â∑•ÂÖ∑ÁöÑËø≠‰ª£Ë∞ÉËØïÂô®ÔºåËøô‰∫õË∞ÉËØïÂô®Âà©Áî®ÁºñËØëÂô®ÊàñÂÖ∂‰ªñÂ∑•ÂÖ∑ÁöÑËøêË°åÊó∂ÂèçÈ¶àÊù•ÊîπËøõÁ≤óÁï•ÁöÑÁ®ãÂ∫èÁîüÊàê„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùËµñ‰∫éÂàùÂßã‰ª£Á†ÅÁîüÊàêÁöÑË¥®ÈáèÔºåËøô‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÂºÄÊîæÁöÑÊåëÊàò„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜCodeSimÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂ§öÊô∫ËÉΩ‰Ωì‰ª£Á†ÅÁîüÊàêÊ°ÜÊû∂ÔºåÈÄöËøáÁ±ª‰∫∫ÊÑüÁü•ÁöÑÊñπÊ≥ïÂÖ®Èù¢Ëß£ÂÜ≥Á®ãÂ∫èÂêàÊàêÁöÑÂêÑ‰∏™Èò∂ÊÆµÔºåÂåÖÊã¨ËßÑÂàí„ÄÅÁºñÁ†ÅÂíåË∞ÉËØï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06049', 'title': 'LM2: Large Memory Models', 'url': 'https://huggingface.co/papers/2502.06049', 'abstract': 'This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.', 'score': 13, 'issue_id': 2140, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': '5f62d7e814a6918f', 'authors': ['Jikun Kang', 'Wenqi Wu', 'Filippos Christianos', 'Alex J. Chan', 'Fraser Greenlee', 'George Thomas', 'Marvin Purtorab', 'Andy Toulis'], 'affiliations': ['Convergence Labs Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2502.06049.jpg', 'data': {'categories': ['#dataset', '#architecture', '#benchmark', '#interpretability', '#long_context', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'LM2: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –ø–∞–º—è—Ç—å—é –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Large Memory Model (LM2), –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–µ–∫–æ–¥–µ—Ä-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –º–æ–¥—É–ª–µ–º –ø–∞–º—è—Ç–∏. LM2 —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ BABILong –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. LM2 —Ç–∞–∫–∂–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –≤—ã–≤–æ–¥–∞—Ö, —á–∏—Å–ª–æ–≤—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö –∏ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.'}, 'en': {'title': 'Enhancing Transformers with Memory for Superior Reasoning', 'desc': 'The paper presents the Large Memory Model (LM2), a new type of Transformer designed to improve multi-step reasoning and information synthesis over long contexts. LM2 features an auxiliary memory module that stores contextual information and interacts with input data through cross attention, allowing it to update its memory dynamically. This model retains the original capabilities of Transformers while adding a memory pathway that enhances performance on complex tasks. Experimental results show that LM2 significantly outperforms existing models in reasoning tasks and maintains strong performance on general tasks, highlighting the value of integrating explicit memory into Transformer architectures.'}, 'zh': {'title': 'Â§ßÂûãËÆ∞ÂøÜÊ®°ÂûãÔºöÊèêÂçáTransformerÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§ßÂûãËÆ∞ÂøÜÊ®°ÂûãÔºàLM2ÔºâÁöÑËß£Á†ÅÂô®‰ªÖTransformerÊû∂ÊûÑÔºåÊó®Âú®Ëß£ÂÜ≥Ê†áÂáÜTransformerÂú®Â§öÊ≠•Êé®ÁêÜ„ÄÅÂÖ≥Á≥ªËÆ∫ËØÅÂíåÈïø‰∏ä‰∏ãÊñá‰ø°ÊÅØÁªºÂêàÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇLM2ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ËæÖÂä©ËÆ∞ÂøÜÊ®°ÂùóÔºå‰Ωú‰∏∫‰∏ä‰∏ãÊñáË°®Á§∫ÁöÑÂ≠òÂÇ®Â∫ìÔºåÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõ‰∏éËæìÂÖ•Ê†áËÆ∞‰∫§‰∫íÔºåÂπ∂ÈÄöËøáÈó®ÊéßÊú∫Âà∂ËøõË°åÊõ¥Êñ∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLM2Âú®BABILongÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂπ≥ÂùáÊÄßËÉΩÊØîËÆ∞ÂøÜÂ¢ûÂº∫ÁöÑRMTÊ®°ÂûãÊèêÈ´ò‰∫Ü37.1%ÔºåÊØîÂü∫Á∫øLlama-3.2Ê®°ÂûãÊèêÈ´ò‰∫Ü86.3%„ÄÇLM2Âú®Â§öË∑≥Êé®ÁêÜ„ÄÅÊï∞ÂÄºÊé®ÁêÜÂíåÂ§ß‰∏ä‰∏ãÊñáÈóÆÁ≠îÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËØÅÊòé‰∫ÜÊòæÂºèËÆ∞ÂøÜÂú®Â¢ûÂº∫TransformerÊû∂ÊûÑ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05415', 'title': 'Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.05415', 'abstract': 'There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as a notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify a unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), a qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce a trajectory segmentation strategy and a curriculum learning procedure to improve the training convergence. Empirically, in text-to-image generation, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo exhibits a 1.5x speedup without significantly sacrificing performance. The code is available at https://github.com/zhijie-group/Show-o-Turbo.', 'score': 12, 'issue_id': 2144, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 8', 'zh': '2Êúà8Êó•'}, 'hash': '521adeebda96668f', 'authors': ['Chenkai Xu', 'Xu Wang', 'Zhenyi Liao', 'Yishun Li', 'Tianqi Hou', 'Zhijie Deng'], 'affiliations': ['Huawei', 'Shanghai Jiao Tong University', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05415.jpg', 'data': {'categories': ['#training', '#optimization', '#multimodal', '#diffusion'], 'emoji': 'üöÄ', 'ru': {'title': 'Show-o Turbo: –£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Show-o Turbo - —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Show-o –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É –∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—é –¥–ª—è –æ–±–æ–∏—Ö —Ç–∏–ø–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –û–Ω–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç –º–µ—Ç–æ–¥ consistency distillation –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—É –∫—É—Ä–∏–∫—É–ª—è—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Show-o Turbo –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'Show-o Turbo: Accelerating Multimodal Generation with Unified Denoising', 'desc': 'This paper presents Show-o Turbo, an advanced model for multimodal understanding and generation that improves upon the original Show-o framework. It addresses inefficiencies in the generation process by introducing a unified denoising approach that allows for parallel decoding of text tokens. The authors enhance the training process using consistency distillation and a new trajectory segmentation strategy, which leads to faster convergence. Empirical results show that Show-o Turbo achieves better performance in both text-to-image and image-to-text tasks, significantly reducing the number of sampling steps required for generation.'}, 'zh': {'title': 'ÊèêÂçáÂ§öÊ®°ÊÄÅÁîüÊàêÊïàÁéáÁöÑShow-o Turbo', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÊ®°ÊÄÅÁîüÊàêÊ®°ÂûãShow-o TurboÔºåÊó®Âú®ÊèêÈ´òÊñáÊú¨Âà∞ÂõæÂÉèÂíåÂõæÂÉèÂà∞ÊñáÊú¨ÁîüÊàêÁöÑÊïàÁéá„ÄÇÈÄöËøáÂπ∂Ë°åËß£Á†ÅÊñáÊú¨Ê†áËÆ∞ÔºåShow-o TurboÈááÁî®Áªü‰∏ÄÁöÑÂéªÂô™ËßÜËßíÔºåÁº©Áü≠‰∫ÜÂéªÂô™ËøáÁ®ã„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçËΩ®ËøπÂàÜÂâ≤Á≠ñÁï•ÂíåËØæÁ®ãÂ≠¶‰π†Á®ãÂ∫èÔºå‰ª•ÊîπÂñÑËÆ≠ÁªÉÊî∂ÊïõÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåShow-o TurboÂú®ÁîüÊàêÂõæÂÉèÊó∂ÁöÑÊïàÁéáÊòæËëóÊèêÈ´òÔºåÂêåÊó∂Âú®ÁîüÊàêÊñáÊú¨Êó∂‰πüÂÆûÁé∞‰∫Ü1.5ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05609', 'title': 'Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding', 'url': 'https://huggingface.co/papers/2502.05609', 'abstract': 'Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.', 'score': 12, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 8', 'zh': '2Êúà8Êó•'}, 'hash': '6f559083c224138c', 'authors': ['Sukmin Cho', 'Sangjin Choi', 'Taeho Hwang', 'Jeongyeon Seo', 'Soyeong Jeong', 'Huije Lee', 'Hoyun Song', 'Jong C. Park', 'Youngjin Kwon'], 'affiliations': ['School of Computing, Graduate School of AI, Korea Advanced Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.05609.jpg', 'data': {'categories': ['#training', '#architecture', '#inference', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —á–µ—Ä–Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –≤—ã–≤–æ–¥–∞ –≤ LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Hierarchy Drafting (HD). HD –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–µ—Ä–Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ HD –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —á–µ—Ä–Ω–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –Ω–∞–¥–µ–∂–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤—ã–≤–æ–¥–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –∑–∞–¥–∞—á –∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä.'}, 'en': {'title': 'Boosting Inference Speed with Hierarchy Drafting in LLMs', 'desc': 'This paper focuses on improving the speed of inference in Large Language Models (LLMs) for real-time applications. It introduces a new method called Hierarchy Drafting (HD), which organizes token sources into a hierarchical structure to enhance the drafting process. By accessing these token databases based on their temporal locality, HD ensures faster and more consistent token generation across various tasks. Experimental results show that HD significantly outperforms existing methods, providing robust speed improvements for LLMs of different sizes and tasks.'}, 'zh': {'title': 'Â±ÇÊ¨°ËçâÊãüÔºöÂä†ÈÄüÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Âä†ÈÄüÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜÂØπ‰∫éÂÆûÊó∂‰∫§‰∫íËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó†ÊçüËçâÊãüÊñπÊ≥ïÔºåÁß∞‰∏∫Â±ÇÊ¨°ËçâÊãüÔºàHDÔºâÔºåÂÆÉÈÄöËøáÂü∫‰∫éÊó∂Èó¥Â±ÄÈÉ®ÊÄßÁöÑÂ±ÇÊ¨°Ê°ÜÊû∂ÁªÑÁªáÂ§öÁßç‰ª§ÁâåÊ∫ê„ÄÇHDÂú®ËçâÊãüÊ≠•È™§‰∏≠‰æùÊ¨°ËÆøÈóÆÂ§ö‰∏™Êï∞ÊçÆÂ∫ìÔºå‰ªéÊúÄÈ´òÂà∞ÊúÄ‰ΩéÁöÑÂ±ÄÈÉ®ÊÄßËé∑ÂèñËçâÊãü‰ª§ÁâåÔºå‰ªéËÄåÁ°Æ‰øùÂú®‰∏çÂêå‰ªªÂä°‰∏≠‰∏ÄËá¥ÁöÑÂä†ÈÄüÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHDÂú®Êé®ÁêÜÈÄüÂ∫¶‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊï∞ÊçÆÂ∫ìËçâÊãüÊñπÊ≥ïÔºåÈÄÇÁî®‰∫é‰∏çÂêåËßÑÊ®°ÁöÑÊ®°ÂûãÂíå‰ªªÂä°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06772', 'title': 'ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates', 'url': 'https://huggingface.co/papers/2502.06772', 'abstract': 'We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux', 'score': 11, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '1ac59597c9610fb2', 'authors': ['Ling Yang', 'Zhaochen Yu', 'Bin Cui', 'Mengdi Wang'], 'affiliations': ['Peking University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06772.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#reasoning', '#math', '#benchmark', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å ReasonFlux-32B, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º —à–∞–±–ª–æ–Ω–æ–≤ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ —Ä–µ—à–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ—â–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ OpenAI o1-preview –∏ DeepSeek V3. ReasonFlux-32B –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É —à–∞–±–ª–æ–Ω–æ–≤ –º—ã—à–ª–µ–Ω–∏—è –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —à–∞–±–ª–æ–Ω–æ–≤. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MATH –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 91.2%, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è o1-preview –Ω–∞ 6.7%.'}, 'en': {'title': 'Revolutionizing Math Reasoning with Hierarchical Thought Templates', 'desc': 'This paper introduces ReasonFlux-32B, a model that enhances mathematical reasoning in large language models (LLMs) by using hierarchical reasoning with thought templates. It features a library of 500 structured thought templates that help generalize reasoning across similar problems. The model employs hierarchical reinforcement learning to optimize the sequence of thought templates, allowing it to tackle complex problems more effectively. With these innovations, ReasonFlux-32B achieves state-of-the-art performance on math benchmarks, significantly outperforming existing models like OpenAI o1-preview and DeepSeek V3.'}, 'zh': {'title': 'Â±ÇÊ¨°ÂåñÊé®ÁêÜÔºåÊï∞Â≠¶ËÉΩÂäõÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ÊñáÊèêÂá∫ÈÄöËøáÊâ©Â±ïÊÄùÁª¥Ê®°ÊùøÁöÑÂ±ÇÊ¨°ÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜÔºåÂèØ‰ª•ÊúâÊïà‰ºòÂåñÊé®ÁêÜÊêúÁ¥¢Á©∫Èó¥ÔºåÂπ∂Ë∂ÖË∂äÂº∫Â§ßÁöÑLLMÂ¶ÇOpenAI o1-previewÂíåDeepSeek V3ÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ËÆ≠ÁªÉÁöÑReasonFlux-32BÊ®°Âûã‰ªÖ‰ΩøÁî®8‰∏™GPUÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏âÈ°πÂàõÊñ∞Ôºö‰∏ÄÊòØÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´Á∫¶500‰∏™È´òÂ±ÇÊ¨°ÊÄùÁª¥Ê®°ÊùøÁöÑÁªìÊûÑÂåñÈÄöÁî®Ê®°ÊùøÂ∫ìÔºåËÉΩÂ§üÊé®ÂπøÂà∞Á±ª‰ººÁöÑÊé®ÁêÜÈóÆÈ¢òÔºõ‰∫åÊòØÂØπÊÄùÁª¥Ê®°ÊùøÂ∫èÂàóËøõË°åÂ±ÇÊ¨°ÂåñÂº∫ÂåñÂ≠¶‰π†ÔºåËÄå‰∏çÊòØÈïøÈìæÁöÑÊÄùÁª¥ÔºàCoTsÔºâÔºå‰ºòÂåñÂü∫Á°ÄLLM‰ª•ËßÑÂàíÂá∫Â§ÑÁêÜÂ§çÊùÇÈóÆÈ¢òÁöÑÊúÄ‰Ω≥Ê®°ÊùøËΩ®ËøπÔºõ‰∏âÊòØÂÖ®Êñ∞ÁöÑÊé®ÁêÜÊâ©Â±ïÁ≥ªÁªüÔºåÈÄöËøáÂú®Êé®ÁêÜÊó∂Ëá™ÈÄÇÂ∫îÊâ©Â±ïÊÄùÁª¥Ê®°ÊùøÔºåÂÆûÁé∞Â±ÇÊ¨°ÂåñLLMÊé®ÁêÜ„ÄÇÈÄöËøáÂåÖÂê´È°∫Â∫èÊÄùÁª¥Ê®°ÊùøÁöÑÊ®°ÊùøËΩ®ËøπÔºåReasonFlux-32BÂú®Êï∞Â≠¶Êé®ÁêÜËÉΩÂäõ‰∏äÊòæËëóÊèêÂçáÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06786', 'title': 'Matryoshka Quantization', 'url': 'https://huggingface.co/papers/2502.06786', 'abstract': 'Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model.', 'score': 9, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '1126a5fe83c7422d', 'authors': ['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.06786.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': 'ü™Ü', 'ru': {'title': 'MatQuant: –û–¥–Ω–∞ –º–æ–¥–µ–ª—å - –º–Ω–æ–∂–µ—Å—Ç–≤–æ —É—Ä–æ–≤–Ω–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏', 'desc': 'MatQuant - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –∏ –æ–±—Å–ª—É–∂–∏–≤–∞—Ç—å –æ–¥–Ω—É –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –∑–∞—Ç–µ–º –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –∏ –∫–æ-–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –º–æ–¥–µ–ª–∏ int2, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é MatQuant, –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ 10% —Ç–æ—á–Ω–µ–µ, —á–µ–º –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ int2. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è —Ç–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å Gemma-2 9B —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π FFN –¥–æ int2 –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ—á–Ω–µ–µ, —á–µ–º –º–æ–¥–µ–ª—å Gemma-2 2B —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π FFN –¥–æ int8.'}, 'en': {'title': 'One Model, Multiple Precision: Revolutionizing Quantization with MatQuant', 'desc': 'This paper introduces Matryoshka Quantization (MatQuant), a new technique for quantizing model weights that allows for multiple precision levels without sacrificing model quality. Traditional low-precision quantization, especially to int2, often leads to significant degradation in performance, forcing practitioners to manage several models. MatQuant leverages the nested structure of integer data types to enable a single model to be trained and served at various precision levels. The method also enhances the accuracy of int2 models by up to 10% compared to standard quantization techniques, showcasing its effectiveness in reducing the need for multiple quantized models.'}, 'zh': {'title': 'MatryoshkaÈáèÂåñÔºöÂçïÊ®°ÂûãÂ§öÁ≤æÂ∫¶ÊúçÂä°ÁöÑÂàõÊñ∞', 'desc': 'ÈáèÂåñÊ®°ÂûãÊùÉÈáçÂØπ‰∫éÂáèÂ∞ëÂ§ßÂûãÊ®°ÂûãÁöÑÈÄö‰ø°ÂíåÊé®ÁêÜÊàêÊú¨Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜÊ®°ÂûãÈáèÂåñÂà∞‰ΩéÁ≤æÂ∫¶ÔºàÂ¶Çint4Êàñint2ÔºâÊó∂ÔºåÊ®°ÂûãË¥®Èáè‰ºöÂèóÂà∞ÂΩ±ÂìçÔºåÂ∞§ÂÖ∂ÊòØint2‰ºöÊòæËëóÈôç‰ΩéÊ®°ÂûãÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÂ∞∫Â∫¶ÈáèÂåñÊäÄÊúØ‚Äî‚ÄîMatryoshkaÈáèÂåñÔºàMatQuantÔºâÔºåÂÆÉÂÖÅËÆ∏Âè™ËÆ≠ÁªÉÂíåÁª¥Êä§‰∏Ä‰∏™Ê®°ÂûãÔºåÂπ∂Âú®‰∏çÂêåÁ≤æÂ∫¶Á∫ßÂà´‰∏ãËøõË°åÊúçÂä°„ÄÇÈÄöËøáMatQuantÁöÑÂÖ±ÂêåËÆ≠ÁªÉÂíåÂÖ±ÂêåËí∏È¶èÊ≠£ÂàôÂåñÔºåÊèêÂèñÁöÑint2Á≤æÂ∫¶Ê®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÊØîÊ†áÂáÜÁöÑint2ÈáèÂåñÈ´òÂá∫10%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06788', 'title': 'EVEv2: Improved Baselines for Encoder-Free Vision-Language Models', 'url': 'https://huggingface.co/papers/2502.06788', 'abstract': 'Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.', 'score': 9, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '4374edb93ca102c6', 'authors': ['Haiwen Diao', 'Xiaotong Li', 'Yufeng Cui', 'Yueze Wang', 'Haoge Deng', 'Ting Pan', 'Wenxuan Wang', 'Huchuan Lu', 'Xinlong Wang'], 'affiliations': ['BAAI', 'BUPT', 'CASIA', 'DLUT', 'PKU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2502.06788.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#agi', '#multimodal'], 'emoji': 'üî¨', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: EVEv2.0 - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è EVEv2.0, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å—Å–ª–µ–¥—É—é—Ç —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ —Å —ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏ –∏ –±–µ–∑ –Ω–∏—Ö, —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö. –û–Ω–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∞—Å—Å–æ—Ü–∏–∞—Ü–∏—è –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –≤ –µ–¥–∏–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å–Ω–∏–∂–∞–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏—é –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏. EVEv2.0 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ —Å–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.'}, 'en': {'title': 'EVEv2.0: Bridging the Gap in Vision-Language Models Without Encoders', 'desc': 'This paper discusses the advancements in encoder-free vision-language models (VLMs) that are closing the performance gap with traditional encoder-based models. The authors explore the characteristics of these encoder-free VLMs and propose efficient strategies to enhance their performance. They introduce EVEv2.0, a new family of encoder-free VLMs that effectively integrates vision and language while minimizing interference. The study demonstrates that a well-structured training approach and hierarchical association of modalities lead to improved data efficiency and vision-reasoning capabilities.'}, 'zh': {'title': 'Êó†ÁºñÁ†ÅÂô®VLMÁöÑÊΩúÂäõ‰∏éÂàõÊñ∞', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊó†ÁºñÁ†ÅÂô®ÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ÊÄßËÉΩ‰∏ä‰∏éÂü∫‰∫éÁºñÁ†ÅÂô®ÁöÑÊ®°Âûã‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÊàë‰ª¨Á≥ªÁªüÊÄßÂú∞ÂàÜÊûê‰∫Ü‰ΩøÁî®È¢ÑËÆ≠ÁªÉËßÜËßâÁºñÁ†ÅÂô®ÂíåÁÆÄÁ∫¶ËßÜËßâÂ±ÇÁöÑÊó†ÁºñÁ†ÅÂô®VLMsÁöÑÁâπÊÄß„ÄÇÈÄöËøáÂºÄÂèëÈ´òÊïàÁöÑÁ≠ñÁï•ÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜEVEv2.0Ôºå‰∏Ä‰∏™ÊîπËøõÁöÑÊó†ÁºñÁ†ÅÂô®VLMÁ≥ªÂàóÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êï∞ÊçÆÊïàÁéáÂíåËßÜËßâÊé®ÁêÜËÉΩÂäõ‰∏äÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂêàÁêÜÂàÜËß£ÂíåÂ±ÇÊ¨°ÂÖ≥ËÅîËßÜËßâ‰∏éËØ≠Ë®ÄÂèØ‰ª•ÂáèÂ∞ëÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂπ≤Êâ∞ÔºåÂπ∂ÈÄöËøáËâØÂ•ΩÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÂÆûÁé∞ÊúâÊïà‰ºòÂåñ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03628', 'title': 'The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering', 'url': 'https://huggingface.co/papers/2502.03628', 'abstract': 'Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.', 'score': 9, 'issue_id': 2140, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '04b182d80abf9219', 'authors': ['Zhuowei Li', 'Haizhou Shi', 'Yunhe Gao', 'Di Liu', 'Zhenting Wang', 'Yuxiao Chen', 'Ting Liu', 'Long Zhao', 'Hao Wang', 'Dimitris N. Metaxas'], 'affiliations': ['Google DeepMind', 'Rutgers University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.03628.jpg', 'data': {'categories': ['#cv', '#training', '#hallucinations', '#benchmark', '#inference', '#interpretability', '#multimodal'], 'emoji': 'üîç', 'ru': {'title': '–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –º–µ—Ç–æ–¥ VISTA', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM) –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π, –∏–∑—É—á–∞—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ VISTA –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∏ —É—Å–∏–ª–µ–Ω–∏—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VISTA –≤ —Å—Ä–µ–¥–Ω–µ–º —Å–Ω–∏–∂–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –Ω–∞ 40% –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–∫—Ä—ã—Ç–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ —á–µ—Ç—ã—Ä–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'VISTA: Reducing Hallucination in Vision-Language Models', 'desc': 'This paper explores the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate plausible text that does not correspond to visual inputs. The authors identify three patterns in the generation process: a gradual loss of visual information, early activation of semantically meaningful tokens, and the presence of high-ranking visually grounded tokens that are not ultimately chosen. To address these issues, they introduce VISTA, a framework that enhances visual information during inference without requiring additional training. VISTA effectively reduces hallucination by about 40% and outperforms existing methods across multiple benchmarks and architectures.'}, 'zh': {'title': 'ÂáèÂ∞ëÂπªËßâÔºåÊèêÂçáÁúüÂÆû‰ø°ÊÅØÁöÑVISTAÊ°ÜÊû∂', 'desc': 'Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâËÉΩÂ§üÊúâÊïàÂú∞Â§ÑÁêÜÊñáÊú¨ÂíåËßÜËßâËæìÂÖ•Ôºå‰ΩÜÂÆÉ‰ª¨ÂæÄÂæÄ‰ºö‰∫ßÁîüËØ≠Ê≥ï‰∏äËøûË¥Ø‰ΩÜËßÜËßâ‰∏ä‰∏çÁúüÂÆûÁöÑÂÜÖÂÆπ„ÄÇÊú¨ÊñáÁ†îÁ©∂‰∫ÜÂπªËßâÁöÑÂÜÖÈÉ®Âä®ÊÄÅÔºåÂèëÁé∞LVLMsÂú®ÁîüÊàêËøáÁ®ã‰∏≠Â§ÑÁêÜ‰ø°ÊÅØÁöÑ‰∏âÁßçÂÖ≥ÈîÆÊ®°ÂºèÔºöÈÄêÊ∏ê‰∏ßÂ§±ËßÜËßâ‰ø°ÊÅØ„ÄÅÊó©ÊúüÊøÄÊ¥ªÂíåÈöêËóèÁöÑÁúüÂÆû‰ø°ÊÅØ„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVISTAÔºàËßÜËßâ‰ø°ÊÅØÂºïÂØº‰∏éÊ†áËÆ∞ÈÄªËæëÂ¢ûÂº∫ÔºâÔºåËøôÊòØ‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÁöÑÊé®ÁêÜÊó∂Âπ≤È¢ÑÊ°ÜÊû∂ÔºåÊó®Âú®ÂáèÂ∞ëÂπªËßâÂπ∂‰øÉËøõÁúüÂÆû‰ø°ÊÅØÁöÑÁîüÊàê„ÄÇÂÆûÈ™åË°®ÊòéÔºåVISTAÂú®ÂºÄÊîæÂºèÁîüÊàê‰ªªÂä°‰∏≠Âπ≥ÂùáÂáèÂ∞ë‰∫ÜÁ∫¶40%ÁöÑÂπªËßâÔºåÂπ∂Âú®Âõõ‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Âú®‰∏âÁßçËß£Á†ÅÁ≠ñÁï•‰∏ãÂßãÁªà‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06782', 'title': 'Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT', 'url': 'https://huggingface.co/papers/2502.06782', 'abstract': "Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.", 'score': 8, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '3b903654a6ff6710', 'authors': ['Dongyang Liu', 'Shicheng Li', 'Yutong Liu', 'Zhen Li', 'Kai Wang', 'Xinyue Li', 'Qi Qin', 'Yufei Liu', 'Yi Xin', 'Zhongyu Li', 'Bin Fu', 'Chenyang Si', 'Yuewen Cao', 'Conghui He', 'Ziwei Liu', 'Yu Qiao', 'Qibin Hou', 'Hongsheng Li', 'Peng Gao'], 'affiliations': ['Nankai University', 'Shanghai Correspondence AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.06782.jpg', 'data': {'categories': ['#video', '#architecture', '#synthetic', '#diffusion', '#audio', '#training'], 'emoji': 'üé¨', 'ru': {'title': 'Lumina-Video: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Lumina-Video - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ Diffusion Transformers (DiT). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Next-DiT, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —É—Ä–æ–≤–Ω—è—Ö –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å–ª–æ–≤–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–∏–Ω–∞–º–∏–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –≤–∏–¥–µ–æ. –ë–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π —Å—Ö–µ–º–µ –æ–±—É—á–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, Lumina-Video –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø–ª–∞–≤–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è –ø—Ä–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ.'}, 'en': {'title': 'Revolutionizing Video Generation with Lumina-Video', 'desc': 'This paper introduces Lumina-Video, a new framework for generating videos using the strengths of Diffusion Transformers, specifically the Next-DiT model. It addresses the challenges of capturing the complex movements and changes in video data by using a Multi-scale Next-DiT architecture that learns from different segments of the video. The framework also incorporates a motion score to allow for better control over the dynamics of the generated videos. Additionally, Lumina-Video employs a progressive training approach to improve the quality and smoothness of the output while maintaining efficiency in both training and inference.'}, 'zh': {'title': 'Lumina-VideoÔºöÈ´òÊïàÁîüÊàêËßÜÈ¢ëÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'ÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiTsÔºâÂú®ÁîüÊàêÂª∫Ê®°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂü∫‰∫éËøô‰∏ÄÊàêÂäüÔºåLumina-NextÂú®ÁîüÊàêÈÄºÁúüÂõæÂÉèÊñπÈù¢ÂèñÂæó‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩÔºå‰ΩÜÂú®ËßÜÈ¢ëÁîüÊàêÊñπÈù¢‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜLumina-VideoÊ°ÜÊû∂ÔºåÂÆÉÁªìÂêà‰∫ÜNext-DiTÁöÑ‰ºòÂäøÔºåÂπ∂ÈíàÂØπËßÜÈ¢ëÂêàÊàêÂºïÂÖ•‰∫ÜÂÆöÂà∂ÂåñÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÈÄöËøáÂ§öÂ∞∫Â∫¶Next-DiTÊû∂ÊûÑÂíåËøêÂä®ËØÑÂàÜÁöÑÂºïÂÖ•ÔºåLumina-VideoÂÆûÁé∞‰∫ÜÈ´òÊïà„ÄÅÁÅµÊ¥ªÁöÑËßÜÈ¢ëÁîüÊàêÔºåÂπ∂Âú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÊïàÁéá‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06764', 'title': 'History-Guided Video Diffusion', 'url': 'https://huggingface.co/papers/2502.06764', 'abstract': 'Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance', 'score': 8, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '66644a3e757a5d21', 'authors': ['Kiwhan Song', 'Boyuan Chen', 'Max Simchowitz', 'Yilun Du', 'Russ Tedrake', 'Vincent Sitzmann'], 'affiliations': ['MIT'], 'pdf_title_img': 'assets/pdf/title_img/2502.06764.jpg', 'data': {'categories': ['#diffusion', '#training', '#video', '#architecture'], 'emoji': 'üé¨', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –≥–∏–±–∫–æ–≥–æ –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Diffusion Forcing Transformer (DFoT), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –û–Ω–∏ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é History Guidance - —Å–µ–º–µ–π—Å—Ç–≤–æ –º–µ—Ç–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —É–ª—É—á—à–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –≤–∏–¥–µ–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –¥–≤–∏–∂–µ–Ω–∏—è –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Enhancing Video Diffusion with Flexible History Guidance', 'desc': 'This paper introduces the Diffusion Forcing Transformer (DFoT), a novel architecture designed for video diffusion that allows for flexible conditioning on a variable number of context frames. The authors address challenges related to fixed-size conditioning architectures and the inefficacy of traditional classifier-free guidance (CFG) methods when applied to variable-length history. They propose History Guidance, a set of techniques that leverage DFoT to improve video generation quality and temporal consistency. The results demonstrate that these methods enhance motion dynamics and enable the generation of longer videos with better compositional generalization.'}, 'zh': {'title': 'ÁÅµÊ¥ªÂéÜÂè≤ÂºïÂØºÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàêË¥®Èáè', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÊû∂ÊûÑÔºåÁß∞‰∏∫Diffusion Forcing TransformerÔºàDFoTÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Âú®ÂèØÂèòÈïøÂ∫¶ÂéÜÂè≤Â∏ßÊù°‰ª∂‰∏ãËøõË°åËßÜÈ¢ëÁîüÊàêÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÂèëÁé∞Ôºå‰º†ÁªüÁöÑÂàÜÁ±ªÂô®Êó†ÂÖ≥ÂºïÂØºÔºàCFGÔºâÊñπÊ≥ïÂú®Â§ÑÁêÜÂèØÂèòÈïøÂ∫¶ÂéÜÂè≤Êó∂ÊïàÊûú‰∏ç‰Ω≥ÔºåÂõ†Ê≠§Êàë‰ª¨ËÆæËÆ°‰∫ÜÊñ∞ÁöÑÂºïÂØºÊñπÊ≥ïÔºåÁß∞‰∏∫ÂéÜÂè≤ÂºïÂØº„ÄÇDFoTÂÖÅËÆ∏ÁÅµÊ¥ªÂú∞‰ΩøÁî®ÂéÜÂè≤Â∏ßËøõË°åÊù°‰ª∂ÁîüÊàêÔºå‰ªéËÄåÊòæËëóÊèêÈ´òËßÜÈ¢ëÁîüÊàêÁöÑË¥®ÈáèÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÈÄöËøáÂºïÂÖ•Êõ¥È´òÁ∫ßÁöÑÂéÜÂè≤ÂºïÂØºÊñπÊ≥ïÔºåÊàë‰ª¨Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫‰∫ÜËøêÂä®Âä®ÊÄÅÔºåÂπ∂ÂÆûÁé∞‰∫ÜÂØπË∂ÖÂá∫ÂàÜÂ∏ÉÂéÜÂè≤ÁöÑÁªÑÂêàÊ≥õÂåñ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05957', 'title': 'MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents', 'url': 'https://huggingface.co/papers/2502.05957', 'abstract': "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce MetaChain-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, MetaChain comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, MetaChain also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate MetaChain's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.", 'score': 7, 'issue_id': 2144, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': 'f3a18de353dcfad8', 'authors': ['Jiabin Tang', 'Tianyu Fan', 'Chao Huang'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.05957.jpg', 'data': {'categories': ['#rag', '#games', '#agents', '#benchmark', '#optimization', '#agi'], 'emoji': 'ü§ñ', 'ru': {'title': 'MetaChain: –ò–ò-–∞–≥–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –±–µ–∑ –∫–æ–¥–∞', 'desc': 'MetaChain - —ç—Ç–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏ —Å–∞–º–æ—É–ø—Ä–∞–≤–ª—è–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —É—Ç–∏–ª–∏—Ç, –¥–≤–∏–∂–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, —Å–∞–º–æ—É–ø—Ä–∞–≤–ª—è–µ–º–æ–π —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –∏ –º–æ–¥—É–ª—è —Å–∞–º–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤. MetaChain –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –∞–≥–µ–Ω—Ç—ã –∏ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –±–µ–∑ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑–∞–ª–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (RAG).'}, 'en': {'title': 'Empowering Everyone to Build LLM Agents with MetaChain', 'desc': 'This paper introduces MetaChain, a framework designed to allow users to create and deploy Large Language Model (LLM) agents using only natural language, eliminating the need for programming skills. MetaChain operates as an autonomous Agent Operating System, featuring components like an Actionable Engine and a Self-Managing File System to facilitate dynamic agent development. The framework addresses the accessibility gap in LLM agent creation, enabling a broader audience to leverage AI technology. Evaluations on the GAIA benchmark indicate that MetaChain outperforms existing methods in multi-agent tasks and demonstrates superior capabilities in Retrieval-Augmented Generation (RAG).'}, 'zh': {'title': 'ËÆ©ÊØè‰∏™‰∫∫ÈÉΩËÉΩÁî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊûÑÂª∫Êô∫ËÉΩ‰ª£ÁêÜ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂú®‰ªªÂä°Ëá™Âä®ÂåñÂíåÊô∫ËÉΩÂÜ≥Á≠ñÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁé∞ÊúâÁöÑÂºÄÂèëÊ°ÜÊû∂‰∏ªË¶ÅÈù¢ÂêëÊäÄÊúØËÉåÊôØÊ∑±ÂéöÁöÑÂºÄÂèëËÄÖÔºåÈôêÂà∂‰∫ÜÊôÆÈÄöÁî®Êà∑ÁöÑ‰ΩøÁî®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMetaChainÔºå‰∏Ä‰∏™ÂÆåÂÖ®Ëá™Âä®Âåñ‰∏îÈ´òÂ∫¶Ëá™ÊàëÂèëÂ±ïÁöÑÊ°ÜÊû∂ÔºåÂÖÅËÆ∏Áî®Êà∑‰ªÖÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÂàõÂª∫ÂíåÈÉ®ÁΩ≤LLM‰ª£ÁêÜ„ÄÇMetaChain‰Ωú‰∏∫‰∏Ä‰∏™Ëá™‰∏ª‰ª£ÁêÜÊìç‰ΩúÁ≥ªÁªüÔºåÂåÖÂê´Âõõ‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºåËÉΩÂ§üÈ´òÊïàÂä®ÊÄÅÂú∞ÂàõÂª∫Âíå‰øÆÊîπÂ∑•ÂÖ∑„ÄÅ‰ª£ÁêÜÂíåÂ∑•‰ΩúÊµÅÁ®ãÔºåËÄåÊó†ÈúÄÁºñÂÜô‰ª£Á†Å„ÄÇÁªèËøáGAIAÂü∫ÂáÜÁöÑÂÖ®Èù¢ËØÑ‰º∞ÔºåMetaChainÂú®ÈÄöÁî®Â§ö‰ª£ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âº∫Â§ßÁöÑËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06023', 'title': 'Dual Caption Preference Optimization for Diffusion Models', 'url': 'https://huggingface.co/papers/2502.06023', 'abstract': "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.", 'score': 6, 'issue_id': 2141, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': '07782353b3b8b697', 'authors': ['Amir Saeidi', 'Yiran Luo', 'Agneet Chatterjee', 'Shamanthak Hegde', 'Bimsara Pathiraja', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06023.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#dataset', '#diffusion', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–î–≤–æ–π–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –æ–ø–∏—Å–∞–Ω–∏—è–º', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Dual Caption Preference Optimization (DCPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. DCPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Pick-Double Caption —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –¥–ª—è –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –º–µ–Ω–µ–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DCPO –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–º–ø—Ç–∞–º –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Enhancing Image Generation with Dual Captions!', 'desc': 'This paper presents Dual Caption Preference Optimization (DCPO), a new method to enhance text-to-image diffusion models by addressing issues in human preference optimization. It identifies problems with existing preference datasets, such as overlapping distributions and irrelevant prompts that hinder the denoising process. To overcome these challenges, DCPO employs two distinct captions for preferred and less preferred images, utilizing a modified dataset called Pick-Double Caption. The results demonstrate that DCPO significantly improves image quality and relevance, outperforming several existing models across various evaluation metrics.'}, 'zh': {'title': 'ÂèåÈáçÊ†áÈ¢ò‰ºòÂåñÔºåÊèêÂçáÂõæÂÉèË¥®ÈáèÔºÅ', 'desc': 'ÊúÄËøëÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÂèëÂ±ïÁöÑ‰∫∫Á±ªÂÅèÂ•Ω‰ºòÂåñÊäÄÊúØÔºåÊòæÁ§∫Âá∫Âú®ÊîπËøõÊñáÊú¨Âà∞ÂõæÂÉèÊâ©Êï£Ê®°ÂûãÊñπÈù¢ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇËøô‰∫õÊñπÊ≥ïÊó®Âú®Â≠¶‰π†ÂÅèÂ•ΩÊ†∑Êú¨ÁöÑÂàÜÂ∏ÉÔºåÂπ∂Â∞ÜÂÖ∂‰∏é‰∏çÂ§™ÂÅèÂ•ΩÁöÑÊ†∑Êú¨Âå∫ÂàÜÂºÄÊù•„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂÅèÂ•ΩÊï∞ÊçÆÈõÜÈÄöÂ∏∏Â≠òÂú®ÂàÜÂ∏ÉÈáçÂè†ÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÂÜ≤Á™ÅÂàÜÂ∏É„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂèëÁé∞ËæìÂÖ•ÊèêÁ§∫‰∏≠ÂåÖÂê´‰∏é‰∏çÂ§™ÂÅèÂ•ΩÁöÑÂõæÂÉèÊó†ÂÖ≥ÁöÑ‰ø°ÊÅØÔºåËøôÈôêÂà∂‰∫ÜÂéªÂô™ÁΩëÁªúÂú®ÂÅèÂ•Ω‰ºòÂåñÊñπÊ≥ï‰∏≠ÁöÑÂáÜÁ°ÆÈ¢ÑÊµãËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂèåÈáçÊ†áÈ¢òÂÅèÂ•Ω‰ºòÂåñÔºàDCPOÔºâÔºåÂà©Áî®‰∏§‰∏™‰∏çÂêåÁöÑÊ†áÈ¢òÊù•ÂáèËΩªÊó†ÂÖ≥ÊèêÁ§∫ÁöÑÈóÆÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06155', 'title': 'Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile', 'url': 'https://huggingface.co/papers/2502.06155', 'abstract': 'Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.', 'score': 6, 'issue_id': 2140, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '2f8d5e54db328d39', 'authors': ['Hangliang Ding', 'Dacheng Li', 'Runlong Su', 'Peiyuan Zhang', 'Zhijie Deng', 'Ion Stoica', 'Hao Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.06155.jpg', 'data': {'categories': ['#training', '#diffusion', '#optimization', '#inference', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ (DiTs). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–æ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ 3D-–≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è —Å –Ω–∏–∑–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 7.4-7.8 —Ä–∞–∑ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ 720p —Å 29 –∏ 93 –∫–∞–¥—Ä–∞–º–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—Å–µ–≥–æ 0.1% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Speeding Up Video Generation with Efficient Attention Mechanisms', 'desc': 'This paper presents a solution to the inefficiency of Diffusion Transformers (DiTs) in generating high-fidelity videos. It introduces a method to prune 3D full attention by recognizing repetitive patterns in video data, leading to a sparse attention mechanism that reduces computational complexity. Additionally, the authors propose a multi-step consistency distillation approach to shorten the sampling process, allowing for faster video generation. The resulting model, Open-Sora-Plan-1.2, achieves significant speed improvements while maintaining performance, especially when utilizing distributed inference across multiple GPUs.'}, 'zh': {'title': 'È´òÊïàËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑDiffusion TransformersÔºàDiTsÔºâÊ®°ÂûãÔºå‰ª•Ëß£ÂÜ≥ÁîüÊàêÈ´ò‰øùÁúüËßÜÈ¢ëÊó∂ÁöÑÊïàÁéáÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÈÄöËøáËØÜÂà´ËßÜÈ¢ëÊï∞ÊçÆ‰∏≠ÁöÑÂÜó‰ΩôÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ®ÄÁñèÁöÑ3DÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ΩøÂÖ∂Âú®ËßÜÈ¢ëÂ∏ßÊï∞Èáè‰∏äÂÖ∑ÊúâÁ∫øÊÄßÂ§çÊùÇÂ∫¶„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÈááÁî®Â§öÊ≠•‰∏ÄËá¥ÊÄßËí∏È¶èÊäÄÊúØÔºåÁº©Áü≠‰∫ÜÈááÊ†∑ËøáÁ®ãÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊõ¥Âø´ÈÄüÁöÑËßÜÈ¢ëÁîüÊàê„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®‰ΩøÁî®ÊûÅÂ∞ëÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÊó∂ÔºåÁîüÊàêÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü7.4Âà∞7.8ÂÄçÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05795', 'title': 'The Curse of Depth in Large Language Models', 'url': 'https://huggingface.co/papers/2502.05795', 'abstract': 'In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models(LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.', 'score': 5, 'issue_id': 2147, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': '3b1a3626926ac2f4', 'authors': ['Wenfang Sun', 'Xinyuan Song', 'Pengxiang Li', 'Lu Yin', 'Yefeng Zheng', 'Shiwei Liu'], 'affiliations': ['Dalian University of Technology, China', 'Emory University, USA', 'Medical Artificial Intelligence Laboratory, Westlake University, China', 'University of Surrey, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.05795.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': 'üß†', 'ru': {'title': "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ '–ü—Ä–æ–∫–ª—è—Ç–∏—è –≥–ª—É–±–∏–Ω—ã' –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", 'desc': "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è '–ü—Ä–æ–∫–ª—è—Ç–∏—è –≥–ª—É–±–∏–Ω—ã', –æ–±—ä—è—Å–Ω—è—é—â–∞—è –Ω–∏–∑–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ—á—Ç–∏ –ø–æ–ª–æ–≤–∏–Ω—ã —Å–ª–æ–µ–≤ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —à–∏—Ä–æ–∫–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —è–≤–ª–µ–Ω–∏—è —Å—Ä–µ–¥–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å–µ–º–µ–π—Å—Ç–≤ LLM, —Ç–∞–∫–∏—Ö –∫–∞–∫ Llama, Mistral, DeepSeek –∏ Qwen. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–∏—á–∏–Ω–æ–π –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≥–ª—É–±–æ–∫–∏—Ö —Å–ª–æ–µ–≤ —è–≤–ª—è–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–µ–≤ (Pre-LN). –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LayerNorm, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–∫–ª–∞–¥ –≥–ª—É–±–æ–∫–∏—Ö —Å–ª–æ–µ–≤ –≤ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏."}, 'en': {'title': 'Unlocking the Power of Deep Layers in LLMs', 'desc': "This paper introduces the 'Curse of Depth', which describes a problem in Large Language Models (LLMs) where many layers do not perform as well as expected. The authors find that this issue is common in popular LLMs like Llama and Mistral, and it stems from the use of Pre-Layer Normalization (Pre-LN). Pre-LN helps stabilize training but leads to increased output variance in deeper layers, making them less effective. To address this, the authors propose LayerNorm Scaling, which reduces the output variance of deeper layers, resulting in improved training performance and better contributions from these layers."}, 'zh': {'title': 'Ëß£ÂÜ≥Ê∑±Â∫¶Ê®°ÂûãÁöÑËÆ≠ÁªÉÂõ∞Â¢É', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜÊ∑±Â∫¶ËØÖÂííÁöÑÊ¶ÇÂøµÔºåÂº∫Ë∞É‰∫ÜÁé∞‰ª£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠Ëøë‰∏ÄÂçäÂ±ÇÁöÑÊïàÊûú‰Ωé‰∫éÈ¢ÑÊúüÁöÑÁé∞Ë±°„ÄÇÊàë‰ª¨Á°ÆËÆ§‰∫ÜËøô‰∏ÄÁé∞Ë±°Âú®ÊµÅË°åÁöÑLLMÂÆ∂Êóè‰∏≠ÊôÆÈÅçÂ≠òÂú®ÔºåÂ¶ÇLlama„ÄÅMistral„ÄÅDeepSeekÂíåQwen„ÄÇÂàÜÊûêË°®ÊòéÔºåÊ∑±Â±ÇÊó†ÊïàÁöÑÊ†πÊú¨ÂéüÂõ†ÊòØÂπøÊ≥õ‰ΩøÁî®ÁöÑÈ¢ÑÂ±ÇÂΩí‰∏ÄÂåñÔºàPre-LNÔºâÔºåÂÆÉÂØºËá¥ËæìÂá∫ÊñπÂ∑ÆÈöèÁùÄÊ®°ÂûãÊ∑±Â∫¶ÁöÑÂ¢ûÂä†ËÄåÊåáÊï∞Â¢ûÈïø„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ±ÇÂΩí‰∏ÄÂåñÁº©ÊîæÔºàLayerNorm ScalingÔºâÔºåÈÄöËøáÂØπÂ±ÇÂΩí‰∏ÄÂåñÁöÑËæìÂá∫ÊñπÂ∑ÆËøõË°åÁº©ÊîæÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ∑±Â±ÇÁöÑË¥°ÁåÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06527', 'title': 'CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2502.06527', 'abstract': 'Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.', 'score': 5, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '44b3a6931980556a', 'authors': ['D. She', 'Mushui Liu', 'Jingxuan Pang', 'Jin Wang', 'Zhen Yang', 'Wanggui He', 'Guanghao Zhang', 'Yi Wang', 'Qihan Huang', 'Haobin Tang', 'Yunlong Yu', 'Siming Fu'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'University of Science and Technology of China', 'Zhejiang Univerisity'], 'pdf_title_img': 'assets/pdf/title_img/2502.06527.jpg', 'data': {'categories': ['#diffusion', '#video', '#benchmark', '#3d'], 'emoji': 'üé¨', 'ru': {'title': 'CustomVideoX: –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è', 'desc': 'CustomVideoX - —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ—Å–µ—Ç–∏ –∏ –æ–±—É—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LoRA –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–µ 3D Reference Attention –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ –≤—Å–µ–º–∏ –∫–∞–¥—Ä–∞–º–∏ –≤–∏–¥–µ–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –∏–∑–º–µ—Ä–µ–Ω–∏—è—Ö. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ Time-Aware Reference Attention Bias –∏ Entity Region-Aware Enhancement.'}, 'en': {'title': 'Revolutionizing Personalized Video Generation with CustomVideoX', 'desc': 'This paper presents CustomVideoX, a new framework designed for generating personalized videos from a reference image. It utilizes a video diffusion transformer and focuses on training LoRA parameters to efficiently extract features from the reference image. The framework introduces 3D Reference Attention to enhance the interaction between the reference image and video frames, addressing temporal inconsistencies. Additionally, it employs the Time-Aware Reference Attention Bias and the Entity Region-Aware Enhancement modules to improve video quality and consistency, validated by a new benchmark called VideoBench.'}, 'zh': {'title': '‰∏™ÊÄßÂåñËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': '‰∏™ÊÄßÂåñËßÜÈ¢ëÁîüÊàêÂú®ÂõæÂÉèÂêàÊàêÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÁî±‰∫éÊó∂Èó¥‰∏ç‰∏ÄËá¥ÊÄßÂíåË¥®Èáè‰∏ãÈôçÔºå‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜCustomVideoXÔºå‰∏Ä‰∏™ÂàõÊñ∞Ê°ÜÊû∂ÔºåÂà©Áî®ËßÜÈ¢ëÊâ©Êï£ÂèòÊç¢Âô®‰ªéÂèÇËÄÉÂõæÂÉèÁîüÊàê‰∏™ÊÄßÂåñËßÜÈ¢ë„ÄÇCustomVideoXÈÄöËøá‰∏ìÈó®ËÆ≠ÁªÉLoRAÂèÇÊï∞Êù•ÊèêÂèñÂèÇËÄÉÁâπÂæÅÔºåÁ°Æ‰øù‰∫ÜÊïàÁéáÂíåÈÄÇÂ∫îÊÄß„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü3DÂèÇËÄÉÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ª•‰æøÂú®Á©∫Èó¥ÂíåÊó∂Èó¥Áª¥Â∫¶‰∏äÁõ¥Êé•ÂíåÂêåÊó∂Âú∞Â∞ÜÂèÇËÄÉÂõæÂÉèÁâπÂæÅ‰∏éÊâÄÊúâËßÜÈ¢ëÂ∏ßËøõË°å‰∫§‰∫í„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05431', 'title': 'APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding', 'url': 'https://huggingface.co/papers/2502.05431', 'abstract': "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (APE), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5times speedup by reducing 28times prefilling time for a 128K-length context.", 'score': 5, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 8', 'zh': '2Êúà8Êó•'}, 'hash': '7bc5b7aeb3716893', 'authors': ['Xinyu Yang', 'Tianqi Chen', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2502.05431.jpg', 'data': {'categories': ['#rag', '#optimization', '#inference', '#long_context'], 'emoji': '‚ö°', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (APE) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. APE –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è—Ç—å –∏ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å KV-—Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—â–∏–π –ø—Ä–µ—Ñ–∏–∫—Å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –≤–Ω–∏–º–∞–Ω–∏—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π —Ñ–∞–∫—Ç–æ—Ä. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ APE —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ 98% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –æ–±—ã—á–Ω–æ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 3.6-7.9% –≤ –∑–∞–¥–∞—á–∞—Ö RAG –∏ ICL.'}, 'en': {'title': 'Boosting Efficiency in Context-Augmented Generation with APE', 'desc': 'This paper introduces Adaptive Parallel Encoding (APE) as a solution to improve the efficiency of context-augmented generation (CAG) techniques like RAG and ICL. Traditional methods face high computational costs when combining multiple contexts for generating responses, as they require re-encoding for each request. APE allows for the pre-computation and caching of key-value (KV) states for each context, which can then be loaded during inference, significantly speeding up the process. The proposed method aligns the attention distribution of parallel encoding with that of sequential encoding, achieving high performance while handling many contexts efficiently.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÂπ∂Ë°åÁºñÁ†ÅÔºöÊèêÂçá‰∏ä‰∏ãÊñáÁîüÊàêÊïàÁéáÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏ä‰∏ãÊñáÂ¢ûÂº∫ÁîüÊàêÔºàCAGÔºâÊäÄÊúØ‰∏≠ÁöÑÂπ∂Ë°åÁºñÁ†ÅÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÁîüÊàêÁî®Êà∑Êü•ËØ¢ÂìçÂ∫îÁöÑÊïàÁéá„ÄÇ‰º†ÁªüÊñπÊ≥ïÂú®ÊØèÊ¨°ËØ∑Ê±ÇÊó∂ÈÉΩÈúÄË¶ÅÈáçÊñ∞ÁºñÁ†ÅÂ§ö‰∏™‰∏ä‰∏ãÊñáÔºåÂØºËá¥ËÆ°ÁÆóË¥üÊãÖËøáÈáç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜËá™ÈÄÇÂ∫îÂπ∂Ë°åÁºñÁ†ÅÔºàAPEÔºâÔºåÈÄöËøáÂÖ±‰∫´ÂâçÁºÄ„ÄÅÊ≥®ÊÑèÂäõÊ∏©Â∫¶ÂíåÁº©ÊîæÂõ†Â≠êÊù•Ë∞ÉÊï¥Âπ∂Ë°åÁºñÁ†Å‰∏éÈ°∫Â∫èÁºñÁ†ÅÁöÑÊ≥®ÊÑèÂäõÂàÜÂ∏ÉÔºå‰ªéËÄåÊèêÈ´òÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAPEÂú®‰øùÊåÅÈ´òÊÄßËÉΩÁöÑÂêåÊó∂ÔºåËÉΩÂ§üÊòæËëóÂä†Âø´Â§ÑÁêÜÈÄüÂ∫¶ÔºåÈÄÇÁî®‰∫éÂ§ÑÁêÜÂ§ßÈáè‰∏ä‰∏ãÊñá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04370', 'title': 'DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2502.04370', 'abstract': 'Text-to-3D generation automates 3D content creation from textual descriptions, which offers transformative potential across various fields. However, existing methods often struggle to align generated content with human preferences, limiting their applicability and flexibility. To address these limitations, in this paper, we propose DreamDPO, an optimization-based framework that integrates human preferences into the 3D generation process, through direct preference optimization. Practically, DreamDPO first constructs pairwise examples, then compare their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D representation with a preference-driven loss function. By leveraging pairwise comparison to reflect preferences, DreamDPO reduces reliance on precise pointwise quality evaluations while enabling fine-grained controllability through preference-guided optimization. Experiments demonstrate that DreamDPO achieves competitive results, and provides higher-quality and more controllable 3D content compared to existing methods. The code and models will be open-sourced.', 'score': 4, 'issue_id': 2145, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'a475f5281f318a1e', 'authors': ['Zhenglin Zhou', 'Xiaobo Xia', 'Fan Ma', 'Hehe Fan', 'Yi Yang', 'Tat-Seng Chua'], 'affiliations': ['National University of Singapore', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.04370.jpg', 'data': {'categories': ['#training', '#optimization', '#3d', '#alignment', '#open_source', '#multimodal'], 'emoji': 'üé®', 'ru': {'title': '–°–æ–∑–¥–∞–Ω–∏–µ 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —É—á–µ—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π', 'desc': 'DreamDPO - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏–ª–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. DreamDPO –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DreamDPO –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π 3D-–∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'DreamDPO: Aligning 3D Generation with Human Preferences', 'desc': 'This paper introduces DreamDPO, a new framework for generating 3D content from text that incorporates human preferences. It uses an optimization approach that focuses on pairwise comparisons to better align the generated 3D models with what people actually want. By employing a preference-driven loss function, DreamDPO enhances the quality and control of the generated content without needing exact quality scores. The results show that DreamDPO outperforms existing methods, making it a significant advancement in text-to-3D generation.'}, 'zh': {'title': 'DreamDPOÔºöÂ∞Ü‰∫∫Á±ªÂÅèÂ•ΩËûçÂÖ•3DÁîüÊàêÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'ÊñáÊú¨Âà∞3DÁîüÊàêÊäÄÊúØÂèØ‰ª•Ê†πÊçÆÊñáÊú¨ÊèèËø∞Ëá™Âä®ÂàõÂª∫3DÂÜÖÂÆπÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ÁîüÊàêÂÜÖÂÆπ‰∏é‰∫∫Á±ªÂÅèÂ•Ω‰πãÈó¥ÁöÑÂØπÈΩê‰∏äÂ≠òÂú®Âõ∞ÈöæÔºåÈôêÂà∂‰∫ÜÂÖ∂ÈÄÇÁî®ÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜDreamDPOÔºå‰∏Ä‰∏™Âü∫‰∫é‰ºòÂåñÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÂ∞Ü‰∫∫Á±ªÂÅèÂ•ΩËûçÂÖ•3DÁîüÊàêËøáÁ®ã‰∏≠„ÄÇÂÆûÈ™åË°®ÊòéÔºåDreamDPOÂú®ÁîüÊàêÈ´òË¥®ÈáèÂíåÂèØÊéßÁöÑ3DÂÜÖÂÆπÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06635', 'title': 'Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM', 'url': 'https://huggingface.co/papers/2502.06635', 'abstract': "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.", 'score': 4, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '2deb5075264d7660', 'authors': ['Qingshui Gu', 'Shu Li', 'Tianyu Zheng', 'Zhaoxiang Zhang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Institute of Automation, Chinese Academy of Sciences', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06635.jpg', 'data': {'categories': ['#small_models', '#multilingual', '#training', '#data', '#benchmark', '#open_source', '#dataset', '#low_resource'], 'emoji': 'üá®üá≥', 'ru': {'title': '–°–æ–∑–¥–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∫–∏—Ç–∞–π—Å–∫–æ—è–∑—ã—á–Ω–æ–π LLM —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º', 'desc': 'Steel-LLM - —ç—Ç–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å –Ω—É–ª—è –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö. –ú–æ–¥–µ–ª—å —Å 1 –º–∏–ª–ª–∏–∞—Ä–¥–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ. Steel-LLM –ø–æ–∫–∞–∑–∞–ª–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö CEVAL –∏ CMMLU, –ø—Ä–µ–≤–∑–æ–π–¥—è —Ä–∞–Ω–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã—Ö –∏–Ω—Å—Ç–∏—Ç—É—Ç–æ–≤. –°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, –≤–∫–ª—é—á–∞—è —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –¥–∏–∑–∞–π–Ω –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering Chinese NLP with Steel-LLM: Open-Source Innovation', 'desc': "Steel-LLM is a language model specifically designed for the Chinese language, built from the ground up to be open-source and accessible. It features 1 billion parameters and was trained on a large dataset primarily consisting of Chinese text, with some English data to fill existing gaps. The model has shown strong performance on various benchmarks, surpassing earlier models from larger organizations. This paper details the project's contributions, including data collection, model architecture, training techniques, and the challenges faced, serving as a guide for others in the field of language model development."}, 'zh': {'title': 'ÊâìÈÄ†‰∏≠Êñá‰ºòË¥®ÂºÄÊ∫êËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé¢Á¥¢', 'desc': 'Steel-LLMÊòØ‰∏Ä‰∏™‰ª•‰∏≠Êñá‰∏∫‰∏≠ÂøÉÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®Âú®ÊúâÈôêÁöÑËÆ°ÁÆóËµÑÊ∫ê‰∏ãÂºÄÂèëÂá∫È´òË¥®ÈáèÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÇËØ•È°πÁõÆ‰∫é2024Âπ¥3ÊúàÂêØÂä®ÔºåËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Êã•Êúâ10‰∫øÂèÇÊï∞ÁöÑÂ§ßËßÑÊ®°Ê®°ÂûãÔºåÈáçÁÇπÂÖ≥Ê≥®ÈÄèÊòéÂ∫¶ÂíåÂÆûÁî®ËßÅËß£ÁöÑÂàÜ‰∫´„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏≠‰∏ªË¶Å‰ΩøÁî®‰∏≠ÊñáÊï∞ÊçÆÔºåÂπ∂ÈÄÇÈáèÂåÖÂê´Ëã±ÊñáÊï∞ÊçÆÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâÂºÄÊ∫êÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫ÁôΩ„ÄÇSteel-LLMÂú®CEVALÂíåCMMLUÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåË∂ÖË∂ä‰∫ÜÂ§ßÂûãÊú∫ÊûÑÁöÑÊó©ÊúüÊ®°ÂûãÔºå‰∏∫Á†îÁ©∂‰∫∫ÂëòÂíåÂÆûË∑µËÄÖÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑËµÑÊ∫ê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06776', 'title': 'Towards Internet-Scale Training For Agents', 'url': 'https://huggingface.co/papers/2502.06776', 'abstract': 'The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web. Code will be available at: data-for-agents.github.io.', 'score': 1, 'issue_id': 2157, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '5ca8acf0bf4a0a58', 'authors': ['Brandon Trabucco', 'Gunnar Sigurdsson', 'Robinson Piramuthu', 'Ruslan Salakhutdinov'], 'affiliations': ['Amazon', 'Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06776.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#data', '#open_source', '#training'], 'emoji': 'üåê', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤: –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–Ω–≤–µ–π–µ—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (–õ–Ø–ú) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –¥–ª—è 150 —Ç—ã—Å—è—á —Å–∞–π—Ç–æ–≤, –∞–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –õ–Ø–ú –≤—ã–ø–æ–ª–Ω—è—é—Ç —ç—Ç–∏ –∑–∞–¥–∞—á–∏, –∞ –∑–∞—Ç–µ–º –¥—Ä—É–≥–∞—è –õ–Ø–ú –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–¥–∞—á –∏ –æ—Ü–µ–Ω–∫–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç—Ç–∏–º –∫–æ–Ω–≤–µ–π–µ—Ä–æ–º, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ —Å –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Automating Web Navigation Agent Training with LLMs', 'desc': "This paper presents a new method for training web navigation agents that reduces reliance on human-generated data. It introduces a pipeline where a large language model (LLM) creates tasks for a vast number of websites, allowing agents to learn from these automatically generated tasks. The LLM also evaluates the agents' performance, achieving high accuracy in detecting harmful content and judging task success. The results show that training with this pipeline can significantly enhance the agents' ability to generalize across diverse websites compared to traditional human data methods."}, 'zh': {'title': 'Êó†È°ª‰∫∫Â∑•Ê†áÊ≥®ÔºåÊô∫ËÉΩ‰ª£ÁêÜËá™ÊàëËÆ≠ÁªÉÔºÅ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÊù•ËÆ≠ÁªÉÁΩëÁªúÂØºËà™‰ª£ÁêÜÔºåÈÅøÂÖç‰∫ÜÁπÅÁêêÁöÑ‰∫∫Á±ªÊ†áÊ≥®„ÄÇÈ¶ñÂÖàÔºå‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏∫150,000‰∏™‰∏çÂêåÁöÑÁΩëÁ´ôÁîüÊàê‰ªªÂä°„ÄÇÊé•ÁùÄÔºåLLM‰ª£ÁêÜÂÆåÊàêËøô‰∫õ‰ªªÂä°Âπ∂ÁîüÊàêËΩ®ËøπÔºåÊúÄÂêéÂÜçÁî±LLMÂØπËΩ®ËøπËøõË°åËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éÊàë‰ª¨ÁöÑÊñπÊ≥ïËÆ≠ÁªÉÁöÑ‰ª£ÁêÜÂú®Â§öÊ†∑ÂåñÁΩëÁ´ô‰∏äË°®Áé∞‰ºòÂºÇÔºå‰∏îÂú®Êï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÊòæËëóÊèêÈ´ò‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18676', 'title': 'Embodied Red Teaming for Auditing Robotic Foundation Models', 'url': 'https://huggingface.co/papers/2411.18676', 'abstract': 'Language-conditioned robot models have the potential to enable robots to perform a wide range of tasks based on natural language instructions. However, assessing their safety and effectiveness remains challenging because it is difficult to test all the different ways a single task can be phrased. Current benchmarks have two key limitations: they rely on a limited set of human-generated instructions, missing many challenging cases, and focus only on task performance without assessing safety, such as avoiding damage. To address these gaps, we introduce Embodied Red Teaming (ERT), a new evaluation method that generates diverse and challenging instructions to test these models. ERT uses automated red teaming techniques with Vision Language Models (VLMs) to create contextually grounded, difficult instructions. Experimental results show that state-of-the-art language-conditioned robot models fail or behave unsafely on ERT-generated instructions, underscoring the shortcomings of current benchmarks in evaluating real-world performance and safety. Code and videos are available at: https://s-karnik.github.io/embodied-red-team-project-page.', 'score': 0, 'issue_id': 2154, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '8f3c4c8885d5b2d0', 'authors': ['Sathwik Karnik', 'Zhang-Wei Hong', 'Nishant Abhangi', 'Yen-Chen Lin', 'Tsun-Hsuan Wang', 'Christophe Dupuy', 'Rahul Gupta', 'Pulkit Agrawal'], 'affiliations': ['Amazon', 'Improbable AI Lab', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2411.18676.jpg', 'data': {'categories': ['#benchmark', '#robotics', '#alignment', '#security', '#agents'], 'emoji': 'ü§ñ', 'ru': {'title': '–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Embodied Red Teaming (ERT). ERT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ red teaming –≤–º–µ—Å—Ç–µ —Å Vision Language Models –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ —á–∞—Å—Ç–æ –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ ERT. –≠—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Enhancing Robot Safety with Embodied Red Teaming', 'desc': 'This paper introduces a new evaluation method called Embodied Red Teaming (ERT) for assessing language-conditioned robot models. ERT generates diverse and challenging instructions using automated red teaming techniques combined with Vision Language Models (VLMs). The study reveals that existing benchmarks are inadequate, as they do not cover a wide range of task phrasings and fail to evaluate safety measures. Experimental results indicate that current state-of-the-art models often fail or act unsafely when faced with ERT-generated instructions, highlighting the need for improved evaluation methods.'}, 'zh': {'title': 'ÂÖ∑Ë∫´Á∫¢ÈòüËØÑ‰º∞ÔºöÊèêÂçáÊú∫Âô®‰∫∫ÂÆâÂÖ®ÊÄß‰∏éÊúâÊïàÊÄßÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞ÊñπÊ≥ïÔºåÁß∞‰∏∫ÂÖ∑Ë∫´Á∫¢ÈòüËØÑ‰º∞ÔºàERTÔºâÔºåÊó®Âú®ÊµãËØïËØ≠Ë®ÄÊù°‰ª∂Êú∫Âô®‰∫∫Ê®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÂíåÊúâÊïàÊÄß„ÄÇÂΩìÂâçÁöÑËØÑ‰º∞Âü∫ÂáÜÂ≠òÂú®Â±ÄÈôêÊÄßÔºå‰∏ªË¶Å‰æùËµñ‰∫éÊúâÈôêÁöÑ‰∫∫Á±ªÁîüÊàêÊåá‰ª§Ôºå‰∏îÊú™ËÉΩËÄÉËôëÂÆâÂÖ®ÊÄßÈóÆÈ¢ò„ÄÇERTÈÄöËøáËá™Âä®ÂåñÁ∫¢ÈòüÊäÄÊúØ‰∏éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁªìÂêàÔºåÁîüÊàêÂ§öÊ†∑‰∏îÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊåá‰ª§Ôºå‰ª•Êõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞Êú∫Âô®‰∫∫Ê®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁé∞ÊúâÁöÑÂÖàËøõËØ≠Ë®ÄÊù°‰ª∂Êú∫Âô®‰∫∫Ê®°ÂûãÂú®ERTÁîüÊàêÁöÑÊåá‰ª§‰∏äË°®Áé∞‰∏ç‰Ω≥Êàñ‰∏çÂÆâÂÖ®ÔºåÁ™ÅÊòæ‰∫ÜÂΩìÂâçËØÑ‰º∞Âü∫ÂáÜÂú®ÁúüÂÆû‰∏ñÁïåÊÄßËÉΩÂíåÂÆâÂÖ®ÊÄßËØÑ‰º∞‰∏≠ÁöÑ‰∏çË∂≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01237', 'title': 'The Differences Between Direct Alignment Algorithms are a Blur', 'url': 'https://huggingface.co/papers/2502.01237', 'abstract': 'Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the beta parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms.', 'score': 79, 'issue_id': 2022, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '18ba45e237fff5e1', 'authors': ['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.01237.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment'], 'emoji': 'üéØ', 'ru': {'title': '–ü—Ä—è–º–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ: –ø—Ä–æ—Å—Ç–æ–π –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø—Ä—è–º–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (DAA) –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç DAA –ø–æ —Ç–∏–ø—É —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º –Ω–∞–≥—Ä–∞–¥–∞–º –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ–¥–Ω–æ—ç—Ç–∞–ø–Ω—ã–µ, –∞ –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —è–≤–ª—è–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ø–∞—Ä–Ω—ã—Ö, –∞ –Ω–µ –ø–æ—Ç–æ—á–µ—á–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Simplifying Language Model Alignment with Direct Optimization', 'desc': 'This paper introduces Direct Alignment Algorithms (DAAs) as a simpler alternative to traditional Reinforcement Learning from Human Feedback (RLHF) methods. DAAs optimize language model alignment directly, avoiding the complexities of reinforcement learning and reward modeling. The study compares one-stage and two-stage methods, revealing that incorporating a Supervised Fine-Tuning (SFT) phase significantly enhances performance. The findings emphasize the importance of the choice between pairwise and pointwise ranking losses in achieving effective alignment, cautioning against hasty conclusions about algorithm superiority.'}, 'zh': {'title': '‰ºòÂåñÂØπÈΩêÁÆóÊ≥ïÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩÔºÅ', 'desc': 'Áõ¥Êé•ÂØπÈΩêÁÆóÊ≥ïÔºàDAAsÔºâÈÄöËøáÁõ¥Êé•‰ºòÂåñÁ≠ñÁï•Êù•ÁÆÄÂåñËØ≠Ë®ÄÊ®°ÂûãÁöÑÂØπÈΩêÔºåÂèñ‰ª£‰∫Ü‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÂº∫ÂåñÂ≠¶‰π†ÂíåÂ•ñÂä±Âª∫Ê®°„ÄÇDAAsÂèØ‰ª•Ê†πÊçÆÂÖ∂ÊéíÂêçÊçüÂ§±ÔºàÊàêÂØπ‰∏éÁÇπÂØπÔºâÂíå‰ΩøÁî®ÁöÑÂ•ñÂä±Á±ªÂûãËøõË°åÂàÜÁ±ª„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰∏ÄÈò∂ÊÆµÊñπÊ≥ïÁöÑË°®Áé∞‰∏çÂ¶Ç‰∏§Èò∂ÊÆµÊñπÊ≥ïÔºåÂõ†Ê≠§Êàë‰ª¨ÂºïÂÖ•‰∫ÜÊòæÂºèÁöÑÁõëÁù£ÂæÆË∞ÉÈò∂ÊÆµÔºåÂπ∂Âú®ÂçïÈò∂ÊÆµÁöÑORPOÂíåASFT‰∏≠Âä†ÂÖ•‰∫ÜÊéßÂà∂ÂÅèÂ•Ω‰ºòÂåñÂº∫Â∫¶ÁöÑbetaÂèÇÊï∞„ÄÇËøô‰∫õÊîπËøõ‰ΩøÂæóÂÆÉ‰ª¨Âú®Alpaca Eval 2‰∏≠ÁöÑË°®Áé∞ÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçáÔºåË°®ÊòéÈÄâÊã©ÊàêÂØπÊàñÁÇπÂØπÁõÆÊ†áÊòØÂÖ≥ÈîÆÂõ†Á¥†ÔºåËÄå‰∏çÊòØÂÖ∑‰ΩìÁöÑÈöêÂºèÂ•ñÂä±ÊàñÊçüÂ§±ÂáΩÊï∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01061', 'title': 'OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models', 'url': 'https://huggingface.co/papers/2502.01061', 'abstract': 'End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)', 'score': 74, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '56b819a66e336562', 'authors': ['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.01061.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#training', '#diffusion'], 'emoji': 'üé≠', 'ru': {'title': 'OmniHuman: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏', 'desc': 'OmniHuman - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–º–µ—à–∞–Ω–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏ —Å–ª–æ–∂–Ω—ã–µ –ø–æ–∑—ã —Ç–µ–ª–∞. OmniHuman –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ, –≤–∏–¥–µ–æ –∏–ª–∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è.'}, 'en': {'title': 'OmniHuman: Revolutionizing Realistic Human Animation Generation', 'desc': 'The paper presents OmniHuman, a new framework for generating realistic human animations from audio inputs. It utilizes a Diffusion Transformer architecture that enhances the training process by incorporating motion-related conditions, allowing for better scalability in video generation. OmniHuman is designed to handle various types of human portraits and interactions, producing high-quality videos that can depict talking, singing, and complex body movements. This approach not only improves the realism of the generated videos but also increases flexibility by supporting multiple input modalities such as audio and video.'}, 'zh': {'title': 'OmniHumanÔºöÁÅµÊ¥ªÁúüÂÆûÁöÑ‰∫∫Á±ªÂä®ÁîªÁîüÊàê', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫OmniHumanÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçá‰∫∫Á±ªÂä®ÁîªÁîüÊàêÁöÑË¥®ÈáèÂíåÁÅµÊ¥ªÊÄß„ÄÇËØ•Ê°ÜÊû∂Âü∫‰∫éÊâ©Êï£ÂèòÊç¢Âô®ÔºåÈÄöËøáÂú®ËÆ≠ÁªÉÈò∂ÊÆµÊ∑∑Âêà‰∏éËøêÂä®Áõ∏ÂÖ≥ÁöÑÊù°‰ª∂Êù•Êâ©Â±ïÊï∞ÊçÆËßÑÊ®°„ÄÇOmniHumanÊîØÊåÅÂ§öÁßç‰∫∫ÂÉèÂÜÖÂÆπÂíå‰∏çÂêåÁöÑÈ©±Âä®Ê®°ÂºèÔºåÂ¶ÇÈü≥È¢ëÈ©±Âä®ÂíåËßÜÈ¢ëÈ©±Âä®ÔºåËÉΩÂ§üÁîüÊàêÈ´òÂ∫¶ÁúüÂÆûÁöÑ‰∫∫Á±ªËßÜÈ¢ë„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåOmniHuman‰∏ç‰ªÖÁîüÊàêÊõ¥ÁúüÂÆûÁöÑËßÜÈ¢ëÔºåËøòÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑËæìÂÖ•ÁÅµÊ¥ªÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01456', 'title': 'Process Reinforcement through Implicit Rewards', 'url': 'https://huggingface.co/papers/2502.01456', 'abstract': "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.", 'score': 41, 'issue_id': 2019, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '9d62c40e4bafac91', 'authors': ['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Shanghai Jiaotong University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.01456.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'PRIME: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò-–º–æ–¥–µ–ª–µ–π —Å –Ω–µ—è–≤–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π PRIME. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—è–≤–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤ —Ä–µ–∂–∏–º–µ –æ–Ω–ª–∞–π–Ω, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –º–µ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. PRIME —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–ª–æ—Ç–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —É—è–∑–≤–∏–º–æ—Å—Ç—å –∫ –≤–∑–ª–æ–º—É –Ω–∞–≥—Ä–∞–¥ –∏ –≤—ã—Å–æ–∫–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å —Å–±–æ—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏ —ç—Ç–æ–º –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Unlocking LLM Potential with PRIME: Efficient Training through Implicit Rewards', 'desc': 'This paper introduces PRIME, a method that enhances the training of large language models (LLMs) using dense process rewards instead of traditional sparse outcome rewards. Dense rewards help improve training efficiency and address credit assignment issues, but collecting high-quality process labels has been a challenge. PRIME allows for online updates of process reward models using only policy rollouts and outcome labels, which reduces the need for extensive reward model training. The results show that PRIME significantly improves reasoning performance in tasks like math and coding, achieving better results with less training data compared to existing models.'}, 'zh': {'title': 'PRIMEÔºöÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïPRIMEÔºàÈÄöËøáÈöêÂºèÂ•ñÂä±ËøõË°åËøáÁ®ãÂº∫ÂåñÂ≠¶‰π†ÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÂ§öÊ≠•È™§Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑËÆ≠ÁªÉÊïàÁéáÈóÆÈ¢ò„ÄÇ‰º†ÁªüÁöÑÁ®ÄÁñèÁªìÊûúÂ•ñÂä±Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Â≠òÂú®ÊïàÁéá‰Ωé‰∏ãÂíå‰ø°Áî®ÂàÜÈÖçÁ≠âÈóÆÈ¢òÔºåËÄåPRIMEÈÄöËøá‰ªÖ‰ΩøÁî®Á≠ñÁï•ÂõûÊªöÂíåÁªìÊûúÊ†áÁ≠æÊù•ÂÆûÁé∞Âú®Á∫øËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâÁöÑÊõ¥Êñ∞„ÄÇËØ•ÊñπÊ≥ïÈÅøÂÖç‰∫ÜÁé∞ÊúâÊñπÊ≥ï‰∏≠ÈúÄË¶ÅÁöÑ‰∏ìÈó®Â•ñÂä±Ê®°ÂûãËÆ≠ÁªÉÈò∂ÊÆµÔºå‰ªéËÄåÊòæËëóÈôç‰Ωé‰∫ÜÂºÄÂèëÊàêÊú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPRIMEÂú®Êï∞Â≠¶ÂíåÁºñÁ†ÅÁ´ûËµõ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÁõ∏ËæÉ‰∫é‰º†ÁªüÊ®°ÂûãÊúâÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18636', 'title': 'SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model', 'url': 'https://huggingface.co/papers/2501.18636', 'abstract': 'The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.', 'score': 24, 'issue_id': 2023, 'pub_date': '2025-01-28', 'pub_date_card': {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'}, 'hash': '3a201d426049658a', 'authors': ['Xun Liang', 'Simin Niu', 'Zhiyu Li', 'Sensen Zhang', 'Hanyu Wang', 'Feiyu Xiong', 'Jason Zhaoxin Fan', 'Bo Tang', 'Shichao Song', 'Mengwei Wang', 'Jiawei Yang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China', 'Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.18636.jpg', 'data': {'categories': ['#rag', '#security', '#dataset', '#benchmark'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': 'SafeRAG: –æ—Ü–µ–Ω–∫–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ SafeRAG –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (RAG). –ê–≤—Ç–æ—Ä—ã –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç –∞—Ç–∞–∫–∏ –Ω–∞ RAG –∏ —Å–æ–∑–¥–∞—é—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏—Ö –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RAG —É—è–∑–≤–∏–º –∫–æ –≤—Å–µ–º —Ç–∏–ø–∞–º –∞—Ç–∞–∫, –¥–∞–∂–µ —Å–∞–º—ã–º –æ—á–µ–≤–∏–¥–Ω—ã–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã RAG –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–æ—Ç–∏–≤–æ—Å—Ç–æ—è—Ç—å –∞—Ç–∞–∫–∞–º, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–Ω–∏–∂–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': 'Strengthening RAG: Evaluating Vulnerabilities in Knowledge Integration', 'desc': 'This paper addresses the security vulnerabilities of retrieval-augmented generation (RAG) systems, which combine external knowledge with large language models (LLMs) for knowledge-intensive tasks. The authors introduce a benchmark called SafeRAG to evaluate the security of RAG by classifying various attack types that can manipulate knowledge. They create a dataset specifically for testing these vulnerabilities and simulate different attack scenarios to assess the impact on RAG performance. The results show that RAG systems are significantly susceptible to these attacks, leading to a decline in service quality, highlighting the need for improved security measures.'}, 'zh': {'title': 'ÊèêÂçáRAGÂÆâÂÖ®ÊÄßÔºåÊäµÂæ°Áü•ËØÜÊîªÂáªÔºÅ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SafeRAGÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇÊàë‰ª¨Â∞ÜÊîªÂáª‰ªªÂä°ÂàÜ‰∏∫Èì∂Âô™Â£∞„ÄÅ‰∏ä‰∏ãÊñáÂÜ≤Á™Å„ÄÅËΩØÂπøÂëäÂíåÊãíÁªùÊúçÂä°Á≠âÁ±ªÂûãÔºåÂπ∂‰∏∫ÊØèÁßç‰ªªÂä°ÊâãÂä®ÊûÑÂª∫‰∫ÜÂÆâÂÖ®ËØÑ‰º∞Êï∞ÊçÆÈõÜ„ÄÇÈÄöËøá‰ΩøÁî®SafeRAGÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨Ê®°Êãü‰∫ÜRAGÂèØËÉΩÈÅáÂà∞ÁöÑÂêÑÁßçÊîªÂáªÂú∫ÊôØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRAGÂØπÊâÄÊúâÊîªÂáª‰ªªÂä°Ë°®Áé∞Âá∫ÊòæËëóÁöÑËÑÜÂº±ÊÄßÔºåÁîöËá≥ÊúÄÊòéÊòæÁöÑÊîªÂáª‰ªªÂä°‰πüËÉΩËΩªÊòìÁªïËøáÁé∞ÊúâÁöÑÊ£ÄÁ¥¢Âô®ÂíåËøáÊª§Âô®ÔºåÂØºËá¥RAGÊúçÂä°Ë¥®Èáè‰∏ãÈôç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01341', 'title': 'AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding', 'url': 'https://huggingface.co/papers/2502.01341', 'abstract': 'Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.', 'score': 23, 'issue_id': 2030, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '88ea73fcb0da69ba', 'authors': ['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-Andr√© No√´l', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], 'affiliations': ['CIFAR AI Chair', 'Ecole de Technologie Superieure', 'McGill University', 'Mila', 'Polytechnique Montreal', 'ServiceNow', 'Universite de Montreal', 'University of British Columbia', 'University of Waterloo', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01341.jpg', 'data': {'categories': ['#alignment', '#cv', '#multimodal'], 'emoji': 'üîÄ', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': 'AlignVLM - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –≤–∏–¥–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–æ—Ä—ã, –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. AlignVLM –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –∑–∞–¥–∞—á –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É.'}, 'en': {'title': 'Aligning Vision and Language for Better Understanding', 'desc': 'This paper addresses the challenge of aligning visual features with language embeddings in vision-language models (VLMs). The authors introduce AlignVLM, a new method that connects visual features to a weighted average of language model (LLM) text embeddings, enhancing semantic similarity. By utilizing the linguistic knowledge embedded in the LLM, AlignVLM ensures that visual features are accurately represented in a way that the LLM can understand. The results show that AlignVLM outperforms existing methods, particularly in tasks involving document understanding, while also demonstrating improved robustness to noisy inputs.'}, 'zh': {'title': 'ËßÜËßâ‰∏éËØ≠Ë®ÄÁöÑÂÆåÁæéÂØπÈΩê', 'desc': 'Âú®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâ‰∏≠ÔºåÂ∞ÜËßÜËßâÁâπÂæÅ‰∏éËØ≠Ë®ÄÂµåÂÖ•ÂØπÈΩêÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàò„ÄÇÁé∞ÊúâÁöÑËøûÊé•Âô®ÔºåÂ¶ÇÂ§öÂ±ÇÊÑüÁü•Âô®ÔºàMLPÔºâÔºåÂ∏∏Â∏∏‰ºö‰∫ßÁîüÂàÜÂ∏ÉÂ§ñÊàñÂô™Â£∞ËæìÂÖ•ÔºåÂØºËá¥Ê®°ÊÄÅ‰πãÈó¥ÁöÑ‰∏çÂØπÈΩê„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜËßâÊñáÊú¨ÂØπÈΩêÊñπÊ≥ïAlignVLMÔºåÂÆÉÂ∞ÜËßÜËßâÁâπÂæÅÊò†Â∞ÑÂà∞LLMÊñáÊú¨ÂµåÂÖ•ÁöÑÂä†ÊùÉÂπ≥ÂùáÂÄº„ÄÇAlignVLMÂú®ÊñáÊ°£ÁêÜËß£‰ªªÂä°‰∏≠Ë°®Áé∞Â∞§‰∏∫Âá∫Ëâ≤ÔºåËÉΩÂ§üÊúâÊïàÊèêÈ´òËßÜËßâÁâπÂæÅ‰∏éÊñáÊú¨ÂÜÖÂÆπÁöÑÂØπÈΩêÂíåÊäóÂô™Â£∞ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01534', 'title': 'Preference Leakage: A Contamination Problem in LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2502.01534', 'abstract': 'Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.', 'score': 22, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'd2508b2b8b82b41b', 'authors': ['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], 'affiliations': ['Arizona State University', 'University of California, Los Angeles', 'University of Illinois Urbana Champaign', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2502.01534.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#training', '#dataset', '#leakage'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–û—Å—Ç–æ—Ä–æ–∂–Ω–æ: LLM-—Å—É–¥—å–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–µ–¥–≤–∑—è—Ç—ã!', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É '—É—Ç–µ—á–∫–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π' –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–∞ –ø—Ä–æ–±–ª–µ–º–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞–º–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ LLM-–æ—Ü–µ–Ω—â–∏–∫–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å —Å—É–¥–µ–π –∫ —Å–≤—è–∑–∞–Ω–Ω—ã–º —Å –Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º-—É—á–µ–Ω–∏–∫–∞–º –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ —É—Ç–µ—á–∫–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —è–≤–ª—è–µ—Ç—Å—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω–æ–π –∏ —Ç—Ä—É–¥–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥–µ–π."}, 'en': {'title': 'Uncovering Preference Leakage: A Hidden Bias in LLM Evaluation', 'desc': 'This paper discusses a problem called preference leakage in the context of using Large Language Models (LLMs) as judges for data annotation. Preference leakage occurs when the relationship between the data generators and the evaluators leads to biased evaluations, particularly when they are similar or related models. The authors identify three types of relatedness that can cause this issue and demonstrate through experiments that judges show bias towards their related models. The findings highlight that preference leakage is a significant and often unnoticed challenge in LLM-based model development.'}, 'zh': {'title': 'ÂÅèÂ•ΩÊ≥ÑÊºèÔºöLLMËØÑÂà§‰∏≠ÁöÑÈöêÊÇ£', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰Ωú‰∏∫ËØÑÂà§ËÄÖÂíåÂü∫‰∫éLLMÁöÑÊï∞ÊçÆÂêàÊàêÂú®Ê®°ÂûãÂºÄÂèë‰∏≠ÁöÑÂ∫îÁî®„ÄÇÊàë‰ª¨Êè≠Á§∫‰∫ÜÂÅèÂ•ΩÊ≥ÑÊºèËøô‰∏ÄÈóÆÈ¢òÔºåÂÆÉÊòØÁî±ÂêàÊàêÊï∞ÊçÆÁîüÊàêÂô®‰∏éLLMËØÑ‰º∞ËÄÖ‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÂºïËµ∑ÁöÑ„ÄÇÈÄöËøáÂÆö‰πâ‰∏âÁßçÂ∏∏ËßÅÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÊàë‰ª¨ËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåËØÅÂÆû‰∫ÜËØÑÂà§ËÄÖÂØπÂÖ∂Áõ∏ÂÖ≥Â≠¶ÁîüÊ®°ÂûãÁöÑÂÅèËßÅ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂÅèÂ•ΩÊ≥ÑÊºèÊòØ‰∏Ä‰∏™ÊôÆÈÅçÂ≠òÂú®‰∏îÈöæ‰ª•Ê£ÄÊµãÁöÑÈóÆÈ¢òÔºåÂΩ±Âìç‰∫ÜLLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01639', 'title': 'SliderSpace: Decomposing the Visual Capabilities of Diffusion Models', 'url': 'https://huggingface.co/papers/2502.01639', 'abstract': "We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. Our code, data and trained weights are available at https://sliderspace.baulab.info", 'score': 15, 'issue_id': 2027, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '559003a020b42709', 'authors': ['Rohit Gandikota', 'Zongze Wu', 'Richard Zhang', 'David Bau', 'Eli Shechtman', 'Nick Kolkin'], 'affiliations': ['Adobe Research', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01639.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#dataset', '#open_source', '#multimodal', '#interpretability', '#cv'], 'emoji': 'üéöÔ∏è', 'ru': {'title': 'SliderSpace: –†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'SliderSpace - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ –∏ –ø–æ–Ω—è—Ç–Ω—ã–µ —á–µ–ª–æ–≤–µ–∫—É –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –û–Ω –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ö–∞–∂–¥–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—É—á–∞–µ—Ç—Å—è –∫–∞–∫ –∞–¥–∞–ø—Ç–µ—Ä –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–Ω–≥–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å SliderSpace –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, –≤–∫–ª—é—á–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç–∏–ª–µ–π.'}, 'en': {'title': 'Unlocking Creativity in Diffusion Models with SliderSpace', 'desc': "SliderSpace is a novel framework designed to break down the visual capabilities of diffusion models into controllable and understandable components. It automatically identifies multiple interpretable directions from a single text prompt, eliminating the need for users to define each edit direction manually. By training each direction as a low-rank adaptor, SliderSpace allows for compositional control and reveals unexpected possibilities within the model's latent space. Extensive experiments show that it effectively enhances concept decomposition, artistic style exploration, and diversity, outperforming existing methods in generating diverse and useful variations."}, 'zh': {'title': 'SliderSpaceÔºöÂèØÊéßÁöÑËßÜËßâËÉΩÂäõÂàÜËß£', 'desc': 'SliderSpaceÊòØ‰∏Ä‰∏™Ê°ÜÊû∂ÔºåÁî®‰∫éËá™Âä®ÂàÜËß£Êâ©Êï£Ê®°ÂûãÁöÑËßÜËßâËÉΩÂäõÔºå‰ΩøÂÖ∂ÂèØÊéß‰∏îÊòì‰∫éÁêÜËß£„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåSliderSpaceÂèØ‰ª•‰ªéÂçï‰∏™ÊñáÊú¨ÊèêÁ§∫‰∏≠ÂêåÊó∂ÂèëÁé∞Â§ö‰∏™ÂèØËß£ÈáäÂíåÂ§öÊ†∑ÂåñÁöÑÊñπÂêëÔºåËÄåÊó†ÈúÄÁî®Êà∑ÈÄê‰∏™ÊåáÂÆöÂ±ûÊÄß„ÄÇÊØè‰∏™ÊñπÂêëË¢´ËÆ≠ÁªÉ‰∏∫‰ΩéÁß©ÈÄÇÈÖçÂô®Ôºå‰ªéËÄåÂÆûÁé∞ÁªÑÂêàÊéßÂà∂ÔºåÂπ∂Âú®Ê®°ÂûãÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ÂèëÁé∞ÊÑèÊÉ≥‰∏çÂà∞ÁöÑÂèØËÉΩÊÄß„ÄÇÈÄöËøáÂØπÊúÄÂÖàËøõÁöÑÊâ©Êï£Ê®°ÂûãËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜSliderSpaceÂú®Ê¶ÇÂøµÂàÜËß£„ÄÅËâ∫ÊúØÈ£éÊ†ºÊé¢Á¥¢ÂíåÂ§öÊ†∑ÊÄßÂ¢ûÂº∫Á≠â‰∏â‰∏™Â∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00698', 'title': 'MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models', 'url': 'https://huggingface.co/papers/2502.00698', 'abstract': 'IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.   Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.', 'score': 13, 'issue_id': 2027, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '960e3460f0ab8e56', 'authors': ['Huanqia Cai', 'Yijun Yang', 'Winston Hu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00698.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': '–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò –ø—Ä–æ–≤–∞–ª–∏–≤–∞–µ—Ç —Ç–µ—Å—Ç –Ω–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MM-IQ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 2,710 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞–Ω–∏–π –ø–æ 8 —Ç–∏–ø–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç–µ—Å—Ç–∞–º IQ –¥–ª—è –ª—é–¥–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ª–∏—à—å –Ω–µ–º–Ω–æ–≥–æ –≤—ã—à–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–≥–∞–¥—ã–≤–∞–Ω–∏—è (27.49% –ø—Ä–æ—Ç–∏–≤ 25%). –≠—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ç–µ–∫—É—â–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –ò–ò –∏ –±–∞–∑–æ–≤—ã–º–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞.'}, 'en': {'title': 'Bridging the Cognitive Divide in AI with MM-IQ', 'desc': 'This paper introduces MM-IQ, a new evaluation framework designed to measure cognitive abilities in artificial intelligence systems, similar to how IQ tests assess human intelligence. It includes 2,710 test items across 8 reasoning paradigms, providing a comprehensive benchmark for multimodal models. The evaluation of various state-of-the-art models shows that their performance is only slightly better than random guessing, indicating significant limitations in their reasoning capabilities. This finding emphasizes the urgent need for advancements in AI to better replicate human-like reasoning skills.'}, 'zh': {'title': 'MM-IQÔºöËØÑ‰º∞Â§öÊ®°ÊÄÅÁ≥ªÁªüÁöÑËÆ§Áü•ËÉΩÂäõÊñ∞Ê†áÂáÜ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫MM-IQÁöÑÁªºÂêàËØÑ‰º∞Ê°ÜÊû∂ÔºåÁî®‰∫éÈáèÂåñÂ§öÊ®°ÊÄÅÁ≥ªÁªü‰∏≠ÁöÑËÆ§Áü•ËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´2710‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑÊµãËØïÈ°πÁõÆÔºåÊ∂µÁõñ8Áßç‰∏çÂêåÁöÑÊé®ÁêÜËåÉÂºè„ÄÇÈÄöËøáÂØπÈ¢ÜÂÖàÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãËøõË°åÁ≥ªÁªüËØÑ‰º∞ÔºåÁªìÊûúÊòæÁ§∫Âç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊû∂ÊûÑÔºåÂÖ∂Ë°®Áé∞‰πü‰ªÖÁï•È´ò‰∫éÈöèÊú∫ÁåúÊµã„ÄÇËøô‰∏™ÊòæËëóÁöÑÊÄßËÉΩÂ∑ÆË∑ùË°®ÊòéÂΩìÂâçÂ§öÊ®°ÊÄÅÁ≥ªÁªüÂú®Êé•Ëøë‰∫∫Á±ªÂü∫Êú¨Êé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÂº∫Ë∞É‰∫ÜÈúÄË¶ÅËøõË°åËåÉÂºèËΩ¨ÂèòÁöÑËøõÊ≠•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01068', 'title': 'FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation', 'url': 'https://huggingface.co/papers/2502.01068', 'abstract': 'While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00times and 1.40times improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.', 'score': 12, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '58ab72f123d7a4b6', 'authors': ['Dongwon Jo', 'Jiwon Song', 'Yulhwa Kim', 'Jae-Joon Kim'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea', 'Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.01068.jpg', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': 'FastKV: –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FastKV - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –∫—ç—à–∞ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ (KV) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. FastKV –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥ Token-Selective Propagation (TSP), –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Å–ª–æ—è—Ö LLM –∏ –≤—ã–±–æ—Ä–æ—á–Ω–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏—Ö —Å–ª–æ—è—Ö. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç —Å–∂–∞—Ç–∏–µ –∫—ç—à–∞ KV —Å —É—á–µ—Ç–æ–º grouped-query attention (GQA) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FastKV –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∏ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–∑–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.'}, 'en': {'title': 'FastKV: Speeding Up Long-Context Processing in LLMs', 'desc': 'This paper presents FastKV, a new method for compressing key-value (KV) caches in large language models (LLMs) to improve computational efficiency and reduce latency. FastKV uses a Token-Selective Propagation (TSP) strategy that keeps full context information in the early layers of the model while selectively passing on only part of this information in the deeper layers. Additionally, it employs grouped-query attention (GQA) to enhance both memory usage and processing speed. Experimental results demonstrate that FastKV significantly improves time-to-first-token and throughput while maintaining accuracy on long-context tasks.'}, 'zh': {'title': 'FastKVÔºöÊèêÂçáÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜÈÄüÂ∫¶ÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫FastKVÁöÑKVÁºìÂ≠òÂéãÁº©ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÂ∫èÂàóÁöÑÂ§ÑÁêÜÈÄüÂ∫¶„ÄÇFastKVÈááÁî®‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÈÄâÊã©ÊÄß‰º†Êí≠ÊñπÊ≥ïÔºàTSPÔºâÔºåÂú®LLMÁöÑÂàùÂßãÂ±Ç‰øùÁïôÂÆåÊï¥ÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåËÄåÂú®Êõ¥Ê∑±Â±ÇÊ¨°‰∏≠‰ªÖÈÄâÊã©ÊÄß‰º†Êí≠ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇËØ•ÊñπÊ≥ïËøòÁªìÂêà‰∫ÜÂàÜÁªÑÊü•ËØ¢Ê≥®ÊÑèÂäõÔºàGQAÔºâÊù•‰ºòÂåñÂÜÖÂ≠òÂíåËÆ°ÁÆóÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFastKVÂú®È¶ñÊ¨°‰ª§ÁâåÊó∂Èó¥ÂíåÂêûÂêêÈáèÊñπÈù¢ÂàÜÂà´ÊØîÁé∞ÊúâÁöÑHeadKVÊñπÊ≥ïÊèêÈ´ò‰∫Ü2.00ÂÄçÂíå1.40ÂÄçÔºåÂêåÊó∂Âú®Èïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠‰øùÊåÅ‰∫Ü‰∏éÂü∫Á∫øÁõ∏ÂΩìÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00094', 'title': 'AIN: The Arabic INclusive Large Multimodal Model', 'url': 'https://huggingface.co/papers/2502.00094', 'abstract': "Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.", 'score': 12, 'issue_id': 2018, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': 'e2d63540ee133732', 'authors': ['Ahmed Heakl', 'Sara Ghaboura', 'Omkar Thawkar', 'Fahad Shahbaz Khan', 'Hisham Cholakkal', 'Rao Muhammad Anwer', 'Salman Khan'], 'affiliations': ['Aalto University', 'Australian National University', 'Link√∂ping University', 'Mohamed bin Zayed University of AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.00094.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#low_resource', '#multimodal', '#multilingual'], 'emoji': 'üåç', 'ru': {'title': 'AIN: –ü—Ä–æ—Ä—ã–≤ –≤ –∞—Ä–∞–±–æ—è–∑—ã—á–Ω–æ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å AIN - –¥–≤—É—è–∑—ã—á–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞—Ä–∞–±—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ 3,6 –º–∏–ª–ª–∏–æ–Ω–∞—Ö –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞—Ä–∞–±—Å–∫–æ-–∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–∞–Ω–Ω—ã—Ö. AIN –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∞—Ä–∞–±—Å–∫–æ–º —è–∑—ã–∫–µ, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–∏–ª—å–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ CAMEL-Bench, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–º 38 –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤, 7B-–≤–µ—Ä—Å–∏—è AIN –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç GPT-4o –Ω–∞ 3,4% –≤ —Å—Ä–µ–¥–Ω–µ–º –ø–æ –≤–æ—Å—å–º–∏ –¥–æ–º–µ–Ω–∞–º.'}, 'en': {'title': 'Empowering Arabic with Advanced Multimodal AI', 'desc': 'This paper presents AIN, the Arabic Inclusive Multimodal Model, which is designed to enhance the performance of large multimodal models (LMMs) specifically for Arabic and English. AIN utilizes a substantial dataset of 3.6 million high-quality Arabic-English multimodal samples to achieve state-of-the-art results in Arabic language tasks. The model excels across various domains, as evidenced by its performance on the CAMEL-Bench benchmark, where it surpasses GPT-4o in multiple sub-domains. AIN aims to provide advanced generative AI tools for Arabic speakers, addressing the current limitations in Arabic multimodal understanding.'}, 'zh': {'title': 'Êé®Âä®ÈòøÊãâ‰ºØËØ≠Â§öÊ®°ÊÄÅAIÁöÑËøõÊ≠•', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÈòøÊãâ‰ºØËØ≠ÁöÑÁ†îÁ©∂‰ªçÁÑ∂Áõ∏ÂØπÊªûÂêé„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜAINÊ®°ÂûãÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®ÊèêÂçáÈòøÊãâ‰ºØËØ≠ÂíåËã±ËØ≠ÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÂà©Áî®‰∫Ü360‰∏áÈ´òË¥®ÈáèÁöÑÈòøÊãâ‰ºØËØ≠-Ëã±ËØ≠Â§öÊ®°ÊÄÅÊï∞ÊçÆÊ†∑Êú¨„ÄÇAINÂú®Â§ö‰∏™È¢ÜÂüüË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇÁöÑËßÜËßâÁêÜËß£ÂíåÂ§öÂõæÂÉèÁêÜËß£ÊñπÈù¢ÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑGPT-4oÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÁöÑ‰ºòË∂äÊÄßËÉΩ‰∏∫ÈòøÊãâ‰ºØËØ≠‰ΩøÁî®ËÄÖÊèê‰æõ‰∫ÜÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅÁîüÊàêAIÂ∑•ÂÖ∑ÔºåÊé®Âä®‰∫ÜÁõ∏ÂÖ≥Â∫îÁî®ÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01142', 'title': 'DeepRAG: Thinking to Retrieval Step by Step for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01142', 'abstract': 'Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.', 'score': 9, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'e26994166d750227', 'authors': ['Xinyan Guan', 'Jiali Zeng', 'Fandong Meng', 'Chunlei Xin', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun', 'Jie Zhou'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.01142.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#reasoning', '#rag', '#rl'], 'emoji': 'üß†', 'ru': {'title': 'DeepRAG: —É–º–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –ò–ò', 'desc': 'DeepRAG - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –º–æ–¥–µ–ª–∏—Ä—É—é—â–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–∏—Å–∫–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫–∞–∫ –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –û–Ω –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤–Ω–µ—à–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DeepRAG –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ 21.99%. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π –∑–∞–¥–∞—á –ø—Ä–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –ø–æ–∏—Å–∫–æ–º.'}, 'en': {'title': 'Enhancing Reasoning with Smart Retrieval', 'desc': 'This paper introduces DeepRAG, a new framework that enhances retrieval-augmented reasoning in large language models. It treats the retrieval process as a Markov Decision Process (MDP), allowing for more strategic and adaptive retrieval of information. By breaking down queries into smaller parts, DeepRAG can decide whether to fetch external knowledge or use its built-in reasoning capabilities at each step. The results show that DeepRAG not only makes retrieval more efficient but also increases the accuracy of answers by nearly 22%.'}, 'zh': {'title': 'DeepRAGÔºö‰ºòÂåñÊ£ÄÁ¥¢Â¢ûÂº∫Êé®ÁêÜÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜÊñπÈù¢Â±ïÁé∞‰∫ÜÊòæËëóÁöÑÊΩúÂäõÔºå‰ΩÜ‰ªçÁÑ∂Èù¢‰∏¥‰∏•ÈáçÁöÑ‰∫ãÂÆûÂπªËßâÈóÆÈ¢òÔºå‰∏ªË¶ÅÁî±‰∫éÂèÇÊï∞Áü•ËØÜÁöÑÊó∂ÊïàÊÄß„ÄÅÂáÜÁ°ÆÊÄßÂíåË¶ÜÁõñÁéá‰∏çË∂≥„ÄÇÂ∞ÜÊé®ÁêÜ‰∏éÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁªìÂêàËµ∑Êù•‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄßÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫‰ªªÂä°ÂàÜËß£‰∏çÊúâÊïàÂíåÂÜó‰ΩôÊ£ÄÁ¥¢ÂèØËÉΩÂºïÂÖ•Âô™Â£∞ÔºåÈôç‰ΩéÂìçÂ∫îË¥®Èáè„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜDeepRAGÊ°ÜÊû∂ÔºåÂ∞ÜÊ£ÄÁ¥¢Â¢ûÂº∫Êé®ÁêÜÂª∫Ê®°‰∏∫È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºàMDPÔºâÔºåÂÆûÁé∞‰∫ÜÊàòÁï•ÊÄßÂíåËá™ÈÄÇÂ∫îÁöÑÊ£ÄÁ¥¢„ÄÇÂÆûÈ™åË°®ÊòéÔºåDeepRAGÂú®ÊèêÈ´òÊ£ÄÁ¥¢ÊïàÁéáÁöÑÂêåÊó∂ÔºåÁ≠îÊ°àÂáÜÁ°ÆÊÄßÊèêÈ´ò‰∫Ü21.99%ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®‰ºòÂåñÊ£ÄÁ¥¢Â¢ûÂº∫Êé®ÁêÜÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01637', 'title': 'Scaling Embedding Layers in Language Models', 'url': 'https://huggingface.co/papers/2502.01637', 'abstract': 'We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent n-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached n-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.', 'score': 9, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '478a2a0ee08530a8', 'authors': ['Da Yu', 'Edith Cohen', 'Badih Ghazi', 'Yangsibo Huang', 'Pritish Kamath', 'Ravi Kumar', 'Daogao Liu', 'Chiyuan Zhang'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.01637.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#training', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç', 'desc': 'SCONE - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–ª–æ—ë–≤ –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ—è. –û–Ω –≤–≤–æ–¥–∏—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞—Å—Ç—ã—Ö n-–≥—Ä–∞–º–º, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –≠—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ–±—É—á–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. SCONE –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö n-–≥—Ä–∞–º–º–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –º–æ–¥–µ–ª—å –¥–ª—è –∏—Ö –æ–±—É—á–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ FLOPS –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ.'}, 'en': {'title': 'Enhancing Language Models with SCONE: Scalable N-gram Embeddings for Better Performance', 'desc': 'SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding) is a novel approach designed to improve the performance of language models as they grow in size. It introduces embeddings for common n-grams while keeping the original vocabulary intact, which helps in providing better contextual representations for input tokens. These n-gram embeddings are learned through a separate model during training and stored in off-accelerator memory to ensure fast inference. By scaling both the number of cached n-gram embeddings and the model that learns them, SCONE achieves superior performance compared to a large baseline model while maintaining efficient inference-time computations.'}, 'zh': {'title': 'SCONEÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñπÊ≥ïSCONEÔºàÂèØÊâ©Â±ïÁöÑ‰∏ä‰∏ãÊñáÂåñÁöÑÁ¶ªÁ∫øN-gramÂµåÂÖ•ÔºâÔºåÊó®Âú®ÈÄöËøáÊâ©Â±ïËæìÂÖ•ÂµåÂÖ•Â±ÇÊù•ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇSCONEÂú®‰øùÊåÅÂéüÊúâËØçÊ±áÁöÑÂêåÊó∂ÔºåÂºïÂÖ•‰∫Ü‰∏ÄÁªÑÂ∏∏ËßÅn-gramÁöÑÂµåÂÖ•Ôºå‰ª•Êèê‰æõÊØè‰∏™ËæìÂÖ•Ê†áËÆ∞ÁöÑ‰∏ä‰∏ãÊñáÂåñË°®Á§∫„ÄÇËøô‰∫õÂµåÂÖ•Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Áî±‰∏Ä‰∏™ÂçïÁã¨ÁöÑÊ®°ÂûãÂ≠¶‰π†ÔºåÂπ∂Âú®Êé®ÁêÜÊó∂È¢ÑÂÖàËÆ°ÁÆóÂπ∂Â≠òÂÇ®Âú®Á¶ªÁ∫øÂä†ÈÄüÂô®ÂÜÖÂ≠ò‰∏≠ÔºåÂá†‰πé‰∏çÂΩ±ÂìçÊé®ÁêÜÈÄüÂ∫¶„ÄÇÈÄöËøáÂ¢ûÂä†ÁºìÂ≠òÁöÑn-gramÂµåÂÖ•Êï∞ÈáèÂíåÊâ©Â±ïÂ≠¶‰π†ÂÆÉ‰ª¨ÁöÑÊ®°ÂûãÔºåSCONEÂú®Â§öÁßçËØ≠ÊñôÂ∫ì‰∏äË∂ÖË∂ä‰∫Ü1.9BÂèÇÊï∞ÁöÑÂü∫Á∫øÔºåÂêåÊó∂‰ªÖ‰ΩøÁî®‰∏ÄÂçäÁöÑÊé®ÁêÜÊó∂Èó¥FLOPS„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01100', 'title': 'ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning', 'url': 'https://huggingface.co/papers/2502.01100', 'abstract': 'We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.   Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.', 'score': 8, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '901fd196d7bfe394', 'authors': ['Bill Yuchen Lin', 'Ronan Le Bras', 'Kyle Richardson', 'Ashish Sabharwal', 'Radha Poovendran', 'Peter Clark', 'Yejin Choi'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.01100.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#benchmark'], 'emoji': 'üß©', 'ru': {'title': '–ü—Ä–æ–∫–ª—è—Ç–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ –ª–æ–≥–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ LLM', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –∏—Ö –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–µ–º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞. –î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ ZebraLogic, –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π (CSP). ZebraLogic –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑—É—á–∞—Ç—å –ø—Ä–µ–¥–µ–ª—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Llama –∏ DeepSeek-R1. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –º–µ—Ä–µ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á, —á—Ç–æ –∞–≤—Ç–æ—Ä—ã –Ω–∞–∑—ã–≤–∞—é—Ç "–ø—Ä–æ–∫–ª—è—Ç–∏–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏".'}, 'en': {'title': 'Unraveling the Limits of Logical Reasoning in Large Language Models', 'desc': "This paper examines how well large language models (LLMs) can perform logical reasoning, especially in complex scenarios where reasoning does not follow a straightforward path. The authors introduce ZebraLogic, a new framework designed to evaluate LLMs on logic grid puzzles that are based on constraint satisfaction problems (CSPs). Through this framework, they discover that as the complexity of the puzzles increases, the accuracy of the models significantly decreases, a challenge they refer to as the 'curse of complexity.' The study also suggests methods to improve reasoning capabilities, such as using advanced sampling techniques and self-verification prompts, while highlighting the limitations of current LLMs in handling complex reasoning tasks."}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂ§çÊùÇÊÄßÊåëÊàò', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõÂèäÂÖ∂Âú®Â§çÊùÇÈùûÂçïË∞ÉÊé®ÁêÜ‰∏≠ÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜZebraLogicÔºå‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÁî®‰∫éËØÑ‰º∞LLMÂú®Âü∫‰∫éÁ∫¶ÊùüÊª°Ë∂≥ÈóÆÈ¢òÔºàCSPsÔºâÁöÑÈÄªËæëÁΩëÊ†ºË∞úÈ¢ò‰∏äÁöÑÊé®ÁêÜË°®Áé∞„ÄÇÁ†îÁ©∂ÁªìÊûúÊòæÁ§∫ÔºåÈöèÁùÄÈóÆÈ¢òÂ§çÊùÇÊÄßÁöÑÂ¢ûÂä†ÔºåÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÊòæËëó‰∏ãÈôçÔºåËøô‰∏ÄÁé∞Ë±°Ë¢´Áß∞‰∏∫Â§çÊùÇÊÄßËØÖÂíí„ÄÇÊàë‰ª¨ËøòÊé¢ËÆ®‰∫ÜÂ¢ûÂº∫ÈÄªËæëÊé®ÁêÜÁöÑÁ≠ñÁï•ÔºåÂåÖÊã¨ÊúÄ‰Ω≥ÈááÊ†∑„ÄÅÂõûÊ∫ØÊú∫Âà∂ÂíåËá™ÊàëÈ™åËØÅÊèêÁ§∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01081', 'title': 'The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles', 'url': 'https://huggingface.co/papers/2502.01081', 'abstract': "The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.", 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '7585b424ff041825', 'authors': ['Vernon Y. H. Toh', 'Yew Ken Chia', 'Deepanway Ghosal', 'Soujanya Poria'], 'affiliations': ['Singapore University of Technology and Design (SUTD)'], 'pdf_title_img': 'assets/pdf/title_img/2502.01081.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agi', '#open_source', '#training', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–≠–≤–æ–ª—é—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —ç–≤–æ–ª—é—Ü–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–µ—Ä–∏–π GPT –∏ OpenAI. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –º–æ–¥–µ–ª–∏ o3 –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –Ω–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –ü—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –æ–±—â—É—é —Ç–µ–Ω–¥–µ–Ω—Ü–∏—é —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –Ω–æ –≤—ã—è–≤–ª—è—é—Ç —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–µ—Å—è —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –¥–∞–∂–µ —É –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ç–∏–ø–∞—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'Advancing Reasoning in Multimodal AI: A New Era for LLMs', 'desc': "This paper discusses the advancements in Large Language Models (LLMs) with the release of OpenAI's o1 and o3, which show improved reasoning abilities. The o3 model has demonstrated superior problem-solving skills compared to humans on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, the study highlights that these models primarily focus on symbolic reasoning, while human reasoning often involves multimodal inputs like vision and language. The authors emphasize the need for further research into multimodal reasoning capabilities, as the o1 model, despite its high performance, still faces challenges with simple multimodal and algorithmic puzzles."}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÁöÑÊé¢Á¥¢‰∏éÊåëÊàò', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜOpenAIÁöÑo1Âíåo3Ê®°ÂûãÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÖàËøõÊé®ÁêÜËÉΩÂäõ„ÄÇo3Âú®ÊäΩË±°ÂíåÊé®ÁêÜËØ≠ÊñôÂ∫ìÔºàARC-AGIÔºâ‰∏≠Ë∂ÖË∂ä‰∫Ü‰∫∫Á±ªÔºåË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜËØ•Âü∫ÂáÜ‰ªÖÈôê‰∫éÁ¨¶Âè∑Ê®°Âºè„ÄÇ‰∫∫Á±ªÈÄöÂ∏∏Âú®Â§öÊ®°ÊÄÅÂú∫ÊôØ‰∏≠ËøõË°åÊé®ÁêÜÔºåÂõ†Ê≠§ÈúÄË¶ÅÁ†îÁ©∂Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÁöÑÈ´òÁ∫ßÊé®ÁêÜËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°o1Âú®Êé®ÁêÜËÉΩÂäõ‰∏äÊúâÊâÄÊèêÂçáÔºå‰ΩÜÂú®ÁÆÄÂçïÁöÑÂ§öÊ®°ÊÄÅÈöæÈ¢òÂíåÁÆóÊ≥ïÈöæÈ¢ò‰∏ä‰ªçÁÑ∂Â≠òÂú®‰∏çË∂≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01591', 'title': 'Improving Transformer World Models for Data-Efficient RL', 'url': 'https://huggingface.co/papers/2502.01591', 'abstract': 'We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.', 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'd9195e9417fce419', 'authors': ['Antoine Dedieu', 'Joseph Ortiz', 'Xinghua Lou', 'Carter Wendelken', 'Wolfgang Lehrach', 'J Swaroop Guntupalli', 'Miguel Lazaro-Gredilla', 'Kevin Patrick Murphy'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.01591.jpg', 'data': {'categories': ['#rl', '#architecture', '#benchmark', '#games', '#training', '#reasoning', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ model-based RL: –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —á–µ–ª–æ–≤–µ–∫–∞ –≤ Craftax-classic', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏, –¥–æ—Å—Ç–∏–≥–∞—é—â–∏–π –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ Craftax-classic. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–ª–≥–æ—Ä–∏—Ç–º, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π –∫–∞–∫ –ø—Ä–µ–¥—ã–¥—É—â–∏–π SOTA-–º–µ—Ç–æ–¥ DreamerV3, —Ç–∞–∫ –∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–ª–∏—Ç–∏–∫–∏ —Å CNN –∏ RNN, –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ –≤–æ–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–µ—Ç–æ–¥–æ–º –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –∏ –±–ª–æ—á–Ω–æ–µ teacher forcing –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞. –≠—Ç–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –ø–æ–∑–≤–æ–ª–∏–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Å–ª–æ–∂–Ω–æ–π —Å—Ä–µ–¥–µ, —Ç—Ä–µ–±—É—é—â–µ–π —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –Ω–∞–≤—ã–∫–æ–≤.'}, 'en': {'title': 'Revolutionizing Model-Based RL for Superior Game Performance', 'desc': "This paper introduces a new model-based reinforcement learning (MBRL) approach that excels in the Craftax-classic benchmark, a complex 2D survival game. The proposed algorithm achieves a remarkable reward of 67.4% after just 1 million environment steps, surpassing previous methods like DreamerV3 and even human performance. Key innovations include a novel policy architecture that integrates convolutional neural networks (CNNs) and recurrent neural networks (RNNs), along with enhancements like 'Dyna with warmup' for training on both real and simulated data. Additional techniques such as a nearest neighbor tokenizer for image patches and block teacher forcing for future reasoning further boost the model's efficiency and effectiveness."}, 'zh': {'title': 'Âü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†Êñ∞Á™ÅÁ†¥ÔºÅ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂú®Craftax-classicÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊñ∞ÁöÑÊúÄ‰Ω≥Ë°®Áé∞„ÄÇËøôÊòØ‰∏ÄÊ¨æÂºÄÊîæ‰∏ñÁïåÁöÑ2DÁîüÂ≠òÊ∏∏ÊàèÔºåË¶ÅÊ±ÇÊô∫ËÉΩ‰ΩìÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÅÊ∑±Â∫¶Êé¢Á¥¢ËÉΩÂäõÂíåÈïøÊúüÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑMBRLÁÆóÊ≥ïÂú®‰ªÖ1MÁéØÂ¢ÉÊ≠•È™§ÂêéËé∑Âæó‰∫Ü67.4%ÁöÑÂ•ñÂä±ÔºåÊòæËëóË∂ÖË∂ä‰∫ÜDreamerV3ÁöÑ53.2%ÔºåÂπ∂È¶ñÊ¨°Ë∂ÖËøá‰∫Ü‰∫∫Á±ªË°®Áé∞ÁöÑ65.0%„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™ÊúÄÂÖàËøõÁöÑÊó†Ê®°ÂûãÂü∫Á∫øÔºåÂπ∂ÁªìÂêàCNNÂíåRNNÁöÑÊñ∞ÂûãÁ≠ñÁï•Êû∂ÊûÑÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊ†∑Êú¨ÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01208', 'title': 'Almost Surely Safe Alignment of Large Language Models at Inference-Time', 'url': 'https://huggingface.co/papers/2502.01208', 'abstract': "Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.", 'score': 6, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'fdbe8816f1c6476a', 'authors': ['Xiaotong Ji', 'Shyam Sundhar Ramesh', 'Matthieu Zimmer', 'Ilija Bogunovic', 'Jun Wang', 'Haitham Bou Ammar'], 'affiliations': ['Huawei Noahs Ark Lab', 'Imperial College London', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2502.01208.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#inference', '#alignment'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ LLM –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é, –±–ª–∏–∑–∫–æ–π –∫ –µ–¥–∏–Ω–∏—Ü–µ. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –∑–∞–¥–∞—á—É –∫–∞–∫ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏. –û–Ω–∏ –≤–≤–æ–¥—è—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —ç–≤–æ–ª—é—Ü–∏—é –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–∫–∞–∑–∞—Ç—å —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –≥–∞—Ä–∞–Ω—Ç–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ InferenceGuard, –∫–æ—Ç–æ—Ä—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞.'}, 'en': {'title': 'InferenceGuard: Ensuring Safe Responses from LLMs at Inference Time', 'desc': "This paper addresses the issue of biased or unsafe responses from large language models (LLMs) by introducing a new method for aligning their outputs during inference. Instead of retraining the model, the authors propose a constrained Markov decision process that operates in the LLM's latent space to ensure safe response generation. They enhance this process with a safety state that monitors safety constraints, providing formal guarantees of safety. The proposed method, called InferenceGuard, shows improved performance in generating safe and aligned responses compared to existing techniques without altering the model's weights."}, 'zh': {'title': 'Êé®ÁêÜÊó∂ÂÆâÂÖ®ÂØπÈΩêÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜÊó∂ÂØπÈΩêÊñπÊ≥ïÔºåÊó®Âú®Á°Æ‰øùÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁîüÊàêÂÆâÂÖ®ÁöÑÂìçÂ∫î„ÄÇÊàë‰ª¨Â∞ÜÂÆâÂÖ®ÁîüÊàêÊé®ÁêÜÊó∂ÂìçÂ∫îÁöÑÈóÆÈ¢òÊ°ÜÊû∂Âåñ‰∏∫‰∏Ä‰∏™Á∫¶ÊùüÁöÑÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºàMDPÔºâÔºåÂπ∂Âú®LLMÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÂ§ÑÁêÜ„ÄÇÈÄöËøáÂºïÂÖ•‰∏Ä‰∏™ÂÆâÂÖ®Áä∂ÊÄÅÊù•Ë∑üË∏™ÂÆâÂÖ®Á∫¶ÊùüÁöÑÊºîÂèòÔºåÊàë‰ª¨ËÉΩÂ§üÂú®Ëß£ÂÜ≥MDPÊó∂Êèê‰æõÊ≠£ÂºèÁöÑÂÆâÂÖ®‰øùËØÅ„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑInferenceGuardÂÆûÁé∞‰∫ÜÂú®‰∏ç‰øÆÊîπÊ®°ÂûãÊùÉÈáçÁöÑÊÉÖÂÜµ‰∏ãÂÆâÂÖ®ÂØπÈΩêLLMÔºåÂπ∂Âú®ÁîüÊàêÂÆâÂÖ®ÂíåÂØπÈΩêÂìçÂ∫îÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01441', 'title': 'Improved Training Technique for Latent Consistency Models', 'url': 'https://huggingface.co/papers/2502.01441', 'abstract': 'Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-c scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/', 'score': 6, 'issue_id': 2018, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '2ea077ca6fd7397f', 'authors': ['Quan Dao', 'Khanh Doan', 'Di Liu', 'Trung Le', 'Dimitris Metaxas'], 'affiliations': ['Monash University', 'Rutgers University', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01441.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#architecture', '#open_source', '#diffusion', '#video'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–º–ø—É–ª—å—Å–∏–≤–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —É—Ö—É–¥—à–∞—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å iCT. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –æ–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –ö–æ—à–∏ –≤–º–µ—Å—Ç–æ –ü—Å–µ–≤–¥–æ-–•—É–±–µ—Ä–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–∞—Ö –∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç. –≠—Ç–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∑–≤–æ–ª–∏–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∏—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω—ã–µ –∫ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—é –∑–∞ –æ–¥–∏–Ω-–¥–≤–∞ —à–∞–≥–∞.'}, 'en': {'title': 'Enhancing Latent Consistency Models for High-Quality Generation', 'desc': "This paper introduces advancements in consistency models, a type of generative model that can create high-quality outputs efficiently. The authors focus on improving performance in latent spaces, where data often contains outliers that hinder model effectiveness. By replacing traditional loss functions with Cauchy losses and incorporating diffusion loss, they enhance the model's robustness against these outliers. Additionally, they propose an adaptive scaling-c scheduler and Non-scaling LayerNorm to optimize training, resulting in latent consistency models that perform comparably to diffusion models in generating images and videos."}, 'zh': {'title': 'ÊèêÂçá‰∏ÄËá¥ÊÄßÊ®°ÂûãÊÄßËÉΩÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': '‰∏ÄËá¥ÊÄßÊ®°ÂûãÊòØ‰∏ÄÁßçÊñ∞ÂûãÁîüÊàêÊ®°ÂûãÔºåËÉΩÂ§üÂú®ÂçïÊ≠•ÊàñÂ§öÊ≠•‰∏≠ÁîüÊàêÈ´òË¥®ÈáèÊ†∑Êú¨„ÄÇÊúÄËøëÔºåËøô‰∫õÊ®°ÂûãÂú®ÂÉèÁ¥†Á©∫Èó¥‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞‰∫Ü‰∏éÊâ©Êï£Ê®°ÂûãÁõ∏ÂΩìÁöÑÊïàÊûú„ÄÇÁÑ∂ËÄåÔºåÂú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∏ÄËá¥ÊÄßËÆ≠ÁªÉÁöÑÊàêÂäüÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàê‰ªªÂä°‰∏≠ÔºåÂèñÂÜ≥‰∫éÊΩúÂú®Á©∫Èó¥ÁöÑË°®Áé∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ÊΩúÂú®Êï∞ÊçÆ‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºÂØπÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰ΩøÁî®CauchyÊçüÂ§±Êõø‰ª£‰º™HuberÊçüÂ§±ÔºåÂπ∂ÂºïÂÖ•Êâ©Êï£ÊçüÂ§±ÂíåÊúÄ‰ºò‰º†ËæìÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01584', 'title': 'PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01584', 'abstract': "Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.", 'score': 5, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '6cf23c9aeb70961a', 'authors': ['Carolyn Jane Anderson', 'Joydeep Biswas', 'Aleksander Boruch-Gruszecki', 'Federico Cassano', 'Molly Q Feldman', 'Arjun Guha', 'Francesca Lucchetti', 'Zixuan Wu'], 'affiliations': ['Charles University', 'Cursor', 'Northeastern University', 'Oberlin College', 'University of Texas at Austin', 'Wellesley College'], 'pdf_title_img': 'assets/pdf/title_img/2502.01584.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#inference', '#benchmark'], 'emoji': 'üß©', 'ru': {'title': '–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö NPR Sunday Puzzle Challenge. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ—Å—Ç–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π, —ç—Ç–æ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –∏ –ª–µ–≥–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ OpenAI o1 –Ω–∞–¥ –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–Ω–∞–ª–∏–∑ —Ç–∞–∫–∂–µ –æ–±–Ω–∞—Ä—É–∂–∏–ª –Ω–æ–≤—ã–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫ —É –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∫–∞–ø–∏—Ç—É–ª—è—Ü–∏—è –∏ –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–∞—Ö.'}, 'en': {'title': 'Bridging Knowledge Gaps in AI Reasoning with General Knowledge Benchmarks', 'desc': "This paper introduces a new benchmark for evaluating machine learning models, focusing on general knowledge rather than specialized expertise. The benchmark is based on the NPR Sunday Puzzle Challenge, making it accessible yet challenging for both humans and AI models. The authors highlight that existing benchmarks may not reveal certain capability gaps, as demonstrated by OpenAI's model outperforming others in reasoning tasks. Additionally, the study identifies specific failure modes in models, such as premature concessions and uncertainty in outputs, suggesting improvements in inference techniques are necessary."}, 'zh': {'title': 'ÊåëÊàòÊÄß‰∏éÂèØÈ™åËØÅÊÄßÁöÑÂÖ®Êñ∞Âü∫ÂáÜÊµãËØï', 'desc': 'Áé∞ÊúâÁöÑÂâçÊ≤øÊ®°ÂûãÂü∫ÂáÜÊµãËØïÈÄöÂ∏∏ËÄÉÂØü‰∏ì‰∏öÁöÑ„ÄÅÈöæ‰ª•ÁêÜËß£ÁöÑÁü•ËØÜ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éNPRÂë®Êó•Ë∞úÈ¢òÊåëÊàòÁöÑÂü∫ÂáÜÔºåË¶ÅÊ±Ç‰ªÖÂÖ∑Â§á‰∏ÄËà¨Áü•ËØÜ„ÄÇËØ•Âü∫ÂáÜÂØπ‰∫∫Á±ªÂíåÊ®°ÂûãÈÉΩÂÖ∑ÊúâÊåëÊàòÊÄßÔºå‰ΩÜÊ≠£Á°ÆÁ≠îÊ°àÊòì‰∫éÈ™åËØÅÔºåÊ®°ÂûãÁöÑÈîôËØØ‰πüÂÆπÊòìÂèëÁé∞„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Êè≠Á§∫‰∫ÜÁé∞ÊúâÂü∫ÂáÜ‰∏≠Êú™ÊòæÁé∞ÁöÑËÉΩÂäõÂ∑ÆË∑ùÔºåOpenAI o1Âú®Êé®ÁêÜÊ®°Âûã‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏îÂàÜÊûêÊé®ÁêÜËæìÂá∫Êè≠Á§∫‰∫ÜÊñ∞ÁöÑÂ§±Ë¥•Á±ªÂûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01636', 'title': 'Lifelong Sequential Knowledge Editing without Model Degradation', 'url': 'https://huggingface.co/papers/2502.01636', 'abstract': 'Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this "importance hacking", the edited layers provide a much larger contributions to the model\'s output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B.', 'score': 4, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '919dd5274b620b3a', 'authors': ['Akshat Gupta', 'Phudish Prateepamornkul', 'Maochuan Lu', 'Ahmed Alaa', 'Thomas Hartvigsen', 'Gopala Anumanchipalli'], 'affiliations': ['SCB DataX', 'University of California, Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2502.01636.jpg', 'data': {'categories': ['#training', '#interpretability', '#architecture', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ –Ω–æ—Ä–º—ã –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ENCORE, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏ —Ä–æ—Å—Ç –Ω–æ—Ä–º—ã, –ø–æ–∑–≤–æ–ª—è—è –≤—ã–ø–æ–ª–Ω—è—Ç—å –¥–æ 10 000 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–∫ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. ENCORE —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π.'}, 'en': {'title': 'ENCORE: Efficient Knowledge Editing Without Degradation', 'desc': "This paper investigates the challenges of sequential knowledge editing in machine learning models, particularly focusing on the degradation of model performance after numerous edits. It identifies that traditional locate-then-edit methods can lead to overfitting and excessive growth in the norm of the edited parameters. The authors introduce a new method called ENCORE, which employs early stopping and norm constraints to prevent these issues, allowing for effective long-term editing. ENCORE not only maintains the model's performance after 10,000 edits but also operates significantly faster than existing methods."}, 'zh': {'title': 'ENCOREÔºöÈ´òÊïàÁöÑÁü•ËØÜÁºñËæëËß£ÂÜ≥ÊñπÊ°à', 'desc': 'Êú¨ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂú®Áü•ËØÜÁºñËæë‰∏≠ËøõË°åÂ§ßËßÑÊ®°È°∫Â∫èÁºñËæëÊó∂Ê®°ÂûãÊÄßËÉΩ‰∏ãÈôçÁöÑÂéüÂõ†„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂÆö‰ΩçÂêéÁºñËæëÁöÑÊñπÊ≥ïÂÆπÊòìÂØºËá¥ÂØπÁºñËæë‰∫ãÂÆûÁöÑËøáÊãüÂêàÔºåÂπ∂‰∏îËøûÁª≠ÁöÑÁü•ËØÜÁºñËæë‰ºöÂØºËá¥ÁºñËæëÁü©ÈòµÁöÑËåÉÊï∞‰∏çÊàêÊØî‰æãÂú∞Â¢ûÈïø„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜENCOREÊñπÊ≥ïÔºåÈÄöËøáÊó©ÂÅúÂíåËåÉÊï∞Á∫¶ÊùüÊù•ÊéßÂà∂ËøáÊãüÂêàÂíåËåÉÊï∞Â¢ûÈïøÔºå‰ªéËÄåÂÆûÁé∞ÈïøÊó∂Èó¥ÁöÑÈ°∫Â∫èÁºñËæë„ÄÇENCOREËÉΩÂ§üÂú®‰∏çÊçüÂ§±‰∏ãÊ∏∏ÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÔºåËøõË°åÂ§öËææ10,000Ê¨°ÁöÑÈ°∫Â∫èÁºñËæëÔºåÂπ∂‰∏îÊØîÁé∞ÊúâÊñπÊ≥ïÊõ¥Âø´„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01619', 'title': 'Learning to Generate Unit Tests for Automated Debugging', 'url': 'https://huggingface.co/papers/2502.01619', 'abstract': "Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to a large language model (LLM) as it iteratively debugs faulty code, motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions and candidate code. We integrate UTGen into UTDebug, a robust debugging pipeline that uses generated tests to help LLMs debug effectively. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), UTDebug (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and back-tracks edits based on multiple generated UTs to avoid overfitting. We show that UTGen outperforms UT generation baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen-2.5 7B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3% and 12.35% (respectively) over other LLM-based UT generation baselines.", 'score': 2, 'issue_id': 2036, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '0806c010a6d2569d', 'authors': ['Archiki Prasad', 'Elias Stengel-Eskin', 'Justin Chih-Yao Chen', 'Zaid Khan', 'Mohit Bansal'], 'affiliations': ['Department of Computer Science, UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2502.01619.jpg', 'data': {'categories': ['#training', '#interpretability', '#optimization', '#plp'], 'emoji': 'üß™', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —é–Ω–∏—Ç-—Ç–µ—Å—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–ª–∞–¥–∫–∏ –∫–æ–¥–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UTGen - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —é–Ω–∏—Ç-—Ç–µ—Å—Ç–æ–≤, –≤—ã—è–≤–ª—è—é—â–∏—Ö –æ—à–∏–±–∫–∏ –≤ –∫–æ–¥–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. UTGen –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ –∫–æ–Ω–≤–µ–π–µ—Ä –æ—Ç–ª–∞–¥–∫–∏ UTDebug, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ç–ª–∞–¥–∫–∏ –∫–æ–¥–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. UTDebug –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç UTGen –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —é–Ω–∏—Ç-—Ç–µ—Å—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ UTGen –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —é–Ω–∏—Ç-—Ç–µ—Å—Ç–æ–≤, –∞ UTDebug —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–ª–∞–¥–∫–∏ –∫–æ–¥–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'Enhancing Debugging with Smart Unit Test Generation', 'desc': 'This paper introduces UTGen, a method that helps large language models (LLMs) generate unit test inputs that can effectively reveal errors in faulty code while also predicting the correct outputs. The authors highlight a challenge where generating tests that expose errors can lead to incorrect output predictions without having the correct solutions available. To overcome this, UTGen is integrated into a debugging pipeline called UTDebug, which enhances the debugging process by validating and refining the generated tests. The results show that UTGen significantly improves the accuracy of LLMs in debugging tasks, outperforming existing methods in generating effective unit tests.'}, 'zh': {'title': 'Ëá™Âä®ÂåñÂçïÂÖÉÊµãËØïÁîüÊàê‰∏éË∞ÉËØïÁöÑÂàõÊñ∞ÊñπÊ°à', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫UTGenÁöÑËá™Âä®ÂåñÂçïÂÖÉÊµãËØïÁîüÊàêÊñπÊ≥ïÔºåÊó®Âú®Â∏ÆÂä©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁîüÊàêËÉΩÂ§üÊè≠Á§∫ÈîôËØØÁöÑÂçïÂÖÉÊµãËØïËæìÂÖ•ÂèäÂÖ∂Ê≠£Á°ÆÁöÑÈ¢ÑÊúüËæìÂá∫„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁîüÊàêÁöÑÂçïÂÖÉÊµãËØïËæìÂÖ•Âú®Êè≠Á§∫ÈîôËØØÂíåÊ≠£Á°ÆÈ¢ÑÊµãËæìÂá∫‰πãÈó¥Â≠òÂú®ÊùÉË°°„ÄÇUTGenË¢´ÈõÜÊàêÂà∞UTDebugË∞ÉËØïÁÆ°ÈÅì‰∏≠Ôºå‰ª•ÊèêÈ´òLLMÁöÑË∞ÉËØïÊïàÊûúÔºåÂπ∂ÈÄöËøáÂ§öÊ¨°ÁîüÊàêÁöÑÊµãËØïÊù•È™åËØÅÂíåÂõûÊ∫ØÁºñËæëÔºåÈÅøÂÖçËøáÊãüÂêà„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUTGenÂú®ÁîüÊàêÊúâÊïàÂçïÂÖÉÊµãËØïÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºåÂπ∂ÊòæËëóÊèêÈ´ò‰∫ÜLLMÂú®Ë∞ÉËØï‰ªªÂä°‰∏≠ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18055', 'title': 'Current Pathology Foundation Models are unrobust to Medical Center Differences', 'url': 'https://huggingface.co/papers/2501.18055', 'abstract': 'Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences. We introduce the Robustness Index. This novel robustness metric reflects to what degree biological features dominate confounding features. Ten current publicly available pathology FMs are evaluated. We find that all current pathology foundation models evaluated represent the medical center to a strong degree. Significant differences in the robustness index are observed. Only one model so far has a robustness index greater than one, meaning biological features dominate confounding features, but only slightly. A quantitative approach to measure the influence of medical center differences on FM-based prediction performance is described. We analyze the impact of unrobustness on classification performance of downstream models, and find that cancer-type classification errors are not random, but specifically attributable to same-center confounders: images of other classes from the same medical center. We visualize FM embedding spaces, and find these are more strongly organized by medical centers than by biological factors. As a consequence, the medical center of origin is predicted more accurately than the tissue source and cancer type. The robustness index introduced here is provided with the aim of advancing progress towards clinical adoption of robust and reliable pathology FMs.', 'score': 1, 'issue_id': 2024, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '444c1f08656649e7', 'authors': ['Edwin D. de Jong', 'Eric Marcus', 'Jonas Teuwen'], 'affiliations': ['Aignostics', 'Antoni van Leeuwenhoek Hospital (AvL)', 'Kaiko', 'The Netherlands Cancer Institute Amsterdam (NKI)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18055.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#security', '#dataset'], 'emoji': 'üî¨', 'ru': {'title': '–ü—É—Ç—å –∫ –Ω–∞–¥–µ–∂–Ω—ã–º —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤ –ø–∞—Ç–æ–ª–æ–≥–∏–∏: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ü–µ–Ω—Ç—Ä–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–§–ú) –≤ –ø–∞—Ç–æ–ª–æ–≥–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É - –ò–Ω–¥–µ–∫—Å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å—Ç–µ–ø–µ–Ω—å –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞–¥ –º–µ—à–∞—é—â–∏–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ü–µ–Ω—Ç—Ä–æ–≤. –û—Ü–µ–Ω–∫–∞ –¥–µ—Å—è—Ç–∏ –ø—É–±–ª–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –§–ú –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –≤—Å–µ –æ–Ω–∏ —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ü–µ–Ω—Ç—Ä–æ–≤, –∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –∏–Ω–¥–µ–∫—Å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–µ –µ–¥–∏–Ω–∏—Ü—ã. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∏–ø–æ–≤ —Ä–∞–∫–∞ —Å–≤—è–∑–∞–Ω—ã —Å –∫–æ–Ω—Ñ–∞—É–Ω–¥–µ—Ä–∞–º–∏ –∏–∑ —Ç–æ–≥–æ –∂–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞, –∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤–ª–æ–∂–µ–Ω–∏–π –§–ú –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω—ã –±–æ–ª—å—à–µ –ø–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º —Ü–µ–Ω—Ç—Ä–∞–º, —á–µ–º –ø–æ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º —Ñ–∞–∫—Ç–æ—Ä–∞–º.'}, 'en': {'title': 'Ensuring Robustness in Pathology Models for Clinical Use', 'desc': "This paper discusses the evaluation of Pathology Foundation Models (FMs) to ensure they are reliable for clinical use. It introduces a new metric called the Robustness Index, which measures how much these models focus on biological features versus confounding factors from different medical centers. The study finds that most current pathology FMs are heavily influenced by the medical center's characteristics rather than the actual biological data. The authors emphasize the need for models that prioritize biological features to improve classification accuracy and support clinical adoption."}, 'zh': {'title': 'Á°Æ‰øùÁóÖÁêÜÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÔºåÂä©Âäõ‰∏¥Â∫äÂ∫îÁî®', 'desc': 'ÁóÖÁêÜÂü∫Á°ÄÊ®°ÂûãÔºàFMsÔºâÂú®ÂåªÁñó‰øùÂÅ•‰∏≠ÂÖ∑ÊúâÂæàÂ§ßÊΩúÂäõÔºå‰ΩÜÂú®‰∏¥Â∫äÂ∫îÁî®‰πãÂâçÔºåÂøÖÈ°ªÁ°Æ‰øùÂÆÉ‰ª¨ÂØπ‰∏çÂêåÂåªÁñó‰∏≠ÂøÉÁöÑÂèòÂåñÂÖ∑ÊúâÈ≤ÅÊ£íÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÈ≤ÅÊ£íÊÄßÊåáÊï∞ÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÈ≤ÅÊ£íÊÄßÂ∫¶ÈáèÔºåÂèçÊò†ÁîüÁâ©ÁâπÂæÅÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏ä‰∏ªÂØº‰∫ÜÊ∑∑Ê∑ÜÁâπÂæÅ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂΩìÂâçÁöÑÁóÖÁêÜÂü∫Á°ÄÊ®°ÂûãÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ª£Ë°®‰∫ÜÂåªÁñó‰∏≠ÂøÉÔºåÂè™Êúâ‰∏Ä‰∏™Ê®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÊåáÊï∞Â§ß‰∫é‰∏ÄÔºåË°®ÊòéÁîüÁâ©ÁâπÂæÅÁï•ÂæÆ‰∏ªÂØºÊ∑∑Ê∑ÜÁâπÂæÅ„ÄÇÈÄöËøáÂÆöÈáèÊñπÊ≥ïÂàÜÊûêÂåªÁñó‰∏≠ÂøÉÂ∑ÆÂºÇÂØπÊ®°ÂûãÈ¢ÑÊµãÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÂèëÁé∞ÁôåÁóáÁ±ªÂûãÂàÜÁ±ªÈîôËØØ‰∏éÂêå‰∏≠ÂøÉÊ∑∑Ê∑ÜÂõ†Á¥†ÂØÜÂàáÁõ∏ÂÖ≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00314', 'title': 'A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation', 'url': 'https://huggingface.co/papers/2502.00314', 'abstract': "The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub.", 'score': 1, 'issue_id': 2023, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '51ad114da14800b7', 'authors': ['Moein Heidari', 'Ehsan Khodapanah Aghdam', 'Alexander Manzella', 'Daniel Hsu', 'Rebecca Scalabrino', 'Wenjin Chen', 'David J. Foran', 'Ilker Hacihaliloglu'], 'affiliations': ['Beth Israel Deaconess Medical Center, Boston, MA, United States', 'Center for Biomedical Imaging and Informatics, Rutgers Cancer Institute, New Brunswick, NJ, United States', 'Department of Medicine, University of British Columbia, British Columbia, Canada', 'Department of Radiology, University of British Columbia, British Columbia, Canada', 'Harvard Medical School, Boston, MA, United States', 'Independent Researcher, Tabriz, Iran', 'Memorial Sloan Kettering Cancer Center, New York, NY, United States', 'Rutgers Robert Wood Johnson Medical School, New Brunswick, NJ, United States', 'School of Biomedical Engineering, University of British Columbia, British Columbia, Canada', 'Weill Cornell Medical School, New York, NY, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.00314.jpg', 'data': {'categories': ['#architecture', '#cv', '#training', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–ø—É—Ö–æ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–ø—É—Ö–æ–ª–µ–π –∑–∞–±—Ä—é—à–∏–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å U-Net –∏ –µ–≥–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π, –≤–∫–ª—é—á–∞—è CNN, ViT, Mamba SSM –∏ xLSTM, –Ω–∞ –Ω–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –ö–¢. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ViLU-Net –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç Vi-–±–ª–æ–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ xLSTM –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–∞–±–æ—Ç—É –≤ —Ä–∞–º–∫–∞—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã U-Net.'}, 'en': {'title': 'Efficient Tumor Segmentation with ViLU-Net: Merging U-Net and Vision Transformers', 'desc': "This paper addresses the challenges of segmenting tumors in the retroperitoneum, which can be irregularly shaped and difficult to analyze. It explores the use of advanced machine learning models, particularly U-Net and its enhancements, to automate the segmentation process. The study introduces the ViLU-Net model, which incorporates Vision Transformer blocks to improve segmentation accuracy while maintaining computational efficiency. Results indicate that the xLSTM architecture significantly enhances the U-Net framework's performance, making it a promising approach for medical image analysis."}, 'zh': {'title': 'È´òÊïàËÇøÁò§ÂàÜÂâ≤ÔºöViLU-NetÁöÑÂàõÊñ∞Â∫îÁî®', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂú®ÂêéËÖπËÜúËÇøÁò§ÁöÑËá™Âä®ÂàÜÂâ≤‰∏≠‰ΩøÁî®U-NetÂèäÂÖ∂Âèò‰ΩìÁöÑÊúâÊïàÊÄß„ÄÇËøô‰∫õËÇøÁò§ÂΩ¢Áä∂‰∏çËßÑÂàôÔºåÊâãÂä®ÂàÜÂâ≤ËÄóÊó∂‰∏îÂõ∞ÈöæÔºåÂõ†Ê≠§ÈúÄË¶ÅÊõ¥È´òÊïàÁöÑËá™Âä®ÂåñÊñπÊ≥ï„ÄÇÁ†îÁ©∂‰∏≠ÂºïÂÖ•‰∫ÜMambaÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÂíåÊâ©Â±ïÈïøÁü≠ÊúüËÆ∞ÂøÜÔºàxLSTMÔºâÁ≠âÊû∂ÊûÑÔºå‰ª•Èôç‰ΩéËÆ°ÁÆóËµÑÊ∫êÊ∂àËÄóÂπ∂Â§ÑÁêÜÈïøË∑ùÁ¶ª‰æùËµñ„ÄÇÊúÄÁªàÊèêÂá∫ÁöÑViLU-NetÊ®°ÂûãÈÄöËøáÈõÜÊàêVi-blocksÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂàÜÂâ≤ÊïàÊûúÔºåxLSTMÂú®U-NetÊ°ÜÊû∂‰∏≠ÁöÑÊïàÁéáË°®Áé∞Â∞§‰∏∫Á™ÅÂá∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01126', 'title': 'Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences', 'url': 'https://huggingface.co/papers/2502.01126', 'abstract': 'Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty. However, models struggle to provide absolute assessments of confidence (i.e. judging confidence in answering a question independent of other questions) and the coarse-grained scores they produce are not useful for evaluating the correctness of their answers. We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence ("Which question are you more confident in answering correctly?"). Treating each question as a "player" in a series of matchups against other questions and the model\'s preferences as match outcomes, we can use rank aggregation methods like Elo rating and Bradley-Terry to translate the model\'s confidence preferences into confidence scores. We evaluate relative confidence estimation against absolute confidence estimation and self-consistency confidence methods on five state-of-the-art LMs -- GPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405B -- across 14 challenging STEM, social science, and commonsense reasoning question answering tasks. Our results demonstrate that relative confidence estimation consistently provides more reliable confidence scores than absolute confidence estimation, with average gains of 3.5% in selective classification AUC over direct absolute confidence estimation methods and 1.7% over self-consistency approaches across all models and datasets.', 'score': 0, 'issue_id': 2035, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'ed2eef6c3e310379', 'authors': ['Vaishnavi Shrivastava', 'Ananya Kumar', 'Percy Liang'], 'affiliations': ['cs.stanford.edu'], 'pdf_title_img': 'assets/pdf/title_img/2502.01126.jpg', 'data': {'categories': ['#interpretability', '#rlhf', '#reasoning', '#benchmark', '#data'], 'emoji': 'üß†', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≥–¥–µ –º–æ–¥–µ–ª—å —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å–≤–æ—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ —Ä–∞–∑–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ä–µ–π—Ç–∏–Ω–≥ –≠–ª–æ –∏ –º–æ–¥–µ–ª—å –ë—Ä—ç–¥–ª–∏-–¢–µ—Ä—Ä–∏, –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –º–æ–¥–µ–ª–∏ –≤ –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–±—Å–æ–ª—é—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –∏ –º–µ—Ç–æ–¥—ã —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ 3.5% –∏ 1.7% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ –º–µ—Ç—Ä–∏–∫–µ AUC –≤—ã–±–æ—Ä–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –Ω–∞ –ø—è—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ 14 —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ –æ—Ç–≤–µ—Ç–∞–º –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤ –æ–±–ª–∞—Å—Ç—è—Ö STEM, —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞—É–∫ –∏ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞.'}, 'en': {'title': 'Boosting Confidence: Relative Estimation Outshines Absolute Scores in Language Models', 'desc': 'This paper discusses the importance of reliable confidence estimates from language models (LMs) to help users identify potential errors in their outputs. It highlights the challenges LMs face in providing absolute confidence assessments and proposes a new method called relative confidence estimation. This method involves comparing questions against each other to determine which the model is more confident in answering correctly, using techniques like Elo rating and Bradley-Terry for ranking. The study shows that relative confidence estimation outperforms traditional absolute confidence methods, leading to improved reliability in confidence scores across various question answering tasks.'}, 'zh': {'title': 'Áõ∏ÂØπÁΩÆ‰ø°Â∫¶‰º∞ËÆ°ÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØÈù†ÊÄß', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÂ¶Ç‰ΩïÊèê‰æõÂèØÈù†ÁöÑÁΩÆ‰ø°Â∫¶ËØÑ‰º∞Ôºå‰ª•Â∏ÆÂä©Áî®Êà∑ËØÜÂà´ËæìÂá∫‰∏≠ÁöÑÈîôËØØ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁõ∏ÂØπÁΩÆ‰ø°Â∫¶‰º∞ËÆ°ÁöÑÊñπÊ≥ïÔºåÈÄöËøáÊØîËæÉÈóÆÈ¢ò‰πãÈó¥ÁöÑÁõ∏ÂØπÁΩÆ‰ø°Â∫¶Êù•ËØÑ‰º∞Ê®°ÂûãÁöÑ‰ø°ÂøÉ„ÄÇ‰∏éÁªùÂØπÁΩÆ‰ø°Â∫¶ËØÑ‰º∞Áõ∏ÊØîÔºåÁõ∏ÂØπÁΩÆ‰ø°Â∫¶‰º∞ËÆ°Âú®Â§ö‰∏™ÂÖàËøõÁöÑËØ≠Ë®ÄÊ®°Âûã‰∏äË°®Áé∞Âá∫Êõ¥È´òÁöÑÂèØÈù†ÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈÄâÊã©ÊÄßÂàÜÁ±ª‰ªªÂä°‰∏≠„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÁõ∏ÂØπÁΩÆ‰ø°Â∫¶‰º∞ËÆ°Âú®ÂáÜÁ°ÆÊÄß‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü3.5%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.19393', 'title': 's1: Simple test-time scaling', 'url': 'https://huggingface.co/papers/2501.19393', 'abstract': 'Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI\'s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model\'s thinking process or lengthening it by appending "Wait" multiple times to the model\'s generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.', 'score': 48, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': '8fcf84a9effc288f', 'authors': ['Niklas Muennighoff', 'Zitong Yang', 'Weijia Shi', 'Xiang Lisa Li', 'Li Fei-Fei', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer', 'Percy Liang', 'Emmanuel Cand√®s', 'Tatsunori Hashimoto'], 'affiliations': ['Allen Institute for AI', 'Contextual AI', 'Stanford University', 'University of Washington, Seattle'], 'pdf_title_img': 'assets/pdf/title_img/2501.19393.jpg', 'data': {'categories': ['#dataset', '#open_source', '#reasoning', '#training', '#math', '#optimization', '#data'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–æ—Å—Ç–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —è–∑—ã–∫–æ–≤–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π —Ç–µ—Å—Ç–æ–≤—ã–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥–µ–ª—å s1, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ Qwen2.5-32B-Instruct, –∏ –º–µ—Ç–æ–¥ –±—é–¥–∂–µ—Ç–Ω–æ–≥–æ —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö s1K –∏–∑ 1000 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ s1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª—å o1-preview –æ—Ç OpenAI –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞.'}, 'en': {'title': 'Enhancing Language Models with Test-Time Scaling', 'desc': "This paper introduces a method called test-time scaling for enhancing language model performance during evaluation. The authors create a dataset of 1,000 questions with reasoning traces to train their model, focusing on difficulty, diversity, and quality. They implement a technique called budget forcing, which manipulates the model's response time to encourage deeper reasoning and correct errors. After fine-tuning their model, they demonstrate significant improvements in solving math competition questions compared to previous models, showcasing the effectiveness of their approach."}, 'zh': {'title': 'ÊµãËØïÊó∂Èó¥Êâ©Â±ïÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØ≠Ë®ÄÂª∫Ê®°ÊñπÊ≥ï‚Äî‚ÄîÊµãËØïÊó∂Èó¥Êâ©Â±ïÔºåÊó®Âú®ÈÄöËøáÂ¢ûÂä†ÊµãËØïÊó∂Èó¥ËÆ°ÁÆóÊù•ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇÁ†îÁ©∂ËÄÖ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´1000‰∏™ÈóÆÈ¢òÂèäÂÖ∂Êé®ÁêÜËøáÁ®ãÁöÑÂ∞èÊï∞ÊçÆÈõÜs1KÔºåÂπ∂ÈÄöËøáÈöæÂ∫¶„ÄÅÂ§öÊ†∑ÊÄßÂíåË¥®Èáè‰∏â‰∏™Ê†áÂáÜËøõË°åÈ™åËØÅ„ÄÇ‰∏∫‰∫ÜÊéßÂà∂ÊµãËØïÊó∂Èó¥ËÆ°ÁÆóÔºåÊèêÂá∫‰∫ÜÈ¢ÑÁÆóÂº∫Âà∂ÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂº∫Âà∂ÁªàÊ≠¢Ê®°ÂûãÁöÑÊÄùËÄÉËøáÁ®ãÊàñÂú®Ê®°ÂûãÁîüÊàêÊó∂Â§öÊ¨°Ê∑ªÂä†‚ÄúÁ≠âÂæÖ‚ÄùÊù•Âª∂ÈïøÊÄùËÄÉÊó∂Èó¥Ôºå‰ªéËÄå‰øÉ‰ΩøÊ®°ÂûãÊ£ÄÊü•Á≠îÊ°à„ÄÇÁªèËøáÁõëÁù£ÂæÆË∞ÉÂêéÔºåÊ®°Âûãs1Âú®Êï∞Â≠¶Á´ûËµõÈóÆÈ¢ò‰∏äË∂ÖË∂ä‰∫ÜOpenAIÁöÑo1Ê®°ÂûãÔºåË°®Áé∞ÊèêÂçáËææ27%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.19324', 'title': 'Reward-Guided Speculative Decoding for Efficient LLM Reasoning', 'url': 'https://huggingface.co/papers/2501.19324', 'abstract': '', 'score': 26, 'issue_id': 1995, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': 'ce2d414eedfb7a1e', 'authors': ['Baohao Liao', 'Yuhui Xu', 'Hanze Dong', 'Junnan Li', 'Christof Monz', 'Silvio Savarese', 'Doyen Sahoo', 'Caiming Xiong'], 'affiliations': ['Language Technology Lab, University of Amsterdam', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2501.19324.jpg', 'data': {'categories': [], 'emoji': 'ü§ñ', 'ru': {'title': '–ù–æ–≤—ã–π —à–∞–≥ –≤ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —É–ª—É—á—à–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è AI –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞.'}, 'en': {'title': 'Hybrid Networks: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': '‰ºòÂåñÊï∞ÊçÆÂ§ÑÁêÜÔºåÊèêÂçáÊú∫Âô®Â≠¶‰π†ÊÄßËÉΩ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàõÊñ∞ÁöÑÊñπÊ≥ïÔºåÈÄöËøá‰ºòÂåñÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÂíåÁâπÂæÅÈÄâÊã©Êù•Â¢ûÂº∫Â≠¶‰π†ÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÁÆóÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÊäÄÊúØ„ÄÇÊúÄÁªàÔºåÁ†îÁ©∂Ë°®ÊòéÔºåÊîπËøõÁöÑÊï∞ÊçÆÂ§ÑÁêÜÊµÅÁ®ãÂØπÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑÊÄßËÉΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18119', 'title': 'Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models', 'url': 'https://huggingface.co/papers/2501.18119', 'abstract': 'Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.', 'score': 12, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'd751c8a690173842', 'authors': ['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], 'affiliations': ['National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2501.18119.jpg', 'data': {'categories': ['#inference', '#graphs', '#transfer_learning', '#training', '#multimodal', '#data'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π –∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–¥—ã', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (SSQR) –¥–ª—è —Å–∂–∞—Ç–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π –≥—Ä–∞—Ñ–∞ –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ–¥—ã. –≠—Ç–∏ –∫–æ–¥—ã –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ SSQR –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π LLaMA2 –∏ LLaMA3.1 –Ω–∞ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –≥—Ä–∞—Ñ–∞–º–∏ –∑–Ω–∞–Ω–∏–π.'}, 'en': {'title': 'Seamless Integration of Knowledge Graphs and Language Models', 'desc': 'This paper addresses the challenge of integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) by proposing a two-stage framework. The framework utilizes a self-supervised quantized representation (SSQR) method to convert KG structural and semantic information into discrete codes that resemble language tokens. By treating these codes as features for LLMs, the approach allows for a more efficient and effective integration of KGs with LLMs. Experimental results show that SSQR outperforms traditional methods, enabling better performance in tasks like KG link prediction and triple classification with significantly fewer tokens.'}, 'zh': {'title': 'Êó†ÁºùÊï¥ÂêàÁü•ËØÜÂõæË∞±‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁü•ËØÜÂõæË∞±ÔºàKGÔºâÁªìÊûÑ‰∏éËá™ÁÑ∂ËØ≠Ë®Ä‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏§Èò∂ÊÆµÊ°ÜÊû∂Ôºå‰ª•ÂÆûÁé∞KG‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊúâÊïàÊï¥Âêà„ÄÇÈ¶ñÂÖàÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÁõëÁù£ÈáèÂåñË°®Á§∫ÔºàSSQRÔºâÊñπÊ≥ïÔºåÂ∞ÜKGÁöÑÁªìÊûÑÂíåËØ≠‰πâÁü•ËØÜÂéãÁº©‰∏∫Á¶ªÊï£‰ª£Á†ÅÔºàÂç≥‰ª§ÁâåÔºâÔºå‰ΩøÂÖ∂‰∏éËØ≠Ë®ÄÂè•Â≠êÁöÑÊ†ºÂºèÂØπÈΩê„ÄÇÊé•ÁùÄÔºåËÆæËÆ°‰∫ÜKGÊåá‰ª§Ë∑üÈöèÊï∞ÊçÆÔºåÂ∞ÜËøô‰∫õÂ≠¶‰π†Âà∞ÁöÑ‰ª£Á†ÅËßÜ‰∏∫ÁâπÂæÅÔºåÁõ¥Êé•ËæìÂÖ•Âà∞LLM‰∏≠Ôºå‰ªéËÄåÂÆûÁé∞Êó†ÁºùÊï¥Âêà„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSSQRÂú®Êó†ÁõëÁù£ÈáèÂåñÊñπÊ≥ï‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÁîüÊàêÁöÑ‰ª£Á†ÅÊõ¥ÂÖ∑ÂèØÂå∫ÂàÜÊÄßÔºå‰∏îÁªèËøáÂæÆË∞ÉÁöÑLLaMA2ÂíåLLaMA3.1Âú®KGÈìæÊé•È¢ÑÊµãÂíå‰∏âÂÖÉÁªÑÂàÜÁ±ª‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ªÖ‰ΩøÁî®ÊØè‰∏™ÂÆû‰Ωì16‰∏™‰ª§ÁâåÔºåËÄå‰∏çÊòØ‰º†ÁªüÊñπÊ≥ï‰∏≠ÁöÑÊï∞ÂçÉ‰∏™„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.19339', 'title': 'PixelWorld: Towards Perceiving Everything as Pixels', 'url': 'https://huggingface.co/papers/2501.19339', 'abstract': 'Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models\' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models\' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance.', 'score': 9, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': '3e10b792328f7a4b', 'authors': ['Zhiheng Lyu', 'Xueguang Ma', 'Wenhu Chen'], 'affiliations': ['Department of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2501.19339.jpg', 'data': {'categories': ['#agi', '#benchmark', '#optimization', '#dataset', '#open_source', '#reasoning', '#multimodal', '#interpretability', '#cv'], 'emoji': 'üß†', 'ru': {'title': '–ï–¥–∏–Ω—ã–π –ø–∏–∫—Å–µ–ª—å–Ω—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –º–∏—Ä: –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –¥–ª—è –ò–ò', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–∞–Ω–Ω—ã—Ö (—Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫–æ–¥ –∏ —Ç.–¥.) –≤ –≤–∏–¥–µ –ø–∏–∫—Å–µ–ª–µ–π, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π PEAP (Perceive Everything as Pixels). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö PixelWorld –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ —ç—Ç–æ–º unified –ø–æ–¥—Ö–æ–¥–µ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PEAP –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –≤—ã—è–≤–ª—è–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –ø–∏–∫—Å–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'Unifying Perception: Everything as Pixels', 'desc': "This paper introduces a new approach called 'Perceive Everything as Pixels' (PEAP), which aims to unify various input modalities like text, images, and diagrams into a single pixel-based format. The authors present PixelWorld, a novel evaluation suite designed to assess the performance of existing models when using this unified pixel input. Their experiments reveal that PEAP outperforms traditional token-based methods in multimodal datasets, although it highlights a decline in reasoning and coding abilities across models when using pixel inputs. The study concludes that while current models excel in pixel perception, there is still significant potential for enhancing their overall perceptual capabilities."}, 'zh': {'title': 'Áªü‰∏ÄÊÑüÁü•ÔºöÂ∞Ü‰∏ÄÂàáËßÜ‰∏∫ÂÉèÁ¥†', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁªü‰∏ÄÊÑüÁü•Ê°ÜÊû∂ÔºåÁß∞‰∏∫‚ÄúÂ∞Ü‰∏ÄÂàáËßÜ‰∏∫ÂÉèÁ¥†‚ÄùÔºàPEAPÔºâÔºåÊó®Âú®Â∞ÜÊñáÊú¨„ÄÅË°®Ê†º„ÄÅ‰ª£Á†Å„ÄÅÂõæË°®ÂíåÂõæÂÉèÁ≠âÂ§öÁßçËæìÂÖ•ÂΩ¢ÂºèÁªü‰∏Ä‰∏∫ÂÉèÁ¥†ËæìÂÖ•„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜPixelWorldËØÑ‰º∞Â•ó‰ª∂Ôºå‰ª•Âú®ÂÉèÁ¥†Á©∫Èó¥‰∏≠ËØÑ‰º∞Áé∞ÊúâÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåPEAPÂú®Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÂü∫‰∫éÊ†áËÆ∞ÁöÑËæìÂÖ•ÔºåÊòæÁ§∫Âá∫Áªü‰∏ÄËæìÂÖ•Âú®Ê∂àÊ≠ß‰πâÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇÂêåÊó∂ÔºåÂ§ÑÁêÜÂÉèÁ¥†ËæìÂÖ•Êó∂ÔºåÊâÄÊúâÊ®°ÂûãÁöÑÊé®ÁêÜÂíåÁºñÁ†ÅËÉΩÂäõÊòæËëó‰∏ãÈôçÔºåË°®ÊòéÈúÄË¶ÅÂ¢ûÂº∫Âü∫Á°ÄÊ®°ÂûãÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.14677', 'title': 'MatAnyone: Stable Video Matting with Consistent Memory Propagation', 'url': 'https://huggingface.co/papers/2501.14677', 'abstract': 'Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively integrates memory from the previous frame. This ensures semantic stability in core regions while preserving fine-grained details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, boosting matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust and accurate video matting results in diverse real-world scenarios, outperforming existing methods.', 'score': 6, 'issue_id': 2010, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 —è–Ω–≤–∞—Ä—è', 'en': 'January 24', 'zh': '1Êúà24Êó•'}, 'hash': 'a9968478421ddc33', 'authors': ['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2501.14677.jpg', 'data': {'categories': ['#training', '#dataset', '#video'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': 'MatAnyone: –¢–æ—á–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ø–∞–º—è—Ç–∏ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è', 'desc': 'MatAnyone - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–¥–µ–ª–µ–Ω–∏—é –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –≤–∏–¥–µ–æ –±–µ–∑ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å –ø–∞–º—è—Ç–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∫–∞–¥—Ä–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω—è—è –¥–µ—Ç–∞–ª–∏ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –¥–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏. MatAnyone –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞.'}, 'en': {'title': 'MatAnyone: Robust Video Matting with Memory Propagation', 'desc': 'The paper introduces MatAnyone, a new framework for video matting that does not require auxiliary inputs. It utilizes a memory-based approach with a memory propagation module that adapts memory from previous frames to maintain semantic consistency and detail. The authors also present a larger and more diverse dataset for training, along with a novel strategy that uses extensive segmentation data to enhance matting stability. Overall, MatAnyone achieves superior performance in complex video environments compared to existing methods.'}, 'zh': {'title': 'MatAnyoneÔºöËßÜÈ¢ëÊä†ÂõæÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫MatAnyoneÁöÑËßÜÈ¢ëÊä†ÂõæÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Â§çÊùÇËÉåÊôØ‰∏ãÁöÑÊä†ÂõæÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫éËÆ∞ÂøÜ‰º†Êí≠Ê®°ÂùóÔºåÈÄöËøáÂå∫ÂüüËá™ÈÄÇÂ∫îËÆ∞ÂøÜËûçÂêàÔºåÂä®ÊÄÅÊï¥ÂêàÂâç‰∏ÄÂ∏ßÁöÑËÆ∞ÂøÜ‰ø°ÊÅØÔºå‰ªéËÄåÁ°Æ‰øùÊ†∏ÂøÉÂå∫ÂüüÁöÑËØ≠‰πâÁ®≥ÂÆöÊÄß„ÄÇ‰∏∫‰∫ÜÊèêÈ´òËÆ≠ÁªÉÁöÑÈ≤ÅÊ£íÊÄßÔºåÁ†îÁ©∂Âõ¢ÈòüÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êõ¥Â§ß„ÄÅÊõ¥È´òË¥®Èáè‰∏îÂ§öÊ†∑ÂåñÁöÑËßÜÈ¢ëÊä†ÂõæÊï∞ÊçÆÈõÜÔºåÂπ∂ÈááÁî®‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂÖÖÂàÜÂà©Áî®Â§ßËßÑÊ®°ÂàÜÂâ≤Êï∞ÊçÆ„ÄÇÊúÄÁªàÔºåMatAnyoneÂú®Â§öÁßçÁúüÂÆûÂú∫ÊôØ‰∏≠Â±ïÁé∞Âá∫‰ºòË∂äÁöÑÊä†ÂõæÊïàÊûúÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.19399', 'title': 'Scalable-Softmax Is Superior for Attention', 'url': 'https://huggingface.co/papers/2501.19399', 'abstract': "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.", 'score': 6, 'issue_id': 2009, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': '12ed1cad789702aa', 'authors': ['Ken M. Nakanishi'], 'affiliations': ['Institute for Physics of Intelligence, The University of Tokyo, Tokyo 113-0033, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2501.19399.jpg', 'data': {'categories': ['#architecture', '#training', '#long_context', '#optimization'], 'emoji': 'üîç', 'ru': {'title': 'SSMax: —É–ª—É—á—à–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é Scalable-Softmax (SSMax) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏. SSMax —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —É–ø–ª–æ—â–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å SSMax –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–∞—é—Ç—Å—è –∏ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏. SSMax –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–∂–µ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.'}, 'en': {'title': 'Enhancing Attention with Scalable-Softmax for Better Context Handling', 'desc': "This paper addresses a limitation in Transformer-based language models where the Softmax function causes attention scores to flatten as the input size increases. This flattening reduces the model's ability to focus on important information, especially in longer contexts. The authors propose a new method called Scalable-Softmax (SSMax) that replaces the traditional Softmax function, allowing for better attention distribution and improved performance in long contexts. Experimental results show that SSMax enhances loss reduction during pretraining and enables better retrieval of key information, even for models that have already begun pretraining."}, 'zh': {'title': 'ÂèØÊâ©Â±ïSoftmaxÔºöÊèêÂçáTransformerÊ®°ÂûãÁöÑÊ≥®ÊÑèÂäõËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫ÂèØÊâ©Â±ïSoftmaxÔºàSSMaxÔºâÔºåÊó®Âú®Ëß£ÂÜ≥TransformerÊ®°ÂûãÂú®Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÊó∂ÁöÑÊ≥®ÊÑèÂäõÂàÜÂ∏ÉÊâÅÂπ≥ÂåñÈóÆÈ¢ò„ÄÇ‰º†ÁªüÁöÑSoftmaxÂáΩÊï∞Âú®ËæìÂÖ•ÂêëÈáèÂ¢ûÂ§ßÊó∂ÔºåÊúÄÂ§ßÂÖÉÁ¥†Ë∂ãËøë‰∫éÈõ∂ÔºåÂØºËá¥Ê®°ÂûãÊó†Ê≥ïÊúâÊïàÂú∞‰ºòÂÖàËÄÉËôëÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇSSMaxÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Áé∞ÊúâÁöÑTransformerÊû∂ÊûÑ‰∏≠ÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®SSMaxÁöÑÊ®°ÂûãÂú®ËØ≠Ë®ÄÂª∫Ê®°‰∏≠‰∏ç‰ªÖÂú®È¢ÑËÆ≠ÁªÉÊúüÈó¥ÂÆûÁé∞‰∫ÜÊõ¥Âø´ÁöÑÊçüÂ§±ÂáèÂ∞ëÔºåËøòÊòæËëóÊèêÈ´ò‰∫ÜÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠ÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÂàÜÊûêÊ≥®ÊÑèÂäõÂàÜÊï∞ÔºåSSMax‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠Êõ¥Â•ΩÂú∞ÂÖ≥Ê≥®ÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÈïøÂ∫¶Ê≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.04983', 'title': 'DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning', 'url': 'https://huggingface.co/papers/2411.04983', 'abstract': 'The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.', 'score': 6, 'issue_id': 1999, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 –Ω–æ—è–±—Ä—è', 'en': 'November 7', 'zh': '11Êúà7Êó•'}, 'hash': 'e72081596b626524', 'authors': ['Gaoyue Zhou', 'Hengkai Pan', 'Yann LeCun', 'Lerrel Pinto'], 'affiliations': ['Courant Institute, New York University', 'Meta-FAIR'], 'pdf_title_img': 'assets/pdf/title_img/2411.04983.jpg', 'data': {'categories': ['#cv', '#agents', '#reasoning', '#optimization', '#rl'], 'emoji': 'üß†', 'ru': {'title': 'DINO-WM: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–µ–∑ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –±–µ–∑ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞ - DINO World Model (DINO-WM). DINO-WM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–∞—Ç—á–µ–π, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é DINOv2, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–º—É —É—á–∏—Ç—å—Å—è –Ω–∞ –æ—Ñ–ª–∞–π–Ω-—Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö –ø–æ–≤–µ–¥–µ–Ω–∏—è, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è –±—É–¥—É—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–∞—Ç—á–µ–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç DINO-WM –¥–æ—Å—Ç–∏–≥–∞—Ç—å –Ω–∞–±–ª—é–¥–∞–µ–º—ã—Ö —Ü–µ–ª–µ–π –ø—É—Ç–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π, –æ–±–ª–µ–≥—á–∞—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–µ –æ—Ç –∑–∞–¥–∞—á–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DINO-WM –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è —Å –Ω—É–ª—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Å–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'DINO-WM: Predicting the Future with Passive Data for Task-Agnostic Planning', 'desc': 'This paper introduces DINO World Model (DINO-WM), a novel approach for creating predictive models that can reason and plan across various tasks using only passive data. DINO-WM is designed to learn from offline trajectories without needing to reconstruct the visual environment, focusing instead on predicting future visual features. The model is capable of optimizing behavior at test time and supports task-agnostic reasoning, making it versatile for different applications. Experimental results show that DINO-WM can effectively generate solutions in diverse scenarios without relying on expert demonstrations or pre-learned models, showcasing its strong generalization abilities.'}, 'zh': {'title': 'DINO-WMÔºöÊó†‰ªªÂä°‰æùËµñÁöÑ‰∏ñÁïåÊ®°Âûã', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰∏ñÁïåÊ®°ÂûãDINO-WMÔºåÊó®Âú®ÈÄöËøáË¢´Âä®Êï∞ÊçÆËøõË°åÊé®ÁêÜÂíåËßÑÂàí„ÄÇDINO-WMÂÖ∑Êúâ‰∏â‰∏™ÂÖ≥ÈîÆÁâπÊÄßÔºöÂèØ‰ª•Âú®Á¶ªÁ∫øÊî∂ÈõÜÁöÑËΩ®Ëøπ‰∏äËøõË°åËÆ≠ÁªÉÔºåÊîØÊåÅÊµãËØïÊó∂Ë°å‰∏∫‰ºòÂåñÔºåÂπ∂‰øÉËøõ‰ªªÂä°Êó†ÂÖ≥ÁöÑÊé®ÁêÜ„ÄÇËØ•Ê®°ÂûãÂà©Áî®DINOv2È¢ÑËÆ≠ÁªÉÁöÑÁ©∫Èó¥Ë°•‰∏ÅÁâπÂæÅÔºåÈÄöËøáÈ¢ÑÊµãÊú™Êù•ÁöÑË°•‰∏ÅÁâπÂæÅÊù•Â≠¶‰π†Ôºå‰ªéËÄåÂÆûÁé∞ËßÇÂØüÁõÆÊ†áÁöÑË°å‰∏∫ËßÑÂàí„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDINO-WMÂú®Â§öÁßç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂú®Ê≤°Êúâ‰∏ìÂÆ∂Á§∫ËåÉÂíåÂ•ñÂä±Âª∫Ê®°ÁöÑÊÉÖÂÜµ‰∏ãÁîüÊàêÈõ∂-shotË°å‰∏∫Ëß£ÂÜ≥ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18837', 'title': 'Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming', 'url': 'https://huggingface.co/papers/2501.18837', 'abstract': 'Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable.', 'score': 4, 'issue_id': 1996, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': '62d14973b1140e58', 'authors': ['Mrinank Sharma', 'Meg Tong', 'Jesse Mu', 'Jerry Wei', 'Jorrit Kruthoff', 'Scott Goodfriend', 'Euan Ong', 'Alwin Peng', 'Raj Agarwal', 'Cem Anil', 'Amanda Askell', 'Nathan Bailey', 'Joe Benton', 'Emma Bluemke', 'Samuel R. Bowman', 'Eric Christiansen', 'Hoagy Cunningham', 'Andy Dau', 'Anjali Gopal', 'Rob Gilson', 'Logan Graham', 'Logan Howard', 'Nimit Kalra', 'Taesung Lee', 'Kevin Lin', 'Peter Lofgren', 'Francesco Mosconi', "Clare O'Hara", 'Catherine Olsson', 'Linda Petrini', 'Samir Rajani', 'Nikhil Saxena', 'Alex Silverstein', 'Tanya Singh', 'Theodore Sumers', 'Leonard Tang', 'Kevin K. Troy', 'Constantin Weisser', 'Ruiqi Zhong', 'Giulio Zhou', 'Jan Leike', 'Jared Kaplan', 'Ethan Perez'], 'affiliations': ['Safeguards Research Team, Anthropic'], 'pdf_title_img': 'assets/pdf/title_img/2501.18837.jpg', 'data': {'categories': ['#synthetic', '#training', '#architecture', '#dataset', '#security', '#inference'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–µ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã: –Ω–∞–¥–µ–∂–Ω–∞—è –∑–∞—â–∏—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã—Ö –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ - –∑–∞—â–∏—Ç–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏—Ö –¥–æ–ø—É—Å—Ç–∏–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. –≠—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–æ—Ç–∏–≤–æ—Å—Ç–æ—è—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –º–µ—Ç–æ–¥–∞–º –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã, –Ω–µ –ø–æ–∑–≤–æ–ª—è—è –∏–∑–≤–ª–µ–∫–∞—Ç—å –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∑–∞—â–∏—â–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞—Ç–∞–∫–∞–º –∏ –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –≤–ª–∏—è–Ω–∏–µ–º –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∑–∞—â–∏—Ç—ã LLM –æ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–±—Ö–æ–¥–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏.'}, 'en': {'title': 'Defending LLMs with Constitutional Classifiers', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to universal jailbreaks, which are strategies that allow users to bypass safety measures. The authors propose a solution called Constitutional Classifiers, which are trained on synthetic data generated by LLMs using natural language rules that define what content is allowed or restricted. Through extensive testing, these classifiers showed strong resistance to jailbreak attempts, effectively protecting the model without significantly impacting its performance. The findings suggest that it is possible to defend against such attacks while still ensuring the model can be used effectively in real-world applications.'}, 'zh': {'title': 'ÂÆ™Ê≥ïÂàÜÁ±ªÂô®Ôºö‰øùÊä§Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂÆπÊòìÂèóÂà∞ÊôÆÈÅçË∂äÁã±ÊîªÂáªÔºåËøôÁßçÊîªÂáªÂèØ‰ª•ÁªïËøáÊ®°ÂûãÁöÑÂÆâÂÖ®Êé™ÊñΩÔºåÂÖÅËÆ∏Áî®Êà∑ËøõË°åÊúâÂÆ≥Êìç‰Ωú„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂÆ™Ê≥ïÂàÜÁ±ªÂô®ÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÂêàÊàêÊï∞ÊçÆËÆ≠ÁªÉÁöÑÂÆâÂÖ®Êé™ÊñΩÔºåÂêàÊàêÊï∞ÊçÆÊòØÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄËßÑÂàôÔºàÂç≥ÂÆ™Ê≥ïÔºâÊèêÁ§∫LLMsÁîüÊàêÁöÑÔºåËßÑÂÆö‰∫ÜÂÖÅËÆ∏ÂíåÈôêÂà∂ÁöÑÂÜÖÂÆπ„ÄÇÂú®Ë∂ÖËøá3000Â∞èÊó∂ÁöÑÁ∫¢ÈòüÊµãËØï‰∏≠ÔºåÊ≤°ÊúâÁ∫¢ÈòüÊàêÂëòÊâæÂà∞ËÉΩÂ§ü‰ªéÊó©ÊúüÂàÜÁ±ªÂô®‰øùÊä§ÁöÑLLM‰∏≠ÊèêÂèñ‰ø°ÊÅØÁöÑÊôÆÈÅçË∂äÁã±ÊñπÊ≥ï„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂú®‰øùÊåÅÂÆûÈôÖÈÉ®ÁΩ≤ÂèØË°åÊÄßÁöÑÂêåÊó∂ÔºåÈò≤Âæ°ÊôÆÈÅçË∂äÁã±ÊîªÂáªÊòØÂèØË°åÁöÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18841', 'title': 'Trading Inference-Time Compute for Adversarial Robustness', 'url': 'https://huggingface.co/papers/2501.18841', 'abstract': 'We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.', 'score': 3, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': 'f1e75e6b24f3e044', 'authors': ['Wojciech Zaremba', 'Evgenia Nitishinskaya', 'Boaz Barak', 'Stephanie Lin', 'Sam Toyer', 'Yaodong Yu', 'Rachel Dias', 'Eric Wallace', 'Kai Xiao', 'Johannes Heidecke', 'Amelia Glaese'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2501.18841.jpg', 'data': {'categories': ['#security', '#reasoning', '#inference'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ë–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π - –≤—ã—à–µ –∑–∞—â–∏—Ç–∞: –ø–æ–≤—ã—à–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –ò–ò –∫ –∞—Ç–∞–∫–∞–º', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –≤–ª–∏—è–Ω–∏—é —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –Ω–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∫ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –∞—Ç–∞–∫–∞–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø—Ä–∏ –≤—ã–≤–æ–¥–µ —É–ª—É—á—à–∞–µ—Ç —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞—Ç–∞–∫–∞–º. –í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å–ª—É—á–∞–µ–≤ –¥–æ–ª—è —É—Å–ø–µ—à–Ω—ã—Ö –∞—Ç–∞–∫ —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ –Ω—É–ª—é –ø—Ä–∏ —Ä–æ—Å—Ç–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø—Ä–∏ –≤—ã–≤–æ–¥–µ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –∞—Ç–∞–∫–∞–º.'}, 'en': {'title': 'Boosting Robustness: More Compute, Less Vulnerability', 'desc': "This paper investigates how increasing the amount of compute used during inference can enhance the robustness of reasoning models, specifically OpenAI's o1-preview and o1-mini, against adversarial attacks. The authors find that as the compute increases, the success rate of these attacks generally decreases, indicating improved model resilience. Notably, this improvement occurs without any adversarial training, simply by allowing the models to utilize more resources for reasoning tasks. The study also examines new types of attacks and scenarios where increased compute does not lead to better reliability, providing insights into potential solutions."}, 'zh': {'title': 'Â¢ûÂä†Êé®ÁêÜËÆ°ÁÆóÔºåÊèêÂçáÊ®°ÂûãÈ≤ÅÊ£íÊÄß', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂú®Êé®ÁêÜÊ®°Âûã‰∏≠Â¢ûÂä†Êé®ÁêÜÊó∂Èó¥ËÆ°ÁÆóÂØπÂÖ∂ÊäµÂæ°ÂØπÊäóÊîªÂáªÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÈöèÁùÄÊé®ÁêÜÊó∂Èó¥ËÆ°ÁÆóÁöÑÂ¢ûÂä†ÔºåÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂæóÂà∞‰∫ÜÊèêÂçá„ÄÇÂ§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÔºåÊîªÂáªÊàêÂäüÁöÑÊ®°ÂûãÊ†∑Êú¨ÊØî‰æãÈöèÁùÄÊµãËØïÊó∂Èó¥ËÆ°ÁÆóÁöÑÂ¢ûÂä†ËÄåË∂ãËøë‰∫éÈõ∂„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåÊé®ÁêÜÊó∂Èó¥ËÆ°ÁÆóÊúâÊΩúÂäõÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂØπÊäóÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18052', 'title': 'SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2501.18052', 'abstract': "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https://github.com/cywinski/SAeUron.", 'score': 2, 'issue_id': 2011, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'd94056a77d806ada', 'authors': ['Bartosz Cywi≈Ñski', 'Kamil Deja'], 'affiliations': ['IDEAS NCBR', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2501.18052.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#security', '#architecture', '#ethics', '#training', '#benchmark'], 'emoji': 'üßπ', 'ru': {'title': '–ß–∏—Å—Ç–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: SAeUron —É–¥–∞–ª—è–µ—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'SAeUron - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (SAE) –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–æ –≤–º–µ—à–∏–≤–∞—Ç—å—Å—è –≤ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Ü–µ–ª–µ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. SAeUron –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏ –º–æ–∂–µ—Ç —É–¥–∞–ª—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.'}, 'en': {'title': 'SAeUron: Safeguarding Diffusion Models with Sparse Autoencoders', 'desc': "This paper presents SAeUron, a new method designed to improve the safety of text-to-image diffusion models by removing unwanted concepts. It utilizes sparse autoencoders (SAEs) to learn and identify specific features from the model's activations, allowing for targeted interventions. The method enables precise control over the model's outputs while maintaining its overall performance. Evaluation shows that SAeUron outperforms existing techniques in unlearning tasks and effectively reduces the risk of generating harmful content."}, 'zh': {'title': 'SAeUronÔºöÂéªÈô§‰∏çËâØÂÜÖÂÆπÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êâ©Êï£Ê®°ÂûãËôΩÁÑ∂Âº∫Â§ßÔºå‰ΩÜÂèØËÉΩ‰ºöÁîüÊàêÊúâÂÆ≥Êàñ‰∏çËâØÂÜÖÂÆπÔºåÂ∏¶Êù•‰º¶ÁêÜÂíåÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇÊúÄËøëÁöÑÊú∫Âô®ÈÅóÂøòÊñπÊ≥ïÊèê‰æõ‰∫ÜÊΩúÂú®ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜÈÄöÂ∏∏Áº∫‰πèÈÄèÊòéÊÄßÔºåÈöæ‰ª•ÁêÜËß£ÂØπÂü∫Á°ÄÊ®°ÂûãÁöÑÊõ¥Êîπ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïSAeUronÔºåÂà©Áî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâÂ≠¶‰π†ÁöÑÁâπÂæÅÊù•ÂéªÈô§ÊñáÊú¨Âà∞ÂõæÂÉèÊâ©Êï£Ê®°Âûã‰∏≠ÁöÑ‰∏çÂøÖË¶ÅÊ¶ÇÂøµ„ÄÇÈÄöËøáÂú®Â§ö‰∏™ÂéªÂô™Êó∂Èó¥Ê≠•ÁöÑÊøÄÊ¥ª‰∏äÊó†ÁõëÁù£ËÆ≠ÁªÉSAEÔºåÊàë‰ª¨ÊçïÊçâÂà∞‰∏éÁâπÂÆöÊ¶ÇÂøµÂØπÂ∫îÁöÑÁ®ÄÁñèÂíåÂèØËß£ÈáäÁâπÂæÅÔºå‰ªéËÄåÂÆûÁé∞Á≤æÁ°ÆÂπ≤È¢Ñ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18804', 'title': 'Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion', 'url': 'https://huggingface.co/papers/2501.18804', 'abstract': 'Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.', 'score': 2, 'issue_id': 2010, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '32db517ad974401b', 'authors': ['Vitor Guizilini', 'Muhammad Zubair Irshad', 'Dian Chen', 'Greg Shakhnarovich', 'Rares Ambrus'], 'affiliations': ['Toyota Research Institute (TRI)', 'Toyota Technological Institute at Chicago (TTIC)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18804.jpg', 'data': {'categories': ['#training', '#3d', '#diffusion', '#architecture', '#benchmark', '#optimization'], 'emoji': 'üé≠', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è 3D —Å—Ü–µ–Ω –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MVGD - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∫–∞—Ä—Ç –≥–ª—É–±–∏–Ω—ã —Å –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–π–∫–∞—Ä—Ç—ã –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ö–ª—é—á–µ–≤–æ–π –∞—Å–ø–µ–∫—Ç - –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∫–∞—Ä—Ç –≥–ª—É–±–∏–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–∞–µ–º—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 60 –º–∏–ª–ª–∏–æ–Ω–∞—Ö –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with Direct Pixel-Level Generation', 'desc': 'This paper presents MVGD, a new diffusion-based model for generating images and depth maps from multiple input views in 3D scene reconstruction. Unlike traditional methods that rely on intermediate 3D representations, MVGD directly produces pixel-level outputs, enhancing visual features with spatial information through raymap conditioning. The model employs multi-task learning, using task embeddings to effectively guide the generation process for both images and depth maps. Trained on a vast dataset of over 60 million samples, MVGD achieves state-of-the-art performance in novel view synthesis and depth estimation tasks.'}, 'zh': {'title': 'MVGDÔºö‰ªéÂ§öËßÜËßíÁõ¥Êé•ÁîüÊàêÂõæÂÉè‰∏éÊ∑±Â∫¶ÂõæÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫MVGDÁöÑÊâ©Êï£Âü∫Á°ÄÊû∂ÊûÑÔºåËÉΩÂ§üÁõ¥Êé•‰ªéÂ§ö‰∏™ËßÜËßíÁîüÊàêÂõæÂÉèÂíåÊ∑±Â∫¶Âõæ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂÖâÁ∫øÂõæÊù°‰ª∂ÂåñÔºåÂ¢ûÂº∫‰∫ÜËßÜËßâÁâπÂæÅÂπ∂ÂºïÂØºÁîüÊàêËøáÁ®ã„ÄÇÊàë‰ª¨ÈááÁî®Â§ö‰ªªÂä°ÁîüÊàêÊäÄÊúØÔºåÂêåÊó∂ÁîüÊàêÂõæÂÉèÂíåÊ∑±Â∫¶ÂõæÔºåÂπ∂‰ΩøÁî®ÂèØÂ≠¶‰π†ÁöÑ‰ªªÂä°ÂµåÂÖ•Êù•‰ºòÂåñÊâ©Êï£ËøáÁ®ã„ÄÇÁªèËøáÂú®Ë∂ÖËøá6000‰∏áÂ§öËßÜËßíÊ†∑Êú¨‰∏äÁöÑËÆ≠ÁªÉÔºåÊàë‰ª¨Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18965', 'title': 'The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training', 'url': 'https://huggingface.co/papers/2501.18965', 'abstract': 'We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms. Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules.', 'score': 2, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': 'a136293a2241150e', 'authors': ['Fabian Schaipp', 'Alexander H√§gele', 'Adrien Taylor', 'Umut Simsekli', 'Francis Bach'], 'affiliations': ['EPFL, Lausanne, Switzerland', 'Inria, Departement dInformatique de lEcole Normale Superieure, PSL Research University, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2501.18965.jpg', 'data': {'categories': ['#transfer_learning', '#math', '#training', '#optimization'], 'emoji': 'üìà', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —ç—Ç–∏—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ –∏–∑ —Ç–µ–æ—Ä–∏–∏ –Ω–µ–≤—ã–ø—É–∫–ª–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —ç—Ç–æ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–∏. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –ø–æ–∑–≤–æ–ª–∏–ª–æ —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ Llama —Ä–∞–∑–º–µ—Ä–æ–º 124M –∏ 210M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Optimizing Learning Rates: Bridging Theory and Practice', 'desc': 'This paper explores the relationship between learning-rate schedules in large model training and concepts from non-smooth convex optimization theory. It establishes a performance bound for a constant learning-rate schedule with a linear cooldown, highlighting the practical advantages of this approach. The authors demonstrate that the alignment between theoretical optimization and practical training can be leveraged for better learning-rate tuning. By optimizing the learning-rate schedule, they achieve significant improvements in training large Llama-type models.'}, 'zh': {'title': '‰ºòÂåñÂ≠¶‰π†ÁéáË∞ÉÂ∫¶ÔºåÊèêÂçáÂ§ßÊ®°ÂûãËÆ≠ÁªÉÊïàÊûú', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÊ®°ÂûãËÆ≠ÁªÉ‰∏≠ÁöÑÂ≠¶‰π†ÁéáË∞ÉÂ∫¶‰∏éÈùûÂÖâÊªëÂá∏‰ºòÂåñÁêÜËÆ∫‰∏≠ÁöÑÊÄßËÉΩÁïåÈôê‰πãÈó¥ÁöÑÁõ∏‰ººÊÄß„ÄÇÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™Á∫øÊÄßÂÜ∑Âç¥ÁöÑÂ∏∏Êï∞Ë∞ÉÂ∫¶ÁöÑÁïåÈôêÔºåÁâπÂà´ÊòØÂÜ∑Âç¥ÁöÑÂÆûÈôÖÂ•ΩÂ§ÑÂú®‰∫éÊ≤°ÊúâÂØπÊï∞È°πÁöÑÂΩ±Âìç„ÄÇËøõ‰∏ÄÊ≠•Âú∞ÔºåÊàë‰ª¨Â±ïÁ§∫‰∫Ü‰ºòÂåñÁêÜËÆ∫‰∏éÂÆûË∑µ‰πãÈó¥ÁöÑÁ¥ßÂØÜËÅîÁ≥ªÂèØ‰ª•Áî®‰∫éÂ≠¶‰π†ÁéáË∞É‰ºòÔºöÈÄöËøáÂª∂ÈïøË∞ÉÂ∫¶‰ª•ÁªßÁª≠ËÆ≠ÁªÉÂπ∂‰ΩøÁî®ÊúÄ‰Ω≥Â≠¶‰π†ÁéáÔºåÊàë‰ª¨Âú®ËÆ≠ÁªÉ124MÂíå210MÁöÑLlamaÁ±ªÂûãÊ®°ÂûãÊó∂ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊîπËøõ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ËøòÂ±ïÁ§∫‰∫ÜÂú®‰∏çÂêåË∞ÉÂ∫¶‰πãÈó¥ËΩ¨ÁßªÊúÄ‰Ω≥Â≠¶‰π†ÁéáÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18753', 'title': 'INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation', 'url': 'https://huggingface.co/papers/2501.18753', 'abstract': 'Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability.', 'score': 2, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '000663cf445862a1', 'authors': ['Jian Hu', 'Zixu Cheng', 'Shaogang Gong'], 'affiliations': ['Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2501.18753.jpg', 'data': {'categories': ['#dataset', '#healthcare', '#optimization', '#cv'], 'emoji': 'üîç', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –ø—Ä–æ–º–ø—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º INT (Instance-specific Negative Mining for Task-Generic Promptable Segmentation). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—É—Ç–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É–º–µ–Ω—å—à–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏—è –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ —É—Å–∏–ª–µ–Ω–∏—è –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö. INT —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤, –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–∞—Å–∫–∏. –ú–µ—Ç–æ–¥ –±—ã–ª –ø—Ä–æ–≤–µ—Ä–µ–Ω –Ω–∞ —à–µ—Å—Ç–∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –∫–∞–º—É—Ñ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å.'}, 'en': {'title': 'Enhancing Image Segmentation with Smart Prompting', 'desc': 'This paper presents a new method called Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT) to improve image segmentation using a single task description. The method addresses the challenge of Vision-Language Models (VLMs) struggling to generalize to certain image instances, which can lead to poor segmentation results. INT works by selectively reducing the impact of irrelevant information while enhancing the use of relevant prior knowledge through a process called negative mining. The effectiveness of INT is validated across six diverse datasets, showing its ability to produce accurate and robust segmentation results.'}, 'zh': {'title': 'ÂÆû‰æãÁâπÂÆöË¥üÈááÊ†∑‰ºòÂåñÂõæÂÉèÂàÜÂâ≤', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫ÂÆû‰æãÁâπÂÆöË¥üÈááÊ†∑ÔºàINTÔºâÔºåÁî®‰∫é‰ªªÂä°ÈÄöÁî®ÁöÑÂèØÊèêÁ§∫ÂõæÂÉèÂàÜÂâ≤„ÄÇINTÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØËá™ÈÄÇÂ∫îÂú∞ÂáèÂ∞ëÊó†ÂÖ≥ÁöÑÂÖàÈ™åÁü•ËØÜÁöÑÂΩ±ÂìçÔºåÂêåÊó∂Â¢ûÂä†ÊúÄÊúâÂèØËÉΩÁöÑÂÖàÈ™åÁü•ËØÜÁöÑ‰ΩøÁî®Ôºå‰ª•‰ºòÂåñÂÆû‰æãÁâπÂÆöÊèêÁ§∫ÁöÑÁîüÊàê„ÄÇËØ•ÊñπÊ≥ïÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöÂÆû‰æãÁâπÂÆöÊèêÁ§∫ÁîüÊàêÂíåËØ≠‰πâÊé©Á†ÅÁîüÊàêÔºåÁ°Æ‰øùÊØè‰∏™ÂõæÂÉèÂÆû‰æãÁöÑÂàÜÂâ≤‰∏éÊèêÁ§∫ÁöÑËØ≠‰πâÁõ∏ÂåπÈÖç„ÄÇÈÄöËøáÂú®ÂÖ≠‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÈ™åËØÅÔºåINTÂ±ïÁ§∫‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.18128', 'title': 'Unraveling the Capabilities of Language Models in News Summarization', 'url': 'https://huggingface.co/papers/2501.18128', 'abstract': "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.", 'score': 2, 'issue_id': 2000, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '1c3f3a16953a5a59', 'authors': ['Abdurrahman Odaba≈üƒ±', 'G√∂ksel Biricik'], 'affiliations': ['Department of Computer Engineering, Turkish-German University, 34820, Istanbul, Turkiye', 'Department of Computer Engineering, Yƒ±ldƒ±z Technical University, 34220, Istanbul, Turkiye'], 'pdf_title_img': 'assets/pdf/title_img/2501.18128.jpg', 'data': {'categories': ['#survey', '#multilingual', '#small_models', '#benchmark', '#transfer_learning'], 'emoji': 'üì∞', 'ru': {'title': '–ú–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –±—Ä–æ—Å–∞—é—Ç –≤—ã–∑–æ–≤ –≥–∏–≥–∞–Ω—Ç–∞–º –≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π', 'desc': '–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ 20 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –º–µ–Ω—å—à–∏–µ –º–æ–¥–µ–ª–∏, –¥–ª—è –∑–∞–¥–∞—á–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º–∞—Ö zero-shot –∏ few-shot –Ω–∞ —Ç—Ä–µ—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é –æ—Ü–µ–Ω–∫—É –∏ LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥—å–∏. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –≤–∫–ª—é—á–µ–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ —Ä–µ–∂–∏–º–µ few-shot –Ω–µ —É–ª—É—á—à–∏–ª–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π, –∞ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –¥–∞–∂–µ —É—Ö—É–¥—à–∏–ª–æ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Å–≤–æ–¥–æ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ GPT-3.5-Turbo –∏ GPT-4, –Ω–æ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Qwen1.5-7B –∏ SOLAR-10.7B-Instruct-v1.0.'}, 'en': {'title': 'Benchmarking News Summarization: Small Models Can Compete!', 'desc': 'This paper benchmarks 20 recent language models specifically for the task of news summarization, emphasizing smaller models. It evaluates their performance in zero-shot and few-shot learning scenarios across three different datasets with varying writing styles. The study reveals that providing demonstration examples in few-shot settings often does not improve, and can even degrade, the quality of summaries due to the inadequacy of reference summaries. Notably, while larger models like GPT-3.5-Turbo and GPT-4 excel, several smaller models also show competitive performance, suggesting they could be viable alternatives for summarization tasks.'}, 'zh': {'title': 'Â∞èÊ®°ÂûãÂú®Êñ∞ÈóªÊëòË¶Å‰∏≠ÁöÑÊΩúÂäõ‰∏éÊåëÊàò', 'desc': 'Êú¨Á†îÁ©∂ÂØπ20ÁßçÊúÄÊñ∞ÁöÑËØ≠Ë®ÄÊ®°ÂûãËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØïÔºåÈáçÁÇπÂÖ≥Ê≥®ËæÉÂ∞èÁöÑÊ®°ÂûãÂú®Êñ∞ÈóªÊëòË¶Å‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨Á≥ªÁªüÂú∞ÊµãËØï‰∫ÜËøô‰∫õÊ®°ÂûãÂú®‰∏çÂêåÈ£éÊ†ºÁöÑÊñ∞ÈóªÊñáÁ´†ÊëòË¶Å‰∏≠ÁöÑËÉΩÂäõÂíåÊúâÊïàÊÄßÔºå‰ΩøÁî®‰∫Ü‰∏âÁßç‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂú®Â∞ëÈáèÁ§∫‰æãÂ≠¶‰π†ÁöÑËÆæÁΩÆ‰∏≠ÔºåÊèê‰æõÁ§∫‰æãÂπ∂Êú™ÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂèçËÄåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÂØºËá¥ÁîüÊàêÊëòË¶ÅÁöÑË¥®Èáè‰∏ãÈôç„ÄÇËøô‰∏ªË¶ÅÊòØÁî±‰∫éÂèÇËÄÉÊëòË¶ÅÁöÑË¥®ÈáèËæÉÂ∑ÆÔºåÂΩ±Âìç‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞ÔºåÂêåÊó∂Êàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúÊòæÁ§∫ÔºåGPT-3.5-TurboÂíåGPT-4Âú®ÊÄßËÉΩ‰∏äË°®Áé∞‰ºòÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2404.07097', 'title': 'Fast Encoder-Based 3D from Casual Videos via Point Track Processing', 'url': 'https://huggingface.co/papers/2404.07097', 'abstract': 'This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.   Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation. TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\\%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time.', 'score': 1, 'issue_id': 1999, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 –∞–ø—Ä–µ–ª—è', 'en': 'April 10', 'zh': '4Êúà10Êó•'}, 'hash': 'a526ae197fe3a8c7', 'authors': ['Yoni Kasten', 'Wuyue Lu', 'Haggai Maron'], 'affiliations': ['NVIDIA Research', 'Simon Fraser University', 'Technion'], 'pdf_title_img': 'assets/pdf/title_img/2404.07097.jpg', 'data': {'categories': ['#3d', '#training', '#cv', '#architecture'], 'emoji': 'üé•', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TracksTo4D - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D —Å—Ç—Ä—É–∫—Ç—É—Ä –∏–∑ –≤–∏–¥–µ–æ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, –æ–±—É—á–µ–Ω–Ω—É—é –±–µ–∑ —É—á–∏—Ç–µ–ª—è –Ω–∞ 2D —Ç—Ä–µ–∫–∞—Ö —Ç–æ—á–µ–∫, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏–∑ –æ–±—ã—á–Ω—ã—Ö –≤–∏–¥–µ–æ. TracksTo4D –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –æ–±–ª–∞–∫–æ —Ç–æ—á–µ–∫ –∏ –ø–æ–ª–æ–∂–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ —Å–µ—Ç–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞—è –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∏–º–º–µ—Ç—Ä–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–≤–∏–∂–µ–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction from Casual Videos', 'desc': 'This paper introduces TracksTo4D, a novel machine learning approach for reconstructing 3D structures from casual videos. Unlike traditional methods that require extensive optimization or are limited to specific video types, TracksTo4D processes 2D point tracks directly in a single, efficient feed-forward pass. The architecture is designed to leverage the symmetries in point track data and utilizes low-rank approximations to represent movement patterns effectively. Trained unsupervised on a dataset of casual videos, TracksTo4D achieves high accuracy in reconstructing temporal point clouds and camera positions while significantly reducing processing time.'}, 'zh': {'title': 'È´òÊïàÈáçÂª∫3DÁªìÊûÑÔºåTracksTo4DÂºïÈ¢ÜÊñ∞ÊΩÆÊµÅ', 'desc': 'Êú¨ÊñáËß£ÂÜ≥‰∫Ü‰ªéÂä®ÊÄÅÂÜÖÂÆπËßÜÈ¢ëÈáçÂª∫3DÁªìÊûÑÁöÑÈïøÊúüÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÂ§ÑÁêÜÊôÆÈÄöÁõ∏Êú∫ÂΩïÂà∂ÁöÑÈöèÊÑèËßÜÈ¢ëÔºåÊàñÈúÄË¶ÅËæÉÈïøÁöÑ‰ºòÂåñÊó∂Èó¥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ≠¶‰π†ÁöÑÊñπÊ≥ïTracksTo4DÔºåÈÄöËøáÂçïÊ¨°È´òÊïàÁöÑÂâçÈ¶à‰º†ÈÄíÔºå‰ªéÈöèÊÑèËßÜÈ¢ë‰∏≠Êé®Êñ≠3DÁªìÊûÑÂíåÁõ∏Êú∫‰ΩçÁΩÆ„ÄÇËØ•ÊñπÊ≥ïÁõ¥Êé•Â§ÑÁêÜ2DÁÇπËΩ®ËøπÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ìÈó®ÁöÑÊû∂ÊûÑÔºåËÉΩÂ§üÂú®Êó†ÁõëÁù£ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åËÆ≠ÁªÉÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈáçÂª∫ÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11089', 'title': 'Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention', 'url': 'https://huggingface.co/papers/2502.11089', 'abstract': 'Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.', 'score': 55, 'issue_id': 2271, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': 'aa26756957a07b01', 'authors': ['Jingyang Yuan', 'Huazuo Gao', 'Damai Dai', 'Junyu Luo', 'Liang Zhao', 'Zhengyan Zhang', 'Zhenda Xie', 'Y. X. Wei', 'Lean Wang', 'Zhiping Xiao', 'Yuqing Wang', 'Chong Ruan', 'Ming Zhang', 'Wenfeng Liang', 'Wangding Zeng'], 'affiliations': ['DeepSeek-AI', 'Key Laboratory for Multimedia Information Processing, Peking University, PKU-Anker LLM Lab', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.11089.jpg', 'data': {'categories': ['#long_context', '#training', '#optimization', '#architecture'], 'emoji': 'üöÄ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NSA - –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –Ω–∞—Ç–∏–≤–Ω–æ –æ–±—É—á–∞–µ–º–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. NSA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏, —Å–æ—á–µ—Ç–∞—é—â—É—é —Å–∂–∞—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –≥—Ä—É–±–æ–º —É—Ä–æ–≤–Ω–µ —Å —Ç–æ—á–Ω—ã–º –≤—ã–±–æ—Ä–æ–º —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω–µ. –ê–≤—Ç–æ—Ä—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞ —Å—á–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —Å NSA —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏ —Å –ø–æ–ª–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –ø—Ä–∏ —ç—Ç–æ–º –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π 64 —Ç—ã—Å—è—á–∏ —Ç–æ–∫–µ–Ω–æ–≤.'}, 'en': {'title': 'Efficient Long-Context Modeling with Natively Trainable Sparse Attention', 'desc': 'This paper introduces NSA, a new Sparse Attention mechanism designed for efficient long-context modeling in language models. NSA combines coarse-grained token compression with fine-grained token selection to enhance both global context and local detail while reducing computational costs. The method features innovations that optimize performance on modern hardware and allow for end-to-end training, which cuts down on pretraining time without losing effectiveness. Experimental results demonstrate that NSA not only matches but often surpasses traditional Full Attention models in various tasks, proving its efficiency and capability in handling long sequences.'}, 'zh': {'title': 'È´òÊïàÈïø‰∏ä‰∏ãÊñáÂª∫Ê®°ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Èïø‰∏ä‰∏ãÊñáÂª∫Ê®°ÂØπ‰∏ã‰∏Ä‰ª£ËØ≠Ë®ÄÊ®°ÂûãËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÊ†áÂáÜÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÈ´òËÆ°ÁÆóÊàêÊú¨Â∏¶Êù•‰∫ÜÊòæËëóÁöÑÊåëÊàò„ÄÇÁ®ÄÁñèÊ≥®ÊÑèÂäõÊèê‰æõ‰∫Ü‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊñπÂêëÔºåÂèØ‰ª•Âú®‰øùÊåÅÊ®°ÂûãËÉΩÂäõÁöÑÂêåÊó∂ÊèêÈ´òÊïàÁéá„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜNSAÔºå‰∏ÄÁßçÂéüÁîüÂèØËÆ≠ÁªÉÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁªìÂêà‰∫ÜÁÆóÊ≥ïÂàõÊñ∞ÂíåÁ°¨‰ª∂‰ºòÂåñÔºå‰ª•ÂÆûÁé∞È´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáÂª∫Ê®°„ÄÇNSAÈááÁî®Âä®ÊÄÅÂàÜÂ±ÇÁ®ÄÁñèÁ≠ñÁï•ÔºåÁªìÂêàÁ≤óÁ≤íÂ∫¶ÁöÑ‰ª§ÁâåÂéãÁº©ÂíåÁªÜÁ≤íÂ∫¶ÁöÑ‰ª§ÁâåÈÄâÊã©ÔºåÊó¢‰øùÁïô‰∫ÜÂÖ®Â±Ä‰∏ä‰∏ãÊñáÊÑèËØÜÔºåÂèà‰øùÊåÅ‰∫ÜÂ±ÄÈÉ®Á≤æÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12152', 'title': 'Learning Getting-Up Policies for Real-World Humanoid Robots', 'url': 'https://huggingface.co/papers/2502.12152', 'abstract': 'Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of humanoid locomotion learning, the getting-up task involves complex contact patterns, which necessitates accurately modeling the collision geometry and sparser rewards. We address these challenges through a two-phase approach that follows a curriculum. The first stage focuses on discovering a good getting-up trajectory under minimal constraints on smoothness or speed / torque limits. The second stage then refines the discovered motions into deployable (i.e. smooth and slow) motions that are robust to variations in initial configuration and terrains. We find these innovations enable a real-world G1 humanoid robot to get up from two main situations that we considered: a) lying face up and b) lying face down, both tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass and snowfield). To the best of our knowledge, this is the first successful demonstration of learned getting-up policies for human-sized humanoid robots in the real world. Project page: https://humanoid-getup.github.io/', 'score': 30, 'issue_id': 2266, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': 'c87382b187d7b745', 'authors': ['Xialin He', 'Runpei Dong', 'Zixuan Chen', 'Saurabh Gupta'], 'affiliations': ['Simon Fraser University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.12152.jpg', 'data': {'categories': ['#training', '#games', '#robotics', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': '–†–æ–±–æ—Ç—ã —É—á–∞—Ç—Å—è –≤—Å—Ç–∞–≤–∞—Ç—å: –ø—Ä–æ—Ä—ã–≤ –≤ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –≥—É–º–∞–Ω–æ–∏–¥–∞–º–∏', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Å–∏—Å—Ç–µ–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–æ–≤, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã–º —Ä–æ–±–æ—Ç–∞–º –≤—Å—Ç–∞–≤–∞—Ç—å –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª–æ–∂–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—É—Ä—Ä–∏–∫—É–ª—É–º–∞: —Å–Ω–∞—á–∞–ª–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –ø–æ–¥—ä–µ–º–∞, –∞ –∑–∞—Ç–µ–º –æ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç—Å—è –¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏ –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏. –°–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –≥—É–º–∞–Ω–æ–∏–¥–Ω–æ–º —Ä–æ–±–æ—Ç–µ G1 –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –≤–∫–ª—é—á–∞—è —Å–∫–æ–ª—å–∑–∫–∏–µ –∏ –Ω–∞–∫–ª–æ–Ω–Ω—ã–µ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏. –≠—Ç–æ –ø–µ—Ä–≤–∞—è —É—Å–ø–µ—à–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–¥—ä–µ–º–∞ –¥–ª—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ.'}, 'en': {'title': 'Learning to Get Up: Humanoid Robots Rise to the Challenge!', 'desc': 'This paper presents a novel learning framework for enabling humanoid robots to recover from falls by getting up from various positions and terrains. The approach consists of a two-phase curriculum where the first phase focuses on discovering effective getting-up trajectories with minimal constraints, while the second phase refines these motions to ensure they are smooth and robust. The framework addresses the complexities of contact patterns and collision geometry, which are critical for the getting-up task. The results demonstrate successful real-world applications, allowing a humanoid robot to rise from both face-up and face-down positions on challenging surfaces.'}, 'zh': {'title': '‰∫∫ÂΩ¢Êú∫Âô®‰∫∫Ëá™‰∏ªËµ∑Ë∫´ÁöÑÂàõÊñ∞Â≠¶‰π†Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰Ωø‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ËÉΩÂ§ü‰ªé‰∏çÂêåÁöÑÂßøÂäøÂíåÂú∞ÂΩ¢‰∏≠Ëá™Ë°åÁ´ôËµ∑„ÄÇÁî±‰∫é‰∫∫ÂΩ¢Êú∫Âô®‰∫∫Âú®Ë∑åÂÄíÂêéÂèØËÉΩÂ§Ñ‰∫éÂ§öÁßçÂ§çÊùÇÁöÑÂßøÂäøÔºåËÆæËÆ°ÊéßÂà∂Âô®ÈùûÂ∏∏Âõ∞Èöæ„ÄÇÊàë‰ª¨ÈááÁî®‰∫Ü‰∏§Èò∂ÊÆµÁöÑÊñπÊ≥ïÔºåÁ¨¨‰∏ÄÈò∂ÊÆµÂèëÁé∞ÈÄÇÂêàÁöÑËµ∑Ë∫´ËΩ®ËøπÔºåÁ¨¨‰∫åÈò∂ÊÆµÂàôÂ∞ÜËøô‰∫õËΩ®Ëøπ‰ºòÂåñ‰∏∫Âπ≥Êªë‰∏îÁ®≥ÂÅ•ÁöÑÂä®‰Ωú„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ï‰ΩøÂæó‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ËÉΩÂ§üÂú®Â§öÁßçÁúüÂÆûÁéØÂ¢É‰∏≠ÊàêÂäüÁ´ôËµ∑„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12115', 'title': 'SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?', 'url': 'https://huggingface.co/papers/2502.12115', 'abstract': 'We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \\1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to \\$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.', 'score': 23, 'issue_id': 2266, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '0b2638455d4393b0', 'authors': ['Samuel Miserendino', 'Michele Wang', 'Tejal Patwardhan', 'Johannes Heidecke'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.12115.jpg', 'data': {'categories': ['#dataset', '#science', '#benchmark', '#open_source'], 'emoji': 'üíª', 'ru': {'title': 'SWE-Lancer: –ò–∑–º–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ò–ò –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û', 'desc': 'SWE-Lancer - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª–µ–µ 1400 —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã Upwork –æ–±—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é 1 –º–∏–ª–ª–∏–æ–Ω –¥–æ–ª–ª–∞—Ä–æ–≤. –ó–∞–¥–∞—á–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ (–æ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –¥–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∫—Ä—É–ø–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π) –∏ —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —Ä–µ—à–µ–Ω–∏—è–º–∏ –æ–ø—ã—Ç–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤.'}, 'en': {'title': "Unlocking AI's Potential in Freelance Software Engineering", 'desc': "SWE-Lancer is a new benchmark that includes over 1,400 freelance software engineering tasks sourced from Upwork, with a total value of $1 million. It features both independent tasks, like bug fixes and large feature implementations, and managerial tasks that require decision-making between different technical proposals. The tasks are rigorously evaluated, with independent tasks tested by experienced engineers and managerial decisions compared to those made by original engineering managers. Despite the evaluation, current advanced models struggle to complete most tasks, highlighting the need for further research in AI's economic implications."}, 'zh': {'title': 'SWE-LancerÔºöÊé®Âä®AIÊ®°ÂûãÁªèÊµéÂΩ±ÂìçÁ†îÁ©∂ÁöÑÂü∫ÂáÜ', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜSWE-LancerÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´1400Â§ö‰∏™Êù•Ëá™UpworkÁöÑËá™Áî±ËΩØ‰ª∂Â∑•Á®ã‰ªªÂä°ÁöÑÂü∫ÂáÜÔºå‰ªªÂä°ÊÄª‰ª∑ÂÄºËææÂà∞100‰∏áÁæéÂÖÉ„ÄÇSWE-LancerÂåÖÊã¨Áã¨Á´ãÁöÑÂ∑•Á®ã‰ªªÂä°ÔºåÂ¶Ç50‰∏™bug‰øÆÂ§çÂíå‰ª∑ÂÄº32000ÁæéÂÖÉÁöÑÂäüËÉΩÂÆûÁé∞Ôºå‰ª•ÂèäÁÆ°ÁêÜ‰ªªÂä°ÔºåÊ®°ÂûãÈúÄË¶ÅÂú®ÊäÄÊúØÂÆûÊñΩÊèêÊ°à‰∏≠ËøõË°åÈÄâÊã©„ÄÇÁã¨Á´ã‰ªªÂä°ÈÄöËøáÁªèÈ™å‰∏∞ÂØåÁöÑËΩØ‰ª∂Â∑•Á®ãÂ∏àËøõË°å‰∏âÈáçÈ™åËØÅÁöÑÁ´ØÂà∞Á´ØÊµãËØïÊù•ËØÑÂàÜÔºåËÄåÁÆ°ÁêÜÂÜ≥Á≠ñÂàô‰∏éÂéüÈõá‰Ω£ÁöÑÂ∑•Á®ãÁªèÁêÜÁöÑÈÄâÊã©ËøõË°åÊØîËæÉ„ÄÇÊàë‰ª¨ËØÑ‰º∞‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞ÔºåÂèëÁé∞ÂâçÊ≤øÊ®°Âûã‰ªçÁÑ∂Êó†Ê≥ïËß£ÂÜ≥Â§ßÂ§öÊï∞‰ªªÂä°ÔºåSWE-LancerÁöÑÂºÄÊ∫êÂ∞Ü‰øÉËøõÊú™Êù•ÁöÑÁ†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11190', 'title': 'ReLearn: Unlearning via Learning for Large Language Models', 'url': 'https://huggingface.co/papers/2502.11190', 'abstract': 'Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.', 'score': 17, 'issue_id': 2266, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': '83a57c7c971f5060', 'authors': ['Haoming Xu', 'Ningyuan Zhao', 'Liming Yang', 'Sendong Zhao', 'Shumin Deng', 'Mengru Wang', 'Bryan Hooi', 'Nay Oo', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Harbin Institute of Technology', 'National University of Singapore', 'Tsinghua University', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11190.jpg', 'data': {'categories': ['#data', '#training', '#hallucinations', '#open_source', '#benchmark', '#optimization'], 'emoji': 'üß†', 'ru': {'title': 'ReLearn: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ReLearn –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, ReLearn –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –Ω–∞—Ä—É—à–µ–Ω–∏—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏: KFR, KRR –∏ LS. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ReLearn —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª—è–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'ReLearn: Effective Unlearning Without Sacrificing Coherence', 'desc': "This paper introduces ReLearn, a novel approach for unlearning in large language models that avoids the pitfalls of reverse optimization, which can harm the model's ability to predict subsequent tokens. ReLearn employs a data augmentation and fine-tuning pipeline that effectively targets specific knowledge for removal while maintaining the fluency and relevance of generated text. The authors propose new evaluation metrics, including Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR), to assess the balance between forgetting and retaining knowledge, alongside a Linguistic Score (LS) for output quality. Experimental results demonstrate that ReLearn achieves its unlearning goals without sacrificing the coherence of the model's text generation capabilities."}, 'zh': {'title': 'ReLearnÔºöÈ´òÊïàÂéªÂ≠¶‰π†‰∏éËØ≠Ë®ÄÁîüÊàêÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'ÂΩìÂâçÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂéªÂ≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂèçÂêë‰ºòÂåñÊù•Èôç‰ΩéÁõÆÊ†áËØçÁöÑÊ¶ÇÁéá„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ï‰ºöÂπ≤Êâ∞ÂêéÁª≠ËØçÁöÑÈ¢ÑÊµãÔºåÂØºËá¥Ê®°ÂûãÊÄßËÉΩÂíåËØ≠Ë®ÄËøûË¥ØÊÄß‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâÁöÑËØÑ‰º∞ÊåáÊ†áËøá‰∫éÂº∫Ë∞É‰∏ä‰∏ãÊñáÈÅóÂøòÔºåËÄåÂØπÂìçÂ∫îÁöÑÊµÅÁïÖÊÄßÂíåÁõ∏ÂÖ≥ÊÄßËØÑ‰º∞‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜReLearnÔºå‰∏Ä‰∏™ÊúâÊïàÁöÑÂéªÂ≠¶‰π†ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÂíåÂæÆË∞ÉÊµÅÁ®ãÔºåÂπ∂ÂºïÂÖ•‰∫ÜÂÖ®Èù¢ÁöÑËØÑ‰º∞Ê°ÜÊû∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09061', 'title': 'CRANE: Reasoning with constrained LLM generation', 'url': 'https://huggingface.co/papers/2502.09061', 'abstract': 'Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.', 'score': 16, 'issue_id': 2264, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '4a44947deeb14cf4', 'authors': ['Debangshu Banerjee', 'Tarun Suresh', 'Shubham Ugare', 'Sasa Misailovic', 'Gagandeep Singh'], 'affiliations': ['Department of Computer Science, University of Illinois Urbana-Champaign, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.09061.jpg', 'data': {'categories': ['#dataset', '#optimization', '#benchmark', '#reasoning', '#training', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É —Å—Ç—Ä–æ–≥–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö LLM –º–æ–∂–µ—Ç —Å–Ω–∏–∂–∞—Ç—å –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –û–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º CRANE, –∫–æ—Ç–æ—Ä—ã–π –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–µ–∂–¥—É –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å—é –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –≥–∏–±–∫–æ—Å—Ç—å—é –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CRANE –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∏ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Å–∏–º–≤–æ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Balancing Correctness and Reasoning in LLM Outputs with CRANE', 'desc': "This paper discusses the challenges of generating outputs from large language models (LLMs) that are both correct in form and meaning, especially in tasks like code generation and symbolic reasoning. It explains that overly strict constraints on the grammar can hinder the model's reasoning abilities. The authors propose a new approach called CRANE, which enhances the output grammar with additional rules to maintain reasoning capabilities while ensuring syntactic and semantic correctness. Their experiments show that CRANE outperforms existing methods, achieving significant accuracy improvements on difficult reasoning tasks."}, 'zh': {'title': 'Âπ≥Ë°°Êé®ÁêÜËÉΩÂäõ‰∏éÁîüÊàêÊ≠£Á°ÆÊÄßÁöÑÂàõÊñ∞Ëß£Á†ÅÁÆóÊ≥ï', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂú®ÁîüÊàê‰ª£Á†ÅÂíåÁ¨¶Âè∑Êï∞Â≠¶Êé®ÁêÜÁ≠â‰ªªÂä°‰∏≠ÔºåÁ°Æ‰øùÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËæìÂá∫ÁöÑËØ≠Ê≥ïÂíåËØ≠‰πâÊ≠£Á°ÆÊÄß„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåËøá‰∫é‰∏•Ê†ºÁöÑËØ≠Ê≥ïÁ∫¶Êùü‰ºöÈôç‰ΩéÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËß£Á†ÅÁÆóÊ≥ïCRANEÔºåÈÄöËøáÂ¢ûÂä†Á≤æÂøÉËÆæËÆ°ÁöÑÈ¢ùÂ§ñËßÑÂàôÔºåÊó¢ËÉΩ‰øùÊåÅËæìÂá∫ÁöÑÊ≠£Á°ÆÊÄßÔºåÂèàËÉΩÂ¢ûÂº∫Êé®ÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCRANEÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑËß£Á†ÅÁ≠ñÁï•ÔºåÊèêÂçá‰∫ÜÁ¨¶Âè∑Êé®ÁêÜ‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12148', 'title': 'HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.12148', 'abstract': 'The remarkable success of the autoregressive paradigm has made significant advancement in Multimodal Large Language Models (MLLMs), with powerful models like Show-o, Transfusion and Emu3 achieving notable progress in unified image understanding and generation. For the first time, we uncover a common phenomenon: the understanding capabilities of MLLMs are typically stronger than their generative capabilities, with a significant gap between the two. Building on this insight, we propose HermesFlow, a simple yet general framework designed to seamlessly bridge the gap between understanding and generation in MLLMs. Specifically, we take the homologous data as input to curate homologous preference data of both understanding and generation. Through Pair-DPO and self-play iterative optimization, HermesFlow effectively aligns multimodal understanding and generation using homologous preference data. Extensive experiments demonstrate the significant superiority of our approach over prior methods, particularly in narrowing the gap between multimodal understanding and generation. These findings highlight the potential of HermesFlow as a general alignment framework for next-generation multimodal foundation models. Code: https://github.com/Gen-Verse/HermesFlow', 'score': 14, 'issue_id': 2265, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '09e80125d90fd3df', 'authors': ['Ling Yang', 'Xinchen Zhang', 'Ye Tian', 'Chenming Shang', 'Minghao Xu', 'Wentao Zhang', 'Bin Cui'], 'affiliations': ['Mila - Quebec AI Institute', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12148.jpg', 'data': {'categories': ['#training', '#multimodal', '#dataset', '#alignment'], 'emoji': 'üåâ', 'ru': {'title': 'HermesFlow: –º–æ—Å—Ç –º–µ–∂–¥—É –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ HermesFlow –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ MLLM –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –æ–±—ã—á–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∏—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏. HermesFlow –ø—Ä–∏–∑–≤–∞–Ω —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —ç—Ç–æ—Ç —Ä–∞–∑—Ä—ã–≤, –∏—Å–ø–æ–ª—å–∑—É—è –≥–æ–º–æ–ª–æ–≥–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–µ—Ç–æ–¥—ã Pair-DPO –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ HermesFlow –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤ MLLM.'}, 'en': {'title': 'Bridging the Gap: Enhancing Understanding and Generation in MLLMs with HermesFlow', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) and identifies a key issue: these models often understand information better than they can generate it. The authors introduce HermesFlow, a new framework that aims to improve the balance between understanding and generation in MLLMs. By using homologous data to create preference data for both tasks, HermesFlow employs Pair-DPO and self-play optimization to align these capabilities more effectively. Experimental results show that HermesFlow significantly reduces the performance gap between understanding and generation, suggesting its potential as a foundational model for future multimodal applications.'}, 'zh': {'title': 'HermesFlowÔºöÁº©Â∞èÁêÜËß£‰∏éÁîüÊàêÁöÑÂ∑ÆË∑ù', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜËá™ÂõûÂΩíËåÉÂºèÂú®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ‰∏≠ÁöÑÊàêÂäüÔºåÁâπÂà´ÊòØÂÉèShow-o„ÄÅTransfusionÂíåEmu3ËøôÊ†∑ÁöÑÊ®°ÂûãÂú®ÂõæÂÉèÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢ÁöÑËøõÂ±ï„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåMLLMsÁöÑÁêÜËß£ËÉΩÂäõÈÄöÂ∏∏Âº∫‰∫éÁîüÊàêËÉΩÂäõÔºå‰∏§ËÄÖ‰πãÈó¥Â≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåËÆ∫ÊñáÊèêÂá∫‰∫ÜHermesFlowÊ°ÜÊû∂ÔºåÈÄöËøá‰ΩøÁî®ÂêåÊ∫êÊï∞ÊçÆÊù•‰ºòÂåñÁêÜËß£ÂíåÁîüÊàê‰πãÈó¥ÁöÑÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHermesFlowÂú®Áº©Â∞èÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàê‰πãÈó¥ÁöÑÂ∑ÆË∑ùÊñπÈù¢‰ºò‰∫é‰πãÂâçÁöÑÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂‰Ωú‰∏∫‰∏ã‰∏Ä‰ª£Â§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÂØπÈΩêÊ°ÜÊû∂ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08745', 'title': 'IHEval: Evaluating Language Models on Following the Instruction Hierarchy', 'url': 'https://huggingface.co/papers/2502.08745', 'abstract': "The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models' ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated models experience a sharp performance decline when facing conflicting instructions, compared to their original instruction-following performance. Moreover, the most competitive open-source model only achieves 48% accuracy in resolving such conflicts. Our results underscore the need for targeted optimization in the future development of LMs.", 'score': 13, 'issue_id': 2279, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': 'b61f4ac9281295ed', 'authors': ['Zhihan Zhang', 'Shiyang Li', 'Zixuan Zhang', 'Xin Liu', 'Haoming Jiang', 'Xianfeng Tang', 'Yifan Gao', 'Zheng Li', 'Haodong Wang', 'Zhaoxuan Tan', 'Yichuan Li', 'Qingyu Yin', 'Bing Yin', 'Meng Jiang'], 'affiliations': ['Amazon', 'University of Notre Dame', 'Worcester Polytechnic Institute'], 'pdf_title_img': 'assets/pdf/title_img/2502.08745.jpg', 'data': {'categories': ['#optimization', '#open_source', '#multimodal', '#alignment', '#benchmark'], 'emoji': 'üé≠', 'ru': {'title': '–ò–µ—Ä–∞—Ä—Ö–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π: –∞—Ö–∏–ª–ª–µ—Å–æ–≤–∞ –ø—è—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ IHEval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 3,538 –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–µ–≤—è—Ç–∏ –∑–∞–¥–∞—á–∞—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Å–ª—É—á–∞–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤. –û—Ü–µ–Ω–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –Ø–ú –ø–æ–∫–∞–∑–∞–ª–∞ –∏—Ö —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ —Ä–µ–∑–∫–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –¥–æ—Å—Ç–∏–≥–ª–∞ –ª–∏—à—å 48% —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ —Ç–∞–∫–∏—Ö –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤.'}, 'en': {'title': 'Enhancing Language Models: Prioritizing Instructions for Better Performance', 'desc': 'This paper discusses the importance of instruction hierarchy in language models (LMs), which helps prioritize system messages, user messages, and conversation history. The authors introduce IHEval, a new benchmark with 3,538 examples across nine tasks to evaluate how well LMs follow these instruction priorities. Their findings reveal that popular LMs struggle significantly when faced with conflicting instructions, showing a notable drop in performance. The results indicate a critical need for improvements in LMs to better handle instruction hierarchies and conflicts in the future.'}, 'zh': {'title': 'ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÊåá‰ª§ÁêÜËß£ËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊåá‰ª§Â±ÇÁ∫ßÁöÑÈáçË¶ÅÊÄßÔºåÊåá‰ª§Â±ÇÁ∫ß‰ªéÁ≥ªÁªüÊ∂àÊÅØÂà∞Áî®Êà∑Ê∂àÊÅØ„ÄÅÂØπËØùÂéÜÂè≤ÂíåÂ∑•ÂÖ∑ËæìÂá∫Âª∫Á´ã‰∫Ü‰ºòÂÖàÈ°∫Â∫è„ÄÇËøô‰∏Ä‰∏ªÈ¢òÂú®ËØ≠Ë®ÄÊ®°ÂûãÔºàLMsÔºâ‰∏≠ÂèóÂà∞ÁöÑÂÖ≥Ê≥®ÊúâÈôêÔºåÁº∫‰πèÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊù•ËØÑ‰º∞Ê®°ÂûãÈÅµÂæ™Êåá‰ª§Â±ÇÁ∫ßÁöÑËÉΩÂäõ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜIHEvalÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂü∫ÂáÜÔºåÂåÖÂê´3,538‰∏™Á§∫‰æãÔºåÊ∂µÁõñ‰∫Ü‰∏çÂêå‰ºòÂÖàÁ∫ßÊåá‰ª§‰∏ÄËá¥ÊàñÂÜ≤Á™ÅÁöÑ‰πù‰∏™‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåÊµÅË°åÁöÑËØ≠Ë®ÄÊ®°ÂûãÂú®ËØÜÂà´Êåá‰ª§‰ºòÂÖàÁ∫ßÊñπÈù¢Â≠òÂú®Âõ∞ÈöæÔºåÂ∞§ÂÖ∂Âú®Èù¢ÂØπÂÜ≤Á™ÅÊåá‰ª§Êó∂ÔºåÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11196', 'title': 'How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training', 'url': 'https://huggingface.co/papers/2502.11196', 'abstract': 'Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.', 'score': 12, 'issue_id': 2266, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': 'd97eafe0888b9da4', 'authors': ['Yixin Ou', 'Yunzhi Yao', 'Ningyu Zhang', 'Hui Jin', 'Jiacheng Sun', 'Shumin Deng', 'Zhenguo Li', 'Huajun Chen'], 'affiliations': ['Huawei Noahs Ark Lab', 'National University of Singapore, NUS-NCS Joint Lab, Singapore', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11196.jpg', 'data': {'categories': ['#data', '#training', '#architecture', '#transfer_learning', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: —ç–≤–æ–ª—é—Ü–∏—è —Ü–µ–ø–µ–π –∑–Ω–∞–Ω–∏–π –≤ LLM', 'desc': "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ —É—Å–≤–æ–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —ç–≤–æ–ª—é—Ü–∏—é '—Ü–µ–ø–µ–π –∑–Ω–∞–Ω–∏–π' - –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–¥–≥—Ä–∞—Ñ–æ–≤, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —É—Å–≤–æ–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∏—Ö —Å–≤—è–∑–∏ —Å —É–∂–µ –∏–º–µ—é—â–∏–º–∏—Å—è, –∞ —ç–≤–æ–ª—é—Ü–∏—è —Ü–µ–ø–µ–π –∑–Ω–∞–Ω–∏–π –ø—Ä–æ—Ö–æ–¥–∏—Ç —Ñ–∞–∑—ã —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–≥—É—Ç –ø–æ–º–æ—á—å —É–ª—É—á—à–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π."}, 'en': {'title': 'Understanding Knowledge Integration in Large Language Models', 'desc': 'This paper explores how Large Language Models (LLMs) learn and store new knowledge within their neural networks. It introduces the concept of knowledge circuit evolution, which refers to the computational pathways that help LLMs process and retain information. The study finds that new knowledge is better integrated when it relates to what the model already knows, and that the process of knowledge circuit evolution shifts from forming new connections to optimizing existing ones. Additionally, the research reveals that this evolution tends to move from deeper to shallower layers of the model, offering insights that could improve continual pre-training methods for LLMs.'}, 'zh': {'title': 'ÁêÜËß£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁü•ËØÜÂÜÖÂåñ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Áü•ËØÜÂØÜÈõÜÂûã‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨Âú®ÁêÜËß£Â¶Ç‰ΩïÂÜÖÂåñÊñ∞Áü•ËØÜÊñπÈù¢Â≠òÂú®ÈáçË¶ÅÁº∫Âè£„ÄÇÊú¨ÊñáÈÄöËøáÁü•ËØÜÁîµË∑ØÊºîÂåñÁöÑËßÜËßíÔºåËØÜÂà´Âá∫‰øÉËøõÁü•ËØÜÂ≠òÂÇ®ÂíåÂ§ÑÁêÜÁöÑËÆ°ÁÆóÂ≠êÂõæ„ÄÇÊàë‰ª¨ÁöÑÁ≥ªÁªüÂàÜÊûêË°®ÊòéÔºåÊñ∞Áü•ËØÜÁöÑËé∑ÂèñÂèóÂ∑≤ÊúâÁü•ËØÜÁõ∏ÂÖ≥ÊÄßÁöÑÂΩ±ÂìçÔºåÁü•ËØÜÁîµË∑ØÁöÑÊºîÂåñÁªèÂéÜ‰ªéÂΩ¢ÊàêÂà∞‰ºòÂåñÁöÑÊòéÊòæÈò∂ÊÆµËΩ¨ÂèòÔºåÂπ∂‰∏îÊºîÂåñÊ®°ÂºèÂëàÁé∞Áî±Ê∑±Âà∞ÊµÖÁöÑÁâπÂæÅ„ÄÇËøô‰∫õÂèëÁé∞‰∏ç‰ªÖÊé®Âä®‰∫ÜÊàë‰ª¨ÂØπLLMs‰∏≠Êñ∞Áü•ËØÜËé∑ÂèñÊú∫Âà∂ÁöÑÁêÜËÆ∫ÁêÜËß£ÔºåËøòÊúâÂä©‰∫éÊîπËøõÊåÅÁª≠È¢ÑËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10458', 'title': 'I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models', 'url': 'https://huggingface.co/papers/2502.10458', 'abstract': 'This paper presents ThinkDiff, a novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on pixel-level reconstruction rather than in-context reasoning, and are constrained by the complexity and limited availability of reasoning-based datasets. ThinkDiff addresses these challenges by leveraging vision-language training as a proxy task, aligning VLMs with the decoder of an encoder-decoder large language model (LLM) instead of a diffusion decoder. This proxy task builds on the observation that the LLM decoder shares the same input feature space with diffusion decoders that use the corresponding LLM encoder for prompt embedding. As a result, aligning VLMs with diffusion decoders can be simplified through alignment with the LLM decoder. Without complex training and datasets, ThinkDiff effectively unleashes understanding, reasoning, and composing capabilities in diffusion models. Experiments demonstrate that ThinkDiff significantly improves accuracy from 19.2% to 46.3% on the challenging CoBSAT benchmark for multimodal in-context reasoning generation, with only 5 hours of training on 4 A100 GPUs. Additionally, ThinkDiff demonstrates exceptional performance in composing multiple images and texts into logically coherent images. Project page: https://mizhenxing.github.io/ThinkDiff.', 'score': 11, 'issue_id': 2270, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': 'bd036baca649b696', 'authors': ['Zhenxing Mi', 'Kuan-Chieh Wang', 'Guocheng Qian', 'Hanrong Ye', 'Runtao Liu', 'Sergey Tulyakov', 'Kfir Aberman', 'Dan Xu'], 'affiliations': ['Department of Computer Science and Engineering (CSE), The Hong Kong University of Science and Technology (HKUST)', 'Snap Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2502.10458.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#diffusion', '#benchmark', '#alignment'], 'emoji': 'üß†', 'ru': {'title': 'ThinkDiff: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'ThinkDiff - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–¥–µ–ª—è–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞ (VLM). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∑—Ä–µ–Ω–∏—é-—è–∑—ã–∫—É –∫–∞–∫ –ø—Ä–æ–∫—Å–∏-–∑–∞–¥–∞—á—É, –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—è VLM —Å –¥–µ–∫–æ–¥–µ—Ä–æ–º —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM) –≤–º–µ—Å—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–µ—Ä–∞. ThinkDiff —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –±–µ–∑ —Å–ª–æ–∂–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å 19.2% –¥–æ 46.3% –Ω–∞ —Å–ª–æ–∂–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ CoBSAT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.'}, 'en': {'title': 'Empowering Diffusion Models with Multimodal Reasoning', 'desc': 'ThinkDiff is a new approach that enhances text-to-image diffusion models by integrating vision-language models (VLMs) for better understanding and reasoning. Unlike previous methods that focused on pixel-level accuracy, ThinkDiff emphasizes in-context reasoning, overcoming challenges related to complex datasets. It aligns VLMs with the decoder of a large language model (LLM), simplifying the training process and improving performance. Experiments show that ThinkDiff significantly boosts accuracy on multimodal reasoning tasks and excels in creating coherent images from multiple inputs.'}, 'zh': {'title': 'ThinkDiffÔºöÊèêÂçáÊñáÊú¨Âà∞ÂõæÂÉèÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫ÜThinkDiffÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂØπÈΩêËåÉÂºèÔºåÊó®Âú®ÈÄöËøáÊï¥ÂêàËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑ‰ºòÂäøÔºåÂ¢ûÂº∫ÊñáÊú¨Âà∞ÂõæÂÉèÊâ©Êï£Ê®°ÂûãÁöÑÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÊâ©Êï£ÂæÆË∞ÉÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®ÂÉèÁ¥†Á∫ßÈáçÂª∫ÔºåËÄåÂøΩËßÜ‰∫Ü‰∏ä‰∏ãÊñáÊé®ÁêÜÔºåÂπ∂ÂèóÂà∞Êé®ÁêÜÂü∫Á°ÄÊï∞ÊçÆÈõÜÂ§çÊùÇÊÄßÂíåÊúâÈôêÊÄßÁöÑÈôêÂà∂„ÄÇThinkDiffÈÄöËøáÂ∞ÜËßÜËßâ-ËØ≠Ë®ÄËÆ≠ÁªÉ‰Ωú‰∏∫‰ª£ÁêÜ‰ªªÂä°ÔºåËß£ÂÜ≥‰∫ÜËøô‰∫õÊåëÊàòÔºåÂ∞ÜVLM‰∏éÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËß£Á†ÅÂô®ÂØπÈΩêÔºåËÄå‰∏çÊòØÊâ©Êï£Ëß£Á†ÅÂô®„ÄÇÂÆûÈ™åË°®ÊòéÔºåThinkDiffÂú®Â§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÊé®ÁêÜÁîüÊàêÁöÑCoBSATÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂáÜÁ°ÆÁéá‰ªé19.2%ÊòæËëóÊèêÈ´òÂà∞46.3%Ôºå‰ªÖÈúÄÂú®4‰∏™A100 GPU‰∏äËÆ≠ÁªÉ5Â∞èÊó∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11167', 'title': 'SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors', 'url': 'https://huggingface.co/papers/2502.11167', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of a program without actually running it. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple open-source and proprietary LLMs on SURGE and conduct a scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at https://github.com/Imbernoulli/SURGE.', 'score': 11, 'issue_id': 2266, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': '99e183fd31de3be0', 'authors': ['Bohan Lyu', 'Siqiao Huang', 'Zichen Liang'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua', 'Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua'], 'pdf_title_img': 'assets/pdf/title_img/2502.11167.jpg', 'data': {'categories': ['#training', '#plp', '#dataset', '#agi', '#open_source', '#benchmark', '#optimization'], 'emoji': 'üß†', 'ru': {'title': 'LLM –∫–∞–∫ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–∏ –∫–æ–¥–∞: –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ SURGE - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –±–µ–∑ –µ–≥–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –≤–æ—Å–µ–º—å –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤, –≤–∫–ª—é—á–∞—è –∑–∞–¥–∞—á–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∞–Ω–∞–ª–∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –∏ –Ω–∞—É—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è. –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –Ω–∞ SURGE –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö, –Ω–æ –∏–º–µ—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã—Ö –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–∞–µ—Ç —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã—Ö –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π –∫–æ–¥–∞.'}, 'en': {'title': 'Exploring LLMs as Surrogate Code Executors with SURGE', 'desc': 'This paper explores the potential of large language models (LLMs) to act as surrogate code executors, which means predicting the output of code without actually running it. The authors introduce a benchmark called SURGE, which tests LLMs on various programming tasks, including multi-language support and analysis of complex algorithms. They evaluate different LLMs to see how well they can predict code execution results and identify common errors in their predictions. The results show that while LLMs can succeed in some scenarios, they still have significant limitations in general-purpose code execution prediction.'}, 'zh': {'title': 'Êé¢Á¥¢Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫‰ª£Á†ÅÊâßË°åÂô®ÁöÑÊΩúÂäõ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨ËÉΩÂê¶‰Ωú‰∏∫ÈÄöÁî®ÁöÑÊõø‰ª£‰ª£Á†ÅÊâßË°åÂô®Êù•È¢ÑÊµãÁ®ãÂ∫èÁöÑËæìÂá∫ÂíåË°å‰∏∫‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÈóÆÈ¢ò„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSURGEÔºå‰∏Ä‰∏™Ê∂µÁõñÂÖ´‰∏™ÂÖ≥ÈîÆÊñπÈù¢ÁöÑÁªºÂêàÂü∫ÂáÜÔºåÂåÖÊã¨Â§öËØ≠Ë®ÄÁºñÁ®ã‰ªªÂä°ÂíåÈ´òÊàêÊú¨ÁßëÂ≠¶ËÆ°ÁÆóÁ≠â„ÄÇÊàë‰ª¨ÂØπÂ§öÁßçÂºÄÊ∫êÂíå‰∏ìÊúâÁöÑLLMsËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂπ∂Á†îÁ©∂‰∫ÜÊ®°ÂûãËßÑÊ®°ÂíåËÆ≠ÁªÉÊï∞ÊçÆËßÑÊ®°ÂØπÊõø‰ª£ÊâßË°åÂáÜÁ°ÆÊÄßÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°LLMsÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãËÉΩÂ§üÈ¢ÑÊµã‰ª£Á†ÅÊâßË°åÁªìÊûúÔºå‰ΩÜÂú®ÈÄöÁî®Êõø‰ª£ÊâßË°åÊñπÈù¢‰ªçÂ≠òÂú®Â±ÄÈôêÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12146', 'title': 'Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening', 'url': 'https://huggingface.co/papers/2502.12146', 'abstract': 'We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening', 'score': 9, 'issue_id': 2265, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '25a69f8cf847067e', 'authors': ['Ye Tian', 'Ling Yang', 'Xinchen Zhang', 'Yunhai Tong', 'Mengdi Wang', 'Bin Cui'], 'affiliations': ['Peking University', 'Princeton University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12146.jpg', 'data': {'categories': ['#training', '#rlhf', '#optimization', '#diffusion', '#alignment', '#rl'], 'emoji': 'üéØ', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Diffusion-Sharpening –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ç–µ–≥—Ä–∞–ª—ã –ø–æ –ø—É—Ç—è–º –∏ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—é. Diffusion-Sharpening –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º, –≤–∫–ª—é—á–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ª—é–¥–µ–π.'}, 'en': {'title': 'Optimize Sampling Paths for Better Model Alignment!', 'desc': 'The paper introduces Diffusion-Sharpening, a novel fine-tuning method that improves the alignment of machine learning models by optimizing the paths taken during sampling. Unlike traditional reinforcement learning (RL) methods that focus on individual training steps, this approach considers the entire trajectory, which enhances overall performance. By employing a path integral framework, Diffusion-Sharpening efficiently selects the best trajectories while minimizing inference costs. Experimental results show that this method not only converges faster but also achieves better performance than existing RL-based and trajectory optimization techniques across various evaluation metrics.'}, 'zh': {'title': 'Êâ©Êï£ÈîêÂåñÔºöÈ´òÊïàÁöÑÂæÆË∞ÉÊñ∞ÊñπÊ≥ï', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Êâ©Êï£ÈîêÂåñÔºàDiffusion-SharpeningÔºâÁöÑÂæÆË∞ÉÊñπÊ≥ïÔºåÈÄöËøá‰ºòÂåñÈááÊ†∑ËΩ®ËøπÊù•Â¢ûÂº∫‰∏ãÊ∏∏ÂØπÈΩê„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂæÆË∞ÉÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®Âçï‰∏™ËÆ≠ÁªÉÊó∂Èó¥Ê≠•ÔºåÂøΩËßÜ‰∫ÜËΩ®ËøπÁ∫ßÂà´ÁöÑÂØπÈΩêÔºåËÄåÊúÄËøëÁöÑÈááÊ†∑ËΩ®Ëøπ‰ºòÂåñÊñπÊ≥ïÂàôÂ∏¶Êù•‰∫ÜÊòæËëóÁöÑÊé®ÁêÜÊàêÊú¨„ÄÇÊâ©Êï£ÈîêÂåñÈÄöËøá‰ΩøÁî®Ë∑ØÂæÑÁßØÂàÜÊ°ÜÊû∂Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÈÄâÊã©ÊúÄ‰Ω≥ËΩ®ËøπÔºåÂà©Áî®Â•ñÂä±ÂèçÈ¶àÂπ∂ÊëäÈîÄÊé®ÁêÜÊàêÊú¨Ôºå‰ªéËÄåÂÖãÊúç‰∫ÜËøô‰∫õÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÊâ©Êï£ÈîêÂåñÂú®ËÆ≠ÁªÉÊïàÁéáÂíåÊé®ÁêÜÊïàÁéá‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÂæÆË∞ÉÊñπÊ≥ïÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ï‰∏îÈ´òÊïàÁöÑÊú™Êù•Êâ©Êï£Ê®°ÂûãÂæÆË∞ÉËß£ÂÜ≥ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11831', 'title': 'Intuitive physics understanding emerges from self-supervised pretraining on natural videos', 'url': 'https://huggingface.co/papers/2502.11831', 'abstract': 'We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.', 'score': 6, 'issue_id': 2270, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '269bf0ee605c4537', 'authors': ['Quentin Garrido', 'Nicolas Ballas', 'Mahmoud Assran', 'Adrien Bardes', 'Laurent Najman', 'Michael Rabbat', 'Emmanuel Dupoux', 'Yann LeCun'], 'affiliations': ['EHESS', 'FAIR at Meta', 'Univ Gustave Eiffel'], 'pdf_title_img': 'assets/pdf/title_img/2502.11831.jpg', 'data': {'categories': ['#reasoning', '#video', '#architecture', '#multimodal', '#agi'], 'emoji': 'üß†', 'ru': {'title': '–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–∞—è —Ñ–∏–∑–∏–∫–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∏–∑–∏–∫–∏ –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö, –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –æ–±–ª–∞—Å—Ç–∏ –≤ –≤–∏–¥–µ–æ. –ò—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥ –Ω–∞—Ä—É—à–µ–Ω–∏—è –æ–∂–∏–¥–∞–Ω–∏–π, –æ–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≤—ã—É—á–µ–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤. –ù–∞–ø—Ä–æ—Ç–∏–≤, –º–æ–¥–µ–ª–∏, —Ä–∞–±–æ—Ç–∞—é—â–∏–µ —Å –ø–∏–∫—Å–µ–ª—å–Ω—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º, –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–ª–∏–∑–∫–∏–µ –∫ —Å–ª—É—á–∞–π–Ω—ã–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–ª—è –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏—è –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∏–∑–∏–∫–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–º—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —á–∞—Å—Ç–µ–π —Å–µ–Ω—Å–æ—Ä–Ω–æ–≥–æ –≤–≤–æ–¥–∞.'}, 'en': {'title': 'Learning Intuitive Physics Through Video Prediction', 'desc': 'This paper explores how deep neural networks can learn intuitive physics by predicting missing parts of videos. The researchers found that models trained in a learned representation space can grasp concepts like object permanence and shape consistency. In contrast, models that operate directly on pixel data or rely on text reasoning perform poorly. The study suggests that understanding intuitive physics can emerge from training on sensory input without needing pre-existing core knowledge.'}, 'zh': {'title': 'ÈÄöËøáÈ¢ÑÊµãÂ≠¶‰π†Áõ¥ËßÇÁâ©ÁêÜÁêÜËß£', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÈÄöÁî®Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÊ®°ÂûãÂú®È¢ÑÊµãËá™ÁÑ∂ËßÜÈ¢ë‰∏≠Ë¢´ÈÅÆÊå°Âå∫ÂüüÊó∂ÔºåÂ¶Ç‰Ωï‰∫ßÁîüÁõ¥ËßÇÁâ©ÁêÜÁêÜËß£„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁªèËøáËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Â≠¶‰π†ÁöÑË°®Á§∫Á©∫Èó¥‰∏≠È¢ÑÊµãÁªìÊûúÊó∂ÔºåËÉΩÂ§üÁêÜËß£Áâ©‰ΩìÁöÑÊåÅ‰πÖÊÄßÂíåÂΩ¢Áä∂‰∏ÄËá¥ÊÄßÁ≠âÁõ¥ËßÇÁâ©ÁêÜÁâπÊÄß„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂú®ÂÉèÁ¥†Á©∫Èó¥‰∏≠ËøõË°åËßÜÈ¢ëÈ¢ÑÊµãÁöÑÊ®°ÂûãÂíåÈÄöËøáÊñáÊú¨Êé®ÁêÜÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂÖ∂Ë°®Áé∞Êé•ËøëÈöèÊú∫Ê∞¥Âπ≥„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËÅîÂêàÂ≠¶‰π†ÊäΩË±°Ë°®Á§∫Á©∫Èó¥Âπ∂È¢ÑÊµãÊÑüÂÆòËæìÂÖ•ÁöÑÁº∫Â§±ÈÉ®ÂàÜÔºåË∂≥‰ª•Ëé∑ÂæóÁõ¥ËßÇÁâ©ÁêÜÁöÑÁêÜËß£ÔºåÁîöËá≥Âú®‰ªÖÁî®‰∏ÄÂë®Áã¨ÁâπËßÜÈ¢ëËÆ≠ÁªÉÁöÑÊ®°Âûã‰πüËÉΩË°®Áé∞Âá∫Ë∂ÖÂá∫ÈöèÊú∫ÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11438', 'title': 'SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL', 'url': 'https://huggingface.co/papers/2502.11438', 'abstract': 'Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.', 'score': 6, 'issue_id': 2264, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '93b545df707b1538', 'authors': ['Jimin Lee', 'Ingeol Baek', 'Byeongjeong Kim', 'Hwanhee Lee'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.11438.jpg', 'data': {'categories': ['#data', '#dataset', '#transfer_learning', '#optimization', '#training'], 'emoji': 'üîç', 'ru': {'title': '–°–∞–º–æ—É—Å–∏–ª–µ–Ω–∏–µ –ò–ò –≤ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ SQL', 'desc': 'SAFE-SQL - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ SQL-–∑–∞–ø—Ä–æ—Å—ã. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, —É–ª—É—á—à–∞—è –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. SAFE-SQL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –∑–∞–¥–∞—á–∞—Ö zero-shot –∏ few-shot, –¥–æ—Å—Ç–∏–≥–∞—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤. –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –≤ —Å–ª–æ–∂–Ω—ã—Ö –∏ —Ä–∞–Ω–µ–µ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–≤—à–∏—Ö—Å—è —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, –≥–¥–µ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —á–∞—Å—Ç–æ –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è.'}, 'en': {'title': 'Transforming Language to SQL with Self-Augmented Learning', 'desc': 'This paper introduces SAFE-SQL, a new framework for converting natural language questions into SQL queries. It addresses the limitations of previous methods that rely on existing training examples, which may not be available in real-world situations. SAFE-SQL enhances SQL generation by creating and filtering self-generated examples using a large language model (LLM). The framework demonstrates improved execution accuracy, especially in challenging and unseen scenarios, outperforming traditional zero-shot and few-shot approaches.'}, 'zh': {'title': 'Ëá™ÊàëÂ¢ûÂº∫ÔºåÊèêÂçáText-to-SQLÁöÑÂáÜÁ°ÆÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂SAFE-SQLÔºåÁî®‰∫éÂ∞ÜËá™ÁÑ∂ËØ≠Ë®ÄÈóÆÈ¢òËΩ¨Êç¢‰∏∫ÂèØÊâßË°åÁöÑSQLÊü•ËØ¢„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáËá™ÊàëÂ¢ûÂº∫ÁöÑ‰∏ä‰∏ãÊñáÂ≠¶‰π†ÂíåÁªÜÁ≤íÂ∫¶Á§∫‰æãÈÄâÊã©Êù•ÊèêÈ´òSQLÁîüÊàêÁöÑË¥®Èáè„ÄÇSAFE-SQLÈ¶ñÂÖàÁîüÊàêÂ§ö‰∏™‰∏éÊµãËØïËæìÂÖ•Áõ∏ÂÖ≥ÁöÑText-to-SQLÁ§∫‰æãÔºåÁÑ∂ÂêéÈÄöËøá‰∏âÁßçÁõ∏ÂÖ≥ÊÄßËØÑ‰º∞ÂØπËøô‰∫õÁ§∫‰æãËøõË°åËøáÊª§Ôºå‰ªéËÄåÊûÑÂª∫È´òË¥®ÈáèÁöÑÂ≠¶‰π†Á§∫‰æã„ÄÇ‰∏é‰º†ÁªüÁöÑÈõ∂Ê†∑Êú¨ÂíåÂ∞ëÊ†∑Êú¨ÊñπÊ≥ïÁõ∏ÊØîÔºåSAFE-SQLÂú®ÊâßË°åÂáÜÁ°ÆÊÄß‰∏äÂèñÂæó‰∫ÜÊòæËëóÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®Âõ∞ÈöæÂíåÊú™ËßÅËøáÁöÑÂú∫ÊôØ‰∏≠Ë°®Áé∞Êõ¥‰Ω≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11357', 'title': 'Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents', 'url': 'https://huggingface.co/papers/2502.11357', 'abstract': 'Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible.', 'score': 5, 'issue_id': 2277, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '26d055a62173bda2', 'authors': ['Vardaan Pahuja', 'Yadong Lu', 'Corby Rosset', 'Boyu Gou', 'Arindam Mitra', 'Spencer Whitehead', 'Yu Su', 'Ahmed Awadallah'], 'affiliations': ['Microsoft Research, Redmond', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11357.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#synthetic', '#dataset', '#agents', '#benchmark'], 'emoji': 'üåê', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –∫–ª—é—á –∫ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—é –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –±–æ–ª–µ–µ 94 —Ç—ã—Å—è—á —É—Å–ø–µ—à–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 49 —Ç—ã—Å—è—á —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL-–∞–¥—Ä–µ—Å–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª –æ–±—É—á–µ–Ω –≤–µ–±-–∞–≥–µ–Ω—Ç Explorer, –ø–æ–∫–∞–∑–∞–≤—à–∏–π –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —É–≤–µ–ª–∏—á–µ–Ω–∏—è –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Unlocking Web Tasks with Scalable Multimodal Datasets', 'desc': 'This paper presents a solution to the challenge of training large multimodal models (LMMs) for web tasks by creating a vast and diverse dataset of web trajectories. The dataset includes over 94,000 successful multimodal web interactions, which were synthesized through extensive web exploration, making it cost-effective to produce. The authors introduce Explorer, a multimodal web agent trained on this dataset, which shows improved performance on various benchmarks compared to previous models. The study emphasizes the importance of data scaling in enhancing the capabilities of web agents, aiming to make advanced LMM research more accessible to the community.'}, 'zh': {'title': 'ÂêàÊàêÂ§öÊ†∑ÂåñÊï∞ÊçÆÈõÜÔºåÊèêÂçáÂ§öÊ®°ÊÄÅ‰ª£ÁêÜËÉΩÂäõ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éÂêàÊàêÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMÔºâÊâÄÈúÄÁöÑÂ§öÊ†∑ÂåñËΩ®ËøπÁ∫ßÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´Ë∂ÖËøá94,000‰∏™ÊàêÂäüÁöÑÂ§öÊ®°ÊÄÅÁΩëÁªúËΩ®ËøπÁöÑÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ49,000‰∏™Áã¨ÁâπÁöÑURLÂíå720,000‰∏™Êà™Âõæ„ÄÇÈÄöËøáÂπøÊ≥õÁöÑÁΩëÁªúÊé¢Á¥¢Âíå‰ªªÂä°ÊÑèÂõæÁöÑÁªÜÂåñÔºåÊàë‰ª¨ËÉΩÂ§ü‰ª•‰ΩéÊàêÊú¨Êî∂ÈõÜÂ§öÊ†∑ÂåñÁöÑÊï∞ÊçÆ„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÊï∞ÊçÆËßÑÊ®°ÁöÑÊâ©Â§ßÊòØÊèêÂçáÁΩëÁªú‰ª£ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12135', 'title': 'MagicArticulate: Make Your 3D Models Articulation-Ready', 'url': 'https://huggingface.co/papers/2502.12135', 'abstract': 'With the explosive growth of 3D content creation, there is an increasing demand for automatically converting static 3D models into articulation-ready versions that support realistic animation. Traditional approaches rely heavily on manual annotation, which is both time-consuming and labor-intensive. Moreover, the lack of large-scale benchmarks has hindered the development of learning-based solutions. In this work, we present MagicArticulate, an effective framework that automatically transforms static 3D models into articulation-ready assets. Our key contributions are threefold. First, we introduce Articulation-XL, a large-scale benchmark containing over 33k 3D models with high-quality articulation annotations, carefully curated from Objaverse-XL. Second, we propose a novel skeleton generation method that formulates the task as a sequence modeling problem, leveraging an auto-regressive transformer to naturally handle varying numbers of bones or joints within skeletons and their inherent dependencies across different 3D models. Third, we predict skinning weights using a functional diffusion process that incorporates volumetric geodesic distance priors between vertices and joints. Extensive experiments demonstrate that MagicArticulate significantly outperforms existing methods across diverse object categories, achieving high-quality articulation that enables realistic animation. Project page: https://chaoyuesong.github.io/MagicArticulate.', 'score': 5, 'issue_id': 2270, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': 'ff42a91250856a78', 'authors': ['Chaoyue Song', 'Jianfeng Zhang', 'Xiu Li', 'Fan Yang', 'Yiwen Chen', 'Zhongcong Xu', 'Jun Hao Liew', 'Xiaoyang Guo', 'Fayao Liu', 'Jiashi Feng', 'Guosheng Lin'], 'affiliations': ['ByteDance Seed', 'Institute for Inforcomm Research, A*STAR', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12135.jpg', 'data': {'categories': ['#3d', '#benchmark', '#dataset', '#architecture'], 'emoji': 'ü¶æ', 'ru': {'title': '–ú–∞–≥–∏—è –æ–∂–∏–≤–ª–µ–Ω–∏—è 3D: –æ—Ç —Å—Ç–∞—Ç–∏–∫–∏ –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏', 'desc': 'MagicArticulate - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –≤ –≥–æ—Ç–æ–≤—ã–µ –∫ –∞–Ω–∏–º–∞—Ü–∏–∏ –≤–µ—Ä—Å–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Articulation-XL - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –±–æ–ª–µ–µ —á–µ–º 33 —Ç—ã—Å—è—á–∞–º–∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∫–µ–ª–µ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ—Å—Ç–µ–π –∏ —Å—É—Å—Ç–∞–≤–æ–≤. –î–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ—Å–æ–≤ —Å–∫–∏–Ω–Ω–∏–Ω–≥–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Å —É—á–µ—Ç–æ–º –æ–±—ä–µ–º–Ω—ã—Ö –≥–µ–æ–¥–µ–∑–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É –≤–µ—Ä—à–∏–Ω–∞–º–∏ –∏ —Å—É—Å—Ç–∞–≤–∞–º–∏.'}, 'en': {'title': 'Transforming 3D Models for Realistic Animation with MagicArticulate', 'desc': 'This paper introduces MagicArticulate, a framework designed to convert static 3D models into articulation-ready versions for realistic animation. It addresses the challenges of manual annotation by providing Articulation-XL, a large-scale benchmark with over 33,000 3D models and high-quality articulation annotations. The framework employs a novel skeleton generation method using an auto-regressive transformer to manage varying bone structures and their dependencies. Additionally, it utilizes a functional diffusion process to predict skinning weights, resulting in superior performance in generating articulated models compared to existing methods.'}, 'zh': {'title': 'Ëá™Âä®Âåñ3DÊ®°ÂûãÂÖ≥ËäÇÂåñÁöÑÈù©ÂëΩÊÄßÊ°ÜÊû∂', 'desc': 'ÈöèÁùÄ3DÂÜÖÂÆπÂàõ‰ΩúÁöÑÂø´ÈÄüÂ¢ûÈïøÔºåËá™Âä®Â∞ÜÈùôÊÄÅ3DÊ®°ÂûãËΩ¨Êç¢‰∏∫ÂèØËøõË°åÁúüÂÆûÂä®ÁîªÁöÑÂÖ≥ËäÇÊ®°ÂûãÁöÑÈúÄÊ±ÇÊó•ÁõäÂ¢ûÂä†„ÄÇ‰º†ÁªüÊñπÊ≥ï‰æùËµñ‰∫éÊâãÂä®Ê†áÊ≥®ÔºåÊó¢ËÄóÊó∂ÂèàË¥πÂäõÔºå‰∏îÁº∫‰πèÂ§ßËßÑÊ®°Âü∫ÂáÜÊµãËØïÈôêÂà∂‰∫ÜÂü∫‰∫éÂ≠¶‰π†ÁöÑËß£ÂÜ≥ÊñπÊ°àÁöÑÂèëÂ±ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜMagicArticulateÊ°ÜÊû∂ÔºåËÉΩÂ§üËá™Âä®Â∞ÜÈùôÊÄÅ3DÊ®°ÂûãËΩ¨Âåñ‰∏∫ÈÄÇÂêàÂÖ≥ËäÇÂä®ÁîªÁöÑËµÑ‰∫ß„ÄÇÊàë‰ª¨ÁöÑ‰∏ªË¶ÅË¥°ÁåÆÂåÖÊã¨Âª∫Á´ã‰∫ÜArticulation-XLÂü∫ÂáÜ„ÄÅÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÈ™®Êû∂ÁîüÊàêÊñπÊ≥ïÔºåÂπ∂‰ΩøÁî®ÂäüËÉΩÊâ©Êï£ËøáÁ®ãÈ¢ÑÊµãËíôÁöÆÊùÉÈáçÔºåÊòæËëóÊèêÂçá‰∫ÜÂä®ÁîªË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11275', 'title': "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest", 'url': 'https://huggingface.co/papers/2502.11275', 'abstract': "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token prediction into extraction for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, Cuckoo, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.", 'score': 5, 'issue_id': 2263, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': '6444052efad6f8be', 'authors': ['Letian Peng', 'Zilong Wang', 'Feng Yao', 'Jingbo Shang'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.11275.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#data', '#transfer_learning'], 'emoji': 'üê£', 'ru': {'title': '–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –ø–ª–µ—á–∞—Ö –≥–∏–≥–∞–Ω—Ç–æ–≤: –∫–∞–∫ IE –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ—Å—É—Ä—Å—ã LLM', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (IE) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ—Å—É—Ä—Å–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ú–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤' (NTE) –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –∑–∞–¥–∞—á—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —É–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å Cuckoo, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 102,6 –º–ª–Ω –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —É—Å–ª–æ–≤–∏—è—Ö –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ IE –º–æ–¥–µ–ª—è–º–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç IE –º–æ–¥–µ–ª—è–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–≤–∏–≤–∞—Ç—å—Å—è –≤–º–µ—Å—Ç–µ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏ –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä—É—á–Ω—ã—Ö —É—Å–∏–ª–∏–π."}, 'en': {'title': 'Leveraging LLMs for Enhanced Information Extraction', 'desc': "This paper introduces a new approach for information extraction (IE) using large language models (LLMs) as a resource. The authors propose a method called next tokens extraction (NTE), which allows IE models to leverage existing LLM data for training. They present a model named Cuckoo, which is trained on 102.6 million extractive data points derived from LLMs, showing superior performance in few-shot scenarios. Cuckoo's design enables it to adapt to various IE tasks while benefiting from ongoing improvements in LLM training without requiring extra manual data preparation."}, 'zh': {'title': 'Âà©Áî®LLMÊèêÂçá‰ø°ÊÅØÊèêÂèñÊ®°ÂûãÁöÑÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊù•ÊèêÂçá‰ø°ÊÅØÊèêÂèñÔºàIEÔºâÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊèêÂèñÊñπÊ≥ïÔºåÁß∞‰∏∫‰∏ã‰∏ÄÊ†áËÆ∞ÊèêÂèñÔºàNTEÔºâÔºåÈÄöËøáÂ∞Ü‰∏ã‰∏Ä‰∏™Ê†áËÆ∞È¢ÑÊµãËΩ¨Âåñ‰∏∫ÂØπ‰∏ä‰∏ãÊñá‰∏≠Â∑≤Â≠òÂú®Ê†áËÆ∞ÁöÑÊèêÂèñÔºå‰ªéËÄå‰ΩøIEÊ®°ÂûãËÉΩÂ§üÂà©Áî®LLMÁöÑËµÑÊ∫ê„ÄÇÊàë‰ª¨ÂºÄÂèëÁöÑCuckooÊ®°ÂûãÂú®Â∞ëÈáèÊ†∑Êú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÊúâÊïàÈÄÇÂ∫î‰º†ÁªüÂíåÂ§çÊùÇÁöÑÊåá‰ª§Ë∑üÈöèIE‰ªªÂä°ÔºåÂπ∂‰∏îË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉIEÊ®°Âûã„ÄÇCuckoo‰Ωú‰∏∫‰∏Ä‰∏™‚ÄúÊê≠‰æøËΩ¶ËÄÖ‚ÄùÔºåËÉΩÂ§üÈöèÁùÄLLMÊï∞ÊçÆÂáÜÂ§áÁöÑËøõÊ≠•ËÄåËá™ÁÑ∂ÊºîÂèòÔºåÊó†ÈúÄÈ¢ùÂ§ñÁöÑ‰∫∫Â∑•Âä™Âäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11157', 'title': 'Dyve: Thinking Fast and Slow for Dynamic Process Verification', 'url': 'https://huggingface.co/papers/2502.11157', 'abstract': "We present Dyve, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation System 1 for straightforward steps and comprehensive analysis System 2 for complex ones. Leveraging a novel step-wise consensus-filtered process supervision technique, combining Monte Carlo estimation with LLM based evaluation, Dyve curates high-quality supervision signals from noisy data. Experimental results on ProcessBench and the MATH dataset confirm that Dyve significantly outperforms existing process-based verifiers and boosts performance in Best-of-N settings.", 'score': 4, 'issue_id': 2272, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': 'df0803abac7073f4', 'authors': ['Jianyuan Zhong', 'Zeju Li', 'Zhijian Xu', 'Xiangyu Wen', 'Qiang Xu'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.11157.jpg', 'data': {'categories': ['#reasoning', '#math', '#training', '#optimization', '#data', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': 'Dyve: —É–º–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Dyve - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, —É–ª—É—á—à–∞—é—â–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω —Å–æ—á–µ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ–µ –∏ –º–µ–¥–ª–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ —Ç–µ–æ—Ä–∏–µ–π –ö–∞–Ω–µ–º–∞–Ω–∞ –æ –¥–≤—É—Ö —Å–∏—Å—Ç–µ–º–∞—Ö. Dyve –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–∏–∫—É –ø–æ—à–∞–≥–æ–≤–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö ProcessBench –∏ MATH –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Dyve –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.'}, 'en': {'title': 'Dyve: Enhancing Language Model Reasoning with Dual Thinking Strategies', 'desc': 'Dyve is a dynamic process verifier designed to improve error detection in large language models by utilizing two types of reasoning: fast (System 1) and slow (System 2) thinking. It applies quick, token-level checks for simple tasks while employing in-depth analysis for more complex processes. The system uses a unique method of step-wise consensus filtering, which combines Monte Carlo estimation with evaluations from large language models to generate reliable supervision signals from noisy data. Experiments show that Dyve outperforms current process verifiers and enhances performance in competitive settings.'}, 'zh': {'title': 'DyveÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÁöÑÂä®ÊÄÅÈ™åËØÅÂô®', 'desc': 'DyveÊòØ‰∏ÄÁßçÂä®ÊÄÅËøáÁ®ãÈ™åËØÅÂô®ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊé®ÁêÜÈîôËØØÊ£ÄÊµãËÉΩÂäõ„ÄÇÂÆÉÁªìÂêà‰∫ÜÂø´ÈÄüÊÄùÁª¥ÂíåÊÖ¢ÈÄüÊÄùÁª¥ÔºåÁÅµÊÑüÊù•Ê∫ê‰∫éÂç°Â∞ºÊõºÁöÑÁ≥ªÁªüÁêÜËÆ∫„ÄÇDyveÊ†πÊçÆ‰ªªÂä°ÁöÑÂ§çÊùÇÊÄßÔºåÁÅµÊ¥ªÂú∞Â∫îÁî®Âç≥Êó∂ÁöÑtokenÁ∫ßÁ°ÆËÆ§ÔºàÁ≥ªÁªü1ÔºâÂíåÂÖ®Èù¢ÂàÜÊûêÔºàÁ≥ªÁªü2Ôºâ„ÄÇÈÄöËøá‰∏ÄÁßçÊñ∞È¢ñÁöÑÈÄêÊ≠•ÂÖ±ËØÜËøáÊª§ËøáÁ®ãÁõëÁù£ÊäÄÊúØÔºåDyve‰ªéÂòàÊùÇÊï∞ÊçÆ‰∏≠ÊèêÂèñÈ´òË¥®ÈáèÁöÑÁõëÁù£‰ø°Âè∑ÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÂÖ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑËøáÁ®ãÈ™åËØÅÂô®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11775', 'title': 'video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model', 'url': 'https://huggingface.co/papers/2502.11775', 'abstract': 'While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities.', 'score': 4, 'issue_id': 2265, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': 'ff6f52de37532ca7', 'authors': ['Guangzhi Sun', 'Yudong Yang', 'Jimin Zhuang', 'Changli Tang', 'Yixuan Li', 'Wei Li', 'Zejun MA', 'Chao Zhang'], 'affiliations': ['ByteDance', 'Tsinghua university', 'Univeristy of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2502.11775.jpg', 'data': {'categories': ['#reasoning', '#video', '#training', '#open_source', '#optimization', '#benchmark', '#multimodal', '#dataset'], 'emoji': 'üé•', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç video-SALMONN-o1 - –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –∞—É–¥–∏–æ–≤–∏–∑—É–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ –∏ –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å—Å–∞ (pDPO) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –¢–∞–∫–∂–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫ RivaBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–∏–¥–µ–æ. video-SALMONN-o1 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Revolutionizing Video Understanding with Enhanced Reasoning', 'desc': 'This paper introduces video-SALMONN-o1, an innovative open-source audio-visual large language model (LLM) aimed at improving general video understanding. It addresses the gap in reasoning capabilities for video content by creating a specialized dataset with complex audio-visual questions and detailed solutions. The authors also present process direct preference optimization (pDPO), a method that enhances reward modeling for multimodal inputs through contrastive step selection. The model demonstrates significant accuracy improvements over existing benchmarks, showcasing its effectiveness in tasks like synthetic video detection without prior training.'}, 'zh': {'title': 'ËßÜÈ¢ëÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥Ôºövideo-SALMONN-o1', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Üvideo-SALMONN-o1ÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ¢ûÂº∫Êé®ÁêÜÈü≥ËßÜÈ¢ëÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥‰∏ÄËà¨ËßÜÈ¢ëÁêÜËß£‰ªªÂä°„ÄÇ‰∏∫‰∫ÜÊèêÂçáÂÖ∂Êé®ÁêÜËÉΩÂäõÔºåÁ†îÁ©∂Âõ¢ÈòüÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÂåÖÂê´ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈü≥ËßÜÈ¢ëÈóÆÈ¢òÂíåÈÄêÊ≠•Ëß£ÂÜ≥ÊñπÊ°àÁöÑÊé®ÁêÜÂØÜÈõÜÂûãÊï∞ÊçÆÈõÜ„ÄÇËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜËøáÁ®ãÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàpDPOÔºâÔºåÂà©Áî®ÂØπÊØîÊ≠•È™§ÈÄâÊã©ÂÆûÁé∞ÈíàÂØπÂ§öÊ®°ÊÄÅËæìÂÖ•ÁöÑÈ´òÊïàÊ≠•È™§Á∫ßÂ•ñÂä±Âª∫Ê®°„ÄÇÊ≠§Â§ñÔºåRivaBench‰Ωú‰∏∫Á¨¨‰∏Ä‰∏™Êé®ÁêÜÂØÜÈõÜÂûãËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÔºåÊèê‰æõ‰∫ÜË∂ÖËøá4000‰∏™È´òË¥®ÈáèÁöÑÈóÆÈ¢ò-Á≠îÊ°àÂØπÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçÂú∫ÊôØ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11098', 'title': 'Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2502.11098', 'abstract': 'Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose Talk Structurally, Act Hierarchically (TalkHier), a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. TalkHier surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier.', 'score': 4, 'issue_id': 2265, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': '9aaac2e7c7b495a4', 'authors': ['Zhao Wang', 'Sota Moriyama', 'Wei-Yao Wang', 'Briti Gangopadhyay', 'Shingo Takamatsu'], 'affiliations': ['Sony Group Corporation, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2502.11098.jpg', 'data': {'categories': ['#open_source', '#optimization', '#agents', '#multimodal', '#alignment'], 'emoji': 'ü§ñ', 'ru': {'title': '–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ TalkHier –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). TalkHier –≤–≤–æ–¥–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –Ω–µ–≤–µ—Ä–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤ –∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–µ–∫–ª–∞–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. TalkHier –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM.'}, 'en': {'title': 'Enhancing Multi-Agent Collaboration with TalkHier Framework', 'desc': 'This paper introduces Talk Hierarchically, Act Structurally (TalkHier), a new framework designed to improve communication and collaboration among multi-agent systems using large language models (LLMs). It features a structured communication protocol that enhances context understanding and a hierarchical refinement system to correct errors and biases in agent outputs. The framework outperforms existing state-of-the-art models in various tasks, demonstrating its effectiveness in open-domain question answering and targeted text generation. Overall, TalkHier aims to establish a new benchmark for LLM-based multi-agent systems, promoting better teamwork and adaptability among agents.'}, 'zh': {'title': 'ÁªìÊûÑÂåñ‰∫§ÊµÅÔºåÂàÜÂ±ÇË°åÂä®ÁöÑÊô∫ËÉΩ‰ΩìÂçè‰ΩúÊñ∞Ê†áÂáÜ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÁß∞‰∏∫Talk Structurally, Act HierarchicallyÔºàTalkHierÔºâÔºåÊó®Âú®ÊîπÂñÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü‰∏≠ÁöÑÈÄö‰ø°ÂíåÂçè‰Ωú„ÄÇËØ•Ê°ÜÊû∂ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁªìÊûÑÂåñÁöÑÈÄö‰ø°ÂçèËÆÆÔºå‰ª•‰æøÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ËøõË°å‰∏∞ÂØåÁöÑ‰∏ä‰∏ãÊñá‰∫§ÊµÅÔºåÂπ∂Âª∫Á´ã‰∫Ü‰∏Ä‰∏™ÂàÜÂ±ÇÁöÑÁ≤æÁÇºÁ≥ªÁªüÔºå‰ª•Ëß£ÂÜ≥ÈîôËØØËæìÂá∫„ÄÅËôöÂÅá‰ø°ÊÅØÂíåÂÅèËßÅÁ≠âÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTalkHierÂú®Â§ö‰∏™‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊäÄÊúØÔºåÂåÖÊã¨ÂºÄÊîæÈ¢ÜÂüüÈóÆÁ≠îÂíåÁâπÂÆöÈ¢ÜÂüüÈÄâÊã©ÊÄßÊèêÈóÆÁ≠â„ÄÇËØ•Á†îÁ©∂‰∏∫LLM-MAÁ≥ªÁªüËÆæÂÆö‰∫ÜÊñ∞ÁöÑÊ†áÂáÜÔºåÊé®Âä®‰∫ÜÊõ¥ÊúâÊïà„ÄÅÁÅµÊ¥ªÂíåÂçè‰ΩúÁöÑÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11901', 'title': 'Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity', 'url': 'https://huggingface.co/papers/2502.11901', 'abstract': "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.", 'score': 4, 'issue_id': 2263, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '9451c99877c67e4d', 'authors': ['Dylan Zhang', 'Justin Wang', 'Tianran Sun'], 'affiliations': ['Shanghai Jiaotong University', 'University of Chicago', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.11901.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#data', '#plp', '#transfer_learning', '#synthetic'], 'emoji': 'üß†', 'ru': {'title': '–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Ö–≤–∞—Ç–∫–∏ –∫–æ—Ä–ø—É—Å–æ–≤ –Ω–∞ —è–∑—ã–∫–∞—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∏ —Å–æ–∑–¥–∞—é—Ç –º–æ–¥–µ–ª—å PoPilot, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç GPT-4 –Ω–∞ 64% –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–µ–∫—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã GPT-4 –Ω–∞ 54% –ø—É—Ç–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –µ–≥–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Enhancing Proof-Oriented Programming with Synthetic Data Augmentation', 'desc': 'This paper addresses the challenges faced by language models (LMs) in proof-oriented programming due to limited data availability. It introduces a novel approach of synthetic data augmentation to enhance the training of LMs for generating and repairing proofs in programming languages like F*. The method involves creating basic proof-oriented programming problems and utilizing diverse coding data to improve reasoning capabilities. The results demonstrate that the fine-tuned 14B parameter model, PoPilot, significantly outperforms existing models, including GPT-4o, in project-level proof-oriented programming tasks.'}, 'zh': {'title': 'ÂêàÊàêÊï∞ÊçÆÂ¢ûÂº∫ÔºåÊèêÂçáËØÅÊòéÁºñÁ®ãËÉΩÂäõÔºÅ', 'desc': 'Áé∞ÊúâÁöÑËØ≠Ë®ÄÊ®°ÂûãÂú®Èù¢ÂêëËØÅÊòéÁöÑÁºñÁ®ã‰∏≠Èù¢‰∏¥Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢òÔºå‰∏ªË¶Å‰ΩìÁé∞Âú®‰∏§‰∏™ÊñπÈù¢ÔºöÁº∫‰πèË∂≥Â§üÁöÑÈù¢ÂêëËØÅÊòéÁºñÁ®ãËØ≠Ë®ÄÔºàÂ¶ÇF*ÔºâÁöÑËØ≠ÊñôÂ∫ìÔºå‰ª•ÂèäÁº∫Â∞ëÂ§ßËßÑÊ®°ÁöÑÈ°πÁõÆÁ∫ßËØÅÊòéÂÆûÁé∞ÔºåÊó†Ê≥ïÊïô‰ºöÊ®°ÂûãÂ§çÊùÇÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂêàÊàêÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÊñπÊ≥ïÔºå‰∏ìÊ≥®‰∫éÈ°πÁõÆÁ∫ßÁöÑÈù¢ÂêëËØÅÊòéÁºñÁ®ãÔºåÊó¢Áî®‰∫éÁîüÊàê‰πüÁî®‰∫é‰øÆÂ§ç„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂêàÊàêÂü∫Êú¨ÁöÑÈù¢ÂêëËØÅÊòéÁºñÁ®ãÈóÆÈ¢òÊù•Ëß£ÂÜ≥Êï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢òÔºåÂπ∂ÁªìÂêàÂ§öÊ†∑ÂåñÁöÑÁºñÁ†ÅÊï∞ÊçÆ‰ª•ÊèêÈ´òÊé®ÁêÜËÉΩÂäõÔºåÂêåÊó∂Âú®Áé∞Êúâ‰ª£Á†ÅÂ∫ì‰∏≠ÂàõÂª∫Êñ∞ÁöÑËØÅÊòéÂíå‰øÆÂ§çÊï∞ÊçÆ„ÄÇÊàë‰ª¨ÁöÑ14BÂèÇÊï∞Ê®°ÂûãPoPilotÁªèËøáÂæÆË∞ÉÂêéÔºåÂú®È°πÁõÆÁ∫ßÈù¢ÂêëËØÅÊòéÁºñÁ®ã‰∏≠Ë∂ÖË∂ä‰∫ÜGPT-4oÊ®°Âûã64%ÁöÑÊÄßËÉΩÔºåÂπ∂ÈÄöËøá‰øÆÂ§çÂÖ∂ËæìÂá∫ÊèêÈ´ò‰∫Ü54%ÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11748', 'title': 'ILIAS: Instance-Level Image retrieval At Scale', 'url': 'https://huggingface.co/papers/2502.11748', 'abstract': 'This work introduces ILIAS, a new test dataset for Instance-Level Image retrieval At Scale. It is designed to evaluate the ability of current and future foundation models and retrieval techniques to recognize particular objects. The key benefits over existing datasets include large scale, domain diversity, accurate ground truth, and a performance that is far from saturated. ILIAS includes query and positive images for 1,000 object instances, manually collected to capture challenging conditions and diverse domains. Large-scale retrieval is conducted against 100 million distractor images from YFCC100M. To avoid false negatives without extra annotation effort, we include only query objects confirmed to have emerged after 2014, i.e. the compilation date of YFCC100M. An extensive benchmarking is performed with the following observations: i) models fine-tuned on specific domains, such as landmarks or products, excel in that domain but fail on ILIAS ii) learning a linear adaptation layer using multi-domain class supervision results in performance improvements, especially for vision-language models iii) local descriptors in retrieval re-ranking are still a key ingredient, especially in the presence of severe background clutter iv) the text-to-image performance of the vision-language foundation models is surprisingly close to the corresponding image-to-image case. website: https://vrg.fel.cvut.cz/ilias/', 'score': 3, 'issue_id': 2277, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '59967e50364f64f1', 'authors': ['Giorgos Kordopatis-Zilos', 'Vladan Stojniƒá', 'Anna Manko', 'Pavel ≈†uma', 'Nikolaos-Antonios Ypsilantis', 'Nikos Efthymiadis', 'Zakaria Laskar', 'Ji≈ô√≠ Matas', 'Ond≈ôej Chum', 'Giorgos Tolias'], 'affiliations': ['VRG, FEE, Czech Technical University in Prague'], 'pdf_title_img': 'assets/pdf/title_img/2502.11748.jpg', 'data': {'categories': ['#cv', '#dataset', '#benchmark'], 'emoji': 'üîç', 'ru': {'title': 'ILIAS: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'ILIAS - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –≤ –∫—Ä—É–ø–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ. –û–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –±—É–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π –æ—Å–Ω–æ–≤–∞–Ω–∏—è –∏ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã. ILIAS –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è 1000 —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤—Ä—É—á–Ω—É—é –¥–ª—è –æ—Ç—Ä–∞–∂–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ —Ñ–æ–Ω–µ 100 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –æ—Ç–≤–ª–µ–∫–∞—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ YFCC100M, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.'}, 'en': {'title': 'ILIAS: Advancing Instance-Level Image Retrieval with Scale and Diversity', 'desc': 'This paper presents ILIAS, a novel dataset aimed at enhancing instance-level image retrieval capabilities. It features a large-scale collection of 1,000 object instances with diverse domains and challenging conditions, evaluated against 100 million distractor images. The dataset allows for benchmarking of various models, revealing that domain-specific fine-tuning can lead to performance drops in broader contexts. Additionally, the study highlights the importance of local descriptors in retrieval tasks and notes that vision-language models perform comparably in text-to-image and image-to-image retrieval scenarios.'}, 'zh': {'title': 'ILIASÔºöÂ§ßËßÑÊ®°ÂÆû‰æãÁ∫ßÂõæÂÉèÊ£ÄÁ¥¢ÁöÑÊñ∞Ê†áÂáÜ', 'desc': 'Êú¨Á†îÁ©∂‰ªãÁªç‰∫ÜILIASÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éÂ§ßËßÑÊ®°ÂÆû‰æãÁ∫ßÂõæÂÉèÊ£ÄÁ¥¢ÁöÑÊñ∞ÊµãËØïÊï∞ÊçÆÈõÜ„ÄÇÂÆÉÊó®Âú®ËØÑ‰º∞ÂΩìÂâçÂíåÊú™Êù•Âü∫Á°ÄÊ®°ÂûãÂèäÊ£ÄÁ¥¢ÊäÄÊúØËØÜÂà´ÁâπÂÆöÂØπË±°ÁöÑËÉΩÂäõ„ÄÇILIASÁöÑ‰ºòÂäøÂú®‰∫éÂÖ∂Â§ßËßÑÊ®°„ÄÅÈ¢ÜÂüüÂ§öÊ†∑ÊÄß„ÄÅÂáÜÁ°ÆÁöÑÁúüÂÆûÊ†áÁ≠æÔºå‰ª•ÂèäÂ∞öÊú™È•±ÂíåÁöÑÊÄßËÉΩË°®Áé∞„ÄÇÊï∞ÊçÆÈõÜ‰∏≠ÂåÖÂê´1000‰∏™ÂØπË±°ÂÆû‰æãÁöÑÊü•ËØ¢ÂíåÊ≠£Ê†∑Êú¨ÂõæÂÉèÔºåÊâãÂä®Êî∂ÈõÜ‰ª•ÊçïÊçâÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊù°‰ª∂ÂíåÂ§öÊ†∑ÁöÑÈ¢ÜÂüü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10550', 'title': 'Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.10550', 'abstract': "Memory is crucial for enabling agents to tackle complex tasks with temporal and spatial dependencies. While many reinforcement learning (RL) algorithms incorporate memory, the field lacks a universal benchmark to assess an agent's memory capabilities across diverse scenarios. This gap is particularly evident in tabletop robotic manipulation, where memory is essential for solving tasks with partial observability and ensuring robust performance, yet no standardized benchmarks exist. To address this, we introduce MIKASA (Memory-Intensive Skills Assessment Suite for Agents), a comprehensive benchmark for memory RL, with three key contributions: (1) we propose a comprehensive classification framework for memory-intensive RL tasks, (2) we collect MIKASA-Base - a unified benchmark that enables systematic evaluation of memory-enhanced agents across diverse scenarios, and (3) we develop MIKASA-Robo - a novel benchmark of 32 carefully designed memory-intensive tasks that assess memory capabilities in tabletop robotic manipulation. Our contributions establish a unified framework for advancing memory RL research, driving the development of more reliable systems for real-world applications. The code is available at https://sites.google.com/view/memorybenchrobots/.", 'score': 3, 'issue_id': 2272, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '99224cee50b48e48', 'authors': ['Egor Cherepanov', 'Nikita Kachaev', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['AIRI, Moscow, Russia', 'MIPT, Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.10550.jpg', 'data': {'categories': ['#rl', '#optimization', '#benchmark', '#agents', '#robotics'], 'emoji': 'ü§ñ', 'ru': {'title': 'MIKASA: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–∞–º—è—Ç–∏ RL-–∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MIKASA - –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø–∞–º—è—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏, –∏ —Å–æ–∑–¥–∞—é—Ç –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ MIKASA-Base –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é. –¢–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω MIKASA-Robo - –Ω–∞–±–æ—Ä –∏–∑ 32 –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–∞–º—è—Ç–∏ –≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è—Ö —Ä–æ–±–æ—Ç–æ–≤. –≠—Ç–æ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–∏–∑–≤–∞–Ω —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å —Ä–∞–∑–≤–∏—Ç–∏—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏.'}, 'en': {'title': 'MIKASA: Advancing Memory in Reinforcement Learning for Robotics', 'desc': 'This paper addresses the importance of memory in reinforcement learning (RL) for agents performing complex tasks that require understanding of time and space. It highlights the lack of standardized benchmarks to evaluate memory capabilities in RL, especially in tabletop robotic manipulation scenarios. To fill this gap, the authors introduce MIKASA, a benchmark suite designed to assess memory-intensive skills in agents. MIKASA includes a classification framework, a unified benchmark called MIKASA-Base, and a set of 32 specific tasks in MIKASA-Robo to systematically evaluate memory-enhanced agents.'}, 'zh': {'title': 'ÊèêÂçáÊô∫ËÉΩ‰ΩìËÆ∞ÂøÜËÉΩÂäõÁöÑÁªü‰∏ÄÂü∫ÂáÜ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËÆ∞ÂøÜÂú®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂ§çÊùÇ‰ªªÂä°Êó∂„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜMIKASAÔºå‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØïÂ•ó‰ª∂ÔºåÁî®‰∫éËØÑ‰º∞Êô∫ËÉΩ‰ΩìÁöÑËÆ∞ÂøÜËÉΩÂäõ„ÄÇMIKASAÂåÖÊã¨‰∏Ä‰∏™ÂàÜÁ±ªÊ°ÜÊû∂Âíå‰∏§‰∏™Âü∫ÂáÜÔºåÂàÜÂà´Áî®‰∫éÁ≥ªÁªüËØÑ‰º∞ËÆ∞ÂøÜÂ¢ûÂº∫Êô∫ËÉΩ‰ΩìÂíåËÆæËÆ°32‰∏™ËÆ∞ÂøÜÂØÜÈõÜÂûã‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰Ωú‰∏∫ËÆ∞ÂøÜÂº∫ÂåñÂ≠¶‰π†Á†îÁ©∂Êèê‰æõ‰∫ÜÁªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÊé®Âä®‰∫ÜÊõ¥ÂèØÈù†Á≥ªÁªüÁöÑÂºÄÂèë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12054', 'title': 'PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning', 'url': 'https://huggingface.co/papers/2502.12054', 'abstract': 'Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason.', 'score': 3, 'issue_id': 2269, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '4aaf92e2d2fd9766', 'authors': ['Xinyu Zhang', 'Yuxuan Dong', 'Yanrui Wu', 'Jiaxing Huang', 'Chengyou Jia', 'Basura Fernando', 'Mike Zheng Shou', 'Lingling Zhang', 'Jun Liu'], 'affiliations': ['Institute of High-Performance Computing, A*STAR', 'Show Lab, National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12054.jpg', 'data': {'categories': ['#math', '#benchmark', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': 'PhysReason: –∏—Å–ø—ã—Ç–∞–Ω–∏–µ —Ñ–∏–∑–∏–∫–æ–π –¥–ª—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PhysReason - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 1200 –∑–∞–¥–∞—á —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤ —Å—Ä–µ–¥–Ω–µ–º 8.1 —à–∞–≥–æ–≤ —Ä–µ—à–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ —Ä–µ—à–µ–Ω–∏–π –∏ –≤—ã—è–≤–ª—è—é—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Ç–µ–æ—Ä–µ–º, –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö –∏ –∞–Ω–∞–ª–∏–∑–µ —É—Å–ª–æ–≤–∏–π. –î–∞–∂–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∏–∂–µ 60% –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'PhysReason: A New Benchmark for Physics-Based Reasoning in AI', 'desc': 'This paper introduces PhysReason, a new benchmark designed to evaluate the physics-based reasoning abilities of large language models. It consists of 1,200 problems, with a mix of knowledge-based and reasoning-based tasks, categorized into three difficulty levels. The benchmark highlights the complexity of physics reasoning, requiring multiple solution steps, with hard problems demanding an average of 15.6 steps. The study also identifies key challenges in physics reasoning, such as theorem application and process understanding, providing insights into the limitations of current models.'}, 'zh': {'title': 'Áâ©ÁêÜÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êï∞Â≠¶ÂíåÈÄªËæëÊé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Áâ©ÁêÜÊé®ÁêÜÊñπÈù¢ÁöÑËØÑ‰º∞‰ªçÁÑ∂‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜPhysReasonÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´1200‰∏™ÈóÆÈ¢òÁöÑÂü∫ÂáÜÊµãËØïÔºåÂÖ∂‰∏≠75%ÊòØÊé®ÁêÜÁ±ªÈóÆÈ¢òÔºåÂàÜ‰∏∫ÁÆÄÂçï„ÄÅ‰∏≠Á≠âÂíåÂõ∞Èöæ‰∏â‰∏™ÈöæÂ∫¶Á∫ßÂà´„ÄÇÈÄöËøáÂºïÂÖ•Áâ©ÁêÜËß£È¢òËá™Âä®ËØÑÂàÜÊ°ÜÊû∂ÔºåÊàë‰ª¨ËÉΩÂ§üÊúâÊïàËØÑ‰º∞Ê®°ÂûãÂú®Áâ©ÁêÜÂÆöÁêÜÂ∫îÁî®„ÄÅËøáÁ®ãÁêÜËß£„ÄÅËÆ°ÁÆóÂíåÊù°‰ª∂ÂàÜÊûêÁ≠âÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÈ°∂Â∞ñÊ®°ÂûãÂú®Áâ©ÁêÜÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑË°®Áé∞‰ªçÊúâÂæÖÊèêÈ´òÔºåÂ∞§ÂÖ∂ÊòØÂú®Âõ∞ÈöæÈóÆÈ¢ò‰∏ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11330', 'title': 'System Message Generation for User Preferences using Open-Source Models', 'url': 'https://huggingface.co/papers/2502.11330', 'abstract': 'System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communication styles. Despite such versatility, publicly available data are often lack system messages and subject to strict license constraints in the industry field. Manual labeling of publicly available data with system messages that align with user instructions demands significant resources. In view of such challenges, our work introduces SysGen, a pipeline for generating system messages with better aligned assistant responses from the supervised fine-tuning dataset without system messages. Training on SysGen data has demonstrated substantial improvements in the alignment of model responses with system messages and user instructions, as demonstrated across various open-source models on the Multifacet benchmark, while maintaining minimal impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our qualitative analysis highlights the importance of diverse system messages to ensure better adaptability across different contexts.', 'score': 3, 'issue_id': 2267, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '36ca5a9ceb25e7fa', 'authors': ['Minbyul Jeong', 'Jungho Cho', 'Minsoo Khang', 'Dawoon Jung', 'Teakgyu Hong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.11330.jpg', 'data': {'categories': ['#open_source', '#alignment', '#training', '#benchmark', '#dataset'], 'emoji': 'ü§ñ', 'ru': {'title': 'SysGen: —É–ª—É—á—à–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ—Ç–≤–µ—Ç–æ–≤ LLM —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SysGen - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). SysGen —Å–æ–∑–¥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ—Ç–≤–µ—Ç–∞–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –±–µ–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö SysGen –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏—è–º –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —á—Ç–æ –±—ã–ª–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–µ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º.'}, 'en': {'title': 'Enhancing LLM Responses with SysGen: Better System Messages for Better Alignment', 'desc': 'This paper presents SysGen, a new method for generating system messages that help large language models (LLMs) respond more accurately to user instructions. System messages are crucial for guiding LLMs in their interactions, but there is a lack of publicly available data that includes these messages. SysGen addresses this issue by creating a pipeline that generates system messages from existing supervised fine-tuning datasets, leading to improved alignment of model responses. The results show that training with SysGen data enhances the performance of various open-source models while keeping their effectiveness on other benchmarks largely unchanged.'}, 'zh': {'title': 'SysGenÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂìçÂ∫îÂØπÈΩêÊÄßÁöÑÁ≥ªÁªüÊ∂àÊÅØÁîüÊàê', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜSysGenÔºå‰∏Ä‰∏™Áî®‰∫éÁîüÊàêÁ≥ªÁªüÊ∂àÊÅØÁöÑÁÆ°ÈÅìÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏éÁî®Êà∑Êåá‰ª§ÁöÑÂØπÈΩêÂ∫¶„ÄÇÁ≥ªÁªüÊ∂àÊÅØÂú®‰∏éLLMsÁöÑ‰∫§‰∫í‰∏≠Ëµ∑ÁùÄÈáçË¶Å‰ΩúÁî®ÔºåËÉΩÂ§üÂ∏ÆÂä©Áî®Êà∑ÊåáÂÆöËßíËâ≤Âíå‰ªªÂä°„ÄÇÈÄöËøáÂú®Ê≤°ÊúâÁ≥ªÁªüÊ∂àÊÅØÁöÑÁõëÁù£ÂæÆË∞ÉÊï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÔºåSysGenÊòæËëóÊîπÂñÑ‰∫ÜÊ®°ÂûãÂìçÂ∫îÁöÑÂØπÈΩêÊÄß„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêË°®ÊòéÔºåÂ§öÊ†∑ÂåñÁöÑÁ≥ªÁªüÊ∂àÊÅØÂØπ‰∫éÂú®‰∏çÂêå‰∏ä‰∏ãÊñá‰∏≠ÂÆûÁé∞Êõ¥Â•ΩÁöÑÈÄÇÂ∫îÊÄßËá≥ÂÖ≥ÈáçË¶Å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.10454', 'title': 'One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs', 'url': 'https://huggingface.co/papers/2502.10454', 'abstract': 'Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of "proof by counterexamples" commonly used in human mathematics education, our work aims to enhance LLMs\' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs\' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.', 'score': 3, 'issue_id': 2265, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '1821254437fc158d', 'authors': ['Yinghui Li', 'Jiayi Kuang', 'Haojing Huang', 'Zhikun Xu', 'Xinnian Liang', 'Yi Yu', 'Wenlian Lu', 'Yangning Li', 'Xiaoyu Tan', 'Chao Qu', 'Ying Shen', 'Hai-Tao Zheng', 'Philip S. Yu'], 'affiliations': ['ARC Lab, Arizona State University', 'Bytedance Inc.', 'INFLY TECH (Shanghai) Co., Ltd.', 'Peng Cheng Laboratory', 'School of Mathematical Science, Fudan University', 'Sun-Yat Sen University', 'Tsinghua University', 'University of Illinois Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2502.10454.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#optimization', '#dataset'], 'emoji': 'üßÆ', 'ru': {'title': '–ö–æ–Ω—Ç—Ä–ø—Ä–∏–º–µ—Ä—ã –∫–∞–∫ –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ LLM –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –≤ –≥–ª—É–±–æ–∫–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç–µ–æ—Ä–µ–º –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–æ–Ω—Ç—Ä–ø—Ä–∏–º–µ—Ä–∞—Ö. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ CounterMATH –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–ø—Ä–∏–º–µ—Ä—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç—Ä–ø—Ä–∏–º–µ—Ä–æ–≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –æ–±—â–∏—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM.'}, 'en': {'title': "Enhancing LLMs' Mathematical Proofs through Counterexamples", 'desc': 'This paper discusses the limitations of current Large Language Models (LLMs) in generating mathematical proofs, emphasizing their dependence on prior exposure to proof processes during training. The authors introduce a new benchmark called CounterMATH, which challenges LLMs to prove mathematical statements by providing counterexamples, thereby testing their understanding of mathematical concepts. They also present a data engineering framework to enhance the training data for LLMs, aiming to improve their reasoning capabilities. The findings suggest that enhancing counterexample-driven reasoning is essential for advancing the mathematical proficiency of LLMs.'}, 'zh': {'title': 'ÈÄöËøáÂèç‰æãÊèêÂçáÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøõË°åÊï∞Â≠¶ËØÅÊòéÁîüÊàêÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåÂΩìÂâçLLMsÁöÑËØÅÊòéËÉΩÂäõ‰∏ªË¶Å‰æùËµñ‰∫éÂÖ∂Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÊòØÂê¶Êé•Ëß¶ËøáÁõ∏ÂÖ≥ÁöÑËØÅÊòéËøáÁ®ãÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÂØπÊï∞Â≠¶ÂÆöÁêÜÂíåÁõ∏ÂÖ≥Ê¶ÇÂøµÁöÑÊ∑±ÂÖ•ÁêÜËß£„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂèç‰æãÁöÑËØÅÊòéÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂèç‰æãÂ¢ûÂº∫LLMsÁöÑÊï∞Â≠¶Êé®ÁêÜÂíåËØÅÊòéËÉΩÂäõ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊâãÂä®ÂàõÂª∫‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÊï∞Â≠¶Âü∫ÂáÜCounterMATHÔºå‰ª•ËØÑ‰º∞LLMsÂú®Êèê‰æõÂèç‰æãÊó∂ÁöÑÊï∞Â≠¶Ê¶ÇÂøµÊéåÊè°ÊÉÖÂÜµ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11085', 'title': 'Towards Data-Efficient Pretraining for Atomic Property Prediction', 'url': 'https://huggingface.co/papers/2502.11085', 'abstract': "This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th of the computational cost. We introduce the Chemical Similarity Index (CSI), a novel metric inspired by computer vision's Fr\\'echet Inception Distance, for molecular graphs which quantifies the alignment between upstream pretraining datasets and downstream tasks. By selecting the most relevant dataset with minimal CSI distance, we show that models pretrained on a smaller, focused dataset consistently outperform those pretrained on massive, mixed datasets such as JMP, even when those larger datasets include the relevant dataset. Counterintuitively, we also find that indiscriminately adding more data can degrade model performance when the additional data poorly aligns with the task at hand. Our findings highlight that quality often outperforms quantity in pretraining for atomic property prediction.", 'score': 2, 'issue_id': 2270, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': 'de635e01a182309d', 'authors': ['Yasir Ghunaim', 'Hasan Abed Al Kader Hammoud', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2502.11085.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#training', '#dataset', '#benchmark', '#data'], 'emoji': 'üß™', 'ru': {'title': '–ö–∞—á–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞—Ç–æ–º–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Å—Ç–∞–≤–∏—Ç –ø–æ–¥ —Å–æ–º–Ω–µ–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –∞—Ç–æ–º–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤, —Å–≤—è–∑—ã–≤–∞—é—â—É—é –ø—Ä–æ–≥—Ä–µ—Å—Å —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω–æ–º, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–º –∑–∞–¥–∞—á–µ –¥–∞—Ç–∞—Å–µ—Ç–µ –º–æ–∂–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å—Å—è –∏–ª–∏ –¥–∞–∂–µ –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 1/24 –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç. –û–Ω–∏ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É - –ò–Ω–¥–µ–∫—Å –•–∏–º–∏—á–µ—Å–∫–æ–≥–æ –°—Ö–æ–¥—Å—Ç–≤–∞ (CSI), –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—É—é —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –§—Ä–µ—à–µ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏, –¥–ª—è –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –º–µ–Ω—å—à–µ–º, –Ω–æ —Ü–µ–ª–µ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, —Å—Ç–∞–±–∏–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ–ª–∏, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö.'}, 'en': {'title': 'Quality Over Quantity in Atomic Property Prediction', 'desc': 'This paper argues that the success of predicting atomic properties does not solely depend on having large datasets and powerful computing resources. Instead, it demonstrates that pretraining on a well-chosen, relevant dataset can achieve better results while using significantly less computational power. The authors introduce the Chemical Similarity Index (CSI), a new metric that measures how well the pretraining dataset aligns with the specific task. Their research shows that using a focused dataset can lead to superior model performance compared to using larger, less relevant datasets, emphasizing that the quality of data is more important than its quantity.'}, 'zh': {'title': 'Ë¥®ÈáèËÉú‰∫éÊï∞ÈáèÔºöÂéüÂ≠êÂ±ûÊÄßÈ¢ÑÊµãÁöÑÊñ∞ËßÜËßí', 'desc': 'ËøôÁØáËÆ∫ÊñáÊåëÊàò‰∫ÜÂéüÂ≠êÂ±ûÊÄßÈ¢ÑÊµãÈ¢ÜÂüüÁöÑ‰º†ÁªüËßÇÂøµÔºåËÆ§‰∏∫ËøõÊ≠•‰∏éÊï∞ÊçÆÈõÜËßÑÊ®°ÂíåËÆ°ÁÆóËµÑÊ∫êÁöÑÂ¢ûÂä†ÊúâÂÖ≥„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂú®Á≤æÂøÉÈÄâÊã©ÁöÑ„ÄÅ‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÊï∞ÊçÆÈõÜ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂèØ‰ª•ÂåπÈÖçÁîöËá≥Ë∂ÖË∂äÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÔºåÂêåÊó∂ËÆ°ÁÆóÊàêÊú¨‰ªÖ‰∏∫1/24„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂåñÂ≠¶Áõ∏‰ººÊÄßÊåáÊï∞ÔºàCSIÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊåáÊ†áÔºåÁî®‰∫éÈáèÂåñ‰∏äÊ∏∏È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏é‰∏ãÊ∏∏‰ªªÂä°‰πãÈó¥ÁöÑÂØπÈΩêÁ®ãÂ∫¶„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÈÄâÊã©ÊúÄÁõ∏ÂÖ≥ÁöÑÊï∞ÊçÆÈõÜÂèØ‰ª•ÊòæËëóÊèêÈ´òÊ®°ÂûãÊÄßËÉΩÔºåÂº∫Ë∞É‰∫ÜÂú®ÂéüÂ≠êÂ±ûÊÄßÈ¢ÑÊµã‰∏≠ÔºåË¥®ÈáèÂæÄÂæÄ‰ºò‰∫éÊï∞Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09509', 'title': 'EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling', 'url': 'https://huggingface.co/papers/2502.09509', 'abstract': 'Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose EQ-VAE, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By finetuning pre-trained autoencoders with EQ-VAE, we enhance the performance of several state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT, achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning. EQ-VAE is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: https://eq-vae.github.io/.', 'score': 1, 'issue_id': 2280, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'd034cfa8688142a4', 'authors': ['Theodoros Kouzelis', 'Ioannis Kakogeorgiou', 'Spyros Gidaris', 'Nikos Komodakis'], 'affiliations': ['Archimedes, Athena RC, Greece', 'IACM-Forth, Greece', 'National Technical University of Athens, Greece', 'University of Crete, Greece', 'valaio.ai, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.09509.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#training', '#cv', '#optimization'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–≠–∫–≤–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EQ-VAE - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç–∫–≤–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏-—Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è–º, —Ç–∞–∫–∏–º –∫–∞–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—Ä–∞—â–µ–Ω–∏–µ. EQ-VAE —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è DiT, SiT, REPA –∏ MaskGIT. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å–æ–≤–º–µ—Å—Ç–∏–º –∫–∞–∫ —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏, —Ç–∞–∫ –∏ —Å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏.'}, 'en': {'title': 'Enhancing Image Synthesis with EQ-VAE: Simplifying Latent Spaces for Better Generative Performance', 'desc': "This paper introduces EQ-VAE, a novel regularization technique designed to improve latent generative models used for image synthesis. The authors highlight that traditional autoencoders struggle with maintaining equivariance to transformations like scaling and rotation, which complicates the latent space and affects generative quality. By enforcing equivariance in the latent space, EQ-VAE simplifies the structure while preserving the quality of image reconstruction. The results show significant performance improvements in various state-of-the-art generative models, demonstrating EQ-VAE's effectiveness and versatility across different types of autoencoders."}, 'zh': {'title': 'EQ-VAEÔºöÊèêÂçáÊΩúÂú®ÁîüÊàêÊ®°ÂûãÊÄßËÉΩÁöÑÂÖ≥ÈîÆ', 'desc': 'ÊΩúÂú®ÁîüÊàêÊ®°ÂûãÂ∑≤Êàê‰∏∫È´òË¥®ÈáèÂõæÂÉèÂêàÊàêÁöÑ‰∏ªË¶ÅÊñπÊ≥ï„ÄÇËøô‰∫õÊ®°Âûã‰ΩøÁî®Ëá™ÁºñÁ†ÅÂô®Â∞ÜÂõæÂÉèÂéãÁº©Âà∞ÊΩúÂú®Á©∫Èó¥ÔºåÁÑ∂ÂêéÈÄöËøáÁîüÊàêÊ®°ÂûãÂ≠¶‰π†ÊΩúÂú®ÂàÜÂ∏É„ÄÇÊàë‰ª¨ÂèëÁé∞Áé∞ÊúâÁöÑËá™ÁºñÁ†ÅÂô®Âú®ËØ≠‰πâ‰øùÊåÅÂèòÊç¢ÔºàÂ¶ÇÁº©ÊîæÂíåÊóãËΩ¨ÔºâÊñπÈù¢Áº∫‰πèÁ≠âÂèòÊÄßÔºåÂØºËá¥Â§çÊùÇÁöÑÊΩúÂú®Á©∫Èó¥ÔºåÂΩ±ÂìçÁîüÊàêÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜEQ-VAEÔºåËøôÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ïÔºåÂèØ‰ª•Âú®ÊΩúÂú®Á©∫Èó¥‰∏≠Âº∫Âà∂Á≠âÂèòÊÄßÔºå‰ªéËÄåÈôç‰ΩéÂ§çÊùÇÊÄßËÄå‰∏çÈôç‰ΩéÈáçÂª∫Ë¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08826', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2502.08826', 'abstract': 'Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.', 'score': 1, 'issue_id': 2279, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': 'e299bbaebf315923', 'authors': ['Mohammad Mahdi Abootorabi', 'Amirhosein Zobeiri', 'Mahdi Dehghani', 'Mohammadali Mohammadkhani', 'Bardia Mohammadi', 'Omid Ghahroodi', 'Mahdieh Soleymani Baghshah', 'Ehsaneddin Asgari'], 'affiliations': ['College of Interdisciplinary Science and Technology, University of Tehran, Tehran, Iran', 'Computer Engineering Department, K.N. Toosi University of Technology, Tehran, Iran', 'Computer Engineering Department, Sharif University of Technology, Tehran, Iran', 'Qatar Computing Research Institute, Doha, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2502.08826.jpg', 'data': {'categories': ['#optimization', '#training', '#rag', '#multimodal', '#survey', '#benchmark', '#hallucinations'], 'emoji': 'ü§ñ', 'ru': {'title': '–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π RAG: –ù–æ–≤—ã–π —Ñ—Ä–æ–Ω—Ç–∏—Ä –≤ –æ–±–æ–≥–∞—â–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–∏—Å—Ç–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (Multimodal RAG), –∫–æ—Ç–æ—Ä—ã–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç –≤–Ω–µ—à–Ω—é—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏, –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –º–µ—Ç—Ä–∏–∫–∏ –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ RAG. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ—Ç–ª–∏—á–∞—é—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π RAG –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–¥–Ω–æ–º–æ–¥–∞–ª—å–Ω–æ–≥–æ. –°—Ç–∞—Ç—å—è —Ç–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞–µ—Ç –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤—ã–∑–æ–≤—ã –∏ –±—É–¥—É—â–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π —Ä–∞–∑–≤–∏–≤–∞—é—â–µ–π—Å—è –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'Enhancing AI with Multimodal Retrieval-Augmented Generation', 'desc': 'This paper discusses the limitations of Large Language Models (LLMs) in handling hallucinations and outdated information due to their static training data. It introduces Retrieval-Augmented Generation (RAG) as a solution that incorporates external, dynamic information to improve the accuracy and relevance of generated content. The paper further explores Multimodal RAG, which combines various data types like text, images, and audio to enhance output quality, while addressing the challenges of cross-modal alignment and reasoning. It provides a comprehensive analysis of methodologies, evaluation metrics, and future research directions to advance the development of more reliable AI systems that utilize multimodal knowledge effectively.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅRAGÔºöÊèêÂçáÁîüÊàêËÉΩÂäõÁöÑÊú™Êù•‰πãË∑Ø', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÂπªËßâÂíåËøáÊó∂Áü•ËØÜÊñπÈù¢Â≠òÂú®Âõ∞ÈöæÔºåÂõ†‰∏∫ÂÆÉ‰ª¨‰æùËµñ‰∫éÈùôÊÄÅËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÈÄöËøáÊï¥ÂêàÂ§ñÈÉ®Âä®ÊÄÅ‰ø°ÊÅØÊù•ÁºìËß£Ëøô‰∫õÈóÆÈ¢òÔºå‰ªéËÄåÂ¢ûÂº∫‰∫ãÂÆûÂíåÊõ¥Êñ∞ÁöÑÂü∫Á°Ä„ÄÇÊúÄËøëÁöÑÂ§öÊ®°ÊÄÅÂ≠¶‰π†ËøõÂ±ïÂØºËá¥‰∫ÜÂ§öÊ®°ÊÄÅRAGÁöÑÂèëÂ±ïÔºåÁªìÂêà‰∫ÜÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ëÂíåËßÜÈ¢ëÁ≠âÂ§öÁßçÊ®°ÊÄÅÔºå‰ª•Â¢ûÂº∫ÁîüÊàêÁöÑËæìÂá∫„ÄÇÁÑ∂ËÄåÔºåË∑®Ê®°ÊÄÅÂØπÈΩêÂíåÊé®ÁêÜ‰∏∫Â§öÊ®°ÊÄÅRAGÂ∏¶Êù•‰∫ÜÁã¨ÁâπÁöÑÊåëÊàòÔºå‰ΩøÂÖ∂‰∏é‰º†ÁªüÁöÑÂçïÊ®°ÊÄÅRAGÊúâÊâÄ‰∏çÂêå„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09969', 'title': 'Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.09969', 'abstract': 'Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.', 'score': 1, 'issue_id': 2278, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 14', 'zh': '2Êúà14Êó•'}, 'hash': '6530066ad48b1daf', 'authors': ['Ishika Agarwal', 'Dilek Hakkani-T√ºr'], 'affiliations': ['UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.09969.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#training', '#small_models'], 'emoji': 'üöÄ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –º–∞–ª—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π InfluenceNetwork. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–±–æ–ª—å—à–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–π –≤–ª–∏—è–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –¥–æ 99%. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º –≤—Å–µ–≥–æ 0,0027% –æ—Ç –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ. –ú–µ—Ç–æ–¥ NN-CIFT –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –ø–æ–∫–∞–∑—ã–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π –≤–ª–∏—è–Ω–∏—è –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —É—Å–∫–æ—Ä–µ–Ω–∏–∏.'}, 'en': {'title': 'Efficient Influence Estimation with InfluenceNetwork', 'desc': 'This paper introduces the InfluenceNetwork, a small neural network designed to efficiently estimate influence values in model training. Traditional methods are computationally expensive and struggle with large datasets, but the InfluenceNetwork achieves up to a 99% reduction in costs. The proposed method, NN-CIFT, allows for effective subset selection in instruction fine-tuning without sacrificing performance compared to existing influence functions. The authors also provide a detailed analysis of hyperparameters to optimize the performance of their approach.'}, 'zh': {'title': 'Â∞èÂûãÁ•ûÁªèÁΩëÁªúÂÆûÁé∞È´òÊïàÂΩ±Âìç‰º∞ËÆ°', 'desc': 'ÂΩ±ÂìçÂáΩÊï∞Âú®Ê®°ÂûãËÆ≠ÁªÉ‰∏≠Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑËßÅËß£Ôºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïËÆ°ÁÆóÊàêÊú¨È´ò‰∏îÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ∞èÂûãÁ•ûÁªèÁΩëÁªúÔºåÁß∞‰∏∫ÂΩ±ÂìçÁΩëÁªúÔºàInfluenceNetworkÔºâÔºåÁî®‰∫é‰º∞ËÆ°ÂΩ±ÂìçÂÄºÔºåÊàêÊú¨Èôç‰ΩéÈ´òËææ99%„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÔºàNN-CIFTÔºâÂú®ÈÄâÊã©Â≠êÈõÜËøõË°åÊåá‰ª§ÂæÆË∞ÉÁöÑ‰∏ãÊ∏∏‰ªªÂä°‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºå‰∏î‰∏é‰º†ÁªüÂΩ±ÂìçÂáΩÊï∞Áõ∏ÊØîÔºåÊÄßËÉΩÊ≤°ÊúâÂ¶•Âçè„ÄÇÈÄöËøáÂØπË∂ÖÂèÇÊï∞ÁöÑÊ∑±ÂÖ•ÂàÜÊûêÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÂ∞èÊ®°Âûã‰πüËÉΩÊúâÊïà‰º∞ËÆ°ÂΩ±ÂìçÂÄº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08820', 'title': 'Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model', 'url': 'https://huggingface.co/papers/2502.08820', 'abstract': 'Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CALM-IT, we train three models CALM 8B, CALM 70B, and CALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks.', 'score': 1, 'issue_id': 2274, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': 'f3f1c79c06903edb', 'authors': ['Emre Can Acikgoz', 'Jeremiah Greer', 'Akul Datta', 'Ze Yang', 'William Zeng', 'Oussama Elachqar', 'Emmanouil Koukoumidis', 'Dilek Hakkani-T√ºr', 'Gokhan Tur'], 'affiliations': ['Oumi', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.08820.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#benchmark', '#multimodal', '#dataset', '#agi', '#agents'], 'emoji': 'ü§ñ', 'ru': {'title': 'CALM: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ CALM (Conversational Agentic Language Model), –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º –∏ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç CALM-IT, —Å–æ—á–µ—Ç–∞—é—â–∏–π –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è ReAct —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ª–æ–∂–Ω—ã—Ö API. –ù–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω—ã –º–æ–¥–µ–ª–∏ CALM 8B, 70B –∏ 405B, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö: MultiWOZ 2.4, BFCL V3 –∏ API-Bank. CALM —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É —Å–∏—Å—Ç–µ–º–∞–º–∏ –∑–∞–¥–∞—á–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞ (TOD) –∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ (LA), –æ–±—ä–µ–¥–∏–Ω—è—è –∏—Ö —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã.'}, 'en': {'title': 'CALM: Bridging the Gap in Conversational AI', 'desc': 'This paper discusses the development of CALM, a new model that combines the strengths of Language Agents (LA) and task-oriented dialogue (TOD) systems. Traditional TOD systems struggle with limited API training and maintaining user intent over multiple interactions, while LAs lack robust multi-turn management. The authors introduce CALM-IT, a multi-task dataset that integrates reasoning and API usage, allowing for better training of conversational agents. The results show that CALM models significantly outperform existing specialized models on key benchmarks, demonstrating the effectiveness of this unified approach.'}, 'zh': {'title': 'CALMÔºöÂØπËØù‰∏é‰ª£ÁêÜËÉΩÂäõÁöÑÁªü‰∏ÄÊ®°Âûã', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂØπËØù‰ª£ÁêÜÊ®°ÂûãCALMÔºàConversational Agentic Language ModelÔºâÔºåÊó®Âú®Ëß£ÂÜ≥‰º†Áªü‰ªªÂä°ÂØºÂêëÂØπËØùÁ≥ªÁªüÔºàTODÔºâÂíåËØ≠Ë®Ä‰ª£ÁêÜÔºàLAÔºâÂú®Â§öËΩÆÂØπËØùÁÆ°ÁêÜÂíåÂäüËÉΩË∞ÉÁî®ÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇÂΩìÂâçÁöÑÁ≥ªÁªüÈÄöÂ∏∏Âú®ÊúâÈôêÁöÑÁõÆÊ†áAPI‰∏äËÆ≠ÁªÉÔºåÂØºËá¥Âú®‰∏éÊñ∞ÊúçÂä°‰∫§‰∫íÊó∂ÈúÄË¶ÅÊñ∞ÁöÑÊï∞ÊçÆÊù•Áª¥ÊåÅË¥®ÈáèÔºåËÄåËØ≠Ë®Ä‰ª£ÁêÜÂàôÊú™ËÉΩÊúâÊïà‰øùÊåÅÁî®Êà∑ÊÑèÂõæ„ÄÇÊàë‰ª¨ÈÄöËøáÂàõÂª∫CALM-ITÊï∞ÊçÆÈõÜÔºåÂ∞ÜÂ§öËΩÆÊé®ÁêÜ‰∏éÂ§çÊùÇAPI‰ΩøÁî®Áõ∏ÁªìÂêàÔºåËÆ≠ÁªÉ‰∫Ü‰∏âÁßç‰∏çÂêåËßÑÊ®°ÁöÑCALMÊ®°ÂûãÔºåÁªìÊûúÊòæÁ§∫Ëøô‰∫õÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÈ¢ÜÂüüÁâπÂÆöÊ®°Âûã„ÄÇËØ•Á†îÁ©∂Ë°®ÊòéÔºåCALMÊ®°ÂûãËÉΩÂ§üÊúâÊïàÊï¥ÂêàÂØπËØùËÉΩÂäõÂíå‰ª£ÁêÜËÉΩÂäõÔºåÊé®Âä®ÂØπËØùÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08441', 'title': 'Better Embeddings with Coupled Adam', 'url': 'https://huggingface.co/papers/2502.08441', 'abstract': 'Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.', 'score': 1, 'issue_id': 2271, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '4357e7dc6b15b5b7', 'authors': ['Felix Stollenwerk', 'Tobias Stollenwerk'], 'affiliations': ['AI Sweden', 'Forschungszentrum J√ºlich'], 'pdf_title_img': 'assets/pdf/title_img/2502.08441.jpg', 'data': {'categories': ['#training', '#optimization', '#dataset'], 'emoji': 'üî¨', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–∏–π —Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é Coupled Adam', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∞–Ω–∏–∑–æ—Ç—Ä–æ–ø–∏–∏ –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö —Å–ª–æ–≤, –æ–±—É—á–∞–µ–º—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –≤—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–µ Adam —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏—á–∏–Ω–æ–π –∞–Ω–∏–∑–æ—Ç—Ä–æ–ø–Ω—ã—Ö –≤–ª–æ–∂–µ–Ω–∏–π. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Coupled Adam –¥–ª—è —Å–º—è–≥—á–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Coupled Adam –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤–ª–æ–∂–µ–Ω–∏–π –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ª—É—á—à–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∫–∞–∫ –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–¥–∞—á –Ω–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Enhancing Embedding Quality with Coupled Adam', 'desc': 'This paper addresses the issue of anisotropic embeddings in large language models (LLMs), which can negatively impact their performance. The authors identify that the second moment in the Adam optimizer contributes to this anisotropy. To counteract this, they propose a new optimizer called Coupled Adam, which modifies the way embeddings are learned. Experimental results show that Coupled Adam not only enhances the quality of embeddings but also improves performance in both upstream and downstream tasks when applied to sufficiently large datasets.'}, 'zh': {'title': 'ÊîπËøõ‰ºòÂåñÂô®ÔºåÊèêÂçáÂµåÂÖ•Ë¥®Èáè', 'desc': 'Â∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂÖ∑ÊúâÂá∫Ëâ≤ÁöÑËÉΩÂäõÔºå‰ΩÜÂÆÉ‰ª¨Â≠¶‰π†ÁöÑËØçË°®Á§∫Â≠òÂú®‰∏ÄÁßç‰∏çÁêÜÊÉ≥‰∏îÂ∞öÊú™ÂÖÖÂàÜÁêÜËß£ÁöÑÁâπÂæÅÔºåÂç≥ÂêÑÂêëÂºÇÊÄß„ÄÇÊú¨ÊñáËÆ§‰∏∫ÔºåAdam‰ºòÂåñÂô®‰∏≠ÁöÑ‰∫åÈò∂Áü©ÊòØÂØºËá¥ÂêÑÂêëÂºÇÊÄßÂµåÂÖ•ÁöÑÂéüÂõ†ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Coupled AdamÁöÑÊîπËøõ‰ºòÂåñÂô®Êù•ÁºìËß£Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåCoupled AdamÊòæËëóÊèêÈ´ò‰∫ÜÂµåÂÖ•ÁöÑË¥®ÈáèÔºåÂêåÊó∂Âú®Ë∂≥Â§üÂ§ßÁöÑÊï∞ÊçÆÈõÜ‰∏ä‰πüÊèêÂçá‰∫Ü‰∏äÊ∏∏Âíå‰∏ãÊ∏∏ÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09083', 'title': "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking", 'url': 'https://huggingface.co/papers/2502.09083', 'abstract': "The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model's reasoning path, reference specific evidence, and highlight uncertainty and information gaps.", 'score': 1, 'issue_id': 2270, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'ecf7018a52bbede5', 'authors': ['Greta Warren', 'Irina Shklovski', 'Isabelle Augenstein'], 'affiliations': ['Link√∂ping University, Link√∂ping, Sweden', 'University of Copenhagen, Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2502.09083.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#ethics', '#healthcare', '#multimodal', '#data'], 'emoji': 'üîç', 'ru': {'title': '–û–±—ä—è—Å–Ω–∏–º–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ –≤ —ç–ø–æ—Ö—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∏–Ω—Ç–µ—Ä–≤—å—é —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ —Ñ–∞–∫—Ç—á–µ–∫–µ—Ä–∞–º–∏, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –æ–Ω–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ –ø—Ä–∏–Ω–∏–º–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –≤ –æ–±—ä—è—Å–Ω–∏–º—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Ñ–∞–∫—Ç—á–µ–∫–µ—Ä–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –ø—Ä–æ–∑—Ä–∞—á–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏—è—Ö, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö —Ö–æ–¥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–∏, —Å—Å—ã–ª–∞—é—â–∏—Ö—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–±–µ–ª—ã –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.'}, 'en': {'title': 'Bridging the Gap: Enhancing Automated Fact-Checking with Human-Centric Explanations', 'desc': 'This paper addresses the growing challenge of misinformation in online media by exploring the role of automated fact-checking systems. It highlights the necessity for these systems to provide clear explanations that align with how human fact-checkers evaluate evidence and make decisions. Through interviews with fact-checking professionals, the study identifies specific requirements for explanations that can enhance the integration of automated tools into fact-checking workflows. The findings reveal critical gaps in current explanation practices and suggest criteria for developing more effective automated fact-checking solutions.'}, 'zh': {'title': 'ÊèêÂçáËá™Âä®Âåñ‰∫ãÂÆûÊ†∏Êü•ÁöÑËß£ÈáäËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÂú®Âú®Á∫øÂ™í‰Ωì‰∏≠ÁöÑÂπøÊ≥õÂ∫îÁî®ÔºåÂº∫Ë∞É‰∫ÜËá™Âä®Âåñ‰∫ãÂÆûÊ†∏Êü•ÁöÑÂøÖË¶ÅÊÄßÔºå‰ª•Â∏ÆÂä©Ê†∏Êü•ÂëòÂ∫îÂØπÊó•ÁõäÂ¢ûÂä†ÂíåÂ§çÊùÇÂåñÁöÑËôöÂÅá‰ø°ÊÅØ„ÄÇÁ†îÁ©∂ÈÄöËøá‰∏é‰∫ãÂÆûÊ†∏Êü•‰∏ì‰∏ö‰∫∫Â£´ÁöÑÂçäÁªìÊûÑÂåñËÆøË∞àÔºåÂàÜÊûê‰∫ÜÊ†∏Êü•ÂëòÂ¶Ç‰ΩïËØÑ‰º∞ËØÅÊçÆ„ÄÅÂÅöÂá∫ÂÜ≥Á≠ñ‰ª•ÂèäËß£Èáä‰ªñ‰ª¨ÁöÑËøáÁ®ã„ÄÇËÆ∫ÊñáËøòËÄÉÂØü‰∫ÜÊ†∏Êü•ÂëòÂú®ÂÆûË∑µ‰∏≠Â¶Ç‰Ωï‰ΩøÁî®Ëá™Âä®ÂåñÂ∑•ÂÖ∑ÔºåÂπ∂ËØÜÂà´‰∫Ü‰ªñ‰ª¨ÂØπËá™Âä®Âåñ‰∫ãÂÆûÊ†∏Êü•Â∑•ÂÖ∑ÁöÑËß£ÈáäÈúÄÊ±Ç„ÄÇÁ†îÁ©∂ÁªìÊûúÊòæÁ§∫ÔºåÊ†∏Êü•ÂëòÂú®Ëß£ÈáäÊñπÈù¢Â≠òÂú®Êú™Êª°Ë∂≥ÁöÑÈúÄÊ±ÇÔºåÂπ∂Á°ÆÂÆö‰∫ÜÂèØÂ§çÂà∂ÁöÑ‰∫ãÂÆûÊ†∏Êü•Ëß£ÈáäÁöÑÈáçË¶ÅÊ†áÂáÜÔºåÂåÖÊã¨ËøΩË∏™Ê®°ÂûãÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÅÂºïÁî®ÂÖ∑‰ΩìËØÅÊçÆ‰ª•ÂèäÁ™ÅÂá∫‰∏çÁ°ÆÂÆöÊÄßÂíå‰ø°ÊÅØÁº∫Âè£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11574', 'title': 'Large Language Models and Mathematical Reasoning Failures', 'url': 'https://huggingface.co/papers/2502.11574', 'abstract': "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling.", 'score': 1, 'issue_id': 2268, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '478a2c4575e67b28', 'authors': ['Johan Boye', 'Birger Moell'], 'affiliations': ['KTH Royal Institute of Technology, Stockholm, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2502.11574.jpg', 'data': {'categories': ['#training', '#dataset', '#reasoning', '#math'], 'emoji': 'üßÆ', 'ru': {'title': '–†–∞—Å–∫—Ä—ã–≤–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ 50 –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á —É—Ä–æ–≤–Ω—è —Å—Ç–∞—Ä—à–µ–π —à–∫–æ–ª—ã. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤, –Ω–æ –∏ —Ö–æ–¥ —Ä–µ—à–µ–Ω–∏—è, –≤—ã—è–≤–ª—è—è –æ—à–∏–±–∫–∏ –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –û—Ü–µ–Ω–∫–∞ –≤–æ—Å—å–º–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –¥–∞–∂–µ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤, –≤—Å–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –æ—à–∏–±–∫–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏, —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏ –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–µ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤, –∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é –∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π.'}, 'en': {'title': 'Evaluating Reasoning, Not Just Answers in LLMs', 'desc': "This paper examines how well large language models (LLMs) can solve high-school-level math word problems by focusing on their reasoning abilities. It evaluates eight advanced models, revealing that while some newer models show better accuracy, they still struggle with spatial reasoning, strategic planning, and arithmetic. The analysis identifies common reasoning failures, such as making incorrect assumptions and having difficulty with multi-step deductions. The findings stress the importance of assessing the reasoning process in addition to the final answers, highlighting the need for improvements in LLMs' structured reasoning skills."}, 'zh': {'title': 'ËØÑ‰º∞Êé®ÁêÜËøáÁ®ãÔºåË∂ÖË∂äÁ≠îÊ°àÊ≠£Á°ÆÊÄß', 'desc': 'Êú¨ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõÔºå‰ΩøÁî®‰∫Ü50‰∏™Êñ∞ÊûÑÂª∫ÁöÑÈ´ò‰∏≠Ê∞¥Âπ≥ÁöÑÊñáÂ≠óÈóÆÈ¢ò„ÄÇ‰∏é‰ª•ÂæÄÂè™ÂÖ≥Ê≥®Á≠îÊ°àÊ≠£Á°ÆÊÄßÁöÑÁ†îÁ©∂‰∏çÂêåÔºåÊàë‰ª¨‰∏•Ê†ºÂàÜÊûê‰∫ÜÊúÄÁªàÁ≠îÊ°àÂíåËß£ÂÜ≥Ê≠•È™§Ôºå‰ª•ËØÜÂà´Êé®ÁêÜÂ§±Ë¥•„ÄÇËØÑ‰º∞‰∫ÜÂåÖÊã¨Mixtral„ÄÅLlama„ÄÅGemini„ÄÅGPT-4oÂíåOpenAIÁöÑo1Âèò‰ΩìÂú®ÂÜÖÁöÑÂÖ´‰∏™ÊúÄÂÖàËøõÊ®°ÂûãÔºåÂèëÁé∞Â∞ΩÁÆ°Êñ∞Ê®°ÂûãÔºàÂ¶Ço3-mini„ÄÅdeepseek-r1ÔºâÂú®ÂáÜÁ°ÆÊÄß‰∏äÊõ¥È´òÔºå‰ΩÜÊâÄÊúâÊ®°ÂûãÂú®Á©∫Èó¥Êé®ÁêÜ„ÄÅÊàòÁï•ËßÑÂàíÂíåÁÆóÊúØÊñπÈù¢ÈÉΩÂ≠òÂú®ÈîôËØØ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÂº∫Ë∞É‰∫ÜËØÑ‰º∞Êé®ÁêÜËøáÁ®ãÁöÑÈáçË¶ÅÊÄßÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÁ≠îÊ°àÔºåÂπ∂Ë≠¶Âëä‰∏çË¶ÅÈ´ò‰º∞LLMsÁöÑËß£ÂÜ≥ÈóÆÈ¢òËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11177', 'title': 'The Mirage of Model Editing: Revisiting Evaluation in the Wild', 'url': 'https://huggingface.co/papers/2502.11177', 'abstract': "Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.", 'score': 0, 'issue_id': 2273, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 16', 'zh': '2Êúà16Êó•'}, 'hash': 'd72ce28b4092ab0f', 'authors': ['Wanli Yang', 'Fei Sun', 'Jiajun Tan', 'Xinyu Ma', 'Qi Cao', 'Dawei Yin', 'Huawei Shen', 'Xueqi Cheng'], 'affiliations': ['Baidu Inc.', 'CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS', 'Huawei', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.11177.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#optimization', '#reasoning', '#training', '#dataset'], 'emoji': 'üîç', 'ru': {'title': '–ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –∏–ª–ª—é–∑–∏–∏ –∫ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ QAEdit –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Ä–∞–±–æ—Ç–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ, —á–µ–º —Å–æ–æ–±—â–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ, –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º –≤ –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö –æ—Ü–µ–Ω–∫–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Rethinking Model Editing: Bridging Theory and Real-World Performance', 'desc': 'This paper investigates the effectiveness of model editing techniques in question answering (QA) systems, particularly focusing on large language models (LLMs). The authors introduce QAEdit, a new benchmark and evaluation framework to rigorously assess how well these editing methods correct errors in LLMs. Their findings reveal that current editing methods perform significantly worse in real-world scenarios than previously reported, highlighting flaws in past evaluation practices. By simulating real-world conditions, they show that existing approaches struggle with even a small number of edits, prompting a reevaluation of model editing methods and their assessment.'}, 'zh': {'title': 'ÈáçÊñ∞ÂÆ°ËßÜÊ®°ÂûãÁºñËæëÁöÑÊúâÊïàÊÄß‰∏éËØÑ‰º∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊ®°ÂûãÁºñËæëÂú®ÈóÆÁ≠îÁ≥ªÁªü‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁúüÂÆûÂ∫îÁî®‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜQAEditÔºå‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Áé∞ÊúâÁºñËæëÊñπÊ≥ïÂú®Á∫†Ê≠£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈîôËØØÊó∂ÁöÑÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÁöÑÁºñËæëÊñπÊ≥ïÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑË°®Áé∞Ëøú‰Ωé‰∫é‰πãÂâçÁöÑÊä•ÂëäÔºåÂè™Êúâ38.5%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÈÄöËøáÊ®°ÂùóÂàÜÊûêÂíåÊéßÂà∂ÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞ËØÑ‰º∞ÂÆûË∑µ‰∏≠ÁöÑÈóÆÈ¢òÂØºËá¥‰∫ÜËøôÁßçÊÄßËÉΩ‰∏ãÈôçÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰∏•Ê†ºÁöÑËØÑ‰º∞Ê°ÜÊû∂Ôºå‰ª•Êé®Âä®Ê®°ÂûãÁºñËæëÁ†îÁ©∂ÁöÑÂèØÈù†ÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11578', 'title': 'Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance', 'url': 'https://huggingface.co/papers/2502.11578', 'abstract': "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.", 'score': 0, 'issue_id': 2268, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '039b4ffb618ff3b1', 'authors': ['Birger Moell', 'Johan Boye'], 'affiliations': ['KTH Royal Institute of Technology, Stockholm, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2502.11578.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#interpretability', '#science'], 'emoji': 'üìä', 'ru': {'title': '–ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–∞ –∫–∞–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–∞, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ LIX –∏ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (ADD). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ —à–≤–µ–¥—Å–∫–∏—Ö —ç—Å—Å–µ —É—Ä–æ–≤–Ω—è —Å—Ç–∞—Ä—à–µ–π —à–∫–æ–ª—ã –∏ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å ChatGPT-o1-mini –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∏–ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –æ–±–µ–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤—ã—á–∏—Å–ª–µ–Ω–∏—è LIX –∏ –æ–±—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MMLU, —á—Ç–æ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–∞ –∫–∞–∫ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–±—â–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM.'}, 'en': {'title': 'Evaluating Language Complexity as a Proxy for LLM Performance', 'desc': "This paper explores how well large language models (LLMs) can measure language complexity using specific metrics like the LIX readability score and Average Dependency Distance (ADD). The authors tested these models on Swedish essays from high school and university students to see how accurately they could compute LIX scores and perform dependency parsing. The results showed that while all models had some success, ChatGPT-o1-mini was the most reliable, achieving the best accuracy in both tasks. Furthermore, a strong correlation was found between the models' LIX computation accuracy and their performance on the MMLU benchmark, indicating that language complexity measurements can be useful for evaluating LLM capabilities without needing extensive datasets."}, 'zh': {'title': 'ËØ≠Ë®ÄÂ§çÊùÇÊÄßÊµãÈáèÔºöËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËØ≠Ë®ÄÂ§çÊùÇÊÄßÊµãÈáè‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÁâπÂà´ÊòØËÆ°ÁÆóLIXÂèØËØªÊÄßÊåáÊ†áÂíåÂπ≥Âùá‰æùËµñË∑ùÁ¶ªÔºàADDÔºâ„ÄÇÊàë‰ª¨‰ΩøÁî®ÁëûÂÖ∏È´ò‰∏≠ÂíåÂ§ßÂ≠¶ÁöÑËÆ∫ÊñáÊù•ËØÑ‰º∞Ê®°ÂûãËÆ°ÁÆóLIXÂàÜÊï∞ÂíåËøõË°å‰æùËµñËß£ÊûêÁöÑËÉΩÂäõÔºåÂπ∂Â∞ÜÁªìÊûú‰∏éÂ∑≤Âª∫Á´ãÁöÑÂü∫ÂáÜËøõË°åÊØîËæÉ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°ÊâÄÊúâÊ®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏äÈÉΩÊúâ‰∏ÄÂÆöËÉΩÂäõÔºå‰ΩÜChatGPT-o1-miniÂú®LIXËÆ°ÁÆóÂíå‰æùËµñËß£Êûê‰∏≠Ë°®Áé∞ÊúÄ‰∏∫‰∏ÄËá¥ÔºåÂáÜÁ°ÆÁéáÊúÄÈ´ò„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËßÇÂØüÂà∞Ê®°ÂûãÂú®ËÆ°ÁÆóLIXÊó∂ÁöÑÂáÜÁ°ÆÊÄß‰∏éÂÖ∂Âú®Â§ßËßÑÊ®°Â§ö‰ªªÂä°ËØ≠Ë®ÄÁêÜËß£ÔºàMMLUÔºâÂü∫ÂáÜ‰∏äÁöÑÊï¥‰ΩìË°®Áé∞‰πãÈó¥Â≠òÂú®ÊòæËëóÁöÑË¥üÁõ∏ÂÖ≥ÂÖ≥Á≥ª„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13923', 'title': 'Qwen2.5-VL Technical Report', 'url': 'https://huggingface.co/papers/2502.13923', 'abstract': 'We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.', 'score': 70, 'issue_id': 2311, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': 'b0f349bddafadc4c', 'authors': ['Shuai Bai', 'Keqin Chen', 'Xuejing Liu', 'Jialin Wang', 'Wenbin Ge', 'Sibo Song', 'Kai Dang', 'Peng Wang', 'Shijie Wang', 'Jun Tang', 'Humen Zhong', 'Yuanzhi Zhu', 'Mingkun Yang', 'Zhaohai Li', 'Jianqiang Wan', 'Pengfei Wang', 'Wei Ding', 'Zheren Fu', 'Yiheng Xu', 'Jiabo Ye', 'Xi Zhang', 'Tianbao Xie', 'Zesen Cheng', 'Hang Zhang', 'Zhibo Yang', 'Haiyang Xu', 'Junyang Lin'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.13923.jpg', 'data': {'categories': ['#agi', '#architecture', '#cv', '#multimodal', '#long_context', '#agents', '#reasoning'], 'emoji': 'üîç', 'ru': {'title': '–ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å Qwen2.5-VL', 'desc': 'Qwen2.5-VL - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏ –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤. Qwen2.5-VL –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ç—Ä–µ—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –∏ —Å—Ä–∞–≤–Ω–∏–º–∞ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'Revolutionizing Vision-Language Interaction with Qwen2.5-VL', 'desc': 'Qwen2.5-VL is a cutting-edge vision-language model that enhances the understanding and interaction with visual data. It features advanced capabilities like precise object localization, effective document parsing, and the ability to comprehend long videos. The model utilizes dynamic resolution processing and absolute time encoding to handle complex inputs, allowing it to analyze images and videos of various sizes efficiently. With its robust performance in both visual and linguistic tasks, Qwen2.5-VL serves as an interactive agent for real-world applications, from edge AI to high-performance computing.'}, 'zh': {'title': 'Qwen2.5-VLÔºöËßÜËßâ‰∏éËØ≠Ë®ÄÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'Qwen2.5-VLÊòØQwenËßÜËßâËØ≠Ë®ÄÁ≥ªÂàóÁöÑÊúÄÊñ∞ÊóóËà∞Ê®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂü∫Á°ÄËÉΩÂäõÂíåÂàõÊñ∞ÂäüËÉΩÁöÑÊòæËëóËøõÊ≠•„ÄÇÂÆÉÂú®ËßÜËßâËØÜÂà´„ÄÅÁâ©‰ΩìÂÆö‰Ωç„ÄÅÊñáÊ°£Ëß£ÊûêÂíåÈïøËßÜÈ¢ëÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÈáçÂ§ßÁ™ÅÁ†¥„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üÂáÜÁ°ÆÂú∞‰ΩøÁî®ËæπÁïåÊ°ÜÊàñÁÇπÊù•ÂÆö‰ΩçÁâ©‰ΩìÔºåÂπ∂‰ªéÂèëÁ•®„ÄÅË°®ÂçïÂíåË°®Ê†º‰∏≠ÊèêÂèñÁªìÊûÑÂåñÊï∞ÊçÆ„ÄÇÈÄöËøáÂä®ÊÄÅÂàÜËæ®ÁéáÂ§ÑÁêÜÂíåÁªùÂØπÊó∂Èó¥ÁºñÁ†ÅÔºåQwen2.5-VLËÉΩÂ§üÂ§ÑÁêÜ‰∏çÂêåÂ§ßÂ∞èÁöÑÂõæÂÉèÂíåÈïøËææÊï∞Â∞èÊó∂ÁöÑËßÜÈ¢ëÔºåÊàê‰∏∫‰∏Ä‰∏™ËÉΩÂ§üÂú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠ËøõË°åÊé®ÁêÜÂíå‰ªªÂä°ÊâßË°åÁöÑ‰∫íÂä®ËßÜËßâ‰ª£ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13144', 'title': 'RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.13144', 'abstract': 'Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the open-loop gap. In this work, we establish a 3DGS-based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct a photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as a regularization term. We introduce a closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3x lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github.io/RAD.', 'score': 27, 'issue_id': 2309, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '0db330614af75888', 'authors': ['Hao Gao', 'Shaoyu Chen', 'Bo Jiang', 'Bencheng Liao', 'Yiang Shi', 'Xiaoyang Guo', 'Yuechuan Pu', 'Haoran Yin', 'Xiangyu Li', 'Xinbang Zhang', 'Ying Zhang', 'Wenyu Liu', 'Qian Zhang', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.13144.jpg', 'data': {'categories': ['#rl', '#benchmark', '#alignment', '#3d', '#games', '#reasoning', '#agents'], 'emoji': 'üöó', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–º–∏—Ä–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –≤–æ–∂–¥–µ–Ω–∏—é, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º—É RAD, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–∏—Ç–∏–∫–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏ —É—á–∏—Ç—å—Å—è —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —Å–∏—Ç—É–∞—Ü–∏—è–º–∏ —á–µ—Ä–µ–∑ –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∞ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é –ø—Ä–∏ –≤–æ–∂–¥–µ–Ω–∏–∏ –±—ã–ª–æ –≤–∫–ª—é—á–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ –ø–æ –∏–º–∏—Ç–∞—Ü–∏–∏ (IL) –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RAD –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–µ—Ç–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –Ω–∞ IL, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–Ω–∏–∂–µ–Ω–∏–∏ —á–∞—Å—Ç–æ—Ç—ã —Å—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–π.'}, 'en': {'title': 'Revolutionizing Autonomous Driving with Closed-Loop Reinforcement Learning', 'desc': "This paper presents a new approach to autonomous driving using a closed-loop Reinforcement Learning (RL) paradigm, addressing limitations of traditional Imitation Learning (IL). By creating a realistic 3D digital environment, the model can explore various driving scenarios and learn from trial and error, improving its ability to handle unexpected situations. The authors introduce specialized rewards to enhance safety by teaching the model to react appropriately to critical events and understand causal relationships in driving. Additionally, they incorporate IL as a regularization term to better align the model's behavior with human driving patterns, resulting in significantly improved performance metrics, including a lower collision rate."}, 'zh': {'title': 'Èó≠ÁéØÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáËá™‰∏ªÈ©æÈ©∂ÂÆâÂÖ®ÊÄß‰∏éÊÄßËÉΩ', 'desc': 'Áé∞ÊúâÁöÑÁ´ØÂà∞Á´ØËá™‰∏ªÈ©æÈ©∂ÁÆóÊ≥ïÈÄöÂ∏∏ÈááÁî®Ê®°‰ªøÂ≠¶‰π†ÔºàILÔºâÊñπÊ≥ïÔºå‰ΩÜÈù¢‰∏¥Âõ†ÊûúÊ∑∑Ê∑ÜÂíåÂºÄÊîæÁéØË∑ØÂ∑ÆË∑ùÁ≠âÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫é3DGSÁöÑÈó≠ÁéØÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËÆ≠ÁªÉËåÉÂºèÔºåÈÄöËøáÊûÑÂª∫ÁúüÂÆûÁâ©ÁêÜ‰∏ñÁïåÁöÑÈÄºÁúüÊï∞Â≠óÂ§çÂà∂ÂìÅÔºå‰ΩøËá™‰∏ªÈ©æÈ©∂Á≠ñÁï•ËÉΩÂ§üÂπøÊ≥õÊé¢Á¥¢Áä∂ÊÄÅÁ©∫Èó¥ÔºåÂπ∂ÈÄöËøáÂ§ßËßÑÊ®°ËØïÈîôÂ≠¶‰π†Â§ÑÁêÜÂàÜÂ∏ÉÂ§ñÂú∫ÊôØ„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÂÆâÂÖ®ÊÄßÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ìÈó®ÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåÂºïÂØºÁ≠ñÁï•ÊúâÊïàÂ∫îÂØπÂÆâÂÖ®ÂÖ≥ÈîÆ‰∫ã‰ª∂ÔºåÂπ∂ÁêÜËß£Áé∞ÂÆû‰∏ñÁïåÁöÑÂõ†ÊûúÂÖ≥Á≥ª„ÄÇÂêåÊó∂Ôºå‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞‰∏é‰∫∫Á±ªÈ©æÈ©∂Ë°å‰∏∫ÂØπÈΩêÔºåÊàë‰ª¨Â∞ÜÊ®°‰ªøÂ≠¶‰π†‰Ωú‰∏∫Ê≠£ÂàôÂåñÈ°πËûçÂÖ•Âà∞Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∏≠„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13128', 'title': 'SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation', 'url': 'https://huggingface.co/papers/2502.13128', 'abstract': 'Text-to-song generation, the task of creating vocals and accompaniment from textual inputs, poses significant challenges due to domain complexity and data scarcity. Existing approaches often employ multi-stage generation procedures, resulting in cumbersome training and inference pipelines. In this paper, we propose SongGen, a fully open-source, single-stage auto-regressive transformer designed for controllable song generation. The proposed model facilitates fine-grained control over diverse musical attributes, including lyrics and textual descriptions of instrumentation, genre, mood, and timbre, while also offering an optional three-second reference clip for voice cloning. Within a unified auto-regressive framework, SongGen supports two output modes: mixed mode, which generates a mixture of vocals and accompaniment directly, and dual-track mode, which synthesizes them separately for greater flexibility in downstream applications. We explore diverse token pattern strategies for each mode, leading to notable improvements and valuable insights. Furthermore, we design an automated data preprocessing pipeline with effective quality control. To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline. The generated samples are showcased on our project page at https://liuzh-19.github.io/SongGen/ , and the code will be available at https://github.com/LiuZH-19/SongGen .', 'score': 24, 'issue_id': 2312, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '30ac46f3e428e6cf', 'authors': ['Zihan Liu', 'Shuangrui Ding', 'Zhixiong Zhang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Zang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Beihang University, Beijing, China', 'Shanghai AI Laboratory, Shanghai, China', 'The Chinese University of Hong Kong, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.13128.jpg', 'data': {'categories': ['#story_generation', '#data', '#audio', '#training', '#open_source', '#dataset'], 'emoji': 'üéµ', 'ru': {'title': 'SongGen: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Å–µ–Ω —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': 'SongGen - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Å–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–∑—ã–∫–∞–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã, –≤–∫–ª—é—á–∞—è —Ç–µ–∫—Å—Ç –ø–µ—Å–Ω–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤–∫—É, –∂–∞–Ω—Ä –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –¥–≤—É—Ö —Ä–µ–∂–∏–º–∞—Ö: —Å–º–µ—à–∞–Ω–Ω–æ–º (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–∫–∞–ª–∞ –∏ –∞–∫–∫–æ–º–ø–∞–Ω–µ–º–µ–Ω—Ç–∞ –≤–º–µ—Å—Ç–µ) –∏ –¥–≤—É—Ö–¥–æ—Ä–æ–∂–µ—á–Ω–æ–º (—Ä–∞–∑–¥–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è). –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–ª–∞–Ω–∏—Ä—É—é—Ç –æ—Ç–∫—Ä—ã—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.'}, 'en': {'title': 'SongGen: Simplifying Text-to-Song Generation with a Unified Model', 'desc': 'This paper introduces SongGen, a novel model for generating songs from text inputs using a single-stage auto-regressive transformer. It addresses the challenges of text-to-song generation by allowing fine control over various musical elements such as lyrics, genre, and mood. The model offers two output modes: mixed mode for simultaneous vocal and accompaniment generation, and dual-track mode for separate synthesis, enhancing flexibility for users. Additionally, the authors provide an automated data preprocessing pipeline and commit to releasing their resources to support further research in this area.'}, 'zh': {'title': 'SongGenÔºöÂèØÊéßÁöÑÊ≠åÊõ≤ÁîüÊàêÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SongGenÁöÑÊñáÊú¨Âà∞Ê≠åÊõ≤ÁîüÊàêÊ®°ÂûãÔºåÊó®Âú®‰ªéÊñáÊú¨ËæìÂÖ•‰∏≠ÁîüÊàêÊ≠åËØçÂíå‰º¥Â•è„ÄÇËØ•Ê®°ÂûãÈááÁî®ÂçïÈò∂ÊÆµËá™ÂõûÂΩíÂèòÊç¢Âô®ÔºåËÉΩÂ§üÂØπÂ§öÁßçÈü≥‰πêÂ±ûÊÄßËøõË°åÁ≤æÁªÜÊéßÂà∂ÔºåÂ¶ÇÊ≠åËØç„ÄÅ‰πêÂô®ÊèèËø∞„ÄÅÈ£éÊ†º„ÄÅÊÉÖÁª™ÂíåÈü≥Ëâ≤„ÄÇSongGenÊîØÊåÅ‰∏§ÁßçËæìÂá∫Ê®°ÂºèÔºöÊ∑∑ÂêàÊ®°ÂºèÂíåÂèåËΩ®Ê®°ÂºèÔºåÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄß‰ª•Êª°Ë∂≥‰∏çÂêåÂ∫îÁî®ÈúÄÊ±Ç„ÄÇÊ≠§Â§ñÔºåÁ†îÁ©∂Âõ¢ÈòüËøòËÆæËÆ°‰∫ÜËá™Âä®ÂåñÁöÑÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÁÆ°ÈÅìÔºå‰ª•Á°Æ‰øùÁîüÊàêÊ†∑Êú¨ÁöÑË¥®ÈáèÔºåÂπ∂ËÆ°ÂàíÂºÄÊîæÊ®°ÂûãÊùÉÈáçÂíåËÆ≠ÁªÉ‰ª£Á†Å‰ª•‰øÉËøõÁ§æÂå∫ÂèÇ‰∏é„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13685', 'title': 'MoM: Linear Sequence Modeling with Mixture-of-Memories', 'url': 'https://huggingface.co/papers/2502.13685', 'abstract': 'Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain\'s ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 19, 'issue_id': 2314, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': 'a9ffa1bb1a5e0137', 'authors': ['Jusen Du', 'Weigao Sun', 'Disen Lan', 'Jiaxi Hu', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.13685.jpg', 'data': {'categories': ['#long_context', '#architecture', '#training', '#open_source'], 'emoji': 'üß†', 'ru': {'title': '–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixture-of-Memories (MoM) –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. MoM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–∞–º—è—Ç–∏ –∏ —Å–µ—Ç—å-–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞–º—è—Ç–∏. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –æ–±—â—É—é –µ–º–∫–æ—Å—Ç—å –ø–∞–º—è—Ç–∏, –º–∏–Ω–∏–º–∏–∑–∏—Ä—É—è –ø—Ä–∏ —ç—Ç–æ–º –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏—é –ø–∞–º—è—Ç–∏. MoM –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–≥–æ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ª–∏–Ω–µ–π–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –¥–∞–∂–µ –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –º–æ–¥–µ–ª—è–º–∏ Transformer.'}, 'en': {'title': 'Unlocking Memory Potential with Mixture-of-Memories', 'desc': 'This paper presents a new architecture called Mixture-of-Memories (MoM) that improves linear sequence modeling methods by using multiple independent memory states. Unlike traditional models that compress input sequences into a single memory state, MoM employs a router network to direct tokens to specific memory states, enhancing memory capacity and reducing interference. This design allows MoM to excel in recall-intensive tasks while maintaining linear complexity during training and constant complexity during inference. Experimental results demonstrate that MoM outperforms existing linear models and achieves performance levels comparable to Transformer models on various language tasks.'}, 'zh': {'title': 'Ê∑∑ÂêàËÆ∞ÂøÜÔºöÊèêÂçáËÆ∞ÂøÜËÉΩÂäõÁöÑÁ∫øÊÄßÂ∫èÂàóÂª∫Ê®°Êñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ®°ÂûãÊû∂ÊûÑÔºåÁß∞‰∏∫Ê∑∑ÂêàËÆ∞ÂøÜÔºàMixture-of-Memories, MoMÔºâÔºåÊó®Âú®ÊèêÈ´òÁ∫øÊÄßÂ∫èÂàóÂª∫Ê®°ÊñπÊ≥ïÂú®ËÆ∞ÂøÜÂØÜÈõÜÂûã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇMoMÈÄöËøá‰ΩøÁî®Â§ö‰∏™Áã¨Á´ãÁöÑËÆ∞ÂøÜÁä∂ÊÄÅÔºåÂπ∂ÈÄöËøáË∑ØÁî±ÁΩëÁªúÂ∞ÜËæìÂÖ•‰ª§ÁâåÂàÜÈÖçÂà∞ÁâπÂÆöÁöÑËÆ∞ÂøÜÁä∂ÊÄÅÔºå‰ªéËÄåÂ¢ûÂº∫‰∫ÜÊï¥‰ΩìËÆ∞ÂøÜÂÆπÈáèÂπ∂ÂáèÂ∞ë‰∫ÜËÆ∞ÂøÜÂπ≤Êâ∞„ÄÇÂ∞ΩÁÆ°ÂºïÂÖ•‰∫ÜÂ§ö‰∏™ËÆ∞ÂøÜÁä∂ÊÄÅÔºåMoMÂú®ËÆ≠ÁªÉÊó∂‰ªç‰øùÊåÅÁ∫øÊÄßÂ§çÊùÇÂ∫¶ÔºåÂú®Êé®ÁêÜÊó∂Âàô‰øùÊåÅÂ∏∏Êï∞Â§çÊùÇÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMoMÂú®‰∏ãÊ∏∏ËØ≠Ë®Ä‰ªªÂä°ÔºåÂ∞§ÂÖ∂ÊòØËÆ∞ÂøÜÂØÜÈõÜÂûã‰ªªÂä°‰∏äÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÁ∫øÊÄßÂ∫èÂàóÊ®°ÂûãÔºåÁîöËá≥ËææÂà∞‰∫Ü‰∏éTransformerÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13347', 'title': 'Craw4LLM: Efficient Web Crawling for LLM Pretraining', 'url': 'https://huggingface.co/papers/2502.13347', 'abstract': "Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Crawl4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Crawl4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Crawl4LLM.", 'score': 18, 'issue_id': 2310, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': 'dd055f606e1ddfe2', 'authors': ['Shi Yu', 'Zhiyuan Liu', 'Chenyan Xiong'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'School of Computer Science, Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2502.13347.jpg', 'data': {'categories': ['#dataset', '#graphs', '#open_source', '#data'], 'emoji': 'üï∑Ô∏è', 'ru': {'title': '–£–º–Ω—ã–π –≤–µ–±-–∫—Ä–∞—É–ª–∏–Ω–≥ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Crawl4LLM - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –≤–µ–±-–∫—Ä–∞—É–ª–∏–Ω–≥–∞ –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –∫—Ä–∞—É–ª–µ—Ä–∞, –∑–∞–º–µ–Ω—è—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –≥—Ä–∞—Ñ–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –≥—Ä–∞—Ñ–µ –∏–∑ 900 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–∫–∞–∑–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å Crawl4LLM –≤ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è. –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—Å–µ–≥–æ 21% URL –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–µ—Ö –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ –∏ –ø—Ä–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–∞—Ö –∫ –∫—Ä–∞—É–ª–∏–Ω–≥—É.'}, 'en': {'title': 'Crawl Smart: Boosting LLMs with Efficient Web Crawling', 'desc': 'This paper introduces Crawl4LLM, a novel web crawling technique designed to enhance the quality of pretraining data for large language models (LLMs). Instead of relying on traditional methods that prioritize web page connectivity, Crawl4LLM uses a priority score based on the potential influence of a webpage on LLM performance. The method was tested on a vast web graph with 900 million pages, showing that it can achieve comparable downstream performance with only 21% of the URLs crawled. This approach not only improves data quality but also minimizes the environmental impact of web crawling by reducing unnecessary data collection.'}, 'zh': {'title': 'È´òÊïàÁà¨Ëô´ÔºåÊèêÂçáLLMÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆË¥®Èáè', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Crawl4LLMÁöÑÈ´òÊïàÁΩëÁªúÁà¨Ëô´ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÁöÑË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÁΩëÈ°µÂú®LLMÈ¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÂΩ±ÂìçÂäõ‰Ωú‰∏∫‰ºòÂÖàÁ∫ßËØÑÂàÜÔºå‰ºòÂåñ‰∫ÜÁà¨Ëô´Ë∞ÉÂ∫¶Âô®ÁöÑÂ∑•‰ΩúÔºåËÄå‰∏çÊòØ‰æùËµñ‰∫é‰º†ÁªüÁöÑÂõæËøûÊé•ÊÄß‰ºòÂÖàÁ∫ß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCrawl4LLMÂú®‰ªÖÁà¨Âèñ21%ÁöÑÁΩëÂùÄÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üËé∑Âæó‰∏é‰πãÂâçÁà¨ÂèñÁõ∏ÂêåÁöÑ‰∏ãÊ∏∏ÊÄßËÉΩÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÁà¨ÂèñÊµ™Ë¥π„ÄÇÊ≠§ÊñπÊ≥ï‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÊï∞ÊçÆË¥®ÈáèÔºåËøòÂáèËΩª‰∫ÜÂØπÁΩëÁ´ôÁöÑË¥üÊãÖ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13922', 'title': 'LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization', 'url': 'https://huggingface.co/papers/2502.13922', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transferring short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully retains short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, \\ourMethod-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales.', 'score': 18, 'issue_id': 2309, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': '93cf8365ba7edb80', 'authors': ['Guanzheng Chen', 'Xin Li', 'Michael Qizhe Shieh', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab, 310023, Hangzhou, China', 'National University of Singapore', 'Shanda AI Research Institute'], 'pdf_title_img': 'assets/pdf/title_img/2502.13922.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#benchmark', '#long_context', '#architecture', '#rlhf'], 'emoji': 'üìè', 'ru': {'title': 'LongPO: –°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LongPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. LongPO –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º, –æ–±—É—á–µ–Ω–Ω—ã–º –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö, —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º, –∏—Å–ø–æ–ª—å–∑—É—è —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ KL –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LongPO –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –Ω–∞–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.'}, 'en': {'title': 'Empowering Short-Context LLMs for Long-Context Mastery', 'desc': 'This paper presents LongPO, a method designed to enhance the performance of short-context Large Language Models (LLMs) on long-context tasks. The challenge arises from the difficulty of aligning LLMs for long contexts due to the lack of human-annotated data and the need to balance performance across different context lengths. LongPO allows LLMs to learn from their own generated data, creating a preference model that helps them adapt their short-context skills to long-context scenarios. The results show that models trained with LongPO maintain their short-context performance while significantly improving their long-context capabilities, even rivaling more advanced models like GPT-4.'}, 'zh': {'title': 'LongPOÔºöÁü≠‰∏ä‰∏ãÊñáÊ®°ÂûãÁöÑÈïø‰∏ä‰∏ãÊñáËá™ÊàëÊºîÂåñ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®È¢ÑËÆ≠ÁªÉÂíåÂØπÈΩêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Èïø‰∏ä‰∏ãÊñáÂú∫ÊôØ‰∏≠ÂèØËÉΩË°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜLongPOÊñπÊ≥ïÔºå‰ΩøÁü≠‰∏ä‰∏ãÊñáÁöÑLLMsËÉΩÂ§üËá™ÊàëÊºîÂåñÔºå‰ª•Âú®Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇLongPOÈÄöËøáÁîüÊàêÁü≠Âà∞ÈïøÁöÑÂÅèÂ•ΩÊï∞ÊçÆÔºåÂ∏ÆÂä©Ê®°ÂûãÂ≠¶‰π†Â¶Ç‰ΩïÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠‰øùÊåÅÁü≠‰∏ä‰∏ãÊñáÁöÑËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®LongPOÁöÑÊ®°ÂûãÂú®Èïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁîöËá≥ÂèØ‰ª•‰∏éÊõ¥Âº∫Â§ßÁöÑLLMsÁõ∏Â™≤Áæé„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12143', 'title': 'Small Models Struggle to Learn from Strong Reasoners', 'url': 'https://huggingface.co/papers/2502.12143', 'abstract': 'Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models (leq3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.', 'score': 15, 'issue_id': 2309, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '5abea4fb025815c2', 'authors': ['Yuetai Li', 'Xiang Yue', 'Zhangchen Xu', 'Fengqing Jiang', 'Luyao Niu', 'Bill Yuchen Lin', 'Bhaskar Ramasubramanian', 'Radha Poovendran'], 'affiliations': ['Carnegie Mellon University', 'University of Washington', 'Western Washington University'], 'pdf_title_img': 'assets/pdf/title_img/2502.12143.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#optimization', '#reasoning', '#small_models'], 'emoji': 'üß†', 'ru': {'title': '–ê–¥–∞–ø—Ç–∞—Ü–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–∞–ª–µ–Ω—å–∫–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (–¥–æ 3 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –Ω–µ –≤—Å–µ–≥–¥–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–∫–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–ª–∏ –ø—Ä–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –æ—Ç –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ç–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏ –ø—Ä–æ—Å—Ç—ã—Ö —Ü–µ–ø–æ—á–∫–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Mix Distillation, —Å–æ—á–µ—Ç–∞—é—â–∏–π –¥–ª–∏–Ω–Ω—ã–µ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–∞–ª–µ–Ω—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º.'}, 'en': {'title': 'Bridging the Learnability Gap for Small Models with Mix Distillation', 'desc': 'This paper explores the challenges faced by small language models (with 3 billion parameters or fewer) in learning from complex reasoning tasks. It identifies a phenomenon called the Small Model Learnability Gap, where these smaller models do not gain advantages from long chain-of-thought (CoT) reasoning or direct distillation from larger models. Instead, they perform better when trained on shorter, simpler reasoning chains that match their learning abilities. To improve their performance, the authors propose a method called Mix Distillation, which combines both long and short reasoning examples, leading to better reasoning outcomes for small models.'}, 'zh': {'title': 'Ê∑∑ÂêàËí∏È¶èÔºöÊèêÂçáÂ∞èÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÊúâÊïàÁ≠ñÁï•', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÊàë‰ª¨ÂèëÁé∞Â∞èÊ®°ÂûãÔºàÂèÇÊï∞Â∞è‰∫éÁ≠â‰∫é3‰∫øÔºâÂú®ÈïøÈìæÊé®ÁêÜÊàñ‰ªéÂ§ßÊ®°ÂûãËí∏È¶è‰∏≠Âπ∂‰∏çÊÄªÊòØÂèóÁõä„ÄÇÁõ∏ÂèçÔºåÂÆÉ‰ª¨Âú®Áü≠Â∞èÁÆÄÂçïÁöÑÊé®ÁêÜÈìæ‰∏äÂæÆË∞ÉÊó∂Ë°®Áé∞Êõ¥Â•ΩÔºåËøô‰∏éÂÆÉ‰ª¨ÁöÑÂ≠¶‰π†ËÉΩÂäõÊõ¥‰∏∫Â•ëÂêà„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊ∑∑ÂêàËí∏È¶èÔºàMix DistillationÔºâÁ≠ñÁï•ÔºåÈÄöËøáÁªìÂêàÈïøÁü≠Êé®ÁêÜÁ§∫‰æãÊàñÊù•Ëá™Â§ßÊ®°ÂûãÂíåÂ∞èÊ®°ÂûãÁöÑÊé®ÁêÜÔºåÂπ≥Ë°°Êé®ÁêÜÂ§çÊùÇÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåÊ∑∑ÂêàËí∏È¶èÊòæËëóÊèêÈ´ò‰∫ÜÂ∞èÊ®°ÂûãÁöÑÊé®ÁêÜÊÄßËÉΩÔºåÂº∫Ë∞É‰∫ÜÁõ¥Êé•Âº∫Ê®°ÂûãËí∏È¶èÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂Á™ÅÂá∫‰∫ÜÈÄÇÂ∫îÊé®ÁêÜÂ§çÊùÇÊÄßÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13965', 'title': 'Autellix: An Efficient Serving Engine for LLM Agents as General Programs', 'url': 'https://huggingface.co/papers/2502.13965', 'abstract': "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.", 'score': 12, 'issue_id': 2310, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': 'f64be1cc6aba8b4c', 'authors': ['Michael Luo', 'Xiaoxiang Shi', 'Colin Cai', 'Tianjun Zhang', 'Justin Wong', 'Yichuan Wang', 'Chi Wang', 'Yanping Huang', 'Zhifeng Chen', 'Joseph E. Gonzalez', 'Ion Stoica'], 'affiliations': ['Google DeepMind', 'Shanghai Jiao Tong University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.13965.jpg', 'data': {'categories': ['#optimization', '#inference', '#agents', '#architecture'], 'emoji': 'üöÄ', 'ru': {'title': 'Autellix: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–∏ LLM –¥–ª—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Autellix - —Å–∏—Å—Ç–µ–º—É –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â—É—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º. Autellix —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–∞–º–º—ã –∫–∞–∫ –æ–±—ä–µ–∫—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞, –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞—è –≤—ã–∑–æ–≤—ã LLM –∏ –æ–±–æ–≥–∞—â–∞—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ–≥—Ä–∞–º–º. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –¥–≤–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–ø–æ—Ç–æ—á–Ω—ã—Ö –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É—é—Ç –≤—ã–∑–æ–≤—ã LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–Ω–µ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Autellix —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º –≤ 4-15 —Ä–∞–∑ –ø—Ä–∏ —Ç–æ–π –∂–µ –∑–∞–¥–µ—Ä–∂–∫–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏.'}, 'en': {'title': 'Autellix: Optimizing LLM Serving for Enhanced Throughput', 'desc': 'This paper discusses the evolution of large language models (LLMs) from simple chatbots to more complex, agentic programs that can perform a variety of tasks. It identifies a significant issue in current LLM serving systems, which overlook the dependencies between different programs and their calls, leading to inefficiencies and long wait times. The authors introduce Autellix, a new LLM serving system that optimizes these processes by treating programs as first-class entities and enhancing scheduling with program-level context. Their proposed scheduling algorithms significantly improve the throughput of LLM programs, achieving 4-15 times better performance compared to existing systems while maintaining similar latency.'}, 'zh': {'title': '‰ºòÂåñLLMË∞ÉÁî®ÔºåÊèêÂçáÁ®ãÂ∫èÊÄßËÉΩÁöÑAutellix', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊúçÂä°Á≥ªÁªüÔºåÂêç‰∏∫Autellix„ÄÇAutellixÈÄöËøáÂ∞ÜÁ®ãÂ∫èËßÜ‰∏∫Á¨¨‰∏ÄÁ±ªÂÖ¨Ê∞ëÔºå‰ºòÂåñ‰∫ÜLLMË∞ÉÁî®ÁöÑË∞ÉÂ∫¶Ôºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÊï¥‰ΩìÂª∂Ëøü„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁé∞ÊúâÁöÑLLMÊúçÂä°Á≥ªÁªüÂøΩËßÜ‰∫ÜÁ®ãÂ∫è‰∏éË∞ÉÁî®‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåÂØºËá¥‰∫ÜÈïøÊó∂Èó¥ÁöÑÁ≠âÂæÖ„ÄÇÈÄöËøáÂºïÂÖ•Êñ∞ÁöÑË∞ÉÂ∫¶ÁÆóÊ≥ïÔºåAutellixÂú®Â§öÁßçLLMÂíå‰ªªÂä°Ë¥üËΩΩ‰∏ãÔºåÊèêÂçá‰∫ÜÁ®ãÂ∫èÁöÑÂêûÂêêÈáèÔºåÊïàÊûúÊòæËëó„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11995', 'title': 'Presumed Cultural Identity: How Names Shape LLM Responses', 'url': 'https://huggingface.co/papers/2502.11995', 'abstract': 'Names are deeply tied to human identity. They can serve as markers of individuality, cultural heritage, and personal history. However, using names as a core indicator of identity can lead to over-simplification of complex identities. When interacting with LLMs, user names are an important point of information for personalisation. Names can enter chatbot conversations through direct user input (requested by chatbots), as part of task contexts such as CV reviews, or as built-in memory features that store user information for personalisation. We study biases associated with names by measuring cultural presumptions in the responses generated by LLMs when presented with common suggestion-seeking queries, which might involve making assumptions about the user. Our analyses demonstrate strong assumptions about cultural identity associated with names present in LLM generations across multiple cultures. Our work has implications for designing more nuanced personalisation systems that avoid reinforcing stereotypes while maintaining meaningful customisation.', 'score': 9, 'issue_id': 2313, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '8bcbe61536105828', 'authors': ['Siddhesh Pawar', 'Arnav Arora', 'Lucie-Aim√©e Kaffee', 'Isabelle Augenstein'], 'affiliations': ['Hugging Face', 'University of Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2502.11995.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#alignment', '#healthcare'], 'emoji': 'üë§', 'ru': {'title': '–ò–º–µ–Ω–∞ –≤ –ò–ò: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∏–º–µ–Ω –Ω–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç, –∫–∞–∫ LLM –¥–µ–ª–∞—é—Ç –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è –æ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –∏–º–µ–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–∏–ª—å–Ω—ã–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∏–º–µ–Ω–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ –Ω—é–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏, –∏–∑–±–µ–≥–∞—é—â–∏—Ö —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø–æ–≤.'}, 'en': {'title': 'Rethinking Personalization: Beyond Names and Stereotypes in LLMs', 'desc': "This paper explores how names influence identity and personalization in interactions with large language models (LLMs). It highlights that while names can provide valuable information for tailoring responses, they can also lead to oversimplified assumptions about a user's cultural background. The authors analyze biases in LLM outputs when names are used in suggestion-seeking queries, revealing strong cultural stereotypes linked to names. The findings suggest the need for more sophisticated personalization systems that respect individual identities without perpetuating stereotypes."}, 'zh': {'title': 'ÂêçÂ≠ó‰∏éË∫´‰ªΩÔºö‰∏™ÊÄßÂåñÁ≥ªÁªü‰∏≠ÁöÑÊñáÂåñÂÅèËßÅ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂêçÂ≠ó‰∏é‰∫∫Á±ªË∫´‰ªΩ‰πãÈó¥ÁöÑÊ∑±ÂàªËÅîÁ≥ª„ÄÇÂêçÂ≠ó‰∏ç‰ªÖÊòØ‰∏™‰ΩìÊÄß„ÄÅÊñáÂåñÈÅó‰∫ßÂíå‰∏™‰∫∫ÂéÜÂè≤ÁöÑÊ†áÂøóÔºåËøòÂèØËÉΩÂØºËá¥ÂØπÂ§çÊùÇË∫´‰ªΩÁöÑËøáÂ∫¶ÁÆÄÂåñ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∫íÂä®Êó∂ÔºåÂêçÂ≠ó‰ºöÂΩ±Âìç‰∏™ÊÄßÂåñÁöÑÂìçÂ∫îÔºåÂèØËÉΩÂºïÂèëÊñáÂåñÂÅèËßÅ„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêÊòæÁ§∫ÔºåLLMsÂú®ÁîüÊàêÂìçÂ∫îÊó∂ÂØπÂêçÂ≠óÁöÑÊñáÂåñË∫´‰ªΩÊúâÂº∫ÁÉàÁöÑÂÅáËÆæÔºåËøôÂØπËÆæËÆ°Êõ¥ÁªÜËá¥ÁöÑ‰∏™ÊÄßÂåñÁ≥ªÁªüÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13946', 'title': "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region", 'url': 'https://huggingface.co/papers/2502.13946', 'abstract': "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.", 'score': 8, 'issue_id': 2311, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': '72612658ea7eefab', 'authors': ['Chak Tou Leong', 'Qingyu Yin', 'Jian Wang', 'Wenjie Li'], 'affiliations': ['Department of Computing, The Hong Kong Polytechnic University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.13946.jpg', 'data': {'categories': ['#alignment', '#security', '#training', '#inference', '#rlhf'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –∏—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –∫ –∞—Ç–∞–∫–∞–º —Ç–∏–ø–∞ 'jailbreak'. –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–≤–∏–≥–∞—é—Ç –≥–∏–ø–æ—Ç–µ–∑—É, —á—Ç–æ –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º —É—è–∑–≤–∏–º–æ—Å—Ç–∏ —è–≤–ª—è–µ—Ç—Å—è —á—Ä–µ–∑–º–µ—Ä–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LLM –æ—Ç —à–∞–±–ª–æ–Ω–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ –º–µ–∂–¥—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –∏ –Ω–∞—á–∞–ª—å–Ω—ã–º –≤—ã–≤–æ–¥–æ–º –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —ç—Ç–∞ –ø—Ä–æ–±–ª–µ–º–∞, –Ω–∞–∑–≤–∞–Ω–Ω–∞—è '–ø—Ä–∏–≤—è–∑–∫–æ–π –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫ —à–∞–±–ª–æ–Ω—É', —à–∏—Ä–æ–∫–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∞ —Å—Ä–µ–¥–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ—Ç–¥–µ–ª–∏—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –æ—Ç —à–∞–±–ª–æ–Ω–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –∞—Ç–∞–∫–∞–º."}, 'en': {'title': 'Strengthening LLM Safety by Breaking Template Ties', 'desc': "This paper investigates the safety alignment of large language models (LLMs) and identifies a vulnerability linked to the use of fixed templates in their design. The authors propose that these templates anchor the models' safety decision-making, making them susceptible to simple jailbreak attacks. Through experiments, they confirm that this 'template-anchored safety alignment' is a common issue across various LLMs. The study suggests that improving safety mechanisms by detaching them from the template region could enhance the models' resilience against such attacks."}, 'zh': {'title': 'Ê®°ÊùøÈîöÂÆöÁöÑÂÆâÂÖ®ÂØπÈΩêÈóÆÈ¢ò', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂÆâÂÖ®ÂØπÈΩê‰ªçÁÑ∂Â≠òÂú®ËÑÜÂº±ÊÄßÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑÂàùÂßãË°å‰∏∫ÂÆπÊòìÂèóÂà∞ÁÆÄÂçïÊîªÂáªÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÂÅáËÆæËæìÂÖ•Êåá‰ª§ÂíåÂàùÂßãÊ®°ÂûãËæìÂá∫‰πãÈó¥ÁöÑÂõ∫ÂÆöÊ®°ÊùøÊòØÂØºËá¥Ëøô‰∫õËÑÜÂº±ÊÄßÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÔºåÂõ†‰∏∫LLMsÁöÑÂÆâÂÖ®ÂÜ≥Á≠ñËøá‰∫é‰æùËµñÊ®°ÊùøÂå∫ÂüüÁöÑ‰ø°ÊÅØ„ÄÇÊàë‰ª¨Áß∞ËøôÁßçÈóÆÈ¢ò‰∏∫Ê®°ÊùøÈîöÂÆöÁöÑÂÆâÂÖ®ÂØπÈΩê„ÄÇÈÄöËøáÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞ËøôÁßçÁé∞Ë±°Âú®Â§öÁßçÂØπÈΩêÁöÑLLMs‰∏≠ÊôÆÈÅçÂ≠òÂú®ÔºåÂπ∂‰∏îÂàÜÁ¶ªÂÆâÂÖ®Êú∫Âà∂‰∏éÊ®°ÊùøÂå∫ÂüüÊúâÂä©‰∫éÂáèËΩªÂØπÊîªÂáªÁöÑËÑÜÂº±ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13233', 'title': 'SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?', 'url': 'https://huggingface.co/papers/2502.13233', 'abstract': "Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge.", 'score': 8, 'issue_id': 2310, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': '58503159cb9ed740', 'authors': ['Yucheng Shi', 'Tianze Yang', 'Canyu Chen', 'Quanzheng Li', 'Tianming Liu', 'Xiang Li', 'Ninghao Liu'], 'affiliations': ['Illinois Institute of Technology', 'Massachusetts General Hospital and Harvard Medical School', 'University of Georgia'], 'pdf_title_img': 'assets/pdf/title_img/2502.13233.jpg', 'data': {'categories': ['#rag', '#synthetic', '#healthcare', '#science'], 'emoji': 'ü©∫', 'ru': {'title': 'SearchRAG: –¢–æ—á–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã —Å –ø–æ–º–æ—â—å—é –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SearchRAG - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, SearchRAG –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–±–æ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SearchRAG –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π.'}, 'en': {'title': 'Enhancing Medical Q&A with Real-Time Search and Smart Querying', 'desc': 'This paper introduces SearchRAG, a new framework designed to enhance the performance of Large Language Models (LLMs) in medical question answering. Unlike traditional Retrieval-Augmented Generation (RAG) methods that rely on static knowledge bases, SearchRAG utilizes real-time search engines to access current and detailed medical information. The framework employs synthetic query generation to transform complex medical inquiries into queries suitable for search engines, and it uses uncertainty-based knowledge selection to ensure that only the most relevant information is included. Experimental results indicate that SearchRAG significantly boosts the accuracy of LLM responses, especially for intricate medical questions that demand precise and updated knowledge.'}, 'zh': {'title': 'ÂÆûÊó∂ÊêúÁ¥¢ÊèêÂçáÂåªÁñóÈóÆÁ≠îÂáÜÁ°ÆÊÄß', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏ÄËà¨È¢ÜÂüüË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÈúÄË¶Å‰∏ì‰∏öÁü•ËØÜÁöÑ‰ªªÂä°‰∏≠Â∏∏Â∏∏ÈÅáÂà∞Âõ∞Èöæ„ÄÇ‰º†ÁªüÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊäÄÊúØÈÄöÂ∏∏‰ªéÈùôÊÄÅÁü•ËØÜÂ∫ì‰∏≠Ê£ÄÁ¥¢Â§ñÈÉ®‰ø°ÊÅØÔºåËøô‰∫õ‰ø°ÊÅØÂèØËÉΩËøáÊó∂Êàñ‰∏çÂÆåÊï¥ÔºåÁº∫‰πèÂáÜÁ°ÆÂåªÁñóÈóÆÁ≠îÊâÄÈúÄÁöÑÁªÜËäÇ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂SearchRAGÔºåÈÄöËøáÂà©Áî®ÂÆûÊó∂ÊêúÁ¥¢ÂºïÊìéÊù•ÂÖãÊúçËøô‰∫õÈôêÂà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂåªÁñóÈóÆÁ≠î‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÂìçÂ∫îÂáÜÁ°ÆÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂØπ‰∫éÈúÄË¶ÅËØ¶ÁªÜÂíåÊúÄÊñ∞Áü•ËØÜÁöÑÂ§çÊùÇÈóÆÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13173', 'title': 'Thinking Preference Optimization', 'url': 'https://huggingface.co/papers/2502.13173', 'abstract': "Supervised Fine-Tuning (SFT) has been a go-to and effective method for enhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by fine-tuning them with long CoT responses from larger LLMs. To continually improve reasoning abilities, we can either collect new high-quality long CoT reasoning SFT data or repeatedly train on existing SFT datasets. However, acquiring new long CoT SFT data is costly and limited, while repeated training often results in a performance plateau or decline. To further boost the performance with the SFT data, we propose Thinking Preference Optimization (ThinkPO), a simple yet effective post-SFT method that enhances long CoT reasoning without requiring new long CoT responses. Instead, ThinkPO utilizes readily available or easily obtainable short CoT reasoning responses as rejected answers and long CoT responses as chosen answers for the same question. It then applies direct preference optimization to encourage the model to favor longer reasoning outputs. Experiments show that ThinkPO further improves the reasoning performance of SFT-ed models, e.g. it increases math reasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%. Notably, ThinkPO is capable of continually boosting the performance of the publicly distilled SFT model, e.g., increasing the official DeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%.", 'score': 7, 'issue_id': 2311, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '6096d2396d4c584e', 'authors': ['Wang Yang', 'Hongye Jin', 'Jingfeng Yang', 'Vipin Chaudhary', 'Xiaotian Han'], 'affiliations': ['case.edu', 'gmail.com', 'tamu.edu'], 'pdf_title_img': 'assets/pdf/title_img/2502.13173.jpg', 'data': {'categories': ['#math', '#training', '#reasoning', '#long_context', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –º—ã—à–ª–µ–Ω–∏—è: –Ω–æ–≤—ã–π —à–∞–≥ –≤ —É–ª—É—á—à–µ–Ω–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': '–≠—Ç–æ—Ç –Ω–∞—É—á–Ω—ã–π —Ç—Ä—É–¥ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Thinking Preference Optimization (ThinkPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –¥–ª–∏–Ω–Ω—ã–º —Ü–µ–ø–æ—á–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. ThinkPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ –∫–∞–∫ –æ—Ç–≤–µ—Ä–≥–Ω—É—Ç—ã–µ –∏ –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∫–∞–∫ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –≤–æ–ø—Ä–æ—Å–∞. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä—è–º—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, —á—Ç–æ–±—ã –ø–æ–æ—â—Ä—è—Ç—å –º–æ–¥–µ–ª—å –≤—ã–¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ThinkPO —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ 8.6% –∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –¥–ª–∏–Ω—É –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ 25.9%.'}, 'en': {'title': 'Boosting Reasoning with Preference Optimization', 'desc': "This paper introduces Thinking Preference Optimization (ThinkPO), a method designed to enhance long chain-of-thought (CoT) reasoning in supervised fine-tuning (SFT) of language models. Instead of needing new long CoT data, ThinkPO leverages existing short CoT responses as negative examples and long CoT responses as positive examples to optimize the model's preferences. The approach leads to significant improvements in reasoning accuracy and output length, demonstrating its effectiveness in refining model performance. Experiments show that ThinkPO can consistently boost the capabilities of SFT-ed models, particularly in mathematical reasoning tasks."}, 'zh': {'title': 'ÊÄùÁª¥ÂÅèÂ•Ω‰ºòÂåñÔºöÊèêÂçáÈïøÈìæÊé®ÁêÜÁöÑÊúâÊïàÊñπÊ≥ï', 'desc': 'ÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊòØ‰∏ÄÁßçÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÊèêÂçáÂ∞èÂûãÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÈïøÈìæÊé®ÁêÜÔºàCoTÔºâÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜÊåÅÁª≠ÊèêÈ´òÊé®ÁêÜËÉΩÂäõÔºåÈÄöÂ∏∏ÈúÄË¶ÅÊî∂ÈõÜÊñ∞ÁöÑÈ´òË¥®ÈáèÈïøCoTÊï∞ÊçÆÔºå‰ΩÜËøôÊàêÊú¨È´ò‰∏îÊúâÈôê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÊÄùÁª¥ÂÅèÂ•Ω‰ºòÂåñÔºàThinkPOÔºâÁöÑÂêéSFTÊñπÊ≥ïÔºåÂÆÉÂà©Áî®Áü≠CoTÊé®ÁêÜ‰Ωú‰∏∫ÊãíÁªùÁ≠îÊ°àÔºåÈïøCoTÊé®ÁêÜ‰Ωú‰∏∫ÈÄâÊã©Á≠îÊ°àÔºåÈÄöËøáÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÊù•Â¢ûÂº∫Ê®°ÂûãÂØπÈïøÊé®ÁêÜËæìÂá∫ÁöÑÂÅèÂ•Ω„ÄÇÂÆûÈ™åË°®ÊòéÔºåThinkPOÊòæËëóÊèêÈ´ò‰∫ÜÁªèËøáSFTËÆ≠ÁªÉÊ®°ÂûãÁöÑÊé®ÁêÜÊÄßËÉΩÔºåÂ∞§ÂÖ∂Âú®Êï∞Â≠¶Êé®ÁêÜÂáÜÁ°ÆÁéá‰∏äÊèêÂçá‰∫Ü8.6%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13962', 'title': 'Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering', 'url': 'https://huggingface.co/papers/2502.13962', 'abstract': 'Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings.', 'score': 6, 'issue_id': 2311, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': '06bee7f6d596d006', 'authors': ['William Jurayj', 'Jeffrey Cheng', 'Benjamin Van Durme'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2502.13962.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#reasoning', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π - –º–µ–Ω—å—à–µ —Ä–∏—Å–∫–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–∞ —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ —Å–≤–æ–∏—Ö –æ—Ç–≤–µ—Ç–∞—Ö –∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å –Ω–µ–Ω—É–ª–µ–≤—ã–º —É—Ä–æ–≤–Ω–µ–º —Ä–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–∞. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤, –Ω–æ –∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º —ç—Ç–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤.'}, 'en': {'title': 'Boosting Confidence in AI Responses through Compute Scaling', 'desc': "This paper discusses how increasing the computational resources available to large language models during inference can improve their performance on reasoning tasks. It highlights the importance of not just providing answers, but also assessing the model's confidence in those answers. By extracting confidence scores, the authors propose a method to filter responses based on their reliability. The study also introduces a new evaluation framework that accounts for the risk associated with model responses, moving beyond the traditional zero-risk assumption."}, 'zh': {'title': 'ÊèêÂçáÊ®°Âûã‰ø°ÂøÉÔºå‰ºòÂåñÊé®ÁêÜÂõûÁ≠î', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÊµãËØïÊó∂ËÆ°ÁÆóËÉΩÂäõÁöÑÊâ©Â±ïÊâÄÂ∏¶Êù•ÁöÑÊòæËëóÊÄßËÉΩÊèêÂçá„ÄÇÁé∞ÊúâÁöÑËØÑ‰º∞ÊñπÊ≥ïÂÅáËÆæÊé®ÁêÜÁ≥ªÁªüÂøÖÈ°ªÂØπÊØè‰∏™ÈóÆÈ¢òÈÉΩÁªôÂá∫Á≠îÊ°àÔºå‰ΩÜËøôÂøΩËßÜ‰∫ÜÊ®°ÂûãÂØπÁ≠îÊ°àÁöÑ‰ø°ÂøÉÂíåÊòØÂê¶ÊÄªÊòØÊèê‰æõÂõûÁ≠îÁöÑÈÄÇÂΩìÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÁ†îÁ©∂ËÄÖÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÊèêÂèñ‰∫ÜÁΩÆ‰ø°Â∫¶ÂàÜÊï∞Ôºå‰ª•‰æøÂØπÊ®°ÂûãÁöÑÂõûÁ≠îËøõË°åÈòàÂÄºÂ§ÑÁêÜ„ÄÇÁªìÊûúË°®ÊòéÔºåÂ¢ûÂä†Êé®ÁêÜÊó∂ÁöÑËÆ°ÁÆóÈ¢ÑÁÆó‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÊ®°ÂûãÊ≠£Á°ÆÂõûÁ≠îÈóÆÈ¢òÁöÑËÉΩÂäõÔºåËøòÂ¢ûÂº∫‰∫ÜÂØπÊ≠£Á°ÆÂõûÁ≠îÁöÑ‰ø°ÂøÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13943', 'title': 'AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence', 'url': 'https://huggingface.co/papers/2502.13943', 'abstract': "Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.", 'score': 5, 'issue_id': 2310, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': '41ab630e56147df2', 'authors': ['Yuliang Liu', 'Junjie Lu', 'Zhaoling Chen', 'Chaofeng Qu', 'Jason Klein Liu', 'Chonghan Liu', 'Zefan Cai', 'Yunhui Xia', 'Li Zhao', 'Jiang Bian', 'Chuheng Zhang', 'Wei Shen', 'Zhouhan Lin'], 'affiliations': ['MSRA', 'Nanjing University', 'Shanghai Jiaotong University', 'UW-Madison', 'University of Technology Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2502.13943.jpg', 'data': {'categories': ['#reasoning', '#plp', '#training', '#open_source', '#math', '#transfer_learning'], 'emoji': 'üß†', 'ru': {'title': 'AdaptiveStep: —É–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —à–∞–≥–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è PRM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ AdaptiveStep –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (PRM). –í–º–µ—Å—Ç–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —à–∞–≥–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã, AdaptiveStep –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ PRM, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é AdaptiveStep, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã.'}, 'en': {'title': 'AdaptiveStep: Smarter Reasoning for Better Reward Models', 'desc': "This paper introduces AdaptiveStep, a novel method for training Process Reward Models (PRMs) that improves the way reasoning steps are defined. Instead of relying on fixed-length steps or predefined tokens, AdaptiveStep adjusts the reasoning process based on the model's confidence in predicting the next word. This approach enhances the decision-making information available at each step, leading to better performance in tasks like reward model learning. Experimental results show that PRMs trained with AdaptiveStep outperform traditional methods in mathematical reasoning and code generation, while also being more cost-effective."}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÊ≠•È™§ÔºöÊèêÂçáÂ•ñÂä±Ê®°ÂûãÁöÑÂÜ≥Á≠ñËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫AdaptiveStep„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊ†πÊçÆÊ®°ÂûãÂØπ‰∏ã‰∏Ä‰∏™ÂçïËØçÈ¢ÑÊµãÁöÑ‰ø°ÂøÉÊù•ÂàíÂàÜÊé®ÁêÜÊ≠•È™§Ôºå‰ªéËÄåÊèê‰æõÊõ¥‰∏∞ÂØåÁöÑÂÜ≥Á≠ñ‰ø°ÊÅØ„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éËßÑÂàôÁöÑÊñπÊ≥ï‰∏çÂêåÔºåAdaptiveStep‰∏çÈúÄË¶ÅÊâãÂä®Ê†áÊ≥®Ôºå‰∏îÂú®Êï∞Â≠¶Êé®ÁêÜÂíå‰ª£Á†ÅÁîüÊàê‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®AdaptiveStepËÆ≠ÁªÉÁöÑPRMÂú®ÊÄßËÉΩ‰∏äË∂ÖËøá‰∫ÜÁé∞ÊúâÁöÑÂºÄÊ∫êPRMÔºåÂπ∂‰∏îÊûÑÂª∫ÊàêÊú¨Èôç‰Ωé‰∫Ü30%‰ª•‰∏ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13622', 'title': 'REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models', 'url': 'https://huggingface.co/papers/2502.13622', 'abstract': 'Hallucinations in large language model (LLM) outputs severely limit their reliability in knowledge-intensive tasks such as question answering. To address this challenge, we introduce REFIND (Retrieval-augmented Factuality hallucINation Detection), a novel framework that detects hallucinated spans within LLM outputs by directly leveraging retrieved documents. As part of the REFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that quantifies the sensitivity of LLM outputs to retrieved evidence. This innovative approach enables REFIND to efficiently and accurately detect hallucinations, setting it apart from existing methods. In the evaluation, REFIND demonstrated robustness across nine languages, including low-resource settings, and significantly outperformed baseline models, achieving superior IoU scores in identifying hallucinated spans. This work highlights the effectiveness of quantifying context sensitivity for hallucination detection, thereby paving the way for more reliable and trustworthy LLM applications across diverse languages.', 'score': 3, 'issue_id': 2319, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': '7c55d6b9ce07af47', 'authors': ['DongGeon Lee', 'Hwanjo Yu'], 'affiliations': ['Pohang University of Science and Technology (POSTECH), Pohang, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.13622.jpg', 'data': {'categories': ['#hallucinations', '#multilingual', '#low_resource', '#rag'], 'emoji': 'üîç', 'ru': {'title': '–¢–æ—á–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ LLM —Å –ø–æ–º–æ—â—å—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ REFIND –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –≤—ã–≤–æ–¥–∞—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). REFIND –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É Context Sensitivity Ratio –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–æ–≤ LLM –∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –≤—ã—è–≤–ª–µ–Ω–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –Ω–∞ –¥–µ–≤—è—Ç–∏ —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–µ. REFIND –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–∑–æ—à–µ–ª –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—é IoU –ø—Ä–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'REFIND: Enhancing LLM Reliability by Detecting Hallucinations', 'desc': 'This paper presents REFIND, a new framework designed to identify hallucinations in outputs from large language models (LLMs) by using information from retrieved documents. It introduces the Context Sensitivity Ratio (CSR), a metric that measures how much LLM outputs depend on the retrieved evidence. REFIND shows improved performance in detecting hallucinations compared to existing methods, achieving better intersection over union (IoU) scores across multiple languages, including those with fewer resources. The findings emphasize the importance of context sensitivity in enhancing the reliability of LLM applications.'}, 'zh': {'title': 'ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØÈù†ÊÄß', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂REFINDÔºåÁî®‰∫éÊ£ÄÊµãÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËæìÂá∫‰∏≠ÁöÑÂπªËßâÁé∞Ë±°„ÄÇREFINDÈÄöËøáÁõ¥Êé•Âà©Áî®Ê£ÄÁ¥¢Âà∞ÁöÑÊñáÊ°£ÔºåËØÜÂà´LLMËæìÂá∫‰∏≠ÁöÑËôöÂÅá‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ∫¶ÈáèÊ†áÂáÜ‚Äî‚Äî‰∏ä‰∏ãÊñáÊïèÊÑüÊÄßÊØîÁéáÔºàCSRÔºâÔºåÁî®‰∫éÈáèÂåñLLMËæìÂá∫ÂØπÊ£ÄÁ¥¢ËØÅÊçÆÁöÑÊïèÊÑüÁ®ãÂ∫¶„ÄÇREFINDÂú®‰πùÁßçËØ≠Ë®ÄÁöÑËØÑ‰º∞‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂú®Â§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÊèêÈ´òLLMÂ∫îÁî®ÂèØÈù†ÊÄßÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.12638', 'title': 'NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation', 'url': 'https://huggingface.co/papers/2502.12638', 'abstract': "3D molecule generation is crucial for drug discovery and material design. While prior efforts focus on 3D diffusion models for their benefits in modeling continuous 3D conformers, they overlook the advantages of 1D SELFIES-based Language Models (LMs), which can generate 100% valid molecules and leverage the billion-scale 1D molecule datasets. To combine these advantages for 3D molecule generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively pretrained molecule LM for 1D molecule generation, and subsequently predicts the generated molecule's 3D conformers with a 3D diffusion model. We enhance NExT-Mol's performance by scaling up the LM's model size, refining the diffusion neural architecture, and applying 1D to 3D transfer learning. Notably, our 1D molecule LM significantly outperforms baselines in distributional similarity while ensuring validity, and our 3D diffusion model achieves leading performances in conformer prediction. Given these improvements in 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD for de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are available at https://github.com/acharkq/NExT-Mol.", 'score': 3, 'issue_id': 2311, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 18', 'zh': '2Êúà18Êó•'}, 'hash': 'eb0d7b097262b590', 'authors': ['Zhiyuan Liu', 'Yanchen Luo', 'Han Huang', 'Enzhi Zhang', 'Sihang Li', 'Junfeng Fang', 'Yaorui Shi', 'Xiang Wang', 'Kenji Kawaguchi', 'Tat-Seng Chua'], 'affiliations': ['Chinese University of Hong Kong', 'Hokkaido University', 'National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12638.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#architecture', '#diffusion', '#dataset', '#3d'], 'emoji': 'üß™', 'ru': {'title': 'NExT-Mol: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–º–æ–ª–µ–∫—É–ª', 'desc': 'NExT-Mol - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –º–æ–ª–µ–∫—É–ª, —Å–æ—á–µ—Ç–∞—é—â–∞—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∞–ª–∏–¥–Ω—ã—Ö –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ SELFIES, –∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Ö 3D-–∫–æ–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —É–ª—É—á—à–∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —É–≤–µ–ª–∏—á–∏–≤ —Ä–∞–∑–º–µ—Ä —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏ –ø—Ä–∏–º–µ–Ω–∏–≤ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. NExT-Mol –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö de novo –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–º–æ–ª–µ–∫—É–ª.'}, 'en': {'title': 'NExT-Mol: Bridging 1D Language Models and 3D Diffusion for Molecule Generation', 'desc': 'This paper presents NExT-Mol, a novel foundation model that integrates 1D SELFIES-based Language Models (LMs) with 3D diffusion models for generating 3D molecular structures. By leveraging the strengths of both approaches, NExT-Mol first generates valid 1D molecular representations and then predicts their corresponding 3D conformers. The model is enhanced through scaling the LM, refining the diffusion architecture, and employing transfer learning from 1D to 3D. The results show significant improvements in both 3D generation and conformer prediction, outperforming existing methods in terms of distributional similarity and validity.'}, 'zh': {'title': 'NExT-MolÔºöÁªìÂêà1DËØ≠Ë®ÄÊ®°Âûã‰∏é3DÊâ©Êï£Ê®°ÂûãÁöÑÂàÜÂ≠êÁîüÊàêÊñ∞ÊñπÊ≥ï', 'desc': '3DÂàÜÂ≠êÁîüÊàêÂØπËçØÁâ©ÂèëÁé∞ÂíåÊùêÊñôËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®3DÊâ©Êï£Ê®°Âûã‰∏äÔºå‰ΩÜÂøΩËßÜ‰∫ÜÂü∫‰∫é1D SELFIESÁöÑËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ºòÂäøÔºåËøô‰∫õÊ®°ÂûãËÉΩÂ§üÁîüÊàê100%ÊúâÊïàÁöÑÂàÜÂ≠êÂπ∂Âà©Áî®Â§ßËßÑÊ®°ÁöÑ1DÂàÜÂ≠êÊï∞ÊçÆÈõÜ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂü∫Á°ÄÊ®°ÂûãNExT-MolÔºåÂÆÉÁªìÂêà‰∫Ü1DËØ≠Ë®ÄÂª∫Ê®°Âíå3DÊâ©Êï£Ê®°ÂûãÁöÑ‰ºòÁÇπÔºåËÉΩÂ§üÊúâÊïàÁîüÊàê3DÂàÜÂ≠ê„ÄÇÈÄöËøáÊâ©Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÑÊ®°„ÄÅ‰ºòÂåñÊâ©Êï£Á•ûÁªèÁΩëÁªúÊû∂ÊûÑ‰ª•ÂèäÂ∫îÁî®1DÂà∞3DÁöÑËøÅÁßªÂ≠¶‰π†ÔºåNExT-MolÂú®3DÁîüÊàêÂíåÊù°‰ª∂ÁîüÊàê‰∏äÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13533', 'title': 'Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models', 'url': 'https://huggingface.co/papers/2502.13533', 'abstract': 'Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter matrices. However, the memory footprint of LoRA is largely dominated by the original model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA training scheme founded on the intuition that many neurons in over-parameterized LLMs have low training utility but are essential for inference. LoRAM presents a unique twist: it trains on a pruned (small) model to obtain pruned low-rank matrices, which are then recovered and utilized with the original (large) model for inference. Additionally, minimal-cost continual pre-training, performed by the model publishers in advance, aligns the knowledge discrepancy between pruned and original models. Our extensive experiments demonstrate the efficacy of LoRAM across various pruning strategies and downstream tasks. For a model with 70 billion parameters, LoRAM enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost that dominates the memory usage in low-rank matrix training by 15.81times (16.95times), while achieving dominant performance gains over both the original LLaMA-3.1-70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).', 'score': 2, 'issue_id': 2318, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': '968a6ad064935831', 'authors': ['Jun Zhang', 'Jue Wang', 'Huan Li', 'Lidan Shou', 'Ke Chen', 'Yang You', 'Guiming Xie', 'Xuejian Gong', 'Kunlong Zhou'], 'affiliations': ['AI Center, Guangdong OPPO Mobile Telecommunications Corp., Ltd.', 'College of Computer Science and Technology, Zhejiang University', 'Department of Computer Science, National University of Singapore', 'Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security', 'The State Key Laboratory of Blockchain and Data Security, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.13533.jpg', 'data': {'categories': ['#small_models', '#transfer_learning', '#architecture', '#inference', '#optimization', '#training'], 'emoji': 'üß†', 'ru': {'title': 'LoRAM: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LoRAM - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). LoRAM –æ–±—É—á–∞–µ—Ç —É—Ä–µ–∑–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä—É–Ω–∏–Ω–≥, –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é (LoRA) –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —Ä–∞–∑—Ä—ã–≤–∞ –≤ –∑–Ω–∞–Ω–∏—è—Ö –º–µ–∂–¥—É —É—Ä–µ–∑–∞–Ω–Ω–æ–π –∏ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LoRAM –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å 70-–º–∏–ª–ª–∏–∞—Ä–¥–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–º GPU —Å 20 –ì–ë –ø–∞–º—è—Ç–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–ª–Ω—ã–º —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–æ–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º LoRA.'}, 'en': {'title': 'LoRAM: Efficient Fine-Tuning for Large Language Models', 'desc': 'This paper introduces LoRAM, a novel training scheme for Low-Rank Adaptation (LoRA) that enhances memory efficiency when fine-tuning large language models (LLMs). By focusing on a pruned version of the model, LoRAM identifies and trains only the essential low-rank matrices, significantly reducing the memory requirements during training. The approach leverages minimal-cost continual pre-training to bridge the knowledge gap between the pruned and original models, ensuring effective performance. Experimental results show that LoRAM can drastically cut down memory usage while maintaining or improving performance across various tasks compared to traditional LoRA methods.'}, 'zh': {'title': 'LoRAMÔºöÈ´òÊïàÁöÑ‰ΩéÁß©ÈÄÇÂ∫îËÆ≠ÁªÉÊñπÊ°à', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåÂÖ∑Â§áÂá∫Ëâ≤ÁöÑ‰ªªÂä°Ê≥õÂåñËÉΩÂäõ„ÄÇ‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÊèê‰æõ‰∫Ü‰∏ÄÁßçÁªèÊµéÈ´òÊïàÁöÑÂæÆË∞ÉÊñπÊ°àÔºåÈÄöËøáÂÜªÁªìÂéüÂßãÊ®°ÂûãÂèÇÊï∞Ôºå‰ªÖËÆ≠ÁªÉËΩªÈáèÁ∫ßÁöÑ‰ΩéÁß©ÈÄÇÈÖçÂô®Áü©Èòµ„ÄÇÁÑ∂ËÄåÔºåLoRAÁöÑÂÜÖÂ≠òÂç†Áî®‰∏ªË¶ÅÁî±ÂéüÂßãÊ®°ÂûãÂèÇÊï∞ÂÜ≥ÂÆö„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜLoRAMÔºå‰∏ÄÁßçÂÜÖÂ≠òÈ´òÊïàÁöÑLoRAËÆ≠ÁªÉÊñπÊ°àÔºåÈÄöËøáÂú®Ââ™ÊûùÂêéÁöÑÂ∞èÊ®°Âûã‰∏äËÆ≠ÁªÉÔºåËé∑ÂæóÂâ™ÊûùÁöÑ‰ΩéÁß©Áü©ÈòµÔºåÂπ∂Âú®Êé®ÁêÜÊó∂‰∏éÂéüÂßãÂ§ßÊ®°ÂûãÁªìÂêà‰ΩøÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13766', 'title': 'GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking', 'url': 'https://huggingface.co/papers/2502.13766', 'abstract': 'Large Vision-Language Models (LVLMs) have recently gained attention due to their distinctive performance and broad applicability. While it has been previously shown that their efficacy in usage scenarios involving non-Western contexts falls short, existing studies are limited in scope, covering just a narrow range of cultures, focusing exclusively on a small number of cultural aspects, or evaluating a limited selection of models on a single task only. Towards globally inclusive LVLM research, we introduce GIMMICK, an extensive multimodal benchmark designed to assess a broad spectrum of cultural knowledge across 144 countries representing six global macro-regions. GIMMICK comprises six tasks built upon three new datasets that span 728 unique cultural events or facets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary and 26 open-weight models of all sizes. We systematically examine (1) regional cultural biases, (2) the influence of model size, (3) input modalities, and (4) external cues. Our analyses reveal strong biases toward Western cultures across models and tasks and highlight strong correlations between model size and performance, as well as the effectiveness of multimodal input and external geographic cues. We further find that models have more knowledge of tangible than intangible aspects (e.g., food vs. rituals) and that they excel in recognizing broad cultural origins but struggle with a more nuanced understanding.', 'score': 2, 'issue_id': 2317, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': 'e8f5148a9ef41989', 'authors': ['Florian Schneider', 'Carolin Holtermann', 'Chris Biemann', 'Anne Lauscher'], 'affiliations': ['Data Science Group, University of Hamburg', 'Language Technology Group, University of Hamburg'], 'pdf_title_img': 'assets/pdf/title_img/2502.13766.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#multimodal', '#ethics', '#dataset'], 'emoji': 'üåç', 'ru': {'title': 'GIMMICK: –≥–ª–æ–±–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π AI-–º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GIMMICK - –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLM) –ø–æ 144 —Å—Ç—Ä–∞–Ω–∞–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 20 LVLM –∏ 11 LLM –º–æ–¥–µ–ª–µ–π, –æ—Ü–µ–Ω–∏–≤–∞—è –∏—Ö –Ω–∞ 6 –∑–∞–¥–∞—á–∞—Ö, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –Ω–∞ 3 –Ω–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–∏–ª—å–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ —Å—Ç–æ—Ä–æ–Ω—É –∑–∞–ø–∞–¥–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ä–æ–º –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é. –¢–∞–∫–∂–µ –æ—Ç–º–µ—á–∞–µ—Ç—Å—è, —á—Ç–æ –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —Ä–∞—Å–ø–æ–∑–Ω–∞—é—Ç –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∫—É–ª—å—Ç—É—Ä—ã, —á–µ–º –Ω–µ–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–µ, –∏ —Ö–æ—Ä–æ—à–æ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –æ–±—â–µ–µ –∫—É–ª—å—Ç—É—Ä–Ω–æ–µ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ, –Ω–æ —Ö—É–∂–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –±–æ–ª–µ–µ —Ç–æ–Ω–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º.'}, 'en': {'title': 'GIMMICK: Bridging Cultural Gaps in Vision-Language Models', 'desc': 'This paper introduces GIMMICK, a comprehensive benchmark for evaluating Large Vision-Language Models (LVLMs) on cultural knowledge across 144 countries. It addresses the limitations of previous studies that focused mainly on Western contexts and a narrow range of cultural aspects. The benchmark includes six tasks and three new datasets, assessing 20 LVLMs and 11 LLMs on their understanding of diverse cultural events. The findings reveal significant biases towards Western cultures, a correlation between model size and performance, and a disparity in knowledge of tangible versus intangible cultural elements.'}, 'zh': {'title': 'ÂÖ®ÁêÉÊñáÂåñÁü•ËØÜÁöÑÂÖ®Èù¢ËØÑ‰º∞', 'desc': 'Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®ÊÄßËÉΩÂíåÂ∫îÁî®ËåÉÂõ¥‰∏äÂºïËµ∑‰∫ÜÂπøÊ≥õÂÖ≥Ê≥®„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁ†îÁ©∂Âú®ÈùûË•øÊñπÊñáÂåñÂú∫ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß‰∏çË∂≥Ôºå‰∏îÁ†îÁ©∂ËåÉÂõ¥ÊúâÈôê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜGIMMICKÔºåËøôÊòØ‰∏Ä‰∏™ÂπøÊ≥õÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞144‰∏™ÂõΩÂÆ∂ÁöÑÊñáÂåñÁü•ËØÜ„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêÊòæÁ§∫ÔºåÊ®°ÂûãÂØπË•øÊñπÊñáÂåñÂ≠òÂú®ÊòéÊòæÂÅèËßÅÔºåÂπ∂‰∏îÊ®°ÂûãÁöÑÂ§ßÂ∞è„ÄÅËæìÂÖ•Ê®°ÊÄÅÂíåÂ§ñÈÉ®Á∫øÁ¥¢ÂØπÊÄßËÉΩÊúâÊòæËëóÂΩ±Âìç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13581', 'title': 'ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation', 'url': 'https://huggingface.co/papers/2502.13581', 'abstract': 'Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a set of item features, which serve as the initial tokens. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Experiments on public datasets demonstrate that ActionPiece consistently outperforms existing action tokenization methods, improving NDCG@10 by 6.00% to 12.82%.', 'score': 2, 'issue_id': 2315, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': '341531ed7311973d', 'authors': ['Yupeng Hou', 'Jianmo Ni', 'Zhankui He', 'Noveen Sachdeva', 'Wang-Cheng Kang', 'Ed H. Chi', 'Julian McAuley', 'Derek Zhiyuan Cheng'], 'affiliations': ['Google DeepMind', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.13581.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#optimization', '#data'], 'emoji': 'üß©', 'ru': {'title': '–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ActionPiece. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, ActionPiece —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–µ–π—Å—Ç–≤–∏–π, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—è –∫–∞–∂–¥–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∫–∞–∫ –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –°–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–æ–≤ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –ø—É—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–æ–∫ –Ω–∞–±–æ—Ä–æ–≤ –¥–ª—è —É—á–µ—Ç–∞ –∏—Ö –Ω–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω–æ–π –ø—Ä–∏—Ä–æ–¥—ã.'}, 'en': {'title': 'Context Matters: Enhancing Generative Recommendations with ActionPiece', 'desc': 'This paper introduces ActionPiece, a novel approach to generative recommendation that enhances the tokenization of user actions by incorporating contextual information. Unlike traditional methods that treat each action independently, ActionPiece uses item features to create a more meaningful representation of actions based on their context. The model constructs a vocabulary of tokens by merging feature patterns that frequently occur together, allowing for a richer understanding of user behavior. Experimental results show that ActionPiece significantly improves recommendation performance, as evidenced by higher NDCG@10 scores compared to existing methods.'}, 'zh': {'title': '‰∏ä‰∏ãÊñáÈ©±Âä®ÁöÑÁîüÊàêÊé®ËçêÊñ∞ÊñπÊ≥ï', 'desc': 'ÁîüÊàêÊé®ËçêÔºàGRÔºâÊòØ‰∏ÄÁßçÊñ∞ÂÖ¥ÁöÑÊé®ËçêÊñπÊ≥ïÔºåÂÆÉÂ∞ÜÁî®Êà∑Ë°å‰∏∫ËΩ¨Âåñ‰∏∫Á¶ªÊï£ÁöÑÊ®°ÂºèËøõË°åÈ¢ÑÊµã„ÄÇÁé∞ÊúâÁöÑGRÊ®°ÂûãÁã¨Á´ãÂú∞ÂØπÊØè‰∏™Ë°å‰∏∫ËøõË°åÊ†áËÆ∞ÔºåÊú™ËÄÉËôë‰∏ä‰∏ãÊñáÂÖ≥Á≥ªÔºåÂØºËá¥Áõ∏ÂêåÁöÑË°å‰∏∫Âú®‰∏çÂêåÂ∫èÂàó‰∏≠Ë¢´Ëµã‰∫àÁõ∏ÂêåÁöÑÂõ∫ÂÆöÊ†áËÆ∞„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÊé®ËçêÊïàÊûúÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜActionPieceÔºåÈÄöËøáÂú®Ê†áËÆ∞Ë°å‰∏∫Â∫èÂàóÊó∂ÊòæÂºèÂú∞ËûçÂÖ•‰∏ä‰∏ãÊñá‰ø°ÊÅØÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåActionPieceÂú®Â§ö‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑË°å‰∏∫Ê†áËÆ∞ÊñπÊ≥ïÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊé®ËçêÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.11573', 'title': 'InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning', 'url': 'https://huggingface.co/papers/2502.11573', 'abstract': 'Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \\InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.', 'score': 1, 'issue_id': 2317, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 17', 'zh': '2Êúà17Êó•'}, 'hash': '6dbc067f1a808c3a', 'authors': ['Congkai Xie', 'Shuo Cai', 'Wenjun Wang', 'Pengxiang Li', 'Zhijie Sang', 'Kejing Yang', 'Yiming Zhang', 'Zhen Li', 'Guanghao Zhu', 'Zeyu Liu', 'Yang Yu', 'Yuhang Liu', 'Su Lu', 'Baoyi He', 'Qi Zhou', 'Xiaotian Han', 'Jianbo Yuan', 'Shengyu Zhang', 'Fei Wu', 'Hongxia Yang'], 'affiliations': ['Amazon', 'Beijing University of Posts and Telecommunications', 'Dalian University of Technology', 'Harbin Institute of Technology', 'Reallm Labs', 'South China University of Technology', 'The Hong Kong Polytechnic University', 'TikTok', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11573.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#small_models'], 'emoji': 'üß†', 'ru': {'title': '–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM) –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MSLM), —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –æ–±–ª–µ–≥—á–∞–µ—Ç —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–∞ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö. –ú–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞—Ç—Ä–∞—Ç –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É. –¶–µ–ª—å—é —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ø—É—Ç–µ–º —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–Ω–∏–∂–µ–Ω–∏—è –±–∞—Ä—å–µ—Ä–æ–≤ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞ —Å—á–µ—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Efficient AI: Small Models, Big Reasoning!', 'desc': 'This paper discusses the development of Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that maintain strong reasoning abilities while being more efficient. It addresses the challenges of high computational costs and privacy issues associated with larger models. The authors propose a new training pipeline that enhances the reasoning capabilities of these smaller models, making them suitable for deployment on edge devices. The goal is to improve AI systems by making them more accessible and secure through reduced model sizes.'}, 'zh': {'title': 'Â∞èÂûãÊ®°ÂûãÔºåÂ§ßÊô∫ÊÖßÔºÅ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜ‰ªçÈù¢‰∏¥È´òËÆ°ÁÆóÈúÄÊ±ÇÂíåÈöêÁßÅÈóÆÈ¢òÁöÑÊåëÊàò„ÄÇÊú¨Êñá‰∏ìÊ≥®‰∫éÂºÄÂèëÈ´òÊïàÁöÑÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMsÔºâÂíåÂ§öÊ®°ÊÄÅÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMSLMsÔºâÔºå‰ª•‰øùÊåÅÁ´û‰∫âÂäõÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËÆ≠ÁªÉÊµÅÁ®ãÔºåÂ¢ûÂº∫‰∫ÜÊé®ÁêÜËÉΩÂäõÔºåÂπ∂‰æø‰∫éÂú®ËæπÁºòËÆæÂ§á‰∏äÈÉ®ÁΩ≤ÔºåÂêåÊó∂Âú®Èôç‰ΩéÂºÄÂèëÊàêÊú¨ÁöÑÂêåÊó∂ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÂáèÂ∞èÊ®°ÂûãËßÑÊ®°Ôºå\nInfR~Êó®Âú®Êé®Âä®‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÂèëÂ±ïÔºåÊîπÂñÑÊé®ÁêÜËÉΩÂäõÔºåÈôç‰ΩéÈááÁî®Èó®ÊßõÔºåÂπ∂Ëß£ÂÜ≥ÈöêÁßÅÈóÆÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.13573', 'title': 'Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective', 'url': 'https://huggingface.co/papers/2502.13573', 'abstract': 'Semi-supervised heterogeneous domain adaptation (SHDA) addresses learning across domains with distinct feature representations and distributions, where source samples are labeled while most target samples are unlabeled, with only a small fraction labeled. Moreover, there is no one-to-one correspondence between source and target samples. Although various SHDA methods have been developed to tackle this problem, the nature of the knowledge transferred across heterogeneous domains remains unclear. This paper delves into this question from an empirical perspective. We conduct extensive experiments on about 330 SHDA tasks, employing two supervised learning methods and seven representative SHDA methods. Surprisingly, our observations indicate that both the category and feature information of source samples do not significantly impact the performance of the target domain. Additionally, noise drawn from simple distributions, when used as source samples, may contain transferable knowledge. Based on this insight, we perform a series of experiments to uncover the underlying principles of transferable knowledge in SHDA. Specifically, we design a unified Knowledge Transfer Framework (KTF) for SHDA. Based on the KTF, we find that the transferable knowledge in SHDA primarily stems from the transferability and discriminability of the source domain. Consequently, ensuring those properties in source samples, regardless of their origin (e.g., image, text, noise), can enhance the effectiveness of knowledge transfer in SHDA tasks. The codes and datasets are available at https://github.com/yyyaoyuan/SHDA.', 'score': 0, 'issue_id': 2317, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 19', 'zh': '2Êúà19Êó•'}, 'hash': '68464eca5ca15556', 'authors': ['Yuan Yao', 'Xiaopu Zhang', 'Yu Zhang', 'Jian Jin', 'Qiang Yang'], 'affiliations': ['Beijing Teleinfo Technology Company Ltd., China Academy of Information and Communications Technology, Beijing 100095, China', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, and also with WeBank, Shenzhen 518052, China', 'Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Research and Development, Inspur Computer Technology Co., Ltd., Beijing 100095, China', 'Research Institute of Industrial Internet of Things, China Academy of Information and Communications Technology, Beijing 100095, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.13573.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#transfer_learning'], 'emoji': 'üîç', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Ç–∞–π–Ω –ø–µ—Ä–µ–¥–∞—á–∏ –∑–Ω–∞–Ω–∏–π –≤ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–æ–º–µ–Ω–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏—Ä–æ–¥—É –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –∑–∞–¥–∞—á–µ –ø–æ–ª—É–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–æ–º–µ–Ω–æ–≤ (SHDA). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 330 –∑–∞–¥–∞—á–∞—Ö SHDA, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏—Å—Ö–æ–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –Ω–µ –æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ü–µ–ª–µ–≤–æ–º –¥–æ–º–µ–Ω–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –∫–ª—é—á–µ–≤—ã–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –∑–Ω–∞–Ω–∏–π –≤ SHDA —è–≤–ª—è—é—Ç—Å—è –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ—Å—Ç—å –∏ —Ä–∞–∑–ª–∏—á–∏–º–æ—Å—Ç—å –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞.'}, 'en': {'title': 'Unlocking Knowledge Transfer in Heterogeneous Domains', 'desc': 'This paper explores Semi-supervised Heterogeneous Domain Adaptation (SHDA), which involves transferring knowledge from labeled source samples to unlabeled target samples that have different feature representations. The authors conducted extensive experiments on 330 SHDA tasks to understand the nature of knowledge transfer across these domains. Surprisingly, they found that the specific category and feature information of source samples do not significantly influence the performance in the target domain. Instead, they propose a Knowledge Transfer Framework (KTF) that emphasizes the importance of transferability and discriminability of source samples to improve knowledge transfer effectiveness in SHDA tasks.'}, 'zh': {'title': 'Êè≠Á§∫ÂçäÁõëÁù£ÂºÇÊûÑÈ¢ÜÂüüÈÄÇÂ∫î‰∏≠ÁöÑÂèØËΩ¨ÁßªÁü•ËØÜ', 'desc': 'ÂçäÁõëÁù£ÂºÇÊûÑÈ¢ÜÂüüÈÄÇÂ∫îÔºàSHDAÔºâÁ†îÁ©∂Âú®ÁâπÂæÅË°®Á§∫ÂíåÂàÜÂ∏É‰∏çÂêåÁöÑÈ¢ÜÂüü‰πãÈó¥ËøõË°åÂ≠¶‰π†ÁöÑÈóÆÈ¢ò„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÊ∫êÊ†∑Êú¨ÊòØÊúâÊ†áÁ≠æÁöÑÔºåËÄåÂ§ßÂ§öÊï∞ÁõÆÊ†áÊ†∑Êú¨ÊòØÊó†Ê†áÁ≠æÁöÑÔºåÂè™ÊúâÂ∞ëÈáèÊ†∑Êú¨ÊòØÊúâÊ†áÁ≠æÁöÑ„ÄÇÊú¨ÊñáÈÄöËøáÂ§ßÈáèÂÆûÈ™åÊé¢ËÆ®‰∫ÜÂú®ÂºÇÊûÑÈ¢ÜÂüü‰∏≠ÂèØËΩ¨ÁßªÁü•ËØÜÁöÑÊú¨Ë¥®ÔºåÂèëÁé∞Ê∫êÊ†∑Êú¨ÁöÑÁ±ªÂà´ÂíåÁâπÂæÅ‰ø°ÊÅØÂØπÁõÆÊ†áÈ¢ÜÂüüÁöÑÊÄßËÉΩÂΩ±Âìç‰∏çÂ§ß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÁü•ËØÜËΩ¨ÁßªÊ°ÜÊû∂ÔºàKTFÔºâÔºåÂπ∂ÂèëÁé∞ÂèØËΩ¨ÁßªÁü•ËØÜ‰∏ªË¶ÅÊù•Ëá™Ê∫êÈ¢ÜÂüüÁöÑÂèØËΩ¨ÁßªÊÄßÂíåÂèØÂå∫ÂàÜÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05173', 'title': 'VideoRoPE: What Makes for Good Video Rotary Position Embedding?', 'url': 'https://huggingface.co/papers/2502.05173', 'abstract': 'While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.', 'score': 48, 'issue_id': 2118, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'ba284ed1a62b3c2c', 'authors': ['Xilin Wei', 'Xiaoran Liu', 'Yuhang Zang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Cao', 'Jian Tong', 'Haodong Duan', 'Qipeng Guo', 'Jiaqi Wang', 'Xipeng Qiu', 'Dahua Lin'], 'affiliations': ['Fudan University, Shanghai, China', 'Shanghai AI Laboratory, Shanghai, China', 'Shanghai Innovation Institute, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.05173.jpg', 'data': {'categories': ['#hallucinations', '#3d', '#architecture', '#video', '#long_context'], 'emoji': 'üé•', 'ru': {'title': 'VideoRoPE: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤–∏–¥–µ–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoRoPE - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ Rotary Position Embedding. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã—è–≤–∏–ª–∏ 4 –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ RoPE –∫ –≤–∏–¥–µ–æ. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É V-NIAH-D –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ RoPE. VideoRoPE –∏–º–µ–µ—Ç 3D-—Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Å–æ—Ö—Ä–∞–Ω—è—é—â—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è, –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã RoPE –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Enhancing Video Understanding with VideoRoPE', 'desc': 'This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data.'}, 'zh': {'title': 'VideoRoPEÔºöËßÜÈ¢ë‰∏≠ÁöÑÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•Êñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÊúâÊïàÂú∞Êâ©Â±ïÂà∞ËßÜÈ¢ëÊï∞ÊçÆ‰∏≠„ÄÇÁ†îÁ©∂ÂàÜÊûê‰∫ÜÂõõ‰∏™ÂÖ≥ÈîÆÁâπÊÄßÔºåËøô‰∫õÁâπÊÄßÂØπ‰∫éRoPEÂú®ËßÜÈ¢ë‰∏≠ÁöÑÈÄÇÂ∫îÊÄßËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑ‰ªªÂä°V-NIAH-DÔºåÂ±ïÁ§∫‰∫ÜÁé∞ÊúâRoPEÂèò‰ΩìÂú®Â§ÑÁêÜËßÜÈ¢ëÊó∂ÂÆπÊòìÂèóÂà∞Âπ≤Êâ∞ÁöÑÁº∫Èô∑„ÄÇÂü∫‰∫éËøô‰∫õÂàÜÊûêÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVideoRoPEÔºåÂÆÉÈÄöËøá3DÁªìÊûÑÊù•‰øùÊåÅÊó∂Á©∫ÂÖ≥Á≥ªÔºåÂπ∂Âú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫é‰πãÂâçÁöÑRoPEÂèò‰Ωì„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04507', 'title': 'Fast Video Generation with Sliding Tile Attention', 'url': 'https://huggingface.co/papers/2502.04507', 'abstract': 'Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.', 'score': 38, 'issue_id': 2120, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'dcbf1070dac1b391', 'authors': ['Peiyuan Zhang', 'Yongqi Chen', 'Runlong Su', 'Hangliang Ding', 'Ion Stoica', 'Zhenghong Liu', 'Hao Zhang'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego', 'University of Michigan, Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2502.04507.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': 'üéûÔ∏è', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –ø–ª–∏—Ç–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –ø–ª–∏—Ç–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (STA) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. STA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, —á—Ç–æ –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –≤ –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö 3D-–æ–∫–Ω–∞—Ö. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–ª–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–º —É—Ä–æ–≤–Ω–µ. STA —É—Å–∫–æ—Ä—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –≤ 2.8-17 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention-2 –∏ –≤ 1.6-10 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention-3, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞—è –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Efficient Video Generation with Sliding Tile Attention', 'desc': 'This paper presents a new method called sliding tile attention (STA) to improve the efficiency of video generation using Diffusion Transformers (DiTs). Traditional full attention mechanisms are computationally expensive, especially for generating high-resolution videos, leading to long inference times. STA reduces this cost by focusing on localized 3D windows, allowing for faster processing without sacrificing the quality of the generated videos. The implementation of STA achieves significant speedups in attention computation, making it a promising solution for real-time video generation tasks.'}, 'zh': {'title': 'ÊªëÂä®Áì¶ÁâáÊ≥®ÊÑèÂäõÔºöÈ´òÊïàËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊªëÂä®Áì¶ÁâáÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºàSTAÔºâÔºåÊó®Âú®ÊèêÈ´òËßÜÈ¢ëÁîüÊàêÁöÑÊïàÁéá„ÄÇ‰º†ÁªüÁöÑÊâ©Êï£ÂèòÊç¢Âô®Âú®ÁîüÊàêËßÜÈ¢ëÊó∂ËÆ°ÁÆóÊàêÊú¨È´òÔºåËÄåSTAÈÄöËøáÂÖ≥Ê≥®Â±ÄÈÉ®ÁöÑÊó∂Á©∫Âå∫ÂüüÊù•ÂáèÂ∞ëÂÜó‰ΩôËÆ°ÁÆó„ÄÇ‰∏é‰º†ÁªüÁöÑÊªëÂä®Á™óÂè£Ê≥®ÊÑèÂäõ‰∏çÂêåÔºåSTAÈááÁî®‰∫ÜÁ°¨‰ª∂ÂèãÂ•ΩÁöÑËÆæËÆ°ÔºåÈÄêÂùóÂ§ÑÁêÜÔºå‰øùÊåÅ‰∫ÜË°®ËææËÉΩÂäõÁöÑÂêåÊó∂ÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇÁªèËøá‰ºòÂåñÔºåSTAÂú®ËßÜÈ¢ëÁîüÊàê‰ªªÂä°‰∏≠ÊòæËëóÂä†ÈÄü‰∫ÜÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåÈôç‰Ωé‰∫ÜÂª∂ËøüÔºåÂêåÊó∂‰∏çÂΩ±ÂìçÁîüÊàêË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04896', 'title': 'Goku: Flow Based Video Generative Foundation Models', 'url': 'https://huggingface.co/papers/2502.04896', 'abstract': 'This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.', 'score': 30, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'ad6ef6eed233cc90', 'authors': ['Shoufa Chen', 'Chongjian Ge', 'Yuqi Zhang', 'Yida Zhang', 'Fengda Zhu', 'Hao Yang', 'Hongxiang Hao', 'Hui Wu', 'Zhichao Lai', 'Yifei Hu', 'Ting-Che Lin', 'Shilong Zhang', 'Fu Li', 'Chuan Li', 'Xing Wang', 'Yanghua Peng', 'Peize Sun', 'Ping Luo', 'Yi Jiang', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaobing Liu'], 'affiliations': ['Bytedance Inc', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04896.jpg', 'data': {'categories': ['#cv', '#training', '#video', '#architecture', '#data', '#benchmark', '#dataset'], 'emoji': 'üêâ', 'ru': {'title': 'Goku: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Goku –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –≤–∫–ª—é—á–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. Goku –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–∫–∞—Ö, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Goku: Revolutionizing Image and Video Generation with Transformers', 'desc': 'This paper presents Goku, a cutting-edge model for generating images and videos using rectified flow Transformers. It discusses key components that contribute to its high-quality output, such as the data curation process and the design of the model architecture. Goku sets new performance records in various tasks, achieving impressive scores on benchmarks for both text-to-image and text-to-video generation. The authors aim to offer insights and advancements that will benefit researchers working on similar generation models.'}, 'zh': {'title': 'GokuÔºöÂõæÂÉè‰∏éËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Ê†áÊùÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜGokuÔºåËøôÊòØ‰∏ÄÁßçÂÖàËøõÁöÑËÅîÂêàÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÂà©Áî®‰∫Ü‰øÆÊ≠£ÊµÅTransformer‰ª•ÂÆûÁé∞Ë°å‰∏öÈ¢ÜÂÖàÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ËØ¶ÁªÜÈòêËø∞‰∫ÜÈ´òË¥®ÈáèËßÜËßâÁîüÊàêÁöÑÂü∫Á°ÄË¶ÅÁ¥†ÔºåÂåÖÊã¨Êï∞ÊçÆÊï¥ÁêÜÊµÅÁ®ã„ÄÅÊ®°ÂûãÊû∂ÊûÑËÆæËÆ°„ÄÅÊµÅÁöÑÂÖ¨ÂºèÂåñ‰ª•ÂèäÈ´òÊïàÁ®≥ÂÅ•ÁöÑÂ§ßËßÑÊ®°ËÆ≠ÁªÉÂü∫Á°ÄËÆæÊñΩ„ÄÇGokuÊ®°ÂûãÂú®ÂÆöÊÄßÂíåÂÆöÈáèËØÑ‰º∞‰∏≠Ë°®Áé∞‰ºòË∂äÔºå‰∏∫‰∏ªË¶Å‰ªªÂä°ËÆæÂÆö‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåGokuÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠ËææÂà∞‰∫Ü0.76ÁöÑGenEvalÂíå83.65ÁöÑDPG-BenchÔºåÂú®ÊñáÊú¨Âà∞ËßÜÈ¢ë‰ªªÂä°‰∏≠ËææÂà∞‰∫Ü84.85ÁöÑVBench„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05003', 'title': 'QuEST: Stable Training of LLMs with 1-Bit Weights and Activations', 'url': 'https://huggingface.co/papers/2502.05003', 'abstract': 'One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the "optimal" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations.   We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the "true" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.', 'score': 28, 'issue_id': 2122, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'c011c3548ad7a5dd', 'authors': ['Andrei Panferov', 'Jiale Chen', 'Soroush Tabesh', 'Roberto L. Castro', 'Mahdi Nikdan', 'Dan Alistarh'], 'affiliations': ['ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05003.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ QuEST –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è. QuEST –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å –≤–µ—Å–∞–º–∏ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º–∏ –≤ 4 –±–∏—Ç–∞ –∏–ª–∏ –º–µ–Ω—å—à–µ, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FP16. –ú–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤–µ—Å–æ–≤ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –Ω–æ–≤—ã–π –æ—Ü–µ–Ω—â–∏–∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–æ–≤–µ—Ä–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ QuEST –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'QuEST: Revolutionizing Quantization for Efficient Language Models', 'desc': 'This paper introduces QuEST, a novel method for training large language models (LLMs) using quantization-aware training (QAT) with significantly reduced bit-widths. QuEST achieves competitive accuracy with traditional FP16 precision while utilizing weights and activations as low as 4 bits, and even supports stable training with 1-bit representations. The method enhances QAT by employing Hadamard normalization for better quantization and a new trust gradient estimator to minimize errors in gradient calculations. Experiments demonstrate that QuEST maintains stable performance across various hardware-supported precisions and can be efficiently executed with provided GPU kernel support.'}, 'zh': {'title': 'QuESTÔºö‰ΩéÁ≤æÂ∫¶È´òÊïàËÆ≠ÁªÉÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫QuESTÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâÊù•ÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇQuESTËÉΩÂ§üÂú®4‰ΩçÊàñÊõ¥‰ΩéÁöÑÁ≤æÂ∫¶‰∏ãËÆ≠ÁªÉÊ®°ÂûãÔºåÂêåÊó∂‰øùÊåÅ‰∏éFP16Á≤æÂ∫¶Áõ∏ÂΩìÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊîπËøõÊùÉÈáçÂíåÊøÄÊ¥ªÁöÑÈáèÂåñËøáÁ®ãÔºå‰ª•ÂèäÂºïÂÖ•Êñ∞ÁöÑ‰ø°‰ªªÊ¢ØÂ∫¶‰º∞ËÆ°Âô®ÔºåÊù•ÂÆûÁé∞Êõ¥Á®≥ÂÆöÁöÑËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQuESTÂú®ÂêÑÁßçÁ°¨‰ª∂ÊîØÊåÅÁöÑÁ≤æÂ∫¶ËåÉÂõ¥ÂÜÖÈÉΩËÉΩÂÆûÁé∞Á®≥ÂÆöÁöÑÊâ©Â±ïÊÄßÔºåÂπ∂‰∏îÂèØ‰ª•ÊúâÊïàÊâßË°å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05171', 'title': 'Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach', 'url': 'https://huggingface.co/papers/2502.05171', 'abstract': 'We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.', 'score': 24, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': '4386159312d9856b', 'authors': ['Jonas Geiping', 'Sean McLeish', 'Neel Jain', 'John Kirchenbauer', 'Siddharth Singh', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Abhinav Bhatele', 'Tom Goldstein'], 'affiliations': ['ELLIS Institute T√ºbingen, Max-Planck Institute for Intelligent Systems, T√ºbingen AI Center', 'Lawrence Livermore National Laboratory', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2502.05171.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ì–ª—É–±–æ–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Ç–µ–º –Ω–µ—è–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–≥–æ –±–ª–æ–∫–∞, —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞—è—Å—å –¥–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ü–µ–ø–æ—á–∫–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–æ 3,5 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ 800 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –ø–æ–∫–∞–∑–∞–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Scaling Reasoning with Latent Space Computation', 'desc': 'This paper presents a new language model architecture that enhances reasoning capabilities by performing computations in a latent space during test-time. Instead of generating more tokens to scale reasoning, the model uses a recurrent block that allows it to unroll computations to any depth. This method does not rely on specialized training data and can effectively handle small context windows, enabling it to capture complex reasoning patterns. The authors demonstrate that their model, with 3.5 billion parameters, can achieve significant improvements on reasoning tasks, comparable to models with much larger parameter counts.'}, 'zh': {'title': 'ÈöêÂºèÊé®ÁêÜÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ°ÁÆóËÉΩÂäõ', 'desc': 'Êàë‰ª¨Á†îÁ©∂‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËØ≠Ë®ÄÊ®°ÂûãÊû∂ÊûÑÔºåËØ•Êû∂ÊûÑËÉΩÂ§üÈÄöËøáÂú®ÊΩúÂú®Á©∫Èó¥‰∏≠ÈöêÂºèÊé®ÁêÜÊù•Êâ©Â±ïÊµãËØïÊó∂ÁöÑËÆ°ÁÆóËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÈÄöËøáËø≠‰ª£ÈÄíÂΩíÂùóÂ∑•‰ΩúÔºå‰ªéËÄåÂú®ÊµãËØïÊó∂ÂèØ‰ª•Â±ïÂºÄÂà∞‰ªªÊÑèÊ∑±Â∫¶„ÄÇËøô‰∏é‰∏ªÊµÅÊé®ÁêÜÊ®°Âûã‰∏çÂêåÔºåÂêéËÄÖÈÄöËøáÁîüÊàêÊõ¥Â§öÁöÑÊ†áËÆ∞Êù•Â¢ûÂä†ËÆ°ÁÆóÈáè„ÄÇÊàë‰ª¨ÁöÑÊ®°Âûã‰∏çÈúÄË¶ÅÁâπÊÆäÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåËÉΩÂ§üÂ§ÑÁêÜÂ∞èÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£ÔºåÂπ∂‰∏îËÉΩÂ§üÊçïÊçâ‰∏çÊòìÁî®ËØ≠Ë®ÄË°®Á§∫ÁöÑÊé®ÁêÜÁ±ªÂûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05176', 'title': 'AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360¬∞ Unbounded Scene Inpainting', 'url': 'https://huggingface.co/papers/2502.05176', 'abstract': 'Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.', 'score': 22, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': '9b52f2788f53c3c0', 'authors': ['Chung-Ho Wu', 'Yang-Jung Chen', 'Ying-Huan Chen', 'Jie-Ying Lee', 'Bo-Hsu Ke', 'Chun-Wei Tuan Mu', 'Yi-Chuan Huang', 'Chin-Yang Lin', 'Min-Hung Chen', 'Yen-Yu Lin', 'Yu-Lun Liu'], 'affiliations': ['NVIDIA', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05176.jpg', 'data': {'categories': ['#dataset', '#3d'], 'emoji': 'üåê', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: AuraFusion360 –¥–ª—è –±–µ–∑—É–ø—Ä–µ—á–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω', 'desc': 'AuraFusion360 - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ Gaussian Splatting. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º–∞—Å–æ–∫ –Ω–µ–≤–∏–¥–∏–º—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å —É—á–µ—Ç–æ–º –≥–ª—É–±–∏–Ω—ã, –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –≥–ª—É–±–∏–Ω—ã –∏ —É–ª—É—á—à–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ SDEdit –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ç–æ—á–∫–∏ –æ–±–∑–æ—Ä–∞. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –ø–µ—Ä–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö 360-USID –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω —Å –æ—Ö–≤–∞—Ç–æ–º 360 –≥—Ä–∞–¥—É—Å–æ–≤.'}, 'en': {'title': 'Revolutionizing 3D Scene Inpainting with AuraFusion360', 'desc': 'AuraFusion360 is a new method for filling in missing parts of 3D scenes, which is important for applications like virtual reality. It uses Gaussian Splatting to represent scenes and introduces techniques for better identifying what should be filled in, ensuring that the filled areas look realistic from different angles. The method also places points accurately without needing extra training and enhances details to keep the views consistent. Additionally, it comes with a new dataset for testing these 3D inpainting methods, showing that AuraFusion360 performs better than previous techniques in both quality and accuracy.'}, 'zh': {'title': 'AuraFusion360Ôºö‰∏âÁª¥Âú∫ÊôØ‰øÆÂ§çÁöÑÊñ∞Á™ÅÁ†¥', 'desc': '‰∏âÁª¥Âú∫ÊôØ‰øÆÂ§çÂú®ËôöÊãüÁé∞ÂÆûÂíåÂª∫Á≠ëÂèØËßÜÂåñÁ≠âÂ∫îÁî®‰∏≠ÈùûÂ∏∏ÈáçË¶ÅÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®360Â∫¶Êó†ÁïåÂú∫ÊôØ‰∏≠Èù¢‰∏¥ËßÜÂõæ‰∏ÄËá¥ÊÄßÂíåÂá†‰ΩïÁ≤æÂ∫¶ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜAuraFusion360ÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÂèÇËÄÉÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÂú®È´òË¥®ÈáèÁöÑ3DÂú∫ÊôØ‰∏≠ËøõË°åÁâ©‰ΩìÁßªÈô§ÂíåÂ≠îÂ°´ÂÖÖ„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÊ∑±Â∫¶ÊÑüÁü•ÁöÑÊú™ËßÅÊé©Á†ÅÁîüÊàê„ÄÅÈÄÇÂ∫îÊÄßÂºïÂØºÊ∑±Â∫¶Êâ©Êï£ÂíåÂü∫‰∫éSDEditÁöÑÁªÜËäÇÂ¢ûÂº∫ÔºåÁ°Æ‰øùÂ§öËßÜÂõæÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄöËøáÂ§ßÈáèÂÆûÈ™åÔºåAuraFusion360Âú®ÊÑüÁü•Ë¥®ÈáèÂíåÂá†‰ΩïÁ≤æÂ∫¶ÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËÉΩÂ§üÂú®ÂâßÁÉàËßÜËßíÂèòÂåñ‰∏≠‰øùÊåÅÈ´òË¥®ÈáèÁöÑ‰øÆÂ§çÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05163', 'title': 'DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails', 'url': 'https://huggingface.co/papers/2502.05163', 'abstract': 'The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \\ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard.', 'score': 17, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'ae863d89ab71ab51', 'authors': ['Yihe Deng', 'Yu Yang', 'Junkai Zhang', 'Wei Wang', 'Bo Li'], 'affiliations': ['University of California, Los Angeles', 'University of Illinois at Urbana-Champaign', 'VirtueAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05163.jpg', 'data': {'categories': ['#low_resource', '#inference', '#synthetic', '#dataset', '#open_source', '#multilingual', '#rl'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LLM —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π-–æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª–µ–π –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç framework —Å –¥–≤—É–º—è –∏–≥—Ä–æ–∫–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≥–¥–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å-–æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ —ç—Ç–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∫–∞–∫ –∏–≥—Ä–∞ –¥–≤—É—Ö –∏–≥—Ä–æ–∫–æ–≤ —Å –¥–æ–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å—é –∫ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—é –ù—ç—à–∞. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –º–µ–Ω—å—à–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.'}, 'en': {'title': 'Enhancing Multilingual Safety in LLMs with Synthetic Data Generation', 'desc': "This paper introduces a new method for creating guardrail models that help ensure the safe use of large language models (LLMs) by detecting harmful content in multiple languages. The authors propose a two-player Reinforcement Learning framework where a generator and a guardrail model work together in a competitive manner to create high-quality synthetic safety data. They demonstrate that this approach not only improves performance on English safety benchmarks but also significantly enhances the model's ability to handle lower-resource languages. The results show that their method is faster and more efficient, making it a promising solution for developing multilingual safety measures in LLMs."}, 'zh': {'title': 'Â§öËØ≠Ë®ÄÊä§Ê†èÊ®°ÂûãÁöÑÂàõÊñ∞ËøõÂ±ï', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÁ°Æ‰øùÂÖ∂Ë¥üË¥£‰ªª‰ΩøÁî®ÁöÑÊä§Ê†èÊ®°ÂûãÈúÄÊ±ÇÂ¢ûÂä†ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê£ÄÊµã‰∏çÂÆâÂÖ®ÂíåÈùûÊ≥ïÂÜÖÂÆπÊñπÈù¢„ÄÇËôΩÁÑ∂Ëã±ËØ≠ÁöÑÂÆâÂÖ®Êï∞ÊçÆÁõ∏ÂØπ‰∏∞ÂØåÔºå‰ΩÜÁî±‰∫éÂÖ∂‰ªñËØ≠Ë®ÄÂºÄÊîæÊ∫ê‰ª£Á†ÅÂÆâÂÖ®Êï∞ÊçÆÁöÑÁ®ÄÁº∫ÔºåÂ§öËØ≠Ë®ÄÊä§Ê†èÂª∫Ê®°‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåÁé©ÂÆ∂Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂÖ∂‰∏≠ÁîüÊàêÂô®ÂíåÊä§Ê†èÊ®°ÂûãÂØπÊäóÊÄßÂú∞ÂÖ±ÂêåËøõÂåñÔºå‰ª•ÁîüÊàêÈ´òË¥®ÈáèÁöÑÂêàÊàêÊï∞ÊçÆÁî®‰∫éÂ§öËØ≠Ë®ÄÊä§Ê†èËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Â§öËØ≠Ë®ÄÂÆâÂÖ®‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑ‰∏çÂπ≥Ë°°ÈóÆÈ¢ò‰∏ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04403', 'title': 'Agency Is Frame-Dependent', 'url': 'https://huggingface.co/papers/2502.04403', 'abstract': "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.", 'score': 13, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '32ceb8df4d77794a', 'authors': ['David Abel', 'Andr√© Barreto', 'Michael Bowling', 'Will Dabney', 'Shi Dong', 'Steven Hansen', 'Anna Harutyunyan', 'Khimya Khetarpal', 'Clare Lyle', 'Razvan Pascanu', 'Georgios Piliouras', 'Doina Precup', 'Jonathan Richens', 'Mark Rowland', 'Tom Schaul', 'Satinder Singh'], 'affiliations': ['Amii, University of Alberta', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.04403.jpg', 'data': {'categories': ['#rl', '#agi', '#reasoning', '#math'], 'emoji': 'ü§ñ', 'ru': {'title': '–ê–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å: –≤—Å–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–∏—Å—Ç–µ–º—ã –æ—Ç—Å—á–µ—Ç–∞. –û–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —ç—Ç–æ—Ç —Ç–µ–∑–∏—Å, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –∫–ª—é—á–µ–≤—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –°—Ç–∞—Ç—å—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —É—á–µ—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏—Å—Ç–µ–º—ã –æ—Ç—Å—á–µ—Ç–∞ –≤ –∏–∑—É—á–µ–Ω–∏–∏ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –æ–±—Å—É–∂–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Agency in Reinforcement Learning: A Frame-Dependent Perspective', 'desc': 'This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning.'}, 'zh': {'title': 'ËÉΩÂä®ÊÄßÔºö‰æùËµñ‰∫éÊ°ÜÊû∂ÁöÑÁ≥ªÁªüËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÁ≥ªÁªüÁöÑËÉΩÂä®ÊÄßÔºåÁâπÂà´ÊòØÂú®Âº∫ÂåñÂ≠¶‰π†ÁöÑËÉåÊôØ‰∏ã„ÄÇËÉΩÂä®ÊÄßÊòØÊåáÁ≥ªÁªüÊúùÁùÄÁõÆÊ†áÂºïÂØºÁªìÊûúÁöÑËÉΩÂäõÔºå‰ΩÜÂà§Êñ≠‰∏Ä‰∏™Á≥ªÁªüÊòØÂê¶ÂÖ∑Â§áËÉΩÂä®ÊÄßÊòØ‰∏Ä‰∏™Â§çÊùÇÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËÉΩÂä®ÊÄßÊòØ‰æùËµñ‰∫éÂèÇËÄÉÊ°ÜÊû∂ÁöÑÔºå‰ªª‰ΩïÂØπÁ≥ªÁªüËÉΩÂä®ÊÄßÁöÑÊµãÈáèÈÉΩÂøÖÈ°ªÁõ∏ÂØπ‰∫éÊüê‰∏™ÂèÇËÄÉÊ°ÜÊû∂ËøõË°å„ÄÇÈÄöËøáÂì≤Â≠¶ËÆ∫ËØÅÔºåÊàë‰ª¨ÊîØÊåÅËøô‰∏ÄËßÇÁÇπÔºåÂπ∂ËÆ®ËÆ∫‰∫ÜËøô‰∏ÄÁªìËÆ∫ÂØπÂº∫ÂåñÂ≠¶‰π†ÁöÑÂΩ±Âìç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04728', 'title': 'Generating Symbolic World Models via Test-time Scaling of Large Language Models', 'url': 'https://huggingface.co/papers/2502.04728', 'abstract': 'Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.', 'score': 12, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'cd97668cd0eee0ee', 'authors': ['Zhouliang Yu', 'Yuhuan Yuan', 'Tim Z. Xiao', 'Fuxiang Frank Xia', 'Jie Fu', 'Ge Zhang', 'Ge Lin', 'Weiyang Liu'], 'affiliations': ['Environmental Systems Research Institute, Inc.', 'Max Planck Institute for Intelligent Systems, T√ºbingen', 'SEED, Bytedance', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.04728.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#data', '#benchmark', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é PDDL –∏ LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —è–∑—ã–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (PDDL) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–Ω–æ–π —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Best-of-N –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–±–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–º–µ–Ω–æ–≤ PDDL –∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è.'}, 'en': {'title': 'Enhancing LLMs for Optimal Planning with PDDL', 'desc': "This paper addresses the challenge of using Large Language Models (LLMs) for complex planning problems by introducing a method to generate Planning Domain Definition Language (PDDL) domains. PDDL serves as a formal language that helps in creating precise state descriptions, which is crucial for avoiding rule violations and ensuring optimal planning. The authors propose a novel algorithm that enhances LLMs' reasoning capabilities through a Best-of-N sampling approach, followed by fine-grained refinement using verbalized machine learning techniques. Their approach significantly improves the generation of PDDL domains, achieving over 50% success in generating high-quality plans from natural language descriptions without additional training."}, 'zh': {'title': 'Âà©Áî®PDDLÊèêÂçáËßÑÂàíÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËß£ÂÜ≥Â§çÊùÇÁöÑËßÑÂàíÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜÈÅøÂÖçËßÑÂàôËøùÂèçÂíåÁ°Æ‰øùÊúÄ‰ºòÊÄßÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÂºïÂÖ•‰∫ÜËßÑÂàíÈ¢ÜÂüüÂÆö‰πâËØ≠Ë®ÄÔºàPDDLÔºâÔºå‰Ωú‰∏∫‰∏ÄÁßçÁ≤æÁ°ÆÁöÑÁä∂ÊÄÅÊèèËø∞Â∑•ÂÖ∑„ÄÇÈÄöËøáPDDLÔºåÂèØ‰ª•ÁîüÊàêÁ¨¶Âè∑‰∏ñÁïåÊ®°ÂûãÔºåÂπ∂Â∫îÁî®ÁªèÂÖ∏ÊêúÁ¥¢ÁÆóÊ≥ïÔºàÂ¶ÇA*ÔºâÊù•ÂØªÊâæÊúÄ‰ºòËÆ°Âàí„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÁÆóÊ≥ïÔºåÈÄöËøáBest-of-NÈááÊ†∑ÂíåÁªÜËá¥ÁöÑÊú∫Âô®Â≠¶‰π†‰ºòÂåñÔºåÊòæËëóÊèêÈ´ò‰∫ÜPDDLÈ¢ÜÂüüÁöÑÁîüÊàêË¥®ÈáèÔºåÊàêÂäüÁéáË∂ÖËøá50%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05179', 'title': 'FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation', 'url': 'https://huggingface.co/papers/2502.05179', 'abstract': 'DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .', 'score': 9, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'c3147244e03af4a6', 'authors': ['Shilong Zhang', 'Wenbo Li', 'Shoufa Chen', 'Chongjian Ge', 'Peize Sun', 'Yida Zhang', 'Yi Jiang', 'Zehuan Yuan', 'Binyue Peng', 'Ping Luo'], 'affiliations': ['ByteDance', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.05179.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': 'üé¨', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–æ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º FlashVideo. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –ø—Ä–æ–º–ø—Ç—É, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –≤–∏–¥–µ–æ –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –í—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–¥ –ø–æ–ª–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫—É—é –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.'}, 'en': {'title': 'FlashVideo: Efficient Text-to-Video Generation with Two-Stage Framework', 'desc': 'The paper introduces FlashVideo, a two-stage framework for text-to-video generation that improves efficiency and quality. In the first stage, it focuses on generating low-resolution videos with high fidelity to text prompts, using large model parameters and sufficient function evaluations. The second stage enhances the video by matching flow between low and high resolutions, adding fine details while minimizing computational demands. This approach not only achieves high-resolution outputs with better efficiency but also allows users to preview results before full generation, making it more practical for commercial use.'}, 'zh': {'title': 'FlashVideoÔºöÈ´òÊïàÁîüÊàêÈ´òÂàÜËæ®ÁéáËßÜÈ¢ëÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'DiTÊâ©Êï£Ê®°ÂûãÂú®ÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÈ´òÂÜÖÂÆπÂíåËøêÂä®‰øùÁúüÂ∫¶ÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÊ®°ÂûãÂèÇÊï∞ÂíåÂáΩÊï∞ËØÑ‰º∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õËÆ°ÁÆóÈúÄÊ±ÇÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰∏§Èò∂ÊÆµÊ°ÜÊû∂FlashVideoÔºåÊó®Âú®Âπ≥Ë°°ÁîüÊàêÁöÑ‰øùÁúüÂ∫¶ÂíåË¥®Èáè„ÄÇÂú®Á¨¨‰∏ÄÈò∂ÊÆµÔºåÈÄöËøá‰ΩéÂàÜËæ®ÁéáÁîüÊàêËøáÁ®ã‰ºòÂÖàËÄÉËôëÊèêÁ§∫‰øùÁúüÂ∫¶ÔºåÂà©Áî®Â§ßÂèÇÊï∞ÂíåË∂≥Â§üÁöÑÂáΩÊï∞ËØÑ‰º∞ÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÂàôÂú®‰ΩéÂàÜËæ®ÁéáÂíåÈ´òÂàÜËæ®Áéá‰πãÈó¥Âª∫Á´ãÊµÅÂåπÈÖçÔºåÊúâÊïàÁîüÊàêÁªÜËäÇÔºå‰∏îÊâÄÈúÄÁöÑÂáΩÊï∞ËØÑ‰º∞ÊúÄÂ∞èÂåñ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04959', 'title': 'No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces', 'url': 'https://huggingface.co/papers/2502.04959', 'abstract': 'Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .', 'score': 8, 'issue_id': 2127, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'a73679e79ff14b7c', 'authors': ['Daniel Marczak', 'Simone Magistri', 'Sebastian Cygert', 'Bart≈Çomiej Twardowski', 'Andrew D. Bagdanov', 'Joost van de Weijer'], 'affiliations': ['Computer Vision Center, Barcelona, Spain', 'Department of Computer Science, Universitat Autonoma de Barcelona, Spain', 'Department of Information Engineering, University of Florence, Italy', 'Gdansk University of Technology, Poland', 'IDEAS NCBR, Warsaw, Poland', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2502.04959.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'üîÄ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –≤–µ—Å–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤ –æ–¥–Ω—É –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—É—é –º–æ–¥–µ–ª—å. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –º–∞—Ç—Ä–∏—Ü –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –º–æ–¥–µ–ª–∏. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏–∑–æ—Ç—Ä–æ–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å–ø–µ–∫—Ç—Ä —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º–∞—Ç—Ä–∏—Ü –∑–∞–¥–∞—á –∏ —É–ª—É—á—à–∞–µ—Ç –∏—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –∑–∞–¥–∞—á –∏ –º–∞—Å—à—Ç–∞–±—ã –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Bridging the Performance Gap in Model Merging', 'desc': 'This paper focuses on model merging, which combines the weights of different models designed for specific tasks into one model that can handle multiple tasks. The authors identify that there is often a performance gap when comparing the merged model to single-task models. They explore the importance of task matrices, which are weight update matrices, and find that better alignment between the components of these matrices leads to improved performance. To address the performance gap, they introduce an isotropic merging framework that optimizes the alignment of task matrices and incorporates both shared and unique features of tasks, achieving top performance across various scenarios.'}, 'zh': {'title': 'ÊúâÊïàÁöÑÊ®°ÂûãÂêàÂπ∂ÊñπÊ≥ïÊèêÂçáÂ§ö‰ªªÂä°ÊÄßËÉΩ', 'desc': 'Ê®°ÂûãÂêàÂπ∂ÊòØÂ∞ÜÂ§ö‰∏™ÁâπÂÆö‰ªªÂä°Ê®°ÂûãÁöÑÊùÉÈáçÊï¥Âêà‰∏∫‰∏Ä‰∏™Â§ö‰ªªÂä°Ê®°ÂûãÁöÑËøáÁ®ã„ÄÇÂ∞ΩÁÆ°Ëøô‰∏ÄÈóÆÈ¢òÂèóÂà∞Ë∂äÊù•Ë∂äÂ§öÁöÑÂÖ≥Ê≥®Ôºå‰ΩÜÂêàÂπ∂Ê®°Âûã‰∏éÂçï‰ªªÂä°Ê®°Âûã‰πãÈó¥‰ªçÂ≠òÂú®ÊòæËëóÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÊú¨ÊñáÁ†îÁ©∂‰∫Ü‰ªªÂä°Áü©ÈòµÁöÑÂÖ≥ÈîÆÁâπÊÄßÔºåËøô‰∫õÁü©ÈòµÊòØÂ∫îÁî®‰∫éÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÊùÉÈáçÊõ¥Êñ∞Áü©ÈòµÔºåÂèëÁé∞‰ªªÂä°ÁâπÂÆöÁü©Èòµ‰∏éÂêàÂπ∂Áü©Èòµ‰πãÈó¥ÁöÑÂØπÈΩêÁ®ãÂ∫¶‰∏éÊÄßËÉΩÊèêÂçáÂØÜÂàáÁõ∏ÂÖ≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêÑÂêëÂêåÊÄßÂêàÂπ∂Ê°ÜÊû∂ÔºåÈÄöËøáÂπ≥Âù¶Âåñ‰ªªÂä°Áü©ÈòµÁöÑÂ•áÂºÇÂÄºË∞±ÔºåÂ¢ûÂº∫ÂØπÈΩêÊÄßÔºå‰ªéËÄåÁº©Â∞èÊÄßËÉΩÂ∑ÆË∑ùÔºåÂπ∂Âú®Â§ö‰∏™Âú∫ÊôØ‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04520', 'title': "Linear Correlation in LM's Compositional Generalization and Hallucination", 'url': 'https://huggingface.co/papers/2502.04520', 'abstract': 'The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., "X lives in the city of" rightarrow "X lives in the country of" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM\'s generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.', 'score': 8, 'issue_id': 2119, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '41ef9027d1533f06', 'authors': ['Letian Peng', 'Chenyang An', 'Shibo Hao', 'Chengyu Dong', 'Jingbo Shang'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.04520.jpg', 'data': {'categories': ['#interpretability', '#training', '#architecture', '#agi', '#data', '#hallucinations'], 'emoji': 'üß†', 'ru': {'title': '–õ–∏–Ω–µ–π–Ω–æ—Å—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ –æ–±–æ–±—â–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ñ–µ–Ω–æ–º–µ–Ω –ª–∏–Ω–µ–π–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏, –∫–æ—Ç–æ—Ä–æ–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –ª–æ–≥–∏—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –æ—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∫ –¥—Ä—É–≥–æ–º—É. –≠—Ç–æ —è–≤–ª–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ –∫ –º–∞—Å—à—Ç–∞–±–Ω–æ–º—É –¥–æ–æ–±—É—á–µ–Ω–∏—é –∏ –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –æ–±–æ–±—â–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç–∞–∫–∏–µ –ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∏–∑—É—á–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ–π –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤–∞—Ä—è.'}, 'en': {'title': 'Unlocking Language Models: The Power of Linear Correlations in Knowledge Composition', 'desc': 'This paper explores how language models (LMs) handle knowledge composition, revealing that they exhibit linear correlations when predicting the next token. It shows that there is a linear transformation that connects related knowledge, allowing LMs to generalize information effectively, similar to how humans relate concepts. The study finds that while these linear transformations can adapt to new information through fine-tuning, they can also lead to incorrect outputs, or hallucinations, when the relationships are not aligned with reality. Overall, the research suggests that understanding these linear correlations can help identify how well LMs generalize knowledge.'}, 'zh': {'title': 'ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ∫øÊÄßÁõ∏ÂÖ≥ÊÄß‰∏éÁü•ËØÜÁªÑÂêà', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÂú®Áü•ËØÜÁªÑÂêà‰∏≠ÁöÑÁ∫øÊÄßÁõ∏ÂÖ≥Áé∞Ë±°„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊüê‰∫õÁõ∏ÂÖ≥Áü•ËØÜ‰πãÈó¥Â≠òÂú®Á∫øÊÄßÂèòÊç¢ÔºåËøôÁßçÂèòÊç¢ÂèØ‰ª•Â∞Ü‰∏Ä‰∏™ÊèêÁ§∫ÁöÑ‰∏ã‰∏Ä‰∏™Ê†áËÆ∞È¢ÑÊµã‰ªé‰∏Ä‰∏™Êò†Â∞ÑÂà∞Âè¶‰∏Ä‰∏™„ÄÇÊØîÂ¶ÇÔºå‰ªé"X ‰ΩèÂú®ÂüéÂ∏Ç"ÂèØ‰ª•ËΩ¨Âèò‰∏∫"X ‰ΩèÂú®ÂõΩÂÆ∂"„ÄÇÁªìÊûúË°®ÊòéÔºåÁ∫øÊÄßÂèòÊç¢Âú®Â§ßËßÑÊ®°ÂæÆË∞É‰∏≠ÂÖ∑ÊúâÈüßÊÄßÔºåÂπ∂‰∏îÂΩì‰∏éÁé∞ÂÆû‰∏ñÁïåÂÖ≥Á≥ª‰∏ÄËá¥Êó∂ËÉΩÂ§üÊé®ÂπøÊõ¥Êñ∞ÁöÑÁü•ËØÜÔºå‰ΩÜÂΩìÂÅèÁ¶ªÊó∂Âàô‰ºöÂØºËá¥ÂπªËßâ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04416', 'title': 'CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference', 'url': 'https://huggingface.co/papers/2502.04416', 'abstract': 'Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/JarvisPei/CMoE.', 'score': 7, 'issue_id': 2125, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '0b4520b0860c835c', 'authors': ['Zehua Pei', 'Lancheng Zou', 'Hui-Ling Zhen', 'Xianzhi Yu', 'Wulong Liu', 'Sinno Jialin Pan', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04416.jpg', 'data': {'categories': ['#architecture', '#optimization', '#open_source', '#training', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω—ã—Ö LLM –≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ MoE –º–æ–¥–µ–ª–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ CMoE (Carved MoE) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π Mixture-of-Experts –∏–∑ –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). CMoE –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –Ω–µ–π—Ä–æ–Ω—ã –≤ –æ–±—â–∏–µ –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä—É–µ–º—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –≥—Ä—É–ø–ø—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Ä–æ–≤–Ω–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Å –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏. CMoE –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ —Å–æ–∑–¥–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é MoE –º–æ–¥–µ–ª—å –∏–∑ –ø–ª–æ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ–±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Efficiently Carving Mixture-of-Experts for Large Language Models', 'desc': 'This paper introduces CMoE (Carved Mixture-of-Experts), a new framework designed to enhance the efficiency of large language models (LLMs) by utilizing a mixture-of-experts architecture. CMoE takes advantage of the high activation sparsity found in feed-forward networks by activating only a subset of parameters, which reduces inference overhead. The framework groups neurons into shared and routed experts based on their activation rates and employs a differentiable routing mechanism that avoids the need for extensive retraining. Remarkably, CMoE can create a functional MoE model from a dense model in just five minutes using limited data, achieving high performance with minimal fine-tuning.'}, 'zh': {'title': 'È´òÊïàÈõïÂàªÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈÄöËøáÂ¢ûÂä†Ê®°ÂûãÂèÇÊï∞ÂÆûÁé∞‰∫ÜÂá∫Ëâ≤ÁöÑÊÄßËÉΩÔºå‰ΩÜËøô‰πüÂ∏¶Êù•‰∫ÜÊòæËëóÁöÑÊé®ÁêÜÂºÄÈîÄ„ÄÇÂâçÈ¶àÁΩëÁªúÔºàFFNsÔºâÂú®LLMÂèÇÊï∞‰∏≠Âç†‰∏ªÂØºÂú∞‰ΩçÔºåÈöêËóèÁ•ûÁªèÂÖÉÁöÑÊøÄÊ¥ªÁ®ÄÁñèÊÄßÂæàÈ´ò„ÄÇ‰∏∫Ê≠§ÔºåÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫ÜÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑÔºå‰ªÖÊøÄÊ¥ª‰∏ÄÈÉ®ÂàÜÂèÇÊï∞„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂíåËµÑÊ∫êÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂÆûÁî®ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCMoEÔºàCarved MoEÔºâÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•È´òÊïàÂú∞‰ªéÁ®†ÂØÜÊ®°Âûã‰∏≠ÈõïÂàªÂá∫MoEÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04404', 'title': 'Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models', 'url': 'https://huggingface.co/papers/2502.04404', 'abstract': "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.", 'score': 7, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'a7bd201755c7ea1d', 'authors': ['Xiao-Wen Yang', 'Xuan-Yi Zhu', 'Wen-Da Wei', 'Ding-Chu Zhang', 'Jie-Jing Shao', 'Zhi Zhou', 'Lan-Zhe Guo', 'Yu-Feng Li'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University, China', 'School of Artificial Intelligence, Nanjing University, China', 'School of Intelligence Science and Technology, Nanjing University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04404.jpg', 'data': {'categories': ['#agi', '#training', '#inference', '#agents', '#architecture', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π –≤–æ–∑–≤—Ä–∞—Ç: –ø—É—Ç—å –∫ –±–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω—ã–º –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–≥–æ –≤–æ–∑–≤—Ä–∞—Ç–∞ (self-backtracking) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –≠—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –∫–æ–≥–¥–∞ –∏ –≥–¥–µ –Ω—É–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å—Å—è –Ω–∞–∑–∞–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —ç—Ç–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø—Ä–µ–≤—Ä–∞—â–∞—è –º–µ–¥–ª–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –±—ã—Å—Ç—Ä–æ–µ —á–µ—Ä–µ–∑ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞.'}, 'en': {'title': 'Empowering LLMs with Self-Backtracking for Enhanced Reasoning', 'desc': 'This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods.'}, 'zh': {'title': 'Ëá™ÊàëÂõûÊ∫ØÊú∫Âà∂ÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'Â∞ÜÊÖ¢ÊÄùËÄÉÊú∫Âà∂Êï¥ÂêàÂà∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠Ôºå‰∏∫ÂÆûÁé∞‰∫åÁ∫ßAGIÊé®ÁêÜÂô®Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÈÄîÂæÑ„ÄÇÂΩìÂâçÁöÑÊåëÊàòÂåÖÊã¨‰ΩéÊïàÁöÑËøáÂ∫¶ÊÄùËÄÉÂíåÂØπËæÖÂä©Â•ñÂä±Ê®°ÂûãÁöÑËøáÂ∫¶‰æùËµñ„ÄÇÊàë‰ª¨ÊåáÂá∫ÔºåËøô‰∫õÈôêÂà∂Ê∫ê‰∫éLLMsÊó†Ê≥ïÂÜÖÂåñÊêúÁ¥¢ËøáÁ®ãÔºåËÄåÊêúÁ¥¢ËøáÁ®ãÊòØÊúâÊïàÊé®ÁêÜÁöÑÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÊàëÂõûÊ∫ØÊú∫Âà∂Ôºå‰ΩøLLMsËÉΩÂ§üÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠Ëá™‰∏ªÂÜ≥ÂÆö‰ΩïÊó∂‰ª•ÂèäÂ¶Ç‰ΩïÂõûÊ∫ØÔºå‰ªéËÄåÊòæËëóÊèêÂçáÊé®ÁêÜËÉΩÂäõÂíåÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03738', 'title': 'Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More', 'url': 'https://huggingface.co/papers/2502.03738', 'abstract': 'Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch_Scaling.', 'score': 6, 'issue_id': 2122, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'aa76478090a36c04', 'authors': ['Feng Wang', 'Yaodong Yu', 'Guoyizhe Wei', 'Wei Shao', 'Yuyin Zhou', 'Alan Yuille', 'Cihang Xie'], 'affiliations': ['Johns Hopkins University', 'UC Berkeley', 'UC Santa Cruz', 'University of Florida'], 'pdf_title_img': 'assets/pdf/title_img/2502.03738.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#cv'], 'emoji': 'üîç', 'ru': {'title': '–ü–∏–∫—Å–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–∞—Ç—á–∏ –≤ vision-–º–æ–¥–µ–ª—è—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–Ω–∞–ª–∏–∑—É –≤–ª–∏—è–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–µ–π –≤ Vision Transformer (ViT) –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–µ–π –¥–æ 1x1 –ø–∏–∫—Å–µ–ª—è (–ø–∏–∫—Å–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è) —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –≠—Ç–æ—Ç —ç—Ñ—Ñ–µ–∫—Ç –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –º–∞—Å—à—Ç–∞–±–æ–≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –≤–∫–ª—é—á–∞—è ViT –∏ Mamba. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª—å –±–∞–∑–æ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å –¥–ª–∏–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 50 176 —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 84,6% –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö ImageNet-1k.'}, 'en': {'title': 'Unlocking Visual Potential: The Power of Smaller Patches in Vision Transformers', 'desc': 'This paper investigates the impact of patchification, a method of dividing images into smaller sections, on the performance of Vision Transformers (ViT) in visual tasks. The authors find that reducing the size of these patches leads to better predictive performance, with the optimal size being 1x1 pixels, which represents pixel tokenization. They conduct experiments that demonstrate this scaling law across various architectures and tasks, revealing that smaller patches diminish the need for complex decoder heads in dense prediction tasks. The study achieves a significant milestone by processing a visual sequence of 50,176 tokens, achieving a competitive accuracy on the ImageNet-1k benchmark, and aims to inform future developments in non-compressive vision models.'}, 'zh': {'title': 'Â∞èÂùóÊõ¥‰ºòÔºåËßÜËßâÁêÜËß£Êõ¥Âº∫ÔºÅ', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜËßÜËßâÂèòÊç¢Âô®ÔºàViTÔºâ‰∏≠ÂõæÂÉèÂàÜÂùóÔºàpatchificationÔºâÂØπ‰ø°ÊÅØÊçüÂ§±ÁöÑÂΩ±Âìç„ÄÇÈÄöËøáÁº©Â∞èÂõæÂÉèÁöÑÁ©∫Èó¥Â§ßÂ∞èÔºåÂàÜÂùóÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂáèÂ∞ë‰ª§ÁâåÂ∫èÂàóÁöÑÈïøÂ∫¶Ôºå‰ªéËÄåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÈöèÁùÄÂàÜÂùóÂ§ßÂ∞èÁöÑÂáèÂ∞èÔºåÊ®°ÂûãÁöÑÈ¢ÑÊµãÊÄßËÉΩÊåÅÁª≠ÊèêÈ´òÔºåÁõ¥Âà∞ËææÂà∞ÊúÄÂ∞èÁöÑ1x1ÂÉèÁ¥†ÂàÜÂùó„ÄÇËØ•Á†îÁ©∂ÁªìÊûúÈÄÇÁî®‰∫éÂ§öÁßçËßÜËßâ‰ªªÂä°Âíå‰∏çÂêåÁöÑÊ®°ÂûãÊû∂ÊûÑÔºå‰∏∫Êú™Êù•ÊûÑÂª∫ÈùûÂéãÁº©ËßÜËßâÊ®°ÂûãÊèê‰æõ‰∫ÜÁêÜËÆ∫Âü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05178', 'title': 'QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.05178', 'abstract': 'We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.', 'score': 6, 'issue_id': 2121, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'bbc1f1a0b7f5423f', 'authors': ['Yue Zhao', 'Fuzhao Xue', 'Scott Reed', 'Linxi Fan', 'Yuke Zhu', 'Jan Kautz', 'Zhiding Yu', 'Philipp Kr√§henb√ºhl', 'De-An Huang'], 'affiliations': ['NVIDIA', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.05178.jpg', 'data': {'categories': ['#multimodal', '#cv', '#training', '#architecture'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'QLIP: –ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'QLIP - —ç—Ç–æ –º–µ—Ç–æ–¥ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —Å –±–∏–Ω–∞—Ä–Ω–æ-—Å—Ñ–µ—Ä–∏—á–µ—Å–∫–∏–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —è–∑—ã–∫–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ—á–µ—Ç–∞–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –±–æ–ª—å—à–∏–º –±–∞—Ç—á–∞–º –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ –ø–∞–º—è—Ç–∏. QLIP –º–æ–∂–µ—Ç –∑–∞–º–µ–Ω–∏—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, –ø–æ–∫–∞–∑—ã–≤–∞—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—É—é –∏–ª–∏ –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'QLIP: Bridging Visual and Language Understanding with Efficient Tokenization', 'desc': 'The paper presents Quantized Language-Image Pretraining (QLIP), a novel method for visual tokenization that achieves high-quality image reconstruction while enhancing zero-shot image understanding. QLIP employs a binary-spherical-quantization-based autoencoder, integrating both reconstruction and language-image alignment objectives in a harmonious manner. By dynamically balancing these objectives during training, the method effectively addresses the challenges of large-batch image-language pre-training and memory constraints. The results show that QLIP can replace existing visual encoders and image tokenizers in models like LLaVA and LlamaGen, improving performance in multimodal understanding and text-conditioned image generation.'}, 'zh': {'title': 'ÈáèÂåñËØ≠Ë®Ä-ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÔºöÂ§öÊ®°ÊÄÅÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÈáèÂåñËØ≠Ë®Ä-ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÔºàQLIPÔºâÔºåËøôÊòØ‰∏ÄÁßçËßÜËßâÊ†áËÆ∞ÂåñÊñπÊ≥ïÔºåÁªìÂêà‰∫ÜÊúÄÂÖàËøõÁöÑÈáçÂª∫Ë¥®ÈáèÂíåÈõ∂-shotÂõæÂÉèÁêÜËß£ËÉΩÂäõ„ÄÇQLIP‰ΩøÁî®Âü∫‰∫é‰∫åÂÖÉÁêÉÈù¢ÈáèÂåñÁöÑËá™ÁºñÁ†ÅÂô®ËøõË°åËÆ≠ÁªÉÔºåÂêåÊó∂‰ºòÂåñÈáçÂª∫ÂíåËØ≠Ë®Ä-ÂõæÂÉèÂØπÈΩêÁõÆÊ†á„ÄÇÊàë‰ª¨È¶ñÊ¨°ËØÅÊòéËøô‰∏§‰∏™ÁõÆÊ†áÂèØ‰ª•Âä®ÊÄÅÂπ≥Ë°°ÔºåËÄå‰∏çÊòØÁõ∏‰∫íÂØπÁ´ã„ÄÇQLIPÂú®Â§öÊ®°ÊÄÅÁêÜËß£ÂíåÊñáÊú¨Êù°‰ª∂ÂõæÂÉèÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂèØ‰ª•‰Ωú‰∏∫LLaVAÁöÑËßÜËßâÁºñÁ†ÅÂô®ÂíåLlamaGenÁöÑÂõæÂÉèÊ†áËÆ∞Âô®ÁöÑÊõø‰ª£ÊñπÊ°àÔºåÊÄßËÉΩÁõ∏ÂΩìÊàñÊõ¥Â•Ω„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04363', 'title': 'On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices', 'url': 'https://huggingface.co/papers/2502.04363', 'abstract': 'We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.', 'score': 6, 'issue_id': 2119, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '339b45dee733174c', 'authors': ['Bosung Kim', 'Kyuhwan Lee', 'Isu Jeong', 'Jungmin Cheon', 'Yeojin Lee', 'Seulki Lee'], 'affiliations': ['Ulsan National Institute of Science and Technology South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.04363.jpg', 'data': {'categories': ['#inference', '#video', '#open_source', '#diffusion', '#architecture', '#low_resource'], 'emoji': 'üì±', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É –ø—Ä—è–º–æ –Ω–∞ –≤–∞—à–µ–º —Å–º–∞—Ä—Ç—Ñ–æ–Ω–µ', 'desc': 'On-device Sora –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—â–µ–µ –Ω–∞ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–∞—Ö. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç—Ä–∏ –Ω–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏: Linear Proportional Leap (LPL) –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —à–∞–≥–æ–≤ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞, Temporal Dimension Token Merging (TDTM) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ —Å–ª–æ—è—Ö –≤–Ω–∏–º–∞–Ω–∏—è, –∏ Concurrent Inference with Dynamic Loading (CI-DL) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ iPhone 15 Pro –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ On-device Sora —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ Open-Sora –Ω–∞ –º–æ—â–Ω—ã—Ö GPU.'}, 'en': {'title': 'Empowering Video Creation on Your Smartphone!', 'desc': 'On-device Sora is a groundbreaking solution for generating videos from text using diffusion models directly on smartphones. It introduces three innovative techniques: Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to optimize token processing in attention layers, and Concurrent Inference with Dynamic Loading (CI-DL) to manage memory efficiently. These advancements allow the system to produce high-quality videos comparable to those generated by powerful GPUs, all while operating within the constraints of mobile devices. This work not only enhances accessibility to advanced generative technologies but also prioritizes user privacy and reduces reliance on cloud services.'}, 'zh': {'title': 'ÁßªÂä®ËÆæÂ§á‰∏äÁöÑÈ´òÊïàËßÜÈ¢ëÁîüÊàêÊñ∞Á™ÅÁ†¥', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫ÜOn-device SoraÔºåËøôÊòØÈ¶ñ‰∏™Âü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÁßªÂä®ËÆæÂ§áÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêËß£ÂÜ≥ÊñπÊ°àÔºåËÉΩÂ§üÈ´òÊïàÂú∞Âú®Êô∫ËÉΩÊâãÊú∫‰∏äËøêË°å„ÄÇËØ•Á≥ªÁªüÈááÁî®‰∫Ü‰∏âÁßçÊñ∞ÊäÄÊúØÊù•Ëß£ÂÜ≥ÁßªÂä®ËÆæÂ§áÂú®ËÆ°ÁÆóÂíåÂÜÖÂ≠òÊñπÈù¢ÁöÑÈôêÂà∂„ÄÇÈ¶ñÂÖàÔºåÁ∫øÊÄßÊØî‰æãË∑≥Ë∑ÉÔºàLPLÔºâÈÄöËøáÈ´òÊïàÁöÑË∑≥Ë∑ÉÊñπÊ≥ïÂáèÂ∞ë‰∫ÜËßÜÈ¢ëÊâ©Êï£‰∏≠ÊâÄÈúÄÁöÑÂéªÂô™Ê≠•È™§„ÄÇÂÖ∂Ê¨°ÔºåÊó∂Èó¥Áª¥Â∫¶‰ª§ÁâåÂêàÂπ∂ÔºàTDTMÔºâÈÄöËøáÊ≤øÊó∂Èó¥Áª¥Â∫¶ÂêàÂπ∂ËøûÁª≠‰ª§ÁâåÔºåÈôç‰Ωé‰∫ÜÊ≥®ÊÑèÂäõÂ±Ç‰∏≠ÂØÜÈõÜÁöÑ‰ª§ÁâåÂ§ÑÁêÜËÆ°ÁÆó„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05092', 'title': 'Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs', 'url': 'https://huggingface.co/papers/2502.05092', 'abstract': "Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) ClockQA, which comprises various types of clock styles-standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks-paired with time related questions; and 2) CalendarQA, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.", 'score': 5, 'issue_id': 2128, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'ea9f14d34d4cbb60', 'authors': ['Rohit Saxena', 'Aryo Pradipta Gema', 'Pasquale Minervini'], 'affiliations': ['ILCC, School of Informatics, University of Edinburgh', 'Miniml.AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05092.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#cv', '#interpretability'], 'emoji': '‚è∞', 'ru': {'title': '–í—Ä–µ–º—è –±—Ä–æ—Å–∞–µ—Ç –≤—ã–∑–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏ –¥–∞—Ç —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–æ–≥–æ–≤—ã–µ —á–∞—Å—ã –∏ –∫–∞–ª–µ–Ω–¥–∞—Ä–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π ClockQA —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç–∏–ª—è–º–∏ —á–∞—Å–æ–≤ –∏ CalendarQA —Å –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏. –¶–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∫–∞–∫ MLLM –≤—ã–ø–æ–ª–Ω—è—é—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, —á–∏—Å–ª–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∞–¥–µ–∂–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Å—Ç–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π –¥–ª—è MLLM, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è.'}, 'en': {'title': 'Unlocking Time: Challenges for Multimodal Language Models', 'desc': "This paper explores how multimodal large language models (MLLMs) understand time through visual aids like clocks and calendars. The authors created a dataset with two parts: ClockQA, which includes different clock styles and related questions, and CalendarQA, featuring yearly calendars with various date-related queries. The study assesses the models' abilities in visual recognition, numerical reasoning, and temporal inference when interpreting these time-related images. Despite progress in MLLMs, the findings indicate that accurately grasping time concepts continues to be a major hurdle."}, 'zh': {'title': 'ÁêÜËß£Êó∂Èó¥ÁöÑÊåëÊàòÔºöÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄß', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÁêÜËß£Êó∂Èó¥ÂíåÊó•ÊúüÊñπÈù¢ÁöÑËÉΩÂäõÔºåÁâπÂà´ÊòØÈÄöËøáÊ®°ÊãüÊó∂ÈíüÂíåÂπ¥Â∫¶Êó•ÂéÜÁöÑËßÜËßâË°®Á§∫„ÄÇÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨‰∏§ÈÉ®ÂàÜÔºöClockQAÔºåÂåÖÂê´‰∏çÂêåÈ£éÊ†ºÁöÑÊó∂ÈíüÂèäÁõ∏ÂÖ≥Êó∂Èó¥ÈóÆÈ¢òÔºõCalendarQAÔºåÂåÖÂê´Âπ¥Â∫¶Êó•ÂéÜÂõæÂÉèÂèäÂ∏∏ËßÅÊó•ÊúüÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÂàÜÊûêMLLMsÂú®Â§ÑÁêÜ‰∏éÊó∂Èó¥Áõ∏ÂÖ≥ÁöÑËßÜËßâÊï∞ÊçÆÊó∂ÁöÑËßÜËßâËØÜÂà´„ÄÅÊï∞ÂÄºÊé®ÁêÜÂíåÊó∂Èó¥Êé®Êñ≠ËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÂèñÂæó‰∫Ü‰∏Ä‰∫õËøõÂ±ïÔºå‰ΩÜMLLMsÂú®ÂèØÈù†ÁêÜËß£Êó∂Èó¥ÊñπÈù¢‰ªçÈù¢‰∏¥ÈáçÂ§ßÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03512', 'title': 'YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment', 'url': 'https://huggingface.co/papers/2502.03512', 'abstract': 'Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that generated visuals not only accurately encapsulate user intents but also conform to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini fiasco, where misaligned outputs triggered significant public backlash, underscore the critical need for robust alignment mechanisms. In contrast, Large Language Models (LLMs) have achieved notable success in alignment. Building on these advancements, researchers are eager to apply similar alignment techniques, such as Direct Preference Optimization (DPO), to T2I systems to enhance image generation fidelity and reliability.   We present YinYangAlign, an advanced benchmarking framework that systematically quantifies the alignment fidelity of T2I systems, addressing six fundamental and inherently contradictory design objectives. Each pair represents fundamental tensions in image generation, such as balancing adherence to user prompts with creative modifications or maintaining diversity alongside visual coherence. YinYangAlign includes detailed axiom datasets featuring human prompts, aligned (chosen) responses, misaligned (rejected) AI-generated outputs, and explanations of the underlying contradictions.', 'score': 4, 'issue_id': 2122, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'ebf6482c46f8fe2f', 'authors': ['Amitava Das', 'Yaswanth Narsupalli', 'Gurpreet Singh', 'Vinija Jain', 'Vasu Sharma', 'Suranjana Trivedy', 'Aman Chadha', 'Amit Sheth'], 'affiliations': ['Amazon AI, USA', 'Artificial Intelligence Institute, University of South Carolina, USA', 'Meta AI, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.03512.jpg', 'data': {'categories': ['#benchmark', '#rag', '#rlhf', '#ethics', '#alignment'], 'emoji': 'üé≠', 'ru': {'title': '–ë–∞–ª–∞–Ω—Å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: YinYangAlign –Ω–∞ —Å—Ç—Ä–∞–∂–µ —Ç–æ—á–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç YinYangAlign - –ø–µ—Ä–µ–¥–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ-–∏–∑–æ–±—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã—Ö (T2I) –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —à–µ—Å—Ç–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã—Ö —Ü–µ–ª—è—Ö –¥–∏–∑–∞–π–Ω–∞, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö –∫–ª—é—á–µ–≤—ã–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. YinYangAlign –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏, –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–º–∏ –∏ –Ω–µ–≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –ò–ò, –∞ —Ç–∞–∫–∂–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π. –°–∏—Å—Ç–µ–º–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—Ä–∏–º–µ–Ω—è—è –º–µ—Ç–æ–¥—ã –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, —É—Å–ø–µ—à–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö.'}, 'en': {'title': 'Enhancing Image Generation Fidelity with YinYangAlign', 'desc': 'This paper discusses the importance of precise alignment in Text-to-Image (T2I) systems to ensure that generated images meet user expectations and ethical standards. It highlights past failures, like the Google Gemini incident, which demonstrate the need for better alignment strategies. The authors propose YinYangAlign, a new benchmarking framework that evaluates T2I systems based on six conflicting design goals, such as user prompt adherence versus creative freedom. By using detailed datasets that include human prompts and AI outputs, YinYangAlign aims to improve the fidelity and reliability of image generation.'}, 'zh': {'title': 'ÊèêÂçáÊñáÊú¨Âà∞ÂõæÂÉèÁ≥ªÁªüÁöÑÂØπÈΩêÁ≤æÂ∫¶', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉèÔºàT2IÔºâÁ≥ªÁªü‰∏≠Á≤æÁ°ÆÂØπÈΩêÁöÑÈáçË¶ÅÊÄßÔºå‰ª•Á°Æ‰øùÁîüÊàêÁöÑÂõæÂÉèÊó¢ËÉΩÂáÜÁ°ÆÂèçÊò†Áî®Êà∑ÊÑèÂõæÔºåÂèàÁ¨¶Âêà‰º¶ÁêÜÂíåÁæéÂ≠¶Ê†áÂáÜ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂÉèGoogle GeminiËøôÊ†∑ÁöÑ‰∫ã‰ª∂Âá∏Êòæ‰∫ÜÂº∫Â§ßÂØπÈΩêÊú∫Âà∂ÁöÑÂøÖË¶ÅÊÄß„ÄÇ‰∏éÊ≠§Áõ∏ÊØîÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂØπÈΩêÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºåÁ†îÁ©∂‰∫∫ÂëòÂ∏åÊúõÂ∞ÜÁ±ª‰ººÁöÑÂØπÈΩêÊäÄÊúØÂ∫îÁî®‰∫éT2IÁ≥ªÁªüÔºå‰ª•ÊèêÈ´òÂõæÂÉèÁîüÊàêÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜYinYangAlignÔºåËøôÊòØ‰∏Ä‰∏™ÂÖàËøõÁöÑÂü∫ÂáÜÊ°ÜÊû∂ÔºåÁ≥ªÁªüÂú∞ÈáèÂåñT2IÁ≥ªÁªüÁöÑÂØπÈΩê‰øùÁúüÂ∫¶ÔºåËß£ÂÜ≥‰∫ÜÂÖ≠‰∏™Âü∫Êú¨‰∏îÂÜÖÂú®ÁüõÁõæÁöÑËÆæËÆ°ÁõÆÊ†á„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04350', 'title': 'CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance', 'url': 'https://huggingface.co/papers/2502.04350', 'abstract': 'Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.', 'score': 4, 'issue_id': 2118, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'ad7829c09c28de41', 'authors': ['Yongchao Chen', 'Yilun Hao', 'Yueying Liu', 'Yang Zhang', 'Chuchu Fan'], 'affiliations': ['Harvard University, Boston, MA, USA', 'MIT-IBM Watson AI Lab, Boston, MA, USA', 'Massachusetts Institute of Technology, Boston, MA, USA', 'University of Illinois Urbana-Champaign, Urbana, IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.04350.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#rlhf', '#open_source', '#optimization', '#reasoning'], 'emoji': 'üß≠', 'ru': {'title': 'CodeSteer: –£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Ä–∞—Å–∫—Ä—ã—Ç–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ LLM –≤ —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö', 'desc': 'CodeSteer - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∫–æ–¥–∞ –∏ —Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SymBench –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –û–Ω–∏ –¥–æ–æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å Llama-3-8B —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∞—è –º–æ–¥–µ–ª—å CodeSteerLLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥—Ä—É–≥–∏—Ö LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥–∞–∂–µ –ª—É—á—à–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'CodeSteer: Guiding LLMs to Master Code and Reasoning!', 'desc': 'This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks.'}, 'zh': {'title': 'CodeSteerÔºöÂºïÂØºLLMÂÆûÁé∞Á¨¶Âè∑ËÆ°ÁÆóÁöÑÁ™ÅÁ†¥', 'desc': 'Áé∞ÊúâÁöÑÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂºïÂØºÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊñáÊú¨Êé®ÁêÜÂíå‰ª£Á†ÅÁîüÊàê‰πãÈó¥ÂàáÊç¢ÔºåÂØºËá¥Á¨¶Âè∑ËÆ°ÁÆóËÉΩÂäõÊú™ÂæóÂà∞ÂÖÖÂàÜÂà©Áî®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫CodeSteerÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÊåáÂØºLLMÁöÑ‰ª£Á†ÅÂíåÊñáÊú¨ÁîüÊàê„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜSymBenchÔºåÂåÖÂê´37‰∏™ÂÖ∑ÊúâÂèØË∞ÉÂ§çÊùÇÂ∫¶ÁöÑÁ¨¶Âè∑‰ªªÂä°ÔºåÂπ∂ÂêàÊàê‰∫ÜÂåÖÂê´1.2‰∏áÂ§öËΩÆÊåáÂØº/ÁîüÊàêËΩ®ËøπÂíå5500ÂØπÊåáÂØºÊØîËæÉÁöÑÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøáÂØπLlama-3-8BÊ®°ÂûãËøõË°åÂ§öËΩÆÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÔºåÊàë‰ª¨ÂæóÂà∞ÁöÑCodeSteerLLMÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂºïÂØºÊõ¥Â§ßÊ®°ÂûãÁöÑ‰ª£Á†Å/ÊñáÊú¨ÁîüÊàê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04327', 'title': 'Value-Based Deep RL Scales Predictably', 'url': 'https://huggingface.co/papers/2502.04327', 'abstract': 'Scaling data and compute is critical to the success of machine learning. However, scaling demands predictability: we want methods to not only perform well with more compute or data, but also have their performance be predictable from small-scale runs, without running the large-scale experiment. In this paper, we show that value-based off-policy RL methods are predictable despite community lore regarding their pathological behavior. First, we show that data and compute requirements to attain a given performance level lie on a Pareto frontier, controlled by the updates-to-data (UTD) ratio. By estimating this frontier, we can predict this data requirement when given more compute, and this compute requirement when given more data. Second, we determine the optimal allocation of a total resource budget across data and compute for a given performance and use it to determine hyperparameters that maximize performance for a given budget. Third, this scaling behavior is enabled by first estimating predictable relationships between hyperparameters, which is used to manage effects of overfitting and plasticity loss unique to RL. We validate our approach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI gym, and IsaacGym, when extrapolating to higher levels of data, compute, budget, or performance.', 'score': 3, 'issue_id': 2133, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '4b04abb62254054b', 'authors': ['Oleh Rybkin', 'Michal Nauman', 'Preston Fu', 'Charlie Snell', 'Pieter Abbeel', 'Sergey Levine', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'University of California, Berkeley', 'University of Warsaw'], 'pdf_title_img': 'assets/pdf/title_img/2502.04327.jpg', 'data': {'categories': ['#rl', '#training', '#optimization'], 'emoji': 'üìà', 'ru': {'title': '–ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤–Ω–µ –ø–æ–ª–∏—Ç–∏–∫–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–µ—Å—É—Ä—Å–∞–º –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ª–µ–∂–∞—Ç –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –ü–∞—Ä–µ—Ç–æ, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –∫ –¥–∞–Ω–Ω—ã–º (UTD). –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —ç—Ç–æ–π –≥—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ —Ä–µ—Å—É—Ä—Å–∞–º –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏. –ü–æ–¥—Ö–æ–¥ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ç—Ä–µ—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö (SAC, BRO, PQL) –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Predictable Scaling in Reinforcement Learning', 'desc': 'This paper discusses how to effectively scale data and compute resources in machine learning, particularly in reinforcement learning (RL). It demonstrates that value-based off-policy RL methods can be predictable, contrary to common beliefs about their erratic behavior. The authors introduce a Pareto frontier that helps estimate the data and compute requirements for achieving desired performance levels. They also provide a method for optimizing resource allocation and hyperparameters to enhance performance while managing overfitting and plasticity loss in RL.'}, 'zh': {'title': 'ÂèØÈ¢ÑÊµãÁöÑÂº∫ÂåñÂ≠¶‰π†Êâ©Â±ïÊñπÊ≥ï', 'desc': 'Âú®Êú∫Âô®Â≠¶‰π†‰∏≠ÔºåÊâ©Â±ïÊï∞ÊçÆÂíåËÆ°ÁÆóËÉΩÂäõÊòØÊàêÂäüÁöÑÂÖ≥ÈîÆ„ÄÇÁÑ∂ËÄåÔºåÊâ©Â±ïÈúÄË¶ÅÂèØÈ¢ÑÊµãÊÄßÔºöÊàë‰ª¨Â∏åÊúõÊñπÊ≥ï‰∏ç‰ªÖÂú®Êõ¥Â§öËÆ°ÁÆóÊàñÊï∞ÊçÆ‰∏ãË°®Áé∞ËâØÂ•ΩÔºåËÄå‰∏îÂÖ∂ÊÄßËÉΩÂèØ‰ª•‰ªéÂ∞èËßÑÊ®°ÂÆûÈ™å‰∏≠È¢ÑÊµã„ÄÇÊú¨ÊñáÂ±ïÁ§∫‰∫ÜÂü∫‰∫é‰ª∑ÂÄºÁöÑÁ¶ªÁ∫øÁ≠ñÁï•Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÂèØÈ¢ÑÊµãÊÄßÊñπÈù¢ÁöÑË°®Áé∞ÔºåÂ∞ΩÁÆ°Á§æÂå∫ÊôÆÈÅçËÆ§‰∏∫ÂÖ∂Ë°å‰∏∫‰∏çÁ®≥ÂÆö„ÄÇÊàë‰ª¨ÈÄöËøá‰º∞ËÆ°Â∏ïÁ¥ØÊâòÂâçÊ≤øÔºåÈ¢ÑÊµãÂú®ÁªôÂÆöËÆ°ÁÆóÊàñÊï∞ÊçÆÊó∂ÊâÄÈúÄÁöÑËµÑÊ∫êÔºåÂπ∂Á°ÆÂÆöÂú®ÁâπÂÆöÈ¢ÑÁÆó‰∏ãÁöÑÊúÄ‰Ω≥ËµÑÊ∫êÂàÜÈÖçÔºå‰ª•ÊúÄÂ§ßÂåñÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04376', 'title': 'MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf', 'url': 'https://huggingface.co/papers/2502.04376', 'abstract': 'In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.', 'score': 3, 'issue_id': 2121, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '049ab6200a9d2eae', 'authors': ['Lingxiang Hu', 'Shurun Yuan', 'Xiaoting Qin', 'Jue Zhang', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Microsoft', 'Northeastern University, China', 'Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04376.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#agi', '#benchmark'], 'emoji': 'ü§ñ', 'ru': {'title': 'LLM –∫–∞–∫ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –¥–µ–ª–µ–≥–∞—Ç—ã: –±—É–¥—É—â–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–µ—â–∞–Ω–∏–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø—Ä–æ—Ç–æ—Ç–∏–ø —Å–∏—Å—Ç–µ–º—ã –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —É—á–∞—Å—Ç–∏—è –≤ —Å–æ–≤–µ—â–∞–Ω–∏—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–µ–Ω–æ–≥—Ä–∞–º–º—ã —Å–æ–≤–µ—â–∞–Ω–∏–π, –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –≤ —Ä–æ–ª–∏ –¥–µ–ª–µ–≥–∞—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ–∫–æ–ª–æ 60% –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—é—Ç –∫–∞–∫ –º–∏–Ω–∏–º—É–º –æ–¥–∏–Ω –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç –∏–∑ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ –≤ —Å–Ω–∏–∂–µ–Ω–∏–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –æ—à–∏–±–∫–∞–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –ø—Ä–æ–±–ª–µ–º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–µ–ª–µ–≥–∞—Ç–æ–≤ –Ω–∞ —Å–æ–≤–µ—â–∞–Ω–∏—è—Ö, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Ü–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã –¥–ª—è –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering Meetings with LLMs: Balancing Engagement and Efficiency', 'desc': 'This paper investigates the use of Large Language Models (LLMs) as meeting delegates to improve the efficiency of workplace meetings. A prototype system was developed and evaluated using real meeting transcripts, revealing that different LLMs exhibit varying engagement strategies during discussions. The results indicate that while some models effectively address key points, there is a need for improvement in handling irrelevant content and transcription errors. The study highlights both the potential benefits and challenges of integrating LLMs into meeting processes, providing insights for future applications.'}, 'zh': {'title': 'Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰ºòÂåñ‰ºöËÆÆÂèÇ‰∏é', 'desc': 'Âú®Áé∞‰ª£Â∑•‰ΩúÂú∫ÊâÄÔºå‰ºöËÆÆÊòØ‰∫§ÊµÅÊÄùÊÉ≥ÂíåÁ°Æ‰øùÂõ¢Èòü‰∏ÄËá¥ÊÄßÁöÑÈáçË¶ÅÁéØËäÇÔºå‰ΩÜÂ∏∏Â∏∏Èù¢‰∏¥Êó∂Èó¥Ê∂àËÄó„ÄÅÊó•Á®ãÂÜ≤Á™ÅÂíåÂèÇ‰∏éÊïàÁéá‰Ωé‰∏ãÁ≠âÊåëÊàò„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ºöËÆÆ‰∏≠ÊúâÊïàÂàÜÈÖçÂèÇ‰∏éËÄÖÁöÑËÉΩÂäõÔºåÂºÄÂèë‰∫Ü‰∏Ä‰∏™Âü∫‰∫éLLMÁöÑ‰ºöËÆÆ‰ª£ÁêÜÁ≥ªÁªüÂéüÂûãÔºåÂπ∂‰ΩøÁî®ÁúüÂÆû‰ºöËÆÆËÆ∞ÂΩïÂàõÂª∫‰∫ÜÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØï„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåGPT-4/4oÂú®ÁßØÊûÅÂíåË∞®ÊÖéÁöÑÂèÇ‰∏éÁ≠ñÁï•‰πãÈó¥‰øùÊåÅ‰∫ÜÂπ≥Ë°°ÔºåËÄåGemini 1.5 ProÂàôÊõ¥ÂÄæÂêë‰∫éË∞®ÊÖéÔºåGemini 1.5 FlashÂíåLlama3-8B/70BÂàôË°®Áé∞Âá∫Êõ¥ÁßØÊûÅÁöÑÂÄæÂêë„ÄÇÂ∞ΩÁÆ°Á∫¶60%ÁöÑÂõûÂ∫îÊ∂µÁõñ‰∫ÜËá≥Â∞ë‰∏Ä‰∏™ÂÖ≥ÈîÆÁÇπÔºå‰ΩÜ‰ªçÈúÄÊîπËøõ‰ª•ÂáèÂ∞ëÊó†ÂÖ≥ÊàñÈáçÂ§çÂÜÖÂÆπÔºåÂπ∂ÊèêÈ´òÂØπÁúüÂÆûÂú∫ÊôØ‰∏≠ËΩ¨ÂΩïÈîôËØØÁöÑÂÆπÂøçÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04689', 'title': 'ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning', 'url': 'https://huggingface.co/papers/2502.04689', 'abstract': 'Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance ("think step by step"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.', 'score': 2, 'issue_id': 2123, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': 'a052e3be1fe147cd', 'authors': ['Yuwei Yin', 'Giuseppe Carenini'], 'affiliations': ['University of British Columbia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04689.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': 'ARR: –ù–æ–≤—ã–π —à–∞–≥ –≤ —É–ª—É—á—à–µ–Ω–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ARR. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö —à–∞–≥–∞: –∞–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ. ARR –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º –ø–æ–¥—Ö–æ–¥–æ–º –∏ –º–µ—Ç–æ–¥–æ–º Chain-of-Thought –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å ARR –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Ç–∏–ø–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'ARR: A Structured Approach to Boost LLM Reasoning', 'desc': "This paper presents a new prompting method called ARR for large language models (LLMs) that enhances their performance on multiple-choice question-answering tasks. ARR stands for Analyze, Retrieve, and Reason, which are the three critical steps it incorporates to improve reasoning capabilities. The method outperforms traditional zero-shot Chain-of-Thought prompting by providing clearer guidance on how to approach questions. Experiments show that each component of ARR contributes positively to the overall effectiveness, especially the analysis of the question's intent."}, 'zh': {'title': 'ARRÔºöÊèêÂçáÈóÆÁ≠îÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈõ∂-shotÊèêÁ§∫ÊñπÊ≥ïARRÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öÈ°πÈÄâÊã©ÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇARRÊòéÁ°ÆÂåÖÂê´‰∏â‰∏™ÂÖ≥ÈîÆÊ≠•È™§ÔºöÂàÜÊûêÈóÆÈ¢òÊÑèÂõæ„ÄÅÊ£ÄÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØÂíåÈÄêÊ≠•Êé®ÁêÜ„ÄÇÈÄöËøáÂú®Â§öÁßçÂ§çÊùÇÈóÆÁ≠î‰ªªÂä°‰∏äÁöÑÂÆûÈ™åÔºåARR consistently outperform‰∫Ü‰º†ÁªüÁöÑÂü∫Á∫øÊñπÊ≥ïÂíåChain-of-ThoughtÔºàCoTÔºâÊèêÁ§∫„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊÑèÂõæÂàÜÊûêÂú®ARR‰∏≠Ëµ∑ÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰ΩúÁî®ÔºåËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÊØè‰∏™ÁªÑÊàêÈÉ®ÂàÜÁöÑÁßØÊûÅË¥°ÁåÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.12387', 'title': 'Continuous 3D Perception Model with Persistent State', 'url': 'https://huggingface.co/papers/2501.12387', 'abstract': 'We present a unified framework capable of solving a broad range of 3D tasks. Our approach features a stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, this evolving state can be used to generate metric-scale pointmaps (per-pixel 3D points) for each new input in an online fashion. These pointmaps reside within a common coordinate system, and can be accumulated into a coherent, dense scene reconstruction that updates as new images arrive. Our model, called CUT3R (Continuous Updating Transformer for 3D Reconstruction), captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen regions of the scene by probing at virtual, unobserved views. Our method is simple yet highly flexible, naturally accepting varying lengths of images that may be either video streams or unordered photo collections, containing both static and dynamic content. We evaluate our method on various 3D/4D tasks and demonstrate competitive or state-of-the-art performance in each. Project Page: https://cut3r.github.io/', 'score': 1, 'issue_id': 2134, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 —è–Ω–≤–∞—Ä—è', 'en': 'January 21', 'zh': '1Êúà21Êó•'}, 'hash': '2269a12b01fff7a4', 'authors': ['Qianqian Wang', 'Yifei Zhang', 'Aleksander Holynski', 'Alexei A. Efros', 'Angjoo Kanazawa'], 'affiliations': ['Google DeepMind', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2501.12387.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': 'üèóÔ∏è', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å CUT3R –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–∞–¥–∞—á 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –≠—Ç–∞ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–≤–æ–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –∫–∞–∂–¥—ã–º –Ω–æ–≤—ã–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ–º, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø–æ—Ç–æ—á–µ—á–Ω—ã–µ –∫–∞—Ä—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. CUT3R —Å–ø–æ—Å–æ–±–Ω–∞ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é –ø–ª–æ—Ç–Ω—É—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —Å—Ü–µ–Ω—ã –∏ –¥–∞–∂–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –Ω–µ–Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ –æ–±–ª–∞—Å—Ç–∏. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –∏–ª–∏ –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö 3D/4D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.'}, 'en': {'title': 'CUT3R: Real-Time 3D Reconstruction with Continuous Updates', 'desc': 'This paper introduces CUT3R, a novel framework for 3D reconstruction that utilizes a stateful recurrent model to continuously update its state with new image observations. The model generates metric-scale pointmaps in real-time, allowing for the accumulation of a dense scene reconstruction as more images are processed. CUT3R is capable of inferring unseen areas of a scene by simulating views that have not been directly observed. The approach is versatile, handling both video streams and unordered photo collections, and shows strong performance across various 3D tasks.'}, 'zh': {'title': 'CUT3RÔºöÊåÅÁª≠Êõ¥Êñ∞ÁöÑ‰∏âÁª¥ÈáçÂª∫Ê®°Âûã', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§üËß£ÂÜ≥ÂπøÊ≥õÁöÑ‰∏âÁª¥‰ªªÂä°„ÄÇËØ•ÊñπÊ≥ïÈááÁî®Áä∂ÊÄÅÈÄíÂΩíÊ®°ÂûãÔºåËÉΩÂ§üÈöèÁùÄÊØè‰∏™Êñ∞ËßÇÂØü‰∏çÊñ≠Êõ¥Êñ∞ÂÖ∂Áä∂ÊÄÅË°®Á§∫„ÄÇÈÄöËøáÂõæÂÉèÊµÅÔºåËøôÁßç‰∏çÊñ≠ÊºîÂèòÁöÑÁä∂ÊÄÅÂèØ‰ª•Âú®Á∫øÁîüÊàêÂ∫¶ÈáèÂ∞∫Â∫¶ÁöÑÁÇπÂõæÔºå‰∏∫ÊØè‰∏™Êñ∞ËæìÂÖ•ÁîüÊàêÊØèÂÉèÁ¥†ÁöÑ‰∏âÁª¥ÁÇπ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãCUT3R‰∏ç‰ªÖÂèØ‰ª•‰ªéÂõæÂÉèËßÇÂØü‰∏≠È¢ÑÊµãÂáÜÁ°ÆÁöÑÁÇπÂõæÔºåËøòÂèØ‰ª•ÈÄöËøáÊé¢ÊµãËôöÊãüÁöÑÊú™ËßÇÂØüËßÜËßíÊé®Êñ≠Âú∫ÊôØ‰∏≠Êú™ËßÅÁöÑÂå∫Âüü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03032', 'title': 'Analyze Feature Flow to Enhance Interpretation and Steering in Language Models', 'url': 'https://huggingface.co/papers/2502.03032', 'abstract': 'We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.', 'score': 49, 'issue_id': 2090, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '030f362f419e9eb4', 'authors': ['Daniil Laptev', 'Nikita Balagansky', 'Yaroslav Aksenov', 'Daniil Gavrilov'], 'affiliations': ['1T-Tech', 'Moscow Institute of Physics and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.03032.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–æ–∑—Ä–∞—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ –º–µ–∂—Å–ª–æ–π–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º, –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ—è–º–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ—Ö–Ω–∏–∫—É –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —ç–≤–æ–ª—é—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–µ –≥—Ä–∞—Ñ—ã –ø–æ—Ç–æ–∫–æ–≤ —Ä–∞–∑–≤–∏—Ç–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ –º–µ–∂—Å–ª–æ–π–Ω—ã–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä—è–º–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –ø—É—Ç–µ–º —É—Å–∏–ª–µ–Ω–∏—è –∏–ª–∏ –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.'}, 'en': {'title': 'Mapping and Manipulating Features in Language Models', 'desc': 'This paper presents a novel method for analyzing features in large language models using sparse autoencoders. It employs a data-free cosine similarity approach to track the evolution of features across different layers of the model. The resulting flow graphs provide detailed insights into how features change and interact during processing. Additionally, the method allows for targeted manipulation of model behavior by adjusting specific features, enhancing interpretability and control in text generation tasks.'}, 'zh': {'title': 'ÁâπÂæÅÊò†Â∞ÑÔºöÂºïÂØºÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁ≥ªÁªüÂú∞Êò†Â∞ÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÂêÑÂ±ÇÂèëÁé∞ÁöÑÁâπÂæÅ„ÄÇÊàë‰ª¨‰ΩøÁî®Êó†Êï∞ÊçÆÁöÑ‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÊäÄÊúØÔºåËøΩË∏™ÁâπÂÆöÁâπÂæÅÂú®ÊØè‰∏™Èò∂ÊÆµÁöÑÊåÅÁª≠„ÄÅËΩ¨ÂèòÊàñÈ¶ñÊ¨°Âá∫Áé∞„ÄÇËøôÁßçÊñπÊ≥ïÁîüÊàê‰∫ÜÁâπÂæÅÊºîÂèòÁöÑÁªÜÁ≤íÂ∫¶ÊµÅÂõæÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãËÆ°ÁÆóÁöÑÂèØËß£ÈáäÊÄßÂíåÊú∫Âà∂Ê¥ûÂØüÂäõ„ÄÇÊàë‰ª¨ËøòÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÊîæÂ§ßÊàñÊäëÂà∂ÈÄâÂÆöÁâπÂæÅÔºåÁõ¥Êé•ÂºïÂØºÊ®°ÂûãË°å‰∏∫ÔºåÂÆûÁé∞ÊñáÊú¨ÁîüÊàê‰∏≠ÁöÑ‰∏ªÈ¢òÊéßÂà∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03544', 'title': 'Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2', 'url': 'https://huggingface.co/papers/2502.03544', 'abstract': 'We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input.', 'score': 28, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '93cdc6bc9f5f22d7', 'authors': ['Yuri Chervonyi', 'Trieu H. Trinh', 'Miroslav Ol≈°√°k', 'Xiaomeng Yang', 'Hoang Nguyen', 'Marcelo Menegali', 'Junehyuk Jung', 'Vikas Verma', 'Quoc V. Le', 'Thang Luong'], 'affiliations': ['Brown University', 'Georgia Institute of Technology', 'Google DeepMind', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2502.03544.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#training', '#architecture', '#optimization', '#synthetic'], 'emoji': 'üß†', 'ru': {'title': '–ò–ò –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —á–µ–ª–æ–≤–µ–∫–∞ –≤ –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏', 'desc': 'AlphaGeometry2 - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–∏—Å—Ç–µ–º—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –æ–ª–∏–º–ø–∏–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ –≥–µ–æ–º–µ—Ç—Ä–∏–∏, –ø—Ä–µ–≤–∑–æ—à–µ–¥—à–∞—è —Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å –∑–æ–ª–æ—Ç–æ–≥–æ –º–µ–¥–∞–ª–∏—Å—Ç–∞. –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Ä–∞—Å—à–∏—Ä–∏–ª–∏ —è–∑—ã–∫ AlphaGeometry –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á, –≤–∫–ª—é—á–∞—è –¥–≤–∏–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –ª–∏–Ω–µ–π–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —É–≥–ª–∞–º–∏, –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏ –∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è–º–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Gemini –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –æ–±–º–µ–Ω–∞ –∑–Ω–∞–Ω–∏—è–º–∏, –∫–æ–º–±–∏–Ω–∏—Ä—É—é—â–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–µ—Ä–µ–≤—å–µ–≤ –ø–æ–∏—Å–∫–∞. AlphaGeometry2 –¥–æ—Å—Ç–∏–≥–ª–∞ 84% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ —Ä–µ—à–µ–Ω–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 25 –ª–µ—Ç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ª–∏–º–ø–∏–∞–¥.'}, 'en': {'title': 'AlphaGeometry2: Mastering Olympiad Geometry with Advanced AI', 'desc': "AlphaGeometry2 is an advanced version of the original AlphaGeometry, designed to solve complex Olympiad geometry problems more effectively. It enhances the language capabilities to handle intricate scenarios involving object movements and linear equations related to angles and distances. The model's performance has improved significantly, increasing its problem coverage from 66% to 88% for International Math Olympiad geometry problems. With a new search process utilizing Gemini architecture and a knowledge-sharing mechanism, AlphaGeometry2 achieves an impressive solving rate of 84%, marking a substantial leap from its predecessor."}, 'zh': {'title': 'AlphaGeometry2ÔºöÂá†‰ΩïÈóÆÈ¢òËß£ÂÜ≥ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'AlphaGeometry2ÊòØÂØπAlphaGeometryÁöÑÊòæËëóÊîπËøõÁâàÊú¨ÔºåËÉΩÂ§üË∂ÖË∂äÂπ≥ÂùáÈáëÁâåÂæó‰∏ªÂú®Ëß£ÂÜ≥Â••ÊûóÂåπÂÖãÂá†‰ΩïÈóÆÈ¢ò‰∏äÁöÑË°®Áé∞„ÄÇÂÆÉÊâ©Â±ï‰∫ÜÂéüÊúâÁöÑËØ≠Ë®ÄÔºåËÉΩÂ§üÂ§ÑÁêÜÊõ¥Â§çÊùÇÁöÑÂØπË±°ËøêÂä®ÂíåÂåÖÂê´ËßíÂ∫¶„ÄÅÊØî‰æãÂèäË∑ùÁ¶ªÁöÑÁ∫øÊÄßÊñπÁ®ãÈóÆÈ¢ò„ÄÇÈÄöËøá‰ΩøÁî®GeminiÊû∂ÊûÑÂíåÊñ∞È¢ñÁöÑÁü•ËØÜÂÖ±‰∫´Êú∫Âà∂ÔºåAlphaGeometry2ÁöÑÊêúÁ¥¢ËøáÁ®ãÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçáÔºåÂá†‰ΩïÈóÆÈ¢òÁöÑËß£ÂÜ≥ÁéáËææÂà∞‰∫Ü84%„ÄÇËØ•Á≥ªÁªüËøòÊúùÁùÄ‰ªéËá™ÁÑ∂ËØ≠Ë®ÄËæìÂÖ•Áõ¥Êé•Ëß£ÂÜ≥Âá†‰ΩïÈóÆÈ¢òÁöÑÂÖ®Ëá™Âä®ÂåñÁ≥ªÁªüËøàÂá∫‰∫ÜÈáçË¶Å‰∏ÄÊ≠•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03621', 'title': 'DynVFX: Augmenting Real Videos with Dynamic Content', 'url': 'https://huggingface.co/papers/2502.03621', 'abstract': 'We present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time. The position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene, resulting in a cohesive and realistic output video. We achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail. Specifically, we introduce a novel inference-based method that manipulates features within the attention mechanism, enabling accurate localization and seamless integration of the new content while preserving the integrity of the original scene. Our method is fully automated, requiring only a simple user instruction. We demonstrate its effectiveness on a wide range of edits applied to real-world videos, encompassing diverse objects and scenarios involving both camera and object motion.', 'score': 23, 'issue_id': 2089, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '8c22f8cda7e633d5', 'authors': ['Danah Yatim', 'Rafail Fridman', 'Omer Bar-Tal', 'Tali Dekel'], 'affiliations': ['Pika Labs, USA', 'Weizmann Institute of Science, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2502.03621.jpg', 'data': {'categories': ['#inference', '#video', '#multimodal', '#synthetic', '#diffusion'], 'emoji': 'üé¨', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –º–µ—Ç–æ–¥ –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ –Ω–æ–≤—ã–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ text-to-video –∏ Vision Language Model –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –±–µ—Å—à–æ–≤–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –ú–µ—Ç–æ–¥ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω –∏ —Ç—Ä–µ–±—É–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Seamless Video Augmentation with Dynamic Content', 'desc': 'This paper introduces a novel approach for enhancing real-world videos by adding dynamic content based on user instructions. The method utilizes a pre-trained text-to-video diffusion transformer to generate new objects and effects that interact naturally with the existing scene. It employs a zero-shot, training-free framework that ensures seamless integration of the new content while considering factors like camera motion and occlusions. The approach is fully automated, allowing users to simply provide text instructions to achieve realistic video augmentation.'}, 'zh': {'title': 'Ëá™Âä®ÂåñËßÜÈ¢ëÂ¢ûÂº∫ÔºåËΩªÊùæÁîüÊàêÂä®ÊÄÅÂÜÖÂÆπÔºÅ', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ¢ûÂº∫Áé∞ÂÆûËßÜÈ¢ëÁöÑÊñ∞ÊñπÊ≥ïÔºåÂèØ‰ª•ÁîüÊàêÂä®ÊÄÅÂÜÖÂÆπ„ÄÇÁî®Êà∑Âè™ÈúÄÊèê‰æõÁÆÄÂçïÁöÑÊñáÊú¨Êåá‰ª§ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂ∞±ËÉΩÂêàÊàê‰∏éÂéüÂßãÂú∫ÊôØËá™ÁÑ∂‰∫íÂä®ÁöÑÂä®ÊÄÅÂØπË±°ÊàñÂ§çÊùÇÂú∫ÊôØÊïàÊûú„ÄÇÊñ∞ÂÜÖÂÆπÁöÑ‰ΩçÁΩÆ„ÄÅÂ§ñËßÇÂíåËøêÂä®‰∏éÂéüÂßãËßÜÈ¢ëÊó†ÁºùÁªìÂêàÔºåÂêåÊó∂ËÄÉËôë‰∫ÜÁõ∏Êú∫ËøêÂä®„ÄÅÈÅÆÊå°ÂíåÂÖ∂‰ªñÂä®ÊÄÅÂØπË±°ÁöÑ‰∫íÂä®„ÄÇËØ•ÊñπÊ≥ïÈááÁî®Èõ∂-shot„ÄÅÊó†ËÆ≠ÁªÉÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨Âà∞ËßÜÈ¢ëÊâ©Êï£ÂèòÊç¢Âô®ÂíåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåÂÆûÁé∞‰∫ÜËá™Âä®ÂåñÁöÑÂÜÖÂÆπÂêàÊàê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04320', 'title': 'ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features', 'url': 'https://huggingface.co/papers/2502.04320', 'abstract': 'Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP.', 'score': 19, 'issue_id': 2101, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '043b9ff9f5eaf227', 'authors': ['Alec Helbling', 'Tuna Han Salih Meral', 'Ben Hoover', 'Pinar Yanardag', 'Duen Horng Chau'], 'affiliations': ['Georgia Tech', 'IBM Research', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.04320.jpg', 'data': {'categories': ['#interpretability', '#transfer_learning', '#dataset', '#multimodal', '#cv', '#benchmark'], 'emoji': 'üîç', 'ru': {'title': 'ConceptAttention: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ConceptAttention, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–µ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ (DiT) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–æ –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. ConceptAttention –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –∑–∞–¥–∞—á–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö ImageNet-Segmentation –∏ PascalVOC. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π DiT, —Ç–∞–∫–∏—Ö –∫–∞–∫ Flux, —Ö–æ—Ä–æ—à–æ –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—Å—è –Ω–∞ –∑–∞–¥–∞—á–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥–∞–∂–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤—Ä–æ–¥–µ CLIP.'}, 'en': {'title': 'Enhancing Interpretability with ConceptAttention in Multi-Modal Transformers', 'desc': 'This paper explores the unique properties of multi-modal diffusion transformers (DiTs) and their interpretability through a new method called ConceptAttention. ConceptAttention utilizes the attention layers of DiTs to create detailed saliency maps that accurately identify textual concepts in images without needing extra training. The study reveals that linear projections in the output space of DiT attention layers lead to sharper saliency maps compared to traditional cross-attention methods. Additionally, ConceptAttention demonstrates superior performance in zero-shot image segmentation tasks, surpassing other interpretability techniques and showing the strong transferability of DiT representations to vision applications.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÊâ©Êï£ÂèòÊç¢Âô®ÁöÑÂèØËß£ÈáäÊÄßÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiTsÔºâÂú®ÂèØËß£ÈáäÊÄßÊñπÈù¢ÁöÑÁã¨ÁâπÊÄßË¥®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïConceptAttentionÔºåÂà©Áî®DiTÊ≥®ÊÑèÂäõÂ±ÇÁöÑË°®ËææËÉΩÂäõÁîüÊàêÈ´òË¥®ÈáèÁöÑÊòæËëóÊÄßÂõæÔºåÂáÜÁ°ÆÂÆö‰ΩçÂõæÂÉè‰∏≠ÁöÑÊñáÊú¨Ê¶ÇÂøµ„ÄÇÈÄöËøáÂØπDiTÊ≥®ÊÑèÂäõÂ±ÇËæìÂá∫Á©∫Èó¥ËøõË°åÁ∫øÊÄßÊäïÂΩ±ÔºåConceptAttentionÁîüÊàêÁöÑÊòæËëóÊÄßÂõæÊØîÂ∏∏Áî®ÁöÑ‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Êõ¥Ê∏ÖÊô∞„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂È¶ñÊ¨°ËØÅÊòé‰∫ÜÂ§öÊ®°ÊÄÅDiTÊ®°ÂûãÁöÑË°®Á§∫Âú®ËßÜËßâ‰ªªÂä°ÔºàÂ¶ÇÂàÜÂâ≤Ôºâ‰∏≠ÂÖ∑ÊúâÈ´òÂ∫¶ÂèØËΩ¨ÁßªÊÄßÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜÂÉèCLIPËøôÊ†∑ÁöÑÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04153', 'title': 'UltraIF: Advancing Instruction Following from the Wild', 'url': 'https://huggingface.co/papers/2502.04153', 'abstract': 'Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https://github.com/kkk-an/UltraIF.', 'score': 19, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '5c7c902f1effa8a3', 'authors': ['Kaikai An', 'Li Sheng', 'Ganqu Cui', 'Shuzheng Si', 'Ning Ding', 'Yu Cheng', 'Baobao Chang'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.04153.jpg', 'data': {'categories': ['#open_source', '#training', '#benchmark', '#alignment', '#rlhf'], 'emoji': 'üß†', 'ru': {'title': 'UltraIF: –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM —Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –ø–æ–¥—Ö–æ–¥ UltraIF –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ú–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å UltraComposer –¥–ª—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ UltraIF –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º —É –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ LLaMA-3.1-8B –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü–æ–¥—Ö–æ–¥ —Ç–∞–∫–∂–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏, —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.'}, 'en': {'title': 'UltraIF: Simplifying Complex Instructions for LLMs', 'desc': 'This paper introduces UltraIF, a method designed to enhance large language models (LLMs) in following complex instructions using open-source data. The approach involves breaking down user prompts into simpler components, such as queries and constraints, and then training a model called UltraComposer to generate these structured prompts. By synthesizing complicated instructions and evaluating responses, UltraIF successfully aligns the LLaMA-3.1-8B-Base model with its instruct version on multiple benchmarks without prior benchmark data. Additionally, the method shows potential for improving existing instruct models through self-alignment, expanding its applicability in various scenarios.'}, 'zh': {'title': 'UltraIFÔºöËÆ©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊõ¥ËÅ™ÊòéÁöÑÁßòÂØÜÊ≠¶Âô®', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫UltraIFÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂØπÂ§çÊùÇÊåá‰ª§ÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÁî®Êà∑ÁöÑÁúüÂÆûËØ∑Ê±ÇÂàÜËß£‰∏∫Êõ¥ÁÆÄÂçïÁöÑÊü•ËØ¢„ÄÅÁ∫¶ÊùüÂíåÁõ∏Â∫îÁöÑËØÑ‰º∞ÈóÆÈ¢òÊù•ÂÆûÁé∞„ÄÇÊé•ÁùÄÔºåËÆ≠ÁªÉ‰∏Ä‰∏™Âêç‰∏∫UltraComposerÁöÑÊ®°ÂûãÔºåËÉΩÂ§üÁîüÊàê‰∏éÁ∫¶ÊùüÁõ∏ÂÖ≥ÁöÑÊèêÁ§∫ÔºåÂπ∂ÁªìÂêàËØÑ‰º∞ÈóÆÈ¢òÊù•ËøáÊª§ÂìçÂ∫î„ÄÇÂÆûÈ™åË°®ÊòéÔºåUltraIFÊàêÂäüÂú∞‰ΩøLLaMA-3.1-8B-BaseÂú®Â§ö‰∏™Êåá‰ª§Ë∑üÈöèÂü∫ÂáÜ‰∏ä‰∏éÂÖ∂Êåá‰ª§ÁâàÊú¨ÂØπÈΩêÔºåÂ±ïÁ§∫‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂπøÊ≥õÂ∫îÁî®ÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04313', 'title': 'Great Models Think Alike and this Undermines AI Oversight', 'url': 'https://huggingface.co/papers/2502.04313', 'abstract': 'As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as "AI Oversight". We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from "weak-to-strong generalization". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.', 'score': 18, 'issue_id': 2091, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'a44111741eb33c43', 'authors': ['Shashwat Goel', 'Joschka Struber', 'Ilze Amanda Auzina', 'Karuna K Chandra', 'Ponnurangam Kumaraguru', 'Douwe Kiela', 'Ameya Prabhu', 'Matthias Bethge', 'Jonas Geiping'], 'affiliations': ['Contextual AI', 'ELLIS Institute Tubingen', 'IIIT Hyderabad', 'Max Planck Institute for Intelligent Systems', 'Stanford University', 'Tubingen AI Center', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2502.04313.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#interpretability', '#training', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': '–°—Ö–æ–¥—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–∏—Å–∫–∏ AI-–Ω–∞–¥–∑–æ—Ä–∞', 'desc': "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ø–ú) —Å –ø–æ–º–æ—â—å—é –¥—Ä—É–≥–∏—Ö –Ø–ú, —á—Ç–æ –∞–≤—Ç–æ—Ä—ã –Ω–∞–∑—ã–≤–∞—é—Ç 'AI Oversight'. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–∞–∫–æ–≥–æ –Ω–∞–¥–∑–æ—Ä–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –æ—Ü–µ–Ω–∫–∏ –Ø–ú-—Å—É–¥–µ–π —Å–º–µ—â–µ–Ω—ã –≤ –ø–æ–ª—å–∑—É –ø–æ—Ö–æ–∂–∏—Ö –º–æ–¥–µ–ª–µ–π, –∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö –Ø–ú –≤–∞–∂–Ω—É—é —Ä–æ–ª—å –∏–≥—Ä–∞–µ—Ç –¥–æ–ø–æ–ª–Ω—è—é—â–µ–µ –∑–Ω–∞–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç —Ç—Ä–µ–≤–æ–∂–Ω—É—é —Ç–µ–Ω–¥–µ–Ω—Ü–∏—é: —Å —Ä–æ—Å—Ç–æ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –Ø–ú –∏—Ö –æ—à–∏–±–∫–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –±–æ–ª–µ–µ —Å—Ö–æ–∂–∏–º–∏, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ä–∏—Å–∫–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–±–æ–µ–≤."}, 'en': {'title': 'Navigating AI Oversight: Understanding Model Similarity and Its Risks', 'desc': 'This paper explores the challenges of evaluating and supervising advanced language models (LMs) as their capabilities grow. It introduces a probabilistic metric to measure LM similarity based on the overlap in their mistakes, which aids in understanding AI oversight. The study reveals that when using one LM to evaluate another, models that are similar tend to score each other favorably, indicating a potential bias. Additionally, it highlights the risks of correlated failures among models as they become more capable, emphasizing the need for careful reporting and correction of model similarities in AI oversight practices.'}, 'zh': {'title': 'AIÁõëÁù£ÔºöÊ®°ÂûãÁõ∏‰ººÊÄßÁöÑÈáçË¶ÅÊÄß', 'desc': 'ÈöèÁùÄËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâËÉΩÂäõÁöÑÊèêÂçáÔºå‰∫∫Á±ªÂØπÂÖ∂ËøõË°åËØÑ‰º∞ÂíåÁõëÁù£ÂèòÂæóË∂äÊù•Ë∂äÂõ∞Èöæ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ®°ÂûãÈîôËØØÈáçÂè†ÁöÑÊ¶ÇÁéáÂ∫¶ÈáèÊù•Á†îÁ©∂Ê®°ÂûãÁõ∏‰ººÊÄßÂØπAIÁõëÁù£ÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑËØ≠Ë®ÄÊ®°ÂûãÊõ¥ÂÄæÂêë‰∫éÂÅèÂ•Ω‰∏éÂÖ∂Áõ∏‰ººÁöÑÊ®°ÂûãÔºåËøô‰∏éÊúÄËøëÁöÑËá™ÊàëÂÅèÂ•ΩÁªìÊûú‰∏ÄËá¥„ÄÇÊ≠§Â§ñÔºåÂº±ÁõëÁù£ËÄÖ‰∏éÂº∫Â≠¶ÁîüÊ®°Âûã‰πãÈó¥ÁöÑ‰∫íË°•Áü•ËØÜÂú®‚ÄúÂº±Âà∞Âº∫ÁöÑÊ≥õÂåñ‚Äù‰∏≠Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04328', 'title': 'Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment', 'url': 'https://huggingface.co/papers/2502.04328', 'abstract': 'Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.', 'score': 18, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '0e25e42e93e9e560', 'authors': ['Zuyan Liu', 'Yuhao Dong', 'Jiahui Wang', 'Ziwei Liu', 'Winston Hu', 'Jiwen Lu', 'Yongming Rao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.04328.jpg', 'data': {'categories': ['#audio', '#open_source', '#video', '#training', '#multimodal'], 'emoji': 'ü¶â', 'ru': {'title': 'Ola: –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ—â–Ω–æ–π –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Ola - –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –ø–æ–Ω–∏–º–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å Ola - —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π, –Ω–∞—á–∏–Ω–∞—é—â–∞—è—Å—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞, –∑–∞—Ç–µ–º —Ä–∞—Å—à–∏—Ä—è—é—â–∞—è—Å—è –Ω–∞ —Ä–µ—á—å –∏ –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –º–µ–Ω–µ–µ –∑–∞—Ç—Ä–∞—Ç–Ω–æ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Ola –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –≤—Å–µ–º –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º.'}, 'en': {'title': 'Ola: Bridging Modalities for Superior Understanding', 'desc': "This paper introduces Ola, an omni-modal language model designed to understand and process multiple types of data, including images, videos, and audio. Ola employs a progressive modality alignment strategy, starting with image and text, and gradually incorporating speech and video data to enhance its capabilities. The model's training pipeline is efficient, allowing it to maintain a smaller dataset for cross-modal alignment, making it easier and more cost-effective to develop. Experimental results show that Ola outperforms existing open omni-modal models and competes well with specialized models, aiming to contribute to future research in omni-modal understanding."}, 'zh': {'title': 'OlaÔºöÂÖ®Ê®°ÊÄÅÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫OlaÁöÑÂÖ®Ê®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂú®ÂõæÂÉè„ÄÅËßÜÈ¢ëÂíåÈü≥È¢ëÁêÜËß£ÊñπÈù¢‰∏é‰∏ìÈó®ÁöÑÂçïÊ®°ÊÄÅÊ®°ÂûãÁ´û‰∫â„ÄÇOlaÁöÑÊ†∏ÂøÉËÆæËÆ°ÊòØÈÄêÊ≠•Ê®°ÊÄÅÂØπÈΩêÁ≠ñÁï•ÔºåÈ¶ñÂÖà‰ªéÂõæÂÉèÂíåÊñáÊú¨ÂºÄÂßãËÆ≠ÁªÉÔºåÁÑ∂ÂêéÈÄêÊ≠•ÂºïÂÖ•ËØ≠Èü≥ÂíåËßÜÈ¢ëÊï∞ÊçÆ„ÄÇËøôÊ†∑ÁöÑËÆ≠ÁªÉÊµÅÁ®ã‰ΩøÂæóË∑®Ê®°ÊÄÅÂØπÈΩêÊï∞ÊçÆÁöÑËßÑÊ®°Áõ∏ÂØπËæÉÂ∞èÔºåÈôç‰Ωé‰∫ÜÂºÄÂèëÂÖ®Ê®°ÊÄÅÊ®°ÂûãÁöÑÊàêÊú¨„ÄÇÈÄöËøáÂπøÊ≥õÁöÑÂÆûÈ™åÔºåOlaÂú®ÊâÄÊúâÊ®°ÊÄÅ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂºÄÊîæÂÖ®Ê®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÔºåÂπ∂Âú®‰∏éÂêåÁ±ª‰∏ìÈó®Ê®°ÂûãÁöÑÁ´û‰∫â‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00473', 'title': 'Weak-to-Strong Diffusion with Reflection', 'url': 'https://huggingface.co/papers/2502.00473', 'abstract': 'The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability.', 'score': 17, 'issue_id': 2095, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '82d22d57d66f1a7a', 'authors': ['Lichen Bai', 'Masashi Sugiyama', 'Zeke Xie'], 'affiliations': ['RIKEN AIP', 'The University of Tokyo', 'xLeaF Lab, The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.00473.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#optimization', '#dataset', '#diffusion', '#benchmark'], 'emoji': 'üöÄ', 'ru': {'title': 'W2SD: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Weak-to-Strong Diffusion (W2SD) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. W2SD –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É —Å–ª–∞–±—ã–º–∏ –∏ —Å–∏–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –∏–¥–µ–∞–ª—å–Ω–æ–π –∏ —Å–∏–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω—É—é –æ–ø–µ—Ä–∞—Ü–∏—é, —á–µ—Ä–µ–¥—É—é—â—É—é –¥–µ–Ω–æ–π–∑–∏–Ω–≥ –∏ –∏–Ω–≤–µ—Ä—Å–∏—é —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω–∏—Ü—ã weak-to-strong, —á—Ç–æ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∫ —Ä–µ–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ W2SD –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–º–ø—Ç—É, –¥–æ—Å—Ç–∏–≥–∞—è SOTA —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö.'}, 'en': {'title': 'Bridging the Gap: Weak-to-Strong Diffusion for Enhanced Generative Models', 'desc': "This paper introduces Weak-to-Strong Diffusion (W2SD), a new framework designed to enhance diffusion generative models by addressing the gap between generated outputs and real data. W2SD leverages the differences between weak and strong models to better approximate the ideal model's performance. By alternating between denoising and inversion processes, it guides latent variables towards areas that closely resemble the real data distribution. The framework shows significant improvements in various applications, achieving state-of-the-art results while maintaining efficiency in computational resources."}, 'zh': {'title': 'Âº±Âà∞Âº∫Êâ©Êï£ÔºöÁº©Â∞èÁîüÊàê‰∏éÁúüÂÆûÊï∞ÊçÆÁöÑÂ∑ÆË∑ù', 'desc': 'Êâ©Êï£ÁîüÊàêÊ®°ÂûãÁöÑÁõÆÊ†áÊòØÈÄöËøáÊ¢ØÂ∫¶ËØÑÂàÜÂåπÈÖçÂ∞ÜÂ≠¶‰π†Âà∞ÁöÑÂàÜÂ∏É‰∏éÁúüÂÆûÊï∞ÊçÆÂàÜÂ∏ÉÂØπÈΩê„ÄÇÁÑ∂ËÄåÔºåËÆ≠ÁªÉÊï∞ÊçÆË¥®Èáè„ÄÅÂª∫Ê®°Á≠ñÁï•ÂíåÊû∂ÊûÑËÆæËÆ°ÁöÑÂõ∫ÊúâÈôêÂà∂ÂØºËá¥ÁîüÊàêËæìÂá∫‰∏éÁúüÂÆûÊï∞ÊçÆ‰πãÈó¥Â≠òÂú®‰∏çÂèØÈÅøÂÖçÁöÑÂ∑ÆË∑ù„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂº±Âà∞Âº∫Êâ©Êï£ÔºàW2SDÔºâÊ°ÜÊû∂ÔºåÂà©Áî®Áé∞ÊúâÂº±Ê®°ÂûãÂíåÂº∫Ê®°Âûã‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊù•Ëøë‰ººÁêÜÊÉ≥Ê®°Âûã‰∏éÂº∫Ê®°Âûã‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇW2SDÈÄöËøáÂèçÂ∞ÑÊìç‰ΩúÂú®ÂéªÂô™ÂíåÂèçÊºî‰πãÈó¥‰∫§ÊõøÔºåÁêÜËÆ∫‰∏äÂºïÂØºÊΩúÂú®ÂèòÈáèÊ≤øÁùÄÈááÊ†∑ËΩ®ËøπÊúùÂêëÁúüÂÆûÊï∞ÊçÆÂàÜÂ∏ÉÁöÑÂå∫ÂüüÔºå‰ªéËÄåÊòæËëóÊèêÈ´òÁîüÊàêÁªìÊûúÁöÑË¥®ÈáèÂíå‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04235', 'title': 'MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion', 'url': 'https://huggingface.co/papers/2502.04235', 'abstract': "Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose MAssive Genre-Audience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations.", 'score': 16, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'b1c2fec586443af8', 'authors': ['Xintong Hao', 'Ke Shen', 'Chenggang Li'], 'affiliations': ['Seed-LLM, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.04235.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#training', '#synthetic'], 'emoji': 'üöÄ', 'ru': {'title': 'MAGA: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ MAGA –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ –±–æ–≥–∞—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –∫–æ—Ä–ø—É—Å MAGACorpus –æ–±—ä–µ–º–æ–º 770 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–ª–ª–∞–ø—Å –æ–±—É—á–µ–Ω–∏—è. –†–∞–±–æ—Ç–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ MAGA –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Ö –∫–∞—á–µ—Å—Ç–≤–æ.'}, 'en': {'title': 'Expanding Language Models with MAGA: A Path Beyond Data Limitations', 'desc': 'This paper addresses the challenge of limited high-quality pretraining data for large language models. It introduces the MAssive Genre-Audience (MAGA) reformulation method, which creates diverse and contextually-rich pretraining data from existing sources. The authors present a new dataset called MAGACorpus, containing 770 billion tokens, and demonstrate its effectiveness across various model sizes. Additionally, they analyze the effects of prompt engineering on training stability and highlight the shortcomings of traditional metrics for detecting training collapse.'}, 'zh': {'title': 'MAGAÔºöÁ™ÅÁ†¥Êï∞ÊçÆÈôêÂà∂ÁöÑÈ¢ÑËÆ≠ÁªÉÊñ∞ÊñπÊ≥ï', 'desc': 'Â∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêÑÁßç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨Âú®Êâ©Â±ïÊó∂Èù¢‰∏¥È´òË¥®ÈáèÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™Áì∂È¢àÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMAssive Genre-AudienceÔºàMAGAÔºâÈáçÊûÑÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÁ≥ªÁªüÂú∞‰ªéÁé∞ÊúâËØ≠ÊñôÂ∫ì‰∏≠ÂêàÊàêÂ§öÊ†∑Âåñ‰∏îÂØåÊúâ‰∏ä‰∏ãÊñáÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë¥°ÁåÆÂåÖÊã¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ß‰∏îÂèØÊâ©Â±ïÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠ÊñôÂ∫ìÊâ©Â±ïÊñπÊ≥ïÔºåÂπ∂ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´7700‰∫ø‰∏™Ê†áËÆ∞ÁöÑMAGACorpus„ÄÇÈÄöËøáÂØπ‰∏çÂêåÊï∞ÊçÆÈ¢ÑÁÆóÊâ©Â±ïÁ≠ñÁï•ÁöÑËØÑ‰º∞ÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÂú®ÂêÑÁßçÊ®°ÂûãËßÑÊ®°‰∏ãÔºà134M-13BÔºâÁöÑ‰∏ÄËá¥ÊÄßÊîπËøõÔºåÂ±ïÁ§∫‰∫Ü‰∏ã‰∏Ä‰ª£Â§ßËßÑÊ®°ÂêàÊàêÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÂøÖË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.02358', 'title': 'MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm', 'url': 'https://huggingface.co/papers/2502.02358', 'abstract': 'Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.', 'score': 14, 'issue_id': 2088, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '967ac00db9aae918', 'authors': ['Ziyan Guo', 'Zeyu Hu', 'Na Zhao', 'De Wen Soh'], 'affiliations': ['LightSpeed Studios, Singapore', 'Singapore University of Technology and Design, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.02358.jpg', 'data': {'categories': ['#training', '#multimodal', '#cv', '#benchmark'], 'emoji': 'ü§ñ', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–µ –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–∞—Ä–∞–¥–∏–≥–º—É Motion-Condition-Motion –∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MotionLab, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è –≤ —Ü–µ–ª–µ–≤–æ–µ. MotionLab –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è MotionFlow Transformer, –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–∞—â–µ–Ω–∏—è, –º–æ–¥—É–ª—è—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á –∏ —É—á–µ–±–Ω—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±–æ–±—â–µ–Ω–∏—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã–≤–æ–¥–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –¥–ª—è –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞.'}, 'en': {'title': 'Unifying Human Motion Generation and Editing with MotionLab', 'desc': 'This paper presents a new approach to human motion generation and editing called MotionLab, which aims to unify various motion-related tasks into a single framework. The proposed paradigm, Motion-Condition-Motion, allows for the integration of source motion, conditions, and target motion, facilitating better control and editing capabilities. MotionLab employs advanced techniques like the MotionFlow Transformer and Aligned Rotational Position Encoding to enhance the generation process and ensure synchronization between motions. Overall, this framework shows improved efficiency and generalization across different benchmarks, making it a versatile tool for real-world applications in computer graphics and vision.'}, 'zh': {'title': 'Áªü‰∏Ä‰∫∫Á±ªËøêÂä®ÁîüÊàê‰∏éÁºñËæëÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': '‰∫∫Á±ªËøêÂä®ÁîüÊàêÂíåÁºñËæëÊòØËÆ°ÁÆóÊú∫ÂõæÂΩ¢Â≠¶ÂíåËßÜËßâÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®ÂàÜ„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÂæÄÂæÄÈíàÂØπÁâπÂÆö‰ªªÂä°Êèê‰æõÂ≠§Á´ãÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÊïàÁéá‰Ωé‰∏ã‰∏î‰∏çÈÄÇÁî®‰∫éÂÆûÈôÖÂ∫îÁî®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËåÉÂºèÔºöËøêÂä®Êù°‰ª∂ËøêÂä®ÔºàMotion-Condition-MotionÔºâÔºåÂÆÉÈÄöËøáÊ∫êËøêÂä®„ÄÅÊù°‰ª∂ÂíåÁõÆÊ†áËøêÂä®‰∏â‰∏™Ê¶ÇÂøµÁªü‰∏ÄÂ§ÑÁêÜÂ§öÁßç‰ªªÂä°„ÄÇÂü∫‰∫éËøô‰∏ÄËåÉÂºèÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜMotionLabÊ°ÜÊû∂ÔºåËÉΩÂ§üÊúâÊïàÂú∞ËøõË°å‰∫∫Á±ªËøêÂä®ÁöÑÁîüÊàêÂíåÁºñËæëÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Â±ïÁ§∫‰∫ÜËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÂíåÊé®ÁêÜÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03860', 'title': 'BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation', 'url': 'https://huggingface.co/papers/2502.03860', 'abstract': "Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM's LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities.", 'score': 14, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'b861ba86ae27e974', 'authors': ['Bo Pang', 'Hanze Dong', 'Jiacheng Xu', 'Silvio Savarese', 'Yingbo Zhou', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.03860.jpg', 'data': {'categories': ['#training', '#benchmark', '#math', '#long_context', '#dataset', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –±–µ–∑ –ø–æ–¥—Å–∫–∞–∑–æ–∫', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LongCoT) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ BOLT –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ —ç—Ç–∞–ø–∞: –Ω–∞—á–∞–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö LongCoT —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–Ω—É—é –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Å–≤–æ–π –º–µ—Ç–æ–¥ –∫ –º–æ–¥–µ–ª—è–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–æ–≤ –∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä—è–¥–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–≤–∏–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Empowering LLMs with Efficient Long Chain-of-Thought Bootstrapping', 'desc': "This paper presents a new method called Bootstrapping Long Chain-of-Thought (BOLT) to enhance the reasoning abilities of large language models (LLMs) without relying on knowledge distillation from existing models. BOLT consists of three stages: bootstrapping LongCoT data using in-context learning, supervised fine-tuning for LongCoT, and online training for further refinement. The approach requires only a few examples to start the bootstrapping process, making it efficient and scalable across different model sizes. Experimental results show that BOLT significantly improves performance on various reasoning and problem-solving benchmarks, demonstrating its effectiveness in developing LLMs' LongCoT capabilities."}, 'zh': {'title': 'ÂºïÂØºÈïøÈìæÊÄùÁª¥ÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂÖ∑Â§áÈïøÈìæÊÄùÁª¥ÔºàLongCoTÔºâËÉΩÂäõÔºåËÄåÊó†ÈúÄ‰æùËµñ‰∫éÁ±ª‰ººo1Ê®°ÂûãÁöÑÁü•ËØÜËí∏È¶èÊàñÊòÇË¥µÁöÑ‰∫∫Á±ªÊ†áÊ≥®„ÄÇËØ•ÊñπÊ≥ïÁß∞‰∏∫BOLTÔºåÂàÜ‰∏∫‰∏â‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÈÄöËøá‰∏ä‰∏ãÊñáÂ≠¶‰π†‰ªéÊ†áÂáÜÊåá‰ª§Ê®°ÂûãÂºïÂØºLongCoTÊï∞ÊçÆÔºåÂÖ∂Ê¨°ËøõË°åLongCoTÁöÑÁõëÁù£ÂæÆË∞ÉÔºåÊúÄÂêéËøõË°åÂú®Á∫øËÆ≠ÁªÉ‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÂçáLongCoTËÉΩÂäõ„ÄÇÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨‰ªÖÊûÑÂª∫‰∫Ü10‰∏™Á§∫‰æãÔºåËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÁöÑÂèØË°åÊÄß„ÄÇÊàë‰ª¨Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜËØ•ÊñπÊ≥ïÂú®Ëß£ÂÜ≥Â§çÊùÇÈóÆÈ¢òÂíåÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04306', 'title': 'ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization', 'url': 'https://huggingface.co/papers/2502.04306', 'abstract': 'Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow', 'score': 13, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '51ace85d35c202d5', 'authors': ['Yinjie Wang', 'Ling Yang', 'Guohao Li', 'Mengdi Wang', 'Bryon Aragam'], 'affiliations': ['Princeton University', 'University of Chicago', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2502.04306.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#agents', '#rlhf', '#small_models'], 'emoji': 'üöÄ', 'ru': {'title': 'ScoreFlow: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ò–ò', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ ScoreFlow –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. ScoreFlow –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤. –ö–ª—é—á–µ–≤—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è Score-DPO - –Ω–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –º–µ—Ç–æ–¥–∞ –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å. –ù–∞ —à–µ—Å—Ç–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö ScoreFlow –ø–æ–∫–∞–∑–∞–ª —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ 8.2% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'ScoreFlow: Optimizing Multi-Agent Systems with Continuous Gradient Techniques', 'desc': 'This paper introduces ScoreFlow, a new framework designed to enhance the performance of multi-agent systems in solving complex problems. It addresses the limitations of existing optimization methods by utilizing gradient-based optimization in a continuous space, which allows for greater flexibility and scalability. ScoreFlow features Score-DPO, a novel approach that incorporates quantitative feedback into the optimization process. The results show that ScoreFlow not only improves performance by 8.2% over previous methods but also enables smaller models to achieve better results than larger models at a lower cost.'}, 'zh': {'title': 'ScoreFlowÔºöÈ´òÊïàÁöÑÂ§öÊô∫ËÉΩ‰Ωì‰ºòÂåñÊ°ÜÊû∂', 'desc': 'ÊúÄËøëÁöÑÁ†îÁ©∂Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÊù•Ëß£ÂÜ≥Â§çÊùÇÈóÆÈ¢òÔºåÂêåÊó∂Âä™ÂäõÂáèÂ∞ëÊûÑÂª∫Ëøô‰∫õÁ≥ªÁªüÊâÄÈúÄÁöÑÊâãÂä®Â∑•‰Ωú„ÄÇÁé∞ÊúâÊñπÊ≥ïÁî±‰∫éË°®Á§∫ÈôêÂà∂„ÄÅÁº∫‰πèÈÄÇÂ∫îÊÄßÂíå‰æùËµñÁ¶ªÊï£‰ºòÂåñÊäÄÊúØËÄåÂØºËá¥ÁÅµÊ¥ªÊÄß‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜScoreFlowÔºåËøôÊòØ‰∏Ä‰∏™ÁÆÄÂçï‰ΩÜÈ´òÊÄßËÉΩÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑ‰ºòÂåñÂú®ËøûÁª≠Á©∫Èó¥‰∏≠ËøõË°å‰ºòÂåñ„ÄÇScoreFlowÂú®ÂÖ≠‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÂü∫Á∫øÔºåÂπ∂‰ΩøËæÉÂ∞èÁöÑÊ®°Âûã‰ª•Êõ¥‰ΩéÁöÑÊé®ÁêÜÊàêÊú¨Ë∂ÖË∂äËæÉÂ§ßÁöÑÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04128', 'title': 'Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis', 'url': 'https://huggingface.co/papers/2502.04128', 'abstract': 'Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.', 'score': 12, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '196e7cb6a29a44ea', 'authors': ['Zhen Ye', 'Xinfa Zhu', 'Chi-Min Chan', 'Xinsheng Wang', 'Xu Tan', 'Jiahe Lei', 'Yi Peng', 'Haohe Liu', 'Yizhu Jin', 'Zheqi DAI', 'Hongzhan Lin', 'Jianyi Chen', 'Xingjian Du', 'Liumeng Xue', 'Yunlin Chen', 'Zhifei Li', 'Lei Xie', 'Qiuqiang Kong', 'Yike Guo', 'Wei Xue'], 'affiliations': ['ASLP Lab, Northwestern Polytechnical University', 'Chinese University of Hong Kong', 'Hong Kong Baptist University', 'Independent Researcher', 'Shanghai Mobvoi Information Technology Co., Ltd.', 'The Hong Kong University of Science and Technology', 'University of Rochester', 'University of Science and Technology Beijing', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2502.04128.jpg', 'data': {'categories': ['#open_source', '#dataset', '#audio', '#training', '#optimization'], 'emoji': 'üó£Ô∏è', 'ru': {'title': 'Llasa: –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ Llasa, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –æ–¥–Ω–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∫–≤–∞–Ω—Ç–æ–≤–∞—Ç–µ–ª—å –∏ –µ–¥–∏–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—É—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —É–ª—É—á—à–∞–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–µ—á–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–æ—Å–æ–¥–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —É–ª—É—á—à–∞–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–µ–º–±—Ä–∞ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –∏ –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–≤–æ–µ–π –º–æ–¥–µ–ª–∏ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏.'}, 'en': {'title': 'Simplifying Speech Synthesis with Scalable LLMs', 'desc': 'This paper discusses advancements in text-to-speech (TTS) systems that utilize large language models (LLMs) like GPT and o1. It introduces Llasa, a new framework that simplifies speech synthesis by using a single-layer vector quantizer and a Transformer architecture, aligning it with standard LLMs. The research shows that increasing training compute enhances the naturalness and complexity of the generated speech. Additionally, it highlights how scaling inference compute can improve emotional expressiveness and accuracy by using speech understanding models as verifiers during the synthesis process.'}, 'zh': {'title': 'ÁÆÄÂåñËØ≠Èü≥ÂêàÊàêÔºåÊèêÂçáËá™ÁÑ∂ÊÄß‰∏éÊÉÖÊÑüË°®Ëææ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊñáÊú¨Âü∫Á°ÄÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËØ≠Èü≥ÂêàÊàê‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØGPTÁ≥ªÂàóÂíåo1Ê®°ÂûãÁöÑÊúÄÊñ∞ËøõÂ±ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫LlasaÁöÑÁÆÄÂçïÊ°ÜÊû∂Ôºå‰ΩøÁî®ÂçïÂ±ÇÂêëÈáèÈáèÂåñÔºàVQÔºâÁºñËß£Á†ÅÂô®ÂíåÂçï‰∏ÄTransformerÊû∂ÊûÑÔºåÊó®Âú®ÁÆÄÂåñÂ§öÈò∂ÊÆµÁöÑËØ≠Èü≥ÂêàÊàêËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ¢ûÂä†ËÆ≠ÁªÉÊó∂Èó¥ÁöÑËÆ°ÁÆóËµÑÊ∫êÂèØ‰ª•ÊòæËëóÊèêÈ´òÂêàÊàêËØ≠Èü≥ÁöÑËá™ÁÑ∂ÊÄßÔºåÂπ∂ÁîüÊàêÊõ¥Â§çÊùÇÁöÑÈüµÂæãÊ®°Âºè„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂèëÁé∞ÔºåÂú®Êé®ÁêÜÊó∂Èó¥Â¢ûÂä†ËÆ°ÁÆóËµÑÊ∫êÂèØ‰ª•ÊîπÂñÑÊÉÖÊÑüË°®Áé∞„ÄÅÈü≥Ëâ≤‰∏ÄËá¥ÊÄßÂíåÂÜÖÂÆπÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04299', 'title': 'MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.04299', 'abstract': 'This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications.', 'score': 10, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '91b39568cf3793e2', 'authors': ['Jinbo Xing', 'Long Mai', 'Cusuh Ham', 'Jiahui Huang', 'Aniruddha Mahapatra', 'Chi-Wing Fu', 'Tien-Tsin Wong', 'Feng Liu'], 'affiliations': ['Adobe Research', 'Monash University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04299.jpg', 'data': {'categories': ['#multimodal', '#video', '#diffusion', '#3d', '#games'], 'emoji': 'üé¨', 'ru': {'title': 'MotionCanvas: –ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–≤–∏–∂–µ–Ω–∏—è', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ MotionCanvas, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –≤–∏–¥–µ–æ–∫–∞–¥—Ä—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ –º–æ–¥–µ–ª–∏ I2V, –ø–æ–∑–≤–æ–ª—è—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∫–∞–º–µ—Ä—ã —Å —É—á–µ—Ç–æ–º —Å—Ü–µ–Ω—ã. MotionCanvas —Å–æ–µ–¥–∏–Ω—è–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—É—é –≥—Ä–∞—Ñ–∏–∫—É –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è 3D-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–µ–º –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏—Ö 3D-–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ –æ–ø–∏—Å—ã–≤–∞—Ç—å –Ω–∞–º–µ—Ä–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å—Ü–µ–Ω—ã –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –∏—Ö –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Empowering Cinematic Creativity with MotionCanvas', 'desc': 'This paper introduces MotionCanvas, a novel method for designing cinematic video shots in image-to-video generation. It addresses two main challenges: capturing user intentions for both camera and object motions, and effectively representing this motion information for video diffusion models. By integrating user-driven controls, MotionCanvas allows for intuitive scene-aware motion design without the need for expensive 3D training data. The method enhances creative workflows in digital content creation, making it easier for users to specify and visualize their motion intentions in video synthesis.'}, 'zh': {'title': 'Áõ¥ËßÇËÆæËÆ°ÁîµÂΩ±ÈïúÂ§¥ÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàê‰ΩìÈ™å', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÔºå‰ΩøÁî®Êà∑ËÉΩÂ§üÂú®ÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÁöÑËÉåÊôØ‰∏ãËÆæËÆ°ÁîµÂΩ±ÈïúÂ§¥„ÄÇÈïúÂ§¥ËÆæËÆ°ÊòØÁîµÂΩ±Âà∂‰Ωú‰∏≠ÁöÑÂÖ≥ÈîÆÁéØËäÇÔºåÊ∂âÂèäÂà∞ÂØπÁõ∏Êú∫ËøêÂä®ÂíåÂú∫ÊôØ‰∏≠Áâ©‰ΩìËøêÂä®ÁöÑÁ≤æÂøÉËßÑÂàí„ÄÇÊàë‰ª¨‰ªãÁªç‰∫ÜMotionCanvasÔºåËøôÊòØ‰∏ÄÁßçÂ∞ÜÁî®Êà∑È©±Âä®ÁöÑÊéßÂà∂ÈõÜÊàêÂà∞ÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏≠ÁöÑÊñπÊ≥ïÔºåÂÖÅËÆ∏Áî®Êà∑‰ª•Âú∫ÊôØÊÑüÁü•ÁöÑÊñπÂºèÊéßÂà∂Áâ©‰ΩìÂíåÁõ∏Êú∫ÁöÑËøêÂä®„ÄÇÈÄöËøáÂ∞ÜÁªèÂÖ∏ËÆ°ÁÆóÊú∫ÂõæÂΩ¢Â≠¶ÁöÑËßÅËß£‰∏éÁé∞‰ª£ËßÜÈ¢ëÁîüÊàêÊäÄÊúØÁõ∏ÁªìÂêàÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂú®‰∏çÈúÄË¶ÅÊòÇË¥µÁöÑ3DËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞I2VÂêàÊàê‰∏≠ÁöÑ3DÊÑüÁü•ËøêÂä®ÊéßÂà∂ÁöÑËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04270', 'title': 'PILAF: Optimal Human Preference Sampling for Reward Modeling', 'url': 'https://huggingface.co/papers/2502.04270', 'abstract': 'As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.', 'score': 7, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '5c0dd6afed760a2b', 'authors': ['Yunzhen Feng', 'Ariel Kwiatkowski', 'Kunhao Zheng', 'Julia Kempe', 'Yaqi Duan'], 'affiliations': ['Meta FAIR', 'NYU'], 'pdf_title_img': 'assets/pdf/title_img/2502.04270.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment', '#optimization'], 'emoji': 'üéØ', 'ru': {'title': 'PILAF: —Ç–æ—á–Ω–æ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –ò–ò —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—è–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π - PILAF (Policy-Interpolated Learning for Aligned Feedback). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ç–µ—Ö–Ω–∏–∫—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (RLHF). PILAF –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è —è–≤–Ω–æ —Å–æ–≥–ª–∞—Å—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º —Å –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–µ–π –±–∞–∑–æ–≤–æ–π –Ω–∞–≥—Ä–∞–¥—ã. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∏ –æ–Ω–ª–∞–π–Ω-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö RLHF, –≥–¥–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ –∫—É—Ä–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏.'}, 'en': {'title': 'Aligning AI with Human Values through PILAF', 'desc': 'This paper introduces Policy-Interpolated Learning for Aligned Feedback (PILAF), a new method for improving Reinforcement Learning from Human Feedback (RLHF). PILAF focuses on aligning preference learning with the true human values by using a response sampling strategy that enhances reward model accuracy. The approach is theoretically sound, showing optimal performance in both optimization and statistical contexts. It is easy to implement and performs well in scenarios where feedback curation is essential, making it a valuable tool for aligning large language models with human values.'}, 'zh': {'title': 'ÊîøÁ≠ñÊèíÂÄºÂ≠¶‰π†ÔºöÂØπÈΩê‰∫∫Á±ªÂèçÈ¶àÁöÑÊñ∞Á≠ñÁï•', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂπøÊ≥õ‰ΩøÁî®Ôºå‰ΩøÂÖ∂‰∏é‰∫∫Á±ª‰ª∑ÂÄºËßÇ‰øùÊåÅ‰∏ÄËá¥ÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂº∫ÂåñÂ≠¶‰π†‰∏é‰∫∫Á±ªÂèçÈ¶àÔºàRLHFÔºâÊàê‰∏∫‰∫Ü‰∏ÄÁßçÂÖ≥ÈîÆÊäÄÊúØÔºåÂÆÉÂ∞ÜÂÅèÂ•ΩÊï∞ÊçÆËΩ¨Âåñ‰∏∫Â•ñÂä±Ê®°ÂûãÔºåÂ∞§ÂÖ∂ÊòØÂú®Êó†Ê≥ïËé∑Âèñ‰∫∫Á±ª‰ª∑ÂÄºËßÇÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂìçÂ∫îÈááÊ†∑Á≠ñÁï•ÔºåÁß∞‰∏∫ÊîøÁ≠ñÊèíÂÄºÂ≠¶‰π†ÔºàPILAFÔºâÔºåÂÆÉÊòéÁ°ÆÂ∞ÜÂÅèÂ•ΩÂ≠¶‰π†‰∏éÊúÄÂ§ßÂåñÂü∫Á°ÄÂ•ñÂä±ÂØπÈΩê„ÄÇPILAFÂú®ÁêÜËÆ∫‰∏äÊòØÊúâ‰æùÊçÆÁöÑÔºå‰ªé‰ºòÂåñÂíåÁªüËÆ°ÁöÑËßíÂ∫¶ÈÉΩÂ±ïÁ§∫‰∫ÜÂÖ∂ÊúÄ‰ºòÊÄßÔºåÂπ∂‰∏îÂú®ÂèçÈ¶àÁ≠ñÂàíËá≥ÂÖ≥ÈáçË¶ÅÁöÑËø≠‰ª£ÂíåÂú®Á∫øRLHFÁéØÂ¢É‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04295', 'title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'url': 'https://huggingface.co/papers/2502.04295', 'abstract': 'Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code will be available at https://github.com/HenryLau7/CFPO.', 'score': 7, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'f72f46ad2c1b9853', 'authors': ['Yuanye Liu', 'Jiahang Xu', 'Li Lyna Zhang', 'Qi Chen', 'Xuan Feng', 'Yang Chen', 'Zhongxin Guo', 'Yuqing Yang', 'Cheng Peng'], 'affiliations': ['Fudan University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04295.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training'], 'emoji': 'üß¨', 'ru': {'title': '–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º—ã –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–∑—ã–≤–∞–µ–º—É—é CFPO. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∫–∞–∫ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ, —Ç–∞–∫ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —É—Ç–æ—á–Ω–µ–Ω–∏—è. CFPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É—Ç–∞—Ü–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–µ—Ç–æ–¥–∞–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ.'}, 'en': {'title': 'Enhancing LLMs with Integrated Content and Format Optimization', 'desc': 'This paper presents a new method called Content-Format Integrated Prompt Optimization (CFPO) that improves the performance of Large Language Models (LLMs) by optimizing both the content and formatting of prompts. The authors argue that while prompt content has been the focus of recent research, the formatting aspect is equally important and has not been thoroughly explored. CFPO uses natural language mutations to create variations in content and a dynamic strategy to test different formatting options. The results show that this integrated approach leads to better performance in various tasks compared to methods that only optimize content.'}, 'zh': {'title': 'ÂÜÖÂÆπ‰∏éÊ†ºÂºèÁöÑÂÆåÁæéÁªìÂêàÔºåÊèêÂçáLLMÊÄßËÉΩÔºÅ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂêÑÁßç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∂ÂÆûÈôÖÊïàÊûúÂæÄÂæÄ‰æùËµñ‰∫éÊèêÁ§∫ËÆæËÆ°„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰ºòÂåñÊèêÁ§∫ÂÜÖÂÆπ‰∏äÔºå‰ΩÜÊèêÁ§∫Ê†ºÂºèÁöÑ‰ΩúÁî®Âç¥Ë¢´ÂøΩËßÜÔºåÁº∫‰πèÁ≥ªÁªüÊÄßÁöÑÁ†îÁ©∂„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ï‚Äî‚ÄîÂÜÖÂÆπÊ†ºÂºèÈõÜÊàêÊèêÁ§∫‰ºòÂåñÔºàCFPOÔºâÔºåÈÄöËøáËø≠‰ª£‰ºòÂåñËøáÁ®ãÂêåÊó∂‰ºòÂåñÊèêÁ§∫ÂÜÖÂÆπÂíåÊ†ºÂºè„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞Ë°®ÊòéÔºåCFPOÂú®Â§ö‰∏™‰ªªÂä°ÂíåÂºÄÊ∫êLLMs‰∏äÁõ∏ËæÉ‰∫é‰ªÖ‰ºòÂåñÂÜÖÂÆπÁöÑÊñπÊ≥ïÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂº∫Ë∞É‰∫ÜÂÜÖÂÆπ‰∏éÊ†ºÂºèÈõÜÊàê‰ºòÂåñÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00989', 'title': 'ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution', 'url': 'https://huggingface.co/papers/2502.00989', 'abstract': 'Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.', 'score': 6, 'issue_id': 2092, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '4d26f9419f20aadb', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['ACM, New York, NY, USA', 'IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00989.jpg', 'data': {'categories': ['#interpretability', '#hallucinations', '#agents', '#cv', '#multimodal'], 'emoji': 'üìä', 'ru': {'title': '–¢–æ—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –ø–æ –≥—Ä–∞—Ñ–∏–∫–∞–º —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏ –æ—Ç –ò–ò', 'desc': 'ChartCitor - —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≥—Ä–∞—Ñ–∏–∫–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∏ –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø—É—Ç–µ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ—á–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≥—Ä–∞—Ñ–∏–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü—É, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –≤–æ–ø—Ä–æ—Å–∞, –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã, –ø–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å –≥—Ä–∞—Ñ–∏–∫–æ–º. ChartCitor –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∏ –ø–æ–≤—ã—à–∞–µ—Ç –¥–æ–≤–µ—Ä–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –ò–ò.'}, 'en': {'title': 'ChartCitor: Enhancing Trust in AI with Accurate Chart Question-Answering', 'desc': "This paper introduces ChartCitor, a multi-agent framework designed to improve the accuracy of question-answering tasks involving charts by addressing the issue of hallucinated responses from Large Language Models (LLMs). It enhances answer attribution by providing precise bounding box citations that link responses to specific parts of chart images, overcoming challenges related to visual-semantic context and complex layouts. The framework includes processes for chart-to-table extraction, answer reformulation, and evidence retrieval, which collectively improve the reliability of the generated answers. User studies indicate that ChartCitor not only boosts the performance of LLMs in chart QA tasks but also increases user trust and productivity by offering clearer explanations of the AI's reasoning."}, 'zh': {'title': 'ChartCitorÔºöÊèêÂçáÂõæË°®ÈóÆÁ≠îÁöÑÂèØ‰ø°Â∫¶‰∏éÊïàÁéá', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂèØ‰ª•ÊâßË°åÂõæË°®ÈóÆÁ≠î‰ªªÂä°Ôºå‰ΩÜÂ∏∏Â∏∏ÁîüÊàêÊú™ÁªèÈ™åËØÅÁöÑËôöÂÅáÂõûÁ≠î„ÄÇÁé∞ÊúâÁöÑÁ≠îÊ°àÂΩíÂ±ûÊñπÊ≥ïÁî±‰∫éËßÜËßâËØ≠‰πâ‰∏ä‰∏ãÊñáÊúâÈôê„ÄÅÂ§çÊùÇÁöÑËßÜËßâÊñáÊú¨ÂØπÈΩêË¶ÅÊ±Ç‰ª•ÂèäÂú®Â§çÊùÇÂ∏ÉÂ±Ä‰∏≠ËøõË°åËæπÁïåÊ°ÜÈ¢ÑÊµãÁöÑÂõ∞ÈöæÔºåÈöæ‰ª•Â∞ÜÂõûÁ≠î‰∏éÊ∫êÂõæË°®ÂÖ≥ËÅî„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜChartCitorÔºå‰∏Ä‰∏™Â§ö‰ª£ÁêÜÊ°ÜÊû∂ÔºåÈÄöËøáËØÜÂà´ÂõæË°®ÂõæÂÉè‰∏≠ÁöÑÊîØÊåÅËØÅÊçÆÊù•Êèê‰æõÁªÜÁ≤íÂ∫¶ÁöÑËæπÁïåÊ°ÜÂºïÁî®„ÄÇChartCitorÂú®‰∏çÂêåÂõæË°®Á±ªÂûã‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÂü∫ÂáÜÔºåÂ¢ûÂº∫‰∫ÜÁî®Êà∑ÂØπÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩÁöÑ‰ø°‰ªªÔºåÂπ∂ÊèêÈ´ò‰∫Ü‰∏ì‰∏ö‰∫∫Â£´ÁöÑÂ∑•‰ΩúÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.03639', 'title': 'Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach', 'url': 'https://huggingface.co/papers/2502.03639', 'abstract': 'We present a novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune a latent diffusion model, enabling it to track 2D objects with 3D Cartesian coordinates. Building on this, we regularize the shape and motion of objects in the video to eliminate undesired artifacts, \\eg, nonphysical deformation. Consequently, we enhance the quality of generated RGB videos and alleviate common issues like object morphing, which are prevalent in current video models due to a lack of shape awareness. With our 3D augmentation and regularization, our model is capable of handling contact-rich scenarios such as task-oriented videos. These videos involve complex interactions of solids, where 3D information is essential for perceiving deformation and contact. Furthermore, our model improves the overall quality of video generation by promoting the 3D consistency of moving objects and reducing abrupt changes in shape and motion.', 'score': 6, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '7875c341693f4d1e', 'authors': ['Yunuo Chen', 'Junli Cao', 'Anil Kag', 'Vidit Goel', 'Sergei Korolev', 'Chenfanfu Jiang', 'Sergey Tulyakov', 'Jian Ren'], 'affiliations': ['Snap Inc.', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2502.03639.jpg', 'data': {'categories': ['#diffusion', '#video', '#3d'], 'emoji': 'üé•', 'ru': {'title': '3D-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–µ–π –∏ –¥–∏–Ω–∞–º–∏–∫–æ–π', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—É—é –≥–µ–æ–º–µ—Ç—Ä–∏—é –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ. –î–≤—É–º–µ—Ä–Ω—ã–µ –≤–∏–¥–µ–æ –¥–æ–ø–æ–ª–Ω—è—é—Ç—Å—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ —Ç–æ—á–µ–∫ –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ü–æ–ª—É—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö PointVid –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å 2D –æ–±—ä–µ–∫—Ç—ã —Å –ø–æ–º–æ—â—å—é 3D –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º—ã –∏ –¥–≤–∏–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö RGB –≤–∏–¥–µ–æ –∏ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã.'}, 'en': {'title': 'Enhancing Video Generation with 3D Awareness', 'desc': 'This paper introduces a new framework for generating videos that combines 3D geometry with an understanding of dynamic movements. It creates a dataset called PointVid by enhancing 2D videos with 3D point trajectories, which helps the model learn to track objects in three dimensions. The framework uses a latent diffusion model that is fine-tuned to ensure that the shapes and motions of objects are realistic, reducing issues like nonphysical deformation. By focusing on 3D consistency, the model improves the quality of generated videos, especially in scenarios where objects interact closely, such as in task-oriented videos.'}, 'zh': {'title': '‰∏âÁª¥ÊÑüÁü•ÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàêË¥®Èáè', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂ÔºåÁªìÂêà‰∫Ü‰∏âÁª¥Âá†‰ΩïÂíåÂä®ÊÄÅÊÑüÁü•„ÄÇÈÄöËøáÂú®ÂÉèÁ¥†Á©∫Èó¥‰∏≠ÂØπÈΩê‰∏âÁª¥ÁÇπËΩ®ËøπÔºåÊàë‰ª¨Â¢ûÂº∫‰∫Ü‰∫åÁª¥ËßÜÈ¢ëÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™‰∏âÁª¥ÊÑüÁü•ÁöÑËßÜÈ¢ëÊï∞ÊçÆÈõÜPointVid„ÄÇÂà©Áî®Ëøô‰∏™Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨ÂØπÊΩúÂú®Êâ©Êï£Ê®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂ËÉΩÂ§üË∑üË∏™ÂÖ∑Êúâ‰∏âÁª¥ÂùêÊ†áÁöÑ‰∫åÁª¥Áâ©‰Ωì„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÈÄöËøáÊ≠£ÂàôÂåñÁâ©‰ΩìÁöÑÂΩ¢Áä∂ÂíåËøêÂä®ÔºåÊ∂àÈô§‰∫Ü‰∏çÂøÖË¶ÅÁöÑ‰º™ÂΩ±ÔºåÊèêÈ´ò‰∫ÜÁîüÊàêRGBËßÜÈ¢ëÁöÑË¥®ÈáèÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜÂ§çÊùÇÁöÑÊé•Ëß¶Âú∫ÊôØÊó∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00988', 'title': 'PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback', 'url': 'https://huggingface.co/papers/2502.00988', 'abstract': 'Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.', 'score': 5, 'issue_id': 2094, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '9548b306478edf6d', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00988.jpg', 'data': {'categories': ['#cv', '#agents', '#science', '#dataset', '#multimodal'], 'emoji': 'üìä', 'ru': {'title': 'PlotGen: –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PlotGen - –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤. PlotGen –≤–∫–ª—é—á–∞–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Python-–∫–æ–¥–∞ –∏ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö, —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–µ—Ç–æ–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PlotGen –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –Ω–∞ 4-6% –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ MatPlotBench, –ø–æ–≤—ã—à–∞—è –¥–æ–≤–µ—Ä–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º, —Å–æ–∑–¥–∞–Ω–Ω—ã–º —Å –ø–æ–º–æ—â—å—é LLM.'}, 'en': {'title': 'Automating Scientific Visualization with PlotGen', 'desc': 'This paper introduces PlotGen, a multi-agent framework designed to automate the creation of accurate scientific visualizations. It utilizes several Large Language Model (LLM)-based agents to streamline the process, including a Query Planning Agent for breaking down user requests and a Code Generation Agent for converting pseudocode into Python code. Additionally, it employs retrieval feedback agents that enhance the quality of visualizations by refining data accuracy and visual elements through iterative self-reflection. Experimental results demonstrate that PlotGen significantly improves visualization accuracy and reduces debugging time, thereby increasing user trust and productivity for novice users.'}, 'zh': {'title': 'Ëá™Âä®ÂåñÁßëÂ≠¶ÂèØËßÜÂåñÁöÑÊú™Êù•', 'desc': 'ÁßëÂ≠¶Êï∞ÊçÆÂèØËßÜÂåñÂØπ‰∫éÂ∞ÜÂéüÂßãÊï∞ÊçÆËΩ¨Âåñ‰∏∫Êòì‰∫éÁêÜËß£ÁöÑËßÜËßâË°®Á§∫Ëá≥ÂÖ≥ÈáçË¶ÅÔºåËÉΩÂ§üÂ∏ÆÂä©ËØÜÂà´Ê®°ÂºèÂíåÈ¢ÑÊµãÁªìÊûú„ÄÇÊñ∞ÊâãÁî®Êà∑Âú®ÈÄâÊã©ÂêàÈÄÇÂ∑•ÂÖ∑ÂíåÊéåÊè°ÂèØËßÜÂåñÊäÄÊúØÊó∂Â∏∏Â∏∏Èù¢‰∏¥Âõ∞Èöæ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫PlotGenÁöÑÊñ∞ÂûãÂ§ö‰ª£ÁêÜÊ°ÜÊû∂ÔºåÊó®Âú®Ëá™Âä®ÂåñÂàõÂª∫Á≤æÁ°ÆÁöÑÁßëÂ≠¶ÂèØËßÜÂåñ„ÄÇÈÄöËøáÂ§ö‰∏™Âü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ª£ÁêÜÔºåPlotGenËÉΩÂ§üÊúâÊïàÂú∞ÂàÜËß£Áî®Êà∑ËØ∑Ê±ÇÂπ∂ÁîüÊàêÈ´òË¥®ÈáèÁöÑÂèØËßÜÂåñÂõæË°®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.19085', 'title': 'Enhancing Code Generation for Low-Resource Languages: No Silver Bullet', 'url': 'https://huggingface.co/papers/2501.19085', 'abstract': "The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights.", 'score': 4, 'issue_id': 2094, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': '8c0ab784750b1038', 'authors': ['Alessandro Giagnorio', 'Alberto Martin-Lopez', 'Gabriele Bavota'], 'affiliations': ['Software Institute USI Universit√† della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2501.19085.jpg', 'data': {'categories': ['#training', '#low_resource', '#plp', '#dataset'], 'emoji': 'üöÄ', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–Ø–ú) –¥–ª—è –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –≤–∫–ª—é—á–∞—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∑–∞–¥–∞—á–µ –ø–µ—Ä–µ–≤–æ–¥–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ø–ú –ª—É—á—à–µ –≤—Å–µ–≥–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ, –∞ –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö - –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –Ø–ú –º–æ–≥—É—Ç —É—Ö—É–¥—à–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –∏–∑-–∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Boosting Code Generation for Low-Resource Languages with LLMs', 'desc': 'This paper explores how Large Language Models (LLMs) can be improved for generating code in low-resource programming languages, which have limited training data. It examines various methods such as fine-tuning, in-context learning, and a pre-training objective that translates between high- and low-resource languages. The study finds that smaller LLMs benefit most from fine-tuning, while larger models perform better with in-context learning due to their architecture. However, fine-tuning large LLMs can lead to worse performance on low-resource languages because they require more data to adjust their parameters effectively.'}, 'zh': {'title': 'ÊèêÂçá‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰ª£Á†ÅÁîüÊàêÁöÑÊúâÊïàÁ≠ñÁï•', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂá∫Áé∞ÊòæËëóÊé®Âä®‰∫ÜËá™Âä®‰ª£Á†ÅÁîüÊàêÈ¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇËøô‰∫õÊ®°Âûã‰æùËµñ‰∫éÂ§ßÈáèÂ§öÊ†∑ÂåñÁöÑÊï∞ÊçÆÈõÜÊù•Â≠¶‰π†ÁºñÁ®ãËØ≠Ë®ÄÁöÑËØ≠Ê≥ï„ÄÅËØ≠‰πâÂíå‰ΩøÁî®Ê®°Âºè„ÄÇÁÑ∂ËÄåÔºåÂØπ‰∫é‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºàÂç≥ËÆ≠ÁªÉÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÂ∞è‰ºóÁºñÁ®ãËØ≠Ë®ÄÔºâÔºåÊï∞ÊçÆÁöÑÊúâÈôêÊÄßÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂØºËá¥‰ª£Á†ÅÁîüÊàêÊÄßËÉΩËæÉÂ∑Æ„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÁ†îÁ©∂‰∫ÜÂá†ÁßçÊèêÂçáLLMsÂú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏äË°®Áé∞ÁöÑÊúâÊïàÊñπÊ≥ïÔºåÂåÖÊã¨ÁªèÂÖ∏ÁöÑÂæÆË∞ÉÂíåÂá†Áßç‰∏ä‰∏ãÊñáÂ≠¶‰π†Âèò‰Ωì„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04296', 'title': 'Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression', 'url': 'https://huggingface.co/papers/2502.04296', 'abstract': 'We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. \\ourshort achieves better visual fidelity and controllability than the previous robotic video generation models with 15 times faster speed in the real world. After post-training, this model can be used as a video simulator from low-level action inputs for evaluating policies and generating synthetic data. See this link https://liruiw.github.io/hma for more information.', 'score': 4, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'da9d11d5ea5d9d9e', 'authors': ['Lirui Wang', 'Kevin Zhao', 'Chaoqi Liu', 'Xinlei Chen'], 'affiliations': ['MIT', 'Meta, FAIR', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.04296.jpg', 'data': {'categories': ['#games', '#robotics', '#dataset', '#video', '#synthetic'], 'emoji': 'ü§ñ', 'ru': {'title': 'HMA: –ë—ã—Å—Ç—Ä–æ–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Heterogeneous Masked Autoregression (HMA) –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏–∫–∏ –≤–∏–¥–µ–æ –¥–µ–π—Å—Ç–≤–∏–π –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. HMA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è—Ö –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–ø–ª–æ—â–µ–Ω–∏–π, –¥–æ–º–µ–Ω–æ–≤ –∏ –∑–∞–¥–∞—á. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö –∏–ª–∏ –º—è–≥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–∏–¥–µ–æ. HMA –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—è –≤ 15 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ.'}, 'en': {'title': 'Revolutionizing Robot Learning with Heterogeneous Video Modeling', 'desc': 'The paper introduces Heterogeneous Masked Autoregression (HMA), a novel approach for modeling the dynamics of action videos to enhance robot learning. HMA addresses the challenges of diverse environments and the need for real-time computational efficiency by leveraging heterogeneous pre-training from various robotic tasks and domains. By employing masked autoregression, HMA generates high-quality video predictions using quantized or soft tokens. The results show that HMA significantly improves visual fidelity and controllability while operating 15 times faster than previous models, making it a valuable tool for simulating video from low-level actions and evaluating robotic policies.'}, 'zh': {'title': 'ÂºÇÊûÑÊé©ËîΩËá™ÂõûÂΩíÔºöÊèêÂçáÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑËßÜÈ¢ëÁîüÊàê', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫ÜÂºÇÊûÑÊé©ËîΩËá™ÂõûÂΩíÔºàHMAÔºâÊ®°ÂûãÔºåÁî®‰∫éÂª∫Ê®°Âä®‰ΩúËßÜÈ¢ëÁöÑÂä®ÊÄÅÔºå‰ª•ÁîüÊàêÈ´òË¥®ÈáèÁöÑÊï∞ÊçÆÂπ∂ËØÑ‰º∞Êú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊâ©Â±ïÊÄß„ÄÇÊûÑÂª∫‰∫§‰∫íÂºèËßÜÈ¢ë‰∏ñÁïåÊ®°ÂûãÂíåÊú∫Âô®‰∫∫Á≠ñÁï•Èù¢‰∏¥ÊåëÊàòÔºåÂõ†‰∏∫ÈúÄË¶ÅÂ§ÑÁêÜÂ§öÊ†∑ÂåñÁöÑÁéØÂ¢ÉÔºåÂêåÊó∂‰øùÊåÅÂÆûÊó∂ËøêË°åÁöÑËÆ°ÁÆóÊïàÁéá„ÄÇHMAÈÄöËøáÂØπ‰∏çÂêåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ„ÄÅÈ¢ÜÂüüÂíå‰ªªÂä°ÁöÑËßÇÂØüÂíåÂä®‰ΩúÂ∫èÂàóËøõË°åÂºÇÊûÑÈ¢ÑËÆ≠ÁªÉÔºåÊù•ÊèêÈ´òÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁªèËøáÂêéÊúüËÆ≠ÁªÉÔºåËØ•Ê®°ÂûãÂèØ‰ª•‰Ωú‰∏∫ËßÜÈ¢ëÊ®°ÊãüÂô®Ôºå‰ªé‰ΩéÁ∫ßÂä®‰ΩúËæìÂÖ•‰∏≠ËØÑ‰º∞Á≠ñÁï•Âπ∂ÁîüÊàêÂêàÊàêÊï∞ÊçÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04322', 'title': 'Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions', 'url': 'https://huggingface.co/papers/2502.04322', 'abstract': 'Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.', 'score': 3, 'issue_id': 2090, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '1ad4a9febd48be28', 'authors': ['Yik Siu Chan', 'Narutatsu Ri', 'Yuxin Xiao', 'Marzyeh Ghassemi'], 'affiliations': ['Brown University', 'Columbia University', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.04322.jpg', 'data': {'categories': ['#dataset', '#ethics', '#multilingual', '#benchmark', '#security'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–ü—Ä–æ—Å—Ç–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ - —Å–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –∞—Ç–∞–∫–∞–º, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –Ω–∞ –æ–±—Ö–æ–¥ —Å–∏—Å—Ç–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É HarmScore –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ LLM –∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞—Ç–∞–∫–∏ Speak Easy, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –ª–µ–≥–∫–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∞–º–∏ –¥–ª—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö —Ü–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∞—Ç–∞–∫ –∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è HarmScore –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ Speak Easy –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º LLM.'}, 'en': {'title': 'Uncovering Hidden Vulnerabilities in Language Models', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks that can lead to harmful behavior. It explores whether these attacks are useful for average users and if safety issues arise in typical human-LLM interactions. The authors introduce HarmScore, a metric to evaluate how well LLM responses can facilitate harmful actions, and Speak Easy, a framework for executing multi-step, multilingual attacks. Their findings indicate that malicious users can exploit common interaction patterns, significantly increasing the success rate of harmful actions.'}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊºèÊ¥û', 'desc': 'Â∞ΩÁÆ°Â∑≤ÁªèËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆâÂÖ®ÂØπÈΩêÂ∑•‰ΩúÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰ªçÁÑ∂ÂÆπÊòìÂèóÂà∞Ë∂äÁã±ÊîªÂáªÔºåËøô‰ºöÂºïÂèëÊúâÂÆ≥Ë°å‰∏∫„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏§‰∏™ÂÖ≥ÈîÆÈóÆÈ¢òÔºöË∂äÁã±ÂìçÂ∫îÊòØÂê¶ÁúüÊ≠£Â∏ÆÂä©ÊôÆÈÄöÁî®Êà∑ÂÆûÊñΩÊúâÂÆ≥Ë°å‰∏∫Ôºå‰ª•ÂèäÂú®Êõ¥Â∏∏ËßÅÁöÑ‰∫∫Á±ª‰∏éLLMÁöÑÁÆÄÂçï‰∫íÂä®‰∏≠ÊòØÂê¶Â≠òÂú®ÂÆâÂÖ®ÊºèÊ¥û„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜHarmScoreÔºåËøôÊòØ‰∏ÄÁßçËØÑ‰º∞LLMÂìçÂ∫îÂ¶Ç‰ΩïÊúâÊïà‰øÉËøõÊúâÂÆ≥Ë°å‰∏∫ÁöÑÊåáÊ†áÔºåÂπ∂‰ªãÁªç‰∫ÜSpeak EasyÔºå‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂ§öÊ≠•È™§„ÄÅÂ§öËØ≠Ë®ÄÊîªÂáªÊ°ÜÊû∂„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊÅ∂ÊÑèÁî®Êà∑ÂèØ‰ª•ËΩªÊùæÂà©Áî®Â∏∏ËßÅÁöÑ‰∫íÂä®Ê®°ÂºèÊù•ÂÆûÁé∞ÊúâÂÆ≥ÊÑèÂõæ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.02492', 'title': 'VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models', 'url': 'https://huggingface.co/papers/2502.02492', 'abstract': "Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/", 'score': 23, 'issue_id': 2042, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '33581479f8c6ed9f', 'authors': ['Hila Chefer', 'Uriel Singer', 'Amit Zohar', 'Yuval Kirstain', 'Adam Polyak', 'Yaniv Taigman', 'Lior Wolf', 'Shelly Sheynin'], 'affiliations': ['GenAI, Meta', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.02492.jpg', 'data': {'categories': ['#video'], 'emoji': 'üé¨', 'ru': {'title': 'VideoJAM: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoJAM - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, —Ä–µ—à–∞—é—â–∏–π –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å —Å–æ–≤–º–µ—Å—Ç–Ω–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –∏ –¥–≤–∏–∂–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –º–µ—Ö–∞–Ω–∏–∑–º Inner-Guidance –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. VideoJAM –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω –∫ –ª—é–±–æ–π –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏ –±–µ–∑ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –∏ –¥–≤–∏–∂–µ–Ω–∏—è —É–ª—É—á—à–∞–µ—Ç –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Ç–∞–∫ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Enhancing Video Generation with Motion Coherence', 'desc': 'This paper addresses the challenges faced by generative video models in accurately capturing real-world motion and dynamics. The authors identify that traditional pixel reconstruction methods prioritize visual appearance over motion coherence, leading to less realistic video outputs. To overcome this, they propose VideoJAM, a framework that integrates a motion prior into video generation by learning a combined representation of appearance and motion. By extending the training objective and introducing a dynamic guidance mechanism during inference, VideoJAM significantly improves motion coherence and visual quality, outperforming existing models without requiring changes to training data or model architecture.'}, 'zh': {'title': 'VideoJAMÔºöÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑËøêÂä®‰∏ÄËá¥ÊÄß‰∏éËßÜËßâË¥®Èáè', 'desc': 'Â∞ΩÁÆ°ÁîüÊàêËßÜÈ¢ëÊ®°ÂûãÂú®ÊúÄËøëÂèñÂæó‰∫ÜÂ∑®Â§ßËøõÂ±ïÔºå‰ΩÜ‰ªçÁÑ∂Èöæ‰ª•ÊçïÊçâÁúüÂÆû‰∏ñÁïåÁöÑËøêÂä®ÂíåÂä®ÊÄÅ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜVideoJAMÊ°ÜÊû∂ÔºåÈÄöËøáÂºïÂÖ•ÊúâÊïàÁöÑËøêÂä®ÂÖàÈ™åÔºåÂ∏ÆÂä©ËßÜÈ¢ëÁîüÊàêÂô®Â≠¶‰π†ËÅîÂêàÁöÑÂ§ñËßÇ-ËøêÂä®Ë°®Á§∫„ÄÇËØ•Ê°ÜÊû∂Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Êâ©Â±ï‰∫ÜÁõÆÊ†áÔºåÈ¢ÑÊµãÁîüÊàêÁöÑÂÉèÁ¥†ÂèäÂÖ∂ÂØπÂ∫îÁöÑËøêÂä®ÔºåÂπ∂Âú®Êé®ÁêÜÈò∂ÊÆµÂºïÂÖ•‰∫ÜÂÜÖÈÉ®ÂºïÂØºÊú∫Âà∂Ôºå‰ª•ÂÆûÁé∞‰∏ÄËá¥ÁöÑËøêÂä®ÁîüÊàê„ÄÇVideoJAMÂú®ËøêÂä®‰∏ÄËá¥ÊÄßÊñπÈù¢ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÊèêÂçá‰∫ÜÁîüÊàêËßÜÈ¢ëÁöÑËßÜËßâË¥®ÈáèÔºåË°®ÊòéÂ§ñËßÇÂíåËøêÂä®ÂèØ‰ª•‰∫í‰∏∫Ë°•ÂÖÖÔºåÂêàÁêÜÊï¥ÂêàÂêéËÉΩÂ¢ûÂº∫ËßÜÈ¢ëÁîüÊàêÁöÑË¥®ÈáèÂíå‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01362', 'title': 'Inverse Bridge Matching Distillation', 'url': 'https://huggingface.co/papers/2502.01362', 'abstract': 'Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose a novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in a one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on a wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup.', 'score': 22, 'issue_id': 2045, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '061049a23278b0f6', 'authors': ['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], 'affiliations': ['Skolkovo Institute of Science and Technology', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01362.jpg', 'data': {'categories': ['#inference', '#optimization', '#cv', '#diffusion'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ—Å—Ç–æ–≤: –∏—Å–∫—É—Å—Å—Ç–≤–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ –º–æ—Å—Ç–∞ (DBM). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –≤—ã–≤–æ–¥ DBM –æ—Ç 4 –¥–æ 100 —Ä–∞–∑, —Å–æ—Ö—Ä–∞–Ω—è—è –∏–ª–∏ –¥–∞–∂–µ —É–ª—É—á—à–∞—è –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–æ—Å—Ç–∞ –∏ –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫–∞–∫ –∫ —É—Å–ª–æ–≤–Ω—ã–º, —Ç–∞–∫ –∏ –∫ –±–µ–∑—É—Å–ª–æ–≤–Ω—ã–º DBM. –¢–µ—Ö–Ω–∏–∫–∞ –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ JPEG.'}, 'en': {'title': 'Accelerating Diffusion Bridge Models with Innovative Distillation Techniques', 'desc': 'This paper introduces a new method to improve the speed and practicality of diffusion bridge models (DBMs), which are used for tasks like image translation. The authors present a distillation technique that leverages inverse bridge matching to create a more efficient training process. This method allows for the distillation of both conditional and unconditional DBMs, using only corrupted images, and enables one-step generation. The results show that their approach can significantly speed up inference times by up to 100 times while maintaining or even enhancing the quality of generated images compared to the original models.'}, 'zh': {'title': 'Âä†ÈÄüÊâ©Êï£Ê°•Ê®°ÂûãÔºåÊèêÂçáÁîüÊàêË¥®ÈáèÔºÅ', 'desc': 'Êâ©Êï£Ê°•Ê®°ÂûãÔºàDBMsÔºâÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊâ©Â±ïÔºåÈÄÇÁî®‰∫éÂõæÂÉèÂà∞ÂõæÂÉèÁöÑËΩ¨Êç¢„ÄÇÁÑ∂ËÄåÔºåDBMsÂú®Êé®ÁêÜÊó∂ÈÄüÂ∫¶ËæÉÊÖ¢ÔºåËøôÊòØÁé∞‰ª£Êâ©Êï£ÂíåÊµÅÊ®°ÂûãÊôÆÈÅçÈù¢‰∏¥ÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈÄÜÊ°•ÂåπÈÖçÁöÑËí∏È¶èÊäÄÊúØÔºåÂπ∂Êé®ÂØºÂá∫ÂèØË°åÁöÑÁõÆÊ†áÊù•ÂÆûÈôÖËß£ÂÜ≥ÂÆÉ„ÄÇÊàë‰ª¨ÁöÑËí∏È¶èÊñπÊ≥ïËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÊù°‰ª∂ÂíåÊó†Êù°‰ª∂ÁöÑDBMsÔºåÂπ∂‰∏îÂè™‰ΩøÁî®ÊçüÂùèÁöÑÂõæÂÉèËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄåÊòæËëóÂä†Âø´Êé®ÁêÜÈÄüÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01718', 'title': 'ACECODER: Acing Coder RL via Automated Test-Case Synthesis', 'url': 'https://huggingface.co/papers/2502.01718', 'abstract': 'Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.', 'score': 15, 'issue_id': 2041, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'b5b43fe7221df9d8', 'authors': ['Huaye Zeng', 'Dongfu Jiang', 'Haozhe Wang', 'Ping Nie', 'Xiaotong Chen', 'Wenhu Chen'], 'affiliations': ['HKUST', 'Independent Researcher', 'Netmind.AI', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2502.01718.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#rl'], 'emoji': 'ü§ñ', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –∫–æ–¥–∞: –º–æ—â—å RL –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä (–≤–æ–ø—Ä–æ—Å, —Ç–µ—Å—Ç-–∫–µ–π—Å—ã) –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–¥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –ø—Ä–∏–≤–µ–ª–æ –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —É–ª—É—á—à–µ–Ω–∏—è–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –≤–∫–ª—é—á–∞—è HumanEval –∏ MBPP. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –±–æ–ª—å—à–æ–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –æ–±–ª–∞—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞.'}, 'en': {'title': 'Unlocking the Power of Reinforcement Learning in Coder Models', 'desc': 'This paper explores the use of reinforcement learning (RL) to improve coder models, which have primarily relied on supervised fine-tuning (SFT). The authors introduce a method for generating large-scale test-case pairs from existing code, which helps create reliable reward signals for training. By employing a preference-based reward model using the Bradley-Terry loss, they achieve significant performance gains in various coding benchmarks. The results demonstrate that RL can substantially enhance the capabilities of coder models, showcasing its untapped potential in this domain.'}, 'zh': {'title': 'Âº∫ÂåñÂ≠¶‰π†ÊèêÂçá‰ª£Á†ÅÊ®°ÂûãÁöÑÊΩúÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®‰ª£Á†ÅÊ®°ÂûãËÆ≠ÁªÉ‰∏≠‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁöÑÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Áº∫‰πèÂèØÈù†Â•ñÂä±Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçËá™Âä®ÂåñÁöÑÂ§ßËßÑÊ®°ÊµãËØïÁî®‰æãÂêàÊàêÁÆ°ÈÅìÔºå‰ª•ÁîüÊàêÂ§ßÈáèÔºàÈóÆÈ¢òÔºåÊµãËØïÁî®‰æãÔºâÂØπÔºå‰ªéËÄåÂ¢ûÂº∫‰ª£Á†ÅÊ®°ÂûãÁöÑËÆ≠ÁªÉ„ÄÇÈÄöËøá‰ΩøÁî®Ëøô‰∫õÊµãËØïÁî®‰æãÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜÂü∫‰∫éÈÄöËøáÁéáÁöÑÂÅèÂ•ΩÂØπÔºåÂπ∂Âà©Áî®Bradley-TerryÊçüÂ§±ËÆ≠ÁªÉÂ•ñÂä±Ê®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÂêéÔºåÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂº∫ÂåñÂ≠¶‰π†Âú®‰ª£Á†ÅÊ®°Âûã‰∏≠ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.02584', 'title': 'QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search', 'url': 'https://huggingface.co/papers/2502.02584', 'abstract': 'Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.', 'score': 11, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'f2d3938d4ad71761', 'authors': ['Zongyu Lin', 'Yao Tang', 'Xingcheng Yao', 'Da Yin', 'Ziniu Hu', 'Yizhou Sun', 'Kai-Wei Chang'], 'affiliations': ['Shanghai Jiaotong University, Shanghai, China', 'University of California, Los Angeles, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.02584.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#inference', '#reasoning', '#agents', '#training', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': 'QLASS: –ü–æ—à–∞–≥–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ QLASS –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. QLASS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—à–∞–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É Q-–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–≤–æ–¥–∏—Ç –¥–µ—Ä–µ–≤–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞. QLASS –ø–æ–∑–≤–æ–ª—è–µ—Ç —è–∑—ã–∫–æ–≤—ã–º –∞–≥–µ–Ω—Ç–∞–º –ª—É—á—à–µ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º —Ü–µ–ª—è–º, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'Enhancing Language Agents with Stepwise Q-Guidance', 'desc': 'This paper introduces QLASS, a method designed to enhance the performance of language agents in complex tasks by providing stepwise guidance through Q-value estimation. Traditional approaches rely on outcome reward models, which can result in sub-optimal decision-making due to the lack of intermediate feedback. QLASS addresses this by creating a reasoning tree that models rewards at each step, allowing agents to learn from their interactions more effectively. The results show that QLASS can maintain strong performance with less annotated data, proving its efficiency in environments with limited supervision.'}, 'zh': {'title': 'QLASSÔºöÊèêÂçáËØ≠Ë®Ä‰ª£ÁêÜÁöÑÂÜ≥Á≠ñËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫QLASSÁöÑÊ®°ÂûãÔºåÁî®‰∫éÊîπËøõËØ≠Ë®Ä‰ª£ÁêÜÂú®Â§çÊùÇ‰∫§‰∫í‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇQLASSÈÄöËøáÈÄêÊ≠•‰º∞ËÆ°QÂÄºÊù•Ëá™Âä®ÁîüÊàê‰∏≠Èó¥‰∫§‰∫íÁöÑÊ≥®ÈáäÔºå‰ªéËÄå‰∏∫ËØ≠Ë®Ä‰ª£ÁêÜÊèê‰æõÊúâÊïàÁöÑÊåáÂØº„ÄÇ‰∏é‰º†ÁªüÁöÑÁªìÊûúÂ•ñÂä±Ê®°Âûã‰∏çÂêåÔºåQLASSÂºïÂÖ•‰∫ÜÊé®ÁêÜÊ†ëÂíåËøáÁ®ãÂ•ñÂä±Âª∫Ê®°Ôºå‰ΩøÂæóÊØè‰∏ÄÊ≠•ÈÉΩÊúâÊòéÁ°ÆÁöÑÊåáÂØº„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂç≥‰ΩøÂú®Ê†áÊ≥®Êï∞ÊçÆÂáèÂ∞ëÁöÑÊÉÖÂÜµ‰∏ãÔºåQLASS‰ªçËÉΩ‰øùÊåÅËâØÂ•ΩÁöÑÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÊúâÈôêÁõëÁù£‰∏ãÁöÑÈ´òÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.02508', 'title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search', 'url': 'https://huggingface.co/papers/2502.02508', 'abstract': "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.", 'score': 9, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '80bd687783bd609b', 'authors': ['Maohao Shen', 'Guangtao Zeng', 'Zhenting Qi', 'Zhang-Wei Hong', 'Zhenfang Chen', 'Wei Lu', 'Gregory Wornell', 'Subhro Das', 'David Cox', 'Chuang Gan'], 'affiliations': ['Harvard', 'MIT', 'MIT-IBM Watson AI Lab, IBM Research', 'Singapore University of Technology and Design', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.02508.jpg', 'data': {'categories': ['#small_models', '#open_source', '#training', '#reasoning', '#math', '#rl'], 'emoji': 'üß†', 'ru': {'title': 'Satori: LLM —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –ø–æ–∏—Å–∫–æ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –û–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ Chain-of-Action-Thought (COAT), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á. –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ —ç—Ç–∞–ø–∞: –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –º–∞—Å—à—Ç–∞–±–µ –∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–µ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ 7-–º–∏–ª–ª–∏–∞—Ä–¥–Ω–∞—è –º–æ–¥–µ–ª—å Satori, –ø–æ–∫–∞–∑–∞–≤—à–∞—è –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –æ–±–æ–±—â–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã–µ –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'Empowering LLMs with Internalized Reasoning through COAT', 'desc': 'This paper discusses the development of Satori, a large language model (LLM) that enhances reasoning capabilities through a novel approach called Chain-of-Action-Thought (COAT). The authors propose a two-stage training process that first tunes the model to internalize COAT reasoning and then employs reinforcement learning for self-improvement. By internalizing the search capabilities typically guided by an external verifier, Satori can perform complex reasoning tasks more effectively. The results show that Satori not only excels in mathematical reasoning benchmarks but also generalizes well to other tasks, indicating its robust performance.'}, 'zh': {'title': 'ÂÜÖÂåñÊêúÁ¥¢ËÉΩÂäõÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ö‰∏™È¢ÜÂüüÂ±ïÁ§∫‰∫ÜÂá∫Ëâ≤ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ¢ûÂä†ÊµãËØïÊó∂ÁöÑËÆ°ÁÆóÂèØ‰ª•ÊèêÂçáLLMsÁöÑÊé®ÁêÜËÉΩÂäõÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂú®Êé®ÁêÜÊó∂ËøõË°åÂ§ßÈáèÈááÊ†∑ÔºåÂπ∂Áî±Â§ñÈÉ®LLMÈ™åËØÅÂô®ÊåáÂØº„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÈóÆÈ¢òÔºöËÉΩÂê¶Â∞ÜÊêúÁ¥¢ËÉΩÂäõÂÜÖÂåñÔºå‰ª•Ê†πÊú¨ÊÄßÂú∞Â¢ûÂº∫Âçï‰∏™LLMÁöÑÊé®ÁêÜËÉΩÂäõÔºüÊàë‰ª¨ÊèêÂá∫‰∫ÜË°åÂä®ÊÄùÁª¥ÈìæÔºàCOATÔºâÊé®ÁêÜÊñπÊ≥ïÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉËåÉÂºèÔºå‰ª•ÂÆûÁé∞LLMÁöÑËá™ÊàëÊîπËøõÂíåËá™ÊàëÂèçÊÄù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01941', 'title': 'Can LLMs Maintain Fundamental Abilities under KV Cache Compression?', 'url': 'https://huggingface.co/papers/2502.01941', 'abstract': "This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of 17.4%-43.3%. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only 9.67%-25.53% performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves 9%-18% performance improvements on long-context generation tasks under aggressive compression ratios.", 'score': 7, 'issue_id': 2041, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '1352d78ee18eadfa', 'authors': ['Xiang Liu', 'Zhenheng Tang', 'Hong Chen', 'Peijie Dong', 'Zeyu Li', 'Xiuze Zhou', 'Bo Li', 'Xuming Hu', 'Xiaowen Chu'], 'affiliations': ['The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China', 'The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.01941.jpg', 'data': {'categories': ['#optimization', '#inference', '#long_context', '#training'], 'emoji': 'üß†', 'ru': {'title': '–°–∂–∞—Ç–∏–µ KV-–∫—ç—à–∞ –≤ LLM: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —Å–∂–∞—Ç–∏—è KV-–∫—ç—à–∞ –Ω–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–∂–∞—Ç–∏—è –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, –∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª, –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–∂–∞—Ç–∏–µ KV-–∫—ç—à–∞ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Ö—É–¥—à–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ShotKV, –∫–æ—Ç–æ—Ä—ã–π –ø–æ-—Ä–∞–∑–Ω–æ–º—É –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–∑—ã –ø—Ä–µ–¥–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Optimizing KV Cache Compression for Enhanced LLM Performance', 'desc': "This paper explores how compressing the KV cache in large language models (LLMs) affects their performance on various tasks. While compression can reduce memory usage, it may also lead to a decline in the model's ability to perform tasks like arithmetic reasoning and code generation. The study finds that different compression methods impact tasks differently, with some methods causing significant performance drops, especially in arithmetic reasoning. To address these issues, the authors introduce ShotKV, a new compression technique that improves performance on long-context tasks while preserving important semantic information."}, 'zh': {'title': 'KVÁºìÂ≠òÂéãÁº©ÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÉΩÂäõÁöÑÂΩ±ÂìçÁ†îÁ©∂', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠‰∏Ä‰∏™Êú™Ë¢´ÂÖÖÂàÜÊé¢ËÆ®ÁöÑÊåëÊàòÔºöKVÁºìÂ≠òÂéãÁº©ÊñπÊ≥ïÂØπLLMsÂü∫Êú¨ËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇËôΩÁÑ∂Áé∞ÊúâÊñπÊ≥ïÂú®Èïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÂéãÁº©ÊØîÔºå‰ΩÜÂÆÉ‰ª¨ÂØπÊ†∏ÂøÉÊ®°ÂûãËÉΩÂäõÁöÑÂΩ±Âìç‰ªçÁÑ∂Áº∫‰πèÁ†îÁ©∂„ÄÇÊàë‰ª¨ÁöÑÂÆûËØÅÁ†îÁ©∂ËØÑ‰º∞‰∫ÜÂ§öÁßçKVÁºìÂ≠òÂéãÁº©ÊñπÊ≥ïÂú®‰∏çÂêå‰ªªÂä°‰∏äÁöÑË°®Áé∞ÔºåÂåÖÊã¨‰∏ñÁïåÁü•ËØÜ„ÄÅÂ∏∏ËØÜÊé®ÁêÜ„ÄÅÁÆóÊúØÊé®ÁêÜ„ÄÅ‰ª£Á†ÅÁîüÊàê„ÄÅÂÆâÂÖ®ÊÄß‰ª•ÂèäÈïø‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÁîüÊàê„ÄÇÂàÜÊûêÁªìÊûúÊòæÁ§∫ÔºåKVÁºìÂ≠òÂéãÁº©ÊñπÊ≥ïÂú®ÁâπÂÆö‰ªªÂä°‰∏äË°®Áé∞Âá∫ÊÄßËÉΩ‰∏ãÈôçÔºåÂ∞§ÂÖ∂ÊòØÁÆóÊúØÊé®ÁêÜ‰ªªÂä°ÂØπÊøÄËøõÂéãÁº©ÁâπÂà´ÊïèÊÑüÔºåÊÄßËÉΩ‰∏ãÈôçÂπÖÂ∫¶ËææÂà∞17.4%-43.3%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.02589', 'title': 'COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.02589', 'abstract': 'This paper introduces the COCONut-PanCap dataset, created to enhance panoptic segmentation and grounded image captioning. Building upon the COCO dataset with advanced COCONut panoptic masks, this dataset aims to overcome limitations in existing image-text datasets that often lack detailed, scene-comprehensive descriptions. The COCONut-PanCap dataset incorporates fine-grained, region-level captions grounded in panoptic segmentation masks, ensuring consistency and improving the detail of generated captions. Through human-edited, densely annotated descriptions, COCONut-PanCap supports improved training of vision-language models (VLMs) for image understanding and generative models for text-to-image tasks. Experimental results demonstrate that COCONut-PanCap significantly boosts performance across understanding and generation tasks, offering complementary benefits to large-scale datasets. This dataset sets a new benchmark for evaluating models on joint panoptic segmentation and grounded captioning tasks, addressing the need for high-quality, detailed image-text annotations in multi-modal learning.', 'score': 6, 'issue_id': 2056, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '28c4625deea8ac72', 'authors': ['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2502.02589.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–î–µ—Ç–∞–ª—å–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö COCONut-PanCap –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–∞–Ω–æ—Ä–∞–º–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –ø—Ä–∏–≤—è–∑–∫–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Ä–∞—Å—à–∏—Ä—è–µ—Ç COCO, –¥–æ–±–∞–≤–ª—è—è –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø–∞–Ω–æ—Ä–∞–º–Ω—ã–µ –º–∞—Å–∫–∏ –∏ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ü–µ–Ω, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ —Ä–µ–≥–∏–æ–Ω–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. COCONut-PanCap –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π —Å—Ü–µ–Ω. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ COCONut-PanCap –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Image Understanding with COCONut-PanCap Dataset', 'desc': 'The COCONut-PanCap dataset is designed to improve panoptic segmentation and grounded image captioning by providing detailed scene descriptions. It builds on the existing COCO dataset by adding advanced panoptic masks and fine-grained, region-level captions. This dataset enhances the training of vision-language models (VLMs) by offering high-quality, human-edited annotations that ensure consistency and detail in generated captions. Experimental results show that COCONut-PanCap significantly enhances performance in both understanding and generation tasks, establishing a new standard for evaluating models in multi-modal learning.'}, 'zh': {'title': 'COCONut-PanCapÔºöÊèêÂçáÂÖ®ÊôØÂàÜÂâ≤‰∏éÂõæÂÉèÊèèËø∞ÁîüÊàêÁöÑÊñ∞Êï∞ÊçÆÈõÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜCOCONut-PanCapÊï∞ÊçÆÈõÜÔºåÊó®Âú®Â¢ûÂº∫ÂÖ®ÊôØÂàÜÂâ≤ÂíåÂü∫‰∫éÂõæÂÉèÁöÑÊèèËø∞ÁîüÊàê„ÄÇËØ•Êï∞ÊçÆÈõÜÂú®COCOÊï∞ÊçÆÈõÜÁöÑÂü∫Á°Ä‰∏äÔºåÁªìÂêà‰∫ÜÂÖàËøõÁöÑCOCONutÂÖ®ÊôØÊé©Á†ÅÔºåÂÖãÊúç‰∫ÜÁé∞ÊúâÂõæÂÉè-ÊñáÊú¨Êï∞ÊçÆÈõÜ‰∏≠Áº∫‰πèËØ¶ÁªÜÂú∫ÊôØÊèèËø∞ÁöÑÂ±ÄÈôêÊÄß„ÄÇCOCONut-PanCapÊï∞ÊçÆÈõÜÂåÖÂê´Âü∫‰∫éÂÖ®ÊôØÂàÜÂâ≤Êé©Á†ÅÁöÑÁªÜÁ≤íÂ∫¶Âå∫ÂüüÁ∫ßÊèèËø∞ÔºåÁ°Æ‰øù‰∫Ü‰∏ÄËá¥ÊÄßÂπ∂ÊèêÈ´ò‰∫ÜÁîüÊàêÊèèËø∞ÁöÑÁªÜËäÇ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCOCONut-PanCapÂú®ÁêÜËß£ÂíåÁîüÊàê‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩÔºå‰∏∫Â§öÊ®°ÊÄÅÂ≠¶‰π†‰∏≠ÁöÑÈ´òË¥®ÈáèÂõæÂÉè-ÊñáÊú¨Ê≥®ÈáäÈúÄÊ±ÇÊèê‰æõ‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00674', 'title': 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?', 'url': 'https://huggingface.co/papers/2502.00674', 'abstract': 'Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of 3.8% improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.', 'score': 5, 'issue_id': 2050, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '34ba4144afc562aa', 'authors': ['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2502.00674.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#multimodal'], 'emoji': 'ü§ñ', 'ru': {'title': '–û–¥–∏–Ω –ª—É—á—à–µ –º–Ω–æ–≥–∏—Ö: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Self-MoA, –∫–æ—Ç–æ—Ä—ã–π –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–π –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Self-MoA –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ Mixture-of-Agents (MoA), –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –Ω–∞ –º–Ω–æ–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∏–ª–∏ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö MoA. –¢–∞–∫–∂–µ –±—ã–ª–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è Self-MoA, —Å–ø–æ—Å–æ–±–Ω–∞—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤.'}, 'en': {'title': 'Self-MoA: Elevating Performance with a Single Top LLM', 'desc': 'This paper investigates the effectiveness of an ensemble method called Self-MoA, which aggregates outputs from a single top-performing Large Language Model (LLM) instead of mixing multiple LLMs. The authors find that Self-MoA significantly outperforms the traditional Mixture-of-Agents (MoA) method, achieving notable improvements on various benchmarks. Their experiments reveal that the quality of outputs is crucial, as mixing different LLMs can reduce overall performance due to lower average quality. Additionally, the paper introduces a sequential version of Self-MoA that efficiently aggregates outputs over multiple rounds, maintaining high effectiveness.'}, 'zh': {'title': 'Âçï‰∏ÄÊ®°ÂûãÈõÜÊàêÔºåË∂ÖË∂äÂ§öÊ†∑ÊÄß', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊ∑∑Âêà‰∏çÂêåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËæìÂá∫ÁöÑÊúâÊïàÊÄßÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈõÜÊàêÊñπÊ≥ïSelf-MoA„ÄÇSelf-MoA‰ªÖËÅöÂêàÂçï‰∏ÄË°®Áé∞ÊúÄ‰Ω≥ÁöÑLLMÁöÑËæìÂá∫ÔºåÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÂÖ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑMixture-of-AgentsÔºàMoAÔºâÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåSelf-MoAÂú®AlpacaEval 2.0Âü∫ÂáÜ‰∏äÊèêÈ´ò‰∫Ü6.6%ÁöÑÊÄßËÉΩÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜ‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü3.8%„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂàÜÊûê‰∫ÜËæìÂá∫Â§öÊ†∑ÊÄß‰∏éË¥®Èáè‰πãÈó¥ÁöÑÊùÉË°°ÔºåÁ°ÆËÆ§Ê∑∑Âêà‰∏çÂêåLLMsÂèØËÉΩ‰ºöÈôç‰ΩéÊ®°ÂûãÁöÑÂπ≥ÂùáË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.19066', 'title': 'Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations', 'url': 'https://huggingface.co/papers/2501.19066', 'abstract': 'Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lack scalability, and/or compromise generation quality. In this work, we propose a novel framework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable concept manipulation in diffusion models. Specifically, we first identify interpretable monosemantic concepts in the latent space of text embeddings and leverage them to precisely steer the generation away or towards a given concept (e.g., nudity) or to introduce a new concept (e.g., photographic style). Through extensive experiments, we demonstrate that our approach is very simple, requires no retraining of the base model nor LoRA adapters, does not compromise the generation quality, and is robust to adversarial prompt manipulations. Our method yields an improvement of 20.01% in unsafe concept removal, is effective in style manipulation, and is sim5x faster than current state-of-the-art.', 'score': 4, 'issue_id': 2050, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': 'a8acff84a873ecb8', 'authors': ['Dahye Kim', 'Deepti Ghadiyaram'], 'affiliations': ['Department of Computer Science, Boston U'], 'pdf_title_img': 'assets/pdf/title_img/2501.19066.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#interpretability', '#security', '#cv', '#training'], 'emoji': 'üé®', 'ru': {'title': '–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ (k-SAE). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Å—Ç–∏–ª–µ–π –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –Ω–∞ 20.01%. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –∞—Ç–∞–∫–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Efficient Concept Control in Text-to-Image Generation', 'desc': 'This paper presents a new method for improving text-to-image generative models by using k-sparse autoencoders (k-SAEs) to manipulate concepts in a more efficient and interpretable way. Instead of fine-tuning models, which can be slow and reduce quality, this approach allows for precise control over the generation of specific concepts, such as removing unsafe content or adding new styles. The authors demonstrate that their method does not require retraining the base model and is significantly faster than existing techniques, achieving a 20.01% improvement in unsafe concept removal. Overall, this framework enhances the safety and versatility of generative models while maintaining high-quality outputs.'}, 'zh': {'title': 'È´òÊïàÊìçÊéßÁîüÊàêÊ®°Âûã‰∏≠ÁöÑÊ¶ÇÂøµ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®kÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®Ôºàk-SAEsÔºâÊù•ÂÆûÁé∞Êâ©Êï£Ê®°Âûã‰∏≠ÁöÑÊ¶ÇÂøµÈ´òÊïà‰∏îÂèØËß£ÈáäÁöÑÊìçÊéß„ÄÇÊàë‰ª¨È¶ñÂÖàÂú®ÊñáÊú¨ÂµåÂÖ•ÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ËØÜÂà´ÂèØËß£ÈáäÁöÑÂçï‰πâÊ¶ÇÂøµÔºåÂπ∂Âà©Áî®Ëøô‰∫õÊ¶ÇÂøµÁ≤æÁ°ÆÂú∞ÂºïÂØºÁîüÊàêÂÜÖÂÆπÔºåÈÅøÂÖçÊàñÂºïÂÖ•ÁâπÂÆöÊ¶ÇÂøµ„ÄÇÈÄöËøáÂ§ßÈáèÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÁÆÄÂçïÊòìÁî®ÔºåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉÂü∫Á°ÄÊ®°ÂûãÊàñ‰ΩøÁî®LoRAÈÄÇÈÖçÂô®Ôºå‰∏î‰∏çÂΩ±ÂìçÁîüÊàêË¥®Èáè„ÄÇÊàë‰ª¨ÁöÑÊäÄÊúØÂú®ÂéªÈô§‰∏çÂÆâÂÖ®Ê¶ÇÂøµÊñπÈù¢ÊèêÈ´ò‰∫Ü20.01%ÔºåÂú®È£éÊ†ºÊìçÊéß‰∏ä‰πüË°®Áé∞Âá∫Ëâ≤ÔºåÈÄüÂ∫¶ÊØîÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÂø´5ÂÄç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01720', 'title': 'Generating Multi-Image Synthetic Data for Text-to-Image Customization', 'url': 'https://huggingface.co/papers/2502.01720', 'abstract': 'Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image quality. We propose a simple approach that addresses both limitations. We first leverage existing text-to-image models and 3D datasets to create a high-quality Synthetic Customization Dataset (SynCD) consisting of multiple images of the same object in different lighting, backgrounds, and poses. We then propose a new encoder architecture based on shared attention mechanisms that better incorporate fine-grained visual details from input images. Finally, we propose a new inference technique that mitigates overexposure issues during inference by normalizing the text and image guidance vectors. Through extensive experiments, we show that our model, trained on the synthetic dataset with the proposed encoder and inference algorithm, outperforms existing tuning-free methods on standard customization benchmarks.', 'score': 2, 'issue_id': 2043, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'd249f21cea90b464', 'authors': ['Nupur Kumari', 'Xi Yin', 'Jun-Yan Zhu', 'Ishan Misra', 'Samaneh Azadi'], 'affiliations': ['Carnegie Mellon University', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2502.01720.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#dataset', '#architecture', '#cv', '#inference'], 'emoji': 'üé®', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π text-to-image —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π text-to-image. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö SynCD —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —ç–Ω–∫–æ–¥–µ—Ä–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–∞—Ö —Ä–∞–∑–¥–µ–ª—è–µ–º–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –¥–ª—è –ª—É—á—à–µ–≥–æ —É—á–µ—Ç–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π. –¢–∞–∫–∂–µ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ –≤—ã–≤–æ–¥–∞, –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏–π –≤–µ–∫—Ç–æ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω—á–µ—Å–∫–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –ø–µ—Ä–µ—ç–∫—Å–ø–æ–∑–∏—Ü–∏–∏.'}, 'en': {'title': 'Enhancing Customization in Text-to-Image Models with Synthetic Datasets', 'desc': 'This paper presents a novel approach to customize text-to-image models, allowing users to generate images of custom concepts in various settings. The authors create a Synthetic Customization Dataset (SynCD) using 3D datasets, which includes multiple images of the same object under different conditions. They introduce a new encoder architecture that utilizes shared attention mechanisms to capture detailed visual information effectively. Additionally, a new inference technique is proposed to address overexposure issues, resulting in improved image quality compared to existing methods.'}, 'zh': {'title': 'È´òË¥®ÈáèÂÆöÂà∂ÂåñÔºöÁ™ÅÁ†¥ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÈôêÂà∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñáÊú¨Âà∞ÂõæÂÉèÊ®°ÂûãÁöÑÂÆöÂà∂ÂåñÊñπÊ≥ïÔºåÂÖÅËÆ∏Áî®Êà∑Âú®Êú™ËßÅËøáÁöÑÁéØÂ¢É‰∏≠ÁîüÊàêËá™ÂÆö‰πâÊ¶ÇÂøµ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÊòÇË¥µÁöÑÊµãËØïÊó∂‰ºòÂåñÊàñÂú®ÂçïÂõæÂÉèÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁºñÁ†ÅÂô®ÔºåÂØºËá¥ÂõæÂÉèË¥®ÈáèËæÉÂ∑Æ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®Áé∞ÊúâÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÊ®°ÂûãÂíå3DÊï∞ÊçÆÈõÜÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÂêàÊàêÂÆöÂà∂Êï∞ÊçÆÈõÜÔºàSynCDÔºâÔºåÂåÖÂê´Âêå‰∏ÄÂØπË±°Âú®‰∏çÂêåÂÖâÁÖß„ÄÅËÉåÊôØÂíåÂßøÂäø‰∏ãÁöÑÂ§öÂº†ÂõæÂÉè„ÄÇÈÄöËøáÊñ∞ÁöÑÁºñÁ†ÅÂô®Êû∂ÊûÑÂíåÊé®ÁêÜÊäÄÊúØÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Ê†áÂáÜÂÆöÂà∂Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊó†Ë∞É‰ºòÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.01839', 'title': 'Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification', 'url': 'https://huggingface.co/papers/2502.01839', 'abstract': "Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one -- typically by verifying each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation that uses only random sampling and direct self-verification results in sustained performance improvements that, for example, elevate the Gemini v1.5 Pro model's reasoning capabilities past that of o1-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts -- chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies.", 'score': 1, 'issue_id': 2055, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'd5c87eae437c7bec', 'authors': ['Eric Zhao', 'Pranjal Awasthi', 'Sreenivas Gollapudi'], 'affiliations': ['Google Research', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.01839.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#reasoning', '#training', '#optimization'], 'emoji': 'üîç', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞: –ø—Ä–æ—Å—Ç–æ—Ç–∞ –≤–µ–¥–µ—Ç –∫ —Å–∏–ª–µ', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–±–æ—Ä–∫–∏ –≤ –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø—Ä–æ—Å—Ç–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤—ã–±–æ—Ä–æ–∫ –∏ –ø—Ä—è–º–∞—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–≥—É—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä Gemini v1.5 Pro. –í—ã—è–≤–ª–µ–Ω–æ —è–≤–ª–µ–Ω–∏–µ –Ω–µ—è–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ–≥–¥–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø—É–ª–∞ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Å—Ç–∏–ª–µ–π –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏.'}, 'en': {'title': 'Boosting Model Performance with Smart Sampling and Verification', 'desc': 'This paper explores how sampling-based search can enhance the performance of machine learning models during testing. By generating multiple candidate responses and verifying them, the authors show that scaling up this approach leads to better reasoning capabilities in models like Gemini v1.5 Pro. They discover that larger pools of sampled responses improve the accuracy of verification, a concept they call implicit scaling. Additionally, the paper highlights two principles for enhancing self-verification: comparing responses to identify errors and using different output styles for various tasks.'}, 'zh': {'title': 'Âü∫‰∫éÈááÊ†∑ÁöÑÊêúÁ¥¢ÔºöÊèêÂçáÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂü∫‰∫éÈááÊ†∑ÁöÑÊêúÁ¥¢ÊñπÊ≥ïÔºåËøôÊòØ‰∏ÄÁßçÂú®ÊµãËØïÊó∂Âà©Áî®ËÆ°ÁÆóËµÑÊ∫êÁöÑÁÆÄÂçïÁ≠ñÁï•„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÈÄöËøáÊâ©Â±ï‰∏Ä‰∏™‰ªÖ‰ΩøÁî®ÈöèÊú∫ÈááÊ†∑ÂíåËá™ÊàëÈ™åËØÅÁöÑÁÆÄÂåñÂÆûÁé∞ÔºåÂèØ‰ª•ÊòæËëóÊèêÈ´òÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏§‰∏™ÂéüÂàôÊù•ÊîπÂñÑËá™ÊàëÈ™åËØÅËÉΩÂäõÔºöÊØîËæÉ‰∏çÂêåÂìçÂ∫îÂèØ‰ª•Â∏ÆÂä©ËØÜÂà´ÈîôËØØ‰ΩçÁΩÆÔºåËÄå‰∏çÂêåÁöÑÊ®°ÂûãËæìÂá∫È£éÊ†ºÂú®‰∏çÂêå‰∏ä‰∏ãÊñá‰∏≠Êúâ‰∏çÂêåÁöÑÊïàÊûú„ÄÇÂ∞ΩÁÆ°ÂèØ‰ª•ÂÆûÁé∞ÂáÜÁ°ÆÁöÑÈ™åËØÅÔºå‰ΩÜÂΩìÂâçÁöÑÂâçÊ≤øÊ®°ÂûãÂú®Ëá™ÊàëÈ™åËØÅËÉΩÂäõ‰∏ä‰ªçÁÑ∂Ë°®Áé∞ËæÉÂº±„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.19389', 'title': 'Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2501.19389', 'abstract': "Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines.", 'score': 0, 'issue_id': 2058, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': 'c106990f1f2dc4d8', 'authors': ['Wenzhi Fang', 'Dong-Jun Han', 'Liangqi Yuan', 'Seyyedali Hosseinalipour', 'Christopher G. Brinton'], 'affiliations': ['Department of Computer Science and Engineering, Yonsei University', 'Department of Electrical Engineering, University at Buffalo-SUNY', 'Department of Electrical and Computer Engineering, Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2501.19389.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#transfer_learning'], 'emoji': 'üì±', 'ru': {'title': 'FSLoRA: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º FSLoRA (Federated Sketching LoRA) –¥–ª—è —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. FSLoRA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º —Å–∫–µ—Ç—á–∏–Ω–≥–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º –≤—ã–±–æ—Ä–æ—á–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–æ–¥–º–∞—Ç—Ä–∏—Ü—ã –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö LoRA-–º–æ–¥—É–ª–µ–π –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ. –ú–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –ø—É—Ç–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ —Å–∫–µ—Ç—á–∏–Ω–≥–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —Å—Ç—Ä–æ–≥–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ FSLoRA –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª—è—Ö.'}, 'en': {'title': 'Adaptive Fine-Tuning for Diverse Devices with FSLoRA', 'desc': "This paper introduces federated sketching LoRA (FSLoRA), a method for fine-tuning large language models (LLMs) on devices with varying computational resources. FSLoRA uses a sketching mechanism that allows devices to update only specific parts of the global LoRA modules, making it adaptable to different device capabilities. The method addresses the challenges of data scarcity and model size by adjusting sketching ratios, which control the ranks of the updates based on device constraints. The authors provide a detailed analysis of FSLoRA's convergence properties and demonstrate its effectiveness through experiments, showing it outperforms existing methods."}, 'zh': {'title': 'ÁÅµÊ¥ªÈÄÇÂ∫îËÆæÂ§áÈôêÂà∂ÁöÑËÅîÈÇ¶ËçâÂõæLoRA', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫ËÅîÈÇ¶ËçâÂõæLoRAÔºàFSLoRAÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Âú®ËÆæÂ§á‰∏äÂæÆË∞ÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊó∂ÁöÑËÆ°ÁÆóËµÑÊ∫êÂºÇË¥®ÊÄßÈóÆÈ¢ò„ÄÇFSLoRAÂà©Áî®ËçâÂõæÊú∫Âà∂Ôºå‰ΩøËÆæÂ§áËÉΩÂ§üÈÄâÊã©ÊÄßÂú∞Êõ¥Êñ∞Áî±ÊúçÂä°Âô®Áª¥Êä§ÁöÑÂÖ®Â±ÄLoRAÊ®°ÂùóÁöÑÂ≠êÁü©Èòµ„ÄÇÈÄöËøáË∞ÉÊï¥ËçâÂõæÊØî‰æãÔºåFSLoRAËÉΩÂ§üÁÅµÊ¥ªÈÄÇÂ∫îËÆæÂ§áÁâπÂÆöÁöÑÈÄö‰ø°ÂíåËÆ°ÁÆóÈôêÂà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFSLoRAÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜÂíåLLMÊ®°Âûã‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂü∫Á∫øÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08910', 'title': 'InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU', 'url': 'https://huggingface.co/papers/2502.08910', 'abstract': 'In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations.', 'score': 124, 'issue_id': 2210, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'eed812d17aeec57e', 'authors': ['Heejun Lee', 'Geon Park', 'Jaduk Suh', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai, Seoul, Korea', 'Graduate School of AI, KAIST, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.08910.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#long_context', '#inference'], 'emoji': 'üöÄ', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –±–∞—Ä—å–µ—Ä–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ LLM', 'desc': 'InfiniteHiP - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –û–Ω–∞ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É, —É–¥–∞–ª—è—è –Ω–µ–≤–∞–∂–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–æ–±—â–∞—Ç—å –Ω–∞ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –ø—Ä–∏–º–µ–Ω—è—è –º–µ—Ç–æ–¥—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ RoPE. InfiniteHiP –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 18.95-–∫—Ä–∞—Ç–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'InfiniteHiP: Unlocking Long Contexts in Language Models Efficiently', 'desc': "This paper presents InfiniteHiP, a new framework designed to improve the efficiency of large language models (LLMs) when processing very long contexts. It addresses the issues of slow inference speeds and high memory costs by using a hierarchical token pruning algorithm to remove irrelevant tokens dynamically. Additionally, InfiniteHiP enhances the model's ability to generalize to longer sequences by adjusting the relative positional encodings based on attention patterns. The framework allows for processing up to 3 million tokens on a single GPU while achieving significant speed improvements without the need for extra training."}, 'zh': {'title': 'InfiniteHiPÔºöÈ´òÊïàÂ§ÑÁêÜË∂ÖÈïø‰∏ä‰∏ãÊñáÁöÑLLMÊ°ÜÊû∂', 'desc': 'Âú®Áé∞‰ª£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÔºåÂ§ÑÁêÜÈùûÂ∏∏ÈïøÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶Èù¢‰∏¥ÊòæËëóÊåëÊàòÔºåÂØºËá¥Êé®ÁêÜÈÄüÂ∫¶ÂèòÊÖ¢ÂíåÂÜÖÂ≠òÊàêÊú¨Â¢ûÂä†„ÄÇÁé∞ÊúâÁöÑÂ§ßÂ§öÊï∞È¢ÑËÆ≠ÁªÉLLMsÊó†Ê≥ïË∂ÖÂá∫ÂÖ∂ÂéüÂßãËÆ≠ÁªÉÂ∫èÂàóÈïøÂ∫¶ËøõË°åÊ≥õÂåñ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜInfiniteHiPÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñ‰∏îÂÆûÁî®ÁöÑLLMÊé®ÁêÜÊ°ÜÊû∂ÔºåÈÄöËøáÊ®°ÂùóÂåñÁöÑÂàÜÂ±Ç‰ª§Áâå‰øÆÂâ™ÁÆóÊ≥ïÂä®ÊÄÅÊ∂àÈô§Êó†ÂÖ≥ÁöÑ‰∏ä‰∏ãÊñá‰ª§ÁâåÔºå‰ªéËÄåÂä†ÈÄüÂ§ÑÁêÜ„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂ËÉΩÂ§üÂú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞ÂØπÈïøËææ300‰∏á‰ª§ÁâåÁöÑÂ§ÑÁêÜÔºåÂπ∂Âú®1Áôæ‰∏á‰ª§Áâå‰∏ä‰∏ãÊñá‰∏≠ÂÆûÁé∞18.95ÂÄçÁöÑÊ≥®ÊÑèÂäõËß£Á†ÅÂä†ÈÄü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08946', 'title': "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding", 'url': 'https://huggingface.co/papers/2502.08946', 'abstract': 'In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.', 'score': 104, 'issue_id': 2209, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'daecc7f38306f7b8', 'authors': ['Mo Yu', 'Lemao Liu', 'Junjie Wu', 'Tsz Ting Chung', 'Shunchi Zhang', 'Jiangnan Li', 'Dit-Yan Yeung', 'Jie Zhou'], 'affiliations': ['HKUST', 'JHU', 'WeChat AI, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2502.08946.jpg', 'data': {'categories': ['#hallucinations', '#dataset', '#training', '#reasoning', '#interpretability'], 'emoji': 'ü¶ú', 'ru': {'title': '–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–ª–∏ –∏–º–∏—Ç–∞—Ü–∏—è?', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–ø—Ä–æ—Å –æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) —Ç–æ–≥–æ, —á—Ç–æ –æ–Ω–∏ –≥–æ–≤–æ—Ä—è—Ç. –û–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∑–∞–¥–∞—á—É PhysiCo –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —Å–µ—Ç–∫–∏ –≤–º–µ—Å—Ç–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM, –≤–∫–ª—é—á–∞—è GPT-4 –∏ Gemini 2.0, –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç –ª—é–¥–µ–π –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 40% –≤ —ç—Ç–æ–π –∑–∞–¥–∞—á–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —Ñ–µ–Ω–æ–º–µ–Ω '—Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–ø—É–≥–∞—è', —Ç–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª–∏ –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —Å–µ—Ç–æ—á–Ω–æ–π –∑–∞–¥–∞—á–µ–π, –Ω–æ —Ö–æ—Ä–æ—à–æ –æ–ø–∏—Å—ã–≤–∞—é—Ç —Ç–µ –∂–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ."}, 'en': {'title': 'Unveiling the Limits of LLM Understanding', 'desc': "This paper investigates whether large language models (LLMs) truly understand the content they generate, addressing the concept of the 'Stochastic Parrot'. The authors introduce a new assessment task called PhysiCo, which uses grid-format inputs to evaluate understanding of physical concepts. Their findings reveal that top-performing LLMs, like GPT-4o and Gemini 2.0, significantly underperform compared to humans, indicating a gap in comprehension. Additionally, the study shows that LLMs struggle with the task due to inherent challenges in understanding rather than just the format of the input data."}, 'zh': {'title': 'Êé¢Á©∂Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁêÜËß£ËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Á≥ªÁªüÊÄßÂú∞Êé¢ËÆ®‰∫Ü‰∏Ä‰∏™Â∏∏ËßÅÈóÆÈ¢òÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊòØÂê¶ÁúüÊ≠£ÁêÜËß£ÂÆÉ‰ª¨ÊâÄËØ¥ÁöÑÂÜÖÂÆπ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫PhysiCoÁöÑËØÑ‰º∞‰ªªÂä°ÔºåÊó®Âú®ÈÄöËøáÁΩëÊ†ºÊ†ºÂºèÁöÑËæìÂÖ•Êù•ÂáèËΩªËÆ∞ÂøÜÈóÆÈ¢òÔºåËøô‰∫õËæìÂÖ•ÊäΩË±°Âú∞ÊèèËø∞‰∫ÜÁâ©ÁêÜÁé∞Ë±°„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑLLMsÂú®ÁêÜËß£ËÉΩÂäõ‰∏äËêΩÂêé‰∫é‰∫∫Á±ªÁ∫¶40%ÔºåÂπ∂‰∏îÂú®ÁΩëÊ†º‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÊòæÁ§∫Âá∫ÈöèÊú∫Èπ¶ÈπâÁé∞Ë±°ÁöÑÂ≠òÂú®„ÄÇÊàë‰ª¨ÁöÑ‰ªªÂä°ÊåëÊàò‰∫ÜLLMsÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÂÜÖÂú®ÁöÑÂõ∞ÈöæÔºåËÄåÈùûÁΩëÊ†ºÊ†ºÂºèÁöÑ‰∏çÁÜüÊÇâ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08690', 'title': 'Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2502.08690', 'abstract': 'Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.', 'score': 31, 'issue_id': 2210, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': 'c7734d31994dcb35', 'authors': ['Hoigi Seo', 'Wongi Jeong', 'Jae-sun Seo', 'Se Young Chun'], 'affiliations': ['Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea', 'INMC & IPAI, Seoul National University, Republic of Korea', 'School of Electrical and Computer Engineering, Cornell Tech, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.08690.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#inference', '#diffusion'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (T2I). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä—É–Ω–∏–Ω–≥–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Skip and Re-use layers (Skrr), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ Skrr –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Å–ª–æ–∏ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –±–ª–æ–∫–∞—Ö, —É—á–∏—Ç—ã–≤–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –∑–∞–¥–∞—á T2I. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Skrr –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ–±–ª–æ—á–Ω–æ–≥–æ –ø—Ä—É–Ω–∏–Ω–≥–∞ –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º –æ—Ü–µ–Ω–∫–∏.'}, 'en': {'title': 'Efficient Memory Use in Text-to-Image Models with Skrr', 'desc': 'This paper introduces a new method called Skip and Re-use layers (Skrr) to improve the efficiency of text encoders in text-to-image diffusion models. Text encoders are crucial for generating images from text but use a lot of memory compared to denoising modules. Skrr reduces memory usage by selectively skipping or reusing layers in the transformer architecture, which helps maintain performance while lowering resource demands. The results show that Skrr achieves high image quality and outperforms other pruning methods, making it a significant advancement in memory efficiency for T2I tasks.'}, 'zh': {'title': 'ÊèêÂçáÊñáÊú¨ÁºñÁ†ÅÂô®ÁöÑÂÜÖÂ≠òÊïàÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Skip and Re-use layersÔºàSkrrÔºâÁöÑÊñ∞Á≠ñÁï•ÔºåÊó®Âú®ÊèêÈ´òÊñáÊú¨Âà∞ÂõæÂÉèÊâ©Êï£Ê®°Âûã‰∏≠ÊñáÊú¨ÁºñÁ†ÅÂô®ÁöÑÂÜÖÂ≠òÊïàÁéá„ÄÇÂ∞ΩÁÆ°ÊñáÊú¨ÁºñÁ†ÅÂô®Âú®Êé®ÁêÜÊó∂Èó¥ÂíåÊµÆÁÇπËøêÁÆóÊñπÈù¢ÁöÑË¥°ÁåÆËæÉÂ∞èÔºå‰ΩÜÂÆÉ‰ª¨ÁöÑÂÜÖÂ≠òÈúÄÊ±ÇÂç¥È´òËææÂéªÂô™Ê®°ÂùóÁöÑÂÖ´ÂÄç„ÄÇSkrrÈÄöËøáÈÄâÊã©ÊÄßË∑≥ËøáÊàñÈáçÁî®Êüê‰∫õÂèòÊç¢Âô®Â±ÇÔºåÂà©Áî®‰∫ÜÂèòÊç¢Âô®Âùó‰∏≠ÁöÑÂÜó‰ΩôÊÄßÔºå‰ªéËÄåÂú®‰∏çÂΩ±ÂìçÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÂáèÂ∞ëÂÜÖÂ≠òÊ∂àËÄó„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSkrrÂú®È´òÁ®ÄÁñèÂ∫¶‰∏ã‰ªçËÉΩ‰øùÊåÅ‰∏éÂéüÂßãÊ®°ÂûãÁõ∏ÂΩìÁöÑÂõæÂÉèË¥®ÈáèÔºåÂπ∂Âú®Â§ö‰∏™ËØÑ‰º∞ÊåáÊ†á‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂÜÖÂ≠òÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09619', 'title': 'Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights', 'url': 'https://huggingface.co/papers/2502.09619', 'abstract': 'With the increasing numbers of publicly available models, there are probably pretrained, online models for most tasks users require. However, current model search methods are rudimentary, essentially a text-based search in the documentation, thus users cannot find the relevant models. This paper presents ProbeLog, a method for retrieving classification models that can recognize a target concept, such as "Dog", without access to model metadata or training data. Differently from previous probing methods, ProbeLog computes a descriptor for each output dimension (logit) of each model, by observing its responses on a fixed set of inputs (probes). Our method supports both logit-based retrieval ("find more logits like this") and zero-shot, text-based retrieval ("find all logits corresponding to dogs"). As probing-based representations require multiple costly feedforward passes through the model, we develop a method, based on collaborative filtering, that reduces the cost of encoding repositories by 3x. We demonstrate that ProbeLog achieves high retrieval accuracy, both in real-world and fine-grained search tasks and is scalable to full-size repositories.', 'score': 27, 'issue_id': 2214, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '2222d8b83a19a957', 'authors': ['Jonathan Kahana', 'Or Nathan', 'Eliahu Horwitz', 'Yedid Hoshen'], 'affiliations': ['School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2502.09619.jpg', 'data': {'categories': ['#rag', '#dataset', '#optimization'], 'emoji': 'üîç', 'ru': {'title': 'ProbeLog: –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –Ω—É–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è', 'desc': 'ProbeLog - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –∏–ª–∏ –æ–±—É—á–∞—é—â–∏–º –¥–∞–Ω–Ω—ã–º. –û–Ω –≤—ã—á–∏—Å–ª—è–µ—Ç –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –Ω–∞–±–ª—é–¥–∞—è –µ–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. ProbeLog –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ –ø–æ–∏—Å–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–∏—Ç–æ–≤, —Ç–∞–∫ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤–º–µ—Å—Ç–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Efficient Model Retrieval with ProbeLog', 'desc': "This paper introduces ProbeLog, a novel method for retrieving classification models that can identify specific concepts without needing detailed model information. It generates a descriptor for each model's output dimension by analyzing its responses to a set of predefined inputs, known as probes. ProbeLog allows users to perform both logit-based and zero-shot retrieval, making it easier to find models relevant to their needs. Additionally, it employs collaborative filtering to significantly reduce the computational cost of processing large model repositories, achieving high accuracy in model retrieval tasks."}, 'zh': {'title': 'È´òÊïàÊ®°ÂûãÊ£ÄÁ¥¢ÔºåËΩªÊùæÊâæÂà∞ÊâÄÈúÄÔºÅ', 'desc': 'ÈöèÁùÄÂÖ¨ÂºÄÊ®°ÂûãÊï∞ÈáèÁöÑÂ¢ûÂä†ÔºåÁî®Êà∑ÊâÄÈúÄÁöÑ‰ªªÂä°Âá†‰πéÈÉΩÊúâÈ¢ÑËÆ≠ÁªÉÁöÑÂú®Á∫øÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÊ®°ÂûãÊêúÁ¥¢ÊñπÊ≥ïÁõ∏ÂØπÁÆÄÂçïÔºå‰∏ªË¶Å‰æùËµñÊñáÊ°£‰∏≠ÁöÑÊñáÊú¨ÊêúÁ¥¢ÔºåÂØºËá¥Áî®Êà∑Êó†Ê≥ïÊâæÂà∞Áõ∏ÂÖ≥Ê®°Âûã„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜProbeLogÔºå‰∏ÄÁßçÊ£ÄÁ¥¢ÂàÜÁ±ªÊ®°ÂûãÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•ËØÜÂà´ÁõÆÊ†áÊ¶ÇÂøµÔºåÂ¶Ç‚ÄúÁãó‚ÄùÔºåËÄåÊó†ÈúÄËÆøÈóÆÊ®°ÂûãÂÖÉÊï∞ÊçÆÊàñËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ‰∏é‰πãÂâçÁöÑÊé¢ÊµãÊñπÊ≥ï‰∏çÂêåÔºåProbeLogÈÄöËøáËßÇÂØüÊ®°ÂûãÂú®Âõ∫ÂÆöËæìÂÖ•ÈõÜ‰∏äÁöÑÂìçÂ∫îÔºå‰∏∫ÊØè‰∏™Ê®°ÂûãÁöÑÊØè‰∏™ËæìÂá∫Áª¥Â∫¶ÔºàlogitÔºâËÆ°ÁÆóÊèèËø∞Á¨¶Ôºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑÊ®°ÂûãÊ£ÄÁ¥¢„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09056', 'title': 'An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging', 'url': 'https://huggingface.co/papers/2502.09056', 'abstract': 'This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks.', 'score': 26, 'issue_id': 2209, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '36c3c29072ae279d', 'authors': ['Kunat Pipatanakul', 'Pittawat Taveekitworachai', 'Potsawee Manakul', 'Kasima Tharnpipitchai'], 'affiliations': ['SCB 10X R&D SCBX Group Bangkok, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2502.09056.jpg', 'data': {'categories': ['#data', '#dataset', '#low_resource', '#multilingual', '#training', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ –ª–æ–≥–∏–∫–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–î–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –º–µ—Ç–æ–¥–∞–º —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —É —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–∞–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–¥—Ö–æ–¥—ã –∫ –æ—Ç–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ª–∏—è–Ω–∏—é –º–æ–¥–µ–ª–µ–π, —á—Ç–æ–±—ã –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏–∑ –º–æ–¥–µ–ª–∏ DeepSeek R1 –≤ –ª–æ–∫–∞–ª—å–Ω—ã–µ LLM. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã —É—Å–∏–ª–∏—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –±—é–¥–∂–µ—Ç–æ–º –∏ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –º–æ–∂–Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM –¥–æ —É—Ä–æ–≤–Ω—è DeepSeek R1.'}, 'en': {'title': 'Empowering Thai LLMs with Enhanced Reasoning Capabilities', 'desc': "This paper explores methods for selecting data and merging models to improve reasoning abilities in language-specific large language models (LLMs), particularly for the Thai language. The authors aim to enhance these models' reasoning skills while ensuring they remain effective in their target languages. They highlight the challenges faced by low-resource languages, which often lack the extensive training data available for high-resource languages like English. The study demonstrates that it is feasible to boost the reasoning capabilities of these LLMs using only publicly available datasets and a modest budget, achieving results comparable to advanced models like DeepSeek R1."}, 'zh': {'title': 'ÊèêÂçá‰ΩéËµÑÊ∫êËØ≠Ë®ÄLLMÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÊï∞ÊçÆÈÄâÊã©ÂíåÊ®°ÂûãÂêàÂπ∂ÁöÑÊñπÊ≥ïÔºåÊó®Âú®Â∞ÜÂÖàËøõÁöÑÊé®ÁêÜËÉΩÂäõÔºàÂ¶ÇDeepSeek R1ÔºâËûçÂÖ•ÁâπÂÆöËØ≠Ë®ÄÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÔºåÁâπÂà´ÂÖ≥Ê≥®Ê≥∞ËØ≠LLM„ÄÇÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÂ¢ûÂº∫ÁâπÂÆöËØ≠Ë®ÄLLMsÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅÂÖ∂ÁõÆÊ†áËØ≠Ë®ÄÁöÑËÉΩÂäõ„ÄÇDeepSeek R1Âú®Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜ‰∏ªË¶ÅÂèóÁõä‰∫éÈ´òËµÑÊ∫êËØ≠Ë®ÄÔºåÂ¶ÇËã±ËØ≠Âíå‰∏≠ÊñáÔºåËÄå‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂàôÂèóÂà∞ÂøΩËßÜ„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫Ü‰ªÖ‰ΩøÁî®ÂÖ¨ÂºÄÊï∞ÊçÆÈõÜÂíå120ÁæéÂÖÉÁöÑËÆ°ÁÆóÈ¢ÑÁÆóÔºåÂ∞±ÂèØ‰ª•ÊèêÂçáÁâπÂÆöËØ≠Ë®ÄLLMsÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ΩøÂÖ∂ËææÂà∞DeepSeek R1ÁöÑÊ∞¥Âπ≥ÔºåËÄå‰∏çÂΩ±ÂìçÂÖ∂Âú®ÁõÆÊ†áËØ≠Ë®Ä‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09604', 'title': 'SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models', 'url': 'https://huggingface.co/papers/2502.09604', 'abstract': 'We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.', 'score': 26, 'issue_id': 2209, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '7aa5ce3731848736', 'authors': ['Yung-Sung Chuang', 'Benjamin Cohen-Wang', 'Shannon Zejiang Shen', 'Zhaofeng Wu', 'Hu Xu', 'Xi Victoria Lin', 'James Glass', 'Shang-Wen Li', 'Wen-tau Yih'], 'affiliations': ['Massachusetts Institute of Technology', 'Meta FAIR'], 'pdf_title_img': 'assets/pdf/title_img/2502.09604.jpg', 'data': {'categories': ['#optimization', '#long_context', '#training', '#alignment', '#rlhf', '#benchmark'], 'emoji': 'üìö', 'ru': {'title': 'SelfCite: –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò –∏—Å–∫—É—Å—Å—Ç–≤—É —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': 'SelfCite - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ü–∏—Ç–∞—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–≥–Ω–∞–ª –Ω–∞–≥—Ä–∞–¥—ã, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º—ã–π —Å–∞–º–æ–π –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ –∞–±–ª—è—Ü–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–π —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. SelfCite –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∫–∏ best-of-N –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º F1-–º–µ—Ä—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ 5.3 –ø—É–Ω–∫—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ LongBench-Cite.'}, 'en': {'title': 'Enhancing Citation Quality with SelfCite', 'desc': 'SelfCite is a self-supervised method designed to enhance the citation quality in responses generated by large language models (LLMs). It uses a unique reward signal derived from the LLM itself, which assesses the necessity of citations by analyzing the impact of removing or retaining cited text. This approach not only improves the citation generation process during inference but also allows for fine-tuning the models to produce better citations through preference optimization. The results show a significant increase in citation accuracy, as evidenced by a 5.3 point improvement in citation F1 scores on the LongBench-Cite benchmark.'}, 'zh': {'title': 'Ëá™ÁõëÁù£ÂºïÁî®ÁîüÊàêÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'SelfCiteÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™ÁõëÁù£ÊñπÊ≥ïÔºåÊó®Âú®‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁîüÊàêÈ´òË¥®Èáè„ÄÅÁªÜÁ≤íÂ∫¶ÁöÑÂè•Â≠êÁ∫ßÂºïÁî®„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰∏ä‰∏ãÊñáÊ∂àËûçÊèê‰æõÁöÑÂ•ñÂä±‰ø°Âè∑ÔºåÂáèÂ∞ëÂØπÊòÇË¥µÂíåÂä≥Âä®ÂØÜÈõÜÂûãÊ≥®ÈáäÁöÑ‰æùËµñ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ¶ÇÊûúÂºïÁî®ÊòØÂøÖË¶ÅÁöÑÔºåÁßªÈô§Ë¢´ÂºïÁî®ÊñáÊú¨Â∫îÈò≤Ê≠¢Áõ∏ÂêåÁöÑÂìçÂ∫îÔºõÂ¶ÇÊûúË∂≥Â§üÔºå‰øùÁïôË¢´ÂºïÁî®ÊñáÊú¨Â∫î‰øùÊåÅÁõ∏ÂêåÁöÑÂìçÂ∫î„ÄÇSelfCiteÂú®LongBench-CiteÂü∫ÂáÜÊµãËØï‰∏≠ÊòæÁ§∫Âá∫ÊúâÊïàÊÄßÔºå‰ΩøÂºïÁî®ÁöÑF1ÂàÜÊï∞ÊèêÈ´ò‰∫Ü5.3‰∏™ÁôæÂàÜÁÇπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09560', 'title': 'EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents', 'url': 'https://huggingface.co/papers/2502.09560', 'abstract': 'Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.', 'score': 25, 'issue_id': 2211, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '019b4d19788a85cc', 'authors': ['Rui Yang', 'Hanyang Chen', 'Junyu Zhang', 'Mark Zhao', 'Cheng Qian', 'Kangrui Wang', 'Qineng Wang', 'Teja Venkat Koripella', 'Marziyeh Movahedi', 'Manling Li', 'Heng Ji', 'Huan Zhang', 'Tong Zhang'], 'affiliations': ['Northwestern University', 'Toyota Technological Institute at Chicago', 'University of Illinois Urbana-Champaign', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2502.09560.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#games', '#reasoning', '#agents'], 'emoji': 'ü§ñ', 'ru': {'title': 'EmbodiedBench: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ –ò–ò', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω EmbodiedBench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 1128 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞–Ω–∏–π –≤ —á–µ—Ç—ã—Ä–µ—Ö —Å—Ä–µ–¥–∞—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö –∫–∞–∫ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, —Ç–∞–∫ –∏ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å 13 –≤–µ–¥—É—â–∏–º–∏ MLLM –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è–º–∏. EmbodiedBench –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Ä–∞–∑–≤–∏—Ç–∏—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ MLLM.'}, 'en': {'title': 'Empowering Embodied Agents with Comprehensive Evaluation', 'desc': 'This paper discusses the development of EmbodiedBench, a benchmark for evaluating multi-modal large language models (MLLMs) in the context of embodied agents. It highlights the gap in existing evaluation frameworks for MLLM-based agents, which are crucial for performing real-world tasks. The benchmark includes a wide range of tasks that assess various capabilities such as commonsense reasoning and spatial awareness. The results indicate that while MLLMs perform well on high-level tasks, they face significant challenges with low-level manipulation tasks.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂä©ÂäõÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑËØÑ‰º∞‰∏éÂèëÂ±ï', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂàõÂª∫ÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÊñπÈù¢ÁöÑÂ∫îÁî®ÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÂÆû‰∏ñÁïå‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜEmbodiedBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØïÊ°ÜÊû∂ÔºåÁî®‰∫éËØÑ‰º∞ËßÜËßâÈ©±Âä®ÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰Ωì„ÄÇEmbodiedBenchÂåÖÂê´1288‰∏™ÊµãËØï‰ªªÂä°ÔºåÊ∂µÁõñÈ´òÂ±ÇËØ≠‰πâ‰ªªÂä°Âíå‰ΩéÂ±ÇÂéüÂ≠êÂä®‰Ωú‰ªªÂä°ÔºåÂπ∂ËØÑ‰º∞Êô∫ËÉΩ‰ΩìÁöÑÂ∏∏ËØÜÊé®ÁêÜ„ÄÅÂ§çÊùÇÊåá‰ª§ÁêÜËß£„ÄÅÁ©∫Èó¥ÊÑèËØÜ„ÄÅËßÜËßâÊÑüÁü•ÂíåÈïøÊúüËßÑÂàíÁ≠âËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°MLLMsÂú®È´òÂ±Ç‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰ΩÜÂú®‰ΩéÂ±ÇÊìç‰Ωú‰ªªÂä°‰∏ä‰ªçÁÑ∂Â≠òÂú®ÊåëÊàòÔºåÊúÄÂ•ΩÁöÑÊ®°ÂûãGPT-4oÁöÑÂπ≥ÂùáÂæóÂàÜ‰ªÖ‰∏∫28.9%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09620', 'title': 'Exploring the Potential of Encoder-free Architectures in 3D LMMs', 'url': 'https://huggingface.co/papers/2502.09620', 'abstract': 'Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL', 'score': 22, 'issue_id': 2214, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '2e519b64f13f6506', 'authors': ['Yiwen Tang', 'Zoey Guo', 'Zhuhao Wang', 'Ray Zhang', 'Qizhi Chen', 'Junli Liu', 'Delin Qu', 'Zhigang Wang', 'Dong Wang', 'Xuelong Li', 'Bin Zhao'], 'affiliations': ['Northwestern Polytechnical University', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09620.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#optimization', '#3d'], 'emoji': 'üß†', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–ø–æ–Ω–∏–º–∞–Ω–∏–∏: –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –±–µ–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –ø–æ–∫–æ—Ä—è—é—Ç –Ω–æ–≤—ã–µ –≤–µ—Ä—à–∏–Ω—ã', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤–æ–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –±–µ–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –¥–ª—è 3D-–ø–æ–Ω–∏–º–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é LLM-embedded Semantic Encoding –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏ Hierarchical Geometry Aggregation –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ENEL —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã–µ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∏–º–µ—é—â–∏–º–∏ 13 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –±–µ–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ 3D-–ø–æ–Ω–∏–º–∞–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing 3D Understanding with Encoder-Free Architectures', 'desc': 'This paper explores the use of encoder-free architectures for 3D understanding, addressing limitations of traditional encoder-based models. It introduces the LLM-embedded Semantic Encoding strategy during pre-training to enhance point cloud representation and proposes a Hybrid Semantic Loss for better semantic extraction. Additionally, the Hierarchical Geometry Aggregation strategy is introduced in the instruction tuning phase to improve local detail focus in point clouds. The proposed model, ENEL, demonstrates competitive performance against existing models, indicating the potential of encoder-free approaches in 3D Large Multimodal Models.'}, 'zh': {'title': 'Êó†ÁºñÁ†ÅÂô®Êû∂ÊûÑÔºö3DÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊó†ÁºñÁ†ÅÂô®Êû∂ÊûÑÂú®3DÁêÜËß£‰∏≠ÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÈ¶ñÊ¨°ÂØπÂÖ∂ËøõË°åÂÖ®Èù¢Á†îÁ©∂„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ≠ñÁï•Ôºå‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÉΩÂ§üÊõø‰ª£‰º†ÁªüÁöÑ3DÁºñÁ†ÅÂô®ÔºåËß£ÂÜ≥‰∫ÜÁÇπ‰∫ëÂàÜËæ®ÁéáÂèòÂåñÂíåÁâπÂæÅËØ≠‰πâÈúÄÊ±Ç‰∏çÂåπÈÖçÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂºïÂÖ•ÂµåÂÖ•ÂºèËØ≠‰πâÁºñÁ†ÅÂíåÂàÜÂ±ÇÂá†‰ΩïËÅöÂêàÁ≠ñÁï•ÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®ÂàÜÁ±ª„ÄÅÊèèËø∞ÂíåËßÜËßâÈóÆÁ≠î‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑ7BÊ®°ÂûãENELÂú®ÊÄßËÉΩ‰∏ä‰∏éÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãShapeLLM-13BÁõ∏Â™≤ÁæéÔºåÊòæÁ§∫Âá∫Êó†ÁºñÁ†ÅÂô®Êû∂ÊûÑÂú®3DÁêÜËß£È¢ÜÂüüÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06608', 'title': 'TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models', 'url': 'https://huggingface.co/papers/2502.06608', 'abstract': 'Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.', 'score': 22, 'issue_id': 2210, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '0602cc4a46e4c69c', 'authors': ['Yangguang Li', 'Zi-Xin Zou', 'Zexiang Liu', 'Dehu Wang', 'Yuan Liang', 'Zhipeng Yu', 'Xingchao Liu', 'Yuan-Chen Guo', 'Ding Liang', 'Wanli Ouyang', 'Yan-Pei Cao'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'The University of Texas at Austin', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2502.06608.jpg', 'data': {'categories': ['#3d', '#open_source', '#training', '#dataset', '#diffusion'], 'emoji': 'üßä', 'ru': {'title': 'TripoSG: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–æ—Ä–º', 'desc': 'TripoSG - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–æ—Ä–º, —Å–ø–æ—Å–æ–±–Ω–∞—è —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ 3D-–º–æ–¥–µ–ª–∏ —Å —Ç–æ—á–Ω—ã–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –≤—Ö–æ–¥–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—Å—à—Ç–∞–±–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Ñ–æ—Ä–º, –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ –æ–±—à–∏—Ä–Ω—ã—Ö –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. TripoSG –ø—Ä–∏–º–µ–Ω—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â—É—é –ø–æ—Ç–µ—Ä–∏ SDF, –Ω–æ—Ä–º–∞–ª–µ–π –∏ —ç–π–∫–æ–Ω–∞–ª–∞ –¥–ª—è 3D VAE. –ë–ª–∞–≥–æ–¥–∞—Ä—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —ç—Ç–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤, TripoSG –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Ñ–æ—Ä–º —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ –≤–µ—Ä–Ω–æ—Å—Ç—å—é –≤—Ö–æ–¥–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º.'}, 'en': {'title': 'TripoSG: Revolutionizing 3D Shape Generation with Diffusion Techniques', 'desc': 'This paper introduces TripoSG, a new method for generating high-quality 3D shapes using diffusion techniques. It addresses the challenges of 3D shape generation by employing a large-scale rectified flow transformer and a hybrid supervised training strategy that enhances reconstruction performance. The model is trained on a vast dataset, producing 2 million high-quality 3D samples, which improves the fidelity and detail of the generated shapes. TripoSG not only achieves state-of-the-art results but also shows strong generalization across various input images, making it a significant advancement in 3D generative models.'}, 'zh': {'title': 'TripoSGÔºöÈ´ò‰øùÁúü3DÂΩ¢Áä∂ÁîüÊàêÁöÑÊñ∞ËåÉÂºè', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ3DÂΩ¢Áä∂ÁîüÊàêÊñπÊ≥ïTripoSGÔºåÊó®Âú®ÊèêÈ´ò3DÁΩëÊ†ºÁöÑÁîüÊàêË¥®Èáè„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§ßËßÑÊ®°ÁöÑÊµÅÂä®ÂèòÊç¢Âô®ÔºåËÉΩÂ§üÂú®Â§ßÈáèÈ´òË¥®ÈáèÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄåÂÆûÁé∞È´ò‰øùÁúüÂ∫¶ÁöÑ3DÂΩ¢Áä∂ÁîüÊàê„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÈááÁî®‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÁõëÁù£ËÆ≠ÁªÉÁ≠ñÁï•ÔºåÁªìÂêà‰∫ÜSDF„ÄÅÊ≥ïÁ∫øÂíåEikonalÊçüÂ§±Ôºå‰ª•ÊèêÈ´ò3DÈáçÂª∫ÊÄßËÉΩ„ÄÇÈÄöËøáÂÖ®Èù¢ÁöÑÂÆûÈ™åÈ™åËØÅÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âú®3DÂΩ¢Áä∂ÁîüÊàêÊñπÈù¢ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂ±ïÁé∞‰∫ÜÂØπËæìÂÖ•ÂõæÂÉèÁöÑÈ´ò‰øùÁúüÂ∫¶ÂíåÂ§öÊ†∑ÂåñÁöÑÁîüÊàêËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09082', 'title': 'CoSER: Coordinating LLM-Based Persona Simulation of Established Roles', 'url': 'https://huggingface.co/papers/2502.09082', 'abstract': 'Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively.', 'score': 20, 'issue_id': 2214, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '3d13dc492344cf84', 'authors': ['Xintao Wang', 'Heng Wang', 'Yifei Zhang', 'Xinfeng Yuan', 'Rui Xu', 'Jen-tse Huang', 'Siyu Yuan', 'Haoran Guo', 'Jiangjie Chen', 'Wei Wang', 'Yanghua Xiao', 'Shuchang Zhou'], 'affiliations': ['Fudan University', 'Johns Hopkins University', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2502.09082.jpg', 'data': {'categories': ['#dataset', '#story_generation', '#benchmark', '#agents', '#open_source'], 'emoji': 'üé≠', 'ru': {'title': 'CoSER: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–æ–ª–µ–≤–æ–º –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CoSER - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ç–æ–∫–æ–ª –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–æ–ª–µ–≤–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö CoSER —Å–æ–¥–µ—Ä–∂–∏—Ç 17 966 –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏–∑ 771 –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∫–Ω–∏–≥–∏, –≤–∫–ª—é—á–∞—è –∞—É—Ç–µ–Ω—Ç–∏—á–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø–æ–Ω—è—Ç–∏–µ '–∞–∫—Ç–µ—Ä—Å–∫–æ–π –∏–≥—Ä—ã –≤ –∑–∞–¥–∞–Ω–Ω—ã—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞—Ö' –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ —Ä–æ–ª–µ–≤—ã—Ö LLM. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ CoSER 8B –∏ CoSER 70B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø—Ä–∏—á–µ–º CoSER 70B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∏–ª–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç GPT-4 –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –æ—Ü–µ–Ω–∫–∏."}, 'en': {'title': 'Empowering Role-Playing Agents with CoSER: A New Dataset and Methodology', 'desc': 'This paper introduces CoSER, a comprehensive dataset designed to enhance role-playing language agents (RPLAs) using large language models (LLMs). It includes 17,966 characters from 771 well-known books, providing authentic dialogues and various data types that reflect real-world interactions. The authors propose a novel training and evaluation method based on acting techniques, allowing LLMs to portray multiple characters in specific scenes. The results show that the CoSER 70B model outperforms existing models like GPT-4o in several benchmarks, demonstrating the effectiveness of the CoSER dataset for training and evaluating RPLAs.'}, 'zh': {'title': 'ÊèêÂçáËßíËâ≤ÊâÆÊºîËØ≠Ë®Ä‰ª£ÁêÜÁöÑÊúâÊïàÊÄß', 'desc': 'ËßíËâ≤ÊâÆÊºîËØ≠Ë®Ä‰ª£ÁêÜÔºàRPLAÔºâÊòØÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊñ∞ÂÖ¥Â∫îÁî®Ôºå‰ΩÜÊ®°ÊãüÂ∑≤Âª∫Á´ãËßíËâ≤ÁöÑ‰ªªÂä°ÂÖ∑ÊúâÊåëÊàòÊÄßÔºåÂõ†‰∏∫Áº∫‰πèÁúüÂÆûÁöÑËßíËâ≤Êï∞ÊçÆÈõÜÂíåÁªÜËá¥ÁöÑËØÑ‰º∞ÊñπÊ≥ï„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜCoSERÔºå‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜ„ÄÅÂºÄÊîæÊ®°ÂûãÂíåËØÑ‰º∞ÂçèËÆÆÔºåÊó®Âú®ÊúâÊïàÊîØÊåÅÂ∑≤Âª∫Á´ãËßíËâ≤ÁöÑRPLA„ÄÇCoSERÊï∞ÊçÆÈõÜÊ∂µÁõñ‰∫Ü771Êú¨ËëóÂêç‰π¶Á±ç‰∏≠ÁöÑ17,966‰∏™ËßíËâ≤ÔºåÊèê‰æõ‰∫ÜÁúüÂÆûÂØπËØùÂíåÂ§öÊ†∑ÂåñÁöÑÊï∞ÊçÆÁ±ªÂûã„ÄÇÈÄöËøáÂºïÂÖ•ÁªôÂÆöÊÉÖÂ¢ÉË°®ÊºîÁöÑÊñπÊ≥ïÔºåÊàë‰ª¨ËÆ≠ÁªÉÂíåËØÑ‰º∞ËßíËâ≤ÊâÆÊºîÁöÑLLMÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéCoSERÊï∞ÊçÆÈõÜÂú®RPLAËÆ≠ÁªÉ„ÄÅËØÑ‰º∞ÂíåÊ£ÄÁ¥¢‰∏≠ÁöÑÈáçË¶Å‰ª∑ÂÄº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09621', 'title': 'MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency', 'url': 'https://huggingface.co/papers/2502.09621', 'abstract': 'Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/', 'score': 19, 'issue_id': 2213, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'ac755f4f5584a58c', 'authors': ['Dongzhi Jiang', 'Renrui Zhang', 'Ziyu Guo', 'Yanwei Li', 'Yu Qi', 'Xinyan Chen', 'Liuhui Wang', 'Jianhan Jin', 'Claire Guo', 'Shen Yan', 'Bo Zhang', 'Chaoyou Fu', 'Peng Gao', 'Hongsheng Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.09621.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ Chain-of-Thought –≤ LMM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MME-CoT - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LMM). –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–µ—Å—Ç—å –æ–±–ª–∞—Å—Ç–µ–π –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ç—Ä–∏ –Ω–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ CoT, –ø—Ä–∏ —ç—Ç–æ–º Kimi k1.5 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç GPT-4o. –û–¥–Ω–∞–∫–æ –≤—ã—è–≤–ª–µ–Ω–æ, —á—Ç–æ CoT –º–æ–∂–µ—Ç —É—Ö—É–¥—à–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LMM –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –∞ –º–æ–¥–µ–ª–∏ —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with MME-CoT', 'desc': 'This paper presents MME-CoT, a benchmark designed to evaluate the Chain-of-Thought (CoT) reasoning capabilities of Large Multimodal Models (LMMs) across various domains. It introduces three new metrics to assess reasoning quality, robustness, and efficiency in a detailed manner. The study reveals that models utilizing a reflection mechanism, like Kimi k1.5, outperform others such as GPT-4o in CoT quality, but may struggle with perception-heavy tasks due to overthinking. Overall, MME-CoT aims to enhance the understanding and development of multimodal reasoning in LMMs.'}, 'zh': {'title': 'MME-CoTÔºöÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂü∫ÂáÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜMME-CoTÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®ËØÑ‰º∞Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®ÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊé®ÁêÜÊÄßËÉΩÁöÑÂü∫ÂáÜÔºåÊ∂µÁõñÊï∞Â≠¶„ÄÅÁßëÂ≠¶„ÄÅÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´„ÄÅÈÄªËæë„ÄÅÊó∂Á©∫Âíå‰∏ÄËà¨Âú∫ÊôØÂÖ≠‰∏™È¢ÜÂüü„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÂ•óÂÖ®Èù¢ÁöÑËØÑ‰º∞Â∑•ÂÖ∑ÔºåÂåÖÂê´‰∏âÁßçÊñ∞È¢ñÁöÑÊåáÊ†áÔºåÁªÜËá¥ËØÑ‰º∞Êé®ÁêÜË¥®Èáè„ÄÅÈ≤ÅÊ£íÊÄßÂíåÊïàÁéá„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂÖ∑ÊúâÂèçÊÄùÊú∫Âà∂ÁöÑÊ®°ÂûãÂú®CoTË¥®Èáè‰∏äË°®Áé∞‰ºòË∂äÔºåËÄåÂú®ÊÑüÁü•ÂØÜÈõÜÂûã‰ªªÂä°‰∏≠ÔºåCoTÊèêÁ§∫ÂèØËÉΩ‰ºöÈôç‰ΩéLMMÁöÑÊÄßËÉΩ„ÄÇÂ∞ΩÁÆ°CoTË¥®ÈáèËæÉÈ´òÔºå‰ΩÜÂÖ∑ÊúâÂèçÊÄùÁöÑLMMÂú®Ê≠£Â∏∏ÂìçÂ∫îÂíåËá™Êàë‰øÆÊ≠£Èò∂ÊÆµË°®Áé∞Âá∫ÊòæËëóÁöÑ‰ΩéÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09100', 'title': 'Logical Reasoning in Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2502.09100', 'abstract': 'With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.', 'score': 17, 'issue_id': 2209, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '72b32d40c559c7e4', 'authors': ['Hanmeng Liu', 'Zhizhang Fu', 'Mengru Ding', 'Ruoxi Ning', 'Chaoli Zhang', 'Xiaozhang Liu', 'Yue Zhang'], 'affiliations': ['Hainan University', 'Westlake University', 'Zhejiang Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09100.jpg', 'data': {'categories': ['#training', '#survey', '#reasoning', '#rl', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–õ–æ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã', 'desc': '–≠—Ç–æ—Ç –æ–±–∑–æ—Ä –ø–æ—Å–≤—è—â–µ–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è–º –≤ –æ–±–ª–∞—Å—Ç–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –í –Ω–µ–º —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã –∏ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–∞—Ä–∞–¥–∏–≥–º–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –¥–µ–¥—É–∫—Ç–∏–≤–Ω–æ–µ, –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–µ, –∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–µ –∏ –∞–Ω–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–µ. –¢–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Ç–∞–∫–∏–µ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã.'}, 'en': {'title': 'Enhancing Logical Reasoning in Large Language Models', 'desc': 'This paper reviews the progress made in enhancing logical reasoning capabilities of large language models (LLMs) like OpenAI o3 and DeepSeek-R1. It discusses various reasoning paradigms such as deductive, inductive, abductive, and analogical reasoning, highlighting their theoretical foundations and evaluation benchmarks. The authors analyze strategies to improve reasoning performance, including data-centric tuning and neuro-symbolic approaches. The paper concludes by suggesting future research directions to further develop logical reasoning in AI systems.'}, 'zh': {'title': 'ÊèêÂçáAIÁ≥ªÁªüÈÄªËæëÊé®ÁêÜËÉΩÂäõÁöÑÊé¢Á¥¢', 'desc': 'ÈöèÁùÄOpenAI o3ÂíåDeepSeek-R1Á≠âÂÖàËøõÊé®ÁêÜÊ®°ÂûãÁöÑÂá∫Áé∞ÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ±ïÁé∞‰∫ÜÂá∫Ëâ≤ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨Âú®ËøõË°å‰∏•Ê†ºÈÄªËæëÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™Ëß£‰πãË∞ú„ÄÇÊú¨ÊñáÁªºËø∞‰∫ÜLLMs‰∏≠ÈÄªËæëÊé®ÁêÜÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÊé¢ËÆ®‰∫ÜÈÄªËæëÊé®ÁêÜÁöÑËåÉÂõ¥„ÄÅÁêÜËÆ∫Âü∫Á°Ä‰ª•ÂèäËØÑ‰º∞Êé®ÁêÜËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫Ü‰∏çÂêåÊé®ÁêÜËåÉÂºèÔºàÂ¶ÇÊºîÁªé„ÄÅÂΩíÁ∫≥„ÄÅÊ∫ØÂõ†ÂíåÁ±ªÊØîÔºâÁöÑÁé∞ÊúâËÉΩÂäõÔºåÂπ∂ËØÑ‰º∞‰∫ÜÂ¢ûÂº∫Êé®ÁêÜÊÄßËÉΩÁöÑÁ≠ñÁï•ÔºåÂåÖÊã¨Êï∞ÊçÆ‰∏≠ÂøÉË∞É‰ºò„ÄÅÂº∫ÂåñÂ≠¶‰π†„ÄÅËß£Á†ÅÁ≠ñÁï•ÂíåÁ•ûÁªèÁ¨¶Âè∑ÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09042', 'title': 'Typhoon T1: An Open Thai Reasoning Model', 'url': 'https://huggingface.co/papers/2502.09042', 'abstract': 'This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.', 'score': 15, 'issue_id': 2213, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '5cb078b546437366', 'authors': ['Pittawat Taveekitworachai', 'Potsawee Manakul', 'Kasima Tharnpipitchai', 'Kunat Pipatanakul'], 'affiliations': ['SCB 10X R&D SCBX Group Bangkok, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2502.09042.jpg', 'data': {'categories': ['#reasoning', '#training', '#multilingual', '#dataset', '#open_source', '#low_resource', '#data', '#synthetic'], 'emoji': 'üß†', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —Ç–∞–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Typhoon T1 - –æ—Ç–∫—Ä—ã—Ç—É—é –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ —Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ. –ú–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π - —ç—Ç–æ –Ω–æ–≤—ã–π —Ç–∏–ø –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–ª–∏–Ω–Ω—É—é —Ü–µ–ø–æ—á–∫—É –º—ã—Å–ª–µ–π –ø–µ—Ä–µ–¥ –≤—ã–¥–∞—á–µ–π –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–ª—è—Ç—Å—è –¥–µ—Ç–∞–ª—è–º–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Ç–∞–∫–æ–π –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –≤–º–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Typhoon T1: Advancing Thai Reasoning Models for Complex Tasks', 'desc': 'This paper presents Typhoon T1, an initiative to create an open reasoning model specifically for the Thai language. Reasoning models are advanced generative models that produce a sequence of thoughts leading to a conclusion, enhancing performance on intricate tasks. The authors focus on developing this model using supervised fine-tuning with open datasets, rather than relying on reinforcement learning, making it more accessible and cost-effective. The paper details the synthetic data generation process, training methods, and shares the model weights, aiming to support future research in low-resource language reasoning models.'}, 'zh': {'title': 'ÂºÄÊîæÊ≥∞ËØ≠Êé®ÁêÜÊ®°ÂûãÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜTyphoon T1ÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÂèëÂºÄÊîæÊ≥∞ËØ≠Êé®ÁêÜÊ®°ÂûãÁöÑÈ°πÁõÆ„ÄÇÊé®ÁêÜÊ®°ÂûãÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÁîüÊàêÊ®°ÂûãÔºåÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊûÑÂª∫ÔºåËÉΩÂ§üÂú®ÂæóÂá∫ÊúÄÁªàÁ≠îÊ°à‰πãÂâçÁîüÊàêÈïøÈìæÊÄùÁª¥Ôºå‰ªéËÄåÊèêÈ´òÂ§çÊùÇ‰ªªÂä°ÁöÑË°®Áé∞„ÄÇTyphoon T1ÈÄöËøáÂà©Áî®ÁõëÁù£ÂæÆË∞ÉÂíåÂºÄÊîæÊï∞ÊçÆÈõÜÔºå‰ª•Êõ¥ÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÊñπÂºèÊ∑±ÂÖ•Êé¢ËÆ®Êé®ÁêÜÊ®°ÂûãÁöÑÂºÄÂèëÔºåÈÅøÂÖç‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ§çÊùÇÊÄß„ÄÇÊàë‰ª¨Â∏åÊúõËøô‰∏™ÂºÄÊîæÈ°πÁõÆ‰∏∫ËØ•È¢ÜÂüüÁöÑËøõ‰∏ÄÊ≠•Á†îÁ©∂Â•†ÂÆöÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09390', 'title': 'SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models', 'url': 'https://huggingface.co/papers/2502.09390', 'abstract': "In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.", 'score': 12, 'issue_id': 2214, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '4caf9cec56011350', 'authors': ['Daniel Fleischer', 'Moshe Berchansky', 'Gad Markovits', 'Moshe Wasserblat'], 'affiliations': ['IntelLabs'], 'pdf_title_img': 'assets/pdf/title_img/2502.09390.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': 'SQuARE: —Å–∞–º–æ–æ–ø—Ä–æ—Å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SQuARE, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. SQuARE –ø–æ–±—É–∂–¥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ —Ä–µ—à–∞—Ç—å –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å, —á—Ç–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —Ç–µ–º—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ Llama 3 –∏ GPT-4 –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ SQuARE –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ü–µ–ø–æ—á–∫–∏ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã, –ø–æ–≤—ã—à–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Unlocking Deeper Reasoning with SQuARE', 'desc': "This paper presents SQuARE, a new prompting technique aimed at enhancing the reasoning abilities of Large Language Models (LLMs) in Natural Language Processing. Unlike traditional methods that may not fully utilize a model's reasoning potential, SQuARE employs a self-interrogation approach, encouraging models to generate and answer multiple auxiliary questions before addressing the main question. This method builds on existing chain-of-thought frameworks, allowing for a deeper exploration of topics. Evaluations with Llama 3 and GPT-4o show that SQuARE outperforms conventional prompting techniques, leading to improved reasoning in question-answering tasks."}, 'zh': {'title': 'SQuAREÔºöÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈù¢‰∏¥Ë∂äÊù•Ë∂äÂ§çÊùÇÁöÑÊé®ÁêÜÊåëÊàò„ÄÇ‰º†ÁªüÁöÑÈìæÂºèÊÄùÁª¥ÊèêÁ§∫ÊñπÊ≥ïËôΩÁÑ∂Êúâ‰∏ÄÂÆöÊïàÊûúÔºå‰ΩÜÂæÄÂæÄÊó†Ê≥ïÂÖÖÂàÜÂèëÊå•Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊèêÁ§∫ÊäÄÊúØSQuAREÔºàÈ°∫Â∫èÈóÆÁ≠îÊé®ÁêÜÂºïÊìéÔºâÔºåÈÄöËøáËá™ÊàëË¥®ËØ¢ÁöÑÊñπÂºèÊù•ÊîπÂñÑÊé®ÁêÜËøáÁ®ã„ÄÇSQuAREÂú®Â§ö‰∏™ÈóÆÁ≠îÊï∞ÊçÆÈõÜ‰∏äÁöÑËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÂÖ∂Âú®Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑÈìæÂºèÊÄùÁª¥ÊèêÁ§∫ÂíåÁé∞ÊúâÁöÑÈáçËø∞-ÂõûÂ∫îÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09601', 'title': 'CoT-Valve: Length-Compressible Chain-of-Thought Tuning', 'url': 'https://huggingface.co/papers/2502.09601', 'abstract': "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.", 'score': 11, 'issue_id': 2212, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': '4b0518724dea8b37', 'authors': ['Xinyin Ma', 'Guangnian Wan', 'Runpeng Yu', 'Gongfan Fang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.09601.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#optimization', '#inference'], 'emoji': 'üß†', 'ru': {'title': '–≠–ª–∞—Å—Ç–∏—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CoT-Valve, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Ä–∞–∑–ª–∏—á–Ω–æ–π –¥–ª–∏–Ω—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–∞–Ω–∏–ø—É–ª–∏—Ä—É—è –∫–æ—Ç–æ—Ä—ã–º, –º–æ–∂–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–π —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –û–Ω–∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –º–µ—Ç–æ–¥—ã —Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–∂–∏–º–∞–µ–º—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –¥–ª–∏–Ω—ã —Ü–µ–ø–æ—á–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CoT-Valve —É—Å–ø–µ—à–Ω–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å –∏ —Å–∂–∏–º–∞–µ–º–æ—Å—Ç—å —Ü–µ–ø–æ—á–∫–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤.'}, 'en': {'title': 'Dynamic Reasoning Control with CoT-Valve', 'desc': 'This paper presents CoT-Valve, a new strategy for controlling the length of reasoning chains in machine learning models, particularly in Chain-of-Thought (CoT) reasoning. The authors observe that while longer reasoning paths improve performance, they also increase inference costs, especially on harder tasks. CoT-Valve allows a single model to dynamically adjust the length of its reasoning based on task difficulty, thus optimizing efficiency. The experiments demonstrate that this method not only compresses reasoning chains significantly but also maintains high performance compared to traditional prompt-based controls.'}, 'zh': {'title': 'Âä®ÊÄÅÊéßÂà∂Êé®ÁêÜÈìæÈïøÂ∫¶ÁöÑÂàõÊñ∞Á≠ñÁï•', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫CoT-ValveÁöÑÊñ∞Á≠ñÁï•ÔºåÁî®‰∫éÂä®ÊÄÅÊéßÂà∂Êé®ÁêÜÈìæÁöÑÈïøÂ∫¶Ôºå‰ª•Èôç‰ΩéÊé®ÁêÜÊ®°ÂûãÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊé®ÁêÜË∑ØÂæÑÂú®ÁÆÄÂçï‰ªªÂä°‰∏≠ÂÆπÊòìÂéãÁº©Ôºå‰ΩÜÂú®Âõ∞Èöæ‰ªªÂä°‰∏≠ÂàôËæÉ‰∏∫Âõ∞ÈöæÔºåÂõ†Ê≠§Êàë‰ª¨Êé¢Á¥¢‰∫ÜÂ¶Ç‰ΩïÂú®Âçï‰∏ÄÊ®°Âûã‰∏≠ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†á„ÄÇÈÄöËøáË∞ÉÊï¥ÂèÇÊï∞Á©∫Èó¥‰∏≠ÁöÑÊñπÂêëÔºåCoT-ValveËÉΩÂ§üÊúâÊïàÊéßÂà∂ÁîüÊàêÁöÑÊé®ÁêÜÈìæÈïøÂ∫¶ÔºåÂπ∂‰∏îÂú®ÂéãÁº©Êé®ÁêÜÈìæÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCoT-ValveÂú®ÊéßÂà∂ÂíåÂéãÁº©Êé®ÁêÜÈìæÊñπÈù¢‰ºò‰∫éÂü∫‰∫éÊèêÁ§∫ÁöÑÊéßÂà∂ÊñπÊ≥ïÔºå‰∏îÂú®ÊÄßËÉΩ‰∏ä‰ªÖÊúâËΩªÂæÆ‰∏ãÈôç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08468', 'title': 'mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data', 'url': 'https://huggingface.co/papers/2502.08468', 'abstract': 'Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.', 'score': 10, 'issue_id': 2211, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': 'ac7814f15d1e9616', 'authors': ['Haonan Chen', 'Liang Wang', 'Nan Yang', 'Yutao Zhu', 'Ziliang Zhao', 'Furu Wei', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'Microsoft Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2502.08468.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#synthetic', '#dataset', '#training', '#data', '#multilingual'], 'emoji': 'üåê', 'ru': {'title': '–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ mmE5, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –ê–≤—Ç–æ—Ä—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ —Ç—Ä–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: —à–∏—Ä–æ–∫–∏–π –æ—Ö–≤–∞—Ç –∑–∞–¥–∞—á –∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π, –Ω–∞–¥–µ–∂–Ω–æ–µ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –≤—ã—Å–æ–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å. –ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã, –æ–Ω–∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ mmE5 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö MMEB –∏ XTD.'}, 'en': {'title': 'Enhancing Multimodal Learning with High-Quality Synthetic Data', 'desc': 'This paper discusses the development of multimodal embedding models that integrate different types of data, like text and images, into a single representation. It highlights the challenge of limited labeled multimodal data and proposes a solution through high-quality synthetic data generation. The authors establish three key criteria for effective synthetic data: broad scope, robust cross-modal alignment, and high fidelity. By adhering to these principles, they create a multimodal multilingual E5 model, mmE5, which demonstrates exceptional performance on various benchmarks.'}, 'zh': {'title': 'È´òË¥®ÈáèÂêàÊàêÊï∞ÊçÆÂä©ÂäõÂ§öÊ®°ÊÄÅÊ®°Âûã', 'desc': 'Â§öÊ®°ÊÄÅÂµåÂÖ•Ê®°ÂûãËÉΩÂ§üÂ∞ÜÊñáÊú¨ÂíåÂõæÂÉèÁ≠â‰∏çÂêåÊ®°ÊÄÅÁöÑÊï∞ÊçÆÊò†Â∞ÑÂà∞Áªü‰∏ÄÁöÑË°®Á§∫Á©∫Èó¥Ôºå‰ΩÜÊúâÈôêÁöÑÊ†áÊ≥®Â§öÊ®°ÊÄÅÊï∞ÊçÆÂ∏∏Â∏∏ÂΩ±ÂìçÂµåÂÖ•ÊÄßËÉΩ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÈ´òË¥®ÈáèÂêàÊàêÂ§öÊ®°ÊÄÅÊï∞ÊçÆÁöÑ‰∏â‰∏™Ê†áÂáÜÔºöÂπøÊ≥õÁöÑËåÉÂõ¥„ÄÅÁ®≥ÂÅ•ÁöÑË∑®Ê®°ÊÄÅÂØπÈΩêÂíåÈ´ò‰øùÁúüÂ∫¶„ÄÇÈÄöËøáËøô‰∫õÊ†áÂáÜÔºåÊàë‰ª¨ÂêàÊàê‰∫ÜË¶ÜÁõñÂ§öÁßç‰ªªÂä°ÂíåÊ®°ÊÄÅÁªÑÂêàÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂Âà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°åÁîüÊàê„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ËÆ≠ÁªÉÁöÑÂ§öÊ®°ÊÄÅÂ§öËØ≠Ë®ÄE5Ê®°ÂûãmmE5Âú®MMEBÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Âú®XTDÂü∫ÂáÜÊµãËØï‰∏≠Â±ïÁé∞‰∫ÜÂçìË∂äÁöÑÂ§öËØ≠Ë®ÄÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08680', 'title': 'Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges', 'url': 'https://huggingface.co/papers/2502.08680', 'abstract': "Mathematical reasoning in Large Language Models (LLMs) is often evaluated using benchmarks with limited numerical ranges, failing to reflect real-world problem-solving across diverse scales. Furthermore, most existing evaluation methods only compare model outputs to ground-truth answers, obscuring insights into reasoning processes. To address these limitations, we introduce GSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs numerical values in math problems to assess model robustness across varying numerical scales. Additionally, we propose a novel grading methodology that distinguishes between logical and non-logical errors, offering a more precise evaluation of reasoning processes beyond computational accuracy. Our experiments with various models reveal a significant increase in logical error rates-up to 14 percentage points-as numerical complexity rises, demonstrating a general weakness in reasoning with out-of-distribution numerical values. Moreover, while models demonstrate high accuracy on standalone arithmetic tasks, their performance deteriorates substantially when computations are embedded within word problems. These findings provide a comprehensive evaluation of LLMs' mathematical reasoning capabilities and inform future research directions for improving numerical generalization in language models.", 'score': 9, 'issue_id': 2221, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '7d5f08a6e4748cfc', 'authors': ['Safal Shrestha', 'Minwu Kim', 'Keith Ross'], 'affiliations': ['New York University Abu Dhabi'], 'pdf_title_img': 'assets/pdf/title_img/2502.08680.jpg', 'data': {'categories': ['#benchmark', '#math', '#dataset', '#reasoning'], 'emoji': 'üßÆ', 'ru': {'title': '–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö GSM-Ranges, –∫–æ—Ç–æ—Ä—ã–π —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–º–µ–Ω—è–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —á–∏—Å–ª–æ–≤—ã–º –º–∞—Å—à—Ç–∞–±–∞–º. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏, —Ä–∞–∑–ª–∏—á–∞—é—â–∞—è –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏ –Ω–µ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ –ø—Ä–∏ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —á–∏—Å–ª–æ–≤–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –≤—ã—è–≤–ª—è—è –æ–±—â—É—é —Å–ª–∞–±–æ—Å—Ç—å –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —á–∏—Å–ª–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.'}, 'en': {'title': 'Enhancing Mathematical Reasoning Evaluation in LLMs with GSM-Ranges', 'desc': 'This paper addresses the limitations of evaluating mathematical reasoning in Large Language Models (LLMs) using benchmarks with narrow numerical ranges. It introduces GSM-Ranges, a dataset generator that modifies numerical values in math problems to test model robustness across different scales. The authors also present a new grading method that differentiates between logical and non-logical errors, enhancing the evaluation of reasoning processes. Experiments show that as numerical complexity increases, models struggle more with logical reasoning, especially in word problems, highlighting areas for improvement in numerical generalization.'}, 'zh': {'title': 'ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄßÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜ‰∏çÂêåÊï∞ÂÄºËåÉÂõ¥ÁöÑÈóÆÈ¢òÊó∂„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫ÜGSM-RangesÊï∞ÊçÆÈõÜÁîüÊàêÂô®ÔºåÈÄöËøáÁ≥ªÁªüÊÄßÂú∞ÊîπÂèòÊï∞Â≠¶ÈóÆÈ¢ò‰∏≠ÁöÑÊï∞ÂÄºÔºåÊù•ËØÑ‰º∞Ê®°ÂûãÂú®‰∏çÂêåÊï∞ÂÄºËßÑÊ®°‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑÂàÜÊñπÊ≥ïÔºåÂèØ‰ª•Âå∫ÂàÜÈÄªËæëÈîôËØØÂíåÈùûÈÄªËæëÈîôËØØÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞Êé®ÁêÜËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÈöèÁùÄÊï∞ÂÄºÂ§çÊùÇÊÄßÁöÑÂ¢ûÂä†ÔºåÊ®°ÂûãÁöÑÈÄªËæëÈîôËØØÁéáÊòæËëó‰∏äÂçáÔºåË°®ÊòéÂú®Â§ÑÁêÜË∂ÖÂá∫ËÆ≠ÁªÉËåÉÂõ¥ÁöÑÊï∞ÂÄºÊó∂ÔºåÊ®°ÂûãÂ≠òÂú®ÊôÆÈÅçÁöÑÊé®ÁêÜÂº±ÁÇπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09614', 'title': 'DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References', 'url': 'https://huggingface.co/papers/2502.09614', 'abstract': "We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness. Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models. We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller. Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations. We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments. At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method. The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity. We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world. Our method achieves over a 10% improvement in success rates compared to leading baselines. The project website with animated results is available at https://meowuu7.github.io/DexTrack/.", 'score': 9, 'issue_id': 2216, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'f83cdb806eef0812', 'authors': ['Xueyi Liu', 'Jianibieke Adalibieke', 'Qianwei Han', 'Yuzhe Qin', 'Li Yi'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.09614.jpg', 'data': {'categories': ['#robotics', '#training', '#optimization', '#rl', '#games', '#agents'], 'emoji': 'ü¶æ', 'ru': {'title': '–ù–µ–π—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –¥–ª—è –ª–æ–≤–∫–∏—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä—É–∫–∏', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π –ø—Ä–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è—Ö —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä—É–∫–æ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–¥—Ö–æ–¥, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–æ–º–æ—Ç–æ–ø–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π.'}, 'en': {'title': 'Empowering Robots with Human-Like Dexterity through Neural Tracking!', 'desc': 'This paper presents a novel neural tracking controller designed for dexterous manipulation of objects by a robot hand, using human references as a guide. The authors tackle the complexities of contact dynamics and the need for the controller to be adaptable and robust across various tasks. They propose a method that combines reinforcement learning and imitation learning, leveraging a large dataset of successful robot tracking demonstrations to improve performance iteratively. The results show a significant improvement in success rates, demonstrating the effectiveness of their approach in both simulated and real-world environments.'}, 'zh': {'title': 'ÈÄöÁî®Á•ûÁªèÊéßÂà∂Âô®ÔºöÊèêÂçáÁÅµÂ∑ßÊìç‰ΩúÁöÑÊàêÂäüÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑÁ•ûÁªèË∑üË∏™ÊéßÂà∂Âô®ÔºåÁî®‰∫é‰ªé‰∫∫Á±ªÂèÇËÄÉ‰∏≠ËøõË°åÁÅµÂ∑ßÊìç‰Ωú„ÄÇËØ•ÊéßÂà∂Âô®Êó®Âú®ÁÆ°ÁêÜÁÅµÂ∑ßÊú∫Âô®‰∫∫ÊâãÔºå‰ª•ÊìçÊéßÂ§öÁßçÁâ©‰ΩìÔºåÈÄÇÂ∫î‰∏çÂêåÁöÑ‰∫∫Êú∫‰∫§‰∫íÈúÄÊ±Ç„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÂ§ßËßÑÊ®°ÊàêÂäüÁöÑÊú∫Âô®‰∫∫Ë∑üË∏™Á§∫‰æãÊù•ËÆ≠ÁªÉÊéßÂà∂Âô®ÔºåÂπ∂ÁªìÂêàÂº∫ÂåñÂ≠¶‰π†ÂíåÊ®°‰ªøÂ≠¶‰π†Êù•ÊèêÂçáÂÖ∂Âú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Âú®‰ªøÁúüÂíåÁé∞ÂÆû‰∏ñÁïå‰∏≠ËØÑ‰º∞‰∫ÜËØ•ÊéßÂà∂Âô®ÔºåÊàêÂäüÁéáÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü10%‰ª•‰∏ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05761', 'title': '3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly', 'url': 'https://huggingface.co/papers/2502.05761', 'abstract': 'Industrial anomaly detection achieves progress thanks to datasets such as MVTec-AD and VisA. However, they suf- fer from limitations in terms of the number of defect sam- ples, types of defects, and availability of real-world scenes. These constraints inhibit researchers from further exploring the performance of industrial detection with higher accuracy. To this end, we propose a new large-scale anomaly detection dataset called 3CAD, which is derived from real 3C produc- tion lines. Specifically, the proposed 3CAD includes eight different types of manufactured parts, totaling 27,039 high- resolution images labeled with pixel-level anomalies. The key features of 3CAD are that it covers anomalous regions of different sizes, multiple anomaly types, and the possibility of multiple anomalous regions and multiple anomaly types per anomaly image. This is the largest and first anomaly de- tection dataset dedicated to 3C product quality control for community exploration and development. Meanwhile, we in- troduce a simple yet effective framework for unsupervised anomaly detection: a Coarse-to-Fine detection paradigm with Recovery Guidance (CFRG). To detect small defect anoma- lies, the proposed CFRG utilizes a coarse-to-fine detection paradigm. Specifically, we utilize a heterogeneous distilla- tion model for coarse localization and then fine localiza- tion through a segmentation model. In addition, to better capture normal patterns, we introduce recovery features as guidance. Finally, we report the results of our CFRG frame- work and popular anomaly detection methods on the 3CAD dataset, demonstrating strong competitiveness and providing a highly challenging benchmark to promote the development of the anomaly detection field. Data and code are available: https://github.com/EnquanYang2022/3CAD.', 'score': 5, 'issue_id': 2215, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': 'f7f244334d53bca7', 'authors': ['Enquan Yang', 'Peng Xing', 'Hanyang Sun', 'Wenbo Guo', 'Yuanwei Ma', 'Zechao Li', 'Dan Zeng'], 'affiliations': ['Changzhou Microintelligence Corporation', 'Nanjing University of Science and Technology', 'School of Communication and Information Engineering, Shanghai University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05761.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset'], 'emoji': 'üîç', 'ru': {'title': '3CAD: –ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 3CAD, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Å —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ª–∏–Ω–∏–π 3C. –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç 27,039 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –ø–∏–∫—Å–µ–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –≤–æ—Å—å–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π. 3CAD –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º —Ä–∞–∑–º–µ—Ä–æ–≤ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π, –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –Ω–∞–ª–∏—á–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –¢–∞–∫–∂–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π CFRG, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—Ç –≥—Ä—É–±–æ–≥–æ –∫ —Ç–æ—á–Ω–æ–º—É —Å —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º –ø–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é.'}, 'en': {'title': '3CAD: A New Benchmark for Anomaly Detection in 3C Products', 'desc': 'This paper introduces a new dataset called 3CAD for industrial anomaly detection, specifically focusing on 3C product quality control. The 3CAD dataset contains 27,039 high-resolution images with pixel-level annotations for eight types of manufactured parts, addressing the limitations of existing datasets like MVTec-AD and VisA. To enhance anomaly detection, the authors propose a Coarse-to-Fine detection framework with Recovery Guidance (CFRG), which improves the localization of small defects by first identifying coarse regions and then refining the detection through segmentation. The results show that the CFRG framework is competitive with existing methods, providing a valuable resource for advancing research in anomaly detection.'}, 'zh': {'title': '3CADÔºöÊé®Âä®3C‰∫ßÂìÅÂºÇÂ∏∏Ê£ÄÊµãÁöÑÊñ∞Êï∞ÊçÆÈõÜ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂ§ßËßÑÊ®°ÂºÇÂ∏∏Ê£ÄÊµãÊï∞ÊçÆÈõÜÔºåÂêç‰∏∫3CADÔºå‰∏ìÊ≥®‰∫é3C‰∫ßÂìÅÁöÑË¥®ÈáèÊéßÂà∂„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´27,039Âº†È´òÂàÜËæ®ÁéáÂõæÂÉèÔºåÊ†áÊ≥®‰∫Ü‰∏çÂêåÁ±ªÂûãÂíåÂ§ßÂ∞èÁöÑÂÉèÁ¥†Á∫ßÂºÇÂ∏∏„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÂºÇÂ∏∏Ê£ÄÊµãÁöÑÂáÜÁ°ÆÊÄßÔºåËÆ∫ÊñáËøò‰ªãÁªç‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÊó†ÁõëÁù£ÂºÇÂ∏∏Ê£ÄÊµãÊ°ÜÊû∂ÔºåÁß∞‰∏∫CFRGÔºåÈááÁî®Á≤óÂà∞ÁªÜÁöÑÊ£ÄÊµãËåÉÂºè„ÄÇÈÄöËøáËØ•Ê°ÜÊû∂ÔºåÁ†îÁ©∂‰∫∫ÂëòÂèØ‰ª•Êõ¥Â•ΩÂú∞ÊçïÊçâÊ≠£Â∏∏Ê®°ÂºèÂπ∂ÂÆö‰ΩçÂ∞èÁº∫Èô∑ÔºåÊé®Âä®ÂºÇÂ∏∏Ê£ÄÊµãÈ¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.09613', 'title': 'Latent Radiance Fields with 3D-aware 2D Representations', 'url': 'https://huggingface.co/papers/2502.09613', 'abstract': 'Latent 3D reconstruction has shown great promise in empowering 3D semantic understanding and 3D generation by distilling 2D features into the 3D space. However, existing approaches struggle with the domain gap between 2D feature space and 3D representations, resulting in degraded rendering performance. To address this challenge, we propose a novel framework that integrates 3D awareness into the 2D latent space. The framework consists of three stages: (1) a correspondence-aware autoencoding method that enhances the 3D consistency of 2D latent representations, (2) a latent radiance field (LRF) that lifts these 3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field (VAE-RF) alignment strategy that improves image decoding from the rendered 2D representations. Extensive experiments demonstrate that our method outperforms the state-of-the-art latent 3D reconstruction approaches in terms of synthesis performance and cross-dataset generalizability across diverse indoor and outdoor scenes. To our knowledge, this is the first work showing the radiance field representations constructed from 2D latent representations can yield photorealistic 3D reconstruction performance.', 'score': 4, 'issue_id': 2231, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 13', 'zh': '2Êúà13Êó•'}, 'hash': 'edb945270af8ccb2', 'authors': ['Chaoyi Zhou', 'Xi Liu', 'Feng Luo', 'Siyu Huang'], 'affiliations': ['Visual Computing Division, School of Computing, Clemson University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09613.jpg', 'data': {'categories': ['#3d', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∏–π —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤ –¥–≤—É–º–µ—Ä–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö —ç—Ç–∞–ø–æ–≤: –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–∏–Ω–≥–∞ —Å —É—á–µ—Ç–æ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π, –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–ª—è –∏–∑–ª—É—á–µ–Ω–∏—è –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è VAE-RF. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É —Å–∏–Ω—Ç–µ–∑–∞ –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Bridging 2D and 3D: A New Era in Latent Reconstruction', 'desc': 'This paper presents a new framework for improving 3D reconstruction from 2D images by addressing the gap between 2D features and 3D representations. The proposed method enhances 3D consistency in 2D latent representations through a correspondence-aware autoencoder. It then transforms these enhanced representations into 3D using a latent radiance field, followed by an alignment strategy to optimize image decoding. The results show that this approach significantly outperforms existing methods in generating realistic 3D models from 2D data across various environments.'}, 'zh': {'title': 'Â∞Ü2DÊΩúÂú®Ë°®Á§∫ËΩ¨Âåñ‰∏∫ÁúüÂÆûÊÑü3DÈáçÂª∫ÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'ÊΩúÂú®3DÈáçÂª∫Âú®3DËØ≠‰πâÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢Â±ïÁé∞‰∫ÜÂ∑®Â§ßÊΩúÂäõÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®2DÁâπÂæÅÁ©∫Èó¥‰∏é3DË°®Á§∫‰πãÈó¥Â≠òÂú®È¢ÜÂüüÂ∑ÆË∑ùÔºåÂØºËá¥Ê∏≤ÊüìÊÄßËÉΩ‰∏ãÈôç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂ∞Ü3DÊÑèËØÜÊï¥ÂêàÂà∞2DÊΩúÂú®Á©∫Èó¥‰∏≠„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÊòØÂ¢ûÂº∫2DÊΩúÂú®Ë°®Á§∫ÁöÑ3D‰∏ÄËá¥ÊÄßÁöÑÂØπÂ∫îÊÑüÁü•Ëá™ÁºñÁ†ÅÊñπÊ≥ïÔºõÂÖ∂Ê¨°ÊòØÂ∞ÜËøô‰∫õ3DÊÑüÁü•ÁöÑ2DË°®Á§∫ÊèêÂçáÂà∞3DÁ©∫Èó¥ÁöÑÊΩúÂú®ËæêÂ∞ÑÂú∫ÔºàLRFÔºâÔºõÊúÄÂêéÊòØÊîπËøõ‰ªéÊ∏≤ÊüìÁöÑ2DË°®Á§∫‰∏≠Ëß£Á†ÅÂõæÂÉèÁöÑVAE-ËæêÂ∞ÑÂú∫ÔºàVAE-RFÔºâÂØπÈΩêÁ≠ñÁï•„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂêàÊàêÊÄßËÉΩÂíåË∑®Êï∞ÊçÆÈõÜÁöÑÊ≥õÂåñËÉΩÂäõÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÊΩúÂú®3DÈáçÂª∫ÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05979', 'title': 'VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer', 'url': 'https://huggingface.co/papers/2502.05979', 'abstract': 'Crafting magic and illusions is one of the most thrilling aspects of filmmaking, with visual effects (VFX) serving as the powerhouse behind unforgettable cinematic experiences. While recent advances in generative artificial intelligence have driven progress in generic image and video synthesis, the domain of controllable VFX generation remains relatively underexplored. In this work, we propose a novel paradigm for animated VFX generation as image animation, where dynamic effects are generated from user-friendly textual descriptions and static reference images.   Our work makes two primary contributions: (i) Open-VFX, the first high-quality VFX video dataset spanning 15 diverse effect categories, annotated with textual descriptions, instance segmentation masks for spatial conditioning, and start-end timestamps for temporal control. (ii) VFX Creator, a simple yet effective controllable VFX generation framework based on a Video Diffusion Transformer. The model incorporates a spatial and temporal controllable LoRA adapter, requiring minimal training videos. Specifically, a plug-and-play mask control module enables instance-level spatial manipulation, while tokenized start-end motion timestamps embedded in the diffusion process, alongside the text encoder, allow precise temporal control over effect timing and pace.   Extensive experiments on the Open-VFX test set demonstrate the superiority of the proposed system in generating realistic and dynamic effects, achieving state-of-the-art performance and generalization ability in both spatial and temporal controllability. Furthermore, we introduce a specialized metric to evaluate the precision of temporal control. By bridging traditional VFX techniques with generative approaches, VFX Creator unlocks new possibilities for efficient and high-quality video effect generation, making advanced VFX accessible to a broader audience.', 'score': 4, 'issue_id': 2220, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'}, 'hash': 'c31da576f1519381', 'authors': ['Xinyu Liu', 'Ailing Zeng', 'Wei Xue', 'Harry Yang', 'Wenhan Luo', 'Qifeng Liu', 'Yike Guo'], 'affiliations': ['Hong Kong University of Science and Technology, China', 'Tencent AI Lab, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.05979.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#benchmark', '#video', '#open_source', '#cv'], 'emoji': 'üé¨', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ VFX: –ò–ò –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –∫–∏–Ω–æ–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ (VFX) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Open-VFX - –ø–µ—Ä–≤—ã–π –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö VFX-–≤–∏–¥–µ–æ, –∏ VFX Creator - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ VFX –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –≤–∏–¥–µ–æ–¥–∏—Ñ—Ñ—É–∑–∏–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π LoRA-–∞–¥–∞–ø—Ç–µ—Ä –∏ –º–æ–¥—É–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–∞—Å–∫–∞–º–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —ç—Ñ—Ñ–µ–∫—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∏ –¥–∏–Ω–∞–º–∏—á–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é –∫–æ–Ω—Ç—Ä–æ–ª—è.'}, 'en': {'title': 'Revolutionizing VFX Generation with AI: Control at Your Fingertips!', 'desc': 'This paper presents a new method for generating visual effects (VFX) in films using artificial intelligence. It introduces Open-VFX, a comprehensive dataset that includes various effect categories, textual descriptions, and detailed annotations for better control over the generated effects. The authors also propose VFX Creator, a framework that utilizes a Video Diffusion Transformer to create dynamic VFX from simple text prompts and static images, allowing for both spatial and temporal adjustments. The results show that this approach outperforms existing methods, making high-quality VFX generation more accessible and efficient.'}, 'zh': {'title': 'ÂºÄÂêØÂèØÊéßËßÜËßâÁâπÊïàÁîüÊàêÁöÑÊñ∞Á∫™ÂÖÉ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂä®ÁîªËßÜËßâÁâπÊïàÁîüÊàêËåÉÂºèÔºåÁß∞‰∏∫ÂõæÂÉèÂä®ÁîªÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøáÊñáÊú¨ÊèèËø∞ÂíåÈùôÊÄÅÂèÇËÄÉÂõæÂÉèÁîüÊàêÂä®ÊÄÅÁâπÊïà„ÄÇÊàë‰ª¨ÂàõÂª∫‰∫ÜOpen-VFXÊï∞ÊçÆÈõÜÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Ê∂µÁõñ15Áßç‰∏çÂêåÁâπÊïàÁ±ªÂà´ÁöÑÈ´òË¥®ÈáèVFXËßÜÈ¢ëÊï∞ÊçÆÈõÜÔºåÂåÖÂê´ÊñáÊú¨ÊèèËø∞ÂíåÂÆû‰æãÂàÜÂâ≤Êé©Á†Å„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫ÜVFX CreatorÔºå‰∏Ä‰∏™Âü∫‰∫éËßÜÈ¢ëÊâ©Êï£ÂèòÊç¢Âô®ÁöÑÂèØÊéßVFXÁîüÊàêÊ°ÜÊû∂ÔºåËÉΩÂ§üÂÆûÁé∞Á©∫Èó¥ÂíåÊó∂Èó¥ÁöÑÁ≤æÁ°ÆÊéßÂà∂„ÄÇÈÄöËøáÂ∞Ü‰º†ÁªüVFXÊäÄÊúØ‰∏éÁîüÊàêÊñπÊ≥ïÁªìÂêàÔºåVFX Creator‰∏∫È´òÊïàÂíåÈ´òË¥®ÈáèÁöÑËßÜÈ¢ëÁâπÊïàÁîüÊàêÂºÄËæü‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08127', 'title': 'Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance', 'url': 'https://huggingface.co/papers/2502.08127', 'abstract': 'Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.', 'score': 37, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': 'fa3f08993ba529cd', 'authors': ['Lingfei Qian', 'Weipeng Zhou', 'Yan Wang', 'Xueqing Peng', 'Jimin Huang', 'Qianqian Xie'], 'affiliations': ['TheFinAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.08127.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#rl', '#open_source', '#training', '#long_context'], 'emoji': 'üíπ', 'ru': {'title': '–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - –∫–ª—é—á –∫ —É—Å–ø–µ—Ö—É –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º –∞–Ω–∞–ª–∏–∑–µ', 'desc': '–í —ç—Ç–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å 16 –º–æ—â–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–≤—ã—à–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –û–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Llama-3.1-8B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤–∑–æ—à–ª–∞ –¥–∞–∂–µ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∫ —Å–ø–µ—Ü–∏—Ñ–∏–∫–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'Enhancing Financial Reasoning in LLMs with Domain-Specific Adaptations', 'desc': 'This paper investigates the performance of large language models (LLMs) in financial reasoning tasks, which include interpreting financial text, analyzing tabular data, and solving equations. The authors evaluate 16 advanced LLMs and find that while improved datasets and pretraining enhance financial reasoning, general techniques like Chain-of-Thought (CoT) fine-tuning do not consistently improve results. They propose a new model, enhanced with CoT fine-tuning and reinforcement learning, which shows a significant performance boost of 10% across various financial tasks. The study emphasizes the importance of domain-specific adaptations for better performance in financial reasoning and introduces a leaderboard for future benchmarking.'}, 'zh': {'title': 'ÈáëËûçÊé®ÁêÜÊ®°ÂûãÁöÑÂàõÊñ∞‰∏éÊèêÂçá', 'desc': 'Êú¨Á†îÁ©∂ËØÑ‰º∞‰∫Ü16ÁßçÂº∫Â§ßÁöÑËØ≠Ë®ÄÊ®°ÂûãÂú®ÈáëËûçÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇËøô‰∫õ‰ªªÂä°ÂåÖÊã¨ÈáëËûçÊñáÊú¨„ÄÅË°®Ê†ºÊï∞ÊçÆÂíåÊñπÁ®ãÂºèÔºåÊ∂âÂèäÊï∞ÂÄºÊé®ÁêÜ„ÄÅË°®Ê†ºËß£ËØªÂíåÈáëËûçÊúØËØ≠ÁêÜËß£Á≠âÊñπÈù¢„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°Êõ¥Â•ΩÁöÑÊï∞ÊçÆÈõÜÂíåÈ¢ÑËÆ≠ÁªÉÂèØ‰ª•ÊèêÂçáÈáëËûçÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÈÄöÁî®ÁöÑÂ¢ûÂº∫ÊñπÊ≥ïÂ¶ÇÈìæÂºèÊé®ÁêÜÂæÆË∞ÉÂπ∂‰∏çÊÄªÊòØÊúâÊïà„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÂü∫‰∫éLlama-3.1-8B-InstructÁöÑÈáëËûçÊé®ÁêÜÂ¢ûÂº∫Ê®°ÂûãÔºåÁªèËøáÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†ÂêéÔºåÂú®Â§ö‰∏™‰ªªÂä°‰∏äÂÆûÁé∞‰∫Ü10%ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07870', 'title': 'TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation', 'url': 'https://huggingface.co/papers/2502.07870', 'abstract': 'Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models.', 'score': 32, 'issue_id': 2188, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '5b552b320e2e69f0', 'authors': ['Alex Jinpeng Wang', 'Dongxing Mao', 'Jiawei Zhang', 'Weiming Han', 'Zhuobai Dong', 'Linjie Li', 'Yiqi Lin', 'Zhengyuan Yang', 'Libo Qin', 'Fuwei Zhang', 'Lijuan Wang', 'Min Li'], 'affiliations': ['Central South University', 'Microsoft', 'National University of Singapore', 'North University of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07870.jpg', 'data': {'categories': ['#dataset', '#open_source', '#long_context', '#benchmark', '#cv'], 'emoji': 'üìö', 'ru': {'title': 'TextAtlas5M: –ù–æ–≤—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç TextAtlas5M –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 5 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–ª–∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä TextAtlasEval –∏–∑ 3000 —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –¥–∞–∂–µ —Å–∞–º—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, GPT4o —Å DallE-3) —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —ç—Ç–∏–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Empowering Long-Text Image Generation with TextAtlas5M', 'desc': 'This paper introduces TextAtlas5M, a new dataset aimed at improving text-conditioned image generation, particularly for long-form text. Current models struggle with generating images that include dense and intricate text, as existing datasets primarily focus on shorter text. TextAtlas5M contains 5 million images with long text, allowing for better evaluation of generative models. The paper also presents TextAtlasEval, a curated test set that highlights the challenges faced by both proprietary and open-source models in this area.'}, 'zh': {'title': 'ÈïøÊñáÊú¨ÂõæÂÉèÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊï∞ÊçÆÈõÜTextAtlas5MÔºåÊó®Âú®Ëß£ÂÜ≥ÊñáÊú¨Êù°‰ª∂‰∏ãÂõæÂÉèÁîüÊàê‰∏≠ÈïøÊñáÊú¨Ê∏≤ÊüìÁöÑÊåëÊàò„ÄÇÁé∞ÊúâÁöÑÊï∞ÊçÆÈõÜÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®Áü≠ÊñáÊú¨ÔºåÈôêÂà∂‰∫ÜÁîüÊàêÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇTextAtlas5MÂåÖÂê´500‰∏áÂº†ÈïøÊñáÊú¨ÁîüÊàêÁöÑÂõæÂÉèÔºåË¶ÜÁõñÂ§öÁßçÊï∞ÊçÆÁ±ªÂûãÔºå‰∏∫Â§ßËßÑÊ®°ÁîüÊàêÊ®°ÂûãÁöÑËØÑ‰º∞Êèê‰æõ‰∫ÜÂü∫Á°Ä„ÄÇÈÄöËøáÂª∫Á´ã3000‰∏™ÁªèËøá‰∫∫Â∑•ÊîπËøõÁöÑÊµãËØïÈõÜTextAtlasEvalÔºåÊú¨Êñá‰∏∫ÊñáÊú¨Êù°‰ª∂ÁîüÊàêÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂπøÊ≥õÁöÑÂü∫ÂáÜÔºåÂ∏ÆÂä©Êú™Êù•ÁöÑÁ†îÁ©∂ÂíåÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08590', 'title': 'Light-A-Video: Training-free Video Relighting via Progressive Light Fusion', 'url': 'https://huggingface.co/papers/2502.08590', 'abstract': "Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source video's appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi.github.io/light-a-video.github.io/.", 'score': 32, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': 'dcc03282320e88b1', 'authors': ['Yujie Zhou', 'Jiazi Bu', 'Pengyang Ling', 'Pan Zhang', 'Tong Wu', 'Qidong Huang', 'Jinsong Li', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Anyi Rao', 'Jiaqi Wang', 'Li Niu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.08590.jpg', 'data': {'categories': ['#video', '#diffusion', '#cv'], 'emoji': 'üí°', 'ru': {'title': '–ü–ª–∞–≤–Ω–æ–µ –ø–µ—Ä–µ–æ—Å–≤–µ—â–µ–Ω–∏–µ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Light-A-Video - –ø–æ–¥—Ö–æ–¥ –∫ –ø–µ—Ä–µ–æ—Å–≤–µ—â–µ–Ω–∏—é –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç–æ–¥–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Å–≤–µ—â–µ–Ω–∏—è: –º–æ–¥—É–ª—å Consistent Light Attention (CLA) –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–æ–Ω–æ–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å–≤–µ—Ç–∞ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Progressive Light Fusion (PLF) –¥–ª—è –ø–ª–∞–≤–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –æ—Å–≤–µ—â–µ–Ω–∏—è –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏. Light-A-Video –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–æ—Å–≤–µ—â–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤–∏–¥–µ–æ, —Ä–µ—à–∞—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å–≤–µ—Ç–∞ –∏ –º–µ—Ä—Ü–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–æ—Å–≤–µ—â–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.'}, 'en': {'title': 'Achieving Smooth Video Relighting with Light-A-Video', 'desc': 'This paper presents Light-A-Video, a novel approach for video relighting that addresses the challenges of lighting consistency and flickering in generated videos. Unlike traditional methods that apply image relighting on a frame-by-frame basis, Light-A-Video utilizes a training-free strategy to enhance temporal smoothness. It introduces a Consistent Light Attention (CLA) module to improve interactions between frames and stabilize background lighting. Additionally, the Progressive Light Fusion (PLF) technique is employed to blend the original and relighted appearances, ensuring coherent lighting transitions across video frames.'}, 'zh': {'title': 'ÂÆûÁé∞ËßÜÈ¢ëÈáçÂÖâÁöÑ‰∏ÄËá¥ÊÄß‰∏éÂπ≥ÊªëÊÄß', 'desc': 'ÊúÄËøëÔºåÂõæÂÉèÈáçÂÖâÊ®°ÂûãÁöÑËøõÂ±ïÂæóÁõä‰∫éÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÂíåÈ¢ÑËÆ≠ÁªÉÁöÑÊâ©Êï£Ê®°ÂûãÔºå‰ΩøÂæó‰∏ÄËá¥ÁöÑÂÖâÁÖßÊïàÊûúÂæó‰ª•ÂÆûÁé∞„ÄÇÁÑ∂ËÄåÔºåËßÜÈ¢ëÈáçÂÖâ‰ªçÁÑ∂ÊªûÂêéÔºå‰∏ªË¶ÅÊòØÁî±‰∫éËÆ≠ÁªÉÊàêÊú¨È´òÂíåÁº∫‰πèÂ§öÊ†∑ÂåñÁöÑÈ´òË¥®ÈáèËßÜÈ¢ëÈáçÂÖâÊï∞ÊçÆÈõÜ„ÄÇÁÆÄÂçïÂú∞Â∞ÜÂõæÂÉèÈáçÂÖâÊ®°ÂûãÈÄêÂ∏ßÂ∫îÁî®‰ºöÂØºËá¥ÂÖâÊ∫ê‰∏ç‰∏ÄËá¥ÂíåÈáçÂÖâÂ§ñËßÇ‰∏ç‰∏ÄËá¥Ôºå‰ªéËÄåÂú®ÁîüÊàêÁöÑËßÜÈ¢ë‰∏≠‰∫ßÁîüÈó™ÁÉÅÁé∞Ë±°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜLight-A-VideoÔºåËøôÊòØ‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÂÆûÁé∞Êó∂Èó¥‰∏äÂπ≥ÊªëÁöÑËßÜÈ¢ëÈáçÂÖâ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07346', 'title': 'BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models', 'url': 'https://huggingface.co/papers/2502.07346', 'abstract': 'Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.', 'score': 31, 'issue_id': 2192, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '8a6e122fc0804618', 'authors': ['Xu Huang', 'Wenhao Zhu', 'Hanxu Hu', 'Conghui He', 'Lei Li', 'Shujian Huang', 'Fei Yuan'], 'affiliations': ['Carnegie Mellon University', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2502.07346.jpg', 'data': {'categories': ['#long_context', '#low_resource', '#machine_translation', '#multilingual', '#dataset', '#open_source', '#benchmark'], 'emoji': 'üåê', 'ru': {'title': 'BenchMAX: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'BenchMAX - —ç—Ç–æ –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ 17 —è–∑—ã–∫–∞—Ö. –û–Ω —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∫–∞–∫ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞. –ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –∞–Ω–Ω–æ—Ç–∏—Ä—É–µ—Ç—Å—è —Ç—Ä–µ–º—è –Ω–æ—Å–∏—Ç–µ–ª—è–º–∏ —è–∑—ã–∫–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM –≤ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, —á—Ç–æ –Ω–µ–ª—å–∑—è –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –ø—Ä–æ—Å—Ç—ã–º —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'BenchMAX: Bridging Multilingual Gaps in Advanced Language Model Evaluation', 'desc': 'This paper introduces BenchMAX, a new multilingual evaluation benchmark designed to assess advanced capabilities of large language models (LLMs) such as instruction following, reasoning, and code generation across multiple languages. Unlike previous benchmarks that focused on simpler tasks, BenchMAX allows for a fair comparison of these complex abilities by using machine-translated data and independent annotations from native speakers. The study reveals significant performance gaps in LLMs when evaluated on these advanced tasks, indicating that merely increasing model size is not sufficient to improve multilingual proficiency. BenchMAX aims to enhance the development of multilingual models by providing a robust platform for evaluation and research.'}, 'zh': {'title': 'BenchMAXÔºöÂ§öËØ≠Ë®ÄËÉΩÂäõËØÑ‰º∞ÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜBenchMAXÔºåËøôÊòØ‰∏Ä‰∏™Â§öËØ≠Ë®ÄËØÑ‰º∞Âü∫ÂáÜÔºåÊó®Âú®ÊØîËæÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êåá‰ª§ÈÅµÂæ™„ÄÅÊé®ÁêÜ„ÄÅÈïøÊñáÊú¨ÁêÜËß£Âíå‰ª£Á†ÅÁîüÊàêÁ≠âÈ´òÁ∫ßËÉΩÂäõ‰∏äÁöÑË°®Áé∞„ÄÇ‰ª•ÂæÄÁöÑÂ§öËØ≠Ë®ÄÂü∫ÂáÜ‰∏ªË¶ÅÂÖ≥Ê≥®ÁÆÄÂçïÁêÜËß£‰ªªÂä°ÔºåËÄåBenchMAXÂàôÂ°´Ë°•‰∫ÜËøô‰∏ÄÁ©∫ÁôΩÔºåÂÖÅËÆ∏ÂØπ‰∏çÂêåËØ≠Ë®ÄÁöÑËÉΩÂäõËøõË°åÂÖ¨Âπ≥ÊØîËæÉ„ÄÇ‰∏∫‰∫ÜÁ°Æ‰øùÊï∞ÊçÆË¥®ÈáèÔºå‰∏â‰ΩçÊØçËØ≠ËØÑÂÆ°ÂëòÁã¨Á´ãÂØπÊØè‰∏™Ê†∑Êú¨ËøõË°åÊ†áÊ≥®ÔºåÁ°Æ‰øùËØÑ‰º∞ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰∏çÂêåËØ≠Ë®ÄÂú®Ê†∏ÂøÉËÉΩÂäõ‰∏äÁöÑË°®Áé∞Â∑ÆÂºÇÔºåË°®Êòé‰ªÖ‰ªÖÂ¢ûÂä†Ê®°ÂûãËßÑÊ®°Êó†Ê≥ïËß£ÂÜ≥Ëøô‰∫õÊÄßËÉΩÂ∑ÆË∑ù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08639', 'title': 'CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.08639', 'abstract': 'In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.', 'score': 28, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '3d3890b6b6bf7904', 'authors': ['Qinghe Wang', 'Yawen Luo', 'Xiaoyu Shi', 'Xu Jia', 'Huchuan Lu', 'Tianfan Xue', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai'], 'affiliations': ['Dalian University of Technology', 'Kuaishou Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.08639.jpg', 'data': {'categories': ['#dataset', '#video', '#diffusion', '#3d', '#games'], 'emoji': 'üé¨', 'ru': {'title': 'CineMaster: –†–µ–∂–∏—Å—Å–∏—Ä—É–π—Ç–µ —Å–≤–æ–µ –≤–∏–¥–µ–æ –≤ 3D', 'desc': 'CineMaster - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è 3D-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Ç–æ—á–Ω–æ —Ä–∞–∑–º–µ—â–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –≤ —Å—Ü–µ–Ω–µ, –≥–∏–±–∫–æ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏ –∫–∞–º–µ—Ä–æ–π –≤ 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ —Å–æ–∑–¥–∞–µ—Ç 3D-—Å–∏–≥–Ω–∞–ª—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –∑–∞—Ç–µ–º —ç—Ç–∏ —Å–∏–≥–Ω–∞–ª—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º 3D-–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫ –∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∫–∞–º–µ—Ä—ã.'}, 'en': {'title': 'Empowering Video Creation with 3D Control', 'desc': 'CineMaster is a new framework designed for generating videos from text while allowing users to control 3D elements like a film director. It operates in two stages: first, users create 3D-aware signals by placing objects and defining camera movements, and second, these signals guide a text-to-video diffusion model to produce the desired video. The framework also includes an automated data annotation system to gather necessary 3D motion and camera data from existing videos. Experiments show that CineMaster outperforms current methods in generating 3D-aware videos from text descriptions.'}, 'zh': {'title': 'CineMasterÔºöËÆ©ËßÜÈ¢ëÁîüÊàêÂ¶ÇÂØºÊºîËà¨ÂèØÊéß', 'desc': 'CineMasterÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÁîüÊàêÂÖ∑Êúâ3DÊÑüÁü•ÂíåÂèØÊéßÊÄßÁöÑÊñáÊú¨Âà∞ËßÜÈ¢ë„ÄÇÂÆÉ‰ΩøÁî®Êà∑ËÉΩÂ§üÂÉè‰∏ì‰∏öÁîµÂΩ±ÂØºÊºî‰∏ÄÊ†∑Á≤æÁ°ÆÊéßÂà∂Âú∫ÊôØ‰∏≠ÁöÑÁâ©‰Ωì‰ΩçÁΩÆ„ÄÅÁÅµÊ¥ªÊìç‰Ωú3DÁ©∫Èó¥‰∏≠ÁöÑÁâ©‰ΩìÂíåÁõ∏Êú∫ÔºåÂπ∂Áõ¥ËßÇÂú∞Â∏ÉÂ±ÄÊ∏≤ÊüìÂ∏ß„ÄÇËØ•Ê°ÜÊû∂ÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÈÄöËøá‰∫§‰∫íÂºèÂ∑•‰ΩúÊµÅÁ®ãÊûÑÂª∫3DÊÑüÁü•ÁöÑÊù°‰ª∂‰ø°Âè∑ÔºåÁ¨¨‰∫åÈò∂ÊÆµÂà©Áî®Ëøô‰∫õ‰ø°Âè∑ÊåáÂØºÊñáÊú¨Âà∞ËßÜÈ¢ëÁöÑÊâ©Êï£Ê®°ÂûãÁîüÊàêÁî®Êà∑ÊâÄÈúÄÁöÑËßÜÈ¢ëÂÜÖÂÆπ„ÄÇÊ≠§Â§ñÔºåCineMasterËøòÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Ëá™Âä®ÂåñÊï∞ÊçÆÊ≥®ÈáäÁÆ°ÈÅìÔºå‰ª•Ëß£ÂÜ≥Áº∫‰πè3DÁâ©‰ΩìËøêÂä®ÂíåÁõ∏Êú∫ÂßøÊÄÅÊ†áÊ≥®ÁöÑÊï∞ÊçÆÈõÜÈóÆÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07864', 'title': 'TransMLA: Multi-head Latent Attention Is All You Need', 'url': 'https://huggingface.co/papers/2502.07864', 'abstract': 'Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.', 'score': 21, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': 'dbff84dafe8c2312', 'authors': ['Fanxu Meng', 'Zengwei Yao', 'Muhan Zhang'], 'affiliations': ['Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, Peking University', 'Xiaomi Corp., Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07864.jpg', 'data': {'categories': ['#optimization', '#architecture', '#inference', '#training', '#long_context'], 'emoji': 'üöÄ', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –≤–Ω–∏–º–∞–Ω–∏—è: MLA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Multi-head Latent Attention (MLA), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö —É–∑–∫–∏—Ö –º–µ—Å—Ç –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. MLA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–Ω–≥–∞ –≤ —Å–ª–æ—è—Ö –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∂–∏–º–∞—Ç—å –∏ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è KV. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ TransMLA –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Group Query Attention (GQA) –≤ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ MLA. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä KV-–∫—ç—à–∞ –∏ —É—Å–∫–æ—Ä–∏—Ç—å –≤—ã–≤–æ–¥, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Transforming Attention: From GQA to Efficient MLA', 'desc': 'This paper introduces Multi-head Latent Attention (MLA) as a solution to communication bottlenecks in large language models (LLMs) caused by hardware limitations. MLA utilizes low-rank matrices in the key-value (KV) layers to compress KV states, significantly reducing cache size and improving inference speed. The authors demonstrate that Group Query Attention (GQA) can be represented by MLA without increasing KV cache overhead, while MLA offers greater flexibility. To facilitate the adoption of MLA, they propose TransMLA, a method for converting GQA-based models into MLA-based ones, allowing for enhanced expressiveness without additional cache size.'}, 'zh': {'title': 'ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊïàÁéáÁöÑÂÖ≥ÈîÆÔºöÂ§öÂ§¥ÊΩúÂú®Ê≥®ÊÑèÂäõ', 'desc': 'Áé∞‰ª£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂΩìÂâçÁ°¨‰ª∂‰∏äÂ∏∏Â∏∏Èù¢‰∏¥ÈÄö‰ø°Áì∂È¢àÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØËÆ°ÁÆóÈôêÂà∂„ÄÇÂ§öÂ§¥ÊΩúÂú®Ê≥®ÊÑèÂäõÔºàMLAÔºâÈÄöËøáÂú®ÈîÆÂÄºÔºàKVÔºâÂ±Ç‰∏≠‰ΩøÁî®‰ΩéÁß©Áü©ÈòµÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºå‰ªéËÄåÂÖÅËÆ∏ÂéãÁº©ÁöÑÊΩúÂú®KVÁä∂ÊÄÅË¢´ÁºìÂ≠ò„ÄÇËøôÁßçÊñπÊ≥ïÊòæËëóÂáèÂ∞ë‰∫ÜKVÁºìÂ≠òÁöÑÂ§ßÂ∞èÔºåÁõ∏ÊØî‰º†ÁªüÁöÑÂ§öÂ§¥Ê≥®ÊÑèÂäõÔºåÊé®ÁêÜÈÄüÂ∫¶Êõ¥Âø´„ÄÇÊ≠§Â§ñÔºåMLA‰ΩøÁî®‰∏äÊäïÂΩ±Áü©ÈòµÊù•Â¢ûÂä†Ë°®ËææËÉΩÂäõÔºå‰ª•È¢ùÂ§ñÁöÑËÆ°ÁÆóÊç¢ÂèñÂáèÂ∞ëÁöÑÈÄö‰ø°ÂºÄÈîÄ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08047', 'title': 'WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation', 'url': 'https://huggingface.co/papers/2502.08047', 'abstract': 'Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation.', 'score': 20, 'issue_id': 2190, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '0f83dccb05181f21', 'authors': ['Henry Hengyuan Zhao', 'Difei Gao', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.08047.jpg', 'data': {'categories': ['#agents', '#benchmark'], 'emoji': 'üñ•Ô∏è', 'ru': {'title': '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ GUI', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WorldGUI - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ GUI-Thinker, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–µ—Ö–∞–Ω–∏–∑–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å—é –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å GUI. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ GUI-Thinker –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Claude-3.5 (Computer Use) –ø–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—é —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ WorldGUI. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏, –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ GUI.'}, 'en': {'title': 'Enhancing GUI Automation with WorldGUI and GUI-Thinker', 'desc': 'This paper addresses the challenges faced by current GUI agents in planning tasks due to their sensitivity to the initial state of the environment. It introduces WorldGUI, a new benchmark that simulates real user interactions by incorporating various initial states across multiple software applications. To tackle the complexities of dynamic GUI automation, the authors propose GUI-Thinker, a framework that utilizes a critique mechanism to improve decision-making in unpredictable scenarios. Experimental results show that GUI-Thinker outperforms existing models, highlighting its effectiveness in enhancing the success rate of GUI automation tasks.'}, 'zh': {'title': 'ÊèêÂçáGUIËá™Âä®ÂåñÁöÑÂÖ≥ÈîÆÊÄùÁª¥Ê°ÜÊû∂', 'desc': 'ÂΩìÂâçÁöÑÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâ‰ª£ÁêÜÂú®ÂÖÉÁ¥†ÂÆö‰ΩçÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ËßÑÂàíÊñπÈù¢‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂØπÁéØÂ¢ÉÂàùÂßãÁä∂ÊÄÅÁöÑÊïèÊÑüÊÄß„ÄÇÂàùÂßãÁä∂ÊÄÅÁöÑÂæÆÂ∞èÂ∑ÆÂºÇÔºå‰æãÂ¶ÇÁõÆÊ†áËΩØ‰ª∂Êú™ÊâìÂºÄÊàñÁïåÈù¢‰∏çÂú®ÈªòËÆ§Áä∂ÊÄÅÔºåÂ∏∏Â∏∏ÂØºËá¥ËßÑÂàíÈîôËØØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜWorldGUIÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑGUIÂü∫ÂáÜÔºåËÆæËÆ°‰∫ÜÂÖ∑ÊúâÂ§öÁßçÂàùÂßãÁä∂ÊÄÅÁöÑGUI‰ªªÂä°Ôºå‰ª•Ê®°ÊãüÁúüÂÆûÁöÑËÆ°ÁÆóÊú∫Áî®Êà∑‰∫§‰∫í„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜGUI-ThinkerÔºå‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÊâπÂà§Êú∫Âà∂ÊúâÊïàÁÆ°ÁêÜGUI‰∫§‰∫íÁöÑ‰∏çÂèØÈ¢ÑÊµãÊÄßÂíåÂ§çÊùÇÊÄßÔºåÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÂÖ∂Âú®WorldGUI‰ªªÂä°‰∏äÁöÑÊàêÂäüÁéáÊØîClaude-3.5È´òÂá∫14.9%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07563', 'title': 'LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid', 'url': 'https://huggingface.co/papers/2502.07563', 'abstract': 'Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 19, 'issue_id': 2188, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': 'f5a4cfd0a0d018ae', 'authors': ['Weigao Sun', 'Disen Lan', 'Yiran Zhong', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.07563.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –Ω–∞ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π LASP-2 –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –Ω–∞ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö. LASP-2 –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, —Ç—Ä–µ–±—É—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –æ–ø–µ—Ä–∞—Ü–∏—é AllGather –¥–ª—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–∞–º—è—Ç–∏, —Ä–∞–∑–º–µ—Ä –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ LASP-2H –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–æ—á–µ—Ç–∞—é—â–∏—Ö –ª–∏–Ω–µ–π–Ω–æ–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª–∏ Linear-Llama3 –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LASP-2 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –ø—Ä–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'Boosting Efficiency in Linear Attention with LASP-2', 'desc': 'This paper presents LASP-2, a new sequence parallelism (SP) method designed to improve the efficiency of training linear attention transformer models with very long input sequences. LASP-2 optimizes communication and computation parallelism by minimizing the communication requirements for linear attention layers, allowing for a single AllGather operation on intermediate memory states. This approach leads to significant enhancements in both communication and computation parallelism, enabling better scalability in distributed systems. Additionally, LASP-2 is extended to LASP-2H, which applies similar optimizations to standard attention modules, providing an efficient solution for hybrid models that utilize both linear and standard attention.'}, 'zh': {'title': 'LASP-2ÔºöÊèêÂçáÁ∫øÊÄßÊ≥®ÊÑèÂäõÊ®°ÂûãÁöÑÂπ∂Ë°åÊÄß', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ∫èÂàóÂπ∂Ë°åÊñπÊ≥ïLASP-2ÔºåÊó®Âú®ÊèêÈ´òÁ∫øÊÄßÊ≥®ÊÑèÂäõÂèòÊç¢Âô®Ê®°ÂûãÂú®Â§ÑÁêÜÈùûÂ∏∏ÈïøËæìÂÖ•Â∫èÂàóÊó∂ÁöÑÈÄö‰ø°ÂíåËÆ°ÁÆóÂπ∂Ë°åÊÄß„ÄÇ‰∏é‰πãÂâçÁöÑLASPÊñπÊ≥ïÁõ∏ÊØîÔºåLASP-2ÈáçÊñ∞ÊÄùËÄÉ‰∫ÜÁ∫øÊÄßÊ≥®ÊÑèÂäõÂ±ÇÁöÑÊúÄÂ∞èÈÄö‰ø°ÈúÄÊ±ÇÔºåÂπ∂ÈáçÊñ∞ÁªÑÁªá‰∫ÜÈÄö‰ø°ÂíåËÆ°ÁÆóÁöÑÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåLASP-2Âè™ÈúÄÂú®‰∏≠Èó¥ÂÜÖÂ≠òÁä∂ÊÄÅ‰∏äËøõË°å‰∏ÄÊ¨°AllGatherÈõÜ‰ΩìÈÄö‰ø°ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈÄö‰ø°ÂíåËÆ°ÁÆóÁöÑÂπ∂Ë°åÊÄßÂèäÂÖ∂ÈáçÂè†„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞Ë°®ÊòéÔºåLASP-2Âú®ËÆ≠ÁªÉÈÄüÂ∫¶‰∏äÊØîLASPÊèêÈ´ò‰∫Ü15.2%ÔºåÊØîÁéØÂΩ¢Ê≥®ÊÑèÂäõÊèêÈ´ò‰∫Ü36.6%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08606', 'title': 'Distillation Scaling Laws', 'url': 'https://huggingface.co/papers/2502.08606', 'abstract': 'We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.', 'score': 16, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '774eeded4c92c597', 'authors': ['Dan Busbridge', 'Amitis Shidani', 'Floris Weers', 'Jason Ramapuram', 'Etai Littwin', 'Russ Webb'], 'affiliations': ['Apple', 'University of Oxford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.08606.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': 'üî¨', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞ –∏ –µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ–∂–¥—É —Å—Ç—É–¥–µ–Ω—Ç–æ–º –∏ —É—á–∏—Ç–µ–ª–µ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ä–µ—Ü–µ–ø—Ç—ã –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, –≤–∫–ª—é—á–∞—è —Å–ª—É—á–∞–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —É—á–∏—Ç–µ–ª–µ–º –∏–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é –µ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—ã—á–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –¥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ —Ä–∞—Å—Ç–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–æ–º –º–æ–¥–µ–ª–∏-—Å—Ç—É–¥–µ–Ω—Ç–∞. –†–∞–±–æ—Ç–∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ü–µ–Ω–Ω—ã–µ insights –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, —É–ª—É—á—à–∞—é—â–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –∏ –∏–Ω—Ñ–æ—Ä–º–∏—Ä—É—é—â–∏–µ –¥–∏–∑–∞–π–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Maximizing Student Performance through Optimal Distillation Strategies', 'desc': 'This paper introduces a distillation scaling law that helps predict how well a distilled model will perform based on the available computing resources and how those resources are divided between the teacher and student models. The authors demonstrate that by optimizing the compute allocation, the performance of the student model can be significantly improved, especially when multiple students are distilled from a pre-trained teacher. They also provide guidelines for when to use distillation versus supervised learning, depending on whether a teacher model is available and whether it requires training. Overall, the study enhances our understanding of model distillation and offers practical strategies for effective implementation in machine learning tasks.'}, 'zh': {'title': '‰ºòÂåñËí∏È¶èÊ®°ÂûãÊÄßËÉΩÁöÑËÆ°ÁÆóÊ≥ïÂàô', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËí∏È¶èÁº©ÊîæÊ≥ïÂàôÔºåÁî®‰∫éÊ†πÊçÆËÆ°ÁÆóÈ¢ÑÁÆóÂíåÂú®Â≠¶Áîü‰∏éÊïôÂ∏à‰πãÈó¥ÁöÑÂàÜÈÖçÊù•‰º∞ËÆ°Ëí∏È¶èÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂ÁªìÊûúÈôç‰Ωé‰∫ÜÂ§ßËßÑÊ®°‰ΩøÁî®Ëí∏È¶èÁöÑÈ£éÈô©ÔºåËÉΩÂ§ü‰ºòÂåñÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÁöÑËÆ°ÁÆóÂàÜÈÖçÔºå‰ª•ÊúÄÂ§ßÂåñÂ≠¶ÁîüÁöÑË°®Áé∞„ÄÇÊàë‰ª¨Êèê‰æõ‰∫ÜËÆ°ÁÆóÊúÄ‰ºòÁöÑËí∏È¶èÊñπÊ°àÔºåÈÄÇÁî®‰∫éÂ∑≤ÊúâÊïôÂ∏àÊàñÈúÄË¶ÅËÆ≠ÁªÉÊïôÂ∏àÁöÑÊÉÖÂÜµ„ÄÇÈÄöËøáÂ§ßËßÑÊ®°Á†îÁ©∂ÔºåÊàë‰ª¨Â¢ûÂä†‰∫ÜÂØπËí∏È¶èÁöÑÁêÜËß£ÔºåÂπ∂‰∏∫ÂÆûÈ™åËÆæËÆ°Êèê‰æõ‰∫ÜÊåáÂØº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06533', 'title': 'Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.06533', 'abstract': 'The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of "critical tokens" which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage.', 'score': 10, 'issue_id': 2192, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '178e4717b984d64d', 'authors': ['Jean Vassoyan', 'Nathana√´l Beau', 'Roman Plaud'], 'affiliations': ['Institut Polytechnique de Paris', 'Universit√© Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, France', 'Universit√© de Paris, LLF, CNRS, France', 'onepoint, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.06533.jpg', 'data': {'categories': ['#rl', '#training', '#long_context', '#small_models', '#reasoning', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è LLM —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤', 'desc': "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö —Ü–µ–ª–µ–π –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö LLM, —á—Ç–æ–±—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞–Ω–Ω—É—é —Ü–µ–ª—å. –û–Ω–∏ –∏—Å—Å–ª–µ–¥—É—é—Ç –¥–∏–Ω–∞–º–∏–∫—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–µ, –ø–æ–∫–∞–∑—ã–≤–∞—è –≤–ª–∏—è–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–∂–Ω–æ—Å—Ç—å '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤'. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è KL-—à—Ç—Ä–∞—Ñ–∞ –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º."}, 'en': {'title': 'Enhancing Exploration in Language Models with Critical Tokens', 'desc': "This paper addresses the challenge of enabling large language models (LLMs) to achieve long-term goals through reinforcement learning (RL). It highlights the difficulty of exploration in LLMs, where a balance must be maintained between discovering new solutions and preserving the model's foundational capabilities. The authors investigate how pre-training affects exploration dynamics in a small language model tasked with simple arithmetic. They introduce a modified Kullback-Leibler (KL) penalty that enhances exploration of 'critical tokens', leading to more efficient RL fine-tuning."}, 'zh': {'title': '‰ºòÂåñÈïøÊúüÁõÆÊ†áÁöÑÊé¢Á¥¢Á≠ñÁï•', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂÆûÁé∞ÈïøÊúüÁõÆÊ†áÊó∂Èù¢‰∏¥ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂØπÈ¢ÑËÆ≠ÁªÉÁöÑLLMsËøõË°åÂæÆË∞ÉÔºå‰ª•‰ºòÂåñÁâπÂÆöÁõÆÊ†áÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈ¢ÑËÆ≠ÁªÉÁöÑÁ®ãÂ∫¶ÂØπÊé¢Á¥¢ËøáÁ®ãÊúâÊòæËëóÂΩ±ÂìçÔºåÂ∞§ÂÖ∂ÊòØ‚ÄúÂÖ≥ÈîÆÊ†áËÆ∞‚ÄùÂú®ÊúÄÁªàÁªìÊûú‰∏≠Ëµ∑ÁùÄÈáçË¶Å‰ΩúÁî®„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂØπKLÊÉ©ÁΩöÁöÑÁÆÄÂçï‰øÆÊîπÔºå‰ª•‰øÉËøõÂØπÂÖ≥ÈîÆÊ†áËÆ∞ÁöÑÊé¢Á¥¢Ôºå‰ªéËÄåÊèêÈ´òRLÂæÆË∞ÉÈò∂ÊÆµÁöÑÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08168', 'title': 'SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation', 'url': 'https://huggingface.co/papers/2502.08168', 'abstract': "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.", 'score': 10, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '1ca2b8d38e35b203', 'authors': ['Zhiming Ma', 'Xiayang Xiao', 'Sihao Dong', 'Peidong Wang', 'HaiPeng Wang', 'Qingyun Pan'], 'affiliations': ['China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China', 'China Mobile Internet Company Ltd., Guangzhou, China', 'School of Computer Science and Engineering, Northeastern University, Shenyang, China', 'The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China', 'The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.08168.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#open_source', '#synthetic'], 'emoji': 'üõ∞Ô∏è', 'ru': {'title': 'SARChat-2M: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π SAR —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∏–∞–ª–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π SAR, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π SARChat-2M. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–∫–æ–ª–æ 2 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ —Ü–µ–ª–µ–π. –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Å–Ω–æ–≤—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–≥–æ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–∞ –±—ã–ª–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ 16 –æ—Å–Ω–æ–≤–Ω—ã—Ö VLM, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–æ–∑–¥–∞—Ç—å –ø–µ—Ä–≤—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –¥–∏–∞–ª–æ–≥–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤ –æ–±–ª–∞—Å—Ç–∏ SAR.'}, 'en': {'title': 'Empowering SAR Image Interpretation with SARChat-2M!', 'desc': 'This paper introduces SARChat-2M, a large-scale multimodal dialogue dataset specifically designed for synthetic aperture radar (SAR) images. It contains around 2 million image-text pairs that cover various scenarios and include detailed annotations for effective visual understanding and object detection. The dataset serves as a benchmark for evaluating vision language models (VLMs) in the SAR domain, demonstrating their capabilities in interpreting SAR images. By conducting experiments on 16 popular VLMs, the study establishes a new multi-task dialogue benchmark, paving the way for advancements in remote sensing applications.'}, 'zh': {'title': 'Êé®Âä®SARÂõæÂÉèËß£ËØªÁöÑÂ§öÊ®°ÊÄÅÂØπËØùÊï∞ÊçÆÈõÜ', 'desc': 'Âú®ÂêàÊàêÂ≠îÂæÑÈõ∑ËææÔºàSARÔºâÈÅ•ÊÑüÂõæÂÉèËß£ËØªÈ¢ÜÂüüÔºåÂ∞ΩÁÆ°ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÂõæÂÉèÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÁî±‰∫éÁº∫‰πè‰∏ì‰∏öÈ¢ÜÂüüÁöÑÁü•ËØÜÔºåÂÖ∂Â∫îÁî®‰ªçÁÑ∂ÊúâÈôê„ÄÇÊú¨ÊñáÂàõÊñ∞ÊÄßÂú∞ÊèêÂá∫‰∫ÜÁ¨¨‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑSARÂõæÂÉèÂ§öÊ®°ÊÄÅÂØπËØùÊï∞ÊçÆÈõÜSARChat-2MÔºåÂåÖÂê´Á∫¶200‰∏áÂØπÈ´òË¥®ÈáèÁöÑÂõæÂÉè-ÊñáÊú¨ÈÖçÂØπÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçÂú∫ÊôØÂíåËØ¶ÁªÜÁöÑÁõÆÊ†áÊ≥®Èáä„ÄÇËØ•Êï∞ÊçÆÈõÜ‰∏ç‰ªÖÊîØÊåÅËßÜËßâÁêÜËß£ÂíåÁõÆÊ†áÊ£ÄÊµãÁ≠âÂÖ≥ÈîÆ‰ªªÂä°ÔºåËøòÂºÄÂèë‰∫ÜSARÈ¢ÜÂüüÁöÑËßÜËßâËØ≠Ë®ÄÊï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÔºåËØÑ‰º∞VLMsÂú®SARÂõæÂÉèËß£ËØª‰∏≠ÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÂØπ16‰∏™‰∏ªÊµÅVLMÁöÑÂÆûÈ™åÈ™åËØÅÔºåËØ•Êï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄßÂæóÂà∞‰∫ÜÂÖÖÂàÜËØÅÊòéÔºåÂπ∂ÊàêÂäüÂª∫Á´ã‰∫ÜSARÈ¢ÜÂüüÁöÑÁ¨¨‰∏Ä‰∏™Â§ö‰ªªÂä°ÂØπËØùÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07599', 'title': 'DPO-Shift: Shifting the Distribution of Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2502.07599', 'abstract': 'Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.', 'score': 10, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '85d178e03a57421f', 'authors': ['Xiliang Yang', 'Feng Jiang', 'Qianen Zhang', 'Lei Zhao', 'Xiao Li'], 'affiliations': ['Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University', 'School of Data Science, The Chinese University of Hong Kong, Shenzhen', 'School of Mathematics, South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.07599.jpg', 'data': {'categories': ['#alignment', '#training', '#rlhf'], 'emoji': 'üîÄ', 'ru': {'title': '–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º DPO-Shift –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–º–µ—â–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–µ—Ç–æ–¥–∞ Direct Preference Optimization (DPO). DPO-Shift –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ —Å–º–µ—â–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ DPO-Shift –Ω–∞–¥ DPO –Ω–∞ —Ä—è–¥–µ –∑–∞–¥–∞—á, –≤–∫–ª—é—á–∞—è MT-Bench.'}, 'en': {'title': 'Mitigating Likelihood Displacement in Language Model Training', 'desc': 'This paper addresses the issue of likelihood displacement in Direct Preference Optimization (DPO) for aligning language models with human preferences. The authors introduce a new method, referred to as \textit{method}, which aims to control the distribution of chosen response probabilities during training. They highlight a trade-off between enhancing the chosen probability and the reward margin, supported by both theoretical insights and experimental results. The findings indicate that \textit{method} outperforms DPO in various downstream tasks, providing a promising solution to the challenges posed by likelihood displacement.'}, 'zh': {'title': 'Ëß£ÂÜ≥ÈÄâÊã©Ê¶ÇÁéá‰∏ãÈôçÁöÑÊúâÊïàÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ï\textit{method}ÔºåÊó®Âú®Ëß£ÂÜ≥Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâ‰∏≠Âá∫Áé∞ÁöÑÈÄâÊã©Ê¶ÇÁéá‰∏ãÈôçÈóÆÈ¢ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÂØπÈÄâÊã©ÂìçÂ∫îÁöÑÊ¶ÇÁéáÂæÄÂæÄ‰ºöÈôç‰ΩéÔºåËøôË¢´Áß∞‰∏∫‰ººÁÑ∂‰ΩçÁßª„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂèØ‰ª•ÊéßÂà∂ÈÄâÊã©Ê¶ÇÁéáÁöÑÂàÜÂ∏ÉÔºå‰ªéËÄåÊîπÂñÑÊ®°ÂûãÁöÑË°®Áé∞„ÄÇÈÄöËøáÁêÜËÆ∫ÂàÜÊûêÂíåÂÆûÈ™åÈ™åËØÅÔºåÊàë‰ª¨ËØÅÊòé‰∫Ü\textit{method}Âú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠‰ºò‰∫é‰º†ÁªüÁöÑDPOÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08524', 'title': 'LLM Pretraining with Continuous Concepts', 'url': 'https://huggingface.co/papers/2502.08524', 'abstract': "Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process.", 'score': 7, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '99ad370e7cd11e3c', 'authors': ['Jihoon Tack', 'Jack Lanchantin', 'Jane Yu', 'Andrew Cohen', 'Ilia Kulikov', 'Janice Lan', 'Shibo Hao', 'Yuandong Tian', 'Jason Weston', 'Xian Li'], 'affiliations': ['FAIR at Meta', 'KAIST', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.08524.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#interpretability', '#training', '#architecture', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': 'CoCoMix: —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Continuous Concept Mixing (CoCoMix). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞, CoCoMix —Å–æ—á–µ—Ç–∞–µ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CoCoMix –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –∫–∞—á–µ—Å—Ç–≤—É –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, CoCoMix —É–ª—É—á—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –ø–æ–∑–≤–æ–ª—è—è –Ω–∞–ø—Ä—è–º—É—é –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏.'}, 'en': {'title': 'Revolutionizing Language Models with Continuous Concept Mixing', 'desc': "This paper introduces Continuous Concept Mixing (CoCoMix), a new framework for pretraining large language models. Unlike traditional methods that focus solely on predicting the next token, CoCoMix integrates continuous concepts derived from a sparse autoencoder into the model's hidden states. This approach improves sample efficiency and enhances performance on various tasks, including language modeling and reasoning. Additionally, CoCoMix allows for better interpretability and control over the model's reasoning by enabling direct manipulation of the predicted concepts."}, 'zh': {'title': 'ËøûÁª≠Ê¶ÇÂøµÊ∑∑ÂêàÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÊïàÁéá‰∏éÂèØËß£ÈáäÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ¢ÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁß∞‰∏∫ËøûÁª≠Ê¶ÇÂøµÊ∑∑ÂêàÔºàCoCoMixÔºâÔºåÂÆÉÁªìÂêà‰∫ÜÁ¶ªÊï£ÁöÑ‰∏ã‰∏Ä‰∏™Ê†áËÆ∞È¢ÑÊµãÂíåËøûÁª≠Ê¶ÇÂøµ„ÄÇCoCoMixÈÄöËøáÂ∞Ü‰ªéÈ¢ÑËÆ≠ÁªÉÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®‰∏≠Â≠¶‰π†ÁöÑËøûÁª≠Ê¶ÇÂøµ‰∏éÊ†áËÆ∞ÁöÑÈöêËóèË°®Á§∫‰∫§ÈîôÊ∑∑ÂêàÔºåÊù•‰ºòÂåñÊ®°ÂûãÁöÑÈöêËóèÁä∂ÊÄÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCoCoMixÂú®Ê†∑Êú¨ÊïàÁéá‰∏äË°®Áé∞Êõ¥‰Ω≥ÔºåÂπ∂‰∏îÂú®ËØ≠Ë®ÄÂª∫Ê®°ÂíåÊé®ÁêÜ‰ªªÂä°‰∏äÂùá‰ºò‰∫é‰º†ÁªüÁöÑ‰∏ã‰∏Ä‰∏™Ê†áËÆ∞È¢ÑÊµãÂíåÁü•ËØÜËí∏È¶èÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïËøòÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÂèØÂºïÂØºÊÄßÔºå‰ΩøÂæóÁî®Êà∑ÂèØ‰ª•Áõ¥Êé•Ê£ÄÊü•Âíå‰øÆÊîπÈ¢ÑÊµãÁöÑÊ¶ÇÂøµÔºå‰ªéËÄåÈÄèÊòéÂú∞ÂºïÂØºÊ®°ÂûãÁöÑÂÜÖÈÉ®Êé®ÁêÜËøáÁ®ã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05167', 'title': 'NoLiMa: Long-Context Evaluation Beyond Literal Matching', 'url': 'https://huggingface.co/papers/2502.05167', 'abstract': 'Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.', 'score': 5, 'issue_id': 2188, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': '4ff0f34526efea9f', 'authors': ['Ali Modarressi', 'Hanieh Deilamsalehy', 'Franck Dernoncourt', 'Trung Bui', 'Ryan A. Rossi', 'Seunghyun Yoon', 'Hinrich Sch√ºtze'], 'affiliations': ['Adobe Research', 'Center for Information and Language Processing'], 'pdf_title_img': 'assets/pdf/title_img/2502.05167.jpg', 'data': {'categories': ['#benchmark', '#long_context'], 'emoji': 'üîç', 'ru': {'title': 'NoLiMa: –ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ NoLiMa –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ—Å—Ç–æ–≤, NoLiMa —Ç—Ä–µ–±—É–µ—Ç –æ—Ç –º–æ–¥–µ–ª–µ–π –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π –º–µ–∂–¥—É –≤–æ–ø—Ä–æ—Å–æ–º –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤ —Ç–µ–∫—Å—Ç–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å 12 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö LLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ø—Ä—è–º—ã—Ö –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.'}, 'en': {'title': 'NoLiMa: Challenging LLMs Beyond Literal Matches', 'desc': 'This paper introduces NoLiMa, a new benchmark designed to evaluate the capabilities of large language models (LLMs) in retrieving relevant information from extensive contexts. Unlike traditional methods that allow models to rely on literal matches, NoLiMa requires models to infer deeper associations between questions and answers with minimal lexical overlap. The study evaluates 12 popular LLMs, revealing that while they perform well in shorter contexts, their performance significantly declines as context length increases, particularly beyond 1K tokens. The findings indicate that the attention mechanism struggles to effectively retrieve relevant information in longer contexts when direct matches are not available.'}, 'zh': {'title': 'Èïø‰∏ä‰∏ãÊñá‰∏≠ÁöÑ‰ø°ÊÅØÊ£ÄÁ¥¢ÊåëÊàò', 'desc': 'ÊúÄËøëÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊîØÊåÅÈïøËææ128KÂà∞1MÁöÑ‰∏ä‰∏ãÊñá„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïNoLiMaÔºåÊó®Âú®ËØÑ‰º∞Ê®°ÂûãÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠Ê£ÄÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØÁöÑËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÈíàÂú®Âπ≤ËçâÂ†ÜÔºàNIAHÔºâÊµãËØï‰∏çÂêåÔºåNoLiMaËÆæËÆ°‰∫ÜÊúÄÂ∞èËØçÊ±áÈáçÂè†ÁöÑÈíàÈõÜÔºåË¶ÅÊ±ÇÊ®°ÂûãÊé®Êñ≠ÊΩúÂú®ÂÖ≥ËÅî‰ª•ÊâæÂà∞Èíà„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåÂ∞ΩÁÆ°Ëøô‰∫õÊ®°ÂûãÂú®Áü≠‰∏ä‰∏ãÊñá‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠ÊÄßËÉΩÊòæËëó‰∏ãÈôçÔºåÂ∞§ÂÖ∂ÊòØÂú®Áº∫‰πèÂ≠óÈù¢ÂåπÈÖçÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07737', 'title': 'Next Block Prediction: Video Generation via Semi-Autoregressive Modeling', 'url': 'https://huggingface.co/papers/2502.07737', 'abstract': 'Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.', 'score': 5, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '8038af0ecacc031f', 'authors': ['Shuhuai Ren', 'Shuming Ma', 'Xu Sun', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07737.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#inference', '#training'], 'emoji': 'üé¨', 'ru': {'title': 'NBP: –ë—ã—Å—Ç—Ä–∞—è –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –±–ª–æ–∫–∞–º–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Next-Block Prediction (NBP). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ Next-Token Prediction, NBP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª—É–∞–≤—Ç–æ—Ä–µr—Ä–µ—Å—Å–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å, —Ä–∞–∑–±–∏–≤–∞—è –≤–∏–¥–µ–æ –Ω–∞ –±–ª–æ–∫–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è –∏—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ú–æ–¥–µ–ª—å NBP –ø—Ä–µ–≤–∑–æ—à–ª–∞ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ –º–µ—Ç—Ä–∏–∫–µ FVD –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö UCF101 –∏ K600, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞.'}, 'en': {'title': 'Revolutionizing Video Generation with Next-Block Prediction', 'desc': 'This paper introduces a new method for generating videos called Next-Block Prediction (NBP), which improves upon the traditional Next-Token Prediction (NTP) approach. NBP changes the way video content is generated by using blocks instead of individual tokens, allowing for simultaneous predictions within each block. This method utilizes bidirectional attention to enhance the understanding of spatial relationships in the video, leading to better quality outputs. As a result, NBP not only speeds up the generation process significantly but also achieves superior performance metrics compared to the standard NTP model.'}, 'zh': {'title': 'ËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥Ôºö‰∏ã‰∏ÄÂùóÈ¢ÑÊµã', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂçäËá™ÂõûÂΩíÊ°ÜÊû∂ÔºåÁß∞‰∏∫‰∏ã‰∏ÄÂùóÈ¢ÑÊµãÔºàNBPÔºâÔºåÁî®‰∫éËßÜÈ¢ëÁîüÊàê„ÄÇ‰∏é‰º†ÁªüÁöÑËá™ÂõûÂΩíÊñπÊ≥ï‰∏çÂêåÔºåNBPÈÄöËøáÂ∞ÜËßÜÈ¢ëÂÜÖÂÆπÂùáÂåÄÂàÜËß£‰∏∫Áõ∏Á≠âÂ§ßÂ∞èÁöÑÂùóÔºå‰ΩøÂæóÊØè‰∏™ÂùóÂÜÖÁöÑÊ†áËÆ∞ÂèØ‰ª•ÂêåÊó∂È¢ÑÊµã‰∏ã‰∏Ä‰∏™ÂùóÁöÑÂØπÂ∫îÊ†áËÆ∞Ôºå‰ªéËÄåÊçïÊçâÊõ¥Âº∫ÁöÑÁ©∫Èó¥‰æùËµñÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂπ∂Ë°åÈ¢ÑÊµãÂ§ö‰∏™Ê†áËÆ∞ÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÁîüÊàêÊ≠•È™§ÔºåÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåËææÂà∞‰∫ÜÊØèÁßíÁîüÊàê8.89Â∏ßÁöÑÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNBPÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫é‰º†ÁªüÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÁîüÊàêË¥®ÈáèÂíåÈÄüÂ∫¶‰∏äÁöÑ‰ºòÂäø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06145', 'title': 'Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance', 'url': 'https://huggingface.co/papers/2502.06145', 'abstract': 'Recent character image animation methods based on diffusion models, such as Animate Anyone, have made significant progress in generating consistent and generalizable character animations. However, these approaches fail to produce reasonable associations between characters and their environments. To address this limitation, we introduce Animate Anyone 2, aiming to animate characters with environment affordance. Beyond extracting motion signals from source video, we additionally capture environmental representations as conditional inputs. The environment is formulated as the region with the exclusion of characters and our model generates characters to populate these regions while maintaining coherence with the environmental context. We propose a shape-agnostic mask strategy that more effectively characterizes the relationship between character and environment. Furthermore, to enhance the fidelity of object interactions, we leverage an object guider to extract features of interacting objects and employ spatial blending for feature injection. We also introduce a pose modulation strategy that enables the model to handle more diverse motion patterns. Experimental results demonstrate the superior performance of the proposed method.', 'score': 4, 'issue_id': 2192, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 10', 'zh': '2Êúà10Êó•'}, 'hash': '66fa48cd36ed02b0', 'authors': ['Li Hu', 'Guangyuan Wang', 'Zhen Shen', 'Xin Gao', 'Dechao Meng', 'Lian Zhuo', 'Peng Zhang', 'Bang Zhang', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.06145.jpg', 'data': {'categories': ['#multimodal', '#cv', '#video', '#diffusion'], 'emoji': 'üé≠', 'ru': {'title': '–û–∂–∏–≤–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å —É—á–µ—Ç–æ–º –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã', 'desc': '–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Animate Anyone 2 - –º–µ—Ç–æ–¥ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å —É—á–µ—Ç–æ–º –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∑–¥–µ—Å—å –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ —Å–∏–≥–Ω–∞–ª—ã –¥–≤–∏–∂–µ–Ω–∏—è, –Ω–æ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —É—Å–ª–æ–≤–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∞—Å–∫–∏, –Ω–µ –∑–∞–≤–∏—Å—è—â–∞—è –æ—Ç —Ñ–æ—Ä–º—ã, –¥–ª—è –ª—É—á—à–µ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –∏ —Å—Ä–µ–¥—ã. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—ä–µ–∫—Ç–Ω—ã–π –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–∏–π –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.'}, 'en': {'title': 'Animating Characters with Environmental Awareness', 'desc': 'This paper presents Animate Anyone 2, an advanced character animation method that improves upon previous diffusion models by integrating environmental context into the animation process. Unlike earlier methods, which struggled to connect characters with their surroundings, this approach captures environmental representations as conditional inputs, allowing for more coherent animations. The authors introduce a shape-agnostic mask strategy to better define the relationship between characters and their environments, enhancing the realism of interactions. Additionally, they implement an object guider and spatial blending techniques to refine object interactions and a pose modulation strategy to accommodate diverse motion patterns, resulting in superior animation quality.'}, 'zh': {'title': 'ËßíËâ≤‰∏éÁéØÂ¢ÉÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßíËâ≤Âä®ÁîªÊñπÊ≥ïAnimate Anyone 2ÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÂä®ÁîªÊñπÊ≥ïÂú®ËßíËâ≤‰∏éÁéØÂ¢É‰πãÈó¥ÁöÑÂÖ≥ËÅî‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÈÄöËøáÊèêÂèñÁéØÂ¢ÉË°®Á§∫‰Ωú‰∏∫Êù°‰ª∂ËæìÂÖ•Ôºå‰ΩøËßíËâ≤Âä®ÁîªËÉΩÂ§ü‰∏éÁéØÂ¢ÉÁöÑÁâπÂæÅÁõ∏‰∏ÄËá¥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂΩ¢Áä∂Êó†ÂÖ≥ÁöÑÊé©Á†ÅÁ≠ñÁï•ÔºåÊõ¥ÊúâÊïàÂú∞ÊèèËø∞ËßíËâ≤‰∏éÁéØÂ¢É‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÂßøÊÄÅË∞ÉËäÇÁ≠ñÁï•Ôºå‰ª•Â§ÑÁêÜÊõ¥‰∏∞ÂØåÁöÑËøêÂä®Ê®°ÂºèÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéËØ•ÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÊäÄÊúØ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.06872', 'title': 'Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2502.06872', 'abstract': "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.", 'score': 4, 'issue_id': 2188, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 8', 'zh': '2Êúà8Êó•'}, 'hash': 'f454782ce3101c66', 'authors': ['Bo Ni', 'Zheyuan Liu', 'Leyao Wang', 'Yongjia Lei', 'Yuying Zhao', 'Xueqi Cheng', 'Qingkai Zeng', 'Luna Dong', 'Yinglong Xia', 'Krishnaram Kenthapadi', 'Ryan Rossi', 'Franck Dernoncourt', 'Md Mehrab Tanjim', 'Nesreen Ahmed', 'Xiaorui Liu', 'Wenqi Fan', 'Erik Blasch', 'Yu Wang', 'Meng Jiang', 'Tyler Derr'], 'affiliations': ['Adobe Research', 'Air Force Research Lab', 'Cisco AI Research', 'Meta', 'North Carolina State University', 'Oracle Health AI', 'The Hong Kong Polytechnic University', 'University of Notre Dame', 'University of Oregon', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06872.jpg', 'data': {'categories': ['#hallucinations', '#interpretability', '#security', '#survey', '#rag', '#ethics'], 'emoji': 'üß†', 'ru': {'title': '–ü—É—Ç—å –∫ –Ω–∞–¥—ë–∂–Ω–æ–º—É –ò–ò: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –º–µ—Ç–æ–¥—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π (RAG), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Ä–∏—Å–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å RAG, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏, –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –¥–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞–¥—ë–∂–Ω—ã—Ö RAG-—Å–∏—Å—Ç–µ–º —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –ø—è—Ç—å –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤. –°—Ç–∞—Ç—å—è —Ç–∞–∫–∂–µ –æ—Å–≤–µ—â–∞–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–¥—ë–∂–Ω—ã—Ö RAG-—Å–∏—Å—Ç–µ–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.'}, 'en': {'title': 'Building Trust in Retrieval-Augmented Generation Systems', 'desc': 'This paper discusses Retrieval-Augmented Generation (RAG), a technique that enhances AI-generated content by incorporating external knowledge for better accuracy and relevance. While RAG improves content generation, it also brings new challenges such as robustness, privacy, and accountability issues that need to be addressed. The authors propose a comprehensive framework that focuses on five key areas: reliability, privacy, safety, fairness, and explainability, to guide future research and development of trustworthy RAG systems. By providing a structured approach, the paper aims to foster innovation and highlight the importance of trust in RAG applications.'}, 'zh': {'title': 'ÊûÑÂª∫ÂèØ‰ø°ËµñÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÁ≥ªÁªü', 'desc': 'Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊòØ‰∏ÄÁßçÂÖàËøõÁöÑÊäÄÊúØÔºåÊó®Âú®Ëß£ÂÜ≥‰∫∫Â∑•Êô∫ËÉΩÁîüÊàêÂÜÖÂÆπÔºàAIGCÔºâÈù¢‰∏¥ÁöÑÊåëÊàò„ÄÇÈÄöËøáÂ∞Ü‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢‰∏éÂÜÖÂÆπÁîüÊàêÁõ∏ÁªìÂêàÔºåRAG Êèê‰æõÂèØÈù†‰∏îÊúÄÊñ∞ÁöÑÂ§ñÈÉ®Áü•ËØÜÔºåÂáèÂ∞ëÂπªËßâÁé∞Ë±°ÔºåÂπ∂Á°Æ‰øùÂú®ÂêÑÁßç‰ªªÂä°‰∏≠‰øùÊåÅÁõ∏ÂÖ≥‰∏ä‰∏ãÊñá„ÄÇÁÑ∂ËÄåÔºåÂ∞ΩÁÆ° RAG ÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËØ•ËåÉÂºè‰πüÂºïÂÖ•‰∫ÜÊñ∞ÁöÑÈ£éÈô©ÔºåÂåÖÊã¨È≤ÅÊ£íÊÄßÈóÆÈ¢ò„ÄÅÈöêÁßÅÈóÆÈ¢ò„ÄÅÂØπÊäóÊÄßÊîªÂáªÂíåÈóÆË¥£ÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊó®Âú®Êèê‰æõ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑË∑ØÁ∫øÂõæÔºå‰ª•ÂºÄÂèëÂèØ‰ø°ËµñÁöÑ RAG Á≥ªÁªüÔºåÂõ¥ÁªïÂèØÈù†ÊÄß„ÄÅÈöêÁßÅ„ÄÅÂÆâÂÖ®ÊÄß„ÄÅÂÖ¨Âπ≥ÊÄß„ÄÅÂèØËß£ÈáäÊÄßÂíåÈóÆË¥£ÊÄßÁ≠â‰∫î‰∏™ÂÖ≥ÈîÆËßÜËßíËøõË°åËÆ®ËÆ∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.04411', 'title': 'Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing', 'url': 'https://huggingface.co/papers/2502.04411', 'abstract': 'Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.', 'score': 3, 'issue_id': 2192, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '9370051a307713bf', 'authors': ['Kunfeng Lai', 'Zhenheng Tang', 'Xinglin Pan', 'Peijie Dong', 'Xiang Liu', 'Haolan Chen', 'Li Shen', 'Bo Li', 'Xiaowen Chu'], 'affiliations': ['Platform and Content Group, Tencent', 'Sun Yatsen University', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.04411.jpg', 'data': {'categories': ['#model merging', '#training', '#architecture', '#reasoning', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–£–º–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É—Å—Ä–µ–¥–Ω—è—Ç—å —Å–ª–æ–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —Å–ª–æ–µ–≤ —Å–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏. –û–Ω–∏ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –Ω–∞ –ø–ª–æ—Ç–Ω—ã–π –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∑–∞—Ç—Ä–∞—Ç –ø–∞–º—è—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö LLaMA –∏ Qwen –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Merging Models Smartly: Harnessing Layer Insights for Better Performance', 'desc': 'This paper presents a method for merging Large Language Models (LLMs) that have been fine-tuned on different tasks, aiming to create a more powerful model. The authors identify that parameter conflicts between models can degrade performance when simply averaging them. To address this, they propose a strategy that averages layers with minimal conflicts and employs task-level expert routing for layers with significant conflicts. Their approach not only reduces storage costs by decoupling fine-tuned experts but also improves performance on real-world reasoning tasks while being more efficient than existing methods.'}, 'zh': {'title': 'Êô∫ËÉΩÂêàÂπ∂ÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩÔºÅ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ®°ÂûãÂêàÂπ∂ÁöÑÊñπÊ≥ïÔºåÊó®Âú®Â∞Ü‰∏çÂêå‰ªªÂä°‰∏äÂæÆË∞ÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÅöÂêàÊàê‰∏Ä‰∏™Êõ¥Âº∫ÁöÑÊ®°Âûã„ÄÇÊàë‰ª¨ÂèëÁé∞‰∏çÂêåÂ±Ç‰πãÈó¥ÁöÑÂèÇÊï∞ÂÜ≤Á™ÅÁ®ãÂ∫¶‰∏çÂêåÔºåÂõ†Ê≠§Êàë‰ª¨ÂØπÂèÇÊï∞ÂÜ≤Á™ÅËæÉÂ∞èÁöÑÂ±ÇËøõË°åÂπ≥ÂùáÔºåËÄåÂØπÂèÇÊï∞ÂÜ≤Á™ÅËæÉÂ§ßÁöÑÂ±ÇÈááÁî®Êñ∞È¢ñÁöÑ‰ªªÂä°Á∫ß‰∏ìÂÆ∂Ë∑ØÁî±„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•Èôç‰ΩéÂ≠òÂÇ®ÊàêÊú¨ÔºåÊàë‰ª¨Â∞ÜÂ§ö‰∏™ÂæÆË∞ÉÁöÑ‰∏ìÂÆ∂Ëß£ËÄ¶‰∏∫‰∏Ä‰∏™Á®†ÂØÜ‰∏ìÂÆ∂ÂíåÂá†‰∏™Á®ÄÁñè‰∏ìÂÆ∂ÔºåÂπ∂Ê†πÊçÆËæìÂÖ•Êï∞ÊçÆÁöÑ‰ªªÂä°‰∏çÁ°ÆÂÆöÊÄßÈÄâÊã©ÂíåÂêàÂπ∂ÂêàÈÄÇÁöÑ‰∏ìÂÆ∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜÁ≥ªÁªüÊàêÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.00963', 'title': 'PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs', 'url': 'https://huggingface.co/papers/2502.00963', 'abstract': 'While recent AI-for-math has made strides in pure mathematics, areas of applied mathematics, particularly PDEs, remain underexplored despite their significant real-world applications. We present PDE-Controller, a framework that enables large language models (LLMs) to control systems governed by partial differential equations (PDEs). Our approach enables LLMs to transform informal natural language instructions into formal specifications, and then execute reasoning and planning steps to improve the utility of PDE control. We build a holistic solution comprising datasets (both human-written cases and 2 million synthetic samples), math-reasoning models, and novel evaluation metrics, all of which require significant effort. Our PDE-Controller significantly outperforms prompting the latest open-source and GPT models in reasoning, autoformalization, and program synthesis, achieving up to a 62% improvement in utility gain for PDE control. By bridging the gap between language generation and PDE systems, we demonstrate the potential of LLMs in addressing complex scientific and engineering challenges. We will release all data, model checkpoints, and code at https://pde-controller.github.io/.', 'score': 1, 'issue_id': 2200, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '44c533b68f27ca69', 'authors': ['Mauricio Soroco', 'Jialin Song', 'Mengzhou Xia', 'Kye Emond', 'Weiran Sun', 'Wuyang Chen'], 'affiliations': ['Department of Computer Science, Princeton University', 'School of Computing Science, Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2502.00963.jpg', 'data': {'categories': ['#open_source', '#training', '#dataset', '#synthetic', '#math', '#science', '#reasoning'], 'emoji': 'üßÆ', 'ru': {'title': '–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–æ—Ä—è—é—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ PDE-Controller - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–∏—Å—Ç–µ–º–∞–º–∏, –æ–ø–∏—Å—ã–≤–∞–µ–º—ã–º–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º–∏ —É—Ä–∞–≤–Ω–µ–Ω–∏—è–º–∏ –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö (PDE). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –≤ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∞ –∑–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è PDE. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—é—â–µ–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏. PDE-Controller –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –∞–≤—Ç–æ—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Å–∏–Ω—Ç–µ–∑–µ –ø—Ä–æ–≥—Ä–∞–º–º, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª LLM –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∏ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'Empowering Language Models to Control Complex PDE Systems', 'desc': 'This paper introduces PDE-Controller, a framework that allows large language models (LLMs) to manage systems described by partial differential equations (PDEs). It transforms informal language instructions into formal specifications, enabling effective reasoning and planning for PDE control. The framework includes a comprehensive solution with datasets, math-reasoning models, and new evaluation metrics, showcasing significant improvements over existing models. By achieving up to a 62% increase in utility for PDE control, this work highlights the capability of LLMs to tackle complex problems in science and engineering.'}, 'zh': {'title': 'Âà©Áî®LLMsÊèêÂçáÂÅèÂæÆÂàÜÊñπÁ®ãÊéßÂà∂ÁöÑÂÆûÁî®ÊÄß', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜPDE-ControllerÊ°ÜÊû∂ÔºåÊó®Âú®Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊéßÂà∂Áî±ÂÅèÂæÆÂàÜÊñπÁ®ãÔºàPDEsÔºâÊâÄÊîØÈÖçÁöÑÁ≥ªÁªü„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÂ∞ÜËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ËΩ¨Âåñ‰∏∫Ê≠£ÂºèËßÑËåÉÔºåÂπ∂ÊâßË°åÊé®ÁêÜÂíåËßÑÂàíÊ≠•È™§Ôºå‰ªéËÄåÊèêÈ´òPDEÊéßÂà∂ÁöÑÂÆûÁî®ÊÄß„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂåÖÊã¨Êï∞ÊçÆÈõÜÔºà‰∫∫Á±ªÁºñÂÜôÁöÑÊ°à‰æãÂíå200‰∏á‰∏™ÂêàÊàêÊ†∑Êú¨Ôºâ„ÄÅÊï∞Â≠¶Êé®ÁêÜÊ®°ÂûãÂíåÊñ∞È¢ñÁöÑËØÑ‰º∞ÊåáÊ†á„ÄÇPDE-ControllerÂú®Êé®ÁêÜ„ÄÅËá™Âä®ÂΩ¢ÂºèÂåñÂíåÁ®ãÂ∫èÂêàÊàêÊñπÈù¢ÊòæËëó‰ºò‰∫éÊúÄÊñ∞ÁöÑÂºÄÊ∫êÂíåGPTÊ®°ÂûãÔºåPDEÊéßÂà∂ÁöÑÂÆûÁî®ÊÄßÊèêÂçáËææ62%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.08213', 'title': 'LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention', 'url': 'https://huggingface.co/papers/2502.08213', 'abstract': 'In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources. Experimental results on the Bespoke-Stratos-17k dataset demonstrate that after 15 epochs of training, the combined model generates responses comparable in quality to those obtained by distillation. We discuss the advantages of the modular approach, provide examples of input queries and comparative analysis, and outline prospects for further extension of the method.', 'score': 1, 'issue_id': 2194, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 12', 'zh': '2Êúà12Êó•'}, 'hash': '2623177431838d6c', 'authors': ['Konstantin Kolomeitsev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.08213.jpg', 'data': {'categories': ['#small_models', '#transfer_learning', '#optimization', '#architecture', '#training'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥—É–ª–µ–π LLM, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∑–Ω–∞–Ω–∏—è –æ—Ç –±–æ–ª—å—à–æ–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫ –º–µ–Ω—å—à–µ–π —Å –ø–æ–º–æ—â—å—é —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è. –ó–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Qwen2-1.5B –ø–µ—Ä–µ–¥–∞–µ—Ç —Å–≤–æ–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ GPT-Neo-125M, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö Bespoke-Stratos-17k –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ—Å–ª–µ 15 —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã, —Å—Ä–∞–≤–Ω–∏–º—ã–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É —Å –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –ø—É—Ç–µ–º –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –æ–±—Å—É–∂–¥–∞—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –º–æ–¥—É–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –º–µ—Ç–æ–¥–∞.'}, 'en': {'title': 'Empowering Smaller Models with Enhanced Knowledge Transfer', 'desc': 'This paper introduces a new architecture called LLM Modules that facilitates knowledge transfer from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. The Qwen2-1.5B model is kept unchanged, and its learned representations are utilized by the smaller GPT-Neo-125M model, which is optimized for limited computational resources. The results from experiments on the Bespoke-Stratos-17k dataset show that after 15 training epochs, the performance of the combined model is on par with traditional distillation methods. The authors highlight the benefits of this modular approach and provide examples and analyses to support their findings, while also discussing future enhancements.'}, 'zh': {'title': 'Áü•ËØÜËΩ¨ÁßªÁöÑÊñ∞ÊñπÊ≥ïÔºöÊ®°ÂùóÂåñÊû∂ÊûÑ‰∏éÂ¢ûÂº∫‰∫§ÂèâÊ≥®ÊÑèÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçLLMÊ®°ÂùóÊû∂ÊûÑÔºåËÉΩÂ§üÈÄöËøáÂ¢ûÂº∫ÁöÑ‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Â∞ÜÁü•ËØÜ‰ªéÂ§ßÂûãÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËΩ¨ÁßªÂà∞ËæÉÂ∞èÁöÑÊ®°Âûã„ÄÇÊàë‰ª¨Â∞ÜQwen2-1.5BÊ®°ÂûãÂõ∫ÂÆöÔºåÂπ∂ÈÄöËøá‰∏ìÈó®ËÆæËÆ°ÁöÑÊ≥®ÊÑèÂäõÂ±ÇÂ∞ÜÂÖ∂Ë°®Á§∫‰º†ÈÄíÁªôÂú®ÊúâÈôêËÆ°ÁÆóËµÑÊ∫ê‰∏äËÆ≠ÁªÉÁöÑGPT-Neo-125MÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Bespoke-Stratos-17kÊï∞ÊçÆÈõÜ‰∏äÁªèËøá15‰∏™ËÆ≠ÁªÉÂë®ÊúüÂêéÔºåÁªÑÂêàÊ®°ÂûãÁîüÊàêÁöÑÂìçÂ∫îË¥®Èáè‰∏éËí∏È¶èËé∑ÂæóÁöÑÁªìÊûúÁõ∏ÂΩì„ÄÇÊàë‰ª¨ËÆ®ËÆ∫‰∫ÜÊ®°ÂùóÂåñÊñπÊ≥ïÁöÑ‰ºòÂäøÔºåÊèê‰æõ‰∫ÜËæìÂÖ•Êü•ËØ¢ÁöÑÁ§∫‰æãÂíåÊØîËæÉÂàÜÊûêÔºåÂπ∂Ê¶ÇËø∞‰∫ÜËØ•ÊñπÊ≥ïËøõ‰∏ÄÊ≠•Êâ©Â±ïÁöÑÂâçÊôØ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.07985', 'title': 'MetaSC: Test-Time Safety Specification Optimization for Language Models', 'url': 'https://huggingface.co/papers/2502.07985', 'abstract': 'We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at https://github.com/vicgalle/meta-self-critique.git .', 'score': 1, 'issue_id': 2190, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 11', 'zh': '2Êúà11Êó•'}, 'hash': '68244a5483bfc513', 'authors': ['V√≠ctor Gallego'], 'affiliations': ['Komorebi AI, Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2502.07985.jpg', 'data': {'categories': ['#optimization', '#training', '#security', '#alignment', '#inference'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏. –ü–æ–¥—Ö–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–µ –º–µ—Ç–∞-–∫—Ä–∏—Ç–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∫—Ä–∏—Ç–∏–∫–∏ –∏ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É–ª—É—á—à–∞–µ—Ç –∑–∞—â–∏—Ç—É –æ—Ç –ø–æ–ø—ã—Ç–æ–∫ –æ–±—Ö–æ–¥–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ –ø–æ–≤—ã—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏.'}, 'en': {'title': 'Dynamic Safety for Language Models: Adapting Prompts for Better Protection', 'desc': "This paper introduces a dynamic safety framework designed to enhance the safety of language models during inference without altering their underlying weights. It utilizes a meta-critique mechanism that adaptively updates safety prompts, known as specifications, to improve the model's ability to handle adversarial inputs and general safety tasks. The approach shows significant improvements in safety performance, particularly in avoiding moral harm and ensuring honest responses. Empirical results indicate that the dynamically optimized safety prompts outperform traditional fixed prompts and static defenses in various scenarios."}, 'zh': {'title': 'Âä®ÊÄÅ‰ºòÂåñÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂÆâÂÖ®ÊÄßÔºÅ', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂä®ÊÄÅÂÆâÂÖ®Ê°ÜÊû∂ÔºåÊó®Âú®‰ºòÂåñËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ÁöÑÂÆâÂÖ®ÊÄßÊé®ÁêÜÔºåËÄåÊó†ÈúÄ‰øÆÊîπÊ®°ÂûãÊùÉÈáç„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫éËá™ÊàëÊâπËØÑÊñπÊ≥ïÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÂà©Áî®ÂÖÉÊâπËØÑÊú∫Âà∂Ëø≠‰ª£Êõ¥Êñ∞ÂÆâÂÖ®ÊèêÁ§∫ÔºàÁß∞‰∏∫ËßÑËåÉÔºâÔºå‰ª•Ëá™ÈÄÇÂ∫îÂú∞Êé®Âä®ÊâπËØÑÂíå‰øÆËÆ¢ËøáÁ®ã„ÄÇÊ≠§ÊµãËØïÊó∂‰ºòÂåñ‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÂØπÊäóÊÄßË∂äÁã±ËØ∑Ê±ÇÁöÑÊÄßËÉΩÔºåËøòÂú®ÈÅøÂÖçÈÅìÂæ∑‰º§ÂÆ≥ÂíåËøΩÊ±ÇËØöÂÆûÂõûÂ∫îÁ≠âÂ§öÁßçÂÆâÂÖ®Áõ∏ÂÖ≥‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÊàë‰ª¨ÁöÑÂÆûËØÅËØÑ‰º∞ÊòæÁ§∫ÔºåÂä®ÊÄÅ‰ºòÂåñÁöÑÂÆâÂÖ®ÊèêÁ§∫Áõ∏ÊØî‰∫éÂõ∫ÂÆöÁ≥ªÁªüÊèêÁ§∫ÂíåÈùôÊÄÅËá™ÊàëÊâπËØÑÈò≤Âæ°ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂÆâÂÖ®ËØÑÂàÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2502.05282', 'title': 'Homeomorphism Prior for False Positive and Negative Problem in Medical Image Dense Contrastive Representation Learning', 'url': 'https://huggingface.co/papers/2502.05282', 'abstract': "Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image-dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables a reliable correspondence discovery for effective dense contrast. We propose a deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate a deformable mapping to predict the pixels' correspondence under topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via a gradient. We also propose a geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code on a companion link: https://github.com/YutingHe-list/GEMINI.", 'score': 0, 'issue_id': 2203, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 7', 'zh': '2Êúà7Êó•'}, 'hash': '966279f30488fb3d', 'authors': ['Yuting He', 'Boyu Wang', 'Rongjun Ge', 'Yang Chen', 'Guanyu Yang', 'Shuo Li'], 'affiliations': ['Centre de Recherche en Information Biomedicale Sino-Francais (CRIBs)', 'Department of Biomedical Engineering and the Department of Computer and Data Science, Case Western Reserve University, Cleveland, OH 44106 USA', 'Department of Computer Science, Western University, London, ON N6A 3K7, Canada', 'Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing, Nanjing, China', 'Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, Nanjing, China', 'School of Instrument Science and Engineering, Southeast University, Nanjing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.05282.jpg', 'data': {'categories': ['#dataset', '#training', '#cv', '#healthcare', '#open_source', '#optimization'], 'emoji': 'üî¨', 'ru': {'title': 'GEMINI: –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ø–ª–æ—Ç–Ω—ã–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º (DCRL) –¥–ª—è –∑–∞–¥–∞—á –ø–ª–æ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–¥—Ö–æ–¥ GEMINI, –∫–æ—Ç–æ—Ä—ã–π –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≥–æ–º–µ–æ–º–æ—Ä—Ñ–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ DCRL –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –¥–µ—Ñ–æ—Ä–º–∏—Ä—É–µ–º–æ–º—É –≥–æ–º–µ–æ–º–æ—Ä—Ñ–∏–∑–º—É (DHL) –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (GSS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–µ–º–∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.'}, 'en': {'title': 'Enhancing Medical Image Learning with GEMINI: Reliable Correspondence Discovery', 'desc': 'This paper introduces GEMINI, a novel approach to improve dense contrastive representation learning (DCRL) for medical images. It addresses the challenge of unreliable correspondence discovery by incorporating a homeomorphism prior, which helps in accurately mapping pixel correspondences while preserving the topological structure of the images. The method includes deformable homeomorphism learning (DHL) to reduce false positive and negative pairs, enhancing the learning process. Additionally, geometric semantic similarity (GSS) is utilized to assess the alignment of features, leading to more effective and reliable dense contrast learning.'}, 'zh': {'title': 'ÊèêÂçáÂåªÁñóÂõæÂÉèÂ≠¶‰π†ÊïàÁéáÁöÑÂá†‰ΩïÂØπÊØîÊñπÊ≥ï', 'desc': 'ÂØÜÈõÜÂØπÊØîË°®Á§∫Â≠¶‰π†ÔºàDCRLÔºâÂú®ÂõæÂÉèÂØÜÈõÜÈ¢ÑÊµã‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÂ≠¶‰π†ÊïàÁéáÔºåÂ∞§ÂÖ∂Âú®ÂåªÁñóÂõæÂÉèÊî∂ÈõÜÂíåÂØÜÈõÜÊ†áÊ≥®ÊñπÈù¢ÂÖ∑ÊúâÂ∑®Â§ßÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÂåªÁñóÂõæÂÉèÁöÑÁâπÊÄßÂØºËá¥‰∫Ü‰∏çÂèØÈù†ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂèëÁé∞ÔºåÈÄ†Êàê‰∫ÜÂ§ßÈáèÁöÑÂÅáÈò≥ÊÄßÂíåÂÅáÈò¥ÊÄßÂØπ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂá†‰ΩïËßÜËßâÂØÜÈõÜÁõ∏‰ººÊÄßÔºàGEMINIÔºâÂ≠¶‰π†ÔºåÈÄöËøáÂºïÂÖ•ÂêåËÉöÊÄßÂÖàÈ™åÊù•Â¢ûÂº∫DCRLÁöÑÂèØÈù†ÊÄßÔºå‰ªéËÄåÊúâÊïàÂú∞ÂèëÁé∞ÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜÂèØÂèòÂΩ¢ÂêåËÉöÂ≠¶‰π†ÔºàDHLÔºâÂíåÂá†‰ΩïËØ≠‰πâÁõ∏‰ººÊÄßÔºàGSSÔºâÔºåËøô‰∏§ËÄÖÂÖ±ÂêåÊèêÈ´ò‰∫ÜÂØπÂ∫îÂ≠¶‰π†ÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (20)', '#agents (33)', '#agi (14)', '#alignment (27)', '#architecture (83)', '#audio (6)', '#benchmark (114)', '#cv (45)', '#data (42)', '#dataset (115)', '#diffusion (39)', '#ethics (12)', '#games (16)', '#graphs (3)', '#hallucinations (13)', '#healthcare (10)', '#inference (53)', '#interpretability (30)', '#leakage (3)', '#long_context (27)', '#low_resource (15)', '#machine_translation (2)', '#math (24)', '#multilingual (19)', '#multimodal (69)', '#open_source (66)', '#optimization (144)', '#plp (7)', '#rag (15)', '#reasoning (89)', '#rl (27)', '#rlhf (23)', '#robotics (8)', '#science (11)', '#security (16)', '#small_models (14)', '#story_generation (2)', '#survey (6)', '#synthetic (22)', '#training (189)', '#transfer_learning (30)', '#video (32)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-02-20 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-20 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-20 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    