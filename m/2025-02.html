
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 45 papers. February 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Февраль 2025</span> | <span id="title-articles-count">45 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-01.html">⬅️ <span id="prev-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-03.html">➡️ <span id="next-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Февраль 2025', 'en': 'February 2025', 'zh': '2月2025年'};
        let feedDateNext = {'ru': '03.2025', 'en': '03/2025', 'zh': '3月2025年'};
        let feedDatePrev = {'ru': '01.2025', 'en': '01/2025', 'zh': '1月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2501.19393', 'title': 's1: Simple test-time scaling', 'url': 'https://huggingface.co/papers/2501.19393', 'abstract': 'Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI\'s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model\'s thinking process or lengthening it by appending "Wait" multiple times to the model\'s generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.', 'score': 48, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '8fcf84a9effc288f', 'authors': ['Niklas Muennighoff', 'Zitong Yang', 'Weijia Shi', 'Xiang Lisa Li', 'Li Fei-Fei', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer', 'Percy Liang', 'Emmanuel Candès', 'Tatsunori Hashimoto'], 'affiliations': ['Allen Institute for AI', 'Contextual AI', 'Stanford University', 'University of Washington, Seattle'], 'pdf_title_img': 'assets/pdf/title_img/2501.19393.jpg', 'data': {'categories': ['#dataset', '#open_source', '#reasoning', '#training', '#math', '#optimization', '#data'], 'emoji': '🧠', 'ru': {'title': 'Простое масштабирование для улучшения рассуждений языковых моделей', 'desc': 'Статья представляет новый подход к языковому моделированию, называемый тестовым масштабированием. Авторы разработали модель s1, основанную на Qwen2.5-32B-Instruct, и метод бюджетного форсирования для контроля вычислений во время тестирования. Модель обучена на специально отобранном наборе данных s1K из 1000 вопросов с рассуждениями. Результаты показывают, что s1 превосходит модель o1-preview от OpenAI на математических задачах, демонстрируя эффективность предложенного подхода.'}, 'en': {'title': 'Enhancing Language Models with Test-Time Scaling', 'desc': "This paper introduces a method called test-time scaling for enhancing language model performance during evaluation. The authors create a dataset of 1,000 questions with reasoning traces to train their model, focusing on difficulty, diversity, and quality. They implement a technique called budget forcing, which manipulates the model's response time to encourage deeper reasoning and correct errors. After fine-tuning their model, they demonstrate significant improvements in solving math competition questions compared to previous models, showcasing the effectiveness of their approach."}, 'zh': {'title': '测试时间扩展：提升语言模型性能的新方法', 'desc': '本文介绍了一种新的语言建模方法——测试时间扩展，旨在通过增加测试时间计算来提高模型性能。研究者们创建了一个包含1000个问题及其推理过程的小数据集s1K，并通过难度、多样性和质量三个标准进行验证。为了控制测试时间计算，提出了预算强制的方法，通过强制终止模型的思考过程或在模型生成时多次添加“等待”来延长思考时间，从而促使模型检查答案。经过监督微调后，模型s1在数学竞赛问题上超越了OpenAI的o1模型，表现提升达27%。'}}}, {'id': 'https://huggingface.co/papers/2501.19324', 'title': 'Reward-Guided Speculative Decoding for Efficient LLM Reasoning', 'url': 'https://huggingface.co/papers/2501.19324', 'abstract': '', 'score': 26, 'issue_id': 1995, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'ce2d414eedfb7a1e', 'authors': ['Baohao Liao', 'Yuhui Xu', 'Hanze Dong', 'Junnan Li', 'Christof Monz', 'Silvio Savarese', 'Doyen Sahoo', 'Caiming Xiong'], 'affiliations': ['Language Technology Lab, University of Amsterdam', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2501.19324.jpg', 'data': {'categories': [], 'emoji': '🤖', 'ru': {'title': 'Новый шаг в обучении больших языковых моделей', 'desc': 'В статье рассматривается новый подход к обучению LLM, который позволяет моделям лучше понимать контекст и улучшать качество генерации текста. Исследователи предложили метод, который использует более сложные архитектуры нейронных сетей для обработки больших объемов данных. Эксперименты показали, что предложенный метод значительно повышает точность и скорость работы моделей. Это открывает новые возможности для применения AI в различных областях, таких как обработка естественного языка и генерация контента.'}, 'en': {'title': 'Hybrid Networks: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': '优化数据处理，提升机器学习性能', 'desc': '这篇论文探讨了一种新的机器学习算法，旨在提高模型的准确性和效率。作者提出了一种创新的方法，通过优化数据预处理和特征选择来增强学习效果。实验结果表明，该算法在多个数据集上表现优于现有技术。最终，研究表明，改进的数据处理流程对机器学习模型的性能至关重要。'}}}, {'id': 'https://huggingface.co/papers/2501.18119', 'title': 'Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models', 'url': 'https://huggingface.co/papers/2501.18119', 'abstract': 'Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.', 'score': 12, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': 'd751c8a690173842', 'authors': ['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], 'affiliations': ['National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2501.18119.jpg', 'data': {'categories': ['#inference', '#graphs', '#transfer_learning', '#training', '#multimodal', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективная интеграция графов знаний и языковых моделей через квантованные коды', 'desc': 'Статья представляет двухэтапный подход к интеграции графов знаний с большими языковыми моделями. Авторы предлагают метод самоконтролируемого квантованного представления (SSQR) для сжатия структурных и семантических знаний графа в дискретные коды. Эти коды затем используются для создания инструкций для обучения языковых моделей. Эксперименты показывают превосходство SSQR над существующими методами и улучшение производительности моделей LLaMA2 и LLaMA3.1 на задачах, связанных с графами знаний.'}, 'en': {'title': 'Seamless Integration of Knowledge Graphs and Language Models', 'desc': 'This paper addresses the challenge of integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) by proposing a two-stage framework. The framework utilizes a self-supervised quantized representation (SSQR) method to convert KG structural and semantic information into discrete codes that resemble language tokens. By treating these codes as features for LLMs, the approach allows for a more efficient and effective integration of KGs with LLMs. Experimental results show that SSQR outperforms traditional methods, enabling better performance in tasks like KG link prediction and triple classification with significantly fewer tokens.'}, 'zh': {'title': '无缝整合知识图谱与大型语言模型', 'desc': '本论文探讨了知识图谱（KG）结构与自然语言之间的差距，提出了一种两阶段框架，以实现KG与大型语言模型（LLM）的有效整合。首先，提出了一种自监督量化表示（SSQR）方法，将KG的结构和语义知识压缩为离散代码（即令牌），使其与语言句子的格式对齐。接着，设计了KG指令跟随数据，将这些学习到的代码视为特征，直接输入到LLM中，从而实现无缝整合。实验结果表明，SSQR在无监督量化方法中表现优越，生成的代码更具可区分性，且经过微调的LLaMA2和LLaMA3.1在KG链接预测和三元组分类任务中表现出色，仅使用每个实体16个令牌，而不是传统方法中的数千个。'}}}, {'id': 'https://huggingface.co/papers/2501.19339', 'title': 'PixelWorld: Towards Perceiving Everything as Pixels', 'url': 'https://huggingface.co/papers/2501.19339', 'abstract': 'Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models\' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models\' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance.', 'score': 9, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '3e10b792328f7a4b', 'authors': ['Zhiheng Lyu', 'Xueguang Ma', 'Wenhu Chen'], 'affiliations': ['Department of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2501.19339.jpg', 'data': {'categories': ['#agi', '#benchmark', '#optimization', '#dataset', '#open_source', '#reasoning', '#multimodal', '#interpretability', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Единый пиксельный взгляд на мир: новая парадигма для ИИ', 'desc': 'В статье предлагается новый подход к обработке различных модальностей данных (текст, изображения, код и т.д.) в виде пикселей, названный PEAP (Perceive Everything as Pixels). Авторы представляют набор данных PixelWorld для оценки эффективности моделей в этом unified подходе. Результаты показывают, что PEAP превосходит базовые модели на мультимодальных данных, но выявляет снижение производительности в задачах рассуждения и кодирования. Исследование демонстрирует потенциал и ограничения восприятия пиксельных данных современными языковыми моделями.'}, 'en': {'title': 'Unifying Perception: Everything as Pixels', 'desc': "This paper introduces a new approach called 'Perceive Everything as Pixels' (PEAP), which aims to unify various input modalities like text, images, and diagrams into a single pixel-based format. The authors present PixelWorld, a novel evaluation suite designed to assess the performance of existing models when using this unified pixel input. Their experiments reveal that PEAP outperforms traditional token-based methods in multimodal datasets, although it highlights a decline in reasoning and coding abilities across models when using pixel inputs. The study concludes that while current models excel in pixel perception, there is still significant potential for enhancing their overall perceptual capabilities."}, 'zh': {'title': '统一感知：将一切视为像素', 'desc': '本论文提出了一种新的统一感知框架，称为“将一切视为像素”（PEAP），旨在将文本、表格、代码、图表和图像等多种输入形式统一为像素输入。我们引入了PixelWorld评估套件，以在像素空间中评估现有模型的性能。研究发现，PEAP在多模态数据集上优于基于标记的输入，显示出统一输入在消歧义方面的优势。同时，处理像素输入时，所有模型的推理和编码能力显著下降，表明需要增强基础模型的感知能力。'}}}, {'id': 'https://huggingface.co/papers/2501.14677', 'title': 'MatAnyone: Stable Video Matting with Consistent Memory Propagation', 'url': 'https://huggingface.co/papers/2501.14677', 'abstract': 'Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively integrates memory from the previous frame. This ensures semantic stability in core regions while preserving fine-grained details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, boosting matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust and accurate video matting results in diverse real-world scenarios, outperforming existing methods.', 'score': 6, 'issue_id': 2010, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 января', 'en': 'January 24', 'zh': '1月24日'}, 'hash': 'a9968478421ddc33', 'authors': ['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2501.14677.jpg', 'data': {'categories': ['#training', '#dataset', '#video'], 'emoji': '✂️', 'ru': {'title': 'MatAnyone: Точное выделение объектов на видео с помощью памяти и адаптивного обучения', 'desc': 'MatAnyone - это новый подход к выделению объектов на видео без вспомогательных данных. Он использует модуль памяти для адаптивного объединения информации из предыдущих кадров, обеспечивая стабильность основных областей и сохраняя детали на границах объектов. Авторы также создали новый большой набор данных для обучения и разработали стратегию, использующую данные сегментации для повышения стабильности. MatAnyone превосходит существующие методы в различных сценариях реального мира.'}, 'en': {'title': 'MatAnyone: Robust Video Matting with Memory Propagation', 'desc': 'The paper introduces MatAnyone, a new framework for video matting that does not require auxiliary inputs. It utilizes a memory-based approach with a memory propagation module that adapts memory from previous frames to maintain semantic consistency and detail. The authors also present a larger and more diverse dataset for training, along with a novel strategy that uses extensive segmentation data to enhance matting stability. Overall, MatAnyone achieves superior performance in complex video environments compared to existing methods.'}, 'zh': {'title': 'MatAnyone：视频抠图的新突破', 'desc': '本论文提出了一种名为MatAnyone的视频抠图框架，旨在解决复杂背景下的抠图问题。该方法基于记忆传播模块，通过区域自适应记忆融合，动态整合前一帧的记忆信息，从而确保核心区域的语义稳定性。为了提高训练的鲁棒性，研究团队构建了一个更大、更高质量且多样化的视频抠图数据集，并采用了一种新颖的训练策略，充分利用大规模分割数据。最终，MatAnyone在多种真实场景中展现出优越的抠图效果，超越了现有的方法。'}}}, {'id': 'https://huggingface.co/papers/2501.19399', 'title': 'Scalable-Softmax Is Superior for Attention', 'url': 'https://huggingface.co/papers/2501.19399', 'abstract': "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.", 'score': 6, 'issue_id': 2009, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '12ed1cad789702aa', 'authors': ['Ken M. Nakanishi'], 'affiliations': ['Institute for Physics of Intelligence, The University of Tokyo, Tokyo 113-0033, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2501.19399.jpg', 'data': {'categories': ['#architecture', '#training', '#long_context', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'SSMax: улучшение внимания трансформеров для длинных текстов', 'desc': 'Статья представляет новую функцию Scalable-Softmax (SSMax) для улучшения работы трансформеров с длинными контекстами. SSMax решает проблему уплощения распределения внимания при увеличении размера входного вектора. Эксперименты показывают, что модели с SSMax быстрее обучаются и лучше работают с длинными текстами. SSMax позволяет модели фокусироваться на ключевой информации даже в длинных контекстах.'}, 'en': {'title': 'Enhancing Attention with Scalable-Softmax for Better Context Handling', 'desc': "This paper addresses a limitation in Transformer-based language models where the Softmax function causes attention scores to flatten as the input size increases. This flattening reduces the model's ability to focus on important information, especially in longer contexts. The authors propose a new method called Scalable-Softmax (SSMax) that replaces the traditional Softmax function, allowing for better attention distribution and improved performance in long contexts. Experimental results show that SSMax enhances loss reduction during pretraining and enables better retrieval of key information, even for models that have already begun pretraining."}, 'zh': {'title': '可扩展Softmax：提升Transformer模型的注意力能力', 'desc': '本文提出了一种新的方法，称为可扩展Softmax（SSMax），旨在解决Transformer模型在处理长上下文时的注意力分布扁平化问题。传统的Softmax函数在输入向量增大时，最大元素趋近于零，导致模型无法有效地优先考虑关键信息。SSMax可以无缝集成到现有的Transformer架构中，实验结果表明，使用SSMax的模型在语言建模中不仅在预训练期间实现了更快的损失减少，还显著提高了在长上下文中的性能。通过分析注意力分数，SSMax使模型能够在长上下文中更好地关注关键信息，提升了模型的长度泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.04983', 'title': 'DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning', 'url': 'https://huggingface.co/papers/2411.04983', 'abstract': 'The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.', 'score': 6, 'issue_id': 1999, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'e72081596b626524', 'authors': ['Gaoyue Zhou', 'Hengkai Pan', 'Yann LeCun', 'Lerrel Pinto'], 'affiliations': ['Courant Institute, New York University', 'Meta-FAIR'], 'pdf_title_img': 'assets/pdf/title_img/2411.04983.jpg', 'data': {'categories': ['#cv', '#agents', '#reasoning', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DINO-WM: универсальная модель мира для планирования поведения без реконструкции', 'desc': 'Статья представляет новый метод моделирования визуальной динамики без реконструкции визуального мира - DINO World Model (DINO-WM). DINO-WM использует пространственные признаки патчей, предварительно обученные с помощью DINOv2, что позволяет ему учиться на офлайн-траекториях поведения, предсказывая будущие признаки патчей. Этот подход позволяет DINO-WM достигать наблюдаемых целей путем оптимизации последовательности действий, облегчая планирование поведения, независимое от задачи. Эксперименты показывают, что DINO-WM может генерировать поведенческие решения с нуля во время тестирования, демонстрируя сильные способности к обобщению по сравнению с предыдущими современными методами.'}, 'en': {'title': 'DINO-WM: Predicting the Future with Passive Data for Task-Agnostic Planning', 'desc': 'This paper introduces DINO World Model (DINO-WM), a novel approach for creating predictive models that can reason and plan across various tasks using only passive data. DINO-WM is designed to learn from offline trajectories without needing to reconstruct the visual environment, focusing instead on predicting future visual features. The model is capable of optimizing behavior at test time and supports task-agnostic reasoning, making it versatile for different applications. Experimental results show that DINO-WM can effectively generate solutions in diverse scenarios without relying on expert demonstrations or pre-learned models, showcasing its strong generalization abilities.'}, 'zh': {'title': 'DINO-WM：无任务依赖的世界模型', 'desc': '本文提出了一种新的世界模型DINO-WM，旨在通过被动数据进行推理和规划。DINO-WM具有三个关键特性：可以在离线收集的轨迹上进行训练，支持测试时行为优化，并促进任务无关的推理。该模型利用DINOv2预训练的空间补丁特征，通过预测未来的补丁特征来学习，从而实现观察目标的行为规划。实验结果表明，DINO-WM在多种任务中表现出色，能够在没有专家示范和奖励建模的情况下生成零-shot行为解决方案。'}}}, {'id': 'https://huggingface.co/papers/2501.18837', 'title': 'Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming', 'url': 'https://huggingface.co/papers/2501.18837', 'abstract': 'Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable.', 'score': 4, 'issue_id': 1996, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '62d14973b1140e58', 'authors': ['Mrinank Sharma', 'Meg Tong', 'Jesse Mu', 'Jerry Wei', 'Jorrit Kruthoff', 'Scott Goodfriend', 'Euan Ong', 'Alwin Peng', 'Raj Agarwal', 'Cem Anil', 'Amanda Askell', 'Nathan Bailey', 'Joe Benton', 'Emma Bluemke', 'Samuel R. Bowman', 'Eric Christiansen', 'Hoagy Cunningham', 'Andy Dau', 'Anjali Gopal', 'Rob Gilson', 'Logan Graham', 'Logan Howard', 'Nimit Kalra', 'Taesung Lee', 'Kevin Lin', 'Peter Lofgren', 'Francesco Mosconi', "Clare O'Hara", 'Catherine Olsson', 'Linda Petrini', 'Samir Rajani', 'Nikhil Saxena', 'Alex Silverstein', 'Tanya Singh', 'Theodore Sumers', 'Leonard Tang', 'Kevin K. Troy', 'Constantin Weisser', 'Ruiqi Zhong', 'Giulio Zhou', 'Jan Leike', 'Jared Kaplan', 'Ethan Perez'], 'affiliations': ['Safeguards Research Team, Anthropic'], 'pdf_title_img': 'assets/pdf/title_img/2501.18837.jpg', 'data': {'categories': ['#synthetic', '#training', '#architecture', '#dataset', '#security', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Конституционные Классификаторы: надежная защита языковых моделей', 'desc': 'Исследование представляет концепцию Конституционных Классификаторов - защитных механизмов для больших языковых моделей (LLM), обученных на синтетических данных с использованием правил, определяющих допустимый контент. Эти классификаторы эффективно противостоят универсальным методам обхода защиты, не позволяя извлекать вредоносную информацию из защищенных моделей. Эксперименты показали устойчивость классификаторов к различным атакам и их практическую применимость с минимальным влиянием на производительность. Исследование демонстрирует возможность эффективной защиты LLM от универсальных методов обхода при сохранении практической применимости.'}, 'en': {'title': 'Defending LLMs with Constitutional Classifiers', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to universal jailbreaks, which are strategies that allow users to bypass safety measures. The authors propose a solution called Constitutional Classifiers, which are trained on synthetic data generated by LLMs using natural language rules that define what content is allowed or restricted. Through extensive testing, these classifiers showed strong resistance to jailbreak attempts, effectively protecting the model without significantly impacting its performance. The findings suggest that it is possible to defend against such attacks while still ensuring the model can be used effectively in real-world applications.'}, 'zh': {'title': '宪法分类器：保护大型语言模型的安全', 'desc': '大型语言模型（LLMs）容易受到普遍越狱攻击，这种攻击可以绕过模型的安全措施，允许用户进行有害操作。为此，我们提出了宪法分类器，这是一种基于合成数据训练的安全措施，合成数据是通过自然语言规则（即宪法）提示LLMs生成的，规定了允许和限制的内容。在超过3000小时的红队测试中，没有红队成员找到能够从早期分类器保护的LLM中提取信息的普遍越狱方法。我们的研究表明，在保持实际部署可行性的同时，防御普遍越狱攻击是可行的。'}}}, {'id': 'https://huggingface.co/papers/2501.18841', 'title': 'Trading Inference-Time Compute for Adversarial Robustness', 'url': 'https://huggingface.co/papers/2501.18841', 'abstract': 'We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.', 'score': 3, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'f1e75e6b24f3e044', 'authors': ['Wojciech Zaremba', 'Evgenia Nitishinskaya', 'Boaz Barak', 'Stephanie Lin', 'Sam Toyer', 'Yaodong Yu', 'Rachel Dias', 'Eric Wallace', 'Kai Xiao', 'Johannes Heidecke', 'Amelia Glaese'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2501.18841.jpg', 'data': {'categories': ['#security', '#reasoning', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Больше вычислений - выше защита: повышение устойчивости ИИ к атакам', 'desc': 'Исследование посвящено влиянию увеличения вычислительных ресурсов во время вывода на устойчивость моделей рассуждений к состязательным атакам. Эксперименты показали, что увеличение вычислений при выводе улучшает робастность моделей к различным атакам. В большинстве случаев доля успешных атак стремится к нулю при росте вычислительных ресурсов. Результаты указывают на потенциал увеличения вычислений при выводе для повышения устойчивости больших языковых моделей к состязательным атакам.'}, 'en': {'title': 'Boosting Robustness: More Compute, Less Vulnerability', 'desc': "This paper investigates how increasing the amount of compute used during inference can enhance the robustness of reasoning models, specifically OpenAI's o1-preview and o1-mini, against adversarial attacks. The authors find that as the compute increases, the success rate of these attacks generally decreases, indicating improved model resilience. Notably, this improvement occurs without any adversarial training, simply by allowing the models to utilize more resources for reasoning tasks. The study also examines new types of attacks and scenarios where increased compute does not lead to better reliability, providing insights into potential solutions."}, 'zh': {'title': '增加推理计算，提升模型鲁棒性', 'desc': '本研究探讨了在推理模型中增加推理时间计算对其抵御对抗攻击的影响。我们发现，随着推理时间计算的增加，模型的鲁棒性得到了提升。大多数情况下，攻击成功的模型样本比例随着测试时间计算的增加而趋近于零。我们的结果表明，推理时间计算有潜力提高大型语言模型的对抗鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2501.18052', 'title': 'SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2501.18052', 'abstract': "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https://github.com/cywinski/SAeUron.", 'score': 2, 'issue_id': 2011, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 января', 'en': 'January 29', 'zh': '1月29日'}, 'hash': 'd94056a77d806ada', 'authors': ['Bartosz Cywiński', 'Kamil Deja'], 'affiliations': ['IDEAS NCBR', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2501.18052.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#security', '#architecture', '#ethics', '#training', '#benchmark'], 'emoji': '🧹', 'ru': {'title': 'Чистка нейросетей: SAeUron удаляет нежелательные концепции из диффузионных моделей', 'desc': 'SAeUron - это новый метод удаления нежелательных концепций в диффузионных моделях для генерации изображений по тексту. Он использует разреженные автоэнкодеры (SAE) для выделения интерпретируемых признаков, соответствующих конкретным концепциям. Метод позволяет точно вмешиваться в активации модели для блокировки целевого контента при сохранении общей производительности. SAeUron показывает лучшие результаты по сравнению с другими методами и может удалять несколько концепций одновременно.'}, 'en': {'title': 'SAeUron: Safeguarding Diffusion Models with Sparse Autoencoders', 'desc': "This paper presents SAeUron, a new method designed to improve the safety of text-to-image diffusion models by removing unwanted concepts. It utilizes sparse autoencoders (SAEs) to learn and identify specific features from the model's activations, allowing for targeted interventions. The method enables precise control over the model's outputs while maintaining its overall performance. Evaluation shows that SAeUron outperforms existing techniques in unlearning tasks and effectively reduces the risk of generating harmful content."}, 'zh': {'title': 'SAeUron：去除不良内容的新方法', 'desc': '扩散模型虽然强大，但可能会生成有害或不良内容，带来伦理和安全问题。最近的机器遗忘方法提供了潜在的解决方案，但通常缺乏透明性，难以理解对基础模型的更改。我们提出了一种新方法SAeUron，利用稀疏自编码器（SAE）学习的特征来去除文本到图像扩散模型中的不必要概念。通过在多个去噪时间步的激活上无监督训练SAE，我们捕捉到与特定概念对应的稀疏和可解释特征，从而实现精确干预。'}}}, {'id': 'https://huggingface.co/papers/2501.18804', 'title': 'Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion', 'url': 'https://huggingface.co/papers/2501.18804', 'abstract': 'Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.', 'score': 2, 'issue_id': 2010, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '32db517ad974401b', 'authors': ['Vitor Guizilini', 'Muhammad Zubair Irshad', 'Dian Chen', 'Greg Shakhnarovich', 'Rares Ambrus'], 'affiliations': ['Toyota Research Institute (TRI)', 'Toyota Technological Institute at Chicago (TTIC)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18804.jpg', 'data': {'categories': ['#training', '#3d', '#diffusion', '#architecture', '#benchmark', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'Генерация 3D сцен из разных ракурсов с помощью диффузионной модели', 'desc': 'Статья представляет MVGD - архитектуру на основе диффузии для генерации изображений и карт глубины с новых ракурсов. Метод использует райкарты для обогащения визуальных признаков пространственной информацией и направления генерации. Ключевой аспект - многозадачная генерация изображений и карт глубины с использованием обучаемых эмбеддингов задач. Модель обучена на более чем 60 миллионах мультиракурсных примеров и показывает современные результаты в задачах синтеза новых ракурсов и оценки глубины.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with Direct Pixel-Level Generation', 'desc': 'This paper presents MVGD, a new diffusion-based model for generating images and depth maps from multiple input views in 3D scene reconstruction. Unlike traditional methods that rely on intermediate 3D representations, MVGD directly produces pixel-level outputs, enhancing visual features with spatial information through raymap conditioning. The model employs multi-task learning, using task embeddings to effectively guide the generation process for both images and depth maps. Trained on a vast dataset of over 60 million samples, MVGD achieves state-of-the-art performance in novel view synthesis and depth estimation tasks.'}, 'zh': {'title': 'MVGD：从多视角直接生成图像与深度图的创新方法', 'desc': '本文提出了一种名为MVGD的扩散基础架构，能够直接从多个视角生成图像和深度图。该方法通过光线图条件化，增强了视觉特征并引导生成过程。我们采用多任务生成技术，同时生成图像和深度图，并使用可学习的任务嵌入来优化扩散过程。经过在超过6000万多视角样本上的训练，我们在多个基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2501.18965', 'title': 'The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training', 'url': 'https://huggingface.co/papers/2501.18965', 'abstract': 'We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms. Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules.', 'score': 2, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'a136293a2241150e', 'authors': ['Fabian Schaipp', 'Alexander Hägele', 'Adrien Taylor', 'Umut Simsekli', 'Francis Bach'], 'affiliations': ['EPFL, Lausanne, Switzerland', 'Inria, Departement dInformatique de lEcole Normale Superieure, PSL Research University, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2501.18965.jpg', 'data': {'categories': ['#transfer_learning', '#math', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Оптимизация графиков обучения для больших языковых моделей', 'desc': 'В статье исследуются графики изменения скорости обучения для больших моделей машинного обучения. Авторы обнаружили неожиданное сходство этих графиков с теоретическими границами из теории невыпуклой оптимизации. Они предлагают новый метод настройки скорости обучения, основанный на этом наблюдении. Применение метода позволило улучшить результаты обучения языковых моделей типа Llama размером 124M и 210M параметров.'}, 'en': {'title': 'Optimizing Learning Rates: Bridging Theory and Practice', 'desc': 'This paper explores the relationship between learning-rate schedules in large model training and concepts from non-smooth convex optimization theory. It establishes a performance bound for a constant learning-rate schedule with a linear cooldown, highlighting the practical advantages of this approach. The authors demonstrate that the alignment between theoretical optimization and practical training can be leveraged for better learning-rate tuning. By optimizing the learning-rate schedule, they achieve significant improvements in training large Llama-type models.'}, 'zh': {'title': '优化学习率调度，提升大模型训练效果', 'desc': '本文探讨了大模型训练中的学习率调度与非光滑凸优化理论中的性能界限之间的相似性。我们提供了一个线性冷却的常数调度的界限，特别是冷却的实际好处在于没有对数项的影响。进一步地，我们展示了优化理论与实践之间的紧密联系可以用于学习率调优：通过延长调度以继续训练并使用最佳学习率，我们在训练124M和210M的Llama类型模型时取得了显著的改进。最后，我们还展示了在不同调度之间转移最佳学习率的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.18753', 'title': 'INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation', 'url': 'https://huggingface.co/papers/2501.18753', 'abstract': 'Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability.', 'score': 2, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '000663cf445862a1', 'authors': ['Jian Hu', 'Zixu Cheng', 'Shaogang Gong'], 'affiliations': ['Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2501.18753.jpg', 'data': {'categories': ['#dataset', '#healthcare', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Адаптивная сегментация изображений с помощью интеллектуального отбора промптов', 'desc': 'Статья представляет новый метод сегментации изображений под названием INT (Instance-specific Negative Mining for Task-Generic Promptable Segmentation). Этот подход направлен на улучшение генерации промптов, специфичных для конкретных экземпляров изображений, путем адаптивного уменьшения влияния нерелевантных знаний и усиления наиболее вероятных. INT состоит из двух компонентов: генерации промптов, специфичных для экземпляров, и генерации семантической маски. Метод был проверен на шести наборах данных, включая камуфлированные объекты и медицинские изображения, демонстрируя эффективность, надежность и масштабируемость.'}, 'en': {'title': 'Enhancing Image Segmentation with Smart Prompting', 'desc': 'This paper presents a new method called Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT) to improve image segmentation using a single task description. The method addresses the challenge of Vision-Language Models (VLMs) struggling to generalize to certain image instances, which can lead to poor segmentation results. INT works by selectively reducing the impact of irrelevant information while enhancing the use of relevant prior knowledge through a process called negative mining. The effectiveness of INT is validated across six diverse datasets, showing its ability to produce accurate and robust segmentation results.'}, 'zh': {'title': '实例特定负采样优化图像分割', 'desc': '这篇论文提出了一种新的方法，称为实例特定负采样（INT），用于任务通用的可提示图像分割。INT的核心思想是自适应地减少无关的先验知识的影响，同时增加最有可能的先验知识的使用，以优化实例特定提示的生成。该方法包括两个主要部分：实例特定提示生成和语义掩码生成，确保每个图像实例的分割与提示的语义相匹配。通过在六个数据集上的验证，INT展示了其有效性、鲁棒性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2501.18128', 'title': 'Unraveling the Capabilities of Language Models in News Summarization', 'url': 'https://huggingface.co/papers/2501.18128', 'abstract': "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.", 'score': 2, 'issue_id': 2000, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '1c3f3a16953a5a59', 'authors': ['Abdurrahman Odabaşı', 'Göksel Biricik'], 'affiliations': ['Department of Computer Engineering, Turkish-German University, 34820, Istanbul, Turkiye', 'Department of Computer Engineering, Yıldız Technical University, 34220, Istanbul, Turkiye'], 'pdf_title_img': 'assets/pdf/title_img/2501.18128.jpg', 'data': {'categories': ['#survey', '#multilingual', '#small_models', '#benchmark', '#transfer_learning'], 'emoji': '📰', 'ru': {'title': 'Маленькие модели бросают вызов гигантам в суммаризации новостей', 'desc': 'В этой работе проводится комплексное сравнение 20 современных языковых моделей, с акцентом на меньшие модели, для задачи суммаризации новостей. Исследование охватывает обучение в режимах zero-shot и few-shot на трех различных наборах данных, используя автоматические метрики, человеческую оценку и LLM в качестве судьи. Интересно, что включение демонстрационных примеров в режиме few-shot не улучшило производительность моделей, а в некоторых случаях даже ухудшило качество генерируемых сводок. Результаты показали превосходство GPT-3.5-Turbo и GPT-4, но также выявили перспективные открытые модели, такие как Qwen1.5-7B и SOLAR-10.7B-Instruct-v1.0.'}, 'en': {'title': 'Benchmarking News Summarization: Small Models Can Compete!', 'desc': 'This paper benchmarks 20 recent language models specifically for the task of news summarization, emphasizing smaller models. It evaluates their performance in zero-shot and few-shot learning scenarios across three different datasets with varying writing styles. The study reveals that providing demonstration examples in few-shot settings often does not improve, and can even degrade, the quality of summaries due to the inadequacy of reference summaries. Notably, while larger models like GPT-3.5-Turbo and GPT-4 excel, several smaller models also show competitive performance, suggesting they could be viable alternatives for summarization tasks.'}, 'zh': {'title': '小模型在新闻摘要中的潜力与挑战', 'desc': '本研究对20种最新的语言模型进行了全面的基准测试，重点关注较小的模型在新闻摘要任务中的表现。我们系统地测试了这些模型在不同风格的新闻文章摘要中的能力和有效性，使用了三种不同的数据集。研究发现，在少量示例学习的设置中，提供示例并未提升模型的性能，反而在某些情况下导致生成摘要的质量下降。这主要是由于参考摘要的质量较差，影响了模型的表现，同时我们的研究结果显示，GPT-3.5-Turbo和GPT-4在性能上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2404.07097', 'title': 'Fast Encoder-Based 3D from Casual Videos via Point Track Processing', 'url': 'https://huggingface.co/papers/2404.07097', 'abstract': 'This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.   Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation. TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\\%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time.', 'score': 1, 'issue_id': 1999, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'a526ae197fe3a8c7', 'authors': ['Yoni Kasten', 'Wuyue Lu', 'Haggai Maron'], 'affiliations': ['NVIDIA Research', 'Simon Fraser University', 'Technion'], 'pdf_title_img': 'assets/pdf/title_img/2404.07097.jpg', 'data': {'categories': ['#3d', '#training', '#cv', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Эффективная 3D реконструкция из видео с помощью глубокого обучения', 'desc': 'Статья представляет TracksTo4D - новый подход к реконструкции 3D структур из видео с динамическим содержанием. Метод использует нейронную сеть, обученную без учителя на 2D треках точек, извлеченных из обычных видео. TracksTo4D позволяет восстанавливать облако точек и положение камеры за один проход сети, значительно сокращая время вычислений по сравнению с существующими методами. Архитектура сети учитывает симметрии входных данных и предполагает низкоранговое представление паттернов движения.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction from Casual Videos', 'desc': 'This paper introduces TracksTo4D, a novel machine learning approach for reconstructing 3D structures from casual videos. Unlike traditional methods that require extensive optimization or are limited to specific video types, TracksTo4D processes 2D point tracks directly in a single, efficient feed-forward pass. The architecture is designed to leverage the symmetries in point track data and utilizes low-rank approximations to represent movement patterns effectively. Trained unsupervised on a dataset of casual videos, TracksTo4D achieves high accuracy in reconstructing temporal point clouds and camera positions while significantly reducing processing time.'}, 'zh': {'title': '高效重建3D结构，TracksTo4D引领新潮流', 'desc': '本文解决了从动态内容视频重建3D结构的长期挑战。现有方法无法处理普通相机录制的随意视频，或需要较长的优化时间。我们提出了一种基于学习的方法TracksTo4D，通过单次高效的前馈传递，从随意视频中推断3D结构和相机位置。该方法直接处理2D点轨迹，并设计了专门的架构，能够在无监督的情况下进行训练，显著提高了重建效率。'}}}, {'id': 'https://huggingface.co/papers/2502.01362', 'title': 'Inverse Bridge Matching Distillation', 'url': 'https://huggingface.co/papers/2502.01362', 'abstract': 'Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose a novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in a one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on a wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup.', 'score': 18, 'issue_id': 2045, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '061049a23278b0f6', 'authors': ['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], 'affiliations': ['Skolkovo Institute of Science and Technology', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01362.jpg', 'data': {'categories': ['#inference', '#optimization', '#cv', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных мостов: искусство эффективной дистилляции', 'desc': 'Статья представляет новый метод дистилляции для моделей диффузионного моста (DBM). Этот подход позволяет ускорить вывод DBM от 4 до 100 раз, сохраняя или даже улучшая качество генерации. Метод основан на формулировке обратного сопоставления моста и может применяться как к условным, так и к безусловным DBM. Техника была успешно протестирована на различных задачах, включая суперразрешение и восстановление JPEG.'}, 'en': {'title': 'Accelerating Diffusion Bridge Models with Innovative Distillation Techniques', 'desc': 'This paper introduces a new method to improve the speed and practicality of diffusion bridge models (DBMs), which are used for tasks like image translation. The authors present a distillation technique that leverages inverse bridge matching to create a more efficient training process. This method allows for the distillation of both conditional and unconditional DBMs, using only corrupted images, and enables one-step generation. The results show that their approach can significantly speed up inference times by up to 100 times while maintaining or even enhancing the quality of generated images compared to the original models.'}, 'zh': {'title': '加速扩散桥模型，提升生成质量！', 'desc': '扩散桥模型（DBMs）是一种有前景的扩展，适用于图像到图像的转换。然而，DBMs在推理时速度较慢，这是现代扩散和流模型普遍面临的问题。为了解决这个问题，我们提出了一种基于逆桥匹配的蒸馏技术，并推导出可行的目标来实际解决它。我们的蒸馏方法能够同时处理条件和无条件的DBMs，并且只使用损坏的图像进行训练，从而显著加快推理速度。'}}}, {'id': 'https://huggingface.co/papers/2502.02492', 'title': 'VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models', 'url': 'https://huggingface.co/papers/2502.02492', 'abstract': "Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/", 'score': 15, 'issue_id': 2042, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '33581479f8c6ed9f', 'authors': ['Hila Chefer', 'Uriel Singer', 'Amit Zohar', 'Yuval Kirstain', 'Adam Polyak', 'Yaniv Taigman', 'Lior Wolf', 'Shelly Sheynin'], 'affiliations': ['GenAI, Meta', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.02492.jpg', 'data': {'categories': ['#video'], 'emoji': '🎬', 'ru': {'title': 'VideoJAM: Реалистичное движение в генеративных видеомоделях', 'desc': 'Статья представляет VideoJAM - новый подход к генерации видео, решающий проблему недостаточной реалистичности движения в существующих моделях. Авторы предлагают обучать модель совместному представлению внешнего вида и движения, а также вводят механизм Inner-Guidance для улучшения когерентности движения при генерации. VideoJAM может быть применен к любой видеомодели без существенных изменений и показывает лучшие результаты по сравнению с существующими решениями. Исследование демонстрирует, что интеграция внешнего вида и движения улучшает как визуальное качество, так и согласованность генерируемого видео.'}, 'en': {'title': 'Enhancing Video Generation with Motion Coherence', 'desc': 'This paper addresses the challenges faced by generative video models in accurately capturing real-world motion and dynamics. The authors identify that traditional pixel reconstruction methods prioritize visual appearance over motion coherence, leading to less realistic video outputs. To overcome this, they propose VideoJAM, a framework that integrates a motion prior into video generation by learning a combined representation of appearance and motion. By extending the training objective and introducing a dynamic guidance mechanism during inference, VideoJAM significantly improves motion coherence and visual quality, outperforming existing models without requiring changes to training data or model architecture.'}, 'zh': {'title': 'VideoJAM：提升视频生成的运动一致性与视觉质量', 'desc': '尽管生成视频模型在最近取得了巨大进展，但仍然难以捕捉真实世界的运动和动态。本文提出了VideoJAM框架，通过引入有效的运动先验，帮助视频生成器学习联合的外观-运动表示。该框架在训练过程中扩展了目标，预测生成的像素及其对应的运动，并在推理阶段引入了内部引导机制，以实现一致的运动生成。VideoJAM在运动一致性方面达到了最先进的性能，同时提升了生成视频的视觉质量，表明外观和运动可以互为补充，合理整合后能增强视频生成的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.01718', 'title': 'ACECODER: Acing Coder RL via Automated Test-Case Synthesis', 'url': 'https://huggingface.co/papers/2502.01718', 'abstract': 'Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.', 'score': 13, 'issue_id': 2041, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'b5b43fe7221df9d8', 'authors': ['Huaye Zeng', 'Dongfu Jiang', 'Haozhe Wang', 'Ping Nie', 'Xiaotong Chen', 'Wenhu Chen'], 'affiliations': ['HKUST', 'Independent Researcher', 'Netmind.AI', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2502.01718.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Революция в обучении моделей кода: мощь RL и автоматизированных тест-кейсов', 'desc': 'В этой статье представлен новый подход к обучению моделей для генерации кода с использованием обучения с подкреплением (RL). Авторы разработали пайплайн для создания пар (вопрос, тест-кейсы) из существующих кодовых данных, которые затем используются для обучения моделей вознаграждения. Применение этого метода привело к значительным улучшениям производительности моделей на различных бенчмарках, включая HumanEval и MBPP. Результаты исследования показывают большой потенциал обучения с подкреплением в области моделей для генерации кода.'}, 'en': {'title': 'Unlocking the Power of Reinforcement Learning in Coder Models', 'desc': 'This paper explores the use of reinforcement learning (RL) to improve coder models, which have primarily relied on supervised fine-tuning (SFT). The authors introduce a method for generating large-scale test-case pairs from existing code, which helps create reliable reward signals for training. By employing a preference-based reward model using the Bradley-Terry loss, they achieve significant performance gains in various coding benchmarks. The results demonstrate that RL can substantially enhance the capabilities of coder models, showcasing its untapped potential in this domain.'}, 'zh': {'title': '强化学习提升代码模型的潜力', 'desc': '本文探讨了在代码模型训练中使用强化学习（RL）的潜力，尤其是在缺乏可靠奖励数据的情况下。我们设计了一种自动化的大规模测试用例合成管道，以生成大量（问题，测试用例）对，从而增强代码模型的训练。通过使用这些测试用例，我们构建了基于通过率的偏好对，并利用Bradley-Terry损失训练奖励模型。实验结果表明，使用强化学习后，模型在多个基准测试上均有显著提升，展示了强化学习在代码模型中的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.02584', 'title': 'QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search', 'url': 'https://huggingface.co/papers/2502.02584', 'abstract': 'Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.', 'score': 8, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'f2d3938d4ad71761', 'authors': ['Zongyu Lin', 'Yao Tang', 'Xingcheng Yao', 'Da Yin', 'Ziniu Hu', 'Yizhou Sun', 'Kai-Wei Chang'], 'affiliations': ['Shanghai Jiaotong University, Shanghai, China', 'University of California, Los Angeles, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.02584.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#inference', '#reasoning', '#agents', '#training', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'QLASS: Пошаговое обучение языковых агентов для повышения эффективности', 'desc': 'Статья представляет новый метод QLASS для обучения языковых агентов. QLASS использует пошаговую оценку Q-значений для генерации аннотаций и улучшения промежуточного обучения. Метод вводит дерево рассуждений и моделирование вознаграждений процесса для эффективного пошагового руководства. QLASS позволяет языковым агентам лучше адаптироваться к долгосрочным целям, значительно улучшая производительность при решении сложных интерактивных задач.'}, 'en': {'title': 'Enhancing Language Agents with Stepwise Q-Guidance', 'desc': 'This paper introduces QLASS, a method designed to enhance the performance of language agents in complex tasks by providing stepwise guidance through Q-value estimation. Traditional approaches rely on outcome reward models, which can result in sub-optimal decision-making due to the lack of intermediate feedback. QLASS addresses this by creating a reasoning tree that models rewards at each step, allowing agents to learn from their interactions more effectively. The results show that QLASS can maintain strong performance with less annotated data, proving its efficiency in environments with limited supervision.'}, 'zh': {'title': 'QLASS：提升语言代理的决策能力', 'desc': '本文提出了一种名为QLASS的模型，用于改进语言代理在复杂交互任务中的表现。QLASS通过逐步估计Q值来自动生成中间交互的注释，从而为语言代理提供有效的指导。与传统的结果奖励模型不同，QLASS引入了推理树和过程奖励建模，使得每一步都有明确的指导。实验结果表明，即使在标注数据减少的情况下，QLASS仍能保持良好的性能，展示了其在有限监督下的高效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01941', 'title': 'Can LLMs Maintain Fundamental Abilities under KV Cache Compression?', 'url': 'https://huggingface.co/papers/2502.01941', 'abstract': "This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of 17.4%-43.3%. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only 9.67%-25.53% performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves 9%-18% performance improvements on long-context generation tasks under aggressive compression ratios.", 'score': 7, 'issue_id': 2041, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '1352d78ee18eadfa', 'authors': ['Xiang Liu', 'Zhenheng Tang', 'Hong Chen', 'Peijie Dong', 'Zeyu Li', 'Xiuze Zhou', 'Bo Li', 'Xuming Hu', 'Xiaowen Chu'], 'affiliations': ['The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China', 'The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.01941.jpg', 'data': {'categories': ['#optimization', '#inference', '#long_context', '#training'], 'emoji': '🧠', 'ru': {'title': 'Сжатие KV-кэша в LLM: баланс между эффективностью и производительностью', 'desc': 'Статья исследует влияние методов сжатия KV-кэша на фундаментальные возможности больших языковых моделей (LLM). Авторы провели комплексный эмпирический анализ различных методов сжатия на разнообразных задачах, включая общие знания, здравый смысл, арифметические рассуждения и генерацию кода. Результаты показали, что сжатие KV-кэша может значительно ухудшать производительность модели, особенно в задачах арифметических рассуждений. На основе анализа авторы предложили новый метод ShotKV, который по-разному обрабатывает фазы предзаполнения и декодирования, сохраняя семантическую целостность.'}, 'en': {'title': 'Optimizing KV Cache Compression for Enhanced LLM Performance', 'desc': "This paper explores how compressing the KV cache in large language models (LLMs) affects their performance on various tasks. While compression can reduce memory usage, it may also lead to a decline in the model's ability to perform tasks like arithmetic reasoning and code generation. The study finds that different compression methods impact tasks differently, with some methods causing significant performance drops, especially in arithmetic reasoning. To address these issues, the authors introduce ShotKV, a new compression technique that improves performance on long-context tasks while preserving important semantic information."}, 'zh': {'title': 'KV缓存压缩对大型语言模型能力的影响研究', 'desc': '本文研究了大型语言模型（LLMs）中一个未被充分探讨的挑战：KV缓存压缩方法对LLMs基本能力的影响。虽然现有方法在长上下文基准测试中取得了令人印象深刻的压缩比，但它们对核心模型能力的影响仍然缺乏研究。我们的实证研究评估了多种KV缓存压缩方法在不同任务上的表现，包括世界知识、常识推理、算术推理、代码生成、安全性以及长上下文理解和生成。分析结果显示，KV缓存压缩方法在特定任务上表现出性能下降，尤其是算术推理任务对激进压缩特别敏感，性能下降幅度达到17.4%-43.3%。'}}}, {'id': 'https://huggingface.co/papers/2502.02508', 'title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search', 'url': 'https://huggingface.co/papers/2502.02508', 'abstract': "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.", 'score': 5, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '80bd687783bd609b', 'authors': ['Maohao Shen', 'Guangtao Zeng', 'Zhenting Qi', 'Zhang-Wei Hong', 'Zhenfang Chen', 'Wei Lu', 'Gregory Wornell', 'Subhro Das', 'David Cox', 'Chuang Gan'], 'affiliations': ['Harvard', 'MIT', 'MIT-IBM Watson AI Lab, IBM Research', 'Singapore University of Technology and Design', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.02508.jpg', 'data': {'categories': ['#small_models', '#open_source', '#training', '#reasoning', '#math', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Satori: LLM с внутренним поиском для улучшенного рассуждения', 'desc': 'Исследователи представили новый подход к улучшению способностей больших языковых моделей (LLM) к рассуждению. Они разработали метод Chain-of-Action-Thought (COAT), который позволяет модели проводить самоанализ и исследовать новые стратегии решения задач. Процесс обучения включает два этапа: настройку формата на небольшом масштабе и масштабное самосовершенствование с использованием обучения с подкреплением. В результате была создана 7-миллиардная модель Satori, показавшая отличные результаты в задачах математического рассуждения и обобщения на новые области.'}, 'en': {'title': 'Empowering LLMs with Internalized Reasoning through COAT', 'desc': 'This paper discusses the development of Satori, a large language model (LLM) that enhances reasoning capabilities through a novel approach called Chain-of-Action-Thought (COAT). The authors propose a two-stage training process that first tunes the model to internalize COAT reasoning and then employs reinforcement learning for self-improvement. By internalizing the search capabilities typically guided by an external verifier, Satori can perform complex reasoning tasks more effectively. The results show that Satori not only excels in mathematical reasoning benchmarks but also generalizes well to other tasks, indicating its robust performance.'}, 'zh': {'title': '内化搜索能力，提升推理能力！', 'desc': '大型语言模型（LLMs）在多个领域展示了出色的推理能力。研究表明，增加测试时的计算可以提升LLMs的推理能力，通常需要在推理时进行大量采样，并由外部LLM验证器指导。本文提出了一个新问题：能否将搜索能力内化，以根本性地增强单个LLM的推理能力？我们提出了行动思维链（COAT）推理方法，并设计了一个两阶段的训练范式，以实现LLM的自我改进和自我反思。'}}}, {'id': 'https://huggingface.co/papers/2502.01720', 'title': 'Generating Multi-Image Synthetic Data for Text-to-Image Customization', 'url': 'https://huggingface.co/papers/2502.01720', 'abstract': 'Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image quality. We propose a simple approach that addresses both limitations. We first leverage existing text-to-image models and 3D datasets to create a high-quality Synthetic Customization Dataset (SynCD) consisting of multiple images of the same object in different lighting, backgrounds, and poses. We then propose a new encoder architecture based on shared attention mechanisms that better incorporate fine-grained visual details from input images. Finally, we propose a new inference technique that mitigates overexposure issues during inference by normalizing the text and image guidance vectors. Through extensive experiments, we show that our model, trained on the synthetic dataset with the proposed encoder and inference algorithm, outperforms existing tuning-free methods on standard customization benchmarks.', 'score': 2, 'issue_id': 2043, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd249f21cea90b464', 'authors': ['Nupur Kumari', 'Xi Yin', 'Jun-Yan Zhu', 'Ishan Misra', 'Samaneh Azadi'], 'affiliations': ['Carnegie Mellon University', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2502.01720.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#dataset', '#architecture', '#cv', '#inference'], 'emoji': '🎨', 'ru': {'title': 'Улучшение кастомизации моделей text-to-image с помощью синтетических данных и новой архитектуры', 'desc': 'Статья представляет новый подход к кастомизации моделей text-to-image. Авторы создали синтетический набор данных SynCD с множественными изображениями объектов в разных условиях. Они предложили новую архитектуру энкодера, основанную на механизмах разделяемого внимания, для лучшего учета визуальных деталей. Также был разработан метод вывода, нормализующий векторы текстового и изображенческого руководства для устранения проблем переэкспозиции.'}, 'en': {'title': 'Enhancing Customization in Text-to-Image Models with Synthetic Datasets', 'desc': 'This paper presents a novel approach to customize text-to-image models, allowing users to generate images of custom concepts in various settings. The authors create a Synthetic Customization Dataset (SynCD) using 3D datasets, which includes multiple images of the same object under different conditions. They introduce a new encoder architecture that utilizes shared attention mechanisms to capture detailed visual information effectively. Additionally, a new inference technique is proposed to address overexposure issues, resulting in improved image quality compared to existing methods.'}, 'zh': {'title': '高质量定制化：突破文本到图像生成的限制', 'desc': '本文提出了一种文本到图像模型的定制化方法，允许用户在未见过的环境中生成自定义概念。现有方法通常依赖于昂贵的测试时优化或在单图像数据集上训练编码器，导致图像质量较差。我们的方法利用现有的文本到图像模型和3D数据集，创建了一个高质量的合成定制数据集（SynCD），包含同一对象在不同光照、背景和姿势下的多张图像。通过新的编码器架构和推理技术，我们的模型在标准定制基准测试中表现优于现有的无调优方法。'}}}, {'id': 'https://huggingface.co/papers/2502.01237', 'title': 'The Differences Between Direct Alignment Algorithms are a Blur', 'url': 'https://huggingface.co/papers/2502.01237', 'abstract': 'Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the beta parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms.', 'score': 79, 'issue_id': 2022, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '18ba45e237fff5e1', 'authors': ['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.01237.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Прямое выравнивание: простой путь к улучшению языковых моделей', 'desc': 'В статье рассматриваются алгоритмы прямого выравнивания (DAA) как альтернатива обучению с подкреплением в контексте выравнивания языковых моделей. Авторы классифицируют DAA по типу функции потерь, используемым наградам и необходимости предварительного обучения. Исследование показывает, что двухэтапные методы превосходят одноэтапные, а ключевым фактором эффективности является использование попарных, а не поточечных целевых функций. Результаты подчеркивают важность тщательной оценки при сравнении алгоритмов выравнивания языковых моделей.'}, 'en': {'title': 'Simplifying Language Model Alignment with Direct Optimization', 'desc': 'This paper introduces Direct Alignment Algorithms (DAAs) as a simpler alternative to traditional Reinforcement Learning from Human Feedback (RLHF) methods. DAAs optimize language model alignment directly, avoiding the complexities of reinforcement learning and reward modeling. The study compares one-stage and two-stage methods, revealing that incorporating a Supervised Fine-Tuning (SFT) phase significantly enhances performance. The findings emphasize the importance of the choice between pairwise and pointwise ranking losses in achieving effective alignment, cautioning against hasty conclusions about algorithm superiority.'}, 'zh': {'title': '优化对齐算法，提升模型性能！', 'desc': '直接对齐算法（DAAs）通过直接优化策略来简化语言模型的对齐，取代了人类反馈强化学习中的强化学习和奖励建模。DAAs可以根据其排名损失（成对与点对）和使用的奖励类型进行分类。研究表明，一阶段方法的表现不如两阶段方法，因此我们引入了显式的监督微调阶段，并在单阶段的ORPO和ASFT中加入了控制偏好优化强度的beta参数。这些改进使得它们在Alpaca Eval 2中的表现得到了显著提升，表明选择成对或点对目标是关键因素，而不是具体的隐式奖励或损失函数。'}}}, {'id': 'https://huggingface.co/papers/2502.01061', 'title': 'OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models', 'url': 'https://huggingface.co/papers/2502.01061', 'abstract': 'End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)', 'score': 74, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '56b819a66e336562', 'authors': ['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.01061.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#training', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'OmniHuman: универсальная модель для генерации реалистичных видео с людьми', 'desc': 'OmniHuman - это новая модель на основе Diffusion Transformer для генерации реалистичных видео с людьми. Она использует смешанные условия движения при обучении, что позволяет масштабировать данные и улучшить качество генерации. Модель поддерживает различные типы портретов, взаимодействие с объектами и сложные позы тела. OmniHuman может генерировать видео на основе аудио, видео или комбинированных сигналов управления.'}, 'en': {'title': 'OmniHuman: Revolutionizing Realistic Human Animation Generation', 'desc': 'The paper presents OmniHuman, a new framework for generating realistic human animations from audio inputs. It utilizes a Diffusion Transformer architecture that enhances the training process by incorporating motion-related conditions, allowing for better scalability in video generation. OmniHuman is designed to handle various types of human portraits and interactions, producing high-quality videos that can depict talking, singing, and complex body movements. This approach not only improves the realism of the generated videos but also increases flexibility by supporting multiple input modalities such as audio and video.'}, 'zh': {'title': 'OmniHuman：灵活真实的人类动画生成', 'desc': '本文提出了一种名为OmniHuman的框架，旨在提升人类动画生成的质量和灵活性。该框架基于扩散变换器，通过在训练阶段混合与运动相关的条件来扩展数据规模。OmniHuman支持多种人像内容和不同的驱动模式，如音频驱动和视频驱动，能够生成高度真实的人类视频。与现有方法相比，OmniHuman不仅生成更真实的视频，还提供了更大的输入灵活性。'}}}, {'id': 'https://huggingface.co/papers/2502.01456', 'title': 'Process Reinforcement through Implicit Rewards', 'url': 'https://huggingface.co/papers/2502.01456', 'abstract': "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.", 'score': 41, 'issue_id': 2019, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '9d62c40e4bafac91', 'authors': ['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Shanghai Jiaotong University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.01456.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'PRIME: Эффективное обучение ИИ-моделей с неявными процессными наградами', 'desc': 'Эта статья представляет новый метод обучения с подкреплением для больших языковых моделей, называемый PRIME. Он использует неявные процессные награды, что позволяет обновлять модели вознаграждения процесса в режиме онлайн, используя только развертывания политики и метки результатов. PRIME решает проблемы обучения с плотными наградами, такие как уязвимость к взлому наград и высокая стоимость сбора качественных меток процесса. Метод показал значительное улучшение производительности на задачах рассуждения по сравнению с базовыми моделями, используя при этом меньше данных для обучения.'}, 'en': {'title': 'Unlocking LLM Potential with PRIME: Efficient Training through Implicit Rewards', 'desc': 'This paper introduces PRIME, a method that enhances the training of large language models (LLMs) using dense process rewards instead of traditional sparse outcome rewards. Dense rewards help improve training efficiency and address credit assignment issues, but collecting high-quality process labels has been a challenge. PRIME allows for online updates of process reward models using only policy rollouts and outcome labels, which reduces the need for extensive reward model training. The results show that PRIME significantly improves reasoning performance in tasks like math and coding, achieving better results with less training data compared to existing models.'}, 'zh': {'title': 'PRIME：提升大语言模型推理效率的新方法', 'desc': '本文提出了一种新的方法PRIME（通过隐式奖励进行过程强化学习），旨在解决大语言模型（LLMs）在复杂多步骤推理任务中的训练效率问题。传统的稀疏结果奖励在训练过程中存在效率低下和信用分配等问题，而PRIME通过仅使用策略回滚和结果标签来实现在线过程奖励模型（PRM）的更新。该方法避免了现有方法中需要的专门奖励模型训练阶段，从而显著降低了开发成本。实验结果表明，PRIME在数学和编码竞赛任务中表现出色，相较于传统模型有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2501.18636', 'title': 'SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model', 'url': 'https://huggingface.co/papers/2501.18636', 'abstract': 'The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.', 'score': 24, 'issue_id': 2023, 'pub_date': '2025-01-28', 'pub_date_card': {'ru': '28 января', 'en': 'January 28', 'zh': '1月28日'}, 'hash': '3a201d426049658a', 'authors': ['Xun Liang', 'Simin Niu', 'Zhiyu Li', 'Sensen Zhang', 'Hanyu Wang', 'Feiyu Xiong', 'Jason Zhaoxin Fan', 'Bo Tang', 'Shichao Song', 'Mengwei Wang', 'Jiawei Yang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China', 'Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.18636.jpg', 'data': {'categories': ['#rag', '#security', '#dataset', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'SafeRAG: оценка уязвимостей генерации с дополнением на основе извлечения', 'desc': 'В статье представлен бенчмарк SafeRAG для оценки безопасности генерации с дополнением на основе извлечения (RAG). Авторы классифицируют атаки на RAG и создают набор данных для их моделирования. Эксперименты показывают, что RAG уязвим ко всем типам атак, даже самым очевидным. Исследование демонстрирует, что существующие компоненты RAG не способны эффективно противостоять атакам, что приводит к снижению качества работы системы.'}, 'en': {'title': 'Strengthening RAG: Evaluating Vulnerabilities in Knowledge Integration', 'desc': 'This paper addresses the security vulnerabilities of retrieval-augmented generation (RAG) systems, which combine external knowledge with large language models (LLMs) for knowledge-intensive tasks. The authors introduce a benchmark called SafeRAG to evaluate the security of RAG by classifying various attack types that can manipulate knowledge. They create a dataset specifically for testing these vulnerabilities and simulate different attack scenarios to assess the impact on RAG performance. The results show that RAG systems are significantly susceptible to these attacks, leading to a decline in service quality, highlighting the need for improved security measures.'}, 'zh': {'title': '提升RAG安全性，抵御知识攻击！', 'desc': '本文介绍了一种名为SafeRAG的基准，用于评估检索增强生成（RAG）模型的安全性。我们将攻击任务分为银噪声、上下文冲突、软广告和拒绝服务等类型，并为每种任务手动构建了安全评估数据集。通过使用SafeRAG数据集，我们模拟了RAG可能遇到的各种攻击场景。实验结果表明，RAG对所有攻击任务表现出显著的脆弱性，甚至最明显的攻击任务也能轻易绕过现有的检索器和过滤器，导致RAG服务质量下降。'}}}, {'id': 'https://huggingface.co/papers/2502.01341', 'title': 'AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding', 'url': 'https://huggingface.co/papers/2502.01341', 'abstract': 'Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.', 'score': 23, 'issue_id': 2030, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '88ea73fcb0da69ba', 'authors': ['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-André Noël', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], 'affiliations': ['CIFAR AI Chair', 'Ecole de Technologie Superieure', 'McGill University', 'Mila', 'Polytechnique Montreal', 'ServiceNow', 'Universite de Montreal', 'University of British Columbia', 'University of Waterloo', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01341.jpg', 'data': {'categories': ['#alignment', '#cv', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'Улучшение выравнивания модальностей в мультимодальных моделях', 'desc': 'AlignVLM - это новый метод выравнивания визуальных и текстовых признаков в мультимодальных моделях. Он отображает визуальные признаки в виде взвешенного среднего текстовых эмбеддингов языковой модели. Этот подход использует лингвистические приоры, закодированные в языковой модели, чтобы обеспечить эффективную интерпретацию визуальных признаков. AlignVLM особенно эффективен для задач понимания документов и демонстрирует улучшенное выравнивание признаков и устойчивость к шуму.'}, 'en': {'title': 'Aligning Vision and Language for Better Understanding', 'desc': 'This paper addresses the challenge of aligning visual features with language embeddings in vision-language models (VLMs). The authors introduce AlignVLM, a new method that connects visual features to a weighted average of language model (LLM) text embeddings, enhancing semantic similarity. By utilizing the linguistic knowledge embedded in the LLM, AlignVLM ensures that visual features are accurately represented in a way that the LLM can understand. The results show that AlignVLM outperforms existing methods, particularly in tasks involving document understanding, while also demonstrating improved robustness to noisy inputs.'}, 'zh': {'title': '视觉与语言的完美对齐', 'desc': '在视觉语言模型（VLMs）中，将视觉特征与语言嵌入对齐是一个关键挑战。现有的连接器，如多层感知器（MLP），常常会产生分布外或噪声输入，导致模态之间的不对齐。我们提出了一种新颖的视觉文本对齐方法AlignVLM，它将视觉特征映射到LLM文本嵌入的加权平均值。AlignVLM在文档理解任务中表现尤为出色，能够有效提高视觉特征与文本内容的对齐和抗噪声能力。'}}}, {'id': 'https://huggingface.co/papers/2502.01534', 'title': 'Preference Leakage: A Contamination Problem in LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2502.01534', 'abstract': 'Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.', 'score': 22, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd2508b2b8b82b41b', 'authors': ['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], 'affiliations': ['Arizona State University', 'University of California, Los Angeles', 'University of Illinois Urbana Champaign', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2502.01534.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#training', '#dataset', '#leakage'], 'emoji': '🕵️', 'ru': {'title': 'Осторожно: LLM-судьи могут быть предвзяты!', 'desc': "Исследование выявляет проблему 'утечки предпочтений' при использовании больших языковых моделей (LLM) в качестве судей для оценки других моделей. Эта проблема возникает из-за связанности между генераторами синтетических данных и LLM-оценщиками. Эксперименты подтверждают предвзятость судей к связанным с ними моделям-ученикам на различных базовых моделях и эталонных тестах. Результаты указывают на то, что утечка предпочтений является распространенной и трудно обнаруживаемой проблемой в области использования LLM в качестве судей."}, 'en': {'title': 'Uncovering Preference Leakage: A Hidden Bias in LLM Evaluation', 'desc': 'This paper discusses a problem called preference leakage in the context of using Large Language Models (LLMs) as judges for data annotation. Preference leakage occurs when the relationship between the data generators and the evaluators leads to biased evaluations, particularly when they are similar or related models. The authors identify three types of relatedness that can cause this issue and demonstrate through experiments that judges show bias towards their related models. The findings highlight that preference leakage is a significant and often unnoticed challenge in LLM-based model development.'}, 'zh': {'title': '偏好泄漏：LLM评判中的隐患', 'desc': '本文探讨了大型语言模型（LLM）作为评判者和基于LLM的数据合成在模型开发中的应用。我们揭示了偏好泄漏这一问题，它是由合成数据生成器与LLM评估者之间的相关性引起的。通过定义三种常见的相关性，我们进行了广泛的实验，证实了评判者对其相关学生模型的偏见。研究表明，偏好泄漏是一个普遍存在且难以检测的问题，影响了LLM作为评判者的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01639', 'title': 'SliderSpace: Decomposing the Visual Capabilities of Diffusion Models', 'url': 'https://huggingface.co/papers/2502.01639', 'abstract': "We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. Our code, data and trained weights are available at https://sliderspace.baulab.info", 'score': 15, 'issue_id': 2027, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '559003a020b42709', 'authors': ['Rohit Gandikota', 'Zongze Wu', 'Richard Zhang', 'David Bau', 'Eli Shechtman', 'Nick Kolkin'], 'affiliations': ['Adobe Research', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01639.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#dataset', '#open_source', '#multimodal', '#interpretability', '#cv'], 'emoji': '🎚️', 'ru': {'title': 'SliderSpace: Раскрытие скрытых возможностей диффузионных моделей', 'desc': 'SliderSpace - это фреймворк для автоматической декомпозиции визуальных возможностей диффузионных моделей на управляемые и понятные человеку направления. Он обнаруживает множество интерпретируемых и разнообразных направлений одновременно из одного текстового запроса. Каждое направление обучается как адаптер низкого ранга, что позволяет осуществлять композиционный контроль. Эксперименты показывают эффективность SliderSpace в различных приложениях, включая декомпозицию концепций и исследование художественных стилей.'}, 'en': {'title': 'Unlocking Creativity in Diffusion Models with SliderSpace', 'desc': "SliderSpace is a novel framework designed to break down the visual capabilities of diffusion models into controllable and understandable components. It automatically identifies multiple interpretable directions from a single text prompt, eliminating the need for users to define each edit direction manually. By training each direction as a low-rank adaptor, SliderSpace allows for compositional control and reveals unexpected possibilities within the model's latent space. Extensive experiments show that it effectively enhances concept decomposition, artistic style exploration, and diversity, outperforming existing methods in generating diverse and useful variations."}, 'zh': {'title': 'SliderSpace：可控的视觉能力分解', 'desc': 'SliderSpace是一个框架，用于自动分解扩散模型的视觉能力，使其可控且易于理解。与现有方法不同，SliderSpace可以从单个文本提示中同时发现多个可解释和多样化的方向，而无需用户逐个指定属性。每个方向被训练为低秩适配器，从而实现组合控制，并在模型的潜在空间中发现意想不到的可能性。通过对最先进的扩散模型进行广泛实验，我们证明了SliderSpace在概念分解、艺术风格探索和多样性增强等三个应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.00698', 'title': 'MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models', 'url': 'https://huggingface.co/papers/2502.00698', 'abstract': 'IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.   Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.', 'score': 13, 'issue_id': 2027, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 февраля', 'en': 'February 2', 'zh': '2月2日'}, 'hash': '960e3460f0ab8e56', 'authors': ['Huanqia Cai', 'Yijun Yang', 'Winston Hu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00698.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальный ИИ проваливает тест на интеллект', 'desc': 'Статья представляет новый фреймворк MM-IQ для оценки когнитивных способностей мультимодальных систем искусственного интеллекта. Фреймворк включает 2,710 тестовых заданий по 8 типам рассуждений, аналогично тестам IQ для людей. Результаты показали, что даже передовые мультимодальные модели демонстрируют точность лишь немного выше случайного угадывания (27.49% против 25%). Это подчеркивает существенный разрыв между текущими возможностями ИИ и базовыми когнитивными способностями человека.'}, 'en': {'title': 'Bridging the Cognitive Divide in AI with MM-IQ', 'desc': 'This paper introduces MM-IQ, a new evaluation framework designed to measure cognitive abilities in artificial intelligence systems, similar to how IQ tests assess human intelligence. It includes 2,710 test items across 8 reasoning paradigms, providing a comprehensive benchmark for multimodal models. The evaluation of various state-of-the-art models shows that their performance is only slightly better than random guessing, indicating significant limitations in their reasoning capabilities. This finding emphasizes the urgent need for advancements in AI to better replicate human-like reasoning skills.'}, 'zh': {'title': 'MM-IQ：评估多模态系统的认知能力新标准', 'desc': '本论文提出了一种名为MM-IQ的综合评估框架，用于量化多模态系统中的认知能力。该框架包含2710个精心策划的测试项目，涵盖8种不同的推理范式。通过对领先的多模态模型进行系统评估，结果显示即使是最先进的架构，其表现也仅略高于随机猜测。这个显著的性能差距表明当前多模态系统在接近人类基本推理能力方面的不足，强调了需要进行范式转变的进步。'}}}, {'id': 'https://huggingface.co/papers/2502.01068', 'title': 'FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation', 'url': 'https://huggingface.co/papers/2502.01068', 'abstract': 'While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00times and 1.40times improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.', 'score': 12, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '58ab72f123d7a4b6', 'authors': ['Dongwon Jo', 'Jiwon Song', 'Yulhwa Kim', 'Jae-Joon Kim'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea', 'Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.01068.jpg', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'FastKV: Ускорение обработки длинных последовательностей в LLM', 'desc': 'Статья представляет FastKV - новый метод сжатия кэша ключ-значение (KV) для больших языковых моделей (LLM), направленный на улучшение латентности при обработке длинных последовательностей. FastKV использует подход Token-Selective Propagation (TSP), который сохраняет полную контекстную информацию в начальных слоях LLM и выборочно распространяет только часть этой информации в более глубоких слоях. Метод также включает сжатие кэша KV с учетом grouped-query attention (GQA) для повышения эффективности памяти и вычислений. Эксперименты показывают, что FastKV достигает значительного улучшения времени до первого токена и пропускной способности по сравнению с современными методами, сохраняя при этом точность на уровне базовых показателей.'}, 'en': {'title': 'FastKV: Speeding Up Long-Context Processing in LLMs', 'desc': 'This paper presents FastKV, a new method for compressing key-value (KV) caches in large language models (LLMs) to improve computational efficiency and reduce latency. FastKV uses a Token-Selective Propagation (TSP) strategy that keeps full context information in the early layers of the model while selectively passing on only part of this information in the deeper layers. Additionally, it employs grouped-query attention (GQA) to enhance both memory usage and processing speed. Experimental results demonstrate that FastKV significantly improves time-to-first-token and throughput while maintaining accuracy on long-context tasks.'}, 'zh': {'title': 'FastKV：提升长上下文处理速度的创新方法', 'desc': '本文介绍了一种名为FastKV的KV缓存压缩方法，旨在提高长上下文序列的处理速度。FastKV采用了一种新颖的选择性传播方法（TSP），在LLM的初始层保留完整的上下文信息，而在更深层次中仅选择性传播部分信息。该方法还结合了分组查询注意力（GQA）来优化内存和计算效率。实验结果表明，FastKV在首次令牌时间和吞吐量方面分别比现有的HeadKV方法提高了2.00倍和1.40倍，同时在长上下文基准测试中保持了与基线相当的准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.00094', 'title': 'AIN: The Arabic INclusive Large Multimodal Model', 'url': 'https://huggingface.co/papers/2502.00094', 'abstract': "Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.", 'score': 12, 'issue_id': 2018, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'e2d63540ee133732', 'authors': ['Ahmed Heakl', 'Sara Ghaboura', 'Omkar Thawkar', 'Fahad Shahbaz Khan', 'Hisham Cholakkal', 'Rao Muhammad Anwer', 'Salman Khan'], 'affiliations': ['Aalto University', 'Australian National University', 'Linköping University', 'Mohamed bin Zayed University of AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.00094.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#low_resource', '#multimodal', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'AIN: Прорыв в арабоязычном мультимодальном ИИ', 'desc': 'Представлена модель AIN - двуязычная мультимодальная языковая модель для арабского и английского языков. Модель обучена на 3,6 миллионах высококачественных мультимодальных арабско-английских образцов данных. AIN демонстрирует передовые результаты в арабском языке, сохраняя при этом сильные визуальные возможности для английского. На бенчмарке CAMEL-Bench, охватывающем 38 поддоменов, 7B-версия AIN превосходит GPT-4o на 3,4% в среднем по восьми доменам.'}, 'en': {'title': 'Empowering Arabic with Advanced Multimodal AI', 'desc': 'This paper presents AIN, the Arabic Inclusive Multimodal Model, which is designed to enhance the performance of large multimodal models (LMMs) specifically for Arabic and English. AIN utilizes a substantial dataset of 3.6 million high-quality Arabic-English multimodal samples to achieve state-of-the-art results in Arabic language tasks. The model excels across various domains, as evidenced by its performance on the CAMEL-Bench benchmark, where it surpasses GPT-4o in multiple sub-domains. AIN aims to provide advanced generative AI tools for Arabic speakers, addressing the current limitations in Arabic multimodal understanding.'}, 'zh': {'title': '推动阿拉伯语多模态AI的进步', 'desc': '随着大型语言模型（LLMs）和多模态模型（LMMs）的快速发展，阿拉伯语的研究仍然相对滞后。我们提出了AIN模型，这是一个旨在提升阿拉伯语和英语的多模态模型，利用了360万高质量的阿拉伯语-英语多模态数据样本。AIN在多个领域表现出色，尤其是在复杂的视觉理解和多图像理解方面，超越了现有的GPT-4o模型。该模型的优越性能为阿拉伯语使用者提供了先进的多模态生成AI工具，推动了相关应用的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.01142', 'title': 'DeepRAG: Thinking to Retrieval Step by Step for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01142', 'abstract': 'Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.', 'score': 9, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'e26994166d750227', 'authors': ['Xinyan Guan', 'Jiali Zeng', 'Fandong Meng', 'Chunlei Xin', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun', 'Jie Zhou'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.01142.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#reasoning', '#rag', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DeepRAG: умное сочетание поиска и рассуждений для ИИ', 'desc': 'DeepRAG - это новый фреймворк, моделирующий рассуждения с поиском информации как марковский процесс принятия решений. Он итеративно декомпозирует запросы, динамически определяя необходимость внешнего поиска или параметрического рассуждения на каждом шаге. Эксперименты показывают, что DeepRAG повышает эффективность поиска и точность ответов на 21.99%. Фреймворк решает проблемы больших языковых моделей, связанные с фактическими галлюцинациями и неэффективной декомпозицией задач при интеграции рассуждений с поиском.'}, 'en': {'title': 'Enhancing Reasoning with Smart Retrieval', 'desc': 'This paper introduces DeepRAG, a new framework that enhances retrieval-augmented reasoning in large language models. It treats the retrieval process as a Markov Decision Process (MDP), allowing for more strategic and adaptive retrieval of information. By breaking down queries into smaller parts, DeepRAG can decide whether to fetch external knowledge or use its built-in reasoning capabilities at each step. The results show that DeepRAG not only makes retrieval more efficient but also increases the accuracy of answers by nearly 22%.'}, 'zh': {'title': 'DeepRAG：优化检索增强推理的新框架', 'desc': '大型语言模型（LLMs）在推理方面展现了显著的潜力，但仍然面临严重的事实幻觉问题，主要由于参数知识的时效性、准确性和覆盖率不足。将推理与检索增强生成（RAG）结合起来仍然具有挑战性，主要是因为任务分解不有效和冗余检索可能引入噪声，降低响应质量。本文提出了DeepRAG框架，将检索增强推理建模为马尔可夫决策过程（MDP），实现了战略性和自适应的检索。实验表明，DeepRAG在提高检索效率的同时，答案准确性提高了21.99%，证明了其在优化检索增强推理方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01637', 'title': 'Scaling Embedding Layers in Language Models', 'url': 'https://huggingface.co/papers/2502.01637', 'abstract': 'We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent n-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached n-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.', 'score': 9, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '478a2a0ee08530a8', 'authors': ['Da Yu', 'Edith Cohen', 'Badih Ghazi', 'Yangsibo Huang', 'Pritish Kamath', 'Ravi Kumar', 'Daogao Liu', 'Chiyuan Zhang'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.01637.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без увеличения вычислительных затрат', 'desc': 'SCONE - это новый метод расширения слоёв входных эмбеддингов для улучшения производительности языковых моделей при увеличении размера слоя. Он вводит эмбеддинги для частых n-грамм, обеспечивая контекстуализированное представление для каждого входного токена. Эти эмбеддинги обучаются отдельной моделью и предварительно вычисляются для использования во время инференса. SCONE позволяет масштабировать количество кэшированных n-граммных эмбеддингов и модель для их обучения, сохраняя фиксированное количество FLOPS при инференсе.'}, 'en': {'title': 'Enhancing Language Models with SCONE: Scalable N-gram Embeddings for Better Performance', 'desc': 'SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding) is a novel approach designed to improve the performance of language models as they grow in size. It introduces embeddings for common n-grams while keeping the original vocabulary intact, which helps in providing better contextual representations for input tokens. These n-gram embeddings are learned through a separate model during training and stored in off-accelerator memory to ensure fast inference. By scaling both the number of cached n-gram embeddings and the model that learns them, SCONE achieves superior performance compared to a large baseline model while maintaining efficient inference-time computations.'}, 'zh': {'title': 'SCONE：提升语言模型性能的新方法', 'desc': '我们提出了一种方法SCONE（可扩展的上下文化的离线N-gram嵌入），旨在通过扩展输入嵌入层来提升语言模型的性能。SCONE在保持原有词汇的同时，引入了一组常见n-gram的嵌入，以提供每个输入标记的上下文化表示。这些嵌入在训练过程中由一个单独的模型学习，并在推理时预先计算并存储在离线加速器内存中，几乎不影响推理速度。通过增加缓存的n-gram嵌入数量和扩展学习它们的模型，SCONE在多种语料库上超越了1.9B参数的基线，同时仅使用一半的推理时间FLOPS。'}}}, {'id': 'https://huggingface.co/papers/2502.01100', 'title': 'ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning', 'url': 'https://huggingface.co/papers/2502.01100', 'abstract': 'We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.   Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.', 'score': 8, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '901fd196d7bfe394', 'authors': ['Bill Yuchen Lin', 'Ronan Le Bras', 'Kyle Richardson', 'Ashish Sabharwal', 'Radha Poovendran', 'Peter Clark', 'Yejin Choi'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.01100.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Проклятие сложности в логическом мышлении LLM', 'desc': 'В статье исследуются возможности логического рассуждения больших языковых моделей (LLM) и их масштабируемость в сложных задачах немонотонного вывода. Для этого представлена ZebraLogic, комплексная система оценки производительности LLM на логических головоломках, основанных на задачах удовлетворения ограничений (CSP). ZebraLogic позволяет генерировать головоломки с контролируемой сложностью, что помогает систематически изучать пределы масштабируемости моделей, таких как Llama и DeepSeek-R1. Результаты показывают значительное снижение точности по мере увеличения сложности задач, что авторы называют "проклятием сложности".'}, 'en': {'title': 'Unraveling the Limits of Logical Reasoning in Large Language Models', 'desc': "This paper examines how well large language models (LLMs) can perform logical reasoning, especially in complex scenarios where reasoning does not follow a straightforward path. The authors introduce ZebraLogic, a new framework designed to evaluate LLMs on logic grid puzzles that are based on constraint satisfaction problems (CSPs). Through this framework, they discover that as the complexity of the puzzles increases, the accuracy of the models significantly decreases, a challenge they refer to as the 'curse of complexity.' The study also suggests methods to improve reasoning capabilities, such as using advanced sampling techniques and self-verification prompts, while highlighting the limitations of current LLMs in handling complex reasoning tasks."}, 'zh': {'title': '揭示大型语言模型推理能力的复杂性挑战', 'desc': '本文研究了大型语言模型（LLMs）的逻辑推理能力及其在复杂非单调推理中的可扩展性。我们引入了ZebraLogic，一个全面的评估框架，用于评估LLM在基于约束满足问题（CSPs）的逻辑网格谜题上的推理表现。研究结果显示，随着问题复杂性的增加，模型的准确性显著下降，这一现象被称为复杂性诅咒。我们还探讨了增强逻辑推理的策略，包括最佳采样、回溯机制和自我验证提示。'}}}, {'id': 'https://huggingface.co/papers/2502.01081', 'title': 'The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles', 'url': 'https://huggingface.co/papers/2502.01081', 'abstract': "The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.", 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '7585b424ff041825', 'authors': ['Vernon Y. H. Toh', 'Yew Ken Chia', 'Deepanway Ghosal', 'Soujanya Poria'], 'affiliations': ['Singapore University of Technology and Design (SUTD)'], 'pdf_title_img': 'assets/pdf/title_img/2502.01081.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agi', '#open_source', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эволюция рассуждений: от символов к мультимодальности', 'desc': 'Статья рассматривает эволюцию возможностей рассуждения в мультимодальных задачах у языковых моделей серий GPT и OpenAI. Авторы отмечают значительный прогресс модели o3 в решении символических паттернов, но подчеркивают необходимость исследования мультимодальных сценариев. Проводится анализ производительности моделей на сложных визуально-лингвистических головоломках, требующих абстрактного и алгоритмического мышления. Результаты показывают общую тенденцию улучшения способностей к рассуждению, но выявляют сохраняющиеся трудности даже у передовых моделей в некоторых типах задач.'}, 'en': {'title': 'Advancing Reasoning in Multimodal AI: A New Era for LLMs', 'desc': "This paper discusses the advancements in Large Language Models (LLMs) with the release of OpenAI's o1 and o3, which show improved reasoning abilities. The o3 model has demonstrated superior problem-solving skills compared to humans on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, the study highlights that these models primarily focus on symbolic reasoning, while human reasoning often involves multimodal inputs like vision and language. The authors emphasize the need for further research into multimodal reasoning capabilities, as the o1 model, despite its high performance, still faces challenges with simple multimodal and algorithmic puzzles."}, 'zh': {'title': '多模态推理能力的探索与挑战', 'desc': '本文探讨了OpenAI的o1和o3模型在大型语言模型中的先进推理能力。o3在抽象和推理语料库（ARC-AGI）中超越了人类，表现出色，但该基准仅限于符号模式。人类通常在多模态场景中进行推理，因此需要研究多模态任务中的高级推理能力。尽管o1在推理能力上有所提升，但在简单的多模态难题和算法难题上仍然存在不足。'}}}, {'id': 'https://huggingface.co/papers/2502.01591', 'title': 'Improving Transformer World Models for Data-Efficient RL', 'url': 'https://huggingface.co/papers/2502.01591', 'abstract': 'We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.', 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd9195e9417fce419', 'authors': ['Antoine Dedieu', 'Joseph Ortiz', 'Xinghua Lou', 'Carter Wendelken', 'Wolfgang Lehrach', 'J Swaroop Guntupalli', 'Miguel Lazaro-Gredilla', 'Kevin Patrick Murphy'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.01591.jpg', 'data': {'categories': ['#rl', '#architecture', '#benchmark', '#games', '#training', '#reasoning', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Новый рубеж в model-based RL: превосходя человека в Craftax-classic', 'desc': 'Статья представляет новый подход к обучению с подкреплением на основе модели, достигающий наилучших результатов на бенчмарке Craftax-classic. Авторы разработали алгоритм, превосходящий как предыдущий SOTA-метод DreamerV3, так и человеческий уровень производительности. Ключевые улучшения включают комбинированную архитектуру политики с CNN и RNN, обучение на реальных и воображаемых данных, токенизацию изображений методом ближайших соседей и блочное teacher forcing для трансформерной модели мира. Эти инновации позволили значительно повысить эффективность использования данных в сложной среде, требующей широкого спектра навыков.'}, 'en': {'title': 'Revolutionizing Model-Based RL for Superior Game Performance', 'desc': "This paper introduces a new model-based reinforcement learning (MBRL) approach that excels in the Craftax-classic benchmark, a complex 2D survival game. The proposed algorithm achieves a remarkable reward of 67.4% after just 1 million environment steps, surpassing previous methods like DreamerV3 and even human performance. Key innovations include a novel policy architecture that integrates convolutional neural networks (CNNs) and recurrent neural networks (RNNs), along with enhancements like 'Dyna with warmup' for training on both real and simulated data. Additional techniques such as a nearest neighbor tokenizer for image patches and block teacher forcing for future reasoning further boost the model's efficiency and effectiveness."}, 'zh': {'title': '基于模型的强化学习新突破！', 'desc': '本文提出了一种基于模型的强化学习方法，在Craftax-classic基准测试中取得了新的最佳表现。这是一款开放世界的2D生存游戏，要求智能体展现出强大的泛化能力、深度探索能力和长期推理能力。我们的MBRL算法在仅1M环境步骤后获得了67.4%的奖励，显著超越了DreamerV3的53.2%，并首次超过了人类表现的65.0%。该方法通过构建一个最先进的无模型基线，并结合CNN和RNN的新型策略架构，进一步提升了样本效率。'}}}, {'id': 'https://huggingface.co/papers/2502.01208', 'title': 'Almost Surely Safe Alignment of Large Language Models at Inference-Time', 'url': 'https://huggingface.co/papers/2502.01208', 'abstract': "Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.", 'score': 6, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'fdbe8816f1c6476a', 'authors': ['Xiaotong Ji', 'Shyam Sundhar Ramesh', 'Matthieu Zimmer', 'Ilija Bogunovic', 'Jun Wang', 'Haitham Bou Ammar'], 'affiliations': ['Huawei Noahs Ark Lab', 'Imperial College London', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2502.01208.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#inference', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Безопасная генерация ответов LLM без переобучения', 'desc': 'Статья представляет новый подход к выравниванию больших языковых моделей (LLM) во время вывода, обеспечивающий генерацию безопасных ответов с вероятностью, близкой к единице. Авторы формулируют задачу как ограниченный марковский процесс принятия решений в латентном пространстве модели. Они вводят состояние безопасности, которое отслеживает эволюцию ограничений безопасности и позволяет доказать формальные гарантии безопасности. На основе этого подхода разработан метод InferenceGuard, который эффективно балансирует безопасность и производительность задачи, превосходя существующие методы выравнивания во время вывода.'}, 'en': {'title': 'InferenceGuard: Ensuring Safe Responses from LLMs at Inference Time', 'desc': "This paper addresses the issue of biased or unsafe responses from large language models (LLMs) by introducing a new method for aligning their outputs during inference. Instead of retraining the model, the authors propose a constrained Markov decision process that operates in the LLM's latent space to ensure safe response generation. They enhance this process with a safety state that monitors safety constraints, providing formal guarantees of safety. The proposed method, called InferenceGuard, shows improved performance in generating safe and aligned responses compared to existing techniques without altering the model's weights."}, 'zh': {'title': '推理时安全对齐的新方法', 'desc': '本文提出了一种新的推理时对齐方法，旨在确保大型语言模型（LLM）生成安全的响应。我们将安全生成推理时响应的问题框架化为一个约束的马尔可夫决策过程（MDP），并在LLM的潜在空间中进行处理。通过引入一个安全状态来跟踪安全约束的演变，我们能够在解决MDP时提供正式的安全保证。我们提出的InferenceGuard实现了在不修改模型权重的情况下安全对齐LLM，并在生成安全和对齐响应方面优于现有的方法。'}}}, {'id': 'https://huggingface.co/papers/2502.01441', 'title': 'Improved Training Technique for Latent Consistency Models', 'url': 'https://huggingface.co/papers/2502.01441', 'abstract': 'Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-c scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/', 'score': 6, 'issue_id': 2018, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '2ea077ca6fd7397f', 'authors': ['Quan Dao', 'Khanh Doan', 'Di Liu', 'Trung Le', 'Dimitris Metaxas'], 'affiliations': ['Monash University', 'Rutgers University', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01441.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#architecture', '#open_source', '#diffusion', '#video'], 'emoji': '🧠', 'ru': {'title': 'Преодоление выбросов в латентном пространстве для улучшения консистентных моделей', 'desc': 'Эта статья представляет новый подход к обучению консистентных моделей в латентном пространстве для генеративных задач. Авторы обнаружили, что латентные данные часто содержат импульсивные выбросы, которые ухудшают производительность iCT. Для решения этой проблемы они предложили использовать функцию потерь Коши вместо Псевдо-Хубера, а также ввели диффузионные потери на ранних временных шагах и применили оптимальный транспорт. Эти стратегии позволили успешно обучить латентные консистентные модели, способные к высококачественному сэмплированию за один-два шага.'}, 'en': {'title': 'Enhancing Latent Consistency Models for High-Quality Generation', 'desc': "This paper introduces advancements in consistency models, a type of generative model that can create high-quality outputs efficiently. The authors focus on improving performance in latent spaces, where data often contains outliers that hinder model effectiveness. By replacing traditional loss functions with Cauchy losses and incorporating diffusion loss, they enhance the model's robustness against these outliers. Additionally, they propose an adaptive scaling-c scheduler and Non-scaling LayerNorm to optimize training, resulting in latent consistency models that perform comparably to diffusion models in generating images and videos."}, 'zh': {'title': '提升一致性模型性能的创新方法', 'desc': '一致性模型是一种新型生成模型，能够在单步或多步中生成高质量样本。最近，这些模型在像素空间中表现出色，达到了与扩散模型相当的效果。然而，在大规模数据集上进行一致性训练的成功，尤其是在文本到图像和视频生成任务中，取决于潜在空间的表现。为了解决潜在数据中的异常值对性能的影响，本文提出了使用Cauchy损失替代伪Huber损失，并引入扩散损失和最优传输方法，以提高模型的鲁棒性和性能。'}}}, {'id': 'https://huggingface.co/papers/2502.01584', 'title': 'PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01584', 'abstract': "Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.", 'score': 5, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '6cf23c9aeb70961a', 'authors': ['Carolyn Jane Anderson', 'Joydeep Biswas', 'Aleksander Boruch-Gruszecki', 'Federico Cassano', 'Molly Q Feldman', 'Arjun Guha', 'Francesca Lucchetti', 'Zixuan Wu'], 'affiliations': ['Charles University', 'Cursor', 'Northeastern University', 'Oberlin College', 'University of Texas at Austin', 'Wellesley College'], 'pdf_title_img': 'assets/pdf/title_img/2502.01584.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Новый бенчмарк раскрывает скрытые возможности и недостатки языковых моделей', 'desc': 'Авторы представляют новый бенчмарк для оценки языковых моделей, основанный на головоломках NPR Sunday Puzzle Challenge. В отличие от существующих тестов, требующих специализированных знаний, этот бенчмарк опирается на общие знания и легко проверяем. Исследование выявило значительное превосходство модели OpenAI o1 над другими моделями рассуждений. Анализ также обнаружил новые типы ошибок у моделей, такие как преждевременная капитуляция и неуверенность в ответах.'}, 'en': {'title': 'Bridging Knowledge Gaps in AI Reasoning with General Knowledge Benchmarks', 'desc': "This paper introduces a new benchmark for evaluating machine learning models, focusing on general knowledge rather than specialized expertise. The benchmark is based on the NPR Sunday Puzzle Challenge, making it accessible yet challenging for both humans and AI models. The authors highlight that existing benchmarks may not reveal certain capability gaps, as demonstrated by OpenAI's model outperforming others in reasoning tasks. Additionally, the study identifies specific failure modes in models, such as premature concessions and uncertainty in outputs, suggesting improvements in inference techniques are necessary."}, 'zh': {'title': '挑战性与可验证性的全新基准测试', 'desc': '现有的前沿模型基准测试通常考察专业的、难以理解的知识。我们提出了一种基于NPR周日谜题挑战的基准，要求仅具备一般知识。该基准对人类和模型都具有挑战性，但正确答案易于验证，模型的错误也容易发现。我们的研究揭示了现有基准中未显现的能力差距，OpenAI o1在推理模型中表现优异，且分析推理输出揭示了新的失败类型。'}}}, {'id': 'https://huggingface.co/papers/2502.01636', 'title': 'Lifelong Sequential Knowledge Editing without Model Degradation', 'url': 'https://huggingface.co/papers/2502.01636', 'abstract': 'Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this "importance hacking", the edited layers provide a much larger contributions to the model\'s output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B.', 'score': 4, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '919dd5274b620b3a', 'authors': ['Akshat Gupta', 'Phudish Prateepamornkul', 'Maochuan Lu', 'Ahmed Alaa', 'Thomas Hartvigsen', 'Gopala Anumanchipalli'], 'affiliations': ['SCB DataX', 'University of California, Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2502.01636.jpg', 'data': {'categories': ['#training', '#interpretability', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование редактирования знаний в нейросетях', 'desc': 'Статья посвящена улучшению методов последовательного редактирования знаний в больших языковых моделях. Авторы выявили проблемы переобучения и непропорционального роста нормы при использовании существующих методов редактирования. Они предложили новый метод ENCORE, который контролирует переобучение и рост нормы, позволяя выполнять до 10 000 последовательных правок без потери производительности модели. ENCORE также показывает значительное ускорение по сравнению с другими методами редактирования знаний.'}, 'en': {'title': 'ENCORE: Efficient Knowledge Editing Without Degradation', 'desc': "This paper investigates the challenges of sequential knowledge editing in machine learning models, particularly focusing on the degradation of model performance after numerous edits. It identifies that traditional locate-then-edit methods can lead to overfitting and excessive growth in the norm of the edited parameters. The authors introduce a new method called ENCORE, which employs early stopping and norm constraints to prevent these issues, allowing for effective long-term editing. ENCORE not only maintains the model's performance after 10,000 edits but also operates significantly faster than existing methods."}, 'zh': {'title': 'ENCORE：高效的知识编辑解决方案', 'desc': '本论文研究了在知识编辑中进行大规模顺序编辑时模型性能下降的原因。我们发现，定位后编辑的方法容易导致对编辑事实的过拟合，并且连续的知识编辑会导致编辑矩阵的范数不成比例地增长。为了解决这些问题，我们提出了ENCORE方法，通过早停和范数约束来控制过拟合和范数增长，从而实现长时间的顺序编辑。ENCORE能够在不损失下游性能的情况下，进行多达10,000次的顺序编辑，并且比现有方法更快。'}}}, {'id': 'https://huggingface.co/papers/2502.01619', 'title': 'Learning to Generate Unit Tests for Automated Debugging', 'url': 'https://huggingface.co/papers/2502.01619', 'abstract': "Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to a large language model (LLM) as it iteratively debugs faulty code, motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions and candidate code. We integrate UTGen into UTDebug, a robust debugging pipeline that uses generated tests to help LLMs debug effectively. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), UTDebug (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and back-tracks edits based on multiple generated UTs to avoid overfitting. We show that UTGen outperforms UT generation baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen-2.5 7B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3% and 12.35% (respectively) over other LLM-based UT generation baselines.", 'score': 2, 'issue_id': 2036, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '0806c010a6d2569d', 'authors': ['Archiki Prasad', 'Elias Stengel-Eskin', 'Justin Chih-Yao Chen', 'Zaid Khan', 'Mohit Bansal'], 'affiliations': ['Department of Computer Science, UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2502.01619.jpg', 'data': {'categories': ['#training', '#interpretability', '#optimization', '#plp'], 'emoji': '🧪', 'ru': {'title': 'Автоматическая генерация юнит-тестов для улучшения отладки кода языковыми моделями', 'desc': 'Статья представляет UTGen - метод обучения языковых моделей генерации юнит-тестов, выявляющих ошибки в коде и предсказывающих корректные выходные данные. UTGen интегрирован в конвейер отладки UTDebug, который использует сгенерированные тесты для эффективной отладки кода языковыми моделями. UTDebug масштабирует UTGen во время выполнения для улучшения предсказания выходных данных тестов и проверяет изменения на основе нескольких сгенерированных юнит-тестов. Результаты показывают, что UTGen превосходит базовые методы генерации юнит-тестов, а UTDebug улучшает точность отладки кода языковыми моделями.'}, 'en': {'title': 'Enhancing Debugging with Smart Unit Test Generation', 'desc': 'This paper introduces UTGen, a method that helps large language models (LLMs) generate unit test inputs that can effectively reveal errors in faulty code while also predicting the correct outputs. The authors highlight a challenge where generating tests that expose errors can lead to incorrect output predictions without having the correct solutions available. To overcome this, UTGen is integrated into a debugging pipeline called UTDebug, which enhances the debugging process by validating and refining the generated tests. The results show that UTGen significantly improves the accuracy of LLMs in debugging tasks, outperforming existing methods in generating effective unit tests.'}, 'zh': {'title': '自动化单元测试生成与调试的创新方案', 'desc': '本文提出了一种名为UTGen的自动化单元测试生成方法，旨在帮助大型语言模型（LLM）生成能够揭示错误的单元测试输入及其正确的预期输出。研究发现，生成的单元测试输入在揭示错误和正确预测输出之间存在权衡。UTGen被集成到UTDebug调试管道中，以提高LLM的调试效果，并通过多次生成的测试来验证和回溯编辑，避免过拟合。实验结果表明，UTGen在生成有效单元测试方面优于现有基线，并显著提高了LLM在调试任务中的准确性。'}}}, {'id': 'https://huggingface.co/papers/2501.18055', 'title': 'Current Pathology Foundation Models are unrobust to Medical Center Differences', 'url': 'https://huggingface.co/papers/2501.18055', 'abstract': 'Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences. We introduce the Robustness Index. This novel robustness metric reflects to what degree biological features dominate confounding features. Ten current publicly available pathology FMs are evaluated. We find that all current pathology foundation models evaluated represent the medical center to a strong degree. Significant differences in the robustness index are observed. Only one model so far has a robustness index greater than one, meaning biological features dominate confounding features, but only slightly. A quantitative approach to measure the influence of medical center differences on FM-based prediction performance is described. We analyze the impact of unrobustness on classification performance of downstream models, and find that cancer-type classification errors are not random, but specifically attributable to same-center confounders: images of other classes from the same medical center. We visualize FM embedding spaces, and find these are more strongly organized by medical centers than by biological factors. As a consequence, the medical center of origin is predicted more accurately than the tissue source and cancer type. The robustness index introduced here is provided with the aim of advancing progress towards clinical adoption of robust and reliable pathology FMs.', 'score': 1, 'issue_id': 2024, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 января', 'en': 'January 29', 'zh': '1月29日'}, 'hash': '444c1f08656649e7', 'authors': ['Edwin D. de Jong', 'Eric Marcus', 'Jonas Teuwen'], 'affiliations': ['Aignostics', 'Antoni van Leeuwenhoek Hospital (AvL)', 'Kaiko', 'The Netherlands Cancer Institute Amsterdam (NKI)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18055.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#security', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Путь к надежным фундаментальным моделям в патологии: преодоление влияния медицинских центров', 'desc': 'Статья рассматривает проблему надежности фундаментальных моделей (ФМ) в патологии для использования в клинической практике. Авторы вводят новую метрику - Индекс надежности, который отражает степень доминирования биологических признаков над мешающими факторами, связанными с особенностями медицинских центров. Оценка десяти публично доступных ФМ показала, что все они сильно зависят от специфики медицинских центров, и только одна модель имеет индекс надежности больше единицы. Исследование демонстрирует, что ошибки классификации типов рака связаны с конфаундерами из того же медицинского центра, а пространства вложений ФМ организованы больше по медицинским центрам, чем по биологическим факторам.'}, 'en': {'title': 'Ensuring Robustness in Pathology Models for Clinical Use', 'desc': "This paper discusses the evaluation of Pathology Foundation Models (FMs) to ensure they are reliable for clinical use. It introduces a new metric called the Robustness Index, which measures how much these models focus on biological features versus confounding factors from different medical centers. The study finds that most current pathology FMs are heavily influenced by the medical center's characteristics rather than the actual biological data. The authors emphasize the need for models that prioritize biological features to improve classification accuracy and support clinical adoption."}, 'zh': {'title': '确保病理模型的鲁棒性，助力临床应用', 'desc': '病理基础模型（FMs）在医疗保健中具有很大潜力，但在临床应用之前，必须确保它们对不同医疗中心的变化具有鲁棒性。我们提出了鲁棒性指数，这是一种新颖的鲁棒性度量，反映生物特征在多大程度上主导了混淆特征。研究发现，当前的病理基础模型在很大程度上代表了医疗中心，只有一个模型的鲁棒性指数大于一，表明生物特征略微主导混淆特征。通过定量方法分析医疗中心差异对模型预测性能的影响，发现癌症类型分类错误与同中心混淆因素密切相关。'}}}, {'id': 'https://huggingface.co/papers/2502.00314', 'title': 'A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation', 'url': 'https://huggingface.co/papers/2502.00314', 'abstract': "The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub.", 'score': 1, 'issue_id': 2023, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '51ad114da14800b7', 'authors': ['Moein Heidari', 'Ehsan Khodapanah Aghdam', 'Alexander Manzella', 'Daniel Hsu', 'Rebecca Scalabrino', 'Wenjin Chen', 'David J. Foran', 'Ilker Hacihaliloglu'], 'affiliations': ['Beth Israel Deaconess Medical Center, Boston, MA, United States', 'Center for Biomedical Imaging and Informatics, Rutgers Cancer Institute, New Brunswick, NJ, United States', 'Department of Medicine, University of British Columbia, British Columbia, Canada', 'Department of Radiology, University of British Columbia, British Columbia, Canada', 'Harvard Medical School, Boston, MA, United States', 'Independent Researcher, Tabriz, Iran', 'Memorial Sloan Kettering Cancer Center, New York, NY, United States', 'Rutgers Robert Wood Johnson Medical School, New Brunswick, NJ, United States', 'School of Biomedical Engineering, University of British Columbia, British Columbia, Canada', 'Weill Cornell Medical School, New York, NY, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.00314.jpg', 'data': {'categories': ['#architecture', '#cv', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Эффективная сегментация опухолей с помощью усовершенствованных нейросетей', 'desc': 'Статья посвящена автоматической сегментации опухолей забрюшинного пространства с использованием различных архитектур глубокого обучения. Авторы сравнивают эффективность U-Net и его модификаций, включая CNN, ViT, Mamba SSM и xLSTM, на новом наборе данных КТ. Предложенная модель ViLU-Net интегрирует Vi-блоки для улучшения сегментации. Результаты показывают, что xLSTM обеспечивает наиболее эффективную работу в рамках архитектуры U-Net.'}, 'en': {'title': 'Efficient Tumor Segmentation with ViLU-Net: Merging U-Net and Vision Transformers', 'desc': "This paper addresses the challenges of segmenting tumors in the retroperitoneum, which can be irregularly shaped and difficult to analyze. It explores the use of advanced machine learning models, particularly U-Net and its enhancements, to automate the segmentation process. The study introduces the ViLU-Net model, which incorporates Vision Transformer blocks to improve segmentation accuracy while maintaining computational efficiency. Results indicate that the xLSTM architecture significantly enhances the U-Net framework's performance, making it a promising approach for medical image analysis."}, 'zh': {'title': '高效肿瘤分割：ViLU-Net的创新应用', 'desc': '本研究探讨了在后腹膜肿瘤的自动分割中使用U-Net及其变体的有效性。这些肿瘤形状不规则，手动分割耗时且困难，因此需要更高效的自动化方法。研究中引入了Mamba状态空间模型和扩展长短期记忆（xLSTM）等架构，以降低计算资源消耗并处理长距离依赖。最终提出的ViLU-Net模型通过集成Vi-blocks，显著提高了分割效果，xLSTM在U-Net框架中的效率表现尤为突出。'}}}, {'id': 'https://huggingface.co/papers/2502.01126', 'title': 'Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences', 'url': 'https://huggingface.co/papers/2502.01126', 'abstract': 'Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty. However, models struggle to provide absolute assessments of confidence (i.e. judging confidence in answering a question independent of other questions) and the coarse-grained scores they produce are not useful for evaluating the correctness of their answers. We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence ("Which question are you more confident in answering correctly?"). Treating each question as a "player" in a series of matchups against other questions and the model\'s preferences as match outcomes, we can use rank aggregation methods like Elo rating and Bradley-Terry to translate the model\'s confidence preferences into confidence scores. We evaluate relative confidence estimation against absolute confidence estimation and self-consistency confidence methods on five state-of-the-art LMs -- GPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405B -- across 14 challenging STEM, social science, and commonsense reasoning question answering tasks. Our results demonstrate that relative confidence estimation consistently provides more reliable confidence scores than absolute confidence estimation, with average gains of 3.5% in selective classification AUC over direct absolute confidence estimation methods and 1.7% over self-consistency approaches across all models and datasets.', 'score': 0, 'issue_id': 2035, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'ed2eef6c3e310379', 'authors': ['Vaishnavi Shrivastava', 'Ananya Kumar', 'Percy Liang'], 'affiliations': ['cs.stanford.edu'], 'pdf_title_img': 'assets/pdf/title_img/2502.01126.jpg', 'data': {'categories': ['#interpretability', '#rlhf', '#reasoning', '#benchmark', '#data'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности языковых моделей через относительную оценку уверенности', 'desc': 'Исследование предлагает метод относительной оценки уверенности языковых моделей, где модель сравнивает свою уверенность в ответах на разные вопросы. Этот подход использует методы ранжирования, такие как рейтинг Эло и модель Брэдли-Терри, для перевода предпочтений модели в оценки уверенности. Результаты показывают, что относительная оценка уверенности превосходит абсолютную оценку и методы самосогласованности на 3.5% и 1.7% соответственно по метрике AUC выборочной классификации. Исследование проводилось на пяти современных языковых моделях и 14 сложных задачах по ответам на вопросы в областях STEM, социальных наук и здравого смысла.'}, 'en': {'title': 'Boosting Confidence: Relative Estimation Outshines Absolute Scores in Language Models', 'desc': 'This paper discusses the importance of reliable confidence estimates from language models (LMs) to help users identify potential errors in their outputs. It highlights the challenges LMs face in providing absolute confidence assessments and proposes a new method called relative confidence estimation. This method involves comparing questions against each other to determine which the model is more confident in answering correctly, using techniques like Elo rating and Bradley-Terry for ranking. The study shows that relative confidence estimation outperforms traditional absolute confidence methods, leading to improved reliability in confidence scores across various question answering tasks.'}, 'zh': {'title': '相对置信度估计：提升语言模型的可靠性', 'desc': '本文探讨了语言模型（LM）如何提供可靠的置信度评估，以帮助用户识别输出中的错误。我们提出了一种相对置信度估计的方法，通过比较问题之间的相对置信度来评估模型的信心。与绝对置信度评估相比，相对置信度估计在多个先进的语言模型上表现出更高的可靠性，尤其是在选择性分类任务中。实验结果显示，相对置信度估计在准确性上平均提高了3.5%。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi (2)', '#alignment (3)', '#architecture (12)', '#audio', '#benchmark (14)', '#cv (9)', '#data (3)', '#dataset (14)', '#diffusion (5)', '#ethics (3)', '#games (1)', '#graphs (1)', '#hallucinations (1)', '#healthcare (2)', '#inference (12)', '#interpretability (6)', '#leakage (1)', '#long_context (5)', '#low_resource (1)', '#machine_translation', '#math (3)', '#multilingual (2)', '#multimodal (8)', '#open_source (9)', '#optimization (20)', '#plp (1)', '#rag (2)', '#reasoning (15)', '#rl (6)', '#rlhf (4)', '#robotics', '#science', '#security (5)', '#small_models (2)', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (26)', '#transfer_learning (3)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-05 13:18',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-05 13:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-05 13:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    