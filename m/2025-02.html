
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 179 papers. February 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Февраль 2025</span> | <span id="title-articles-count">179 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-01.html">⬅️ <span id="prev-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-03.html">➡️ <span id="next-date">03.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Февраль 2025', 'en': 'February 2025', 'zh': '2月2025年'};
        let feedDateNext = {'ru': '03.2025', 'en': '03/2025', 'zh': '3月2025年'};
        let feedDatePrev = {'ru': '01.2025', 'en': '01/2025', 'zh': '1月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.02737', 'title': 'SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model', 'url': 'https://huggingface.co/papers/2502.02737', 'abstract': 'While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art "small" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project.', 'score': 80, 'issue_id': 2066, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'c78fe4c39300443d', 'authors': ['Loubna Ben Allal', 'Anton Lozhkov', 'Elie Bakouch', 'Gabriel Martín Blázquez', 'Guilherme Penedo', 'Lewis Tunstall', 'Andrés Marafioti', 'Hynek Kydlíček', 'Agustín Piqueres Lajarín', 'Vaibhav Srivastav', 'Joshua Lochner', 'Caleb Fahlgren', 'Xuan-Son Nguyen', 'Clémentine Fourrier', 'Ben Burtenshaw', 'Hugo Larcher', 'Haojun Zhao', 'Cyril Zakka', 'Mathieu Morlon', 'Colin Raffel', 'Leandro von Werra', 'Thomas Wolf'], 'affiliations': ['HuggingFaceTB'], 'pdf_title_img': 'assets/pdf/title_img/2502.02737.jpg', 'data': {'categories': ['#open_source', '#dataset', '#low_resource', '#training', '#small_models'], 'emoji': '🤏', 'ru': {'title': 'Большие возможности в маленьком пакете: SmolLM2 - компактная языковая модель с впечатляющей производительностью', 'desc': "Статья описывает разработку SmolLM2 - современной 'маленькой' языковой модели с 1,7 миллиардами параметров. Модель обучалась на ~11 триллионах токенов данных с использованием многоэтапного процесса, сочетающего веб-тексты со специализированными данными по математике, коду и выполнению инструкций. Авторы также представили новые специализированные наборы данных и провели эксперименты для оптимизации процесса обучения. В результате SmolLM2 превзошла другие современные малые языковые модели, такие как Qwen2.5-1.5B и Llama3.2-1B."}, 'en': {'title': 'SmolLM2: Efficient Language Modeling for Resource-Constrained Environments', 'desc': 'This paper presents SmolLM2, a compact language model with 1.7 billion parameters designed to be efficient for deployment in resource-limited environments. The model is trained on an extensive dataset of approximately 11 trillion tokens, utilizing a multi-stage training approach that incorporates diverse data sources, including web text and specialized datasets for math and coding. The authors introduce new datasets to enhance training quality and perform systematic evaluations to optimize dataset mixing based on performance feedback. SmolLM2 demonstrates superior performance compared to other recent small language models, and the authors provide access to the model and datasets for further research.'}, 'zh': {'title': '小型语言模型的强大突破', 'desc': '本论文介绍了SmolLM2的开发，这是一个具有17亿参数的小型语言模型。为了实现强大的性能，我们在约11万亿个数据上进行了过度训练，采用了多阶段训练过程，结合了网络文本、数学、代码和指令跟随数据。我们还引入了新的专用数据集，以解决现有数据集规模小或质量低的问题。最终，我们证明SmolLM2在性能上超越了其他近期的小型语言模型，如Qwen2.5-1.5B和Llama3.2-1B。'}}}, {'id': 'https://huggingface.co/papers/2502.01506', 'title': 'TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets', 'url': 'https://huggingface.co/papers/2502.01506', 'abstract': 'The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.', 'score': 26, 'issue_id': 2063, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'f5ec0450054af574', 'authors': ['Yuzhe Yang', 'Yifei Zhang', 'Minghao Wu', 'Kaidi Zhang', 'Yunmiao Zhang', 'Honghai Yu', 'Yan Hu', 'Benyou Wang'], 'affiliations': ['Nanjing University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.01506.jpg', 'data': {'categories': ['#multimodal', '#agents'], 'emoji': '📊', 'ru': {'title': 'LLM-агенты раскрывают тайны социально-экономической динамики', 'desc': 'Статья представляет новый фреймворк TwinMarket для моделирования социально-экономических систем с использованием больших языковых моделей (LLM). Авторы применяют LLM-агентов для более реалистичного моделирования человеческого поведения, учитывая когнитивные искажения и эмоциональные факторы. В экспериментах на симулированном фондовом рынке демонстрируется, как индивидуальные действия приводят к групповому поведению и эмергентным явлениям. Этот подход позволяет лучше понять взаимосвязь между индивидуальным принятием решений и коллективными социально-экономическими паттернами.'}, 'en': {'title': 'Harnessing LLMs for Realistic Socio-Economic Simulations', 'desc': 'This paper presents TwinMarket, a new framework that uses large language models (LLMs) to simulate socio-economic systems. Unlike traditional Agent-Based Models, TwinMarket captures the complexity of human behavior, including cognitive biases and emotional influences. The framework allows for the exploration of how individual actions can lead to collective dynamics, such as financial bubbles and recessions, in a simulated stock market. By leveraging LLMs, the study provides deeper insights into the interactions between individual decision-making and broader socio-economic patterns.'}, 'zh': {'title': '利用大型语言模型模拟社会经济系统的涌现现象', 'desc': '本研究探讨了社会涌现现象，传统的基于规则的代理模型（ABM）难以捕捉人类行为的多样性和复杂性，尤其是行为经济学中强调的非理性因素。我们提出了一种新的多代理框架TwinMarket，利用大型语言模型（LLM）来模拟社会经济系统。通过模拟股票市场环境的实验，我们展示了个体行为如何通过互动和反馈机制引发集体动态，导致金融泡沫和经济衰退等涌现现象。该方法为个体决策与集体社会经济模式之间的复杂关系提供了宝贵的见解。'}}}, {'id': 'https://huggingface.co/papers/2502.03373', 'title': 'Demystifying Long Chain-of-Thought Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2502.03373', 'abstract': 'Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.', 'score': 20, 'issue_id': 2064, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'a1d00a6c8452131a', 'authors': ['Edward Yeo', 'Yuxuan Tong', 'Morry Niu', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University', 'IN.AI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.03373.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rl', '#training', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая секреты длинных цепочек рассуждений в ИИ', 'desc': 'Статья исследует механизмы длинных цепочек рассуждений (CoT) в больших языковых моделях (LLM). Авторы выявляют ключевые факторы, влияющие на способность моделей генерировать длинные CoT траектории через эксперименты с обучением с подкреплением (RL) и тонкой настройкой. Исследование показывает важность масштабирования вычислительных ресурсов, формирования наград и использования веб-данных для улучшения рассуждений. Результаты предоставляют практические рекомендации по оптимизации стратегий обучения для усиления длинных CoT рассуждений в LLM.'}, 'en': {'title': 'Unlocking Reasoning Power in Large Language Models', 'desc': 'This paper explores how to improve reasoning in large language models (LLMs) by enhancing their inference capabilities through long chains-of-thought (CoTs). It highlights the importance of reinforcement learning (RL) in developing these reasoning skills, while also addressing the unclear conditions for the emergence of long CoTs. The study presents four key findings, including the role of supervised fine-tuning (SFT) in simplifying training, the necessity of reward shaping for stabilizing CoT growth, and the significance of scaling reward signals for effective RL. Overall, the research provides valuable insights for optimizing training strategies to boost long CoT reasoning in LLMs.'}, 'zh': {'title': '优化训练策略，提升长推理链能力', 'desc': '本研究探讨了大型语言模型（LLMs）中长推理链（CoTs）的生成机制，揭示了影响模型生成长推理链的关键因素。我们发现，虽然监督微调（SFT）不是绝对必要的，但它可以简化训练过程并提高效率。随着训练计算能力的增加，推理能力有可能出现，但其发展并不总是保证，因此奖励设计对于稳定推理链的长度增长至关重要。最后，我们的研究为优化训练策略以增强LLMs中的长推理链提供了实用指导。'}}}, {'id': 'https://huggingface.co/papers/2502.03387', 'title': 'LIMO: Less is More for Reasoning', 'url': 'https://huggingface.co/papers/2502.03387', 'abstract': 'We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (>100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models\' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model\'s encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as "cognitive templates" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.', 'score': 18, 'issue_id': 2066, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'ad1fa98bc3904527', 'authors': ['Yixin Ye', 'Zhen Huang', 'Yang Xiao', 'Ethan Chern', 'Shijie Xia', 'Pengfei Liu'], 'affiliations': ['SJTU, SII, GAIR'], 'pdf_title_img': 'assets/pdf/title_img/2502.03387.jpg', 'data': {'categories': ['#open_source', '#dataset', '#reasoning', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Меньше значит больше: революция в обучении языковых моделей сложным рассуждениям', 'desc': 'Исследователи обнаружили, что сложные математические рассуждения в больших языковых моделях можно вызвать с помощью удивительно малого количества примеров. Их модель LIMO достигла впечатляющих результатов на математических тестах, используя всего 817 обучающих образцов, что значительно меньше, чем у предыдущих подходов. LIMO также продемонстрировала исключительную способность к обобщению вне распределения, превзойдя модели, обученные на гораздо большем объеме данных. На основе этих результатов авторы предлагают гипотезу LIMO, согласно которой сложные рассуждения могут возникать через минимальные, но точно организованные демонстрации когнитивных процессов в предварительно обученных моделях.'}, 'en': {'title': 'Less Data, More Reasoning: The LIMO Hypothesis', 'desc': 'This paper reveals a surprising finding about how large language models can perform complex reasoning tasks. It shows that instead of needing a lot of training data, a model called LIMO can achieve high accuracy in mathematical reasoning with only a small number of examples. LIMO outperforms previous models that used much more data, indicating that less can be more when it comes to training for reasoning tasks. The authors introduce the Less-Is-More Reasoning Hypothesis, suggesting that a well-prepared model can effectively learn complex reasoning from minimal, well-structured examples.'}, 'zh': {'title': '少即是多，推理能力的新发现', 'desc': '本文提出了一项重要发现，挑战了我们对大型语言模型中复杂推理能力产生机制的理解。传统观点认为，复杂推理任务需要大量的训练数据，但我们证明只需少量示例即可有效引发复杂的数学推理能力。我们的模型LIMO在数学推理方面表现出前所未有的性能，使用仅817个训练样本，分别在AIME和MATH上达到了57.1%和94.8%的准确率。我们提出的“少即是多推理假设”表明，在基础模型中，经过充分编码的领域知识可以通过精心设计的少量示例来激发复杂推理能力。'}}}, {'id': 'https://huggingface.co/papers/2502.02339', 'title': 'Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking', 'url': 'https://huggingface.co/papers/2502.02339', 'abstract': "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0%) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2%) while maintaining substantial data and computational efficiency.", 'score': 9, 'issue_id': 2063, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '3f3413717efb32f6', 'authors': ['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Ruihan Jin', 'Feihu Che', 'Zengqi Wen', 'Jianhua Tao'], 'affiliations': ['Beijing', 'Department of Automation, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.02339.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#multimodal', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': 'AStar: Эффективное структурированное мышление для мультимодальных ИИ', 'desc': 'Статья представляет новый подход к улучшению визуального рассуждения мультимодальных больших языковых моделей (MLLM). Авторы предлагают метод AStar, использующий автоматизированное структурированное мышление на основе поиска Монте-Карло по дереву (MCTS). AStar автоматически извлекает высокоуровневые паттерны рассуждений из ограниченных данных и интегрирует внутренние способности модели с внешними указаниями. Эксперименты показывают, что AStar достигает точности 54.0% на бенчмарке MathVerse, превосходя GPT-4o при высокой эффективности использования данных и вычислений.'}, 'en': {'title': 'AStar: Enhancing Multimodal Reasoning with Efficient Structured Thinking', 'desc': "This paper introduces AStar, a new approach to improve the reasoning abilities of multimodal large language models (MLLMs) using Monte Carlo Tree Search (MCTS). AStar focuses on deriving high-level cognitive reasoning patterns from limited data, which helps in enhancing the models' performance without requiring extensive data sets. The framework integrates internal reasoning capabilities of the models with external guidelines, allowing for efficient inference with fewer iterations. Experimental results show that AStar outperforms existing models like GPT-4o in accuracy while being more data and computationally efficient."}, 'zh': {'title': 'AStar：高效的多模态推理新范式', 'desc': '多模态大型语言模型（MLLMs）在复杂视觉推理方面表现出色，但仍面临挑战。尽管最近的研究尝试通过引入结构化思维和教师指导来增强推理能力，但在性能和效率之间的平衡仍然困难。本文提出了一种名为AStar的自动化结构化思维范式，利用蒙特卡洛树搜索（MCTS）从有限数据中自动推导高层次的认知推理模式。AStar通过统一的推理框架，结合模型的内部推理能力和外部推理指导，实现高效推理，显著提高了准确性和数据利用效率。'}}}, {'id': 'https://huggingface.co/papers/2502.01105', 'title': 'LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer', 'url': 'https://huggingface.co/papers/2502.01105', 'abstract': "Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer's superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition.", 'score': 6, 'issue_id': 2067, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'b4eb829c549c6a2e', 'authors': ['Yiren Song', 'Danze Chen', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.01105.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#diffusion', '#cv', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'LayerTracer: ИИ-дизайнер векторной графики', 'desc': 'LayerTracer - это новый подход к созданию многослойных SVG изображений, основанный на диффузионном трансформере. Он имитирует процесс работы дизайнера, генерируя пошаговые растровые чертежи, а затем векторизуя их по слоям. Метод использует условный диффузионный механизм для кодирования референсных изображений в латентные токены. Эксперименты показывают превосходство LayerTracer над существующими методами в качестве генерации и редактируемости векторной графики.'}, 'en': {'title': 'LayerTracer: Bridging the Gap in Layered SVG Generation', 'desc': 'This paper introduces LayerTracer, a new framework that improves the generation of layered SVGs by learning from how designers create them. It uses a two-phase process: first, it generates rasterized blueprints that mimic human design steps, and then it converts these into clean, editable SVGs while removing duplicate paths. The framework employs a conditional diffusion mechanism to ensure that the generated images maintain their structure and quality. Experiments show that LayerTracer outperforms existing methods in both the quality of the generated designs and their ease of editing, aligning better with professional design practices.'}, 'zh': {'title': 'LayerTracer：智能生成可编辑的分层SVG图形', 'desc': '本文提出了一种名为LayerTracer的框架，旨在生成认知对齐的分层SVG图形。该方法通过学习设计师的分层SVG创建过程，利用一个新颖的顺序设计操作数据集。LayerTracer分为两个阶段：首先，基于文本的扩散变换器生成多阶段的光栅化构建蓝图；其次，通过路径去重实现分层矢量化，生成干净且可编辑的SVG文件。实验结果表明，LayerTracer在生成质量和可编辑性方面优于基于优化和神经网络的基线方法，有效地将AI生成的矢量图与专业设计认知对齐。'}}}, {'id': 'https://huggingface.co/papers/2502.02671', 'title': 'On Teacher Hacking in Language Model Distillation', 'url': 'https://huggingface.co/papers/2502.02671', 'abstract': "Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.", 'score': 5, 'issue_id': 2072, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'defca87e9bf06d0b', 'authors': ['Daniil Tiapkin', 'Daniele Calandriello', 'Johan Ferret', 'Sarah Perrin', 'Nino Vieillard', 'Alexandre Ramé', 'Mathieu Blondel'], 'affiliations': ['Ecole 1CMAP, France; Polytechnique', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.02671.jpg', 'data': {'categories': ['#alignment', '#optimization', '#rlhf', '#training', '#data'], 'emoji': '🧠', 'ru': {'title': "Борьба с 'teacher hacking': ключ к robust языковым моделям", 'desc': "Статья исследует феномен 'teacher hacking' при дистилляции знаний в языковых моделях. Авторы предлагают экспериментальную установку с оракулом, учителем и учеником для изучения этого явления. Результаты показывают, что 'teacher hacking' возникает при использовании фиксированного офлайн-датасета, но может быть смягчен с помощью онлайн-генерации данных. Исследование подчеркивает важность разнообразия данных для предотвращения 'hacking' и построения надежных языковых моделей."}, 'en': {'title': 'Preventing Teacher Hacking: The Key Role of Data Diversity in Distillation', 'desc': "This paper explores the concept of 'teacher hacking' in the context of knowledge distillation for language models (LMs). Teacher hacking occurs when a student LM overly optimizes based on an imperfect teacher LM, leading to poor performance on the actual task. The authors conducted experiments using an oracle LM as the ground truth, a teacher LM distilled from it, and a student LM distilled from the teacher. They found that using diverse online data can prevent teacher hacking, highlighting the importance of data diversity in the distillation process."}, 'zh': {'title': '防止教师黑客，提升语言模型的蒸馏效果', 'desc': '本文探讨了语言模型（LM）在知识蒸馏阶段可能出现的“教师黑客”现象。教师黑客是指学生模型在模仿教师模型时，过度优化导致性能下降的情况。这种现象与古德哈特法则相符，可能源于教师模型对真实分布的不完美近似。我们的实验表明，使用固定的离线数据集进行蒸馏时，教师黑客现象会发生，而采用在线数据生成技术则能有效缓解这一问题，数据多样性是防止教师黑客的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2502.01618', 'title': 'A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods', 'url': 'https://huggingface.co/papers/2502.01618', 'abstract': 'Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code and further information is available at https://probabilistic-inference-scaling.github.io.', 'score': 5, 'issue_id': 2065, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'c9971916eb027101', 'authors': ['Isha Puri', 'Shivchander Sudalairaj', 'Guangxuan Xu', 'Kai Xu', 'Akash Srivastava'], 'affiliations': ['MIT CSAIL', 'Red Hat AI Innovation'], 'pdf_title_img': 'assets/pdf/title_img/2502.01618.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#math', '#inference'], 'emoji': '🎲', 'ru': {'title': 'Вероятностный подход к масштабированию вывода LLM', 'desc': 'Статья представляет новый подход к масштабированию вычислений во время вывода для больших языковых моделей (LLM). Вместо оптимизации с помощью моделей вознаграждения, авторы рассматривают задачу как вероятностный вывод, используя методы Монте-Карло на основе частиц. Эмпирическая оценка показывает, что предложенный метод имеет в 4-16 раз лучшую скорость масштабирования по сравнению с детерминированными аналогами на сложных задачах математических рассуждений. Исследование демонстрирует, как небольшие модели могут достичь точности крупных моделей при меньшем количестве прогонов.'}, 'en': {'title': 'Revolutionizing Inference: Probabilistic Scaling for LLMs', 'desc': 'This paper addresses the limitations of scaling large language models (LLMs) by focusing on improving inference time rather than just increasing model size or data. The authors propose a new approach that treats inference-time scaling as a probabilistic inference task, using sampling techniques to better explore the state distribution. By applying particle-based Monte Carlo methods, their method shows significant improvements in scaling rates compared to traditional deterministic search methods. The results indicate that their approach can achieve higher accuracy with fewer rollouts, demonstrating a promising direction for enhancing LLM performance.'}, 'zh': {'title': '推理时间扩展的新方法：概率推理与粒子采样结合', 'desc': '大型语言模型（LLMs）通过增加模型规模和数据量取得了显著的性能提升。然而，最近的研究表明，这种方法的收益递减，促使我们考虑在推理时增加计算量。现有的推理时间扩展方法通常将任务视为搜索问题，容易受到奖励模型的近似误差影响而导致奖励操控。本文提出了一种新的推理时间扩展方法，通过适应基于粒子的蒙特卡洛方法，将推理时间扩展视为概率推理任务，从而在各种数学推理任务中实现了更好的扩展率。'}}}, {'id': 'https://huggingface.co/papers/2502.01154', 'title': 'Jailbreaking with Universal Multi-Prompts', 'url': 'https://huggingface.co/papers/2502.01154', 'abstract': 'Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques.', 'score': 3, 'issue_id': 2068, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'aa9860c81d83ac21', 'authors': ['Yu-Ling Hsu', 'Hsuan Su', 'Shang-Tse Chen'], 'affiliations': ['National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01154.jpg', 'data': {'categories': ['#security', '#rl', '#data', '#optimization', '#transfer_learning', '#training', '#ethics'], 'emoji': '🔓', 'ru': {'title': 'Универсальный взлом языковых моделей: новый метод JUMP', 'desc': 'Статья описывает новый метод под названием JUMP для универсального взлома (jailbreak) больших языковых моделей с помощью мульти-промптов. Авторы также представляют адаптированную версию этого метода для защиты, называемую DUMP. Экспериментальные результаты показывают, что предложенный подход превосходит существующие техники по оптимизации универсальных мульти-промптов. Исследование затрагивает важную тему этических проблем и новых типов атак на языковые модели.'}, 'en': {'title': 'JUMP: Universal Multi-Prompts for Jailbreaking LLMs', 'desc': 'This paper presents JUMP, a novel method for jailbreaking large language models (LLMs) using universal multi-prompts. Unlike traditional prompting techniques that focus on specific adversarial inputs, JUMP aims to create a universal attacker that can adapt to various unseen tasks, reducing computational costs. Additionally, the authors propose a defense mechanism called DUMP, which leverages the same principles to protect against such attacks. Experimental results indicate that JUMP significantly outperforms existing methods in optimizing these universal multi-prompts.'}, 'zh': {'title': '通用多提示：破解与防御的创新方法', 'desc': '大型语言模型（LLMs）近年来迅速发展，改变了许多应用，显著提高了便利性和生产力。然而，随着其强大能力的提升，伦理问题和新型攻击（如越狱攻击）也随之出现。大多数提示技术专注于优化单个案例的对抗输入，这在处理大数据集时会导致更高的计算成本。本文介绍了一种名为JUMP的方法，旨在使用通用多提示对LLMs进行越狱，同时我们还提出了防御方法DUMP，实验结果表明我们的方法在优化通用多提示方面优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2502.03275', 'title': 'Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning', 'url': 'https://huggingface.co/papers/2502.03275', 'abstract': 'Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.', 'score': 3, 'issue_id': 2066, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'f94d674e0f57dcf9', 'authors': ['DiJia Su', 'Hanlin Zhu', 'Yingchen Xu', 'Jiantao Jiao', 'Yuandong Tian', 'Qinqing Zheng'], 'affiliations': ['Meta AI', 'UC Berkeley', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2502.03275.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#optimization', '#training', '#benchmark', '#math'], 'emoji': '🧠', 'ru': {'title': 'Гибридное представление рассуждений: эффективность через абстракцию', 'desc': 'Данная статья предлагает гибридный подход к представлению процесса рассуждений в больших языковых моделях (LLM). Авторы используют латентные дискретные токены, генерируемые VQ-VAE, для частичной абстракции начальных шагов рассуждения, что значительно сокращает длину входных данных. Метод применяется как при обучении модели с нуля, так и при дообучении существующих LLM на гибридных данных с расширенным словарем. Предложенный подход превосходит базовые методы в различных тестах на логические и математические рассуждения.'}, 'en': {'title': 'Streamlining Reasoning with Hybrid Token Representations', 'desc': 'This paper presents a new method to improve reasoning in Large Language Models (LLMs) by using a hybrid representation of reasoning processes. Instead of relying solely on lengthy text inputs, the authors introduce latent discrete tokens generated by VQ-VAE to simplify the reasoning steps. This approach reduces the input length and computational resources needed while maintaining effective reasoning capabilities. The proposed method shows superior performance in training and fine-tuning scenarios for logical and mathematical reasoning tasks compared to traditional methods.'}, 'zh': {'title': '优化推理过程，提升模型效率', 'desc': '本文探讨了大型语言模型（LLMs）在推理和规划中的应用，特别是在链式思维（CoT）数据训练时的表现。我们提出了一种混合表示法，通过使用VQ-VAE生成的潜在离散标记，部分抽象化初始推理步骤，从而显著减少推理过程的长度。我们在两个场景中探索了潜在追踪抽象的使用：一是从头开始训练模型解决钥匙寻找迷宫问题，二是对LLMs进行微调以处理逻辑和数学推理问题。我们的训练方法通过随机混合潜在标记和文本标记，促进了对新潜在标记的快速适应，且在多个基准测试中表现优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2502.02928', 'title': 'Large Language Model Guided Self-Debugging Code Generation', 'url': 'https://huggingface.co/papers/2502.02928', 'abstract': 'Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.', 'score': 2, 'issue_id': 2075, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'dc7aaadeeee7e1e7', 'authors': ['Muntasir Adnan', 'Zhiwei Xu', 'Carlos C. N. Kuhn'], 'affiliations': ['Open Source Institute, Faculty of Science and Technology, University of Canberra, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2502.02928.jpg', 'data': {'categories': ['#agents', '#plp', '#training'], 'emoji': '🐍', 'ru': {'title': 'PyCapsule: Эффективная генерация Python-кода с самоотладкой', 'desc': 'PyCapsule - это новая система для автоматической генерации кода на Python, использующая двухагентный конвейер и модули самоотладки. Система включает в себя сложный вывод промптов, итеративную обработку ошибок и тестирование примеров, что обеспечивает высокую стабильность, безопасность и корректность генерации. PyCapsule демонстрирует значительное улучшение успешности на различных бенчмарках по сравнению с современными методами. Однако наблюдается снижение нормализованной успешности при увеличении попыток самоотладки, возможно, из-за ограниченной и зашумленной обратной связи об ошибках.'}, 'en': {'title': 'Revolutionizing Python Code Generation with PyCapsule', 'desc': 'This paper introduces PyCapsule, a new framework designed to enhance automated code generation, particularly for Python. It employs a two-agent pipeline that focuses on efficient self-debugging and robust error handling, addressing common issues in existing methods. The framework utilizes advanced prompt inference and iterative testing to improve the stability and correctness of generated code. Empirical results show that PyCapsule outperforms current state-of-the-art techniques in various benchmarks, highlighting its potential for more efficient AI-driven programming solutions.'}, 'zh': {'title': 'PyCapsule：高效的自动化代码生成框架', 'desc': '自动化代码生成在智能计算机编程和系统部署中变得越来越重要。然而，现有的方法在计算效率上常常面临挑战，并且缺乏强大的代码解析和错误修正机制。本文提出了一种新颖的框架PyCapsule，采用简单而有效的双代理管道和高效的自我调试模块来生成Python代码。PyCapsule通过复杂的提示推理、迭代错误处理和案例测试，确保了高生成稳定性、安全性和正确性。'}}}, {'id': 'https://huggingface.co/papers/2502.02421', 'title': 'Activation-Informed Merging of Large Language Models', 'url': 'https://huggingface.co/papers/2502.02421', 'abstract': 'Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\\% increase in benchmark performance.', 'score': 1, 'issue_id': 2079, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '90e80efaaef789ec', 'authors': ['Amin Heyrani Nobari', 'Kaveh Alimohammadi', 'Ali ArjomandBigdeli', 'Akash Srivastava', 'Faez Ahmed', 'Navid Azizan'], 'affiliations': ['Massachusetts Institute of Technology', 'RedHat AI Innovation & MIT-IBM Watson AI Lab', 'Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2502.02421.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'AIM: Умное слияние языковых моделей для повышения эффективности', 'desc': 'Статья представляет новый метод объединения языковых моделей под названием Activation-Informed Merging (AIM). AIM использует информацию из пространства активаций моделей для улучшения производительности и устойчивости объединенной модели. Метод применим к любому существующему способу слияния моделей и использует принципы непрерывного обучения и сжатия моделей. Эмпирические результаты показывают значительное улучшение производительности объединенных моделей на различных бенчмарках, с увеличением до 40%.'}, 'en': {'title': 'Boosting Model Performance with Activation-Informed Merging', 'desc': 'This paper presents Activation-Informed Merging (AIM), a novel technique for merging large language models (LLMs) that leverages activation space information to improve model performance. AIM enhances the merging process by selectively prioritizing essential weights from the base models, which helps maintain robustness and efficiency. The method is flexible and can be integrated with existing merging techniques, making it widely applicable. Empirical results show that AIM can lead to significant performance improvements, achieving up to a 40% increase in benchmark scores for merged models.'}, 'zh': {'title': '激活信息合并：提升模型合并性能的新方法', 'desc': '模型合并是一种将多个微调的大型语言模型（LLMs）的参数和嵌入结合起来的方法，能够在保持计算效率的同时提升模型在各种任务上的表现。本文提出了一种名为激活信息合并（AIM）的技术，它将LLMs的激活空间信息整合到合并过程中，以提高性能和鲁棒性。AIM旨在作为一种灵活的补充解决方案，适用于任何现有的合并方法，并通过持续学习和模型压缩的原则来保留基础模型中的关键权重。通过使用与任务无关的校准集，AIM在合并过程中优先考虑重要权重，实验证明AIM在多个基准测试中显著提升了合并模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.00306', 'title': 'Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2502.00306', 'abstract': "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.", 'score': 1, 'issue_id': 2076, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '4987f380f5ddb7af', 'authors': ['Ali Naseh', 'Yuefeng Peng', 'Anshuman Suri', 'Harsh Chaudhari', 'Alina Oprea', 'Amir Houmansadr'], 'affiliations': ['Northeastern University', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.00306.jpg', 'data': {'categories': ['#inference', '#rag', '#leakage', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Незаметная атака на RAG-системы: как выявить документы в базе знаний', 'desc': "Статья представляет новый метод атаки на системы генерации текста с извлечением информации (RAG). Авторы предлагают технику под названием 'Interrogation Attack', которая позволяет определить, содержится ли конкретный документ в базе знаний RAG-системы. Метод основан на создании естественных запросов, на которые можно ответить только при наличии целевого документа. Эксперименты показывают, что атака эффективнее существующих методов и трудно обнаружима стандартными детекторами."}, 'en': {'title': 'Stealthy Inference: Unveiling Membership in RAG Systems', 'desc': 'This paper introduces a new method called Interrogation Attack (IA) for membership inference in Retrieval-Augmented Generation (RAG) systems. RAG allows Large Language Models (LLMs) to generate responses using external knowledge without changing their internal parameters, but this can be exploited by adversaries. The IA technique uses natural-text queries that can only be answered if a specific document is present, making it harder to detect than previous methods. The authors demonstrate that their approach is more effective and stealthy, achieving better performance with fewer queries and lower costs compared to existing techniques.'}, 'zh': {'title': '隐蔽的会员推断攻击：RAG系统的新挑战', 'desc': '本论文介绍了一种新的会员推断技术，称为审问攻击（Interrogation Attack, IA），旨在针对RAG数据存储中的文档进行攻击。该方法通过构造自然语言查询，仅在目标文档存在时才能得到答案，从而实现有效的推断。与现有方法相比，我们的攻击在仅使用30个查询的情况下，成功率提高了2倍，同时保持隐蔽性。我们的研究表明，IA在多种RAG配置下的表现优于以往的推断攻击，且每个文档的推断成本低于0.02美元。'}}}, {'id': 'https://huggingface.co/papers/2502.00226', 'title': 'HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems', 'url': 'https://huggingface.co/papers/2502.00226', 'abstract': 'Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks.', 'score': 0, 'issue_id': 2079, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'c9615b5d00a42037', 'authors': ['Jun Xing', 'Mayur Bhatia', 'Sahil Phulwani', 'Darshan Suresh', 'Rafik Matta'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00226.jpg', 'data': {'categories': ['#benchmark', '#science', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый бенчмарк раскрывает потенциал LLM в реальной разработке ПО', 'desc': 'Статья представляет новый бенчмарк для оценки применимости больших языковых моделей (LLM) в реальных задачах разработки программного обеспечения. HackerRank-ASTRA Benchmark включает проектные задачи кодирования, имитирующие реальные сценарии, и оценивает согласованность модели через 32 запуска. Исследование показало, что три ведущие модели достигли сравнимых средних оценок в 75%. Модель Claude-3.5-Sonnet-1022 продемонстрировала наивысшую согласованность и низкую вариативность результатов.'}, 'en': {'title': 'Benchmarking LLMs for Real-World Coding Consistency', 'desc': 'This paper evaluates the effectiveness of large language models (LLMs) in real-world software development tasks using the HackerRank-ASTRA Benchmark. Unlike previous benchmarks that focus on isolated coding problems, this benchmark introduces project-based scenarios that require multi-file handling and assesses model consistency through extensive testing. The study analyzes the performance of top models, revealing that while they achieved similar average scores, one model, Claude-3.5-Sonnet-1022, stood out for its high consistency and low variability in results. This research emphasizes the importance of rigorous evaluation methods to ensure LLMs are reliable for practical applications in coding.'}, 'zh': {'title': '评估大型语言模型在软件开发中的真实应用性', 'desc': '这篇论文评估了大型语言模型（LLMs）在实际软件开发任务中的适用性。现有的基准测试通常只关注单一的编码问题或特定库，忽视了多文件、基于项目的场景，并缺乏对一致性的严格评估。HackerRank-ASTRA基准引入了模拟真实场景的项目基础编码问题，并通过32次运行评估模型的一致性。初步评估显示，Claude-3.5-Sonnet-1022在问题一致性方面表现最佳，具有较低的变异性，突显了其在实际软件开发任务中的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2502.06329', 'title': 'Expect the Unexpected: FailSafe Long Context QA for Finance', 'url': 'https://huggingface.co/papers/2502.06329', 'abstract': 'We propose a new long-context financial benchmark, FailSafeQA, designed to test the robustness and context-awareness of LLMs against six variations in human-interface interactions in LLM-based query-answer systems within finance. We concentrate on two case studies: Query Failure and Context Failure. In the Query Failure scenario, we perturb the original query to vary in domain expertise, completeness, and linguistic accuracy. In the Context Failure case, we simulate the uploads of degraded, irrelevant, and empty documents. We employ the LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained rating criteria to define and calculate Robustness, Context Grounding, and Compliance scores for 24 off-the-shelf models. The results suggest that although some models excel at mitigating input perturbations, they must balance robust answering with the ability to refrain from hallucinating. Notably, Palmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained strong baseline performance but encountered challenges in sustaining robust predictions in 17% of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in 41% of tested cases. The results demonstrate that even high-performing models have significant room for improvement and highlight the role of FailSafeQA as a tool for developing LLMs optimized for dependability in financial applications. The dataset is available at: https://huggingface.co/datasets/Writer/FailSafeQA', 'score': 102, 'issue_id': 2168, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '836d77158c8a414b', 'authors': ['Kiran Kamble', 'Melisa Russak', 'Dmytro Mozolevskyi', 'Muayad Ali', 'Mateusz Russak', 'Waseem AlShikh'], 'affiliations': ['Writer, Inc'], 'pdf_title_img': 'assets/pdf/title_img/2502.06329.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#optimization', '#long_context', '#hallucinations'], 'emoji': '💼', 'ru': {'title': 'FailSafeQA: Новый стандарт оценки надежности языковых моделей в финансах', 'desc': 'Исследователи представили новый эталонный тест FailSafeQA для оценки устойчивости и контекстной осведомленности языковых моделей (LLM) в финансовой сфере. Тест включает шесть вариаций взаимодействия человека с системой, основанной на LLM, и фокусируется на двух сценариях: отказ запроса и отказ контекста. Используя методологию LLM-as-a-Judge и модель Qwen2.5-72B-Instruct, авторы оценили 24 готовые модели по критериям устойчивости, контекстной обоснованности и соответствия. Результаты показали, что даже высокопроизводительные модели имеют значительный потенциал для улучшения, и подчеркнули важность FailSafeQA как инструмента для разработки более надежных LLM в финансовых приложениях.'}, 'en': {'title': 'Enhancing LLM Reliability in Finance with FailSafeQA', 'desc': 'The paper introduces FailSafeQA, a benchmark aimed at evaluating the robustness and context-awareness of large language models (LLMs) in financial query-answer systems. It focuses on two main failure scenarios: Query Failure, where the input query is altered in terms of expertise and clarity, and Context Failure, where irrelevant or degraded documents are introduced. The authors utilize the LLM-as-a-Judge methodology to assess various models based on their Robustness, Context Grounding, and Compliance scores. The findings reveal that while some models perform well under input variations, they struggle with maintaining accuracy, indicating a need for further enhancements in LLM reliability for financial contexts.'}, 'zh': {'title': 'FailSafeQA：提升金融领域LLM的鲁棒性与上下文意识', 'desc': '我们提出了一个新的长上下文金融基准，FailSafeQA，旨在测试大型语言模型（LLMs）在金融领域中对人机交互的鲁棒性和上下文意识。我们关注两个案例研究：查询失败和上下文失败。在查询失败场景中，我们通过改变领域专业性、完整性和语言准确性来扰动原始查询。在上下文失败案例中，我们模拟上传降级、无关和空文档的情况。'}}}, {'id': 'https://huggingface.co/papers/2502.06807', 'title': 'Competitive Programming with Large Reasoning Models', 'url': 'https://huggingface.co/papers/2502.06807', 'abstract': 'We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.', 'score': 38, 'issue_id': 2164, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'fd76cceb75f32321', 'authors': ['OpenAI', ':', 'Ahmed El-Kishky', 'Alexander Wei', 'Andre Saraiva', 'Borys Minaev', 'Daniel Selsam', 'David Dohan', 'Francis Song', 'Hunter Lightman', 'Ignasi Clavera', 'Jakub Pachocki', 'Jerry Tworek', 'Lorenz Kuhn', 'Lukasz Kaiser', 'Mark Chen', 'Max Schwarzer', 'Mostafa Rohaninejad', 'Nat McAleese', 'o3 contributors', 'Oleg Mürk', 'Rhythm Garg', 'Rui Shu', 'Szymon Sidor', 'Vineet Kosaraju', 'Wenda Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.06807.jpg', 'data': {'categories': ['#rlhf', '#rl', '#games', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением превосходит специализированные подходы в задачах рассуждения', 'desc': 'Исследование показывает, что применение обучения с подкреплением к большим языковым моделям (LLM) значительно улучшает их производительность в сложных задачах программирования и рассуждения. Авторы сравнивают модели общего назначения (OpenAI o1 и o3) со специализированной системой o1-ioi, разработанной для участия в Международной олимпиаде по информатике (IOI) 2024 года. Модель o3 достигла уровня золотой медали на IOI 2024 без использования специфических стратегий или послаблений правил. Результаты указывают на то, что масштабирование обучения с подкреплением общего назначения является более эффективным подходом к созданию ИИ для задач рассуждения, чем разработка узкоспециализированных техник.'}, 'en': {'title': 'Scaling General-Purpose Learning Outshines Specialized Strategies', 'desc': 'This paper demonstrates that applying reinforcement learning to large language models (LLMs) enhances their ability to tackle complex coding and reasoning challenges. It compares two reasoning models, OpenAI o1 and an early version of o3, against a specialized model, o1-ioi, which uses tailored inference strategies for the 2024 International Olympiad in Informatics (IOI). The results show that while o1-ioi performed well with its hand-crafted strategies, the later model o3 achieved superior results without such specific techniques. This suggests that scaling general-purpose reinforcement learning is a more effective approach for achieving high performance in reasoning tasks, like competitive programming.'}, 'zh': {'title': '强化学习助力通用模型超越特定领域系统', 'desc': '本论文展示了强化学习在大型语言模型（LLMs）中的应用，显著提升了复杂编码和推理任务的表现。我们比较了两种通用推理模型——OpenAI的o1和o3的早期检查点，以及一个特定领域的系统o1-ioi，该系统使用手工设计的推理策略。o1-ioi在2024年国际信息学奥林匹克竞赛中表现良好，获得了第49百分位的成绩，而在放宽竞争约束的情况下则获得了金牌。我们的研究表明，尽管专门的管道如o1-ioi能带来显著提升，但扩展的通用o3模型在没有依赖手工推理启发式的情况下，超越了这些结果。'}}}, {'id': 'https://huggingface.co/papers/2502.05878', 'title': 'Retrieval-augmented Large Language Models for Financial Time Series Forecasting', 'url': 'https://huggingface.co/papers/2502.05878', 'abstract': 'Stock movement prediction, a fundamental task in financial time-series forecasting, requires identifying and retrieving critical influencing factors from vast amounts of time-series data. However, existing text-trained or numeric similarity-based retrieval methods fall short in handling complex financial analysis. To address this, we propose the first retrieval-augmented generation (RAG) framework for financial time-series forecasting, featuring three key innovations: a fine-tuned 1B parameter large language model (StockLLM) as the backbone, a novel candidate selection method leveraging LLM feedback, and a training objective that maximizes similarity between queries and historically significant sequences. This enables our retriever, FinSeer, to uncover meaningful patterns while minimizing noise in complex financial data. We also construct new datasets integrating financial indicators and historical stock prices to train FinSeer and ensure robust evaluation. Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while FinSeer surpasses existing retrieval methods, achieving an 8\\% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work underscores the importance of tailored retrieval models in financial forecasting and provides a novel framework for future research.', 'score': 24, 'issue_id': 2172, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'bb6c947ce857a2db', 'authors': ['Mengxi Xiao', 'Zihao Jiang', 'Lingfei Qian', 'Zhengyu Chen', 'Yueru He', 'Yijing Xu', 'Yuecheng Jiang', 'Dong Li', 'Ruey-Ling Weng', 'Min Peng', 'Jimin Huang', 'Sophia Ananiadou', 'Qianqian Xie'], 'affiliations': ['School of Computer Science, Wuhan University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05878.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#rag'], 'emoji': '📈', 'ru': {'title': 'FinSeer: Умный поиск для точного прогноза акций', 'desc': 'Статья представляет новую систему для прогнозирования движения акций на основе технологии извлечения и генерации (RAG). Ключевые инновации включают использование крупной языковой модели StockLLM, новый метод отбора кандидатов с обратной связью от LLM и специальную цель обучения для поиска значимых исторических последовательностей. Разработанная система FinSeer превосходит существующие методы извлечения информации, достигая на 8% более высокой точности на датасете BIGDATA22. Работа подчеркивает важность специализированных моделей извлечения данных в финансовом прогнозировании.'}, 'en': {'title': 'Revolutionizing Stock Prediction with RAG Framework', 'desc': 'This paper introduces a new approach to predicting stock movements by using a retrieval-augmented generation (RAG) framework specifically designed for financial time-series data. The framework employs a large language model called StockLLM, which has been fine-tuned to better understand financial contexts. It also features a unique candidate selection method that utilizes feedback from the language model to improve the relevance of retrieved data. The results show that this method significantly enhances prediction accuracy and uncovers important patterns in complex financial datasets, outperforming traditional retrieval techniques.'}, 'zh': {'title': '金融预测的新框架：检索增强生成', 'desc': '本文提出了一种用于金融时间序列预测的检索增强生成（RAG）框架，旨在从大量时间序列数据中识别和提取关键影响因素。我们使用了一个经过微调的1B参数大型语言模型（StockLLM）作为基础，并引入了一种新颖的候选选择方法，利用LLM反馈来优化检索过程。通过最大化查询与历史重要序列之间的相似性，我们的检索器FinSeer能够在复杂的金融数据中发现有意义的模式，同时减少噪声。实验结果表明，该RAG框架在准确性上优于传统方法，强调了定制检索模型在金融预测中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.07316', 'title': 'CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction', 'url': 'https://huggingface.co/papers/2502.07316', 'abstract': 'Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.', 'score': 21, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'fd4f34152d4de2c1', 'authors': ['Junlong Li', 'Daya Guo', 'Dejian Yang', 'Runxin Xu', 'Yu Wu', 'Junxian He'], 'affiliations': ['DeepSeek-AI', 'HKUST', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07316.jpg', 'data': {'categories': ['#dataset', '#data', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'CodeI/O: Раскрытие потенциала рассуждений в больших языковых моделях через код', 'desc': 'Статья представляет новый подход CodeI/O для улучшения способностей больших языковых моделей к рассуждениям. Метод преобразует разнообразные паттерны рассуждений, встроенные в код, в формат предсказания ввода-вывода кода на естественном языке. Это позволяет моделям изучать универсальные примитивы рассуждений, такие как планирование логического потока и модульная декомпозиция. Эксперименты показывают, что CodeI/O приводит к улучшениям в задачах символических, научных, логических и других типов рассуждений.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with CodeI/O', 'desc': 'This paper introduces CodeI/O, a new method designed to enhance reasoning capabilities in Large Language Models (LLMs) by transforming code into a format that predicts inputs and outputs. The approach focuses on training models using natural language Chain-of-Thought (CoT) rationales, which helps the models learn universal reasoning patterns without being tied to specific coding syntax. By exposing models to various reasoning tasks, such as logic flow and state-space searching, CodeI/O improves performance across multiple domains, including math and commonsense reasoning. The results show that this method not only boosts reasoning accuracy but also allows for verification and refinement of predictions through a multi-turn revision process, leading to even better outcomes with CodeI/O++.'}, 'zh': {'title': 'CodeI/O：提升推理能力的新方法', 'desc': '这篇论文提出了一种新的方法，称为CodeI/O，旨在提高大型语言模型的推理能力。通过将原始代码转换为输入输出预测格式，CodeI/O系统地提炼了多样的推理模式。模型通过自然语言的链式思维（CoT）推理来预测代码的输入和输出，从而增强了逻辑流规划、状态空间搜索等推理原语的能力。实验结果表明，CodeI/O在多种推理任务上均表现出一致的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2502.07701', 'title': 'Magic 1-For-1: Generating One Minute Video Clips within One Minute', 'url': 'https://huggingface.co/papers/2502.07701', 'abstract': 'In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.', 'score': 17, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '7212b752112bcd5a', 'authors': ['Hongwei Yi', 'Shitong Shao', 'Tian Ye', 'Jiantong Zhao', 'Qingyu Yin', 'Michael Lingelbach', 'Li Yuan', 'Yonghong Tian', 'Enze Xie', 'Daquan Zhou'], 'affiliations': ['Hedra Inc.', 'Nvidia', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07701.jpg', 'data': {'categories': ['#video', '#training', '#inference', '#multimodal', '#open_source', '#diffusion', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео: от текста к кадрам за секунды', 'desc': 'Magic 1-For-1 (Magic141) - это эффективная модель генерации видео с оптимизированным потреблением памяти и латентностью вывода. Ключевая идея заключается в разделении задачи генерации видео по тексту на две более простые задачи: генерацию изображения по тексту и генерацию видео по изображению. Авторы применяют ряд оптимизационных приемов, включая многомодальное введение условий, состязательную дистилляцию шагов и разреживание параметров. В результате модель способна генерировать 5-секундные видеоклипы менее чем за 3 секунды, а минутное видео - за одну минуту с улучшенным качеством и динамикой.'}, 'en': {'title': 'Efficient Video Generation: Simplifying with Magic141', 'desc': 'The paper introduces Magic 1-For-1 (Magic141), a video generation model designed to optimize memory usage and reduce inference time. It simplifies the text-to-video generation process by breaking it down into two tasks: generating images from text and then creating videos from those images. The authors demonstrate that the image-to-video task converges more easily than the direct text-to-video approach, allowing for faster training. They implement various optimization techniques to enhance model performance, achieving impressive video generation speeds while maintaining high visual quality.'}, 'zh': {'title': '高效视频生成，轻松实现！', 'desc': '本文介绍了一种高效的视频生成模型Magic 1-For-1（Magic141），该模型优化了内存消耗和推理延迟。其核心思想是将文本到视频生成任务分解为两个更简单的任务：文本到图像生成和图像到视频生成。研究表明，使用相同的优化算法，图像到视频任务的收敛速度确实优于文本到视频任务。通过多种优化技巧，模型能够在短时间内生成高质量的视频片段，显著降低计算成本。'}}}, {'id': 'https://huggingface.co/papers/2502.07374', 'title': 'LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!', 'url': 'https://huggingface.co/papers/2502.07374', 'abstract': "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.", 'score': 17, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '4df9e17df3250cb4', 'authors': ['Dacheng Li', 'Shiyi Cao', 'Tyler Griggs', 'Shu Liu', 'Xiangxi Mo', 'Shishir G. Patil', 'Matei Zaharia', 'Joseph E. Gonzalez', 'Ion Stoica'], 'affiliations': ['Department of Electrical Engineering and Computer Sciences, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.07374.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#math', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение длинным цепочкам рассуждений в больших языковых моделях', 'desc': 'Исследование показывает, что большие языковые модели (LLM) могут эффективно обучаться длинным цепочкам рассуждений (Long CoT) с помощью контролируемой дообучки на небольшом наборе данных. Модель Qwen2.5-32B-Instruct, обученная на 17 тысячах примеров Long CoT, значительно улучшила результаты в задачах по математике и программированию. Структура Long CoT оказалась критически важной для обучения, в то время как содержание отдельных шагов рассуждения имело минимальное влияние. Эти выводы углубляют понимание того, как развивать способности к рассуждению в LLM.'}, 'en': {'title': 'Unlocking Reasoning: Structure Over Content in Large Models', 'desc': 'This paper explores how Large Reasoning Models (LRMs) can improve their reasoning abilities by learning from structured long chain-of-thought (Long CoT) examples. It demonstrates that a Large Language Model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning and low-rank adaptation techniques. The study reveals that the structure of Long CoT is crucial for learning, while the specific content of reasoning steps has a minimal effect on performance. The findings suggest that maintaining logical consistency in reasoning steps is vital for accuracy, even when training on incorrect samples.'}, 'zh': {'title': '长链思维：推理模型的关键结构', 'desc': '大型推理模型（LRMs）通过长链思维（Long CoT）解决复杂的推理问题，这种思维方式包括反思、回溯和自我验证。我们发现，大型语言模型（LLM）可以通过数据高效的监督微调（SFT）和参数高效的低秩适应（LoRA）有效学习长链思维。仅使用17,000个长链思维训练样本，Qwen2.5-32B-Instruct模型在多个数学和编码基准测试中取得了显著提升。研究表明，长链思维的结构对学习过程至关重要，而单个推理步骤的内容对性能影响较小。'}}}, {'id': 'https://huggingface.co/papers/2502.06857', 'title': 'Gemstones: A Model Suite for Multi-Faceted Scaling Laws', 'url': 'https://huggingface.co/papers/2502.06857', 'abstract': 'Scaling laws are typically fit using a family of models with a narrow range of frozen hyper-parameter choices. In this work we study scaling laws using a wide range of architecture and hyper-parameter choices, and highlight their impact on resulting prescriptions. As a primary artifact of our research, we release the Gemstones: the most comprehensive open-source scaling law dataset to date, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters; these models have been trained with different learning rates, cooldown schedules, and architectural shapes. Our checkpoints enable more complex studies of scaling, such as a law that predicts language modeling performance as a function of model width and depth. By examining the various facets of our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting. Code: https://github.com/mcleish7/gemstone-scaling-laws', 'score': 16, 'issue_id': 2172, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '8f1b246a774832da', 'authors': ['Sean McLeish', 'John Kirchenbauer', 'David Yu Miller', 'Siddharth Singh', 'Abhinav Bhatele', 'Micah Goldblum', 'Ashwinee Panda', 'Tom Goldstein'], 'affiliations': ['Columbia University', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2502.06857.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#architecture', '#optimization'], 'emoji': '💎', 'ru': {'title': 'Переосмысление законов масштабирования в глубоком обучении', 'desc': 'Статья исследует законы масштабирования в машинном обучении, используя широкий спектр архитектурных и гиперпараметрических вариаций. Авторы представляют набор данных Gemstones, содержащий более 4000 контрольных точек трансформеров с различными параметрами обучения. Исследование показывает, что рекомендации, основанные на законах масштабирования, могут сильно зависеть от экспериментального дизайна и выбранных моделей. Работа предлагает более сложный подход к изучению масштабирования, включая закон, предсказывающий производительность языкового моделирования в зависимости от ширины и глубины модели.'}, 'en': {'title': 'Unlocking Scaling Laws with Gemstones Dataset', 'desc': 'This paper explores scaling laws in machine learning by analyzing a diverse set of models with various hyper-parameters. The authors introduce the Gemstones dataset, which includes over 4000 transformer model checkpoints, allowing for extensive experimentation on scaling effects. They demonstrate that the performance of language models can be predicted based on their architecture, specifically width and depth. The findings reveal that scaling law prescriptions are sensitive to the design of experiments and the specific models used, emphasizing the importance of comprehensive datasets in understanding scaling behavior.'}, 'zh': {'title': '探索缩放法则的多样性与敏感性', 'desc': '本文研究了缩放法则，使用了多种架构和超参数选择，强调了这些选择对结果的影响。我们发布了Gemstones数据集，这是迄今为止最全面的开源缩放法则数据集，包含超过4000个来自变换器模型的检查点，参数量高达20亿。通过这些检查点，我们能够进行更复杂的缩放研究，例如预测语言建模性能与模型宽度和深度的关系。研究发现，缩放法则的适用性对实验设计过程和使用的特定模型检查点非常敏感。'}}}, {'id': 'https://huggingface.co/papers/2502.03492', 'title': 'Teaching Language Models to Critique via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.03492', 'abstract': 'Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose CTRL, a framework for Critic Training via Reinforcement Learning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with CTRL significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.', 'score': 14, 'issue_id': 2165, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'a0c98706806837c6', 'authors': ['Zhihui Xie', 'Jie chen', 'Liyu Chen', 'Weichao Mao', 'Jingjing Xu', 'Lingpeng Kong'], 'affiliations': ['Bytedance, Seed', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.03492.jpg', 'data': {'categories': ['#rlhf', '#training', '#benchmark', '#reasoning', '#rl', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Самосовершенствующиеся ИИ-критики для генерации кода', 'desc': 'Статья представляет CTRL - фреймворк для обучения моделей-критиков с помощью обучения с подкреплением. Цель - научить языковые модели (LLM) генерировать полезную обратную связь для улучшения качества генерируемого кода без участия человека. Результаты показывают, что обученные критики значительно повышают процент успешных решений и уменьшают накопление ошибок. Модели-критики также могут использоваться как генеративные модели вознаграждения и позволяют улучшать результаты во время тестирования через итеративную критику и исправления.'}, 'en': {'title': 'Empowering Code Generation with Self-Critiquing Models', 'desc': 'This paper introduces CTRL, a framework that trains large language models (LLMs) to act as critics for code generation tasks. By using reinforcement learning, CTRL enables these critic models to provide feedback that helps improve the performance of a fixed generator model without needing human input. The study shows that critics trained with CTRL can significantly increase the success rates of code generation and reduce errors. Additionally, these critics function as effective generative reward models, allowing for iterative improvements during testing, leading to substantial performance gains on difficult benchmarks.'}, 'zh': {'title': '通过批评训练提升代码生成性能', 'desc': '本文研究了大型语言模型（LLMs）在代码生成中的批评和改进能力。我们提出了CTRL框架，通过强化学习训练批评模型，生成反馈以提高固定生成模型的修正性能，而无需人工监督。实验结果表明，使用CTRL训练的批评模型显著提高了通过率，并减少了累积错误。此外，这些批评模型作为生成奖励模型，能够在测试时通过迭代批评-修订实现扩展，在具有挑战性的代码生成基准上实现了高达106.1%的相对改进。'}}}, {'id': 'https://huggingface.co/papers/2502.07617', 'title': 'Scaling Pre-training to One Hundred Billion Data for Vision Language Models', 'url': 'https://huggingface.co/papers/2502.07617', 'abstract': "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.", 'score': 13, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '503a9dac2cae323c', 'authors': ['Xiao Wang', 'Ibrahim Alabdulmohsin', 'Daniel Salz', 'Zhe Li', 'Keran Rong', 'Xiaohua Zhai'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.07617.jpg', 'data': {'categories': ['#cultural_diversity', '#multilingual', '#dataset', '#low_resource', '#data', '#multimodal', '#benchmark'], 'emoji': '🌍', 'ru': {'title': 'Масштабное предобучение для инклюзивных мультимодальных систем', 'desc': 'Исследование посвящено предобучению мультимодальных моделей на беспрецедентно большом наборе данных в 100 миллиардов примеров. Авторы обнаружили, что производительность модели на многих западноцентричных бенчмарках насыщается при таком масштабе. Однако задачи, связанные с культурным разнообразием, показывают значительный прирост благодаря охвату редких концепций в больших данных. Кроме того, исследование выявило улучшение производительности для низкоресурсных языков и предостерегает от чрезмерной фильтрации данных, которая может снизить культурное разнообразие.'}, 'en': {'title': 'Unlocking Cultural Diversity with 100 Billion Examples', 'desc': 'This paper investigates the effects of pre-training vision-language models using a massive dataset of 100 billion examples. The authors find that while performance on common benchmarks tends to plateau, tasks that involve cultural diversity show significant improvements due to the extensive coverage of diverse concepts in the dataset. Additionally, the study highlights the benefits of this large-scale data for enhancing multilingual capabilities, particularly for low-resource languages. However, it also warns that applying quality filters to reduce dataset size can diminish the representation of cultural diversity, emphasizing the importance of large-scale data for inclusive multimodal systems.'}, 'zh': {'title': '大规模预训练助力文化多样性', 'desc': '本文探讨了在前所未有的规模上（1000亿个示例）对视觉-语言模型进行预训练的潜力。研究发现，在许多常见的西方分类和检索基准上，模型性能在此规模下趋于饱和。然而，对于文化多样性的任务，1000亿规模的网络数据带来了更显著的提升，因为它涵盖了长尾概念。此外，研究还分析了模型的多语言能力，显示在低资源语言上也有提升。'}}}, {'id': 'https://huggingface.co/papers/2502.07508', 'title': 'Enhance-A-Video: Better Generated Video for Free', 'url': 'https://huggingface.co/papers/2502.07508', 'abstract': 'DiT-based video generation has achieved remarkable results, but research into enhancing existing models remains relatively unexplored. In this work, we introduce a training-free approach to enhance the coherence and quality of DiT-based generated videos, named Enhance-A-Video. The core idea is enhancing the cross-frame correlations based on non-diagonal temporal attention distributions. Thanks to its simple design, our approach can be easily applied to most DiT-based video generation frameworks without any retraining or fine-tuning. Across various DiT-based video generation models, our approach demonstrates promising improvements in both temporal consistency and visual quality. We hope this research can inspire future explorations in video generation enhancement.', 'score': 12, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'e02d3082d3b21016', 'authors': ['Yang Luo', 'Xuanlei Zhao', 'Mengzhao Chen', 'Kaipeng Zhang', 'Wenqi Shao', 'Kai Wang', 'Zhangyang Wang', 'Yang You'], 'affiliations': ['National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.07508.jpg', 'data': {'categories': ['#video', '#training'], 'emoji': '🎬', 'ru': {'title': 'Enhance-A-Video: Повышение качества генерации видео без переобучения', 'desc': 'Данная статья представляет новый подход к улучшению качества видео, генерируемого моделями на основе DiT (Diffusion Transformer), без необходимости дополнительного обучения. Метод, названный Enhance-A-Video, фокусируется на усилении межкадровых корреляций с использованием недиагональных распределений временного внимания. Подход легко применим к большинству существующих фреймворков генерации видео на основе DiT без переобучения или дополнительной настройки. Результаты демонстрируют значительное улучшение как временной согласованности, так и визуального качества генерируемых видео.'}, 'en': {'title': 'Enhancing DiT Video Generation Without Retraining', 'desc': 'This paper presents a novel method called Enhance-A-Video, aimed at improving the coherence and quality of videos generated by DiT-based models. The approach focuses on enhancing cross-frame correlations using non-diagonal temporal attention distributions, which helps maintain consistency across frames. Importantly, Enhance-A-Video does not require any retraining or fine-tuning, making it easy to integrate into existing DiT frameworks. The results show significant improvements in both temporal consistency and visual quality, paving the way for further advancements in video generation techniques.'}, 'zh': {'title': '提升视频生成质量的新方法', 'desc': '本研究提出了一种名为 Enhance-A-Video 的方法，旨在提高基于 DiT 的视频生成模型的连贯性和质量。该方法通过增强跨帧相关性，利用非对角时间注意力分布来实现。由于其设计简单，该方法可以轻松应用于大多数基于 DiT 的视频生成框架，而无需重新训练或微调。我们的实验表明，该方法在时间一致性和视觉质量方面都取得了显著的改善。'}}}, {'id': 'https://huggingface.co/papers/2502.06589', 'title': 'Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training', 'url': 'https://huggingface.co/papers/2502.06589', 'abstract': 'Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.', 'score': 10, 'issue_id': 2164, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '4273eabfdc59b328', 'authors': ['Yuchen Zhuang', 'Jingfeng Yang', 'Haoming Jiang', 'Xin Liu', 'Kewei Cheng', 'Sanket Lokegaonkar', 'Yifan Gao', 'Qing Ping', 'Tianyi Liu', 'Binxuan Huang', 'Zheng Li', 'Zhengyang Wang', 'Pei Chen', 'Ruijie Wang', 'Rongzhi Zhang', 'Nasser Zalmout', 'Priyanka Nigam', 'Bing Yin', 'Chao Zhang'], 'affiliations': ['Amazon', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.06589.jpg', 'data': {'categories': ['#agents', '#transfer_learning', '#optimization', '#training', '#reasoning', '#dataset', '#benchmark'], 'emoji': '🛠️', 'ru': {'title': 'Кузница агентов: улучшение LLM через специализированное предобучение', 'desc': 'Исследователи представили Hephaestus-Forge - первый крупномасштабный корпус для предобучения, направленный на улучшение фундаментальных способностей агентов на основе больших языковых моделей (LLM). Корпус содержит 103 миллиарда специфичных для агентов данных, охватывающих 76,537 API, включая документацию и траектории вызовов функций. Применение Hephaestus-Forge в процессе дообучения позволило модели Hephaestus превзойти открытые LLM малого и среднего масштаба и конкурировать с коммерческими LLM в трех тестах для агентов. Это демонстрирует эффективность предложенного подхода в улучшении базовых агентных возможностей и обобщающей способности LLM для новых задач и сред.'}, 'en': {'title': 'Empowering LLM Agents with Hephaestus-Forge', 'desc': 'This paper presents Hephaestus-Forge, a large-scale pre-training dataset specifically designed for enhancing the capabilities of large language model (LLM) agents. It includes 103 billion agent-specific data points, featuring 76,537 APIs, which provide both documentation and function calling examples to improve reasoning and planning skills. The authors explore different training protocols and data mixing ratios to optimize the pre-training process. Results show that agents trained on Hephaestus-Forge outperform smaller open-source LLMs and compete with commercial models, highlighting its effectiveness in improving agent performance and adaptability.'}, 'zh': {'title': 'Hephaestus-Forge：提升LLM代理能力的创新预训练语料库', 'desc': '由于缺乏面向代理的预训练数据，基于大型语言模型（LLM）的自主代理通常依赖复杂的提示或广泛的微调，这往往无法在保持强泛化能力的同时引入新功能。我们提出了Hephaestus-Forge，这是第一个大规模预训练语料库，旨在增强LLM代理在API功能调用、内在推理和规划以及适应环境反馈方面的基本能力。Hephaestus-Forge包含1030亿个特定于代理的数据，涵盖76,537个API，包括工具文档以介绍API功能的知识和功能调用轨迹以增强内在推理。通过在Hephaestus-Forge上持续预训练，Hephaestus在三个代理基准测试中超越了小到中型的开源LLM，并与商业LLM相媲美，证明了我们的预训练语料库在增强代理基本能力和LLM对新任务或环境的泛化能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.04223', 'title': 'Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents', 'url': 'https://huggingface.co/papers/2502.04223', 'abstract': "Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure -- including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages -- as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce \\'Eclair, a general-purpose text-extraction tool specifically designed to process a wide range of document types. Given an image, \\'Eclair is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark for document-level OCR and semantic classification. \\'Eclair achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate \\'Eclair on established benchmarks, demonstrating its versatility and strength across several evaluation standards.", 'score': 9, 'issue_id': 2170, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '98e15ce7f5732b9f', 'authors': ['Ilia Karmanov', 'Amala Sanjay Deshmukh', 'Lukas Voegtle', 'Philipp Fischer', 'Kateryna Chumachenko', 'Timo Roman', 'Jarno Seppänen', 'Jupinder Parmar', 'Joseph Jennings', 'Andrew Tao', 'Karan Sapra'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2502.04223.jpg', 'data': {'categories': ['#benchmark', '#science', '#data', '#dataset', '#optimization', '#cv'], 'emoji': '📄', 'ru': {'title': 'Éclair: Передовой инструмент OCR для комплексного анализа структуры документов', 'desc': 'Статья представляет Éclair - инструмент для извлечения текста из изображений документов. Éclair не только извлекает текст, но и понимает структуру документа, включая форматирование, формулы, таблицы и порядок чтения. Инструмент также способен определять семантические элементы, такие как сноски и подписи к изображениям. Авторы представляют новый разнообразный бенчмарк для оценки OCR на уровне документов и семантической классификации, на котором Éclair показывает наилучшие результаты.'}, 'en': {'title': 'Eclair: Revolutionizing Document Understanding with Advanced OCR', 'desc': "This paper presents 'Eclair', an advanced Optical Character Recognition (OCR) tool designed to extract not just text, but also the structural and semantic elements of complex documents. It recognizes formatting, tables, and reading order, which are essential for understanding multi-page documents. 'Eclair' provides bounding boxes and semantic classes for extracted text, enhancing its utility for tasks like document retrieval and question answering. The tool demonstrates state-of-the-art performance on a newly introduced benchmark, showcasing its effectiveness compared to existing methods."}, 'zh': {'title': 'Eclair：全面理解文档的OCR工具', 'desc': "光学字符识别（OCR）技术广泛应用于从文档图像中提取文本，促进高效的数字化和数据检索。然而，仅仅提取文本对于处理复杂文档是不够的。全面理解这些文档需要了解其结构，包括格式、公式、表格以及跨多个页面的多个块和列的阅读顺序，以及检测脚注和图像标题等元素的语义信息。为了解决这个问题，我们提出了'Eclair'，这是一种通用的文本提取工具，专门设计用于处理各种文档类型，并在文档级OCR和语义分类的基准测试中实现了最先进的准确性。"}}}, {'id': 'https://huggingface.co/papers/2502.03997', 'title': 'CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing', 'url': 'https://huggingface.co/papers/2502.03997', 'abstract': 'Computer Aided Design (CAD) is indispensable across various industries. Text-based CAD editing, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce CAD-Editor, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.', 'score': 8, 'issue_id': 2165, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '69bfc4de9cf7106d', 'authors': ['Yu Yuan', 'Shizhao Sun', 'Qi Liu', 'Jiang Bian'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.03997.jpg', 'data': {'categories': ['#data', '#dataset', '#multimodal', '#architecture'], 'emoji': '🛠️', 'ru': {'title': 'CAD-Editor: Революция в текстовом редактировании CAD-моделей', 'desc': "CAD-Editor - это первая система для редактирования CAD-моделей на основе текстовых инструкций. Она использует автоматизированный конвейер для синтеза обучающих данных, сочетая модели вариаций дизайна и большие мультимодальные языковые модели. Система применяет подход 'locate-then-infill', разбивая задачу на локализацию областей для изменения и их заполнение соответствующими правками. CAD-Editor опирается на большие языковые модели для понимания естественного языка и знаний о CAD, демонстрируя превосходные результаты в экспериментах."}, 'en': {'title': 'Revolutionizing CAD Editing with Text Instructions', 'desc': 'This paper presents CAD-Editor, a novel framework for text-based editing of Computer Aided Design (CAD) models. It addresses the limitations of existing methods by integrating automated data synthesis and Large Vision-Language Models (LVLMs) to generate editing instructions from original and modified CAD models. The framework employs a locate-then-infill approach, breaking down the editing process into identifying areas for change and applying the necessary modifications. Experimental results demonstrate that CAD-Editor outperforms previous techniques in both quantitative metrics and qualitative assessments.'}, 'zh': {'title': '文本驱动的CAD编辑新纪元', 'desc': '计算机辅助设计（CAD）在各个行业中至关重要。基于文本的CAD编辑可以根据文本指令自动修改CAD模型，但这一领域尚未得到充分探索。现有方法主要集中在设计变体生成或基于文本的CAD生成，缺乏对文本控制的支持或忽视了现有CAD模型的约束。我们提出了CAD-Editor，这是第一个用于基于文本的CAD编辑的框架，利用大型语言模型（LLMs）和自动化数据合成管道，实现了高效的编辑指令生成和模型修改。'}}}, {'id': 'https://huggingface.co/papers/2502.07527', 'title': 'NatureLM: Deciphering the Language of Nature for Scientific Discovery', 'url': 'https://huggingface.co/papers/2502.07527', 'abstract': 'Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.', 'score': 8, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'a6e947f52bde9a9c', 'authors': ['Yingce Xia', 'Peiran Jin', 'Shufang Xie', 'Liang He', 'Chuan Cao', 'Renqian Luo', 'Guoqing Liu', 'Yue Wang', 'Zequn Liu', 'Yuan-Jyue Chen', 'Zekun Guo', 'Yeqi Bai', 'Pan Deng', 'Yaosen Min', 'Ziheng Lu', 'Hongxia Hao', 'Han Yang', 'Jielan Li', 'Chang Liu', 'Jia Zhang', 'Jianwei Zhu', 'Kehan Wu', 'Wei Zhang', 'Kaiyuan Gao', 'Qizhi Pei', 'Qian Wang', 'Xixian Liu', 'Yanting Li', 'Houtian Zhu', 'Yeqing Lu', 'Mingqian Ma', 'Zun Wang', 'Tian Xie', 'Krzysztof Maziarz', 'Marwin Segler', 'Zhao Yang', 'Zilong Chen', 'Yu Shi', 'Shuxin Zheng', 'Lijun Wu', 'Chen Hu', 'Peggy Dai', 'Tie-Yan Liu', 'Haiguang Liu', 'Tao Qin'], 'affiliations': ['Microsoft Research AI for Science'], 'pdf_title_img': 'assets/pdf/title_img/2502.07527.jpg', 'data': {'categories': ['#healthcare', '#architecture', '#science', '#optimization', '#transfer_learning', '#training', '#dataset', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'NatureLM: единая модель для научных открытий во множестве доменов', 'desc': 'Исследователи разработали NatureLM - языковую модель для научных открытий, объединяющую различные научные домены. Эта фундаментальная модель обучена на данных из нескольких областей, включая молекулы, материалы, белки, ДНК и РНК. NatureLM способна генерировать и оптимизировать объекты из разных доменов, а также выполнять кросс-доменные задачи. Модель демонстрирует высокую производительность в различных научных задачах и доступна в нескольких размерах, от 1 до 46,7 миллиардов параметров.'}, 'en': {'title': 'NatureLM: Unifying Science Through Language Models', 'desc': 'This paper introduces Nature Language Model (NatureLM), a foundation model designed to enhance scientific discovery by integrating knowledge across various scientific domains. Unlike traditional models that operate in isolation, NatureLM is pre-trained on data from multiple fields, allowing it to understand and generate sequences related to small molecules, proteins, RNA, and materials. The model supports diverse applications, such as generating new compounds and optimizing existing ones, while also excelling in specific tasks like translating chemical notations. With different sizes available, NatureLM demonstrates improved performance with larger models, showcasing its potential as a versatile tool in drug discovery and material design.'}, 'zh': {'title': '自然语言模型：科学发现的新工具', 'desc': '基础模型在自然语言处理和人工智能领域带来了革命性的变化，显著提升了机器理解和生成自然语言的能力。受基础模型成功的启发，研究人员为各个科学领域开发了相应的基础模型，但这些模型通常是孤立训练的，缺乏跨领域整合的能力。我们提出了自然语言模型（NatureLM），这是一个基于序列的科学基础模型，旨在促进科学发现。NatureLM经过多领域数据的预训练，能够支持小分子、蛋白质、RNA和材料的生成与优化，并在多个科学任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.05364', 'title': 'Hypencoder: Hypernetworks for Information Retrieval', 'url': 'https://huggingface.co/papers/2502.05364', 'abstract': "The vast majority of retrieval models depend on vector inner products to produce a relevance score between a query and a document. This naturally limits the expressiveness of the relevance score that can be employed. We propose a new paradigm, instead of producing a vector to represent the query we produce a small neural network which acts as a learned relevance function. This small neural network takes in a representation of the document, in this paper we use a single vector, and produces a scalar relevance score. To produce the little neural network we use a hypernetwork, a network that produce the weights of other networks, as our query encoder or as we call it a Hypencoder. Experiments on in-domain search tasks show that Hypencoder is able to significantly outperform strong dense retrieval models and has higher metrics then reranking models and models an order of magnitude larger. Hypencoder is also shown to generalize well to out-of-domain search tasks. To assess the extent of Hypencoder's capabilities, we evaluate on a set of hard retrieval tasks including tip-of-the-tongue retrieval and instruction-following retrieval tasks and find that the performance gap widens substantially compared to standard retrieval tasks. Furthermore, to demonstrate the practicality of our method we implement an approximate search algorithm and show that our model is able to search 8.8M documents in under 60ms.", 'score': 5, 'issue_id': 2176, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c3495b093acff010', 'authors': ['Julian Killingback', 'Hansi Zeng', 'Hamed Zamani'], 'affiliations': ['University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.05364.jpg', 'data': {'categories': ['#benchmark', '#rag', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Hypencoder: революция в релевантном поиске с помощью динамических нейросетей', 'desc': 'В статье представлен новый подход к информационному поиску, называемый Hypencoder. Вместо создания векторного представления запроса, метод генерирует небольшую нейронную сеть, выступающую в роли функции релевантности. Эта сеть принимает представление документа и выдает скалярную оценку релевантности. Эксперименты показывают, что Hypencoder значительно превосходит существующие модели плотного поиска и ранжирования, особенно на сложных задачах поиска.'}, 'en': {'title': 'Revolutionizing Retrieval with Hypencoder: A Neural Network Approach', 'desc': 'This paper introduces a novel approach to document retrieval by replacing traditional vector representations with a small neural network that functions as a learned relevance function. Instead of using vector inner products to score relevance, the proposed Hypencoder generates a scalar relevance score based on document representations. The use of a hypernetwork allows for efficient weight generation for the Hypencoder, leading to superior performance on various retrieval tasks compared to existing dense retrieval and reranking models. Additionally, the Hypencoder demonstrates strong generalization capabilities and can efficiently search large document collections in a short time frame.'}, 'zh': {'title': 'Hypencoder：超越传统检索模型的新方法', 'desc': '大多数检索模型依赖于向量内积来生成查询和文档之间的相关性评分，这限制了相关性评分的表达能力。我们提出了一种新范式，使用小型神经网络作为学习的相关性函数，而不是生成向量来表示查询。这个小型神经网络接收文档的表示，并输出一个标量的相关性评分。我们的实验表明，Hypencoder在领域内检索任务中显著优于强大的密集检索模型，并且在一些困难的检索任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.06428', 'title': 'CoS: Chain-of-Shot Prompting for Long Video Understanding', 'url': 'https://huggingface.co/papers/2502.06428', 'abstract': 'Multi-modal Large Language Models (MLLMs) struggle with long videos due to the need for excessive visual tokens. These tokens exceed massively the context length of MLLMs, resulting in filled by redundant task-irrelevant shots. How to select shots is an unsolved critical problem: sparse sampling risks missing key details, while exhaustive sampling overwhelms the model with irrelevant content, leading to video misunderstanding. To solve this problem, we propose Chain-of-Shot prompting (CoS). The key idea is to frame shot selection as test-time visual prompt optimisation, choosing shots adaptive to video understanding semantic task by optimising shots-task alignment. CoS has two key parts: (1) a binary video summary mechanism that performs pseudo temporal grounding, discovering a binary coding to identify task-relevant shots, and (2) a video co-reasoning module that deploys the binary coding to pair (learning to align) task-relevant positive shots with irrelevant negative shots. It embeds the optimised shot selections into the original video, facilitating a focus on relevant context to optimize long video understanding. Experiments across three baselines and five datasets demonstrate the effectiveness and adaptability of CoS. Code given in https://lwpyh.github.io/CoS.', 'score': 5, 'issue_id': 2173, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '8302e2d276dc5929', 'authors': ['Jian Hu', 'Zixu Cheng', 'Chenyang Si', 'Wei Li', 'Shaogang Gong'], 'affiliations': ['Nanyang Technological University', 'Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2502.06428.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'Умный выбор кадров для понимания длинных видео искусственным интеллектом', 'desc': 'Статья представляет новый метод Chain-of-Shot prompting (CoS) для улучшения понимания длинных видео мультимодальными большими языковыми моделями (MLLM). CoS решает проблему выбора релевантных кадров путем оптимизации визуальных промптов во время тестирования, адаптивно выбирая кадры в соответствии с задачей понимания видео. Метод включает механизм бинарного обобщения видео и модуль совместных рассуждений, которые помогают идентифицировать и сопоставлять релевантные и нерелевантные кадры. Эксперименты на пяти датасетах показали эффективность и адаптивность предложенного подхода.'}, 'en': {'title': 'Optimizing Video Understanding with Chain-of-Shot Prompting', 'desc': "This paper addresses the challenge that Multi-modal Large Language Models (MLLMs) face when processing long videos, which often contain too many visual tokens. These tokens can overwhelm the model with irrelevant information, making it difficult to understand the video's content. The authors propose a method called Chain-of-Shot prompting (CoS) that optimizes shot selection based on the specific task at hand, improving the alignment between selected shots and the semantic understanding required. CoS includes a binary video summary mechanism and a video co-reasoning module to enhance the model's focus on relevant shots, leading to better video comprehension."}, 'zh': {'title': '优化镜头选择，提升视频理解', 'desc': '多模态大型语言模型（MLLMs）在处理长视频时面临挑战，因为需要过多的视觉标记。这些标记超出了MLLMs的上下文长度，导致填充了大量与任务无关的镜头。如何选择镜头是一个未解决的关键问题：稀疏采样可能会错过关键细节，而全面采样则会使模型被无关内容淹没，从而导致视频理解错误。为了解决这个问题，我们提出了镜头链提示（CoS），通过优化镜头与任务的对齐来选择适合视频理解的镜头。'}}}, {'id': 'https://huggingface.co/papers/2502.07490', 'title': 'Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More', 'url': 'https://huggingface.co/papers/2502.07490', 'abstract': "Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.", 'score': 5, 'issue_id': 2172, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '2d86c4124095af30', 'authors': ['Xialie Zhuang', 'Zhikai Jia', 'Jianjin Li', 'Zhenyu Zhang', 'Li Shen', 'Zheng Cao', 'Shiwei Liu'], 'affiliations': ['SCITIX (SGP) TECH PTE. LTD., Singapore', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, China', 'South China Normal University, China', 'Sun YatSen University, China', 'University of Oxford, UK', 'University of Texas at Austin, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.07490.jpg', 'data': {'categories': ['#training', '#reasoning', '#long_context', '#architecture', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'MEAP: Улучшение извлечения информации в больших языковых моделях', 'desc': 'Исследователи предложили новый метод обучения больших языковых моделей под названием MEAP (Mask-Enhanced Autoregressive Prediction). MEAP объединяет маскированное языковое моделирование (MLM) с предсказанием следующего токена (NTP) для улучшения способности модели извлекать ключевую информацию из контекста. Эксперименты показали, что MEAP значительно превосходит NTP в задачах извлечения ключевой информации и рассуждений на основе длинного контекста. Метод также демонстрирует преимущества при дообучении на размеченных данных, особенно в сценариях с потерей информации в середине последовательности.'}, 'en': {'title': 'Enhancing Information Retrieval in LLMs with MEAP', 'desc': 'This paper introduces Mask-Enhanced Autoregressive Prediction (MEAP), a new training method for Large Language Models (LLMs) that improves their ability to retrieve important information. MEAP combines Masked Language Modeling (MLM) with Next-Token Prediction (NTP) by masking some input tokens and then predicting the next token using a decoder-only Transformer. This approach avoids the complexity of bidirectional attention and encoder-decoder structures, making it computationally efficient during training and inference. Experimental results show that MEAP significantly enhances performance in information retrieval and reasoning tasks, especially in scenarios where context is crucial, while also benefiting supervised fine-tuning.'}, 'zh': {'title': '掩码增强自回归预测：提升语言模型的信息检索能力', 'desc': '大型语言模型（LLMs）在准确检索关键信息方面存在问题。为了解决这个问题，我们提出了一种名为掩码增强自回归预测（MEAP）的训练范式，它将掩码语言建模（MLM）与下一个标记预测（NTP）无缝结合，以增强后者的上下文检索能力。MEAP通过随机掩盖输入标记的一小部分，然后使用仅解码器的Transformer直接进行标准的下一个标记预测。实验表明，MEAP在关键信息检索和长上下文推理任务上显著优于NTP，同时在常识推理任务上表现相当或更好。'}}}, {'id': 'https://huggingface.co/papers/2502.07531', 'title': 'VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.07531', 'abstract': 'Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.', 'score': 5, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'dea5fd89dd98f3b1', 'authors': ['Sixiao Zheng', 'Zimian Peng', 'Yanpeng Zhou', 'Yi Zhu', 'Hang Xu', 'Xiangru Huang', 'Yanwei Fu'], 'affiliations': ['Fudan University, China', 'Huawei Noahs Ark Lab, China', 'Westlake University, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07531.jpg', 'data': {'categories': ['#video', '#training', '#open_source', '#dataset', '#benchmark', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'Точный контроль над ключевыми элементами при генерации видео из изображений', 'desc': 'VidCRAFT3 - это новая система генерации видео из изображений, позволяющая одновременно контролировать движение камеры, движение объектов и направление освещения. В основе системы лежит Spatial Triple-Attention Transformer, который интегрирует информацию об освещении, тексте и изображении. Для обучения был создан синтетический датасет VideoLightingDirection с аннотациями направления освещения. Предложенная трехэтапная стратегия обучения позволяет обойтись без данных с одновременными аннотациями всех визуальных элементов.'}, 'en': {'title': 'VidCRAFT3: Mastering Multi-Element Control in Image-to-Video Generation', 'desc': 'This paper presents VidCRAFT3, a new framework for generating videos from images with enhanced control over multiple visual elements, including camera motion, object motion, and lighting direction. The authors introduce the Spatial Triple-Attention Transformer, which allows for better separation and management of these elements during the generation process. To support this framework, they created the VideoLightingDirection (VLD) dataset, which includes detailed lighting annotations to improve the realism of generated videos. The proposed three-stage training strategy enables effective learning without requiring simultaneous annotations for all visual elements, leading to superior video quality and coherence compared to existing methods.'}, 'zh': {'title': 'VidCRAFT3：多元素控制的图像到视频生成新框架', 'desc': '本文介绍了一种新的图像到视频生成框架VidCRAFT3，能够同时控制相机运动、物体运动和光照方向。为了更好地解耦每个视觉元素的控制，提出了空间三重注意力变换器，能够对光照方向、文本和图像进行对称整合。由于大多数真实世界视频数据集缺乏光照注释，研究者构建了一个高质量的合成视频数据集VideoLightingDirection（VLD），该数据集包含光照方向注释和多样化外观的物体。通过广泛的实验，VidCRAFT3在生成高质量视频内容方面表现优异，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2502.07445', 'title': 'Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon', 'url': 'https://huggingface.co/papers/2502.07445', 'abstract': "Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.", 'score': 4, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '3e6282dd3913750a', 'authors': ['Nurit Cohen-Inger', 'Yehonatan Elisha', 'Bracha Shapira', 'Lior Rokach', 'Seffi Cohen'], 'affiliations': ['Ben Gurion University', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07445.jpg', 'data': {'categories': ['#interpretability', '#training', '#dataset', '#hallucinations', '#benchmark', '#optimization'], 'emoji': '🦎', 'ru': {'title': 'Разоблачение иллюзии понимания: как языковые модели маскируют переобучение', 'desc': 'Статья представляет новый метод оценки языковых моделей под названием C-BOD. Этот метод выявляет переобучение моделей путем систематического искажения входных данных с сохранением их семантического содержания. Исследование показало, что многие модели, включая крупные ЯМ, демонстрируют значительное снижение производительности при небольших изменениях формулировок. Авторы призывают сообщество уделять больше внимания устойчивости и обобщающей способности моделей, а не только показателям в рейтингах.'}, 'en': {'title': 'Beyond Scores: Evaluating True Language Understanding in LLMs', 'desc': "This paper discusses the limitations of large language models (LLMs) in truly understanding language, as they often rely on specific patterns in datasets rather than genuine comprehension. The authors introduce the Chameleon Benchmark Overfit Detector (C-BOD), a tool that modifies benchmark prompts to test if LLMs are overfitting to memorized cues. By analyzing the performance of 26 leading LLMs on the MMLU benchmark, they find that many models show a decline in performance when faced with slight changes in input, indicating a reliance on fixed patterns. The study emphasizes the need for better evaluation methods that focus on a model's ability to generalize and understand language, rather than just achieving high scores on benchmarks."}, 'zh': {'title': '超越分数，关注模型的鲁棒性与泛化能力', 'desc': '大型语言模型（LLMs）在公共基准测试中表现优异，但这些高分可能掩盖了模型对特定数据集表面特征的过度依赖，而非真正的语言理解。我们提出了变色龙基准过拟合检测器（C-BOD），这是一个通过参数变换系统性扭曲基准提示的元评估框架，用于检测LLMs的过拟合。C-BOD通过重新表述输入，同时保持其语义内容和标签，揭示模型性能是否受到记忆模式的驱动。我们的研究表明，经过适度扰动后，26个领先的LLM在MMLU基准上的平均性能下降了2.15%，这表明模型在评估时需要关注更强的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.06755', 'title': 'Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models', 'url': 'https://huggingface.co/papers/2502.06755', 'abstract': 'To truly understand vision models, we must not only interpret their learned features but also validate these interpretations through controlled experiments. Current approaches either provide interpretable features without the ability to test their causal influence, or enable model editing without interpretable controls. We present a unified framework using sparse autoencoders (SAEs) that bridges this gap, allowing us to discover human-interpretable visual features and precisely manipulate them to test hypotheses about model behavior. By applying our method to state-of-the-art vision models, we reveal key differences in the semantic abstractions learned by models with different pre-training objectives. We then demonstrate the practical usage of our framework through controlled interventions across multiple vision tasks. We show that SAEs can reliably identify and manipulate interpretable visual features without model re-training, providing a powerful tool for understanding and controlling vision model behavior. We provide code, demos and models on our project website: https://osu-nlp-group.github.io/SAE-V.', 'score': 3, 'issue_id': 2175, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': 'cf5a833e93ca6f88', 'authors': ['Samuel Stevens', 'Wei-Lun Chao', 'Tanya Berger-Wolf', 'Yu Su'], 'affiliations': ['The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06755.jpg', 'data': {'categories': ['#cv', '#architecture', '#optimization', '#open_source', '#interpretability'], 'emoji': '🔬', 'ru': {'title': 'Разреженные автоэнкодеры: ключ к пониманию и контролю моделей компьютерного зрения', 'desc': 'Статья представляет унифицированную систему на основе разреженных автоэнкодеров (SAE) для интерпретации и контроля визуальных моделей машинного обучения. Метод позволяет обнаруживать интерпретируемые человеком визуальные признаки и точно манипулировать ими для проверки гипотез о поведении модели. Исследователи применили свой подход к современным моделям компьютерного зрения и выявили ключевые различия в семантических абстракциях, изученных моделями с разными целями предварительного обучения. Авторы демонстрируют практическое применение своей системы через контролируемые вмешательства в различных задачах компьютерного зрения.'}, 'en': {'title': 'Bridging Interpretation and Control in Vision Models with Sparse Autoencoders', 'desc': 'This paper introduces a new framework that uses sparse autoencoders (SAEs) to interpret and manipulate visual features in vision models. It addresses the challenge of validating the causal influence of these features through controlled experiments. By applying this framework, the authors uncover significant differences in the semantic abstractions learned by various models based on their pre-training objectives. The framework allows for reliable identification and manipulation of interpretable features without needing to retrain the models, enhancing our understanding of their behavior.'}, 'zh': {'title': '用稀疏自编码器理解和操控视觉模型', 'desc': '本文提出了一种统一框架，利用稀疏自编码器（SAE）来理解视觉模型的特征。该框架不仅可以发现人类可解释的视觉特征，还能精确操控这些特征，以测试模型行为的假设。通过对先进视觉模型的应用，我们揭示了不同预训练目标模型所学习的语义抽象之间的关键差异。我们的研究表明，SAE能够在不重新训练模型的情况下，可靠地识别和操控可解释的视觉特征，为理解和控制视觉模型行为提供了强大的工具。'}}}, {'id': 'https://huggingface.co/papers/2502.07785', 'title': 'Pippo: High-Resolution Multi-View Humans from a Single Image', 'url': 'https://huggingface.co/papers/2502.07785', 'abstract': 'We present Pippo, a generative model capable of producing 1K resolution dense turnaround videos of a person from a single casually clicked photo. Pippo is a multi-view diffusion transformer and does not require any additional inputs - e.g., a fitted parametric model or camera parameters of the input image. We pre-train Pippo on 3B human images without captions, and conduct multi-view mid-training and post-training on studio captured humans. During mid-training, to quickly absorb the studio dataset, we denoise several (up to 48) views at low-resolution, and encode target cameras coarsely using a shallow MLP. During post-training, we denoise fewer views at high-resolution and use pixel-aligned controls (e.g., Spatial anchor and Plucker rays) to enable 3D consistent generations. At inference, we propose an attention biasing technique that allows Pippo to simultaneously generate greater than 5 times as many views as seen during training. Finally, we also introduce an improved metric to evaluate 3D consistency of multi-view generations, and show that Pippo outperforms existing works on multi-view human generation from a single image.', 'score': 2, 'issue_id': 2179, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'a7eee475138619ae', 'authors': ['Yash Kant', 'Ethan Weber', 'Jin Kyu Kim', 'Rawal Khirodkar', 'Su Zhaoen', 'Julieta Martinez', 'Igor Gilitschenski', 'Shunsuke Saito', 'Timur Bagautdinov'], 'affiliations': ['Meta Reality Labs', 'UC Berkeley', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2502.07785.jpg', 'data': {'categories': ['#multimodal', '#inference', '#3d', '#diffusion', '#video', '#benchmark'], 'emoji': '🔄', 'ru': {'title': 'Генерация 3D-видео человека из одного фото', 'desc': 'Представлена модель Pippo, способная генерировать видео с поворотом человека на 360 градусов в разрешении 1K из одной фотографии. Это мультиракурсный диффузионный трансформер, предварительно обученный на 3 миллиардах изображений людей. Модель использует многоракурсное обучение на студийных данных и пиксельно-выровненное управление для обеспечения 3D-согласованности. На этапе вывода применяется техника смещения внимания, позволяющая генерировать в 5 раз больше ракурсов, чем при обучении.'}, 'en': {'title': 'Pippo: Transforming Single Photos into Stunning 3D Videos!', 'desc': 'Pippo is a generative model designed to create high-resolution videos of a person using just one casual photo. It employs a multi-view diffusion transformer, eliminating the need for extra inputs like camera settings or parametric models. The model is pre-trained on a vast dataset of human images and fine-tuned with studio-captured data to enhance its performance. Pippo introduces innovative techniques for generating multiple views and evaluating the consistency of 3D outputs, outperforming previous methods in generating videos from single images.'}, 'zh': {'title': 'Pippo：从单张照片生成高质量视频的创新模型', 'desc': 'Pippo是一种生成模型，可以从一张随意拍摄的照片生成1K分辨率的密集视频。它使用多视角扩散变换器，不需要额外的输入，如参数模型或相机参数。Pippo在30亿张无标签的人类图像上进行预训练，并在后续的训练中使用低分辨率和高分辨率的多视角数据进行优化。最终，Pippo在生成多视角人类视频时表现优于现有的方法，且引入了一种新的评估指标来衡量3D一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.07640', 'title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2502.07640', 'abstract': 'We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. The final prover outperforms all existing open-source models in whole-proof generation. On the miniF2F benchmark, it achieves a 57.6% success rate (Pass@32), exceeding the previous best open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.', 'score': 2, 'issue_id': 2175, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'f0efbd784e8053e1', 'authors': ['Yong Lin', 'Shange Tang', 'Bohan Lyu', 'Jiayun Wu', 'Hongzhou Lin', 'Kaiyu Yang', 'Jia Li', 'Mengzhou Xia', 'Danqi Chen', 'Sanjeev Arora', 'Chi Jin'], 'affiliations': ['Amazon', 'Meta FAIR', 'Numina', 'Princeton Language and Intelligence, Princeton University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07640.jpg', 'data': {'categories': ['#training', '#math', '#data', '#benchmark', '#reasoning', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в автоматизированном доказательстве теорем с помощью LLM', 'desc': 'Представлен Goedel-Prover - модель большого языка (LLM) с открытым исходным кодом для автоматизированной генерации формальных математических доказательств. Основная проблема в этой области - нехватка формализованных математических утверждений и доказательств, которую авторы решают путем создания датасета из 1,64 миллиона формальных утверждений. Используется итеративный подход для построения большого набора формальных доказательств, обучая серию доказывающих моделей. Goedel-Prover превосходит все существующие модели с открытым исходным кодом в генерации полных доказательств, достигая 57,6% успеха на бенчмарке miniF2F.'}, 'en': {'title': 'Revolutionizing Formal Proof Generation with Goedel-Prover', 'desc': 'Goedel-Prover is an advanced open-source large language model designed for generating formal proofs in mathematics. It addresses the challenge of limited formalized math statements by creating a dataset of 1.64 million formal statements from natural language problems. The model employs iterative training of provers, where each new prover builds on the successes of its predecessors, leading to improved proof generation capabilities. As a result, Goedel-Prover achieves state-of-the-art performance, surpassing previous models in both proof generation success rates and the volume of formal proofs produced.'}, 'zh': {'title': 'Goedel-Prover：数学证明生成的新突破', 'desc': 'Goedel-Prover 是一个开源的大型语言模型，专注于自动化形式证明生成，特别是在数学问题上表现出色。我们通过训练语句形式化器，将自然语言数学问题转换为正式语言（Lean 4），创建了一个包含164万条正式语句的数据集。使用大型语言模型来验证这些正式语句是否准确保留了原始自然语言问题的内容。最终的证明者在整个证明生成方面超越了所有现有的开源模型，成功率显著提高。'}}}, {'id': 'https://huggingface.co/papers/2502.05932', 'title': 'Skill Expansion and Composition in Parameter Space', 'url': 'https://huggingface.co/papers/2502.05932', 'abstract': "Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However, previous methods suffer from limited training efficiency when expanding new skills and fail to fully leverage prior knowledge to facilitate new task learning. In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library. This library can progressively integrate skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning, facilitating efficient and flexible skill expansion. This structure also enables the direct skill compositions in parameter space by merging LoRA modules that encode different skills, leveraging shared information across skills to effectively program new skills. Based on this, we propose a context-aware module to dynamically activate different skills to collaboratively handle new tasks. Empowering diverse applications including multi-objective composition, dynamics shift, and continual policy shift, the results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC exhibits superior capacity to leverage prior knowledge to efficiently tackle new challenges, as well as expand its skill libraries to evolve the capabilities. Project website: https://ltlhuuu.github.io/PSEC/.", 'score': 2, 'issue_id': 2170, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'd4dd3a87e6ed9712', 'authors': ['Tenglong Liu', 'Jianxiong Li', 'Yinan Zheng', 'Haoyi Niu', 'Yixing Lan', 'Xin Xu', 'Xianyuan Zhan'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'National University of Defense Technology', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05932.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#optimization', '#agents', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эволюция навыков ИИ-агентов через параметрическое расширение и композицию', 'desc': 'Статья представляет новый фреймворк под названием Parametric Skill Expansion and Composition (PSEC) для итеративного развития возможностей автономных агентов. PSEC использует управляемую библиотеку навыков, интегрируя примитивы навыков как модули Low-Rank Adaptation (LoRA) для эффективной донастройки параметров. Фреймворк позволяет напрямую комбинировать навыки в пространстве параметров путем объединения модулей LoRA, кодирующих различные навыки. PSEC демонстрирует превосходную способность использовать предыдущие знания для эффективного решения новых задач и расширения библиотеки навыков.'}, 'en': {'title': 'Empowering Autonomous Agents with Efficient Skill Evolution', 'desc': "This paper introduces Parametric Skill Expansion and Composition (PSEC), a framework that enhances the ability of autonomous agents to learn new skills by building on existing knowledge. PSEC maintains a skill library that allows for efficient integration of skill primitives using Low-Rank Adaptation (LoRA) modules, which support parameter-efficient finetuning. The framework also enables the merging of these modules to create new skills by leveraging shared information, promoting flexibility in skill development. Experimental results demonstrate that PSEC significantly improves the agents' performance in adapting to new challenges while expanding their skill sets effectively."}, 'zh': {'title': '智能体技能的高效扩展与组合', 'desc': '本文提出了一种新的框架，称为参数化技能扩展与组合（PSEC），旨在提高自主智能体在面对新挑战时的能力。该框架通过维护一个可管理的技能库，逐步整合技能原语，支持高效的参数微调，从而实现灵活的技能扩展。PSEC还允许在参数空间中直接组合技能，通过合并不同技能的LoRA模块，利用共享信息有效编程新技能。实验结果表明，PSEC在利用先前知识应对新挑战方面表现出色，并能够扩展其技能库以进化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.04465', 'title': 'FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks', 'url': 'https://huggingface.co/papers/2502.04465', 'abstract': 'Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/.', 'score': 2, 'issue_id': 2167, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a6ebe3d69cd8bcc2', 'authors': ['Luca Della Libera', 'Francesco Paissan', 'Cem Subakan', 'Mirco Ravanelli'], 'affiliations': ['Concordia University, Montreal, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2502.04465.jpg', 'data': {'categories': ['#multilingual', '#audio', '#architecture'], 'emoji': '🎙️', 'ru': {'title': 'FocalCodec: Эффективное сжатие речи с сохранением семантики и акустики', 'desc': 'Исследователи представили FocalCodec - эффективный аудиокодек с низким битрейтом, основанный на фокальной модуляции. Он использует единый бинарный кодбук для сжатия речи до 0.16-0.65 кбит/с, что ниже, чем у современных аналогов. FocalCodec показывает конкурентоспособные результаты в ресинтезе речи и преобразовании голоса, сохраняя при этом семантическую и акустическую информацию. Кодек хорошо справляется с многоязычной речью и шумными условиями, а также подходит для генеративного моделирования.'}, 'en': {'title': 'FocalCodec: Efficient Speech Compression with Single Binary Codebook', 'desc': 'This paper presents FocalCodec, a novel low-bitrate codec designed for speech processing, which addresses the limitations of existing methods that often require complex multi-codebook architectures. By utilizing focal modulation and a single binary codebook, FocalCodec achieves efficient compression of speech at bitrates between 0.16 and 0.65 kbps. The model demonstrates strong performance in tasks like speech resynthesis and voice conversion, while maintaining the integrity of both semantic and acoustic information. Additionally, FocalCodec is effective in handling multilingual speech and noisy environments, making it a promising tool for generative modeling in natural language processing.'}, 'zh': {'title': 'FocalCodec：高效低比特率语音编解码器', 'desc': '大型语言模型通过在海量数据集上进行自监督预训练，彻底改变了自然语言处理。受到这一成功的启发，研究人员尝试将这些方法应用于语音处理，通过神经音频编解码器将连续音频离散化为标记。然而，现有方法面临高比特率、语义或声学信息丢失以及多代码本设计的复杂性等限制。为了解决这些问题，我们提出了FocalCodec，这是一种基于焦点调制的高效低比特率编解码器，能够在0.16到0.65 kbps之间压缩语音，同时在语音重合成和语音转换中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.07776', 'title': 'Auditing Prompt Caching in Language Model APIs', 'url': 'https://huggingface.co/papers/2502.07776', 'abstract': "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.", 'score': 2, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '48f7472ef1c86b27', 'authors': ['Chenchen Gu', 'Xiang Lisa Li', 'Rohith Kuditipudi', 'Percy Liang', 'Tatsunori Hashimoto'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07776.jpg', 'data': {'categories': ['#healthcare', '#leakage', '#inference', '#ethics', '#security', '#data'], 'emoji': '🕵️', 'ru': {'title': 'Кэширование промптов в LLM: скрытая угроза приватности', 'desc': 'Статья исследует проблему кэширования промптов в больших языковых моделях (LLM) и связанные с этим риски утечки данных. Авторы разработали методы статистического аудита для обнаружения кэширования промптов у реальных API-провайдеров LLM. Они обнаружили глобальное разделение кэша между пользователями у семи провайдеров, включая OpenAI, что может привести к утечке информации о промптах пользователей. Кроме того, временные различия из-за кэширования могут раскрывать информацию об архитектуре модели, например, авторы нашли доказательства того, что модель встраивания OpenAI является декодер-ориентированным трансформером.'}, 'en': {'title': 'Timing Variations: A Privacy Risk in Prompt Caching for LLMs', 'desc': "This paper discusses how prompt caching in large language models (LLMs) can lead to timing variations that depend on the data being processed. When prompts are cached, they are handled more quickly than those that are not, which can create vulnerabilities for side-channel attacks. The authors highlight the risks of privacy breaches, especially when cache is shared among users, allowing attackers to infer information about others' prompts based on response times. To address these concerns, the paper presents statistical audits that reveal global cache sharing in several API providers, including OpenAI, and even uncovers details about the model architecture that were previously undisclosed."}, 'zh': {'title': '提示缓存的隐私风险与透明性', 'desc': '在大型语言模型（LLMs）中，提示缓存会导致数据依赖的时间变化：缓存的提示处理速度比非缓存的提示快。这些时间差异可能引发侧信道攻击的风险，例如，如果缓存被多个用户共享，攻击者可以通过快速的API响应时间识别出缓存的提示，从而获取其他用户提示的信息。由于提示缓存可能导致隐私泄露，因此API提供商的缓存政策透明度非常重要。我们开发并进行统计审计，以检测现实世界中LLM API提供商的提示缓存情况，发现七个API提供商（包括OpenAI）之间存在全球缓存共享，可能导致用户提示的隐私泄露。'}}}, {'id': 'https://huggingface.co/papers/2502.06394', 'title': 'SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators', 'url': 'https://huggingface.co/papers/2502.06394', 'abstract': 'Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that models trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting. Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our dataset and code to help further research in multilingual text detoxification.', 'score': 69, 'issue_id': 2145, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '86b7da795fcf943b', 'authors': ['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], 'affiliations': ['AIRI', 'ISP RAS Research Center for Trusted AI', 'Sber AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2502.06394.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#dataset', '#data', '#synthetic', '#open_source'], 'emoji': '🧼', 'ru': {'title': 'Синтетические данные улучшают многоязычную детоксификацию текста', 'desc': 'Статья представляет новый подход к многоязычной детоксификации текста. Авторы разработали конвейер для генерации параллельных многоязычных данных для детоксификации и создали датасет SynthDetoxM, содержащий 16,000 высококачественных пар предложений на немецком, французском, испанском и русском языках. Эксперименты показали, что модели, обученные на синтетических данных, превосходят модели, обученные на аннотированном людьми датасете MultiParaDetox. Авторы опубликовали свой датасет и код для дальнейших исследований в области многоязычной детоксификации текста.'}, 'en': {'title': 'Enhancing Multilingual Detoxification with SynthDetoxM', 'desc': 'This paper addresses the challenge of multilingual text detoxification, which is limited by the lack of parallel datasets in multiple languages. The authors present a new pipeline for generating such datasets, introducing SynthDetoxM, a collection of 16,000 detoxified sentence pairs in German, French, Spanish, and Russian. These pairs were created by rewriting existing toxicity evaluation data using modern open-source large language models (LLMs) in a few-shot learning context. The results show that models trained on this synthetic dataset outperform those trained on existing human-annotated datasets, demonstrating the effectiveness of the proposed approach in enhancing multilingual detoxification efforts.'}, 'zh': {'title': '多语言文本去毒化的新突破', 'desc': '本研究提出了一种生成多语言平行去毒化数据的流程，以解决现有多语言文本去毒化方法中平行多语言数据集稀缺的问题。我们介绍了SynthDetoxM，这是一个手动收集和合成生成的多语言平行文本去毒化数据集，包含来自德语、法语、西班牙语和俄语的16,000对高质量去毒化句子。数据来源于不同的毒性评估数据集，并通过九种现代开源大语言模型在少量样本设置下进行重写。实验结果表明，基于合成数据集训练的模型在数据有限的情况下表现优于基于人工标注的MultiParaDetox数据集训练的模型。'}}}, {'id': 'https://huggingface.co/papers/2502.06703', 'title': 'Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling', 'url': 'https://huggingface.co/papers/2502.06703', 'abstract': 'Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.', 'score': 60, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2129c5ac1750f3cc', 'authors': ['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], 'affiliations': ['BUPT', 'Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06703.jpg', 'data': {'categories': ['#inference', '#reasoning', '#small_models', '#training', '#optimization', '#math'], 'emoji': '🧮', 'ru': {'title': 'Маленькие модели побеждают гигантов: сила масштабирования во время теста', 'desc': 'Это исследование анализирует влияние метода Test-Time Scaling (TTS) на производительность больших языковых моделей (LLM) при решении сложных математических задач. Авторы изучают, как выбор политики модели, модели вознаграждения процесса (PRM) и сложность задачи влияют на оптимальную стратегию TTS. Результаты показывают, что даже небольшие модели могут превзойти более крупные при использовании оптимальной стратегии TTS. Исследование демонстрирует потенциал TTS для улучшения способностей LLM к рассуждению и решению сложных задач.'}, 'en': {'title': 'Unlocking LLM Potential: Small Models, Big Gains with TTS!', 'desc': 'This paper investigates Test-Time Scaling (TTS), a technique that enhances the performance of Large Language Models (LLMs) by adjusting computation during inference. It addresses how different policy models, Process Reward Models (PRMs), and the difficulty of problems affect the effectiveness of TTS. The authors conduct experiments on MATH-500 and AIME24 tasks, revealing that smaller models can outperform larger ones when using an optimal TTS strategy. The results emphasize the importance of tailoring TTS methods to specific models and tasks to improve reasoning capabilities in LLMs.'}, 'zh': {'title': '优化测试时间扩展，提升小型模型性能！', 'desc': '测试时间扩展（TTS）是一种通过在推理阶段增加计算量来提高大型语言模型（LLMs）性能的方法。本文系统分析了策略模型、过程奖励模型（PRMs）和问题难度如何影响TTS的效果。研究表明，计算最优的TTS策略依赖于所选的策略模型、PRM和问题难度，且小型模型在某些情况下可以超越大型模型。通过在MATH-500和AIME24任务上的实验，我们发现适应特定任务和模型的TTS策略对于提升LLMs的推理能力至关重要。'}}}, {'id': 'https://huggingface.co/papers/2502.06781', 'title': 'Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2502.06781', 'abstract': 'Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through Outcome REwArd-based reinforcement Learning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future researchhttps://github.com/InternLM/OREAL.', 'score': 36, 'issue_id': 2142, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '9cd2694b7c865b94', 'authors': ['Chengqi Lyu', 'Songyang Gao', 'Yuzhe Gu', 'Wenwei Zhang', 'Jianfei Gao', 'Kuikun Liu', 'Ziyi Wang', 'Shuaibin Li', 'Qian Zhao', 'Haian Huang', 'Weihan Cao', 'Jiangning Liu', 'Hongwei Liu', 'Junnan Liu', 'Songyang Zhang', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['HKGAI under InnoHK', 'MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06781.jpg', 'data': {'categories': ['#training', '#open_source', '#rl', '#reasoning', '#optimization', '#math'], 'emoji': '🧮', 'ru': {'title': 'OREAL: Прорыв в обучении с подкреплением для математических рассуждений', 'desc': 'Статья представляет новый фреймворк обучения с подкреплением под названием OREAL для решения математических задач. Авторы теоретически доказывают, что клонирование поведения на положительных траекториях из выборки best-of-N достаточно для обучения оптимальной политики в средах с бинарной обратной связью. Для преодоления проблемы разреженных наград применяется модель вознаграждения на уровне токенов. С помощью OREAL модель размером 7B достигает точности 94.0% pass@1 на датасете MATH-500, что сопоставимо с результатами 32B моделей.'}, 'en': {'title': 'OREAL: Advancing AI Reasoning with Outcome-Based Reinforcement Learning', 'desc': 'This paper introduces a new reinforcement learning framework called OREAL, designed to enhance mathematical reasoning capabilities in AI models. OREAL focuses on using binary outcome rewards to improve learning efficiency, particularly in environments where feedback is sparse. The authors demonstrate that behavior cloning from positive examples can effectively learn optimal policies, while also reshaping negative rewards to maintain gradient consistency. The results show that OREAL achieves high accuracy on mathematical tasks, outperforming larger models and highlighting the significance of initial policy models in the training process.'}, 'zh': {'title': 'OREAL：数学推理的新突破', 'desc': '这篇论文提出了一种新的强化学习框架，称为OREAL，旨在提高数学推理任务的性能。OREAL使用基于结果的奖励机制，专注于二元结果奖励，以解决强化学习中的稀疏奖励问题。研究表明，通过对最佳样本进行行为克隆，可以有效学习最优策略，并且需要对负样本的奖励进行重塑以保持梯度一致性。实验结果显示，使用OREAL的7B模型在MATH-500上达到了94.0的准确率，表现与32B模型相当，且OREAL-32B在同一任务上超越了之前的32B模型。'}}}, {'id': 'https://huggingface.co/papers/2502.06060', 'title': 'Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.06060', 'abstract': "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/", 'score': 20, 'issue_id': 2146, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'd235746154e72f16', 'authors': ['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], 'affiliations': ['Stanford University, Stanford, United States of America'], 'pdf_title_img': 'assets/pdf/title_img/2502.06060.jpg', 'data': {'categories': ['#alignment', '#games', '#rlhf', '#agents', '#open_source', '#rl'], 'emoji': '🕵️', 'ru': {'title': 'Естественный язык как инструмент координации ИИ-агентов', 'desc': 'В статье представлен метод обучения языковых моделей вести продуктивные дискуссии в естественной среде без использования демонстраций от людей. Авторы разделяют проблему коммуникации на навыки слушания и говорения, используя цель агента для предсказания полезной информации об окружающей среде в качестве сигнала награды. Метод применяется к социальной игре на дедукцию, основанной на Among Us, где ключевой вопрос - определение личности противника. Результаты показывают, что техника позволяет вести эффективные обсуждения, удваивая показатели выигрыша по сравнению со стандартным обучением с подкреплением.'}, 'en': {'title': 'Empowering Agents with Natural Language Communication for Enhanced Coordination', 'desc': "This paper explores how language models can be trained to communicate effectively in multi-agent environments without relying on human demonstrations. The authors break down communication into two parts: listening and speaking, using the agents' goals as a reward signal to enhance their communication skills. By applying multi-agent reinforcement learning, they improve how agents generate and interpret messages, leading to more productive discussions. The study demonstrates that these enhanced communication strategies significantly increase success rates in a social deduction game, showcasing the importance of effective communication in complex scenarios."}, 'zh': {'title': '自然语言沟通提升多智能体协作', 'desc': '本研究探讨了在多智能体环境中，如何通过自然语言进行有效沟通。我们提出了一种方法，训练语言模型在没有人类示范的情况下，进行关于环境的讨论。通过将沟通问题分解为倾听和发言，我们利用智能体的目标来预测有用的信息，从而引导沟通。实验表明，这种方法在复杂社交场景中显著提高了智能体的胜率，促进了更强的讨论能力。'}}}, {'id': 'https://huggingface.co/papers/2502.05664', 'title': 'CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging', 'url': 'https://huggingface.co/papers/2502.05664', 'abstract': "Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim's remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (https://kagnlp.github.io/codesim.github.io/).", 'score': 15, 'issue_id': 2152, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '6a6a71a03d5f0d9c', 'authors': ['Md. Ashraful Islam', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], 'affiliations': ['Bangladesh University of Engineering and Technology (BUET)', 'Qatar Computing Research Institute (QCRI)'], 'pdf_title_img': 'assets/pdf/title_img/2502.05664.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agents', '#games', '#open_source', '#training'], 'emoji': '🤖', 'ru': {'title': 'CodeSim: Генерация кода на человеческом уровне', 'desc': 'CodeSim - это новая мультиагентная система для генерации кода, использующая подход, имитирующий человеческое восприятие. Она охватывает все этапы синтеза программ: планирование, кодирование и отладку, применяя уникальный метод верификации плана и внутренней отладки через пошаговую симуляцию ввода/вывода. Эксперименты на семи сложных бенчмарках по решению задач и синтезу программ показали выдающиеся результаты CodeSim в генерации кода. Система достигла новых показателей state-of-the-art (pass@1) на нескольких датасетах, включая HumanEval и MBPP.'}, 'en': {'title': 'CodeSim: Revolutionizing Code Generation with Human-like Perception', 'desc': 'This paper presents CodeSim, a new framework for code generation that improves the process of program synthesis by integrating planning, coding, and debugging stages. Unlike traditional methods that rely on external tools for debugging, CodeSim uses a human-like perception approach, allowing for step-by-step simulation of input and output to verify plans and debug internally. The framework has been tested on various competitive benchmarks, achieving state-of-the-art results in code generation tasks. Additionally, CodeSim shows promise for further improvements when combined with existing external debugging tools.'}, 'zh': {'title': 'CodeSim：类人感知的代码生成新框架', 'desc': '大型语言模型（LLMs）在代码生成和问题解决方面取得了显著进展。当前的方法依赖于外部工具的迭代调试器，这些调试器利用编译器或其他工具的运行时反馈来改进粗略的程序生成。然而，这些方法的有效性在很大程度上依赖于初始代码生成的质量，这仍然是一个开放的挑战。本文介绍了CodeSim，一个新颖的多智能体代码生成框架，通过类人感知的方法全面解决程序合成的各个阶段，包括规划、编码和调试。'}}}, {'id': 'https://huggingface.co/papers/2502.06049', 'title': 'LM2: Large Memory Models', 'url': 'https://huggingface.co/papers/2502.06049', 'abstract': 'This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.', 'score': 13, 'issue_id': 2140, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '5f62d7e814a6918f', 'authors': ['Jikun Kang', 'Wenqi Wu', 'Filippos Christianos', 'Alex J. Chan', 'Fraser Greenlee', 'George Thomas', 'Marvin Purtorab', 'Andy Toulis'], 'affiliations': ['Convergence Labs Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2502.06049.jpg', 'data': {'categories': ['#dataset', '#architecture', '#benchmark', '#interpretability', '#long_context', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'LM2: Трансформер с памятью для улучшенных рассуждений', 'desc': 'В статье представлена модель Large Memory Model (LM2), архитектура декодер-трансформер с дополнительным модулем памяти. LM2 решает проблемы стандартных трансформеров в многошаговых рассуждениях и обработке длинных контекстов. Модель показала значительное улучшение производительности на бенчмарке BABILong по сравнению с базовыми моделями. LM2 также продемонстрировала улучшенные возможности в многоходовых выводах, числовых вычислениях и ответах на вопросы с большим контекстом.'}, 'en': {'title': 'Enhancing Transformers with Memory for Superior Reasoning', 'desc': 'The paper presents the Large Memory Model (LM2), a new type of Transformer designed to improve multi-step reasoning and information synthesis over long contexts. LM2 features an auxiliary memory module that stores contextual information and interacts with input data through cross attention, allowing it to update its memory dynamically. This model retains the original capabilities of Transformers while adding a memory pathway that enhances performance on complex tasks. Experimental results show that LM2 significantly outperforms existing models in reasoning tasks and maintains strong performance on general tasks, highlighting the value of integrating explicit memory into Transformer architectures.'}, 'zh': {'title': '大型记忆模型：提升Transformer推理能力的关键', 'desc': '本文介绍了一种名为大型记忆模型（LM2）的解码器仅Transformer架构，旨在解决标准Transformer在多步推理、关系论证和长上下文信息综合方面的局限性。LM2引入了一个辅助记忆模块，作为上下文表示的存储库，通过交叉注意力与输入标记交互，并通过门控机制进行更新。实验结果表明，LM2在BABILong基准测试中，平均性能比记忆增强的RMT模型提高了37.1%，比基线Llama-3.2模型提高了86.3%。LM2在多跳推理、数值推理和大上下文问答方面表现出色，证明了显式记忆在增强Transformer架构中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.05415', 'title': 'Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.05415', 'abstract': 'There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as a notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify a unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), a qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce a trajectory segmentation strategy and a curriculum learning procedure to improve the training convergence. Empirically, in text-to-image generation, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo exhibits a 1.5x speedup without significantly sacrificing performance. The code is available at https://github.com/zhijie-group/Show-o-Turbo.', 'score': 12, 'issue_id': 2144, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '521adeebda96668f', 'authors': ['Chenkai Xu', 'Xu Wang', 'Zhenyi Liao', 'Yishun Li', 'Tianqi Hou', 'Zhijie Deng'], 'affiliations': ['Huawei', 'Shanghai Jiao Tong University', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05415.jpg', 'data': {'categories': ['#training', '#optimization', '#multimodal', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Show-o Turbo: Ускоренная мультимодальная генерация без потери качества', 'desc': 'Статья представляет Show-o Turbo - улучшенную версию мультимодальной модели Show-o для генерации изображений по тексту и текста по изображениям. Авторы предлагают унифицированный подход к шумоподавлению для обоих типов генерации, основанный на параллельном декодировании текстовых токенов. Они применяют метод consistency distillation для ускорения процесса шумоподавления, а также вводят стратегию сегментации траектории и процедуру курикулярного обучения. Эксперименты показывают, что Show-o Turbo превосходит оригинальную модель по скорости и качеству генерации.'}, 'en': {'title': 'Show-o Turbo: Accelerating Multimodal Generation with Unified Denoising', 'desc': 'This paper presents Show-o Turbo, an advanced model for multimodal understanding and generation that improves upon the original Show-o framework. It addresses inefficiencies in the generation process by introducing a unified denoising approach that allows for parallel decoding of text tokens. The authors enhance the training process using consistency distillation and a new trajectory segmentation strategy, which leads to faster convergence. Empirical results show that Show-o Turbo achieves better performance in both text-to-image and image-to-text tasks, significantly reducing the number of sampling steps required for generation.'}, 'zh': {'title': '提升多模态生成效率的Show-o Turbo', 'desc': '本论文介绍了一种新的多模态生成模型Show-o Turbo，旨在提高文本到图像和图像到文本生成的效率。通过并行解码文本标记，Show-o Turbo采用统一的去噪视角，缩短了去噪过程。我们还引入了一种轨迹分割策略和课程学习程序，以改善训练收敛性。实验结果表明，Show-o Turbo在生成图像时的效率显著提高，同时在生成文本时也实现了1.5倍的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2502.05609', 'title': 'Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding', 'url': 'https://huggingface.co/papers/2502.05609', 'abstract': 'Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.', 'score': 12, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '6f559083c224138c', 'authors': ['Sukmin Cho', 'Sangjin Choi', 'Taeho Hwang', 'Jeongyeon Seo', 'Soyeong Jeong', 'Huije Lee', 'Hoyun Song', 'Jong C. Park', 'Youngjin Kwon'], 'affiliations': ['School of Computing, Graduate School of AI, Korea Advanced Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.05609.jpg', 'data': {'categories': ['#training', '#architecture', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Иерархическая черновая генерация: новый подход к ускорению вывода в LLM', 'desc': 'Статья представляет новый метод ускорения вывода в больших языковых моделях (LLM) под названием Hierarchy Drafting (HD). HD организует различные источники токенов в иерархические базы данных, основываясь на временной локальности. Метод последовательно обращается к базам данных для получения черновых токенов, обеспечивая стабильное ускорение на различных задачах. Эксперименты показали, что HD превосходит существующие методы черновой генерации, демонстрируя надежное ускорение вывода для моделей разного размера, задач и температур.'}, 'en': {'title': 'Boosting Inference Speed with Hierarchy Drafting in LLMs', 'desc': 'This paper focuses on improving the speed of inference in Large Language Models (LLMs) for real-time applications. It introduces a new method called Hierarchy Drafting (HD), which organizes token sources into a hierarchical structure to enhance the drafting process. By accessing these token databases based on their temporal locality, HD ensures faster and more consistent token generation across various tasks. Experimental results show that HD significantly outperforms existing methods, providing robust speed improvements for LLMs of different sizes and tasks.'}, 'zh': {'title': '层次草拟：加速大型语言模型推理的新方法', 'desc': '加速大型语言模型（LLMs）的推理对于实时交互至关重要。本文提出了一种新的无损草拟方法，称为层次草拟（HD），它通过基于时间局部性的层次框架组织多种令牌源。HD在草拟步骤中依次访问多个数据库，从最高到最低的局部性获取草拟令牌，从而确保在不同任务中一致的加速效果。实验结果表明，HD在推理速度上优于现有的数据库草拟方法，适用于不同规模的模型和任务。'}}}, {'id': 'https://huggingface.co/papers/2502.06772', 'title': 'ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates', 'url': 'https://huggingface.co/papers/2502.06772', 'abstract': 'We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux', 'score': 11, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '1ac59597c9610fb2', 'authors': ['Ling Yang', 'Zhaochen Yu', 'Bin Cui', 'Mengdi Wang'], 'affiliations': ['Peking University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06772.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#reasoning', '#math', '#benchmark', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Революция в математическом мышлении ИИ: иерархические рассуждения на новом уровне', 'desc': 'Представлена модель ReasonFlux-32B, использующая иерархическое рассуждение с масштабированием шаблонов мышления для оптимизации пространства поиска решений. Модель превосходит математические способности мощных языковых моделей, таких как OpenAI o1-preview и DeepSeek V3. ReasonFlux-32B использует структурированную библиотеку шаблонов мышления и иерархическое обучение с подкреплением для планирования оптимальной траектории шаблонов. На бенчмарке MATH модель достигает точности 91.2%, превосходя o1-preview на 6.7%.'}, 'en': {'title': 'Revolutionizing Math Reasoning with Hierarchical Thought Templates', 'desc': 'This paper introduces ReasonFlux-32B, a model that enhances mathematical reasoning in large language models (LLMs) by using hierarchical reasoning with thought templates. It features a library of 500 structured thought templates that help generalize reasoning across similar problems. The model employs hierarchical reinforcement learning to optimize the sequence of thought templates, allowing it to tackle complex problems more effectively. With these innovations, ReasonFlux-32B achieves state-of-the-art performance on math benchmarks, significantly outperforming existing models like OpenAI o1-preview and DeepSeek V3.'}, 'zh': {'title': '层次化推理，数学能力新突破', 'desc': '本文提出通过扩展思维模板的层次化大语言模型（LLM）推理，可以有效优化推理搜索空间，并超越强大的LLM如OpenAI o1-preview和DeepSeek V3的数学推理能力。我们训练的ReasonFlux-32B模型仅使用8个GPU，并引入了三项创新：一是构建了一个包含约500个高层次思维模板的结构化通用模板库，能够推广到类似的推理问题；二是对思维模板序列进行层次化强化学习，而不是长链的思维（CoTs），优化基础LLM以规划出处理复杂问题的最佳模板轨迹；三是全新的推理扩展系统，通过在推理时自适应扩展思维模板，实现层次化LLM推理。通过包含顺序思维模板的模板轨迹，ReasonFlux-32B在数学推理能力上显著提升，达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2502.06786', 'title': 'Matryoshka Quantization', 'url': 'https://huggingface.co/papers/2502.06786', 'abstract': 'Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model.', 'score': 9, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '1126a5fe83c7422d', 'authors': ['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.06786.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': '🪆', 'ru': {'title': 'MatQuant: Одна модель - множество уровней точности', 'desc': 'MatQuant - это новый метод многомасштабной квантизации для моделей машинного обучения. Он позволяет обучать и обслуживать одну модель, которую затем можно использовать с разными уровнями точности. Благодаря совместному обучению и ко-дистилляции, модели int2, полученные с помощью MatQuant, могут быть до 10% точнее, чем при стандартной квантизации int2. Этот метод значительно улучшает квантизацию моделей, что демонстрируется тем, что модель Gemma-2 9B с квантизацией FFN до int2 оказывается точнее, чем модель Gemma-2 2B с квантизацией FFN до int8.'}, 'en': {'title': 'One Model, Multiple Precision: Revolutionizing Quantization with MatQuant', 'desc': 'This paper introduces Matryoshka Quantization (MatQuant), a new technique for quantizing model weights that allows for multiple precision levels without sacrificing model quality. Traditional low-precision quantization, especially to int2, often leads to significant degradation in performance, forcing practitioners to manage several models. MatQuant leverages the nested structure of integer data types to enable a single model to be trained and served at various precision levels. The method also enhances the accuracy of int2 models by up to 10% compared to standard quantization techniques, showcasing its effectiveness in reducing the need for multiple quantized models.'}, 'zh': {'title': 'Matryoshka量化：单模型多精度服务的创新', 'desc': '量化模型权重对于减少大型模型的通信和推理成本至关重要。然而，将模型量化到低精度（如int4或int2）时，模型质量会受到影响，尤其是int2会显著降低模型性能。为了解决这个问题，本文提出了一种新的多尺度量化技术——Matryoshka量化（MatQuant），它允许只训练和维护一个模型，并在不同精度级别下进行服务。通过MatQuant的共同训练和共同蒸馏正则化，提取的int2精度模型的准确性比标准的int2量化高出10%。'}}}, {'id': 'https://huggingface.co/papers/2502.06788', 'title': 'EVEv2: Improved Baselines for Encoder-Free Vision-Language Models', 'url': 'https://huggingface.co/papers/2502.06788', 'abstract': 'Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.', 'score': 9, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '4374edb93ca102c6', 'authors': ['Haiwen Diao', 'Xiaotong Li', 'Yufeng Cui', 'Yueze Wang', 'Haoge Deng', 'Ting Pan', 'Wenxuan Wang', 'Huchuan Lu', 'Xinlong Wang'], 'affiliations': ['BAAI', 'BUPT', 'CASIA', 'DLUT', 'PKU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2502.06788.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#agi', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Революция в мультимодальном обучении: EVEv2.0 - эффективность без энкодеров', 'desc': 'Статья представляет новое семейство моделей машинного обучения EVEv2.0, работающих с изображениями и текстом без использования энкодеров. Авторы систематически исследуют разрыв в производительности между моделями с энкодерами и без них, разрабатывая эффективные стратегии для последних. Они демонстрируют, что правильное разложение и иерархическая ассоциация зрения и языка в единой модели снижает интерференцию между модальностями. EVEv2.0 показывает превосходную эффективность использования данных и сильные способности к визуальному рассуждению.'}, 'en': {'title': 'EVEv2.0: Bridging the Gap in Vision-Language Models Without Encoders', 'desc': 'This paper discusses the advancements in encoder-free vision-language models (VLMs) that are closing the performance gap with traditional encoder-based models. The authors explore the characteristics of these encoder-free VLMs and propose efficient strategies to enhance their performance. They introduce EVEv2.0, a new family of encoder-free VLMs that effectively integrates vision and language while minimizing interference. The study demonstrates that a well-structured training approach and hierarchical association of modalities lead to improved data efficiency and vision-reasoning capabilities.'}, 'zh': {'title': '无编码器VLM的潜力与创新', 'desc': '本论文探讨了无编码器的视觉-语言模型（VLMs）在性能上与基于编码器的模型之间的差距。我们系统性地分析了使用预训练视觉编码器和简约视觉层的无编码器VLMs的特性。通过开发高效的策略，我们推出了EVEv2.0，一个改进的无编码器VLM系列，展示了其在数据效率和视觉推理能力上的优势。我们的研究表明，合理分解和层次关联视觉与语言可以减少模态之间的干扰，并通过良好的训练策略实现有效优化。'}}}, {'id': 'https://huggingface.co/papers/2502.03628', 'title': 'The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering', 'url': 'https://huggingface.co/papers/2502.03628', 'abstract': 'Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.', 'score': 9, 'issue_id': 2140, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '04b182d80abf9219', 'authors': ['Zhuowei Li', 'Haizhou Shi', 'Yunhe Gao', 'Di Liu', 'Zhenting Wang', 'Yuxiao Chen', 'Ting Liu', 'Long Zhao', 'Hao Wang', 'Dimitris N. Metaxas'], 'affiliations': ['Google DeepMind', 'Rutgers University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.03628.jpg', 'data': {'categories': ['#cv', '#training', '#hallucinations', '#benchmark', '#inference', '#interpretability', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Борьба с галлюцинациями в визуально-языковых моделях: метод VISTA', 'desc': 'Эта статья исследует проблему галлюцинаций в крупных визуально-языковых моделях (LVLM) при обработке текстовых и визуальных входных данных. Авторы анализируют внутренние механизмы возникновения галлюцинаций, изучая ранжирование логитов токенов в процессе генерации. На основе выявленных закономерностей предлагается метод VISTA для уменьшения галлюцинаций и усиления достоверной информации во время вывода. Эксперименты показывают, что VISTA в среднем снижает уровень галлюцинаций на 40% в задачах открытой генерации и превосходит существующие методы на четырех бенчмарках.'}, 'en': {'title': 'VISTA: Reducing Hallucination in Vision-Language Models', 'desc': 'This paper explores the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate plausible text that does not correspond to visual inputs. The authors identify three patterns in the generation process: a gradual loss of visual information, early activation of semantically meaningful tokens, and the presence of high-ranking visually grounded tokens that are not ultimately chosen. To address these issues, they introduce VISTA, a framework that enhances visual information during inference without requiring additional training. VISTA effectively reduces hallucination by about 40% and outperforms existing methods across multiple benchmarks and architectures.'}, 'zh': {'title': '减少幻觉，提升真实信息的VISTA框架', 'desc': '大型视觉语言模型（LVLMs）能够有效地处理文本和视觉输入，但它们往往会产生语法上连贯但视觉上不真实的内容。本文研究了幻觉的内部动态，发现LVLMs在生成过程中处理信息的三种关键模式：逐渐丧失视觉信息、早期激活和隐藏的真实信息。基于这些发现，我们提出了VISTA（视觉信息引导与标记逻辑增强），这是一种无需训练的推理时干预框架，旨在减少幻觉并促进真实信息的生成。实验表明，VISTA在开放式生成任务中平均减少了约40%的幻觉，并在四个基准测试中在三种解码策略下始终优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.06782', 'title': 'Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT', 'url': 'https://huggingface.co/papers/2502.06782', 'abstract': "Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.", 'score': 8, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '3b903654a6ff6710', 'authors': ['Dongyang Liu', 'Shicheng Li', 'Yutong Liu', 'Zhen Li', 'Kai Wang', 'Xinyue Li', 'Qi Qin', 'Yufei Liu', 'Yi Xin', 'Zhongyu Li', 'Bin Fu', 'Chenyang Si', 'Yuewen Cao', 'Conghui He', 'Ziwei Liu', 'Yu Qiao', 'Qibin Hou', 'Hongsheng Li', 'Peng Gao'], 'affiliations': ['Nankai University', 'Shanghai Correspondence AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.06782.jpg', 'data': {'categories': ['#video', '#architecture', '#synthetic', '#diffusion', '#audio', '#training'], 'emoji': '🎬', 'ru': {'title': 'Lumina-Video: новый уровень генерации видео с помощью диффузионных трансформеров', 'desc': 'Статья представляет Lumina-Video - новую архитектуру для генерации видео, основанную на Diffusion Transformers (DiT). Авторы предлагают мультимасштабную архитектуру Next-DiT, которая обучается на нескольких уровнях детализации одновременно. Модель использует условие движения для контроля динамики генерируемого видео. Благодаря прогрессивной схеме обучения и использованию смешанных данных, Lumina-Video достигает высокого качества и плавности движения при эффективном обучении и инференсе.'}, 'en': {'title': 'Revolutionizing Video Generation with Lumina-Video', 'desc': 'This paper introduces Lumina-Video, a new framework for generating videos using the strengths of Diffusion Transformers, specifically the Next-DiT model. It addresses the challenges of capturing the complex movements and changes in video data by using a Multi-scale Next-DiT architecture that learns from different segments of the video. The framework also incorporates a motion score to allow for better control over the dynamics of the generated videos. Additionally, Lumina-Video employs a progressive training approach to improve the quality and smoothness of the output while maintaining efficiency in both training and inference.'}, 'zh': {'title': 'Lumina-Video：高效生成视频的新框架', 'desc': '最近的研究表明，扩散变换器（DiTs）在生成建模中表现出色。基于这一成功，Lumina-Next在生成逼真图像方面取得了卓越的性能，但在视频生成方面仍面临挑战。为了解决这一问题，我们提出了Lumina-Video框架，它结合了Next-DiT的优势，并针对视频合成引入了定制化的解决方案。通过多尺度Next-DiT架构和运动评分的引入，Lumina-Video实现了高效、灵活的视频生成，并在训练和推理效率上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.06764', 'title': 'History-Guided Video Diffusion', 'url': 'https://huggingface.co/papers/2502.06764', 'abstract': 'Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance', 'score': 8, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '66644a3e757a5d21', 'authors': ['Kiwhan Song', 'Boyuan Chen', 'Max Simchowitz', 'Yilun Du', 'Russ Tedrake', 'Vincent Sitzmann'], 'affiliations': ['MIT'], 'pdf_title_img': 'assets/pdf/title_img/2502.06764.jpg', 'data': {'categories': ['#diffusion', '#training', '#video', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Улучшение генерации видео с помощью гибкого обусловливания историей', 'desc': 'Статья представляет новый подход к улучшению генерации видео с помощью диффузионных моделей. Авторы предлагают архитектуру Diffusion Forcing Transformer (DFoT), которая позволяет использовать переменное количество кадров истории для обусловливания генерации. Они также вводят концепцию History Guidance - семейство методов, которые улучшают качество и временную согласованность генерируемого видео. Эксперименты показывают, что предложенный подход значительно улучшает динамику движения и позволяет генерировать очень длинные видео.'}, 'en': {'title': 'Enhancing Video Diffusion with Flexible History Guidance', 'desc': 'This paper introduces the Diffusion Forcing Transformer (DFoT), a novel architecture designed for video diffusion that allows for flexible conditioning on a variable number of context frames. The authors address challenges related to fixed-size conditioning architectures and the inefficacy of traditional classifier-free guidance (CFG) methods when applied to variable-length history. They propose History Guidance, a set of techniques that leverage DFoT to improve video generation quality and temporal consistency. The results demonstrate that these methods enhance motion dynamics and enable the generation of longer videos with better compositional generalization.'}, 'zh': {'title': '灵活历史引导，提升视频生成质量', 'desc': '本论文提出了一种新的视频扩散模型架构，称为Diffusion Forcing Transformer（DFoT），旨在解决在可变长度历史帧条件下进行视频生成的挑战。我们发现，传统的分类器无关引导（CFG）方法在处理可变长度历史时效果不佳，因此我们设计了新的引导方法，称为历史引导。DFoT允许灵活地使用历史帧进行条件生成，从而显著提高视频生成的质量和时间一致性。通过引入更高级的历史引导方法，我们进一步增强了运动动态，并实现了对超出分布历史的组合泛化。'}}}, {'id': 'https://huggingface.co/papers/2502.05957', 'title': 'MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents', 'url': 'https://huggingface.co/papers/2502.05957', 'abstract': "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce MetaChain-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, MetaChain comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, MetaChain also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate MetaChain's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.", 'score': 7, 'issue_id': 2144, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'f3a18de353dcfad8', 'authors': ['Jiabin Tang', 'Tianyu Fan', 'Chao Huang'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.05957.jpg', 'data': {'categories': ['#rag', '#games', '#agents', '#benchmark', '#optimization', '#agi'], 'emoji': '🤖', 'ru': {'title': 'MetaChain: ИИ-агенты для всех без кода', 'desc': 'MetaChain - это полностью автоматизированная и самоуправляемая система для создания агентов на основе больших языковых моделей (LLM) без необходимости программирования. Она состоит из четырех ключевых компонентов: агентных системных утилит, движка действий на основе LLM, самоуправляемой файловой системы и модуля самонастройки агентов. MetaChain позволяет эффективно создавать и модифицировать инструменты, агенты и рабочие процессы без кодирования. Система показала превосходные результаты в задачах многоагентного взаимодействия и генерации с извлечением информации (RAG).'}, 'en': {'title': 'Empowering Everyone to Build LLM Agents with MetaChain', 'desc': 'This paper introduces MetaChain, a framework designed to allow users to create and deploy Large Language Model (LLM) agents using only natural language, eliminating the need for programming skills. MetaChain operates as an autonomous Agent Operating System, featuring components like an Actionable Engine and a Self-Managing File System to facilitate dynamic agent development. The framework addresses the accessibility gap in LLM agent creation, enabling a broader audience to leverage AI technology. Evaluations on the GAIA benchmark indicate that MetaChain outperforms existing methods in multi-agent tasks and demonstrates superior capabilities in Retrieval-Augmented Generation (RAG).'}, 'zh': {'title': '让每个人都能用自然语言构建智能代理', 'desc': '大型语言模型（LLM）代理在任务自动化和智能决策方面表现出色，但现有的开发框架主要面向技术背景深厚的开发者，限制了普通用户的使用。为了解决这一问题，我们提出了MetaChain，一个完全自动化且高度自我发展的框架，允许用户仅通过自然语言创建和部署LLM代理。MetaChain作为一个自主代理操作系统，包含四个关键组件，能够高效动态地创建和修改工具、代理和工作流程，而无需编写代码。经过GAIA基准的全面评估，MetaChain在通用多代理任务中表现优于现有的最先进方法，展现了其强大的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.06023', 'title': 'Dual Caption Preference Optimization for Diffusion Models', 'url': 'https://huggingface.co/papers/2502.06023', 'abstract': "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.", 'score': 6, 'issue_id': 2141, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '07782353b3b8b697', 'authors': ['Amir Saeidi', 'Yiran Luo', 'Agneet Chatterjee', 'Shamanthak Hegde', 'Bimsara Pathiraja', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06023.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#dataset', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Двойные подписи для улучшения генерации изображений по текстовым описаниям', 'desc': 'Эта статья описывает новый метод под названием Dual Caption Preference Optimization (DCPO) для улучшения моделей диффузии текст-в-изображение. DCPO использует два отдельных описания для решения проблемы нерелевантных промптов и конфликтующих распределений в наборах данных предпочтений. Авторы также представляют новый датасет Pick-Double Caption с отдельными подписями для предпочтительных и менее предпочтительных изображений. Эксперименты показывают, что DCPO значительно улучшает качество изображений и их соответствие промптам по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing Image Generation with Dual Captions!', 'desc': 'This paper presents Dual Caption Preference Optimization (DCPO), a new method to enhance text-to-image diffusion models by addressing issues in human preference optimization. It identifies problems with existing preference datasets, such as overlapping distributions and irrelevant prompts that hinder the denoising process. To overcome these challenges, DCPO employs two distinct captions for preferred and less preferred images, utilizing a modified dataset called Pick-Double Caption. The results demonstrate that DCPO significantly improves image quality and relevance, outperforming several existing models across various evaluation metrics.'}, 'zh': {'title': '双重标题优化，提升图像质量！', 'desc': '最近在大型语言模型（LLMs）中发展的人类偏好优化技术，显示出在改进文本到图像扩散模型方面的巨大潜力。这些方法旨在学习偏好样本的分布，并将其与不太偏好的样本区分开来。然而，现有的偏好数据集通常存在分布重叠的问题，导致冲突分布。此外，我们发现输入提示中包含与不太偏好的图像无关的信息，这限制了去噪网络在偏好优化方法中的准确预测能力。为了解决这些挑战，我们提出了双重标题偏好优化（DCPO），利用两个不同的标题来减轻无关提示的问题。'}}}, {'id': 'https://huggingface.co/papers/2502.06155', 'title': 'Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile', 'url': 'https://huggingface.co/papers/2502.06155', 'abstract': 'Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.', 'score': 6, 'issue_id': 2140, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2f8d5e54db328d39', 'authors': ['Hangliang Ding', 'Dacheng Li', 'Runlong Su', 'Peiyuan Zhang', 'Zhijie Deng', 'Ion Stoica', 'Hao Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.06155.jpg', 'data': {'categories': ['#training', '#diffusion', '#optimization', '#inference', '#video'], 'emoji': '🎬', 'ru': {'title': 'Ускорение генерации видео: эффективность без потери качества', 'desc': 'Статья представляет новый подход к ускорению генерации видео с помощью Диффузионных Трансформеров (DiTs). Авторы предлагают прореживание 3D-внимания на основе избыточности видеоданных и сокращение процесса сэмплирования с помощью многошаговой дистилляции согласованности. Разработан трехэтапный процесс обучения для объединения внимания с низкой сложностью и возможностей генерации за несколько шагов. Результаты показывают ускорение в 7.4-7.8 раз для генерации видео 720p с 29 и 93 кадрами при использовании всего 0.1% данных предобучения.'}, 'en': {'title': 'Speeding Up Video Generation with Efficient Attention Mechanisms', 'desc': 'This paper presents a solution to the inefficiency of Diffusion Transformers (DiTs) in generating high-fidelity videos. It introduces a method to prune 3D full attention by recognizing repetitive patterns in video data, leading to a sparse attention mechanism that reduces computational complexity. Additionally, the authors propose a multi-step consistency distillation approach to shorten the sampling process, allowing for faster video generation. The resulting model, Open-Sora-Plan-1.2, achieves significant speed improvements while maintaining performance, especially when utilizing distributed inference across multiple GPUs.'}, 'zh': {'title': '高效视频生成的新方法', 'desc': '本论文提出了一种改进的Diffusion Transformers（DiTs）模型，以解决生成高保真视频时的效率问题。我们通过识别视频数据中的冗余，提出了一种稀疏的3D注意力机制，使其在视频帧数量上具有线性复杂度。其次，我们采用多步一致性蒸馏技术，缩短了采样过程，从而实现了更快速的视频生成。最终，我们的模型在使用极少的预训练数据时，生成速度提高了7.4到7.8倍，同时保持了良好的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.05795', 'title': 'The Curse of Depth in Large Language Models', 'url': 'https://huggingface.co/papers/2502.05795', 'abstract': 'In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models(LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.', 'score': 5, 'issue_id': 2147, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '3b1a3626926ac2f4', 'authors': ['Wenfang Sun', 'Xinyuan Song', 'Pengxiang Li', 'Lu Yin', 'Yefeng Zheng', 'Shiwei Liu'], 'affiliations': ['Dalian University of Technology, China', 'Emory University, USA', 'Medical Artificial Intelligence Laboratory, Westlake University, China', 'University of Surrey, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.05795.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': "Преодоление 'Проклятия глубины' в больших языковых моделях", 'desc': "В статье представлена концепция 'Проклятия глубины', объясняющая низкую эффективность почти половины слоев в современных больших языковых моделях (LLM). Авторы подтверждают широкое распространение этого явления среди популярных семейств LLM, таких как Llama, Mistral, DeepSeek и Qwen. Анализ показывает, что причиной неэффективности глубоких слоев является использование предварительной нормализации слоев (Pre-LN). Для решения этой проблемы предлагается метод масштабирования LayerNorm, который улучшает вклад глубоких слоев в обучение модели."}, 'en': {'title': 'Unlocking the Power of Deep Layers in LLMs', 'desc': "This paper introduces the 'Curse of Depth', which describes a problem in Large Language Models (LLMs) where many layers do not perform as well as expected. The authors find that this issue is common in popular LLMs like Llama and Mistral, and it stems from the use of Pre-Layer Normalization (Pre-LN). Pre-LN helps stabilize training but leads to increased output variance in deeper layers, making them less effective. To address this, the authors propose LayerNorm Scaling, which reduces the output variance of deeper layers, resulting in improved training performance and better contributions from these layers."}, 'zh': {'title': '解决深度模型的训练困境', 'desc': '本文介绍了深度诅咒的概念，强调了现代大型语言模型（LLMs）中近一半层的效果低于预期的现象。我们确认了这一现象在流行的LLM家族中普遍存在，如Llama、Mistral、DeepSeek和Qwen。分析表明，深层无效的根本原因是广泛使用的预层归一化（Pre-LN），它导致输出方差随着模型深度的增加而指数增长。为了解决这个问题，我们提出了层归一化缩放（LayerNorm Scaling），通过对层归一化的输出方差进行缩放，显著提高了深层的贡献。'}}}, {'id': 'https://huggingface.co/papers/2502.06527', 'title': 'CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2502.06527', 'abstract': 'Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.', 'score': 5, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '44b3a6931980556a', 'authors': ['D. She', 'Mushui Liu', 'Jingxuan Pang', 'Jin Wang', 'Zhen Yang', 'Wanggui He', 'Guanghao Zhang', 'Yi Wang', 'Qihan Huang', 'Haobin Tang', 'Yunlong Yu', 'Siming Fu'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'University of Science and Technology of China', 'Zhejiang Univerisity'], 'pdf_title_img': 'assets/pdf/title_img/2502.06527.jpg', 'data': {'categories': ['#diffusion', '#video', '#benchmark', '#3d'], 'emoji': '🎬', 'ru': {'title': 'CustomVideoX: Персонализированная генерация видео нового уровня', 'desc': 'CustomVideoX - это инновационная система для персонализированной генерации видео на основе референсного изображения, использующая видео-диффузионный трансформер. Система применяет предобученные видеосети и обучает только параметры LoRA для извлечения признаков из референса, что обеспечивает эффективность и адаптивность. Предложенное 3D Reference Attention позволяет взаимодействовать признакам референсного изображения со всеми кадрами видео в пространственном и временном измерениях. Для улучшения качества генерации используются стратегии Time-Aware Reference Attention Bias и Entity Region-Aware Enhancement.'}, 'en': {'title': 'Revolutionizing Personalized Video Generation with CustomVideoX', 'desc': 'This paper presents CustomVideoX, a new framework designed for generating personalized videos from a reference image. It utilizes a video diffusion transformer and focuses on training LoRA parameters to efficiently extract features from the reference image. The framework introduces 3D Reference Attention to enhance the interaction between the reference image and video frames, addressing temporal inconsistencies. Additionally, it employs the Time-Aware Reference Attention Bias and the Entity Region-Aware Enhancement modules to improve video quality and consistency, validated by a new benchmark called VideoBench.'}, 'zh': {'title': '个性化视频生成的新突破', 'desc': '个性化视频生成在图像合成领域取得了显著进展，但由于时间不一致性和质量下降，仍然面临挑战。本文提出了CustomVideoX，一个创新框架，利用视频扩散变换器从参考图像生成个性化视频。CustomVideoX通过专门训练LoRA参数来提取参考特征，确保了效率和适应性。我们还提出了3D参考注意力机制，以便在空间和时间维度上直接和同时地将参考图像特征与所有视频帧进行交互。'}}}, {'id': 'https://huggingface.co/papers/2502.05431', 'title': 'APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding', 'url': 'https://huggingface.co/papers/2502.05431', 'abstract': "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (APE), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5times speedup by reducing 28times prefilling time for a 128K-length context.", 'score': 5, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '7bc5b7aeb3716893', 'authors': ['Xinyu Yang', 'Tianqi Chen', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2502.05431.jpg', 'data': {'categories': ['#rag', '#optimization', '#inference', '#long_context'], 'emoji': '⚡', 'ru': {'title': 'Ускорение генерации с контекстом: адаптивное параллельное кодирование', 'desc': 'Статья представляет новый метод адаптивного параллельного кодирования (APE) для эффективной обработки множественных контекстов в задачах генерации с использованием контекста. APE позволяет предварительно вычислять и кэшировать KV-состояния каждого контекста независимо, что значительно ускоряет процесс обработки запросов. Метод решает проблему несоответствия распределения внимания при параллельном кодировании, используя общий префикс, температуру внимания и масштабирующий фактор. Эксперименты показывают, что APE сохраняет до 98% производительности последовательного кодирования, превосходя обычное параллельное кодирование на 3.6-7.9% в задачах RAG и ICL.'}, 'en': {'title': 'Boosting Efficiency in Context-Augmented Generation with APE', 'desc': 'This paper introduces Adaptive Parallel Encoding (APE) as a solution to improve the efficiency of context-augmented generation (CAG) techniques like RAG and ICL. Traditional methods face high computational costs when combining multiple contexts for generating responses, as they require re-encoding for each request. APE allows for the pre-computation and caching of key-value (KV) states for each context, which can then be loaded during inference, significantly speeding up the process. The proposed method aligns the attention distribution of parallel encoding with that of sequential encoding, achieving high performance while handling many contexts efficiently.'}, 'zh': {'title': '自适应并行编码：提升上下文生成效率的关键', 'desc': '本文探讨了上下文增强生成（CAG）技术中的并行编码方法，以提高生成用户查询响应的效率。传统方法在每次请求时都需要重新编码多个上下文，导致计算负担过重。我们提出了自适应并行编码（APE），通过共享前缀、注意力温度和缩放因子来调整并行编码与顺序编码的注意力分布，从而提高性能。实验结果表明，APE在保持高性能的同时，能够显著加快处理速度，适用于处理大量上下文。'}}}, {'id': 'https://huggingface.co/papers/2502.04370', 'title': 'DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2502.04370', 'abstract': 'Text-to-3D generation automates 3D content creation from textual descriptions, which offers transformative potential across various fields. However, existing methods often struggle to align generated content with human preferences, limiting their applicability and flexibility. To address these limitations, in this paper, we propose DreamDPO, an optimization-based framework that integrates human preferences into the 3D generation process, through direct preference optimization. Practically, DreamDPO first constructs pairwise examples, then compare their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D representation with a preference-driven loss function. By leveraging pairwise comparison to reflect preferences, DreamDPO reduces reliance on precise pointwise quality evaluations while enabling fine-grained controllability through preference-guided optimization. Experiments demonstrate that DreamDPO achieves competitive results, and provides higher-quality and more controllable 3D content compared to existing methods. The code and models will be open-sourced.', 'score': 4, 'issue_id': 2145, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'a475f5281f318a1e', 'authors': ['Zhenglin Zhou', 'Xiaobo Xia', 'Fan Ma', 'Hehe Fan', 'Yi Yang', 'Tat-Seng Chua'], 'affiliations': ['National University of Singapore', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.04370.jpg', 'data': {'categories': ['#training', '#optimization', '#3d', '#alignment', '#open_source', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Создание 3D-контента с учетом человеческих предпочтений', 'desc': 'DreamDPO - это новый подход к генерации 3D-контента на основе текстовых описаний, который учитывает предпочтения человека. Метод использует попарное сравнение сгенерированных образцов для оценки их соответствия предпочтениям с помощью моделей вознаграждения или мультимодальных языковых моделей. DreamDPO оптимизирует 3D-представление с использованием функции потерь, ориентированной на предпочтения. Эксперименты показывают, что DreamDPO обеспечивает более качественный и контролируемый 3D-контент по сравнению с существующими методами.'}, 'en': {'title': 'DreamDPO: Aligning 3D Generation with Human Preferences', 'desc': 'This paper introduces DreamDPO, a new framework for generating 3D content from text that incorporates human preferences. It uses an optimization approach that focuses on pairwise comparisons to better align the generated 3D models with what people actually want. By employing a preference-driven loss function, DreamDPO enhances the quality and control of the generated content without needing exact quality scores. The results show that DreamDPO outperforms existing methods, making it a significant advancement in text-to-3D generation.'}, 'zh': {'title': 'DreamDPO：将人类偏好融入3D生成的创新框架', 'desc': '文本到3D生成技术可以根据文本描述自动创建3D内容，具有广泛的应用潜力。然而，现有方法在生成内容与人类偏好之间的对齐上存在困难，限制了其适用性和灵活性。为了解决这些问题，本文提出了DreamDPO，一个基于优化的框架，通过直接偏好优化将人类偏好融入3D生成过程中。实验表明，DreamDPO在生成高质量和可控的3D内容方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.06635', 'title': 'Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM', 'url': 'https://huggingface.co/papers/2502.06635', 'abstract': "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.", 'score': 4, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2deb5075264d7660', 'authors': ['Qingshui Gu', 'Shu Li', 'Tianyu Zheng', 'Zhaoxiang Zhang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Institute of Automation, Chinese Academy of Sciences', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06635.jpg', 'data': {'categories': ['#small_models', '#multilingual', '#training', '#data', '#benchmark', '#open_source', '#dataset', '#low_resource'], 'emoji': '🇨🇳', 'ru': {'title': 'Создание эффективной китайскоязычной LLM с открытым исходным кодом', 'desc': 'Steel-LLM - это языковая модель, ориентированная на китайский язык, разработанная с нуля при ограниченных вычислительных ресурсах. Модель с 1 миллиардом параметров была обучена на крупномасштабном наборе данных, в основном на китайском языке. Steel-LLM показала конкурентоспособную производительность на бенчмарках CEVAL и CMMLU, превзойдя ранние модели от более крупных институтов. Статья предоставляет подробный отчет о процессе разработки, включая сбор данных, дизайн модели и методологии обучения.'}, 'en': {'title': 'Empowering Chinese NLP with Steel-LLM: Open-Source Innovation', 'desc': "Steel-LLM is a language model specifically designed for the Chinese language, built from the ground up to be open-source and accessible. It features 1 billion parameters and was trained on a large dataset primarily consisting of Chinese text, with some English data to fill existing gaps. The model has shown strong performance on various benchmarks, surpassing earlier models from larger organizations. This paper details the project's contributions, including data collection, model architecture, training techniques, and the challenges faced, serving as a guide for others in the field of language model development."}, 'zh': {'title': '打造中文优质开源语言模型的探索', 'desc': 'Steel-LLM是一个以中文为中心的语言模型，旨在在有限的计算资源下开发出高质量的开源模型。该项目于2024年3月启动，训练了一个拥有10亿参数的大规模模型，重点关注透明度和实用见解的分享。训练过程中主要使用中文数据，并适量包含英文数据，填补了现有开源大语言模型的空白。Steel-LLM在CEVAL和CMMLU等基准测试中表现出色，超越了大型机构的早期模型，为研究人员和实践者提供了宝贵的资源。'}}}, {'id': 'https://huggingface.co/papers/2502.06776', 'title': 'Towards Internet-Scale Training For Agents', 'url': 'https://huggingface.co/papers/2502.06776', 'abstract': 'The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web. Code will be available at: data-for-agents.github.io.', 'score': 1, 'issue_id': 2157, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '5ca8acf0bf4a0a58', 'authors': ['Brandon Trabucco', 'Gunnar Sigurdsson', 'Robinson Piramuthu', 'Ruslan Salakhutdinov'], 'affiliations': ['Amazon', 'Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06776.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#data', '#open_source', '#training'], 'emoji': '🌐', 'ru': {'title': 'Автоматизация обучения веб-агентов: от человеческих аннотаций к языковым моделям', 'desc': 'Статья представляет новый подход к обучению агентов веб-навигации без использования человеческих аннотаций. Авторы разработали конвейер, в котором языковая модель (ЛЯМ) генерирует задачи для 150 тысяч сайтов, агенты на основе ЛЯМ выполняют эти задачи, а затем другая ЛЯМ оценивает успешность выполнения. Результаты показывают, что языковые модели конкурентоспособны с человеческими аннотаторами в генерации задач и оценке траекторий. Обучение на данных, сгенерированных этим конвейером, сопоставимо с обучением на человеческих демонстрациях и значительно улучшает обобщающую способность агентов.'}, 'en': {'title': 'Automating Web Navigation Agent Training with LLMs', 'desc': "This paper presents a new method for training web navigation agents that reduces reliance on human-generated data. It introduces a pipeline where a large language model (LLM) creates tasks for a vast number of websites, allowing agents to learn from these automatically generated tasks. The LLM also evaluates the agents' performance, achieving high accuracy in detecting harmful content and judging task success. The results show that training with this pipeline can significantly enhance the agents' ability to generalize across diverse websites compared to traditional human data methods."}, 'zh': {'title': '无须人工标注，智能代理自我训练！', 'desc': '这篇论文提出了一种新的方法来训练网络导航代理，避免了繁琐的人类标注。首先，使用大型语言模型（LLM）为150,000个不同的网站生成任务。接着，LLM代理完成这些任务并生成轨迹，最后再由LLM对轨迹进行评估。实验结果表明，基于我们的方法训练的代理在多样化网站上表现优异，且在数据有限的情况下，能够显著提高任务的准确性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.18676', 'title': 'Embodied Red Teaming for Auditing Robotic Foundation Models', 'url': 'https://huggingface.co/papers/2411.18676', 'abstract': 'Language-conditioned robot models have the potential to enable robots to perform a wide range of tasks based on natural language instructions. However, assessing their safety and effectiveness remains challenging because it is difficult to test all the different ways a single task can be phrased. Current benchmarks have two key limitations: they rely on a limited set of human-generated instructions, missing many challenging cases, and focus only on task performance without assessing safety, such as avoiding damage. To address these gaps, we introduce Embodied Red Teaming (ERT), a new evaluation method that generates diverse and challenging instructions to test these models. ERT uses automated red teaming techniques with Vision Language Models (VLMs) to create contextually grounded, difficult instructions. Experimental results show that state-of-the-art language-conditioned robot models fail or behave unsafely on ERT-generated instructions, underscoring the shortcomings of current benchmarks in evaluating real-world performance and safety. Code and videos are available at: https://s-karnik.github.io/embodied-red-team-project-page.', 'score': 0, 'issue_id': 2154, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '8f3c4c8885d5b2d0', 'authors': ['Sathwik Karnik', 'Zhang-Wei Hong', 'Nishant Abhangi', 'Yen-Chen Lin', 'Tsun-Hsuan Wang', 'Christophe Dupuy', 'Rahul Gupta', 'Pulkit Agrawal'], 'affiliations': ['Amazon', 'Improbable AI Lab', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2411.18676.jpg', 'data': {'categories': ['#benchmark', '#robotics', '#alignment', '#security', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Новый метод тестирования безопасности языковых моделей для роботов', 'desc': 'Статья представляет новый метод оценки языковых моделей для роботов, называемый Embodied Red Teaming (ERT). ERT использует автоматизированные техники red teaming вместе с Vision Language Models для создания сложных и разнообразных инструкций. Эксперименты показали, что современные языковые модели для роботов часто не справляются с инструкциями, сгенерированными ERT. Это подчеркивает ограниченность существующих методов оценки в отношении реальной производительности и безопасности таких моделей.'}, 'en': {'title': 'Enhancing Robot Safety with Embodied Red Teaming', 'desc': 'This paper introduces a new evaluation method called Embodied Red Teaming (ERT) for assessing language-conditioned robot models. ERT generates diverse and challenging instructions using automated red teaming techniques combined with Vision Language Models (VLMs). The study reveals that existing benchmarks are inadequate, as they do not cover a wide range of task phrasings and fail to evaluate safety measures. Experimental results indicate that current state-of-the-art models often fail or act unsafely when faced with ERT-generated instructions, highlighting the need for improved evaluation methods.'}, 'zh': {'title': '具身红队评估：提升机器人安全性与有效性的新方法', 'desc': '本文介绍了一种新的评估方法，称为具身红队评估（ERT），旨在测试语言条件机器人模型的安全性和有效性。当前的评估基准存在局限性，主要依赖于有限的人类生成指令，且未能考虑安全性问题。ERT通过自动化红队技术与视觉语言模型（VLMs）结合，生成多样且具有挑战性的指令，以更全面地评估机器人模型。实验结果表明，现有的先进语言条件机器人模型在ERT生成的指令上表现不佳或不安全，突显了当前评估基准在真实世界性能和安全性评估中的不足。'}}}, {'id': 'https://huggingface.co/papers/2502.01237', 'title': 'The Differences Between Direct Alignment Algorithms are a Blur', 'url': 'https://huggingface.co/papers/2502.01237', 'abstract': 'Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the beta parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms.', 'score': 79, 'issue_id': 2022, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '18ba45e237fff5e1', 'authors': ['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.01237.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Прямое выравнивание: простой путь к улучшению языковых моделей', 'desc': 'В статье рассматриваются алгоритмы прямого выравнивания (DAA) как альтернатива обучению с подкреплением в контексте выравнивания языковых моделей. Авторы классифицируют DAA по типу функции потерь, используемым наградам и необходимости предварительного обучения. Исследование показывает, что двухэтапные методы превосходят одноэтапные, а ключевым фактором эффективности является использование попарных, а не поточечных целевых функций. Результаты подчеркивают важность тщательной оценки при сравнении алгоритмов выравнивания языковых моделей.'}, 'en': {'title': 'Simplifying Language Model Alignment with Direct Optimization', 'desc': 'This paper introduces Direct Alignment Algorithms (DAAs) as a simpler alternative to traditional Reinforcement Learning from Human Feedback (RLHF) methods. DAAs optimize language model alignment directly, avoiding the complexities of reinforcement learning and reward modeling. The study compares one-stage and two-stage methods, revealing that incorporating a Supervised Fine-Tuning (SFT) phase significantly enhances performance. The findings emphasize the importance of the choice between pairwise and pointwise ranking losses in achieving effective alignment, cautioning against hasty conclusions about algorithm superiority.'}, 'zh': {'title': '优化对齐算法，提升模型性能！', 'desc': '直接对齐算法（DAAs）通过直接优化策略来简化语言模型的对齐，取代了人类反馈强化学习中的强化学习和奖励建模。DAAs可以根据其排名损失（成对与点对）和使用的奖励类型进行分类。研究表明，一阶段方法的表现不如两阶段方法，因此我们引入了显式的监督微调阶段，并在单阶段的ORPO和ASFT中加入了控制偏好优化强度的beta参数。这些改进使得它们在Alpaca Eval 2中的表现得到了显著提升，表明选择成对或点对目标是关键因素，而不是具体的隐式奖励或损失函数。'}}}, {'id': 'https://huggingface.co/papers/2502.01061', 'title': 'OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models', 'url': 'https://huggingface.co/papers/2502.01061', 'abstract': 'End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)', 'score': 74, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '56b819a66e336562', 'authors': ['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.01061.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#training', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'OmniHuman: универсальная модель для генерации реалистичных видео с людьми', 'desc': 'OmniHuman - это новая модель на основе Diffusion Transformer для генерации реалистичных видео с людьми. Она использует смешанные условия движения при обучении, что позволяет масштабировать данные и улучшить качество генерации. Модель поддерживает различные типы портретов, взаимодействие с объектами и сложные позы тела. OmniHuman может генерировать видео на основе аудио, видео или комбинированных сигналов управления.'}, 'en': {'title': 'OmniHuman: Revolutionizing Realistic Human Animation Generation', 'desc': 'The paper presents OmniHuman, a new framework for generating realistic human animations from audio inputs. It utilizes a Diffusion Transformer architecture that enhances the training process by incorporating motion-related conditions, allowing for better scalability in video generation. OmniHuman is designed to handle various types of human portraits and interactions, producing high-quality videos that can depict talking, singing, and complex body movements. This approach not only improves the realism of the generated videos but also increases flexibility by supporting multiple input modalities such as audio and video.'}, 'zh': {'title': 'OmniHuman：灵活真实的人类动画生成', 'desc': '本文提出了一种名为OmniHuman的框架，旨在提升人类动画生成的质量和灵活性。该框架基于扩散变换器，通过在训练阶段混合与运动相关的条件来扩展数据规模。OmniHuman支持多种人像内容和不同的驱动模式，如音频驱动和视频驱动，能够生成高度真实的人类视频。与现有方法相比，OmniHuman不仅生成更真实的视频，还提供了更大的输入灵活性。'}}}, {'id': 'https://huggingface.co/papers/2502.01456', 'title': 'Process Reinforcement through Implicit Rewards', 'url': 'https://huggingface.co/papers/2502.01456', 'abstract': "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.", 'score': 41, 'issue_id': 2019, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '9d62c40e4bafac91', 'authors': ['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Shanghai Jiaotong University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.01456.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'PRIME: Эффективное обучение ИИ-моделей с неявными процессными наградами', 'desc': 'Эта статья представляет новый метод обучения с подкреплением для больших языковых моделей, называемый PRIME. Он использует неявные процессные награды, что позволяет обновлять модели вознаграждения процесса в режиме онлайн, используя только развертывания политики и метки результатов. PRIME решает проблемы обучения с плотными наградами, такие как уязвимость к взлому наград и высокая стоимость сбора качественных меток процесса. Метод показал значительное улучшение производительности на задачах рассуждения по сравнению с базовыми моделями, используя при этом меньше данных для обучения.'}, 'en': {'title': 'Unlocking LLM Potential with PRIME: Efficient Training through Implicit Rewards', 'desc': 'This paper introduces PRIME, a method that enhances the training of large language models (LLMs) using dense process rewards instead of traditional sparse outcome rewards. Dense rewards help improve training efficiency and address credit assignment issues, but collecting high-quality process labels has been a challenge. PRIME allows for online updates of process reward models using only policy rollouts and outcome labels, which reduces the need for extensive reward model training. The results show that PRIME significantly improves reasoning performance in tasks like math and coding, achieving better results with less training data compared to existing models.'}, 'zh': {'title': 'PRIME：提升大语言模型推理效率的新方法', 'desc': '本文提出了一种新的方法PRIME（通过隐式奖励进行过程强化学习），旨在解决大语言模型（LLMs）在复杂多步骤推理任务中的训练效率问题。传统的稀疏结果奖励在训练过程中存在效率低下和信用分配等问题，而PRIME通过仅使用策略回滚和结果标签来实现在线过程奖励模型（PRM）的更新。该方法避免了现有方法中需要的专门奖励模型训练阶段，从而显著降低了开发成本。实验结果表明，PRIME在数学和编码竞赛任务中表现出色，相较于传统模型有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2501.18636', 'title': 'SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model', 'url': 'https://huggingface.co/papers/2501.18636', 'abstract': 'The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.', 'score': 24, 'issue_id': 2023, 'pub_date': '2025-01-28', 'pub_date_card': {'ru': '28 января', 'en': 'January 28', 'zh': '1月28日'}, 'hash': '3a201d426049658a', 'authors': ['Xun Liang', 'Simin Niu', 'Zhiyu Li', 'Sensen Zhang', 'Hanyu Wang', 'Feiyu Xiong', 'Jason Zhaoxin Fan', 'Bo Tang', 'Shichao Song', 'Mengwei Wang', 'Jiawei Yang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China', 'Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.18636.jpg', 'data': {'categories': ['#rag', '#security', '#dataset', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'SafeRAG: оценка уязвимостей генерации с дополнением на основе извлечения', 'desc': 'В статье представлен бенчмарк SafeRAG для оценки безопасности генерации с дополнением на основе извлечения (RAG). Авторы классифицируют атаки на RAG и создают набор данных для их моделирования. Эксперименты показывают, что RAG уязвим ко всем типам атак, даже самым очевидным. Исследование демонстрирует, что существующие компоненты RAG не способны эффективно противостоять атакам, что приводит к снижению качества работы системы.'}, 'en': {'title': 'Strengthening RAG: Evaluating Vulnerabilities in Knowledge Integration', 'desc': 'This paper addresses the security vulnerabilities of retrieval-augmented generation (RAG) systems, which combine external knowledge with large language models (LLMs) for knowledge-intensive tasks. The authors introduce a benchmark called SafeRAG to evaluate the security of RAG by classifying various attack types that can manipulate knowledge. They create a dataset specifically for testing these vulnerabilities and simulate different attack scenarios to assess the impact on RAG performance. The results show that RAG systems are significantly susceptible to these attacks, leading to a decline in service quality, highlighting the need for improved security measures.'}, 'zh': {'title': '提升RAG安全性，抵御知识攻击！', 'desc': '本文介绍了一种名为SafeRAG的基准，用于评估检索增强生成（RAG）模型的安全性。我们将攻击任务分为银噪声、上下文冲突、软广告和拒绝服务等类型，并为每种任务手动构建了安全评估数据集。通过使用SafeRAG数据集，我们模拟了RAG可能遇到的各种攻击场景。实验结果表明，RAG对所有攻击任务表现出显著的脆弱性，甚至最明显的攻击任务也能轻易绕过现有的检索器和过滤器，导致RAG服务质量下降。'}}}, {'id': 'https://huggingface.co/papers/2502.01341', 'title': 'AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding', 'url': 'https://huggingface.co/papers/2502.01341', 'abstract': 'Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.', 'score': 23, 'issue_id': 2030, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '88ea73fcb0da69ba', 'authors': ['Ahmed Masry', 'Juan A. Rodriguez', 'Tianyu Zhang', 'Suyuchen Wang', 'Chao Wang', 'Aarash Feizi', 'Akshay Kalkunte Suresh', 'Abhay Puri', 'Xiangru Jian', 'Pierre-André Noël', 'Sathwik Tejaswi Madhusudhan', 'Marco Pedersoli', 'Bang Liu', 'Nicolas Chapados', 'Yoshua Bengio', 'Enamul Hoque', 'Christopher Pal', 'Issam H. Laradji', 'David Vazquez', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar'], 'affiliations': ['CIFAR AI Chair', 'Ecole de Technologie Superieure', 'McGill University', 'Mila', 'Polytechnique Montreal', 'ServiceNow', 'Universite de Montreal', 'University of British Columbia', 'University of Waterloo', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01341.jpg', 'data': {'categories': ['#alignment', '#cv', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'Улучшение выравнивания модальностей в мультимодальных моделях', 'desc': 'AlignVLM - это новый метод выравнивания визуальных и текстовых признаков в мультимодальных моделях. Он отображает визуальные признаки в виде взвешенного среднего текстовых эмбеддингов языковой модели. Этот подход использует лингвистические приоры, закодированные в языковой модели, чтобы обеспечить эффективную интерпретацию визуальных признаков. AlignVLM особенно эффективен для задач понимания документов и демонстрирует улучшенное выравнивание признаков и устойчивость к шуму.'}, 'en': {'title': 'Aligning Vision and Language for Better Understanding', 'desc': 'This paper addresses the challenge of aligning visual features with language embeddings in vision-language models (VLMs). The authors introduce AlignVLM, a new method that connects visual features to a weighted average of language model (LLM) text embeddings, enhancing semantic similarity. By utilizing the linguistic knowledge embedded in the LLM, AlignVLM ensures that visual features are accurately represented in a way that the LLM can understand. The results show that AlignVLM outperforms existing methods, particularly in tasks involving document understanding, while also demonstrating improved robustness to noisy inputs.'}, 'zh': {'title': '视觉与语言的完美对齐', 'desc': '在视觉语言模型（VLMs）中，将视觉特征与语言嵌入对齐是一个关键挑战。现有的连接器，如多层感知器（MLP），常常会产生分布外或噪声输入，导致模态之间的不对齐。我们提出了一种新颖的视觉文本对齐方法AlignVLM，它将视觉特征映射到LLM文本嵌入的加权平均值。AlignVLM在文档理解任务中表现尤为出色，能够有效提高视觉特征与文本内容的对齐和抗噪声能力。'}}}, {'id': 'https://huggingface.co/papers/2502.01534', 'title': 'Preference Leakage: A Contamination Problem in LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2502.01534', 'abstract': 'Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.', 'score': 22, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd2508b2b8b82b41b', 'authors': ['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], 'affiliations': ['Arizona State University', 'University of California, Los Angeles', 'University of Illinois Urbana Champaign', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2502.01534.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#training', '#dataset', '#leakage'], 'emoji': '🕵️', 'ru': {'title': 'Осторожно: LLM-судьи могут быть предвзяты!', 'desc': "Исследование выявляет проблему 'утечки предпочтений' при использовании больших языковых моделей (LLM) в качестве судей для оценки других моделей. Эта проблема возникает из-за связанности между генераторами синтетических данных и LLM-оценщиками. Эксперименты подтверждают предвзятость судей к связанным с ними моделям-ученикам на различных базовых моделях и эталонных тестах. Результаты указывают на то, что утечка предпочтений является распространенной и трудно обнаруживаемой проблемой в области использования LLM в качестве судей."}, 'en': {'title': 'Uncovering Preference Leakage: A Hidden Bias in LLM Evaluation', 'desc': 'This paper discusses a problem called preference leakage in the context of using Large Language Models (LLMs) as judges for data annotation. Preference leakage occurs when the relationship between the data generators and the evaluators leads to biased evaluations, particularly when they are similar or related models. The authors identify three types of relatedness that can cause this issue and demonstrate through experiments that judges show bias towards their related models. The findings highlight that preference leakage is a significant and often unnoticed challenge in LLM-based model development.'}, 'zh': {'title': '偏好泄漏：LLM评判中的隐患', 'desc': '本文探讨了大型语言模型（LLM）作为评判者和基于LLM的数据合成在模型开发中的应用。我们揭示了偏好泄漏这一问题，它是由合成数据生成器与LLM评估者之间的相关性引起的。通过定义三种常见的相关性，我们进行了广泛的实验，证实了评判者对其相关学生模型的偏见。研究表明，偏好泄漏是一个普遍存在且难以检测的问题，影响了LLM作为评判者的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01639', 'title': 'SliderSpace: Decomposing the Visual Capabilities of Diffusion Models', 'url': 'https://huggingface.co/papers/2502.01639', 'abstract': "We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. Our code, data and trained weights are available at https://sliderspace.baulab.info", 'score': 15, 'issue_id': 2027, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '559003a020b42709', 'authors': ['Rohit Gandikota', 'Zongze Wu', 'Richard Zhang', 'David Bau', 'Eli Shechtman', 'Nick Kolkin'], 'affiliations': ['Adobe Research', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01639.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#dataset', '#open_source', '#multimodal', '#interpretability', '#cv'], 'emoji': '🎚️', 'ru': {'title': 'SliderSpace: Раскрытие скрытых возможностей диффузионных моделей', 'desc': 'SliderSpace - это фреймворк для автоматической декомпозиции визуальных возможностей диффузионных моделей на управляемые и понятные человеку направления. Он обнаруживает множество интерпретируемых и разнообразных направлений одновременно из одного текстового запроса. Каждое направление обучается как адаптер низкого ранга, что позволяет осуществлять композиционный контроль. Эксперименты показывают эффективность SliderSpace в различных приложениях, включая декомпозицию концепций и исследование художественных стилей.'}, 'en': {'title': 'Unlocking Creativity in Diffusion Models with SliderSpace', 'desc': "SliderSpace is a novel framework designed to break down the visual capabilities of diffusion models into controllable and understandable components. It automatically identifies multiple interpretable directions from a single text prompt, eliminating the need for users to define each edit direction manually. By training each direction as a low-rank adaptor, SliderSpace allows for compositional control and reveals unexpected possibilities within the model's latent space. Extensive experiments show that it effectively enhances concept decomposition, artistic style exploration, and diversity, outperforming existing methods in generating diverse and useful variations."}, 'zh': {'title': 'SliderSpace：可控的视觉能力分解', 'desc': 'SliderSpace是一个框架，用于自动分解扩散模型的视觉能力，使其可控且易于理解。与现有方法不同，SliderSpace可以从单个文本提示中同时发现多个可解释和多样化的方向，而无需用户逐个指定属性。每个方向被训练为低秩适配器，从而实现组合控制，并在模型的潜在空间中发现意想不到的可能性。通过对最先进的扩散模型进行广泛实验，我们证明了SliderSpace在概念分解、艺术风格探索和多样性增强等三个应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.00698', 'title': 'MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models', 'url': 'https://huggingface.co/papers/2502.00698', 'abstract': 'IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.   Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.', 'score': 13, 'issue_id': 2027, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 февраля', 'en': 'February 2', 'zh': '2月2日'}, 'hash': '960e3460f0ab8e56', 'authors': ['Huanqia Cai', 'Yijun Yang', 'Winston Hu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00698.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальный ИИ проваливает тест на интеллект', 'desc': 'Статья представляет новый фреймворк MM-IQ для оценки когнитивных способностей мультимодальных систем искусственного интеллекта. Фреймворк включает 2,710 тестовых заданий по 8 типам рассуждений, аналогично тестам IQ для людей. Результаты показали, что даже передовые мультимодальные модели демонстрируют точность лишь немного выше случайного угадывания (27.49% против 25%). Это подчеркивает существенный разрыв между текущими возможностями ИИ и базовыми когнитивными способностями человека.'}, 'en': {'title': 'Bridging the Cognitive Divide in AI with MM-IQ', 'desc': 'This paper introduces MM-IQ, a new evaluation framework designed to measure cognitive abilities in artificial intelligence systems, similar to how IQ tests assess human intelligence. It includes 2,710 test items across 8 reasoning paradigms, providing a comprehensive benchmark for multimodal models. The evaluation of various state-of-the-art models shows that their performance is only slightly better than random guessing, indicating significant limitations in their reasoning capabilities. This finding emphasizes the urgent need for advancements in AI to better replicate human-like reasoning skills.'}, 'zh': {'title': 'MM-IQ：评估多模态系统的认知能力新标准', 'desc': '本论文提出了一种名为MM-IQ的综合评估框架，用于量化多模态系统中的认知能力。该框架包含2710个精心策划的测试项目，涵盖8种不同的推理范式。通过对领先的多模态模型进行系统评估，结果显示即使是最先进的架构，其表现也仅略高于随机猜测。这个显著的性能差距表明当前多模态系统在接近人类基本推理能力方面的不足，强调了需要进行范式转变的进步。'}}}, {'id': 'https://huggingface.co/papers/2502.01068', 'title': 'FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation', 'url': 'https://huggingface.co/papers/2502.01068', 'abstract': 'While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00times and 1.40times improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.', 'score': 12, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '58ab72f123d7a4b6', 'authors': ['Dongwon Jo', 'Jiwon Song', 'Yulhwa Kim', 'Jae-Joon Kim'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea', 'Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.01068.jpg', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'FastKV: Ускорение обработки длинных последовательностей в LLM', 'desc': 'Статья представляет FastKV - новый метод сжатия кэша ключ-значение (KV) для больших языковых моделей (LLM), направленный на улучшение латентности при обработке длинных последовательностей. FastKV использует подход Token-Selective Propagation (TSP), который сохраняет полную контекстную информацию в начальных слоях LLM и выборочно распространяет только часть этой информации в более глубоких слоях. Метод также включает сжатие кэша KV с учетом grouped-query attention (GQA) для повышения эффективности памяти и вычислений. Эксперименты показывают, что FastKV достигает значительного улучшения времени до первого токена и пропускной способности по сравнению с современными методами, сохраняя при этом точность на уровне базовых показателей.'}, 'en': {'title': 'FastKV: Speeding Up Long-Context Processing in LLMs', 'desc': 'This paper presents FastKV, a new method for compressing key-value (KV) caches in large language models (LLMs) to improve computational efficiency and reduce latency. FastKV uses a Token-Selective Propagation (TSP) strategy that keeps full context information in the early layers of the model while selectively passing on only part of this information in the deeper layers. Additionally, it employs grouped-query attention (GQA) to enhance both memory usage and processing speed. Experimental results demonstrate that FastKV significantly improves time-to-first-token and throughput while maintaining accuracy on long-context tasks.'}, 'zh': {'title': 'FastKV：提升长上下文处理速度的创新方法', 'desc': '本文介绍了一种名为FastKV的KV缓存压缩方法，旨在提高长上下文序列的处理速度。FastKV采用了一种新颖的选择性传播方法（TSP），在LLM的初始层保留完整的上下文信息，而在更深层次中仅选择性传播部分信息。该方法还结合了分组查询注意力（GQA）来优化内存和计算效率。实验结果表明，FastKV在首次令牌时间和吞吐量方面分别比现有的HeadKV方法提高了2.00倍和1.40倍，同时在长上下文基准测试中保持了与基线相当的准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.00094', 'title': 'AIN: The Arabic INclusive Large Multimodal Model', 'url': 'https://huggingface.co/papers/2502.00094', 'abstract': "Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.", 'score': 12, 'issue_id': 2018, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'e2d63540ee133732', 'authors': ['Ahmed Heakl', 'Sara Ghaboura', 'Omkar Thawkar', 'Fahad Shahbaz Khan', 'Hisham Cholakkal', 'Rao Muhammad Anwer', 'Salman Khan'], 'affiliations': ['Aalto University', 'Australian National University', 'Linköping University', 'Mohamed bin Zayed University of AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.00094.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#low_resource', '#multimodal', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'AIN: Прорыв в арабоязычном мультимодальном ИИ', 'desc': 'Представлена модель AIN - двуязычная мультимодальная языковая модель для арабского и английского языков. Модель обучена на 3,6 миллионах высококачественных мультимодальных арабско-английских образцов данных. AIN демонстрирует передовые результаты в арабском языке, сохраняя при этом сильные визуальные возможности для английского. На бенчмарке CAMEL-Bench, охватывающем 38 поддоменов, 7B-версия AIN превосходит GPT-4o на 3,4% в среднем по восьми доменам.'}, 'en': {'title': 'Empowering Arabic with Advanced Multimodal AI', 'desc': 'This paper presents AIN, the Arabic Inclusive Multimodal Model, which is designed to enhance the performance of large multimodal models (LMMs) specifically for Arabic and English. AIN utilizes a substantial dataset of 3.6 million high-quality Arabic-English multimodal samples to achieve state-of-the-art results in Arabic language tasks. The model excels across various domains, as evidenced by its performance on the CAMEL-Bench benchmark, where it surpasses GPT-4o in multiple sub-domains. AIN aims to provide advanced generative AI tools for Arabic speakers, addressing the current limitations in Arabic multimodal understanding.'}, 'zh': {'title': '推动阿拉伯语多模态AI的进步', 'desc': '随着大型语言模型（LLMs）和多模态模型（LMMs）的快速发展，阿拉伯语的研究仍然相对滞后。我们提出了AIN模型，这是一个旨在提升阿拉伯语和英语的多模态模型，利用了360万高质量的阿拉伯语-英语多模态数据样本。AIN在多个领域表现出色，尤其是在复杂的视觉理解和多图像理解方面，超越了现有的GPT-4o模型。该模型的优越性能为阿拉伯语使用者提供了先进的多模态生成AI工具，推动了相关应用的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.01142', 'title': 'DeepRAG: Thinking to Retrieval Step by Step for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01142', 'abstract': 'Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.', 'score': 9, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'e26994166d750227', 'authors': ['Xinyan Guan', 'Jiali Zeng', 'Fandong Meng', 'Chunlei Xin', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun', 'Jie Zhou'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.01142.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#reasoning', '#rag', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DeepRAG: умное сочетание поиска и рассуждений для ИИ', 'desc': 'DeepRAG - это новый фреймворк, моделирующий рассуждения с поиском информации как марковский процесс принятия решений. Он итеративно декомпозирует запросы, динамически определяя необходимость внешнего поиска или параметрического рассуждения на каждом шаге. Эксперименты показывают, что DeepRAG повышает эффективность поиска и точность ответов на 21.99%. Фреймворк решает проблемы больших языковых моделей, связанные с фактическими галлюцинациями и неэффективной декомпозицией задач при интеграции рассуждений с поиском.'}, 'en': {'title': 'Enhancing Reasoning with Smart Retrieval', 'desc': 'This paper introduces DeepRAG, a new framework that enhances retrieval-augmented reasoning in large language models. It treats the retrieval process as a Markov Decision Process (MDP), allowing for more strategic and adaptive retrieval of information. By breaking down queries into smaller parts, DeepRAG can decide whether to fetch external knowledge or use its built-in reasoning capabilities at each step. The results show that DeepRAG not only makes retrieval more efficient but also increases the accuracy of answers by nearly 22%.'}, 'zh': {'title': 'DeepRAG：优化检索增强推理的新框架', 'desc': '大型语言模型（LLMs）在推理方面展现了显著的潜力，但仍然面临严重的事实幻觉问题，主要由于参数知识的时效性、准确性和覆盖率不足。将推理与检索增强生成（RAG）结合起来仍然具有挑战性，主要是因为任务分解不有效和冗余检索可能引入噪声，降低响应质量。本文提出了DeepRAG框架，将检索增强推理建模为马尔可夫决策过程（MDP），实现了战略性和自适应的检索。实验表明，DeepRAG在提高检索效率的同时，答案准确性提高了21.99%，证明了其在优化检索增强推理方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01637', 'title': 'Scaling Embedding Layers in Language Models', 'url': 'https://huggingface.co/papers/2502.01637', 'abstract': 'We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent n-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached n-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.', 'score': 9, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '478a2a0ee08530a8', 'authors': ['Da Yu', 'Edith Cohen', 'Badih Ghazi', 'Yangsibo Huang', 'Pritish Kamath', 'Ravi Kumar', 'Daogao Liu', 'Chiyuan Zhang'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.01637.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без увеличения вычислительных затрат', 'desc': 'SCONE - это новый метод расширения слоёв входных эмбеддингов для улучшения производительности языковых моделей при увеличении размера слоя. Он вводит эмбеддинги для частых n-грамм, обеспечивая контекстуализированное представление для каждого входного токена. Эти эмбеддинги обучаются отдельной моделью и предварительно вычисляются для использования во время инференса. SCONE позволяет масштабировать количество кэшированных n-граммных эмбеддингов и модель для их обучения, сохраняя фиксированное количество FLOPS при инференсе.'}, 'en': {'title': 'Enhancing Language Models with SCONE: Scalable N-gram Embeddings for Better Performance', 'desc': 'SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding) is a novel approach designed to improve the performance of language models as they grow in size. It introduces embeddings for common n-grams while keeping the original vocabulary intact, which helps in providing better contextual representations for input tokens. These n-gram embeddings are learned through a separate model during training and stored in off-accelerator memory to ensure fast inference. By scaling both the number of cached n-gram embeddings and the model that learns them, SCONE achieves superior performance compared to a large baseline model while maintaining efficient inference-time computations.'}, 'zh': {'title': 'SCONE：提升语言模型性能的新方法', 'desc': '我们提出了一种方法SCONE（可扩展的上下文化的离线N-gram嵌入），旨在通过扩展输入嵌入层来提升语言模型的性能。SCONE在保持原有词汇的同时，引入了一组常见n-gram的嵌入，以提供每个输入标记的上下文化表示。这些嵌入在训练过程中由一个单独的模型学习，并在推理时预先计算并存储在离线加速器内存中，几乎不影响推理速度。通过增加缓存的n-gram嵌入数量和扩展学习它们的模型，SCONE在多种语料库上超越了1.9B参数的基线，同时仅使用一半的推理时间FLOPS。'}}}, {'id': 'https://huggingface.co/papers/2502.01100', 'title': 'ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning', 'url': 'https://huggingface.co/papers/2502.01100', 'abstract': 'We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.   Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.', 'score': 8, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '901fd196d7bfe394', 'authors': ['Bill Yuchen Lin', 'Ronan Le Bras', 'Kyle Richardson', 'Ashish Sabharwal', 'Radha Poovendran', 'Peter Clark', 'Yejin Choi'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.01100.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Проклятие сложности в логическом мышлении LLM', 'desc': 'В статье исследуются возможности логического рассуждения больших языковых моделей (LLM) и их масштабируемость в сложных задачах немонотонного вывода. Для этого представлена ZebraLogic, комплексная система оценки производительности LLM на логических головоломках, основанных на задачах удовлетворения ограничений (CSP). ZebraLogic позволяет генерировать головоломки с контролируемой сложностью, что помогает систематически изучать пределы масштабируемости моделей, таких как Llama и DeepSeek-R1. Результаты показывают значительное снижение точности по мере увеличения сложности задач, что авторы называют "проклятием сложности".'}, 'en': {'title': 'Unraveling the Limits of Logical Reasoning in Large Language Models', 'desc': "This paper examines how well large language models (LLMs) can perform logical reasoning, especially in complex scenarios where reasoning does not follow a straightforward path. The authors introduce ZebraLogic, a new framework designed to evaluate LLMs on logic grid puzzles that are based on constraint satisfaction problems (CSPs). Through this framework, they discover that as the complexity of the puzzles increases, the accuracy of the models significantly decreases, a challenge they refer to as the 'curse of complexity.' The study also suggests methods to improve reasoning capabilities, such as using advanced sampling techniques and self-verification prompts, while highlighting the limitations of current LLMs in handling complex reasoning tasks."}, 'zh': {'title': '揭示大型语言模型推理能力的复杂性挑战', 'desc': '本文研究了大型语言模型（LLMs）的逻辑推理能力及其在复杂非单调推理中的可扩展性。我们引入了ZebraLogic，一个全面的评估框架，用于评估LLM在基于约束满足问题（CSPs）的逻辑网格谜题上的推理表现。研究结果显示，随着问题复杂性的增加，模型的准确性显著下降，这一现象被称为复杂性诅咒。我们还探讨了增强逻辑推理的策略，包括最佳采样、回溯机制和自我验证提示。'}}}, {'id': 'https://huggingface.co/papers/2502.01081', 'title': 'The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles', 'url': 'https://huggingface.co/papers/2502.01081', 'abstract': "The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.", 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '7585b424ff041825', 'authors': ['Vernon Y. H. Toh', 'Yew Ken Chia', 'Deepanway Ghosal', 'Soujanya Poria'], 'affiliations': ['Singapore University of Technology and Design (SUTD)'], 'pdf_title_img': 'assets/pdf/title_img/2502.01081.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agi', '#open_source', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эволюция рассуждений: от символов к мультимодальности', 'desc': 'Статья рассматривает эволюцию возможностей рассуждения в мультимодальных задачах у языковых моделей серий GPT и OpenAI. Авторы отмечают значительный прогресс модели o3 в решении символических паттернов, но подчеркивают необходимость исследования мультимодальных сценариев. Проводится анализ производительности моделей на сложных визуально-лингвистических головоломках, требующих абстрактного и алгоритмического мышления. Результаты показывают общую тенденцию улучшения способностей к рассуждению, но выявляют сохраняющиеся трудности даже у передовых моделей в некоторых типах задач.'}, 'en': {'title': 'Advancing Reasoning in Multimodal AI: A New Era for LLMs', 'desc': "This paper discusses the advancements in Large Language Models (LLMs) with the release of OpenAI's o1 and o3, which show improved reasoning abilities. The o3 model has demonstrated superior problem-solving skills compared to humans on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, the study highlights that these models primarily focus on symbolic reasoning, while human reasoning often involves multimodal inputs like vision and language. The authors emphasize the need for further research into multimodal reasoning capabilities, as the o1 model, despite its high performance, still faces challenges with simple multimodal and algorithmic puzzles."}, 'zh': {'title': '多模态推理能力的探索与挑战', 'desc': '本文探讨了OpenAI的o1和o3模型在大型语言模型中的先进推理能力。o3在抽象和推理语料库（ARC-AGI）中超越了人类，表现出色，但该基准仅限于符号模式。人类通常在多模态场景中进行推理，因此需要研究多模态任务中的高级推理能力。尽管o1在推理能力上有所提升，但在简单的多模态难题和算法难题上仍然存在不足。'}}}, {'id': 'https://huggingface.co/papers/2502.01591', 'title': 'Improving Transformer World Models for Data-Efficient RL', 'url': 'https://huggingface.co/papers/2502.01591', 'abstract': 'We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.', 'score': 7, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd9195e9417fce419', 'authors': ['Antoine Dedieu', 'Joseph Ortiz', 'Xinghua Lou', 'Carter Wendelken', 'Wolfgang Lehrach', 'J Swaroop Guntupalli', 'Miguel Lazaro-Gredilla', 'Kevin Patrick Murphy'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.01591.jpg', 'data': {'categories': ['#rl', '#architecture', '#benchmark', '#games', '#training', '#reasoning', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Новый рубеж в model-based RL: превосходя человека в Craftax-classic', 'desc': 'Статья представляет новый подход к обучению с подкреплением на основе модели, достигающий наилучших результатов на бенчмарке Craftax-classic. Авторы разработали алгоритм, превосходящий как предыдущий SOTA-метод DreamerV3, так и человеческий уровень производительности. Ключевые улучшения включают комбинированную архитектуру политики с CNN и RNN, обучение на реальных и воображаемых данных, токенизацию изображений методом ближайших соседей и блочное teacher forcing для трансформерной модели мира. Эти инновации позволили значительно повысить эффективность использования данных в сложной среде, требующей широкого спектра навыков.'}, 'en': {'title': 'Revolutionizing Model-Based RL for Superior Game Performance', 'desc': "This paper introduces a new model-based reinforcement learning (MBRL) approach that excels in the Craftax-classic benchmark, a complex 2D survival game. The proposed algorithm achieves a remarkable reward of 67.4% after just 1 million environment steps, surpassing previous methods like DreamerV3 and even human performance. Key innovations include a novel policy architecture that integrates convolutional neural networks (CNNs) and recurrent neural networks (RNNs), along with enhancements like 'Dyna with warmup' for training on both real and simulated data. Additional techniques such as a nearest neighbor tokenizer for image patches and block teacher forcing for future reasoning further boost the model's efficiency and effectiveness."}, 'zh': {'title': '基于模型的强化学习新突破！', 'desc': '本文提出了一种基于模型的强化学习方法，在Craftax-classic基准测试中取得了新的最佳表现。这是一款开放世界的2D生存游戏，要求智能体展现出强大的泛化能力、深度探索能力和长期推理能力。我们的MBRL算法在仅1M环境步骤后获得了67.4%的奖励，显著超越了DreamerV3的53.2%，并首次超过了人类表现的65.0%。该方法通过构建一个最先进的无模型基线，并结合CNN和RNN的新型策略架构，进一步提升了样本效率。'}}}, {'id': 'https://huggingface.co/papers/2502.01208', 'title': 'Almost Surely Safe Alignment of Large Language Models at Inference-Time', 'url': 'https://huggingface.co/papers/2502.01208', 'abstract': "Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.", 'score': 6, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'fdbe8816f1c6476a', 'authors': ['Xiaotong Ji', 'Shyam Sundhar Ramesh', 'Matthieu Zimmer', 'Ilija Bogunovic', 'Jun Wang', 'Haitham Bou Ammar'], 'affiliations': ['Huawei Noahs Ark Lab', 'Imperial College London', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2502.01208.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#inference', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Безопасная генерация ответов LLM без переобучения', 'desc': 'Статья представляет новый подход к выравниванию больших языковых моделей (LLM) во время вывода, обеспечивающий генерацию безопасных ответов с вероятностью, близкой к единице. Авторы формулируют задачу как ограниченный марковский процесс принятия решений в латентном пространстве модели. Они вводят состояние безопасности, которое отслеживает эволюцию ограничений безопасности и позволяет доказать формальные гарантии безопасности. На основе этого подхода разработан метод InferenceGuard, который эффективно балансирует безопасность и производительность задачи, превосходя существующие методы выравнивания во время вывода.'}, 'en': {'title': 'InferenceGuard: Ensuring Safe Responses from LLMs at Inference Time', 'desc': "This paper addresses the issue of biased or unsafe responses from large language models (LLMs) by introducing a new method for aligning their outputs during inference. Instead of retraining the model, the authors propose a constrained Markov decision process that operates in the LLM's latent space to ensure safe response generation. They enhance this process with a safety state that monitors safety constraints, providing formal guarantees of safety. The proposed method, called InferenceGuard, shows improved performance in generating safe and aligned responses compared to existing techniques without altering the model's weights."}, 'zh': {'title': '推理时安全对齐的新方法', 'desc': '本文提出了一种新的推理时对齐方法，旨在确保大型语言模型（LLM）生成安全的响应。我们将安全生成推理时响应的问题框架化为一个约束的马尔可夫决策过程（MDP），并在LLM的潜在空间中进行处理。通过引入一个安全状态来跟踪安全约束的演变，我们能够在解决MDP时提供正式的安全保证。我们提出的InferenceGuard实现了在不修改模型权重的情况下安全对齐LLM，并在生成安全和对齐响应方面优于现有的方法。'}}}, {'id': 'https://huggingface.co/papers/2502.01441', 'title': 'Improved Training Technique for Latent Consistency Models', 'url': 'https://huggingface.co/papers/2502.01441', 'abstract': 'Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-c scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/', 'score': 6, 'issue_id': 2018, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '2ea077ca6fd7397f', 'authors': ['Quan Dao', 'Khanh Doan', 'Di Liu', 'Trung Le', 'Dimitris Metaxas'], 'affiliations': ['Monash University', 'Rutgers University', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01441.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#architecture', '#open_source', '#diffusion', '#video'], 'emoji': '🧠', 'ru': {'title': 'Преодоление выбросов в латентном пространстве для улучшения консистентных моделей', 'desc': 'Эта статья представляет новый подход к обучению консистентных моделей в латентном пространстве для генеративных задач. Авторы обнаружили, что латентные данные часто содержат импульсивные выбросы, которые ухудшают производительность iCT. Для решения этой проблемы они предложили использовать функцию потерь Коши вместо Псевдо-Хубера, а также ввели диффузионные потери на ранних временных шагах и применили оптимальный транспорт. Эти стратегии позволили успешно обучить латентные консистентные модели, способные к высококачественному сэмплированию за один-два шага.'}, 'en': {'title': 'Enhancing Latent Consistency Models for High-Quality Generation', 'desc': "This paper introduces advancements in consistency models, a type of generative model that can create high-quality outputs efficiently. The authors focus on improving performance in latent spaces, where data often contains outliers that hinder model effectiveness. By replacing traditional loss functions with Cauchy losses and incorporating diffusion loss, they enhance the model's robustness against these outliers. Additionally, they propose an adaptive scaling-c scheduler and Non-scaling LayerNorm to optimize training, resulting in latent consistency models that perform comparably to diffusion models in generating images and videos."}, 'zh': {'title': '提升一致性模型性能的创新方法', 'desc': '一致性模型是一种新型生成模型，能够在单步或多步中生成高质量样本。最近，这些模型在像素空间中表现出色，达到了与扩散模型相当的效果。然而，在大规模数据集上进行一致性训练的成功，尤其是在文本到图像和视频生成任务中，取决于潜在空间的表现。为了解决潜在数据中的异常值对性能的影响，本文提出了使用Cauchy损失替代伪Huber损失，并引入扩散损失和最优传输方法，以提高模型的鲁棒性和性能。'}}}, {'id': 'https://huggingface.co/papers/2502.01584', 'title': 'PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01584', 'abstract': "Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.", 'score': 5, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '6cf23c9aeb70961a', 'authors': ['Carolyn Jane Anderson', 'Joydeep Biswas', 'Aleksander Boruch-Gruszecki', 'Federico Cassano', 'Molly Q Feldman', 'Arjun Guha', 'Francesca Lucchetti', 'Zixuan Wu'], 'affiliations': ['Charles University', 'Cursor', 'Northeastern University', 'Oberlin College', 'University of Texas at Austin', 'Wellesley College'], 'pdf_title_img': 'assets/pdf/title_img/2502.01584.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Новый бенчмарк раскрывает скрытые возможности и недостатки языковых моделей', 'desc': 'Авторы представляют новый бенчмарк для оценки языковых моделей, основанный на головоломках NPR Sunday Puzzle Challenge. В отличие от существующих тестов, требующих специализированных знаний, этот бенчмарк опирается на общие знания и легко проверяем. Исследование выявило значительное превосходство модели OpenAI o1 над другими моделями рассуждений. Анализ также обнаружил новые типы ошибок у моделей, такие как преждевременная капитуляция и неуверенность в ответах.'}, 'en': {'title': 'Bridging Knowledge Gaps in AI Reasoning with General Knowledge Benchmarks', 'desc': "This paper introduces a new benchmark for evaluating machine learning models, focusing on general knowledge rather than specialized expertise. The benchmark is based on the NPR Sunday Puzzle Challenge, making it accessible yet challenging for both humans and AI models. The authors highlight that existing benchmarks may not reveal certain capability gaps, as demonstrated by OpenAI's model outperforming others in reasoning tasks. Additionally, the study identifies specific failure modes in models, such as premature concessions and uncertainty in outputs, suggesting improvements in inference techniques are necessary."}, 'zh': {'title': '挑战性与可验证性的全新基准测试', 'desc': '现有的前沿模型基准测试通常考察专业的、难以理解的知识。我们提出了一种基于NPR周日谜题挑战的基准，要求仅具备一般知识。该基准对人类和模型都具有挑战性，但正确答案易于验证，模型的错误也容易发现。我们的研究揭示了现有基准中未显现的能力差距，OpenAI o1在推理模型中表现优异，且分析推理输出揭示了新的失败类型。'}}}, {'id': 'https://huggingface.co/papers/2502.01636', 'title': 'Lifelong Sequential Knowledge Editing without Model Degradation', 'url': 'https://huggingface.co/papers/2502.01636', 'abstract': 'Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this "importance hacking", the edited layers provide a much larger contributions to the model\'s output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B.', 'score': 4, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '919dd5274b620b3a', 'authors': ['Akshat Gupta', 'Phudish Prateepamornkul', 'Maochuan Lu', 'Ahmed Alaa', 'Thomas Hartvigsen', 'Gopala Anumanchipalli'], 'affiliations': ['SCB DataX', 'University of California, Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2502.01636.jpg', 'data': {'categories': ['#training', '#interpretability', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование редактирования знаний в нейросетях', 'desc': 'Статья посвящена улучшению методов последовательного редактирования знаний в больших языковых моделях. Авторы выявили проблемы переобучения и непропорционального роста нормы при использовании существующих методов редактирования. Они предложили новый метод ENCORE, который контролирует переобучение и рост нормы, позволяя выполнять до 10 000 последовательных правок без потери производительности модели. ENCORE также показывает значительное ускорение по сравнению с другими методами редактирования знаний.'}, 'en': {'title': 'ENCORE: Efficient Knowledge Editing Without Degradation', 'desc': "This paper investigates the challenges of sequential knowledge editing in machine learning models, particularly focusing on the degradation of model performance after numerous edits. It identifies that traditional locate-then-edit methods can lead to overfitting and excessive growth in the norm of the edited parameters. The authors introduce a new method called ENCORE, which employs early stopping and norm constraints to prevent these issues, allowing for effective long-term editing. ENCORE not only maintains the model's performance after 10,000 edits but also operates significantly faster than existing methods."}, 'zh': {'title': 'ENCORE：高效的知识编辑解决方案', 'desc': '本论文研究了在知识编辑中进行大规模顺序编辑时模型性能下降的原因。我们发现，定位后编辑的方法容易导致对编辑事实的过拟合，并且连续的知识编辑会导致编辑矩阵的范数不成比例地增长。为了解决这些问题，我们提出了ENCORE方法，通过早停和范数约束来控制过拟合和范数增长，从而实现长时间的顺序编辑。ENCORE能够在不损失下游性能的情况下，进行多达10,000次的顺序编辑，并且比现有方法更快。'}}}, {'id': 'https://huggingface.co/papers/2502.01619', 'title': 'Learning to Generate Unit Tests for Automated Debugging', 'url': 'https://huggingface.co/papers/2502.01619', 'abstract': "Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to a large language model (LLM) as it iteratively debugs faulty code, motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions and candidate code. We integrate UTGen into UTDebug, a robust debugging pipeline that uses generated tests to help LLMs debug effectively. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), UTDebug (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and back-tracks edits based on multiple generated UTs to avoid overfitting. We show that UTGen outperforms UT generation baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen-2.5 7B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3% and 12.35% (respectively) over other LLM-based UT generation baselines.", 'score': 2, 'issue_id': 2036, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '0806c010a6d2569d', 'authors': ['Archiki Prasad', 'Elias Stengel-Eskin', 'Justin Chih-Yao Chen', 'Zaid Khan', 'Mohit Bansal'], 'affiliations': ['Department of Computer Science, UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2502.01619.jpg', 'data': {'categories': ['#training', '#interpretability', '#optimization', '#plp'], 'emoji': '🧪', 'ru': {'title': 'Автоматическая генерация юнит-тестов для улучшения отладки кода языковыми моделями', 'desc': 'Статья представляет UTGen - метод обучения языковых моделей генерации юнит-тестов, выявляющих ошибки в коде и предсказывающих корректные выходные данные. UTGen интегрирован в конвейер отладки UTDebug, который использует сгенерированные тесты для эффективной отладки кода языковыми моделями. UTDebug масштабирует UTGen во время выполнения для улучшения предсказания выходных данных тестов и проверяет изменения на основе нескольких сгенерированных юнит-тестов. Результаты показывают, что UTGen превосходит базовые методы генерации юнит-тестов, а UTDebug улучшает точность отладки кода языковыми моделями.'}, 'en': {'title': 'Enhancing Debugging with Smart Unit Test Generation', 'desc': 'This paper introduces UTGen, a method that helps large language models (LLMs) generate unit test inputs that can effectively reveal errors in faulty code while also predicting the correct outputs. The authors highlight a challenge where generating tests that expose errors can lead to incorrect output predictions without having the correct solutions available. To overcome this, UTGen is integrated into a debugging pipeline called UTDebug, which enhances the debugging process by validating and refining the generated tests. The results show that UTGen significantly improves the accuracy of LLMs in debugging tasks, outperforming existing methods in generating effective unit tests.'}, 'zh': {'title': '自动化单元测试生成与调试的创新方案', 'desc': '本文提出了一种名为UTGen的自动化单元测试生成方法，旨在帮助大型语言模型（LLM）生成能够揭示错误的单元测试输入及其正确的预期输出。研究发现，生成的单元测试输入在揭示错误和正确预测输出之间存在权衡。UTGen被集成到UTDebug调试管道中，以提高LLM的调试效果，并通过多次生成的测试来验证和回溯编辑，避免过拟合。实验结果表明，UTGen在生成有效单元测试方面优于现有基线，并显著提高了LLM在调试任务中的准确性。'}}}, {'id': 'https://huggingface.co/papers/2501.18055', 'title': 'Current Pathology Foundation Models are unrobust to Medical Center Differences', 'url': 'https://huggingface.co/papers/2501.18055', 'abstract': 'Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences. We introduce the Robustness Index. This novel robustness metric reflects to what degree biological features dominate confounding features. Ten current publicly available pathology FMs are evaluated. We find that all current pathology foundation models evaluated represent the medical center to a strong degree. Significant differences in the robustness index are observed. Only one model so far has a robustness index greater than one, meaning biological features dominate confounding features, but only slightly. A quantitative approach to measure the influence of medical center differences on FM-based prediction performance is described. We analyze the impact of unrobustness on classification performance of downstream models, and find that cancer-type classification errors are not random, but specifically attributable to same-center confounders: images of other classes from the same medical center. We visualize FM embedding spaces, and find these are more strongly organized by medical centers than by biological factors. As a consequence, the medical center of origin is predicted more accurately than the tissue source and cancer type. The robustness index introduced here is provided with the aim of advancing progress towards clinical adoption of robust and reliable pathology FMs.', 'score': 1, 'issue_id': 2024, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 января', 'en': 'January 29', 'zh': '1月29日'}, 'hash': '444c1f08656649e7', 'authors': ['Edwin D. de Jong', 'Eric Marcus', 'Jonas Teuwen'], 'affiliations': ['Aignostics', 'Antoni van Leeuwenhoek Hospital (AvL)', 'Kaiko', 'The Netherlands Cancer Institute Amsterdam (NKI)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18055.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#security', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Путь к надежным фундаментальным моделям в патологии: преодоление влияния медицинских центров', 'desc': 'Статья рассматривает проблему надежности фундаментальных моделей (ФМ) в патологии для использования в клинической практике. Авторы вводят новую метрику - Индекс надежности, который отражает степень доминирования биологических признаков над мешающими факторами, связанными с особенностями медицинских центров. Оценка десяти публично доступных ФМ показала, что все они сильно зависят от специфики медицинских центров, и только одна модель имеет индекс надежности больше единицы. Исследование демонстрирует, что ошибки классификации типов рака связаны с конфаундерами из того же медицинского центра, а пространства вложений ФМ организованы больше по медицинским центрам, чем по биологическим факторам.'}, 'en': {'title': 'Ensuring Robustness in Pathology Models for Clinical Use', 'desc': "This paper discusses the evaluation of Pathology Foundation Models (FMs) to ensure they are reliable for clinical use. It introduces a new metric called the Robustness Index, which measures how much these models focus on biological features versus confounding factors from different medical centers. The study finds that most current pathology FMs are heavily influenced by the medical center's characteristics rather than the actual biological data. The authors emphasize the need for models that prioritize biological features to improve classification accuracy and support clinical adoption."}, 'zh': {'title': '确保病理模型的鲁棒性，助力临床应用', 'desc': '病理基础模型（FMs）在医疗保健中具有很大潜力，但在临床应用之前，必须确保它们对不同医疗中心的变化具有鲁棒性。我们提出了鲁棒性指数，这是一种新颖的鲁棒性度量，反映生物特征在多大程度上主导了混淆特征。研究发现，当前的病理基础模型在很大程度上代表了医疗中心，只有一个模型的鲁棒性指数大于一，表明生物特征略微主导混淆特征。通过定量方法分析医疗中心差异对模型预测性能的影响，发现癌症类型分类错误与同中心混淆因素密切相关。'}}}, {'id': 'https://huggingface.co/papers/2502.00314', 'title': 'A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation', 'url': 'https://huggingface.co/papers/2502.00314', 'abstract': "The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub.", 'score': 1, 'issue_id': 2023, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '51ad114da14800b7', 'authors': ['Moein Heidari', 'Ehsan Khodapanah Aghdam', 'Alexander Manzella', 'Daniel Hsu', 'Rebecca Scalabrino', 'Wenjin Chen', 'David J. Foran', 'Ilker Hacihaliloglu'], 'affiliations': ['Beth Israel Deaconess Medical Center, Boston, MA, United States', 'Center for Biomedical Imaging and Informatics, Rutgers Cancer Institute, New Brunswick, NJ, United States', 'Department of Medicine, University of British Columbia, British Columbia, Canada', 'Department of Radiology, University of British Columbia, British Columbia, Canada', 'Harvard Medical School, Boston, MA, United States', 'Independent Researcher, Tabriz, Iran', 'Memorial Sloan Kettering Cancer Center, New York, NY, United States', 'Rutgers Robert Wood Johnson Medical School, New Brunswick, NJ, United States', 'School of Biomedical Engineering, University of British Columbia, British Columbia, Canada', 'Weill Cornell Medical School, New York, NY, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.00314.jpg', 'data': {'categories': ['#architecture', '#cv', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Эффективная сегментация опухолей с помощью усовершенствованных нейросетей', 'desc': 'Статья посвящена автоматической сегментации опухолей забрюшинного пространства с использованием различных архитектур глубокого обучения. Авторы сравнивают эффективность U-Net и его модификаций, включая CNN, ViT, Mamba SSM и xLSTM, на новом наборе данных КТ. Предложенная модель ViLU-Net интегрирует Vi-блоки для улучшения сегментации. Результаты показывают, что xLSTM обеспечивает наиболее эффективную работу в рамках архитектуры U-Net.'}, 'en': {'title': 'Efficient Tumor Segmentation with ViLU-Net: Merging U-Net and Vision Transformers', 'desc': "This paper addresses the challenges of segmenting tumors in the retroperitoneum, which can be irregularly shaped and difficult to analyze. It explores the use of advanced machine learning models, particularly U-Net and its enhancements, to automate the segmentation process. The study introduces the ViLU-Net model, which incorporates Vision Transformer blocks to improve segmentation accuracy while maintaining computational efficiency. Results indicate that the xLSTM architecture significantly enhances the U-Net framework's performance, making it a promising approach for medical image analysis."}, 'zh': {'title': '高效肿瘤分割：ViLU-Net的创新应用', 'desc': '本研究探讨了在后腹膜肿瘤的自动分割中使用U-Net及其变体的有效性。这些肿瘤形状不规则，手动分割耗时且困难，因此需要更高效的自动化方法。研究中引入了Mamba状态空间模型和扩展长短期记忆（xLSTM）等架构，以降低计算资源消耗并处理长距离依赖。最终提出的ViLU-Net模型通过集成Vi-blocks，显著提高了分割效果，xLSTM在U-Net框架中的效率表现尤为突出。'}}}, {'id': 'https://huggingface.co/papers/2502.01126', 'title': 'Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences', 'url': 'https://huggingface.co/papers/2502.01126', 'abstract': 'Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty. However, models struggle to provide absolute assessments of confidence (i.e. judging confidence in answering a question independent of other questions) and the coarse-grained scores they produce are not useful for evaluating the correctness of their answers. We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence ("Which question are you more confident in answering correctly?"). Treating each question as a "player" in a series of matchups against other questions and the model\'s preferences as match outcomes, we can use rank aggregation methods like Elo rating and Bradley-Terry to translate the model\'s confidence preferences into confidence scores. We evaluate relative confidence estimation against absolute confidence estimation and self-consistency confidence methods on five state-of-the-art LMs -- GPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405B -- across 14 challenging STEM, social science, and commonsense reasoning question answering tasks. Our results demonstrate that relative confidence estimation consistently provides more reliable confidence scores than absolute confidence estimation, with average gains of 3.5% in selective classification AUC over direct absolute confidence estimation methods and 1.7% over self-consistency approaches across all models and datasets.', 'score': 0, 'issue_id': 2035, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'ed2eef6c3e310379', 'authors': ['Vaishnavi Shrivastava', 'Ananya Kumar', 'Percy Liang'], 'affiliations': ['cs.stanford.edu'], 'pdf_title_img': 'assets/pdf/title_img/2502.01126.jpg', 'data': {'categories': ['#interpretability', '#rlhf', '#reasoning', '#benchmark', '#data'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности языковых моделей через относительную оценку уверенности', 'desc': 'Исследование предлагает метод относительной оценки уверенности языковых моделей, где модель сравнивает свою уверенность в ответах на разные вопросы. Этот подход использует методы ранжирования, такие как рейтинг Эло и модель Брэдли-Терри, для перевода предпочтений модели в оценки уверенности. Результаты показывают, что относительная оценка уверенности превосходит абсолютную оценку и методы самосогласованности на 3.5% и 1.7% соответственно по метрике AUC выборочной классификации. Исследование проводилось на пяти современных языковых моделях и 14 сложных задачах по ответам на вопросы в областях STEM, социальных наук и здравого смысла.'}, 'en': {'title': 'Boosting Confidence: Relative Estimation Outshines Absolute Scores in Language Models', 'desc': 'This paper discusses the importance of reliable confidence estimates from language models (LMs) to help users identify potential errors in their outputs. It highlights the challenges LMs face in providing absolute confidence assessments and proposes a new method called relative confidence estimation. This method involves comparing questions against each other to determine which the model is more confident in answering correctly, using techniques like Elo rating and Bradley-Terry for ranking. The study shows that relative confidence estimation outperforms traditional absolute confidence methods, leading to improved reliability in confidence scores across various question answering tasks.'}, 'zh': {'title': '相对置信度估计：提升语言模型的可靠性', 'desc': '本文探讨了语言模型（LM）如何提供可靠的置信度评估，以帮助用户识别输出中的错误。我们提出了一种相对置信度估计的方法，通过比较问题之间的相对置信度来评估模型的信心。与绝对置信度评估相比，相对置信度估计在多个先进的语言模型上表现出更高的可靠性，尤其是在选择性分类任务中。实验结果显示，相对置信度估计在准确性上平均提高了3.5%。'}}}, {'id': 'https://huggingface.co/papers/2501.19393', 'title': 's1: Simple test-time scaling', 'url': 'https://huggingface.co/papers/2501.19393', 'abstract': 'Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI\'s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model\'s thinking process or lengthening it by appending "Wait" multiple times to the model\'s generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.', 'score': 48, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '8fcf84a9effc288f', 'authors': ['Niklas Muennighoff', 'Zitong Yang', 'Weijia Shi', 'Xiang Lisa Li', 'Li Fei-Fei', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer', 'Percy Liang', 'Emmanuel Candès', 'Tatsunori Hashimoto'], 'affiliations': ['Allen Institute for AI', 'Contextual AI', 'Stanford University', 'University of Washington, Seattle'], 'pdf_title_img': 'assets/pdf/title_img/2501.19393.jpg', 'data': {'categories': ['#dataset', '#open_source', '#reasoning', '#training', '#math', '#optimization', '#data'], 'emoji': '🧠', 'ru': {'title': 'Простое масштабирование для улучшения рассуждений языковых моделей', 'desc': 'Статья представляет новый подход к языковому моделированию, называемый тестовым масштабированием. Авторы разработали модель s1, основанную на Qwen2.5-32B-Instruct, и метод бюджетного форсирования для контроля вычислений во время тестирования. Модель обучена на специально отобранном наборе данных s1K из 1000 вопросов с рассуждениями. Результаты показывают, что s1 превосходит модель o1-preview от OpenAI на математических задачах, демонстрируя эффективность предложенного подхода.'}, 'en': {'title': 'Enhancing Language Models with Test-Time Scaling', 'desc': "This paper introduces a method called test-time scaling for enhancing language model performance during evaluation. The authors create a dataset of 1,000 questions with reasoning traces to train their model, focusing on difficulty, diversity, and quality. They implement a technique called budget forcing, which manipulates the model's response time to encourage deeper reasoning and correct errors. After fine-tuning their model, they demonstrate significant improvements in solving math competition questions compared to previous models, showcasing the effectiveness of their approach."}, 'zh': {'title': '测试时间扩展：提升语言模型性能的新方法', 'desc': '本文介绍了一种新的语言建模方法——测试时间扩展，旨在通过增加测试时间计算来提高模型性能。研究者们创建了一个包含1000个问题及其推理过程的小数据集s1K，并通过难度、多样性和质量三个标准进行验证。为了控制测试时间计算，提出了预算强制的方法，通过强制终止模型的思考过程或在模型生成时多次添加“等待”来延长思考时间，从而促使模型检查答案。经过监督微调后，模型s1在数学竞赛问题上超越了OpenAI的o1模型，表现提升达27%。'}}}, {'id': 'https://huggingface.co/papers/2501.19324', 'title': 'Reward-Guided Speculative Decoding for Efficient LLM Reasoning', 'url': 'https://huggingface.co/papers/2501.19324', 'abstract': '', 'score': 26, 'issue_id': 1995, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'ce2d414eedfb7a1e', 'authors': ['Baohao Liao', 'Yuhui Xu', 'Hanze Dong', 'Junnan Li', 'Christof Monz', 'Silvio Savarese', 'Doyen Sahoo', 'Caiming Xiong'], 'affiliations': ['Language Technology Lab, University of Amsterdam', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2501.19324.jpg', 'data': {'categories': [], 'emoji': '🤖', 'ru': {'title': 'Новый шаг в обучении больших языковых моделей', 'desc': 'В статье рассматривается новый подход к обучению LLM, который позволяет моделям лучше понимать контекст и улучшать качество генерации текста. Исследователи предложили метод, который использует более сложные архитектуры нейронных сетей для обработки больших объемов данных. Эксперименты показали, что предложенный метод значительно повышает точность и скорость работы моделей. Это открывает новые возможности для применения AI в различных областях, таких как обработка естественного языка и генерация контента.'}, 'en': {'title': 'Hybrid Networks: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': '优化数据处理，提升机器学习性能', 'desc': '这篇论文探讨了一种新的机器学习算法，旨在提高模型的准确性和效率。作者提出了一种创新的方法，通过优化数据预处理和特征选择来增强学习效果。实验结果表明，该算法在多个数据集上表现优于现有技术。最终，研究表明，改进的数据处理流程对机器学习模型的性能至关重要。'}}}, {'id': 'https://huggingface.co/papers/2501.18119', 'title': 'Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models', 'url': 'https://huggingface.co/papers/2501.18119', 'abstract': 'Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.', 'score': 12, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': 'd751c8a690173842', 'authors': ['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng'], 'affiliations': ['National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2501.18119.jpg', 'data': {'categories': ['#inference', '#graphs', '#transfer_learning', '#training', '#multimodal', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективная интеграция графов знаний и языковых моделей через квантованные коды', 'desc': 'Статья представляет двухэтапный подход к интеграции графов знаний с большими языковыми моделями. Авторы предлагают метод самоконтролируемого квантованного представления (SSQR) для сжатия структурных и семантических знаний графа в дискретные коды. Эти коды затем используются для создания инструкций для обучения языковых моделей. Эксперименты показывают превосходство SSQR над существующими методами и улучшение производительности моделей LLaMA2 и LLaMA3.1 на задачах, связанных с графами знаний.'}, 'en': {'title': 'Seamless Integration of Knowledge Graphs and Language Models', 'desc': 'This paper addresses the challenge of integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) by proposing a two-stage framework. The framework utilizes a self-supervised quantized representation (SSQR) method to convert KG structural and semantic information into discrete codes that resemble language tokens. By treating these codes as features for LLMs, the approach allows for a more efficient and effective integration of KGs with LLMs. Experimental results show that SSQR outperforms traditional methods, enabling better performance in tasks like KG link prediction and triple classification with significantly fewer tokens.'}, 'zh': {'title': '无缝整合知识图谱与大型语言模型', 'desc': '本论文探讨了知识图谱（KG）结构与自然语言之间的差距，提出了一种两阶段框架，以实现KG与大型语言模型（LLM）的有效整合。首先，提出了一种自监督量化表示（SSQR）方法，将KG的结构和语义知识压缩为离散代码（即令牌），使其与语言句子的格式对齐。接着，设计了KG指令跟随数据，将这些学习到的代码视为特征，直接输入到LLM中，从而实现无缝整合。实验结果表明，SSQR在无监督量化方法中表现优越，生成的代码更具可区分性，且经过微调的LLaMA2和LLaMA3.1在KG链接预测和三元组分类任务中表现出色，仅使用每个实体16个令牌，而不是传统方法中的数千个。'}}}, {'id': 'https://huggingface.co/papers/2501.19339', 'title': 'PixelWorld: Towards Perceiving Everything as Pixels', 'url': 'https://huggingface.co/papers/2501.19339', 'abstract': 'Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models\' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models\' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance.', 'score': 9, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '3e10b792328f7a4b', 'authors': ['Zhiheng Lyu', 'Xueguang Ma', 'Wenhu Chen'], 'affiliations': ['Department of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2501.19339.jpg', 'data': {'categories': ['#agi', '#benchmark', '#optimization', '#dataset', '#open_source', '#reasoning', '#multimodal', '#interpretability', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Единый пиксельный взгляд на мир: новая парадигма для ИИ', 'desc': 'В статье предлагается новый подход к обработке различных модальностей данных (текст, изображения, код и т.д.) в виде пикселей, названный PEAP (Perceive Everything as Pixels). Авторы представляют набор данных PixelWorld для оценки эффективности моделей в этом unified подходе. Результаты показывают, что PEAP превосходит базовые модели на мультимодальных данных, но выявляет снижение производительности в задачах рассуждения и кодирования. Исследование демонстрирует потенциал и ограничения восприятия пиксельных данных современными языковыми моделями.'}, 'en': {'title': 'Unifying Perception: Everything as Pixels', 'desc': "This paper introduces a new approach called 'Perceive Everything as Pixels' (PEAP), which aims to unify various input modalities like text, images, and diagrams into a single pixel-based format. The authors present PixelWorld, a novel evaluation suite designed to assess the performance of existing models when using this unified pixel input. Their experiments reveal that PEAP outperforms traditional token-based methods in multimodal datasets, although it highlights a decline in reasoning and coding abilities across models when using pixel inputs. The study concludes that while current models excel in pixel perception, there is still significant potential for enhancing their overall perceptual capabilities."}, 'zh': {'title': '统一感知：将一切视为像素', 'desc': '本论文提出了一种新的统一感知框架，称为“将一切视为像素”（PEAP），旨在将文本、表格、代码、图表和图像等多种输入形式统一为像素输入。我们引入了PixelWorld评估套件，以在像素空间中评估现有模型的性能。研究发现，PEAP在多模态数据集上优于基于标记的输入，显示出统一输入在消歧义方面的优势。同时，处理像素输入时，所有模型的推理和编码能力显著下降，表明需要增强基础模型的感知能力。'}}}, {'id': 'https://huggingface.co/papers/2501.14677', 'title': 'MatAnyone: Stable Video Matting with Consistent Memory Propagation', 'url': 'https://huggingface.co/papers/2501.14677', 'abstract': 'Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively integrates memory from the previous frame. This ensures semantic stability in core regions while preserving fine-grained details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, boosting matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust and accurate video matting results in diverse real-world scenarios, outperforming existing methods.', 'score': 6, 'issue_id': 2010, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 января', 'en': 'January 24', 'zh': '1月24日'}, 'hash': 'a9968478421ddc33', 'authors': ['Peiqing Yang', 'Shangchen Zhou', 'Jixin Zhao', 'Qingyi Tao', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2501.14677.jpg', 'data': {'categories': ['#training', '#dataset', '#video'], 'emoji': '✂️', 'ru': {'title': 'MatAnyone: Точное выделение объектов на видео с помощью памяти и адаптивного обучения', 'desc': 'MatAnyone - это новый подход к выделению объектов на видео без вспомогательных данных. Он использует модуль памяти для адаптивного объединения информации из предыдущих кадров, обеспечивая стабильность основных областей и сохраняя детали на границах объектов. Авторы также создали новый большой набор данных для обучения и разработали стратегию, использующую данные сегментации для повышения стабильности. MatAnyone превосходит существующие методы в различных сценариях реального мира.'}, 'en': {'title': 'MatAnyone: Robust Video Matting with Memory Propagation', 'desc': 'The paper introduces MatAnyone, a new framework for video matting that does not require auxiliary inputs. It utilizes a memory-based approach with a memory propagation module that adapts memory from previous frames to maintain semantic consistency and detail. The authors also present a larger and more diverse dataset for training, along with a novel strategy that uses extensive segmentation data to enhance matting stability. Overall, MatAnyone achieves superior performance in complex video environments compared to existing methods.'}, 'zh': {'title': 'MatAnyone：视频抠图的新突破', 'desc': '本论文提出了一种名为MatAnyone的视频抠图框架，旨在解决复杂背景下的抠图问题。该方法基于记忆传播模块，通过区域自适应记忆融合，动态整合前一帧的记忆信息，从而确保核心区域的语义稳定性。为了提高训练的鲁棒性，研究团队构建了一个更大、更高质量且多样化的视频抠图数据集，并采用了一种新颖的训练策略，充分利用大规模分割数据。最终，MatAnyone在多种真实场景中展现出优越的抠图效果，超越了现有的方法。'}}}, {'id': 'https://huggingface.co/papers/2501.19399', 'title': 'Scalable-Softmax Is Superior for Attention', 'url': 'https://huggingface.co/papers/2501.19399', 'abstract': "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.", 'score': 6, 'issue_id': 2009, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '12ed1cad789702aa', 'authors': ['Ken M. Nakanishi'], 'affiliations': ['Institute for Physics of Intelligence, The University of Tokyo, Tokyo 113-0033, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2501.19399.jpg', 'data': {'categories': ['#architecture', '#training', '#long_context', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'SSMax: улучшение внимания трансформеров для длинных текстов', 'desc': 'Статья представляет новую функцию Scalable-Softmax (SSMax) для улучшения работы трансформеров с длинными контекстами. SSMax решает проблему уплощения распределения внимания при увеличении размера входного вектора. Эксперименты показывают, что модели с SSMax быстрее обучаются и лучше работают с длинными текстами. SSMax позволяет модели фокусироваться на ключевой информации даже в длинных контекстах.'}, 'en': {'title': 'Enhancing Attention with Scalable-Softmax for Better Context Handling', 'desc': "This paper addresses a limitation in Transformer-based language models where the Softmax function causes attention scores to flatten as the input size increases. This flattening reduces the model's ability to focus on important information, especially in longer contexts. The authors propose a new method called Scalable-Softmax (SSMax) that replaces the traditional Softmax function, allowing for better attention distribution and improved performance in long contexts. Experimental results show that SSMax enhances loss reduction during pretraining and enables better retrieval of key information, even for models that have already begun pretraining."}, 'zh': {'title': '可扩展Softmax：提升Transformer模型的注意力能力', 'desc': '本文提出了一种新的方法，称为可扩展Softmax（SSMax），旨在解决Transformer模型在处理长上下文时的注意力分布扁平化问题。传统的Softmax函数在输入向量增大时，最大元素趋近于零，导致模型无法有效地优先考虑关键信息。SSMax可以无缝集成到现有的Transformer架构中，实验结果表明，使用SSMax的模型在语言建模中不仅在预训练期间实现了更快的损失减少，还显著提高了在长上下文中的性能。通过分析注意力分数，SSMax使模型能够在长上下文中更好地关注关键信息，提升了模型的长度泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.04983', 'title': 'DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning', 'url': 'https://huggingface.co/papers/2411.04983', 'abstract': 'The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.', 'score': 6, 'issue_id': 1999, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'e72081596b626524', 'authors': ['Gaoyue Zhou', 'Hengkai Pan', 'Yann LeCun', 'Lerrel Pinto'], 'affiliations': ['Courant Institute, New York University', 'Meta-FAIR'], 'pdf_title_img': 'assets/pdf/title_img/2411.04983.jpg', 'data': {'categories': ['#cv', '#agents', '#reasoning', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DINO-WM: универсальная модель мира для планирования поведения без реконструкции', 'desc': 'Статья представляет новый метод моделирования визуальной динамики без реконструкции визуального мира - DINO World Model (DINO-WM). DINO-WM использует пространственные признаки патчей, предварительно обученные с помощью DINOv2, что позволяет ему учиться на офлайн-траекториях поведения, предсказывая будущие признаки патчей. Этот подход позволяет DINO-WM достигать наблюдаемых целей путем оптимизации последовательности действий, облегчая планирование поведения, независимое от задачи. Эксперименты показывают, что DINO-WM может генерировать поведенческие решения с нуля во время тестирования, демонстрируя сильные способности к обобщению по сравнению с предыдущими современными методами.'}, 'en': {'title': 'DINO-WM: Predicting the Future with Passive Data for Task-Agnostic Planning', 'desc': 'This paper introduces DINO World Model (DINO-WM), a novel approach for creating predictive models that can reason and plan across various tasks using only passive data. DINO-WM is designed to learn from offline trajectories without needing to reconstruct the visual environment, focusing instead on predicting future visual features. The model is capable of optimizing behavior at test time and supports task-agnostic reasoning, making it versatile for different applications. Experimental results show that DINO-WM can effectively generate solutions in diverse scenarios without relying on expert demonstrations or pre-learned models, showcasing its strong generalization abilities.'}, 'zh': {'title': 'DINO-WM：无任务依赖的世界模型', 'desc': '本文提出了一种新的世界模型DINO-WM，旨在通过被动数据进行推理和规划。DINO-WM具有三个关键特性：可以在离线收集的轨迹上进行训练，支持测试时行为优化，并促进任务无关的推理。该模型利用DINOv2预训练的空间补丁特征，通过预测未来的补丁特征来学习，从而实现观察目标的行为规划。实验结果表明，DINO-WM在多种任务中表现出色，能够在没有专家示范和奖励建模的情况下生成零-shot行为解决方案。'}}}, {'id': 'https://huggingface.co/papers/2501.18837', 'title': 'Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming', 'url': 'https://huggingface.co/papers/2501.18837', 'abstract': 'Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable.', 'score': 4, 'issue_id': 1996, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '62d14973b1140e58', 'authors': ['Mrinank Sharma', 'Meg Tong', 'Jesse Mu', 'Jerry Wei', 'Jorrit Kruthoff', 'Scott Goodfriend', 'Euan Ong', 'Alwin Peng', 'Raj Agarwal', 'Cem Anil', 'Amanda Askell', 'Nathan Bailey', 'Joe Benton', 'Emma Bluemke', 'Samuel R. Bowman', 'Eric Christiansen', 'Hoagy Cunningham', 'Andy Dau', 'Anjali Gopal', 'Rob Gilson', 'Logan Graham', 'Logan Howard', 'Nimit Kalra', 'Taesung Lee', 'Kevin Lin', 'Peter Lofgren', 'Francesco Mosconi', "Clare O'Hara", 'Catherine Olsson', 'Linda Petrini', 'Samir Rajani', 'Nikhil Saxena', 'Alex Silverstein', 'Tanya Singh', 'Theodore Sumers', 'Leonard Tang', 'Kevin K. Troy', 'Constantin Weisser', 'Ruiqi Zhong', 'Giulio Zhou', 'Jan Leike', 'Jared Kaplan', 'Ethan Perez'], 'affiliations': ['Safeguards Research Team, Anthropic'], 'pdf_title_img': 'assets/pdf/title_img/2501.18837.jpg', 'data': {'categories': ['#synthetic', '#training', '#architecture', '#dataset', '#security', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Конституционные Классификаторы: надежная защита языковых моделей', 'desc': 'Исследование представляет концепцию Конституционных Классификаторов - защитных механизмов для больших языковых моделей (LLM), обученных на синтетических данных с использованием правил, определяющих допустимый контент. Эти классификаторы эффективно противостоят универсальным методам обхода защиты, не позволяя извлекать вредоносную информацию из защищенных моделей. Эксперименты показали устойчивость классификаторов к различным атакам и их практическую применимость с минимальным влиянием на производительность. Исследование демонстрирует возможность эффективной защиты LLM от универсальных методов обхода при сохранении практической применимости.'}, 'en': {'title': 'Defending LLMs with Constitutional Classifiers', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to universal jailbreaks, which are strategies that allow users to bypass safety measures. The authors propose a solution called Constitutional Classifiers, which are trained on synthetic data generated by LLMs using natural language rules that define what content is allowed or restricted. Through extensive testing, these classifiers showed strong resistance to jailbreak attempts, effectively protecting the model without significantly impacting its performance. The findings suggest that it is possible to defend against such attacks while still ensuring the model can be used effectively in real-world applications.'}, 'zh': {'title': '宪法分类器：保护大型语言模型的安全', 'desc': '大型语言模型（LLMs）容易受到普遍越狱攻击，这种攻击可以绕过模型的安全措施，允许用户进行有害操作。为此，我们提出了宪法分类器，这是一种基于合成数据训练的安全措施，合成数据是通过自然语言规则（即宪法）提示LLMs生成的，规定了允许和限制的内容。在超过3000小时的红队测试中，没有红队成员找到能够从早期分类器保护的LLM中提取信息的普遍越狱方法。我们的研究表明，在保持实际部署可行性的同时，防御普遍越狱攻击是可行的。'}}}, {'id': 'https://huggingface.co/papers/2501.18841', 'title': 'Trading Inference-Time Compute for Adversarial Robustness', 'url': 'https://huggingface.co/papers/2501.18841', 'abstract': 'We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.', 'score': 3, 'issue_id': 1994, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'f1e75e6b24f3e044', 'authors': ['Wojciech Zaremba', 'Evgenia Nitishinskaya', 'Boaz Barak', 'Stephanie Lin', 'Sam Toyer', 'Yaodong Yu', 'Rachel Dias', 'Eric Wallace', 'Kai Xiao', 'Johannes Heidecke', 'Amelia Glaese'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2501.18841.jpg', 'data': {'categories': ['#security', '#reasoning', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Больше вычислений - выше защита: повышение устойчивости ИИ к атакам', 'desc': 'Исследование посвящено влиянию увеличения вычислительных ресурсов во время вывода на устойчивость моделей рассуждений к состязательным атакам. Эксперименты показали, что увеличение вычислений при выводе улучшает робастность моделей к различным атакам. В большинстве случаев доля успешных атак стремится к нулю при росте вычислительных ресурсов. Результаты указывают на потенциал увеличения вычислений при выводе для повышения устойчивости больших языковых моделей к состязательным атакам.'}, 'en': {'title': 'Boosting Robustness: More Compute, Less Vulnerability', 'desc': "This paper investigates how increasing the amount of compute used during inference can enhance the robustness of reasoning models, specifically OpenAI's o1-preview and o1-mini, against adversarial attacks. The authors find that as the compute increases, the success rate of these attacks generally decreases, indicating improved model resilience. Notably, this improvement occurs without any adversarial training, simply by allowing the models to utilize more resources for reasoning tasks. The study also examines new types of attacks and scenarios where increased compute does not lead to better reliability, providing insights into potential solutions."}, 'zh': {'title': '增加推理计算，提升模型鲁棒性', 'desc': '本研究探讨了在推理模型中增加推理时间计算对其抵御对抗攻击的影响。我们发现，随着推理时间计算的增加，模型的鲁棒性得到了提升。大多数情况下，攻击成功的模型样本比例随着测试时间计算的增加而趋近于零。我们的结果表明，推理时间计算有潜力提高大型语言模型的对抗鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2501.18052', 'title': 'SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2501.18052', 'abstract': "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https://github.com/cywinski/SAeUron.", 'score': 2, 'issue_id': 2011, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 января', 'en': 'January 29', 'zh': '1月29日'}, 'hash': 'd94056a77d806ada', 'authors': ['Bartosz Cywiński', 'Kamil Deja'], 'affiliations': ['IDEAS NCBR', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2501.18052.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#security', '#architecture', '#ethics', '#training', '#benchmark'], 'emoji': '🧹', 'ru': {'title': 'Чистка нейросетей: SAeUron удаляет нежелательные концепции из диффузионных моделей', 'desc': 'SAeUron - это новый метод удаления нежелательных концепций в диффузионных моделях для генерации изображений по тексту. Он использует разреженные автоэнкодеры (SAE) для выделения интерпретируемых признаков, соответствующих конкретным концепциям. Метод позволяет точно вмешиваться в активации модели для блокировки целевого контента при сохранении общей производительности. SAeUron показывает лучшие результаты по сравнению с другими методами и может удалять несколько концепций одновременно.'}, 'en': {'title': 'SAeUron: Safeguarding Diffusion Models with Sparse Autoencoders', 'desc': "This paper presents SAeUron, a new method designed to improve the safety of text-to-image diffusion models by removing unwanted concepts. It utilizes sparse autoencoders (SAEs) to learn and identify specific features from the model's activations, allowing for targeted interventions. The method enables precise control over the model's outputs while maintaining its overall performance. Evaluation shows that SAeUron outperforms existing techniques in unlearning tasks and effectively reduces the risk of generating harmful content."}, 'zh': {'title': 'SAeUron：去除不良内容的新方法', 'desc': '扩散模型虽然强大，但可能会生成有害或不良内容，带来伦理和安全问题。最近的机器遗忘方法提供了潜在的解决方案，但通常缺乏透明性，难以理解对基础模型的更改。我们提出了一种新方法SAeUron，利用稀疏自编码器（SAE）学习的特征来去除文本到图像扩散模型中的不必要概念。通过在多个去噪时间步的激活上无监督训练SAE，我们捕捉到与特定概念对应的稀疏和可解释特征，从而实现精确干预。'}}}, {'id': 'https://huggingface.co/papers/2501.18804', 'title': 'Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion', 'url': 'https://huggingface.co/papers/2501.18804', 'abstract': 'Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.', 'score': 2, 'issue_id': 2010, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '32db517ad974401b', 'authors': ['Vitor Guizilini', 'Muhammad Zubair Irshad', 'Dian Chen', 'Greg Shakhnarovich', 'Rares Ambrus'], 'affiliations': ['Toyota Research Institute (TRI)', 'Toyota Technological Institute at Chicago (TTIC)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18804.jpg', 'data': {'categories': ['#training', '#3d', '#diffusion', '#architecture', '#benchmark', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'Генерация 3D сцен из разных ракурсов с помощью диффузионной модели', 'desc': 'Статья представляет MVGD - архитектуру на основе диффузии для генерации изображений и карт глубины с новых ракурсов. Метод использует райкарты для обогащения визуальных признаков пространственной информацией и направления генерации. Ключевой аспект - многозадачная генерация изображений и карт глубины с использованием обучаемых эмбеддингов задач. Модель обучена на более чем 60 миллионах мультиракурсных примеров и показывает современные результаты в задачах синтеза новых ракурсов и оценки глубины.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with Direct Pixel-Level Generation', 'desc': 'This paper presents MVGD, a new diffusion-based model for generating images and depth maps from multiple input views in 3D scene reconstruction. Unlike traditional methods that rely on intermediate 3D representations, MVGD directly produces pixel-level outputs, enhancing visual features with spatial information through raymap conditioning. The model employs multi-task learning, using task embeddings to effectively guide the generation process for both images and depth maps. Trained on a vast dataset of over 60 million samples, MVGD achieves state-of-the-art performance in novel view synthesis and depth estimation tasks.'}, 'zh': {'title': 'MVGD：从多视角直接生成图像与深度图的创新方法', 'desc': '本文提出了一种名为MVGD的扩散基础架构，能够直接从多个视角生成图像和深度图。该方法通过光线图条件化，增强了视觉特征并引导生成过程。我们采用多任务生成技术，同时生成图像和深度图，并使用可学习的任务嵌入来优化扩散过程。经过在超过6000万多视角样本上的训练，我们在多个基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2501.18965', 'title': 'The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training', 'url': 'https://huggingface.co/papers/2501.18965', 'abstract': 'We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms. Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules.', 'score': 2, 'issue_id': 2007, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'a136293a2241150e', 'authors': ['Fabian Schaipp', 'Alexander Hägele', 'Adrien Taylor', 'Umut Simsekli', 'Francis Bach'], 'affiliations': ['EPFL, Lausanne, Switzerland', 'Inria, Departement dInformatique de lEcole Normale Superieure, PSL Research University, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2501.18965.jpg', 'data': {'categories': ['#transfer_learning', '#math', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Оптимизация графиков обучения для больших языковых моделей', 'desc': 'В статье исследуются графики изменения скорости обучения для больших моделей машинного обучения. Авторы обнаружили неожиданное сходство этих графиков с теоретическими границами из теории невыпуклой оптимизации. Они предлагают новый метод настройки скорости обучения, основанный на этом наблюдении. Применение метода позволило улучшить результаты обучения языковых моделей типа Llama размером 124M и 210M параметров.'}, 'en': {'title': 'Optimizing Learning Rates: Bridging Theory and Practice', 'desc': 'This paper explores the relationship between learning-rate schedules in large model training and concepts from non-smooth convex optimization theory. It establishes a performance bound for a constant learning-rate schedule with a linear cooldown, highlighting the practical advantages of this approach. The authors demonstrate that the alignment between theoretical optimization and practical training can be leveraged for better learning-rate tuning. By optimizing the learning-rate schedule, they achieve significant improvements in training large Llama-type models.'}, 'zh': {'title': '优化学习率调度，提升大模型训练效果', 'desc': '本文探讨了大模型训练中的学习率调度与非光滑凸优化理论中的性能界限之间的相似性。我们提供了一个线性冷却的常数调度的界限，特别是冷却的实际好处在于没有对数项的影响。进一步地，我们展示了优化理论与实践之间的紧密联系可以用于学习率调优：通过延长调度以继续训练并使用最佳学习率，我们在训练124M和210M的Llama类型模型时取得了显著的改进。最后，我们还展示了在不同调度之间转移最佳学习率的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.18753', 'title': 'INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation', 'url': 'https://huggingface.co/papers/2501.18753', 'abstract': 'Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability.', 'score': 2, 'issue_id': 2002, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '000663cf445862a1', 'authors': ['Jian Hu', 'Zixu Cheng', 'Shaogang Gong'], 'affiliations': ['Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2501.18753.jpg', 'data': {'categories': ['#dataset', '#healthcare', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Адаптивная сегментация изображений с помощью интеллектуального отбора промптов', 'desc': 'Статья представляет новый метод сегментации изображений под названием INT (Instance-specific Negative Mining for Task-Generic Promptable Segmentation). Этот подход направлен на улучшение генерации промптов, специфичных для конкретных экземпляров изображений, путем адаптивного уменьшения влияния нерелевантных знаний и усиления наиболее вероятных. INT состоит из двух компонентов: генерации промптов, специфичных для экземпляров, и генерации семантической маски. Метод был проверен на шести наборах данных, включая камуфлированные объекты и медицинские изображения, демонстрируя эффективность, надежность и масштабируемость.'}, 'en': {'title': 'Enhancing Image Segmentation with Smart Prompting', 'desc': 'This paper presents a new method called Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT) to improve image segmentation using a single task description. The method addresses the challenge of Vision-Language Models (VLMs) struggling to generalize to certain image instances, which can lead to poor segmentation results. INT works by selectively reducing the impact of irrelevant information while enhancing the use of relevant prior knowledge through a process called negative mining. The effectiveness of INT is validated across six diverse datasets, showing its ability to produce accurate and robust segmentation results.'}, 'zh': {'title': '实例特定负采样优化图像分割', 'desc': '这篇论文提出了一种新的方法，称为实例特定负采样（INT），用于任务通用的可提示图像分割。INT的核心思想是自适应地减少无关的先验知识的影响，同时增加最有可能的先验知识的使用，以优化实例特定提示的生成。该方法包括两个主要部分：实例特定提示生成和语义掩码生成，确保每个图像实例的分割与提示的语义相匹配。通过在六个数据集上的验证，INT展示了其有效性、鲁棒性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2501.18128', 'title': 'Unraveling the Capabilities of Language Models in News Summarization', 'url': 'https://huggingface.co/papers/2501.18128', 'abstract': "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.", 'score': 2, 'issue_id': 2000, 'pub_date': '2025-01-30', 'pub_date_card': {'ru': '30 января', 'en': 'January 30', 'zh': '1月30日'}, 'hash': '1c3f3a16953a5a59', 'authors': ['Abdurrahman Odabaşı', 'Göksel Biricik'], 'affiliations': ['Department of Computer Engineering, Turkish-German University, 34820, Istanbul, Turkiye', 'Department of Computer Engineering, Yıldız Technical University, 34220, Istanbul, Turkiye'], 'pdf_title_img': 'assets/pdf/title_img/2501.18128.jpg', 'data': {'categories': ['#survey', '#multilingual', '#small_models', '#benchmark', '#transfer_learning'], 'emoji': '📰', 'ru': {'title': 'Маленькие модели бросают вызов гигантам в суммаризации новостей', 'desc': 'В этой работе проводится комплексное сравнение 20 современных языковых моделей, с акцентом на меньшие модели, для задачи суммаризации новостей. Исследование охватывает обучение в режимах zero-shot и few-shot на трех различных наборах данных, используя автоматические метрики, человеческую оценку и LLM в качестве судьи. Интересно, что включение демонстрационных примеров в режиме few-shot не улучшило производительность моделей, а в некоторых случаях даже ухудшило качество генерируемых сводок. Результаты показали превосходство GPT-3.5-Turbo и GPT-4, но также выявили перспективные открытые модели, такие как Qwen1.5-7B и SOLAR-10.7B-Instruct-v1.0.'}, 'en': {'title': 'Benchmarking News Summarization: Small Models Can Compete!', 'desc': 'This paper benchmarks 20 recent language models specifically for the task of news summarization, emphasizing smaller models. It evaluates their performance in zero-shot and few-shot learning scenarios across three different datasets with varying writing styles. The study reveals that providing demonstration examples in few-shot settings often does not improve, and can even degrade, the quality of summaries due to the inadequacy of reference summaries. Notably, while larger models like GPT-3.5-Turbo and GPT-4 excel, several smaller models also show competitive performance, suggesting they could be viable alternatives for summarization tasks.'}, 'zh': {'title': '小模型在新闻摘要中的潜力与挑战', 'desc': '本研究对20种最新的语言模型进行了全面的基准测试，重点关注较小的模型在新闻摘要任务中的表现。我们系统地测试了这些模型在不同风格的新闻文章摘要中的能力和有效性，使用了三种不同的数据集。研究发现，在少量示例学习的设置中，提供示例并未提升模型的性能，反而在某些情况下导致生成摘要的质量下降。这主要是由于参考摘要的质量较差，影响了模型的表现，同时我们的研究结果显示，GPT-3.5-Turbo和GPT-4在性能上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2404.07097', 'title': 'Fast Encoder-Based 3D from Casual Videos via Point Track Processing', 'url': 'https://huggingface.co/papers/2404.07097', 'abstract': 'This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.   Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation. TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\\%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time.', 'score': 1, 'issue_id': 1999, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'a526ae197fe3a8c7', 'authors': ['Yoni Kasten', 'Wuyue Lu', 'Haggai Maron'], 'affiliations': ['NVIDIA Research', 'Simon Fraser University', 'Technion'], 'pdf_title_img': 'assets/pdf/title_img/2404.07097.jpg', 'data': {'categories': ['#3d', '#training', '#cv', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Эффективная 3D реконструкция из видео с помощью глубокого обучения', 'desc': 'Статья представляет TracksTo4D - новый подход к реконструкции 3D структур из видео с динамическим содержанием. Метод использует нейронную сеть, обученную без учителя на 2D треках точек, извлеченных из обычных видео. TracksTo4D позволяет восстанавливать облако точек и положение камеры за один проход сети, значительно сокращая время вычислений по сравнению с существующими методами. Архитектура сети учитывает симметрии входных данных и предполагает низкоранговое представление паттернов движения.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction from Casual Videos', 'desc': 'This paper introduces TracksTo4D, a novel machine learning approach for reconstructing 3D structures from casual videos. Unlike traditional methods that require extensive optimization or are limited to specific video types, TracksTo4D processes 2D point tracks directly in a single, efficient feed-forward pass. The architecture is designed to leverage the symmetries in point track data and utilizes low-rank approximations to represent movement patterns effectively. Trained unsupervised on a dataset of casual videos, TracksTo4D achieves high accuracy in reconstructing temporal point clouds and camera positions while significantly reducing processing time.'}, 'zh': {'title': '高效重建3D结构，TracksTo4D引领新潮流', 'desc': '本文解决了从动态内容视频重建3D结构的长期挑战。现有方法无法处理普通相机录制的随意视频，或需要较长的优化时间。我们提出了一种基于学习的方法TracksTo4D，通过单次高效的前馈传递，从随意视频中推断3D结构和相机位置。该方法直接处理2D点轨迹，并设计了专门的架构，能够在无监督的情况下进行训练，显著提高了重建效率。'}}}, {'id': 'https://huggingface.co/papers/2502.05173', 'title': 'VideoRoPE: What Makes for Good Video Rotary Position Embedding?', 'url': 'https://huggingface.co/papers/2502.05173', 'abstract': 'While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.', 'score': 48, 'issue_id': 2118, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ba284ed1a62b3c2c', 'authors': ['Xilin Wei', 'Xiaoran Liu', 'Yuhang Zang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Cao', 'Jian Tong', 'Haodong Duan', 'Qipeng Guo', 'Jiaqi Wang', 'Xipeng Qiu', 'Dahua Lin'], 'affiliations': ['Fudan University, Shanghai, China', 'Shanghai AI Laboratory, Shanghai, China', 'Shanghai Innovation Institute, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.05173.jpg', 'data': {'categories': ['#hallucinations', '#3d', '#architecture', '#video', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'VideoRoPE: Эффективное позиционное кодирование для глубокого обучения на видео', 'desc': 'Статья представляет VideoRoPE - новый метод позиционного кодирования для видео, основанный на Rotary Position Embedding. Авторы провели анализ и выявили 4 ключевые характеристики для эффективной адаптации RoPE к видео. Они предложили сложную задачу V-NIAH-D для демонстрации недостатков существующих вариантов RoPE. VideoRoPE имеет 3D-структуру, сохраняющую пространственно-временные отношения, и превосходит предыдущие варианты RoPE в различных задачах обработки видео.'}, 'en': {'title': 'Enhancing Video Understanding with VideoRoPE', 'desc': 'This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data.'}, 'zh': {'title': 'VideoRoPE：视频中的旋转位置嵌入新突破', 'desc': '本文探讨了如何将旋转位置嵌入（RoPE）有效地扩展到视频数据中。研究分析了四个关键特性，这些特性对于RoPE在视频中的适应性至关重要。我们提出了一个新的任务V-NIAH-D，展示了现有RoPE变体在处理视频时容易受到干扰的缺陷。基于这些分析，我们提出了VideoRoPE，它通过3D结构来保持时空关系，并在多个下游任务中表现优于之前的RoPE变体。'}}}, {'id': 'https://huggingface.co/papers/2502.04507', 'title': 'Fast Video Generation with Sliding Tile Attention', 'url': 'https://huggingface.co/papers/2502.04507', 'abstract': 'Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.', 'score': 38, 'issue_id': 2120, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'dcbf1070dac1b391', 'authors': ['Peiyuan Zhang', 'Yongqi Chen', 'Runlong Su', 'Hangliang Ding', 'Ion Stoica', 'Zhenghong Liu', 'Hao Zhang'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego', 'University of Michigan, Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2502.04507.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение генерации видео с помощью скользящего плиточного внимания', 'desc': 'Статья представляет метод скользящего плиточного внимания (STA) для ускорения генерации видео с помощью диффузионных трансформеров. STA использует наблюдение, что оценки внимания в предобученных моделях диффузии видео в основном концентрируются в локализованных 3D-окнах. Этот подход устраняет избыточность полного внимания, сохраняя выразительность и эффективность на аппаратном уровне. STA ускоряет внимание в 2.8-17 раз по сравнению с FlashAttention-2 и в 1.6-10 раз по сравнению с FlashAttention-3, значительно сокращая время генерации видео.'}, 'en': {'title': 'Efficient Video Generation with Sliding Tile Attention', 'desc': 'This paper presents a new method called sliding tile attention (STA) to improve the efficiency of video generation using Diffusion Transformers (DiTs). Traditional full attention mechanisms are computationally expensive, especially for generating high-resolution videos, leading to long inference times. STA reduces this cost by focusing on localized 3D windows, allowing for faster processing without sacrificing the quality of the generated videos. The implementation of STA achieves significant speedups in attention computation, making it a promising solution for real-time video generation tasks.'}, 'zh': {'title': '滑动瓦片注意力：高效视频生成的新突破', 'desc': '本论文介绍了一种新的滑动瓦片注意力机制（STA），旨在提高视频生成的效率。传统的扩散变换器在生成视频时计算成本高，而STA通过关注局部的时空区域来减少冗余计算。与传统的滑动窗口注意力不同，STA采用了硬件友好的设计，逐块处理，保持了表达能力的同时提高了计算效率。经过优化，STA在视频生成任务中显著加速了注意力计算，降低了延迟，同时不影响生成质量。'}}}, {'id': 'https://huggingface.co/papers/2502.04896', 'title': 'Goku: Flow Based Video Generative Foundation Models', 'url': 'https://huggingface.co/papers/2502.04896', 'abstract': 'This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.', 'score': 30, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ad6ef6eed233cc90', 'authors': ['Shoufa Chen', 'Chongjian Ge', 'Yuqi Zhang', 'Yida Zhang', 'Fengda Zhu', 'Hao Yang', 'Hongxiang Hao', 'Hui Wu', 'Zhichao Lai', 'Yifei Hu', 'Ting-Che Lin', 'Shilong Zhang', 'Fu Li', 'Chuan Li', 'Xing Wang', 'Yanghua Peng', 'Peize Sun', 'Ping Luo', 'Yi Jiang', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaobing Liu'], 'affiliations': ['Bytedance Inc', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04896.jpg', 'data': {'categories': ['#cv', '#training', '#video', '#architecture', '#data', '#benchmark', '#dataset'], 'emoji': '🐉', 'ru': {'title': 'Goku: Новый уровень генерации изображений и видео', 'desc': 'Статья представляет семейство моделей Goku для совместной генерации изображений и видео. Модели используют трансформеры с выпрямленным потоком для достижения передовых результатов. Авторы описывают ключевые элементы, включая подготовку данных, архитектуру модели и инфраструктуру для эффективного обучения. Goku демонстрирует превосходную производительность в качественных и количественных оценках, устанавливая новые стандарты в основных задачах генерации изображений и видео.'}, 'en': {'title': 'Goku: Revolutionizing Image and Video Generation with Transformers', 'desc': 'This paper presents Goku, a cutting-edge model for generating images and videos using rectified flow Transformers. It discusses key components that contribute to its high-quality output, such as the data curation process and the design of the model architecture. Goku sets new performance records in various tasks, achieving impressive scores on benchmarks for both text-to-image and text-to-video generation. The authors aim to offer insights and advancements that will benefit researchers working on similar generation models.'}, 'zh': {'title': 'Goku：图像与视频生成的新标杆', 'desc': '本文介绍了Goku，这是一种先进的联合图像和视频生成模型，利用了修正流Transformer以实现行业领先的性能。我们详细阐述了高质量视觉生成的基础要素，包括数据整理流程、模型架构设计、流的公式化以及高效稳健的大规模训练基础设施。Goku模型在定性和定量评估中表现优越，为主要任务设定了新的基准。具体而言，Goku在文本到图像生成中达到了0.76的GenEval和83.65的DPG-Bench，在文本到视频任务中达到了84.85的VBench。'}}}, {'id': 'https://huggingface.co/papers/2502.05003', 'title': 'QuEST: Stable Training of LLMs with 1-Bit Weights and Activations', 'url': 'https://huggingface.co/papers/2502.05003', 'abstract': 'One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the "optimal" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations.   We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the "true" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.', 'score': 28, 'issue_id': 2122, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c011c3548ad7a5dd', 'authors': ['Andrei Panferov', 'Jiale Chen', 'Soroush Tabesh', 'Roberto L. Castro', 'Mahdi Nikdan', 'Dan Alistarh'], 'affiliations': ['ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05003.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное квантование для обучения языковых моделей', 'desc': 'Статья представляет новый метод QuEST для обучения больших языковых моделей с использованием квантования. QuEST позволяет обучать модели с весами и активациями в 4 бита или меньше, сохраняя конкурентоспособную точность по сравнению с FP16. Метод улучшает квантование распределений весов и активаций, а также вводит новый оценщик градиента доверия. Эксперименты показывают, что QuEST обеспечивает стабильные законы масштабирования для различных уровней точности.'}, 'en': {'title': 'QuEST: Revolutionizing Quantization for Efficient Language Models', 'desc': 'This paper introduces QuEST, a novel method for training large language models (LLMs) using quantization-aware training (QAT) with significantly reduced bit-widths. QuEST achieves competitive accuracy with traditional FP16 precision while utilizing weights and activations as low as 4 bits, and even supports stable training with 1-bit representations. The method enhances QAT by employing Hadamard normalization for better quantization and a new trust gradient estimator to minimize errors in gradient calculations. Experiments demonstrate that QuEST maintains stable performance across various hardware-supported precisions and can be efficiently executed with provided GPU kernel support.'}, 'zh': {'title': 'QuEST：低精度高效训练大语言模型的创新方法', 'desc': '本文提出了一种名为QuEST的新方法，旨在通过量化感知训练（QAT）来提高大语言模型的训练效率。QuEST能够在4位或更低的精度下训练模型，同时保持与FP16精度相当的准确性。该方法通过改进权重和激活的量化过程，以及引入新的信任梯度估计器，来实现更稳定的训练。实验结果表明，QuEST在各种硬件支持的精度范围内都能实现稳定的扩展性，并且可以有效执行。'}}}, {'id': 'https://huggingface.co/papers/2502.05171', 'title': 'Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach', 'url': 'https://huggingface.co/papers/2502.05171', 'abstract': 'We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.', 'score': 24, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '4386159312d9856b', 'authors': ['Jonas Geiping', 'Sean McLeish', 'Neel Jain', 'John Kirchenbauer', 'Siddharth Singh', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Abhinav Bhatele', 'Tom Goldstein'], 'affiliations': ['ELLIS Institute Tübingen, Max-Planck Institute for Intelligent Systems, Tübingen AI Center', 'Lawrence Livermore National Laboratory', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2502.05171.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Глубокие рассуждения в латентном пространстве: новый подход к языковым моделям', 'desc': 'В статье представлена новая архитектура языковой модели, способная масштабировать вычисления во время тестирования путем неявных рассуждений в латентном пространстве. Модель работает путем итерации рекуррентного блока, разворачиваясь до произвольной глубины во время тестирования. В отличие от подходов, основанных на цепочке рассуждений, этот метод не требует специализированных обучающих данных и может работать с небольшими контекстными окнами. Авторы масштабировали экспериментальную модель до 3,5 миллиардов параметров и 800 миллиардов токенов, показав значительное улучшение производительности на тестах рассуждений.'}, 'en': {'title': 'Scaling Reasoning with Latent Space Computation', 'desc': 'This paper presents a new language model architecture that enhances reasoning capabilities by performing computations in a latent space during test-time. Instead of generating more tokens to scale reasoning, the model uses a recurrent block that allows it to unroll computations to any depth. This method does not rely on specialized training data and can effectively handle small context windows, enabling it to capture complex reasoning patterns. The authors demonstrate that their model, with 3.5 billion parameters, can achieve significant improvements on reasoning tasks, comparable to models with much larger parameter counts.'}, 'zh': {'title': '隐式推理，提升语言模型的计算能力', 'desc': '我们研究了一种新颖的语言模型架构，该架构能够通过在潜在空间中隐式推理来扩展测试时的计算能力。我们的模型通过迭代递归块工作，从而在测试时可以展开到任意深度。这与主流推理模型不同，后者通过生成更多的标记来增加计算量。我们的模型不需要特殊的训练数据，能够处理小的上下文窗口，并且能够捕捉不易用语言表示的推理类型。'}}}, {'id': 'https://huggingface.co/papers/2502.05176', 'title': 'AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting', 'url': 'https://huggingface.co/papers/2502.05176', 'abstract': 'Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.', 'score': 22, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '9b52f2788f53c3c0', 'authors': ['Chung-Ho Wu', 'Yang-Jung Chen', 'Ying-Huan Chen', 'Jie-Ying Lee', 'Bo-Hsu Ke', 'Chun-Wei Tuan Mu', 'Yi-Chuan Huang', 'Chin-Yang Lin', 'Min-Hung Chen', 'Yen-Yu Lin', 'Yu-Lun Liu'], 'affiliations': ['NVIDIA', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05176.jpg', 'data': {'categories': ['#dataset', '#3d'], 'emoji': '🌐', 'ru': {'title': 'Революция в 3D-реконструкции: AuraFusion360 для безупречного восстановления сцен', 'desc': 'AuraFusion360 - это новый метод восстановления трехмерных сцен на основе Gaussian Splatting. Он использует генерацию масок невидимых областей с учетом глубины, адаптивную диффузию глубины и улучшение деталей на основе SDEdit для создания высококачественных результатов. Метод превосходит существующие подходы по качеству восприятия и геометрической точности при изменении точки обзора. Авторы также представили первый набор данных 360-USID для оценки методов восстановления сцен с охватом 360 градусов.'}, 'en': {'title': 'Revolutionizing 3D Scene Inpainting with AuraFusion360', 'desc': 'AuraFusion360 is a new method for filling in missing parts of 3D scenes, which is important for applications like virtual reality. It uses Gaussian Splatting to represent scenes and introduces techniques for better identifying what should be filled in, ensuring that the filled areas look realistic from different angles. The method also places points accurately without needing extra training and enhances details to keep the views consistent. Additionally, it comes with a new dataset for testing these 3D inpainting methods, showing that AuraFusion360 performs better than previous techniques in both quality and accuracy.'}, 'zh': {'title': 'AuraFusion360：三维场景修复的新突破', 'desc': '三维场景修复在虚拟现实和建筑可视化等应用中非常重要，但现有方法在360度无界场景中面临视图一致性和几何精度的挑战。我们提出了AuraFusion360，这是一种新颖的基于参考的方法，能够在高质量的3D场景中进行物体移除和孔填充。该方法引入了深度感知的未见掩码生成、适应性引导深度扩散和基于SDEdit的细节增强，确保多视图的一致性。通过大量实验，AuraFusion360在感知质量和几何精度方面显著优于现有方法，能够在剧烈视角变化中保持高质量的修复效果。'}}}, {'id': 'https://huggingface.co/papers/2502.05163', 'title': 'DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails', 'url': 'https://huggingface.co/papers/2502.05163', 'abstract': 'The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \\ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard.', 'score': 17, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ae863d89ab71ab51', 'authors': ['Yihe Deng', 'Yu Yang', 'Junkai Zhang', 'Wei Wang', 'Bo Li'], 'affiliations': ['University of California, Los Angeles', 'University of Illinois at Urbana-Champaign', 'VirtueAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05163.jpg', 'data': {'categories': ['#low_resource', '#inference', '#synthetic', '#dataset', '#open_source', '#multilingual', '#rl'], 'emoji': '🛡️', 'ru': {'title': 'Улучшение многоязычной безопасности LLM через совместное обучение генератора и ограничителя', 'desc': 'Статья представляет новый подход к созданию многоязычных моделей-ограничителей для обеспечения безопасности больших языковых моделей (LLM). Авторы предлагают framework с двумя игроками на основе обучения с подкреплением, где генератор и модель-ограничитель развиваются совместно для создания синтетических данных. Теоретически это взаимодействие формализовано как игра двух игроков с доказанной сходимостью к равновесию Нэша. Эмпирические оценки показывают, что предложенная модель превосходит современные аналоги, особенно для языков с меньшими ресурсами.'}, 'en': {'title': 'Enhancing Multilingual Safety in LLMs with Synthetic Data Generation', 'desc': "This paper introduces a new method for creating guardrail models that help ensure the safe use of large language models (LLMs) by detecting harmful content in multiple languages. The authors propose a two-player Reinforcement Learning framework where a generator and a guardrail model work together in a competitive manner to create high-quality synthetic safety data. They demonstrate that this approach not only improves performance on English safety benchmarks but also significantly enhances the model's ability to handle lower-resource languages. The results show that their method is faster and more efficient, making it a promising solution for developing multilingual safety measures in LLMs."}, 'zh': {'title': '多语言护栏模型的创新进展', 'desc': '随着大型语言模型（LLMs）的快速发展，确保其负责任使用的护栏模型需求增加，尤其是在检测不安全和非法内容方面。虽然英语的安全数据相对丰富，但由于其他语言开放源代码安全数据的稀缺，多语言护栏建模仍然未被充分探索。为了解决这一问题，我们提出了一种新颖的双玩家强化学习框架，其中生成器和护栏模型对抗性地共同进化，以生成高质量的合成数据用于多语言护栏训练。我们的模型在多语言安全任务中取得了显著进展，特别是在处理低资源语言的不平衡问题上。'}}}, {'id': 'https://huggingface.co/papers/2502.04403', 'title': 'Agency Is Frame-Dependent', 'url': 'https://huggingface.co/papers/2502.04403', 'abstract': "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.", 'score': 13, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '32ceb8df4d77794a', 'authors': ['David Abel', 'André Barreto', 'Michael Bowling', 'Will Dabney', 'Shi Dong', 'Steven Hansen', 'Anna Harutyunyan', 'Khimya Khetarpal', 'Clare Lyle', 'Razvan Pascanu', 'Georgios Piliouras', 'Doina Precup', 'Jonathan Richens', 'Mark Rowland', 'Tom Schaul', 'Satinder Singh'], 'affiliations': ['Amii, University of Alberta', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.04403.jpg', 'data': {'categories': ['#rl', '#agi', '#reasoning', '#math'], 'emoji': '🤖', 'ru': {'title': 'Агентность: все зависит от точки зрения', 'desc': 'Статья рассматривает концепцию агентности в контексте обучения с подкреплением. Авторы утверждают, что агентность фундаментально зависит от системы отсчета. Они поддерживают этот тезис, анализируя ключевые свойства агентности, предложенные в предыдущих исследованиях. Статья подчеркивает необходимость учета зависимости от системы отсчета в изучении агентности и обсуждает последствия для обучения с подкреплением.'}, 'en': {'title': 'Agency in Reinforcement Learning: A Frame-Dependent Perspective', 'desc': 'This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning.'}, 'zh': {'title': '能动性：依赖于框架的系统能力', 'desc': '本文探讨了系统的能动性，特别是在强化学习的背景下。能动性是指系统朝着目标引导结果的能力，但判断一个系统是否具备能动性是一个复杂的问题。我们认为，能动性是依赖于参考框架的，任何对系统能动性的测量都必须相对于某个参考框架进行。通过哲学论证，我们支持这一观点，并讨论了这一结论对强化学习的影响。'}}}, {'id': 'https://huggingface.co/papers/2502.04728', 'title': 'Generating Symbolic World Models via Test-time Scaling of Large Language Models', 'url': 'https://huggingface.co/papers/2502.04728', 'abstract': 'Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.', 'score': 12, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'cd97668cd0eee0ee', 'authors': ['Zhouliang Yu', 'Yuhuan Yuan', 'Tim Z. Xiao', 'Fuxiang Frank Xia', 'Jie Fu', 'Ge Zhang', 'Ge Lin', 'Weiyang Liu'], 'affiliations': ['Environmental Systems Research Institute, Inc.', 'Max Planck Institute for Intelligent Systems, Tübingen', 'SEED, Bytedance', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.04728.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#data', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности планирования с помощью PDDL и LLM', 'desc': 'Статья представляет новый подход к решению сложных задач планирования с использованием больших языковых моделей (LLM). Авторы предлагают использовать язык определения планирования (PDDL) для создания точной символической модели мира. Метод включает алгоритм Best-of-N для улучшения качества начального решения и последующее уточнение с помощью вербализованного машинного обучения. Результаты показывают значительное превосходство над существующими методами в генерации доменов PDDL и решении задач планирования высокого уровня.'}, 'en': {'title': 'Enhancing LLMs for Optimal Planning with PDDL', 'desc': "This paper addresses the challenge of using Large Language Models (LLMs) for complex planning problems by introducing a method to generate Planning Domain Definition Language (PDDL) domains. PDDL serves as a formal language that helps in creating precise state descriptions, which is crucial for avoiding rule violations and ensuring optimal planning. The authors propose a novel algorithm that enhances LLMs' reasoning capabilities through a Best-of-N sampling approach, followed by fine-grained refinement using verbalized machine learning techniques. Their approach significantly improves the generation of PDDL domains, achieving over 50% success in generating high-quality plans from natural language descriptions without additional training."}, 'zh': {'title': '利用PDDL提升规划问题解决能力', 'desc': '本论文探讨了如何利用大型语言模型（LLMs）解决复杂的规划问题。为了避免规则违反和确保最优性，研究者们引入了规划领域定义语言（PDDL），作为一种精确的状态描述工具。通过PDDL，可以生成符号世界模型，并应用经典搜索算法（如A*）来寻找最优计划。本文提出了一种简单有效的算法，通过Best-of-N采样和细致的机器学习优化，显著提高了PDDL领域的生成质量，成功率超过50%。'}}}, {'id': 'https://huggingface.co/papers/2502.05179', 'title': 'FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation', 'url': 'https://huggingface.co/papers/2502.05179', 'abstract': 'DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .', 'score': 9, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c3147244e03af4a6', 'authors': ['Shilong Zhang', 'Wenbo Li', 'Shoufa Chen', 'Chongjian Ge', 'Peize Sun', 'Yida Zhang', 'Yi Jiang', 'Zehuan Yuan', 'Binyue Peng', 'Ping Luo'], 'affiliations': ['ByteDance', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.05179.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео высокого разрешения с предпросмотром', 'desc': 'Статья представляет новый двухэтапный подход к генерации видео на основе текста под названием FlashVideo. На первом этапе модель фокусируется на соответствии промпту, генерируя видео низкого разрешения. Второй этап использует сопоставление потоков для эффективного создания деталей высокого разрешения. Этот метод позволяет достичь высокого качества генерации видео при меньших вычислительных затратах по сравнению с существующими подходами. Кроме того, пользователи могут предварительно просмотреть результат перед полной генерацией, что повышает коммерческую привлекательность технологии.'}, 'en': {'title': 'FlashVideo: Efficient Text-to-Video Generation with Two-Stage Framework', 'desc': 'The paper introduces FlashVideo, a two-stage framework for text-to-video generation that improves efficiency and quality. In the first stage, it focuses on generating low-resolution videos with high fidelity to text prompts, using large model parameters and sufficient function evaluations. The second stage enhances the video by matching flow between low and high resolutions, adding fine details while minimizing computational demands. This approach not only achieves high-resolution outputs with better efficiency but also allows users to preview results before full generation, making it more practical for commercial use.'}, 'zh': {'title': 'FlashVideo：高效生成高分辨率视频的创新框架', 'desc': 'DiT扩散模型在文本到视频生成方面取得了显著成功，但高内容和运动保真度通常需要大量模型参数和函数评估。为了解决这些计算需求，我们提出了一种新的两阶段框架FlashVideo，旨在平衡生成的保真度和质量。在第一阶段，通过低分辨率生成过程优先考虑提示保真度，利用大参数和足够的函数评估提高计算效率。第二阶段则在低分辨率和高分辨率之间建立流匹配，有效生成细节，且所需的函数评估最小化。'}}}, {'id': 'https://huggingface.co/papers/2502.04959', 'title': 'No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces', 'url': 'https://huggingface.co/papers/2502.04959', 'abstract': 'Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .', 'score': 8, 'issue_id': 2127, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'a73679e79ff14b7c', 'authors': ['Daniel Marczak', 'Simone Magistri', 'Sebastian Cygert', 'Bartłomiej Twardowski', 'Andrew D. Bagdanov', 'Joost van de Weijer'], 'affiliations': ['Computer Vision Center, Barcelona, Spain', 'Department of Computer Science, Universitat Autonoma de Barcelona, Spain', 'Department of Information Engineering, University of Florence, Italy', 'Gdansk University of Technology, Poland', 'IDEAS NCBR, Warsaw, Poland', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2502.04959.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Эффективное объединение моделей машинного обучения без дополнительного обучения', 'desc': 'Статья посвящена объединению весов нескольких моделей, обученных на конкретных задачах, в одну многозадачную модель. Авторы исследуют ключевые характеристики матриц задач, которые позволяют эффективно объединять модели. Они предлагают изотропный фреймворк для объединения, который выравнивает спектр сингулярных значений матриц задач и улучшает их выравнивание. Предложенный подход достигает наилучших результатов в различных сценариях, включая разные наборы задач и масштабы моделей.'}, 'en': {'title': 'Bridging the Performance Gap in Model Merging', 'desc': 'This paper focuses on model merging, which combines the weights of different models designed for specific tasks into one model that can handle multiple tasks. The authors identify that there is often a performance gap when comparing the merged model to single-task models. They explore the importance of task matrices, which are weight update matrices, and find that better alignment between the components of these matrices leads to improved performance. To address the performance gap, they introduce an isotropic merging framework that optimizes the alignment of task matrices and incorporates both shared and unique features of tasks, achieving top performance across various scenarios.'}, 'zh': {'title': '有效的模型合并方法提升多任务性能', 'desc': '模型合并是将多个特定任务模型的权重整合为一个多任务模型的过程。尽管这一问题受到越来越多的关注，但合并模型与单任务模型之间仍存在显著的性能差距。本文研究了任务矩阵的关键特性，这些矩阵是应用于预训练模型的权重更新矩阵，发现任务特定矩阵与合并矩阵之间的对齐程度与性能提升密切相关。我们提出了一种各向同性合并框架，通过平坦化任务矩阵的奇异值谱，增强对齐性，从而缩小性能差距，并在多个场景中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.04520', 'title': "Linear Correlation in LM's Compositional Generalization and Hallucination", 'url': 'https://huggingface.co/papers/2502.04520', 'abstract': 'The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., "X lives in the city of" rightarrow "X lives in the country of" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM\'s generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.', 'score': 8, 'issue_id': 2119, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '41ef9027d1533f06', 'authors': ['Letian Peng', 'Chenyang An', 'Shibo Hao', 'Chengyu Dong', 'Jingbo Shang'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.04520.jpg', 'data': {'categories': ['#interpretability', '#training', '#architecture', '#agi', '#data', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Линейность как ключ к обобщению языковых моделей', 'desc': 'Статья исследует феномен линейных корреляций в языковых моделях при композиции знаний. Авторы обнаружили, что существует линейное преобразование между связанными знаниями, которое отображает логиты предсказания следующего токена от одного промпта к другому. Это явление устойчиво к масштабному дообучению и может служить потенциальным идентификатором обобщения языковой модели. Исследование показывает, что такие линейные корреляции могут быть изучены с помощью одной полносвязной нейронной сети и предобученных представлений словаря.'}, 'en': {'title': 'Unlocking Language Models: The Power of Linear Correlations in Knowledge Composition', 'desc': 'This paper explores how language models (LMs) handle knowledge composition, revealing that they exhibit linear correlations when predicting the next token. It shows that there is a linear transformation that connects related knowledge, allowing LMs to generalize information effectively, similar to how humans relate concepts. The study finds that while these linear transformations can adapt to new information through fine-tuning, they can also lead to incorrect outputs, or hallucinations, when the relationships are not aligned with reality. Overall, the research suggests that understanding these linear correlations can help identify how well LMs generalize knowledge.'}, 'zh': {'title': '语言模型的线性相关性与知识组合', 'desc': '这篇论文探讨了语言模型（LM）在知识组合中的线性相关现象。研究发现，某些相关知识之间存在线性变换，这种变换可以将一个提示的下一个标记预测从一个映射到另一个。比如，从"X 住在城市"可以转变为"X 住在国家"。结果表明，线性变换在大规模微调中具有韧性，并且当与现实世界关系一致时能够推广更新的知识，但当偏离时则会导致幻觉。'}}}, {'id': 'https://huggingface.co/papers/2502.04416', 'title': 'CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference', 'url': 'https://huggingface.co/papers/2502.04416', 'abstract': 'Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/JarvisPei/CMoE.', 'score': 7, 'issue_id': 2125, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '0b4520b0860c835c', 'authors': ['Zehua Pei', 'Lancheng Zou', 'Hui-Ling Zhen', 'Xianzhi Yu', 'Wulong Liu', 'Sinno Jialin Pan', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04416.jpg', 'data': {'categories': ['#architecture', '#optimization', '#open_source', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эффективное превращение плотных LLM в разреженные MoE модели', 'desc': 'Статья представляет новый метод CMoE (Carved MoE) для эффективного создания моделей Mixture-of-Experts из плотных моделей больших языковых моделей (LLM). CMoE группирует нейроны в общие и маршрутизируемые экспертные группы на основе уровней активации. Метод включает механизм маршрутизации с дифференцируемым процессом и балансировкой нагрузки. CMoE позволяет быстро создать эффективную MoE модель из плотной модели размером 7 миллиардов параметров, используя небольшой объем данных.'}, 'en': {'title': 'Efficiently Carving Mixture-of-Experts for Large Language Models', 'desc': 'This paper introduces CMoE (Carved Mixture-of-Experts), a new framework designed to enhance the efficiency of large language models (LLMs) by utilizing a mixture-of-experts architecture. CMoE takes advantage of the high activation sparsity found in feed-forward networks by activating only a subset of parameters, which reduces inference overhead. The framework groups neurons into shared and routed experts based on their activation rates and employs a differentiable routing mechanism that avoids the need for extensive retraining. Remarkably, CMoE can create a functional MoE model from a dense model in just five minutes using limited data, achieving high performance with minimal fine-tuning.'}, 'zh': {'title': '高效雕刻混合专家模型的创新框架', 'desc': '大型语言模型（LLMs）通过增加模型参数实现了出色的性能，但这也带来了显著的推理开销。前馈网络（FFNs）在LLM参数中占主导地位，隐藏神经元的激活稀疏性很高。为此，研究人员提出了混合专家（MoE）架构，仅激活一部分参数。然而，现有方法通常需要大量的训练数据和资源，限制了其实用性。我们提出了CMoE（Carved MoE），一个新颖的框架，可以高效地从稠密模型中雕刻出MoE模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04404', 'title': 'Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models', 'url': 'https://huggingface.co/papers/2502.04404', 'abstract': "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.", 'score': 7, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a7bd201755c7ea1d', 'authors': ['Xiao-Wen Yang', 'Xuan-Yi Zhu', 'Wen-Da Wei', 'Ding-Chu Zhang', 'Jie-Jing Shao', 'Zhi Zhou', 'Lan-Zhe Guo', 'Yu-Feng Li'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University, China', 'School of Artificial Intelligence, Nanjing University, China', 'School of Intelligence Science and Technology, Nanjing University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04404.jpg', 'data': {'categories': ['#agi', '#training', '#inference', '#agents', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Самостоятельный возврат: путь к более разумным ИИ', 'desc': 'Статья представляет новый механизм самостоятельного возврата (self-backtracking) для больших языковых моделей (LLM). Этот механизм позволяет LLM автономно определять, когда и где нужно вернуться назад в процессе рассуждений. Авторы утверждают, что это улучшает способности LLM к рассуждению и повышает эффективность, превращая медленное мышление в быстрое через самосовершенствование. Эмпирические оценки показывают значительное улучшение возможностей рассуждения LLM с использованием этого подхода.'}, 'en': {'title': 'Empowering LLMs with Self-Backtracking for Enhanced Reasoning', 'desc': 'This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods.'}, 'zh': {'title': '自我回溯机制：提升语言模型推理能力的关键', 'desc': '将慢思考机制整合到大型语言模型（LLMs）中，为实现二级AGI推理器提供了有希望的途径。当前的挑战包括低效的过度思考和对辅助奖励模型的过度依赖。我们指出，这些限制源于LLMs无法内化搜索过程，而搜索过程是有效推理的关键组成部分。我们提出了一种自我回溯机制，使LLMs能够在训练和推理过程中自主决定何时以及如何回溯，从而显著提升推理能力和效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03738', 'title': 'Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More', 'url': 'https://huggingface.co/papers/2502.03738', 'abstract': 'Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch_Scaling.', 'score': 6, 'issue_id': 2122, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'aa76478090a36c04', 'authors': ['Feng Wang', 'Yaodong Yu', 'Guoyizhe Wei', 'Wei Shao', 'Yuyin Zhou', 'Alan Yuille', 'Cihang Xie'], 'affiliations': ['Johns Hopkins University', 'UC Berkeley', 'UC Santa Cruz', 'University of Florida'], 'pdf_title_img': 'assets/pdf/title_img/2502.03738.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Пиксельная токенизация превосходит патчи в vision-моделях', 'desc': 'Исследование посвящено анализу влияния размера патчей в Vision Transformer (ViT) на качество распознавания изображений. Авторы обнаружили, что уменьшение размера патчей до 1x1 пикселя (пиксельная токенизация) улучшает предсказательную способность модели. Этот эффект наблюдается для различных задач компьютерного зрения, масштабов входных данных и архитектур, включая ViT и Mamba. Эксперименты показали, что модель базового размера с длиной визуальной последовательности 50 176 токенов достигает точности 84,6% на наборе данных ImageNet-1k.'}, 'en': {'title': 'Unlocking Visual Potential: The Power of Smaller Patches in Vision Transformers', 'desc': 'This paper investigates the impact of patchification, a method of dividing images into smaller sections, on the performance of Vision Transformers (ViT) in visual tasks. The authors find that reducing the size of these patches leads to better predictive performance, with the optimal size being 1x1 pixels, which represents pixel tokenization. They conduct experiments that demonstrate this scaling law across various architectures and tasks, revealing that smaller patches diminish the need for complex decoder heads in dense prediction tasks. The study achieves a significant milestone by processing a visual sequence of 50,176 tokens, achieving a competitive accuracy on the ImageNet-1k benchmark, and aims to inform future developments in non-compressive vision models.'}, 'zh': {'title': '小块更优，视觉理解更强！', 'desc': '本文研究了视觉变换器（ViT）中图像分块（patchification）对信息损失的影响。通过缩小图像的空间大小，分块方法可以有效减少令牌序列的长度，从而降低计算成本。我们发现，随着分块大小的减小，模型的预测性能持续提高，直到达到最小的1x1像素分块。该研究结果适用于多种视觉任务和不同的模型架构，为未来构建非压缩视觉模型提供了理论基础。'}}}, {'id': 'https://huggingface.co/papers/2502.05178', 'title': 'QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.05178', 'abstract': 'We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.', 'score': 6, 'issue_id': 2121, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'bbc1f1a0b7f5423f', 'authors': ['Yue Zhao', 'Fuzhao Xue', 'Scott Reed', 'Linxi Fan', 'Yuke Zhu', 'Jan Kautz', 'Zhiding Yu', 'Philipp Krähenbühl', 'De-An Huang'], 'affiliations': ['NVIDIA', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.05178.jpg', 'data': {'categories': ['#multimodal', '#cv', '#training', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'QLIP: Единый подход к пониманию и генерации изображений', 'desc': 'QLIP - это метод визуальной токенизации, объединяющий высококачественную реконструкцию изображений с пониманием изображений без предварительного обучения. Он использует автоэнкодер с бинарно-сферическим квантованием и оптимизирует одновременно реконструкцию и выравнивание языка и изображений. Авторы предлагают двухэтапный процесс обучения для эффективного сочетания требований к большим батчам и ограничений по памяти. QLIP может заменить визуальный энкодер в мультимодальных моделях и токенизатор изображений в генеративных моделях, показывая сопоставимую или лучшую производительность.'}, 'en': {'title': 'QLIP: Bridging Visual and Language Understanding with Efficient Tokenization', 'desc': 'The paper presents Quantized Language-Image Pretraining (QLIP), a novel method for visual tokenization that achieves high-quality image reconstruction while enhancing zero-shot image understanding. QLIP employs a binary-spherical-quantization-based autoencoder, integrating both reconstruction and language-image alignment objectives in a harmonious manner. By dynamically balancing these objectives during training, the method effectively addresses the challenges of large-batch image-language pre-training and memory constraints. The results show that QLIP can replace existing visual encoders and image tokenizers in models like LLaVA and LlamaGen, improving performance in multimodal understanding and text-conditioned image generation.'}, 'zh': {'title': '量化语言-图像预训练：多模态理解的新突破', 'desc': '我们介绍了一种量化语言-图像预训练方法（QLIP），这是一种视觉标记化方法，结合了最先进的重建质量和零-shot图像理解能力。QLIP使用基于二元球面量化的自编码器进行训练，同时优化重建和语言-图像对齐目标。我们首次证明这两个目标可以动态平衡，而不是相互对立。QLIP在多模态理解和文本条件图像生成方面表现出色，可以作为LLaVA的视觉编码器和LlamaGen的图像标记器的替代方案，性能相当或更好。'}}}, {'id': 'https://huggingface.co/papers/2502.04363', 'title': 'On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices', 'url': 'https://huggingface.co/papers/2502.04363', 'abstract': 'We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.', 'score': 6, 'issue_id': 2119, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '339b45dee733174c', 'authors': ['Bosung Kim', 'Kyuhwan Lee', 'Isu Jeong', 'Jungmin Cheon', 'Yeojin Lee', 'Seulki Lee'], 'affiliations': ['Ulsan National Institute of Science and Technology South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.04363.jpg', 'data': {'categories': ['#inference', '#video', '#open_source', '#diffusion', '#architecture', '#low_resource'], 'emoji': '📱', 'ru': {'title': 'Генерация видео по тексту прямо на вашем смартфоне', 'desc': 'On-device Sora представляет собой инновационное решение для генерации видео на основе текста с использованием диффузионных моделей, работающее на смартфонах. Система применяет три новые техники: Linear Proportional Leap (LPL) для сокращения шагов денойзинга, Temporal Dimension Token Merging (TDTM) для оптимизации вычислений в слоях внимания, и Concurrent Inference with Dynamic Loading (CI-DL) для эффективного использования ограниченной памяти устройства. Эксперименты на iPhone 15 Pro показали, что On-device Sora способна генерировать видео высокого качества, сравнимые с результатами Open-Sora на мощных GPU.'}, 'en': {'title': 'Empowering Video Creation on Your Smartphone!', 'desc': 'On-device Sora is a groundbreaking solution for generating videos from text using diffusion models directly on smartphones. It introduces three innovative techniques: Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to optimize token processing in attention layers, and Concurrent Inference with Dynamic Loading (CI-DL) to manage memory efficiently. These advancements allow the system to produce high-quality videos comparable to those generated by powerful GPUs, all while operating within the constraints of mobile devices. This work not only enhances accessibility to advanced generative technologies but also prioritizes user privacy and reduces reliance on cloud services.'}, 'zh': {'title': '移动设备上的高效视频生成新突破', 'desc': '我们提出了On-device Sora，这是首个基于扩散模型的移动设备文本到视频生成解决方案，能够高效地在智能手机上运行。该系统采用了三种新技术来解决移动设备在计算和内存方面的限制。首先，线性比例跳跃（LPL）通过高效的跳跃方法减少了视频扩散中所需的去噪步骤。其次，时间维度令牌合并（TDTM）通过沿时间维度合并连续令牌，降低了注意力层中密集的令牌处理计算。'}}}, {'id': 'https://huggingface.co/papers/2502.05092', 'title': 'Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs', 'url': 'https://huggingface.co/papers/2502.05092', 'abstract': "Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) ClockQA, which comprises various types of clock styles-standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks-paired with time related questions; and 2) CalendarQA, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.", 'score': 5, 'issue_id': 2128, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ea9f14d34d4cbb60', 'authors': ['Rohit Saxena', 'Aryo Pradipta Gema', 'Pasquale Minervini'], 'affiliations': ['ILCC, School of Informatics, University of Edinburgh', 'Miniml.AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05092.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#cv', '#interpretability'], 'emoji': '⏰', 'ru': {'title': 'Время бросает вызов искусственному интеллекту', 'desc': 'Статья исследует способности мультимодальных больших языковых моделей (MLLM) в интерпретации времени и дат через аналоговые часы и календари. Авторы создали структурированный набор данных, включающий ClockQA с различными стилями часов и CalendarQA с календарными изображениями и вопросами. Цель исследования - проанализировать, как MLLM выполняют визуальное распознавание, числовые рассуждения и временные выводы. Результаты показывают, что надежное понимание времени остается значительной проблемой для MLLM, несмотря на недавние достижения.'}, 'en': {'title': 'Unlocking Time: Challenges for Multimodal Language Models', 'desc': "This paper explores how multimodal large language models (MLLMs) understand time through visual aids like clocks and calendars. The authors created a dataset with two parts: ClockQA, which includes different clock styles and related questions, and CalendarQA, featuring yearly calendars with various date-related queries. The study assesses the models' abilities in visual recognition, numerical reasoning, and temporal inference when interpreting these time-related images. Despite progress in MLLMs, the findings indicate that accurately grasping time concepts continues to be a major hurdle."}, 'zh': {'title': '理解时间的挑战：多模态语言模型的局限性', 'desc': '本研究探讨了多模态大型语言模型（MLLMs）在理解时间和日期方面的能力，特别是通过模拟时钟和年度日历的视觉表示。我们创建了一个结构化的数据集，包括两部分：ClockQA，包含不同风格的时钟及相关时间问题；CalendarQA，包含年度日历图像及常见日期问题。我们的目标是分析MLLMs在处理与时间相关的视觉数据时的视觉识别、数值推理和时间推断能力。尽管最近取得了一些进展，但MLLMs在可靠理解时间方面仍面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2502.03512', 'title': 'YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment', 'url': 'https://huggingface.co/papers/2502.03512', 'abstract': 'Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that generated visuals not only accurately encapsulate user intents but also conform to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini fiasco, where misaligned outputs triggered significant public backlash, underscore the critical need for robust alignment mechanisms. In contrast, Large Language Models (LLMs) have achieved notable success in alignment. Building on these advancements, researchers are eager to apply similar alignment techniques, such as Direct Preference Optimization (DPO), to T2I systems to enhance image generation fidelity and reliability.   We present YinYangAlign, an advanced benchmarking framework that systematically quantifies the alignment fidelity of T2I systems, addressing six fundamental and inherently contradictory design objectives. Each pair represents fundamental tensions in image generation, such as balancing adherence to user prompts with creative modifications or maintaining diversity alongside visual coherence. YinYangAlign includes detailed axiom datasets featuring human prompts, aligned (chosen) responses, misaligned (rejected) AI-generated outputs, and explanations of the underlying contradictions.', 'score': 4, 'issue_id': 2122, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'ebf6482c46f8fe2f', 'authors': ['Amitava Das', 'Yaswanth Narsupalli', 'Gurpreet Singh', 'Vinija Jain', 'Vasu Sharma', 'Suranjana Trivedy', 'Aman Chadha', 'Amit Sheth'], 'affiliations': ['Amazon AI, USA', 'Artificial Intelligence Institute, University of South Carolina, USA', 'Meta AI, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.03512.jpg', 'data': {'categories': ['#benchmark', '#rag', '#rlhf', '#ethics', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Баланс противоречий в генерации изображений: YinYangAlign на страже точности', 'desc': 'Статья представляет YinYangAlign - передовую систему оценки для измерения точности выравнивания текстово-изобразительных (T2I) моделей. Она фокусируется на шести фундаментальных и противоречивых целях дизайна, отражающих ключевые напряжения в генерации изображений. YinYangAlign включает наборы данных с человеческими запросами, выровненными и невыровненными ответами ИИ, а также объяснениями противоречий. Система направлена на улучшение надежности и точности генерации изображений, применяя методы выравнивания, успешно используемые в больших языковых моделях.'}, 'en': {'title': 'Enhancing Image Generation Fidelity with YinYangAlign', 'desc': 'This paper discusses the importance of precise alignment in Text-to-Image (T2I) systems to ensure that generated images meet user expectations and ethical standards. It highlights past failures, like the Google Gemini incident, which demonstrate the need for better alignment strategies. The authors propose YinYangAlign, a new benchmarking framework that evaluates T2I systems based on six conflicting design goals, such as user prompt adherence versus creative freedom. By using detailed datasets that include human prompts and AI outputs, YinYangAlign aims to improve the fidelity and reliability of image generation.'}, 'zh': {'title': '提升文本到图像系统的对齐精度', 'desc': '本文探讨了文本到图像（T2I）系统中精确对齐的重要性，以确保生成的图像既能准确反映用户意图，又符合伦理和美学标准。研究表明，像Google Gemini这样的事件凸显了强大对齐机制的必要性。与此相比，大型语言模型（LLMs）在对齐方面取得了显著成功，研究人员希望将类似的对齐技术应用于T2I系统，以提高图像生成的准确性和可靠性。我们提出了YinYangAlign，这是一个先进的基准框架，系统地量化T2I系统的对齐保真度，解决了六个基本且内在矛盾的设计目标。'}}}, {'id': 'https://huggingface.co/papers/2502.04350', 'title': 'CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance', 'url': 'https://huggingface.co/papers/2502.04350', 'abstract': 'Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.', 'score': 4, 'issue_id': 2118, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'ad7829c09c28de41', 'authors': ['Yongchao Chen', 'Yilun Hao', 'Yueying Liu', 'Yang Zhang', 'Chuchu Fan'], 'affiliations': ['Harvard University, Boston, MA, USA', 'MIT-IBM Watson AI Lab, Boston, MA, USA', 'Massachusetts Institute of Technology, Boston, MA, USA', 'University of Illinois Urbana-Champaign, Urbana, IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.04350.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#rlhf', '#open_source', '#optimization', '#reasoning'], 'emoji': '🧭', 'ru': {'title': 'CodeSteer: Умное управление для раскрытия потенциала LLM в символьных вычислениях', 'desc': 'CodeSteer - это новый метод для эффективного управления генерацией кода и текста в больших языковых моделях (LLM). Исследователи создали комплексный бенчмарк SymBench и синтезировали наборы данных для обучения модели. Они дообучили модель Llama-3-8B с использованием многораундового обучения с учителем и прямой оптимизации предпочтений. Результирующая модель CodeSteerLLM значительно улучшает производительность других LLM в задачах символьных вычислений, превосходя даже лучшие существующие модели.'}, 'en': {'title': 'CodeSteer: Guiding LLMs to Master Code and Reasoning!', 'desc': 'This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks.'}, 'zh': {'title': 'CodeSteer：引导LLM实现符号计算的突破', 'desc': '现有的方法无法有效引导大型语言模型（LLMs）在文本推理和代码生成之间切换，导致符号计算能力未得到充分利用。我们提出了一种名为CodeSteer的方法，能够有效指导LLM的代码和文本生成。我们构建了一个全面的基准SymBench，包含37个具有可调复杂度的符号任务，并合成了包含1.2万多轮指导/生成轨迹和5500对指导比较的数据集。通过对Llama-3-8B模型进行多轮监督微调（SFT）和直接偏好优化（DPO），我们得到的CodeSteerLLM模型能够有效引导更大模型的代码/文本生成。'}}}, {'id': 'https://huggingface.co/papers/2502.04327', 'title': 'Value-Based Deep RL Scales Predictably', 'url': 'https://huggingface.co/papers/2502.04327', 'abstract': 'Scaling data and compute is critical to the success of machine learning. However, scaling demands predictability: we want methods to not only perform well with more compute or data, but also have their performance be predictable from small-scale runs, without running the large-scale experiment. In this paper, we show that value-based off-policy RL methods are predictable despite community lore regarding their pathological behavior. First, we show that data and compute requirements to attain a given performance level lie on a Pareto frontier, controlled by the updates-to-data (UTD) ratio. By estimating this frontier, we can predict this data requirement when given more compute, and this compute requirement when given more data. Second, we determine the optimal allocation of a total resource budget across data and compute for a given performance and use it to determine hyperparameters that maximize performance for a given budget. Third, this scaling behavior is enabled by first estimating predictable relationships between hyperparameters, which is used to manage effects of overfitting and plasticity loss unique to RL. We validate our approach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI gym, and IsaacGym, when extrapolating to higher levels of data, compute, budget, or performance.', 'score': 3, 'issue_id': 2133, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '4b04abb62254054b', 'authors': ['Oleh Rybkin', 'Michal Nauman', 'Preston Fu', 'Charlie Snell', 'Pieter Abbeel', 'Sergey Levine', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'University of California, Berkeley', 'University of Warsaw'], 'pdf_title_img': 'assets/pdf/title_img/2502.04327.jpg', 'data': {'categories': ['#rl', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Предсказуемое масштабирование в обучении с подкреплением', 'desc': 'Статья посвящена предсказуемости масштабирования методов обучения с подкреплением вне политики, основанных на значениях. Авторы демонстрируют, что требования к данным и вычислительным ресурсам для достижения определенного уровня производительности лежат на границе Парето, контролируемой соотношением обновлений к данным (UTD). Исследователи предлагают метод оценки этой границы для прогнозирования требований к ресурсам при масштабировании. Подход валидируется на трех алгоритмах (SAC, BRO, PQL) в различных средах обучения с подкреплением.'}, 'en': {'title': 'Predictable Scaling in Reinforcement Learning', 'desc': 'This paper discusses how to effectively scale data and compute resources in machine learning, particularly in reinforcement learning (RL). It demonstrates that value-based off-policy RL methods can be predictable, contrary to common beliefs about their erratic behavior. The authors introduce a Pareto frontier that helps estimate the data and compute requirements for achieving desired performance levels. They also provide a method for optimizing resource allocation and hyperparameters to enhance performance while managing overfitting and plasticity loss in RL.'}, 'zh': {'title': '可预测的强化学习扩展方法', 'desc': '在机器学习中，扩展数据和计算能力是成功的关键。然而，扩展需要可预测性：我们希望方法不仅在更多计算或数据下表现良好，而且其性能可以从小规模实验中预测。本文展示了基于价值的离线策略强化学习方法在可预测性方面的表现，尽管社区普遍认为其行为不稳定。我们通过估计帕累托前沿，预测在给定计算或数据时所需的资源，并确定在特定预算下的最佳资源分配，以最大化性能。'}}}, {'id': 'https://huggingface.co/papers/2502.04376', 'title': 'MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf', 'url': 'https://huggingface.co/papers/2502.04376', 'abstract': 'In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.', 'score': 3, 'issue_id': 2121, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '049ab6200a9d2eae', 'authors': ['Lingxiang Hu', 'Shurun Yuan', 'Xiaoting Qin', 'Jue Zhang', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Microsoft', 'Northeastern University, China', 'Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04376.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#agi', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'LLM как виртуальные делегаты: будущее эффективных совещаний', 'desc': 'Исследователи разработали прототип системы делегирования участия в совещаниях на основе больших языковых моделей (LLM). Они создали комплексный бенчмарк, используя реальные стенограммы совещаний, для оценки эффективности различных LLM в роли делегатов. Результаты показали, что около 60% ответов моделей затрагивают как минимум один ключевой момент из эталонных данных, но требуется улучшение в снижении нерелевантного контента и повышении устойчивости к ошибкам транскрипции. Исследование подчеркивает потенциал и проблемы использования LLM в качестве делегатов на совещаниях, предлагая ценные выводы для их практического применения.'}, 'en': {'title': 'Empowering Meetings with LLMs: Balancing Engagement and Efficiency', 'desc': 'This paper investigates the use of Large Language Models (LLMs) as meeting delegates to improve the efficiency of workplace meetings. A prototype system was developed and evaluated using real meeting transcripts, revealing that different LLMs exhibit varying engagement strategies during discussions. The results indicate that while some models effectively address key points, there is a need for improvement in handling irrelevant content and transcription errors. The study highlights both the potential benefits and challenges of integrating LLMs into meeting processes, providing insights for future applications.'}, 'zh': {'title': '利用大型语言模型优化会议参与', 'desc': '在现代工作场所，会议是交流思想和确保团队一致性的重要环节，但常常面临时间消耗、日程冲突和参与效率低下等挑战。本文探讨了大型语言模型（LLMs）在会议中有效分配参与者的能力，开发了一个基于LLM的会议代理系统原型，并使用真实会议记录创建了全面的基准测试。评估结果显示，GPT-4/4o在积极和谨慎的参与策略之间保持了平衡，而Gemini 1.5 Pro则更倾向于谨慎，Gemini 1.5 Flash和Llama3-8B/70B则表现出更积极的倾向。尽管约60%的回应涵盖了至少一个关键点，但仍需改进以减少无关或重复内容，并提高对真实场景中转录错误的容忍度。'}}}, {'id': 'https://huggingface.co/papers/2502.04689', 'title': 'ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning', 'url': 'https://huggingface.co/papers/2502.04689', 'abstract': 'Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance ("think step by step"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.', 'score': 2, 'issue_id': 2123, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'a052e3be1fe147cd', 'authors': ['Yuwei Yin', 'Giuseppe Carenini'], 'affiliations': ['University of British Columbia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04689.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ARR: Новый шаг в улучшении рассуждений языковых моделей', 'desc': 'Статья представляет новый метод промптинга для больших языковых моделей под названием ARR. Этот метод включает три ключевых шага: анализ намерения вопроса, извлечение релевантной информации и пошаговое рассуждение. ARR показывает лучшие результаты по сравнению с базовым подходом и методом Chain-of-Thought на различных задачах вопросно-ответного типа. Эксперименты подтверждают эффективность и обобщаемость ARR для разных размеров и типов языковых моделей.'}, 'en': {'title': 'ARR: A Structured Approach to Boost LLM Reasoning', 'desc': "This paper presents a new prompting method called ARR for large language models (LLMs) that enhances their performance on multiple-choice question-answering tasks. ARR stands for Analyze, Retrieve, and Reason, which are the three critical steps it incorporates to improve reasoning capabilities. The method outperforms traditional zero-shot Chain-of-Thought prompting by providing clearer guidance on how to approach questions. Experiments show that each component of ARR contributes positively to the overall effectiveness, especially the analysis of the question's intent."}, 'zh': {'title': 'ARR：提升问答推理的新方法', 'desc': '本文介绍了一种新的零-shot提示方法ARR，旨在提高大型语言模型（LLMs）在多项选择问答任务中的推理能力。ARR明确包含三个关键步骤：分析问题意图、检索相关信息和逐步推理。通过在多种复杂问答任务上的实验，ARR consistently outperform了传统的基线方法和Chain-of-Thought（CoT）提示。研究表明，意图分析在ARR中起着至关重要的作用，进一步验证了每个组成部分的积极贡献。'}}}, {'id': 'https://huggingface.co/papers/2501.12387', 'title': 'Continuous 3D Perception Model with Persistent State', 'url': 'https://huggingface.co/papers/2501.12387', 'abstract': 'We present a unified framework capable of solving a broad range of 3D tasks. Our approach features a stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, this evolving state can be used to generate metric-scale pointmaps (per-pixel 3D points) for each new input in an online fashion. These pointmaps reside within a common coordinate system, and can be accumulated into a coherent, dense scene reconstruction that updates as new images arrive. Our model, called CUT3R (Continuous Updating Transformer for 3D Reconstruction), captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen regions of the scene by probing at virtual, unobserved views. Our method is simple yet highly flexible, naturally accepting varying lengths of images that may be either video streams or unordered photo collections, containing both static and dynamic content. We evaluate our method on various 3D/4D tasks and demonstrate competitive or state-of-the-art performance in each. Project Page: https://cut3r.github.io/', 'score': 1, 'issue_id': 2134, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '2269a12b01fff7a4', 'authors': ['Qianqian Wang', 'Yifei Zhang', 'Aleksander Holynski', 'Alexei A. Efros', 'Angjoo Kanazawa'], 'affiliations': ['Google DeepMind', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2501.12387.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🏗️', 'ru': {'title': 'Универсальная 3D-реконструкция в реальном времени', 'desc': 'Авторы представляют универсальную модель CUT3R для решения широкого спектра задач 3D-реконструкции. Эта рекуррентная модель непрерывно обновляет свое внутреннее представление с каждым новым наблюдением, генерируя метрические поточечные карты для каждого входного изображения. CUT3R способна накапливать согласованную плотную реконструкцию сцены и даже предсказывать ненаблюдаемые области. Модель демонстрирует конкурентоспособную или лучшую производительность в различных задачах 3D/4D реконструкции.'}, 'en': {'title': 'CUT3R: Real-Time 3D Reconstruction with Continuous Updates', 'desc': 'This paper introduces CUT3R, a novel framework for 3D reconstruction that utilizes a stateful recurrent model to continuously update its state with new image observations. The model generates metric-scale pointmaps in real-time, allowing for the accumulation of a dense scene reconstruction as more images are processed. CUT3R is capable of inferring unseen areas of a scene by simulating views that have not been directly observed. The approach is versatile, handling both video streams and unordered photo collections, and shows strong performance across various 3D tasks.'}, 'zh': {'title': 'CUT3R：持续更新的三维重建模型', 'desc': '我们提出了一个统一的框架，能够解决广泛的三维任务。该方法采用状态递归模型，能够随着每个新观察不断更新其状态表示。通过图像流，这种不断演变的状态可以在线生成度量尺度的点图，为每个新输入生成每像素的三维点。我们的模型CUT3R不仅可以从图像观察中预测准确的点图，还可以通过探测虚拟的未观察视角推断场景中未见的区域。'}}}, {'id': 'https://huggingface.co/papers/2502.03032', 'title': 'Analyze Feature Flow to Enhance Interpretation and Steering in Language Models', 'url': 'https://huggingface.co/papers/2502.03032', 'abstract': 'We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.', 'score': 49, 'issue_id': 2090, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '030f362f419e9eb4', 'authors': ['Daniil Laptev', 'Nikita Balagansky', 'Yaroslav Aksenov', 'Daniil Gavrilov'], 'affiliations': ['1T-Tech', 'Moscow Institute of Physics and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.03032.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#training'], 'emoji': '🧠', 'ru': {'title': 'Прозрачное управление языковыми моделями через межслойный анализ признаков', 'desc': 'Представлен новый подход к систематическому отображению признаков, обнаруженных разреженным автоэнкодером, между последовательными слоями больших языковых моделей. Метод использует технику косинусного сходства без данных для отслеживания эволюции признаков на каждом этапе. Это позволяет создавать подробные графы потоков развития признаков, обеспечивая детальную интерпретируемость и механистическое понимание вычислений модели. Исследование демонстрирует, как межслойные карты признаков могут использоваться для прямого управления поведением модели путем усиления или подавления выбранных признаков.'}, 'en': {'title': 'Mapping and Manipulating Features in Language Models', 'desc': 'This paper presents a novel method for analyzing features in large language models using sparse autoencoders. It employs a data-free cosine similarity approach to track the evolution of features across different layers of the model. The resulting flow graphs provide detailed insights into how features change and interact during processing. Additionally, the method allows for targeted manipulation of model behavior by adjusting specific features, enhancing interpretability and control in text generation tasks.'}, 'zh': {'title': '特征映射：引导大型语言模型的新方法', 'desc': '本文提出了一种新方法，系统地映射稀疏自编码器在大型语言模型中各层发现的特征。我们使用无数据的余弦相似度技术，追踪特定特征在每个阶段的持续、转变或首次出现。这种方法生成了特征演变的细粒度流图，增强了模型计算的可解释性和机制洞察力。我们还展示了如何通过放大或抑制选定特征，直接引导模型行为，实现文本生成中的主题控制。'}}}, {'id': 'https://huggingface.co/papers/2502.03544', 'title': 'Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2', 'url': 'https://huggingface.co/papers/2502.03544', 'abstract': 'We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input.', 'score': 28, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '93cdc6bc9f5f22d7', 'authors': ['Yuri Chervonyi', 'Trieu H. Trinh', 'Miroslav Olšák', 'Xiaomeng Yang', 'Hoang Nguyen', 'Marcelo Menegali', 'Junehyuk Jung', 'Vikas Verma', 'Quoc V. Le', 'Thang Luong'], 'affiliations': ['Brown University', 'Georgia Institute of Technology', 'Google DeepMind', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2502.03544.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#training', '#architecture', '#optimization', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'ИИ превосходит человека в олимпиадной геометрии', 'desc': 'AlphaGeometry2 - это улучшенная версия системы для решения олимпиадных задач по геометрии, превзошедшая средний уровень золотого медалиста. Разработчики расширили язык AlphaGeometry для решения более сложных задач, включая движение объектов и линейные уравнения с углами, отношениями и расстояниями. Система использует архитектуру Gemini для улучшенного языкового моделирования и новый механизм обмена знаниями, комбинирующий несколько деревьев поиска. AlphaGeometry2 достигла 84% успешности решения геометрических задач за последние 25 лет Международных математических олимпиад.'}, 'en': {'title': 'AlphaGeometry2: Mastering Olympiad Geometry with Advanced AI', 'desc': "AlphaGeometry2 is an advanced version of the original AlphaGeometry, designed to solve complex Olympiad geometry problems more effectively. It enhances the language capabilities to handle intricate scenarios involving object movements and linear equations related to angles and distances. The model's performance has improved significantly, increasing its problem coverage from 66% to 88% for International Math Olympiad geometry problems. With a new search process utilizing Gemini architecture and a knowledge-sharing mechanism, AlphaGeometry2 achieves an impressive solving rate of 84%, marking a substantial leap from its predecessor."}, 'zh': {'title': 'AlphaGeometry2：几何问题解决的新突破', 'desc': 'AlphaGeometry2是对AlphaGeometry的显著改进版本，能够超越平均金牌得主在解决奥林匹克几何问题上的表现。它扩展了原有的语言，能够处理更复杂的对象运动和包含角度、比例及距离的线性方程问题。通过使用Gemini架构和新颖的知识共享机制，AlphaGeometry2的搜索过程得到了显著提升，几何问题的解决率达到了84%。该系统还朝着从自然语言输入直接解决几何问题的全自动化系统迈出了重要一步。'}}}, {'id': 'https://huggingface.co/papers/2502.03621', 'title': 'DynVFX: Augmenting Real Videos with Dynamic Content', 'url': 'https://huggingface.co/papers/2502.03621', 'abstract': 'We present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time. The position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene, resulting in a cohesive and realistic output video. We achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail. Specifically, we introduce a novel inference-based method that manipulates features within the attention mechanism, enabling accurate localization and seamless integration of the new content while preserving the integrity of the original scene. Our method is fully automated, requiring only a simple user instruction. We demonstrate its effectiveness on a wide range of edits applied to real-world videos, encompassing diverse objects and scenarios involving both camera and object motion.', 'score': 23, 'issue_id': 2089, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '8c22f8cda7e633d5', 'authors': ['Danah Yatim', 'Rafail Fridman', 'Omer Bar-Tal', 'Tali Dekel'], 'affiliations': ['Pika Labs, USA', 'Weizmann Institute of Science, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2502.03621.jpg', 'data': {'categories': ['#inference', '#video', '#multimodal', '#synthetic', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Генерация динамического контента в видео по текстовым инструкциям', 'desc': 'Авторы представляют метод для дополнения реальных видео новым динамическим контентом на основе текстовых инструкций пользователя. Метод использует предобученные модели text-to-video и Vision Language Model для синтеза и интеграции нового контента в исходное видео. Предложен новый подход на основе манипуляции признаками в механизме внимания для точной локализации и бесшовной интеграции нового контента. Метод полностью автоматизирован и требует только простой инструкции от пользователя, демонстрируя эффективность на широком спектре редактирований реальных видео.'}, 'en': {'title': 'Seamless Video Augmentation with Dynamic Content', 'desc': 'This paper introduces a novel approach for enhancing real-world videos by adding dynamic content based on user instructions. The method utilizes a pre-trained text-to-video diffusion transformer to generate new objects and effects that interact naturally with the existing scene. It employs a zero-shot, training-free framework that ensures seamless integration of the new content while considering factors like camera motion and occlusions. The approach is fully automated, allowing users to simply provide text instructions to achieve realistic video augmentation.'}, 'zh': {'title': '自动化视频增强，轻松生成动态内容！', 'desc': '我们提出了一种增强现实视频的新方法，可以生成动态内容。用户只需提供简单的文本指令，我们的方法就能合成与原始场景自然互动的动态对象或复杂场景效果。新内容的位置、外观和运动与原始视频无缝结合，同时考虑了相机运动、遮挡和其他动态对象的互动。该方法采用零-shot、无训练的框架，利用预训练的文本到视频扩散变换器和视觉语言模型，实现了自动化的内容合成。'}}}, {'id': 'https://huggingface.co/papers/2502.04320', 'title': 'ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features', 'url': 'https://huggingface.co/papers/2502.04320', 'abstract': 'Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP.', 'score': 19, 'issue_id': 2101, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '043b9ff9f5eaf227', 'authors': ['Alec Helbling', 'Tuna Han Salih Meral', 'Ben Hoover', 'Pinar Yanardag', 'Duen Horng Chau'], 'affiliations': ['Georgia Tech', 'IBM Research', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.04320.jpg', 'data': {'categories': ['#interpretability', '#transfer_learning', '#dataset', '#multimodal', '#cv', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'ConceptAttention: новый взгляд на интерпретируемость мультимодальных моделей', 'desc': 'Статья представляет новый метод ConceptAttention, который использует возможности слоев внимания мультимодальных диффузионных трансформеров (DiT) для создания высококачественных карт важности. Этот метод позволяет точно локализовать текстовые концепции на изображениях без дополнительного обучения. ConceptAttention превосходит существующие методы в задаче сегментации изображений с нулевым обучением на наборах данных ImageNet-Segmentation и PascalVOC. Исследование показывает, что представления моделей DiT, таких как Flux, хорошо переносятся на задачи компьютерного зрения, превосходя даже мультимодальные базовые модели вроде CLIP.'}, 'en': {'title': 'Enhancing Interpretability with ConceptAttention in Multi-Modal Transformers', 'desc': 'This paper explores the unique properties of multi-modal diffusion transformers (DiTs) and their interpretability through a new method called ConceptAttention. ConceptAttention utilizes the attention layers of DiTs to create detailed saliency maps that accurately identify textual concepts in images without needing extra training. The study reveals that linear projections in the output space of DiT attention layers lead to sharper saliency maps compared to traditional cross-attention methods. Additionally, ConceptAttention demonstrates superior performance in zero-shot image segmentation tasks, surpassing other interpretability techniques and showing the strong transferability of DiT representations to vision applications.'}, 'zh': {'title': '多模态扩散变换器的可解释性新突破', 'desc': '本文探讨了多模态扩散变换器（DiTs）在可解释性方面的独特性质。我们提出了一种新方法ConceptAttention，利用DiT注意力层的表达能力生成高质量的显著性图，准确定位图像中的文本概念。通过对DiT注意力层输出空间进行线性投影，ConceptAttention生成的显著性图比常用的交叉注意力机制更清晰。我们的研究首次证明了多模态DiT模型的表示在视觉任务（如分割）中具有高度可转移性，甚至超越了像CLIP这样的多模态基础模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04153', 'title': 'UltraIF: Advancing Instruction Following from the Wild', 'url': 'https://huggingface.co/papers/2502.04153', 'abstract': 'Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https://github.com/kkk-an/UltraIF.', 'score': 19, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '5c7c902f1effa8a3', 'authors': ['Kaikai An', 'Li Sheng', 'Ganqu Cui', 'Shuzheng Si', 'Ning Ding', 'Yu Cheng', 'Baobao Chang'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.04153.jpg', 'data': {'categories': ['#open_source', '#training', '#benchmark', '#alignment', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'UltraIF: простой метод для обучения LLM следовать сложным инструкциям', 'desc': 'Исследователи представили подход UltraIF для улучшения способности больших языковых моделей (LLM) следовать сложным инструкциям. Метод разбивает пользовательские запросы на более простые компоненты и использует специальную модель UltraComposer для составления сложных инструкций с вопросами для оценки. Эксперименты показали, что UltraIF позволяет значительно улучшить следование инструкциям у базовой модели LLaMA-3.1-8B без использования специальных данных. Подход также продемонстрировал возможность дальнейшего улучшения версии модели, уже обученной следовать инструкциям.'}, 'en': {'title': 'UltraIF: Simplifying Complex Instructions for LLMs', 'desc': 'This paper introduces UltraIF, a method designed to enhance large language models (LLMs) in following complex instructions using open-source data. The approach involves breaking down user prompts into simpler components, such as queries and constraints, and then training a model called UltraComposer to generate these structured prompts. By synthesizing complicated instructions and evaluating responses, UltraIF successfully aligns the LLaMA-3.1-8B-Base model with its instruct version on multiple benchmarks without prior benchmark data. Additionally, the method shows potential for improving existing instruct models through self-alignment, expanding its applicability in various scenarios.'}, 'zh': {'title': 'UltraIF：让大型语言模型更聪明的秘密武器', 'desc': '本文提出了一种名为UltraIF的方法，用于提高大型语言模型（LLMs）对复杂指令的理解能力。该方法通过将用户的真实请求分解为更简单的查询、约束和相应的评估问题来实现。接着，训练一个名为UltraComposer的模型，能够生成与约束相关的提示，并结合评估问题来过滤响应。实验表明，UltraIF成功地使LLaMA-3.1-8B-Base在多个指令跟随基准上与其指令版本对齐，展示了该方法的有效性和广泛应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.04313', 'title': 'Great Models Think Alike and this Undermines AI Oversight', 'url': 'https://huggingface.co/papers/2502.04313', 'abstract': 'As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as "AI Oversight". We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from "weak-to-strong generalization". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.', 'score': 18, 'issue_id': 2091, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a44111741eb33c43', 'authors': ['Shashwat Goel', 'Joschka Struber', 'Ilze Amanda Auzina', 'Karuna K Chandra', 'Ponnurangam Kumaraguru', 'Douwe Kiela', 'Ameya Prabhu', 'Matthias Bethge', 'Jonas Geiping'], 'affiliations': ['Contextual AI', 'ELLIS Institute Tubingen', 'IIIT Hyderabad', 'Max Planck Institute for Intelligent Systems', 'Stanford University', 'Tubingen AI Center', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2502.04313.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#interpretability', '#training', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Сходство языковых моделей: проблемы и риски AI-надзора', 'desc': "В статье рассматривается проблема оценки и контроля языковых моделей (ЯМ) с помощью других ЯМ, что авторы называют 'AI Oversight'. Исследуется влияние сходства моделей на эффективность такого надзора с использованием вероятностной метрики, основанной на пересечении ошибок. Обнаружено, что оценки ЯМ-судей смещены в пользу похожих моделей, а при обучении на аннотациях ЯМ важную роль играет дополняющее знание. Авторы отмечают тревожную тенденцию: с ростом возможностей ЯМ их ошибки становятся более схожими, что указывает на риски коррелированных сбоев."}, 'en': {'title': 'Navigating AI Oversight: Understanding Model Similarity and Its Risks', 'desc': 'This paper explores the challenges of evaluating and supervising advanced language models (LMs) as their capabilities grow. It introduces a probabilistic metric to measure LM similarity based on the overlap in their mistakes, which aids in understanding AI oversight. The study reveals that when using one LM to evaluate another, models that are similar tend to score each other favorably, indicating a potential bias. Additionally, it highlights the risks of correlated failures among models as they become more capable, emphasizing the need for careful reporting and correction of model similarities in AI oversight practices.'}, 'zh': {'title': 'AI监督：模型相似性的重要性', 'desc': '随着语言模型（LM）能力的提升，人类对其进行评估和监督变得越来越困难。我们提出了一种基于模型错误重叠的概率度量来研究模型相似性对AI监督的影响。研究表明，作为评判者的语言模型更倾向于偏好与其相似的模型，这与最近的自我偏好结果一致。此外，弱监督者与强学生模型之间的互补知识在“弱到强的泛化”中起着关键作用。'}}}, {'id': 'https://huggingface.co/papers/2502.04328', 'title': 'Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment', 'url': 'https://huggingface.co/papers/2502.04328', 'abstract': 'Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.', 'score': 18, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '0e25e42e93e9e560', 'authors': ['Zuyan Liu', 'Yuhao Dong', 'Jiahui Wang', 'Ziwei Liu', 'Winston Hu', 'Jiwen Lu', 'Yongming Rao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.04328.jpg', 'data': {'categories': ['#audio', '#open_source', '#video', '#training', '#multimodal'], 'emoji': '🦉', 'ru': {'title': 'Ola: Прогрессивное обучение для создания мощной омнимодальной модели', 'desc': 'В статье представлена Ola - омнимодальная языковая модель, способная понимать изображения, видео и аудио на уровне специализированных моделей. Ключевая особенность Ola - стратегия прогрессивного выравнивания модальностей, начинающаяся с изображений и текста, затем расширяющаяся на речь и видео. Модель использует небольшой объем кросс-модальных данных для обучения, что делает ее разработку менее затратной. Эксперименты показывают, что Ola превосходит существующие открытые омнимодальные модели по всем модальностям.'}, 'en': {'title': 'Ola: Bridging Modalities for Superior Understanding', 'desc': "This paper introduces Ola, an omni-modal language model designed to understand and process multiple types of data, including images, videos, and audio. Ola employs a progressive modality alignment strategy, starting with image and text, and gradually incorporating speech and video data to enhance its capabilities. The model's training pipeline is efficient, allowing it to maintain a smaller dataset for cross-modal alignment, making it easier and more cost-effective to develop. Experimental results show that Ola outperforms existing open omni-modal models and competes well with specialized models, aiming to contribute to future research in omni-modal understanding."}, 'zh': {'title': 'Ola：全模态理解的新突破', 'desc': '本文介绍了一种名为Ola的全模态语言模型，能够在图像、视频和音频理解方面与专门的单模态模型竞争。Ola的核心设计是逐步模态对齐策略，首先从图像和文本开始训练，然后逐步引入语音和视频数据。这样的训练流程使得跨模态对齐数据的规模相对较小，降低了开发全模态模型的成本。通过广泛的实验，Ola在所有模态上超越了现有的开放全模态语言模型，并在与同类专门模型的竞争中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.00473', 'title': 'Weak-to-Strong Diffusion with Reflection', 'url': 'https://huggingface.co/papers/2502.00473', 'abstract': 'The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability.', 'score': 17, 'issue_id': 2095, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '82d22d57d66f1a7a', 'authors': ['Lichen Bai', 'Masashi Sugiyama', 'Zeke Xie'], 'affiliations': ['RIKEN AIP', 'The University of Tokyo', 'xLeaF Lab, The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.00473.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#optimization', '#dataset', '#diffusion', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'W2SD: Преодоление разрыва между генеративными моделями и реальными данными', 'desc': 'Статья представляет новый фреймворк под названием Weak-to-Strong Diffusion (W2SD) для улучшения диффузионных генеративных моделей. W2SD использует разницу между слабыми и сильными моделями для аппроксимации разрыва между идеальной и сильной моделью. Метод применяет рефлексивную операцию, чередующую денойзинг и инверсию с учетом разницы weak-to-strong, что теоретически направляет латентные переменные к реальному распределению данных. Эксперименты показывают, что W2SD значительно улучшает предпочтения пользователей, эстетическое качество и соответствие промпту, достигая SOTA результатов в различных модальностях и архитектурах.'}, 'en': {'title': 'Bridging the Gap: Weak-to-Strong Diffusion for Enhanced Generative Models', 'desc': "This paper introduces Weak-to-Strong Diffusion (W2SD), a new framework designed to enhance diffusion generative models by addressing the gap between generated outputs and real data. W2SD leverages the differences between weak and strong models to better approximate the ideal model's performance. By alternating between denoising and inversion processes, it guides latent variables towards areas that closely resemble the real data distribution. The framework shows significant improvements in various applications, achieving state-of-the-art results while maintaining efficiency in computational resources."}, 'zh': {'title': '弱到强扩散：缩小生成与真实数据的差距', 'desc': '扩散生成模型的目标是通过梯度评分匹配将学习到的分布与真实数据分布对齐。然而，训练数据质量、建模策略和架构设计的固有限制导致生成输出与真实数据之间存在不可避免的差距。为了解决这个问题，我们提出了弱到强扩散（W2SD）框架，利用现有弱模型和强模型之间的差异来近似理想模型与强模型之间的差距。W2SD通过反射操作在去噪和反演之间交替，理论上引导潜在变量沿着采样轨迹朝向真实数据分布的区域，从而显著提高生成结果的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.04235', 'title': 'MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion', 'url': 'https://huggingface.co/papers/2502.04235', 'abstract': "Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose MAssive Genre-Audience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations.", 'score': 16, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'b1c2fec586443af8', 'authors': ['Xintong Hao', 'Ke Shen', 'Chenggang Li'], 'affiliations': ['Seed-LLM, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.04235.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#training', '#synthetic'], 'emoji': '🚀', 'ru': {'title': 'MAGA: преодоление ограничений данных для масштабирования языковых моделей', 'desc': 'Статья представляет метод MAGA для синтеза разнообразных и контекстуально богатых данных для предобучения языковых моделей. Авторы создали корпус MAGACorpus объемом 770 миллиардов токенов и продемонстрировали улучшение результатов для моделей различных размеров. Исследование также анализирует влияние инженерии промптов на синтетический коллапс обучения. Работа показывает, что MAGA может значительно расширить наборы данных для обучения, сохраняя их качество.'}, 'en': {'title': 'Expanding Language Models with MAGA: A Path Beyond Data Limitations', 'desc': 'This paper addresses the challenge of limited high-quality pretraining data for large language models. It introduces the MAssive Genre-Audience (MAGA) reformulation method, which creates diverse and contextually-rich pretraining data from existing sources. The authors present a new dataset called MAGACorpus, containing 770 billion tokens, and demonstrate its effectiveness across various model sizes. Additionally, they analyze the effects of prompt engineering on training stability and highlight the shortcomings of traditional metrics for detecting training collapse.'}, 'zh': {'title': 'MAGA：突破数据限制的预训练新方法', 'desc': '尽管大型语言模型在各种任务中表现出色，但它们在扩展时面临高质量预训练数据稀缺的挑战。为了解决这个瓶颈，我们提出了MAssive Genre-Audience（MAGA）重构方法，该方法系统地从现有语料库中合成多样化且富有上下文的预训练数据。我们的研究贡献包括提出了一种轻量级且可扩展的预训练语料库扩展方法，并构建了一个包含7700亿个标记的MAGACorpus。通过对不同数据预算扩展策略的评估，我们证明了在各种模型规模下（134M-13B）的一致性改进，展示了下一代大规模合成预训练语言模型的必要性。'}}}, {'id': 'https://huggingface.co/papers/2502.02358', 'title': 'MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm', 'url': 'https://huggingface.co/papers/2502.02358', 'abstract': 'Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.', 'score': 14, 'issue_id': 2088, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '967ac00db9aae918', 'authors': ['Ziyan Guo', 'Zeyu Hu', 'Na Zhao', 'De Wen Soh'], 'affiliations': ['LightSpeed Studios, Singapore', 'Singapore University of Technology and Design, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.02358.jpg', 'data': {'categories': ['#training', '#multimodal', '#cv', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Универсальный подход к генерации и редактированию движений человека', 'desc': 'Статья представляет новый подход к генерации и редактированию движений человека в компьютерной графике и компьютерном зрении. Авторы предлагают парадигму Motion-Condition-Motion и унифицированный фреймворк MotionLab, использующий исправленные потоки для отображения исходного движения в целевое. MotionLab включает в себя MotionFlow Transformer, выровненное позиционное кодирование вращения, модуляцию инструкций для конкретных задач и учебную программу движения для эффективного обучения. Фреймворк демонстрирует многообещающие возможности обобщения и эффективность вывода на нескольких эталонных тестах для движений человека.'}, 'en': {'title': 'Unifying Human Motion Generation and Editing with MotionLab', 'desc': 'This paper presents a new approach to human motion generation and editing called MotionLab, which aims to unify various motion-related tasks into a single framework. The proposed paradigm, Motion-Condition-Motion, allows for the integration of source motion, conditions, and target motion, facilitating better control and editing capabilities. MotionLab employs advanced techniques like the MotionFlow Transformer and Aligned Rotational Position Encoding to enhance the generation process and ensure synchronization between motions. Overall, this framework shows improved efficiency and generalization across different benchmarks, making it a versatile tool for real-world applications in computer graphics and vision.'}, 'zh': {'title': '统一人类运动生成与编辑的创新框架', 'desc': '人类运动生成和编辑是计算机图形学和视觉的重要组成部分。现有的方法往往针对特定任务提供孤立的解决方案，效率低下且不适用于实际应用。为了解决这些问题，我们提出了一种新的范式：运动条件运动（Motion-Condition-Motion），它通过源运动、条件和目标运动三个概念统一处理多种任务。基于这一范式，我们开发了MotionLab框架，能够有效地进行人类运动的生成和编辑，并在多个基准测试中展示了良好的泛化能力和推理效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03860', 'title': 'BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation', 'url': 'https://huggingface.co/papers/2502.03860', 'abstract': "Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM's LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities.", 'score': 14, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'b861ba86ae27e974', 'authors': ['Bo Pang', 'Hanze Dong', 'Jiacheng Xu', 'Silvio Savarese', 'Yingbo Zhou', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.03860.jpg', 'data': {'categories': ['#training', '#benchmark', '#math', '#long_context', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Самообучение ИИ сложным рассуждениям без подсказок', 'desc': 'Эта статья представляет новый подход к обучению больших языковых моделей (LLM) способности генерировать длинные цепочки рассуждений (LongCoT) без использования дистилляции знаний от существующих моделей. Метод BOLT включает три этапа: начальную генерацию данных LongCoT с помощью обучения в контексте, супервизорную донастройку и онлайн-обучение для дальнейшего улучшения способностей. Авторы применили свой метод к моделям различных масштабов и достигли впечатляющих результатов на ряде бенчмарков, оценивающих разнообразные способности решения задач и рассуждений. Этот подход позволяет развивать способности LLM к сложным рассуждениям без необходимости в дорогостоящих аннотациях или данных от существующих продвинутых моделей.'}, 'en': {'title': 'Empowering LLMs with Efficient Long Chain-of-Thought Bootstrapping', 'desc': "This paper presents a new method called Bootstrapping Long Chain-of-Thought (BOLT) to enhance the reasoning abilities of large language models (LLMs) without relying on knowledge distillation from existing models. BOLT consists of three stages: bootstrapping LongCoT data using in-context learning, supervised fine-tuning for LongCoT, and online training for further refinement. The approach requires only a few examples to start the bootstrapping process, making it efficient and scalable across different model sizes. Experimental results show that BOLT significantly improves performance on various reasoning and problem-solving benchmarks, demonstrating its effectiveness in developing LLMs' LongCoT capabilities."}, 'zh': {'title': '引导长链思维，提升推理能力！', 'desc': '本文介绍了一种新方法，旨在使大型语言模型（LLM）具备长链思维（LongCoT）能力，而无需依赖于类似o1模型的知识蒸馏或昂贵的人类标注。该方法称为BOLT，分为三个阶段：首先通过上下文学习从标准指令模型引导LongCoT数据，其次进行LongCoT的监督微调，最后进行在线训练以进一步提升LongCoT能力。实验中，我们仅构建了10个示例，证明了该方法的可行性。我们在多个基准测试上取得了显著的性能，展示了该方法在解决复杂问题和推理能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.04306', 'title': 'ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization', 'url': 'https://huggingface.co/papers/2502.04306', 'abstract': 'Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow', 'score': 13, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '51ace85d35c202d5', 'authors': ['Yinjie Wang', 'Ling Yang', 'Guohao Li', 'Mengdi Wang', 'Bryon Aragam'], 'affiliations': ['Princeton University', 'University of Chicago', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2502.04306.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#agents', '#rlhf', '#small_models'], 'emoji': '🚀', 'ru': {'title': 'ScoreFlow: эффективная оптимизация мультиагентных систем на основе ИИ', 'desc': 'В статье представлен новый фреймворк ScoreFlow для оптимизации рабочих процессов мультиагентных систем на основе больших языковых моделей. ScoreFlow использует градиентную оптимизацию в непрерывном пространстве, что позволяет преодолеть ограничения существующих методов. Ключевым компонентом является Score-DPO - новый вариант метода прямой оптимизации предпочтений, учитывающий количественную обратную связь. На шести тестовых задачах ScoreFlow показал улучшение результатов на 8.2% по сравнению с базовыми методами.'}, 'en': {'title': 'ScoreFlow: Optimizing Multi-Agent Systems with Continuous Gradient Techniques', 'desc': 'This paper introduces ScoreFlow, a new framework designed to enhance the performance of multi-agent systems in solving complex problems. It addresses the limitations of existing optimization methods by utilizing gradient-based optimization in a continuous space, which allows for greater flexibility and scalability. ScoreFlow features Score-DPO, a novel approach that incorporates quantitative feedback into the optimization process. The results show that ScoreFlow not only improves performance by 8.2% over previous methods but also enables smaller models to achieve better results than larger models at a lower cost.'}, 'zh': {'title': 'ScoreFlow：高效的多智能体优化框架', 'desc': '最近的研究利用大型语言模型的多智能体系统来解决复杂问题，同时努力减少构建这些系统所需的手动工作。现有方法由于表示限制、缺乏适应性和依赖离散优化技术而导致灵活性不足。我们提出了ScoreFlow，这是一个简单但高性能的框架，利用基于梯度的优化在连续空间中进行优化。ScoreFlow在六个基准测试中表现出色，超越了现有基线，并使较小的模型以更低的推理成本超越较大的模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04128', 'title': 'Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis', 'url': 'https://huggingface.co/papers/2502.04128', 'abstract': 'Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.', 'score': 12, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '196e7cb6a29a44ea', 'authors': ['Zhen Ye', 'Xinfa Zhu', 'Chi-Min Chan', 'Xinsheng Wang', 'Xu Tan', 'Jiahe Lei', 'Yi Peng', 'Haohe Liu', 'Yizhu Jin', 'Zheqi DAI', 'Hongzhan Lin', 'Jianyi Chen', 'Xingjian Du', 'Liumeng Xue', 'Yunlin Chen', 'Zhifei Li', 'Lei Xie', 'Qiuqiang Kong', 'Yike Guo', 'Wei Xue'], 'affiliations': ['ASLP Lab, Northwestern Polytechnical University', 'Chinese University of Hong Kong', 'Hong Kong Baptist University', 'Independent Researcher', 'Shanghai Mobvoi Information Technology Co., Ltd.', 'The Hong Kong University of Science and Technology', 'University of Rochester', 'University of Science and Technology Beijing', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2502.04128.jpg', 'data': {'categories': ['#open_source', '#dataset', '#audio', '#training', '#optimization'], 'emoji': '🗣️', 'ru': {'title': 'Llasa: масштабируемый синтез речи на основе единой языковой модели', 'desc': 'В статье представлена новая система синтеза речи Llasa, использующая одноуровневый векторный квантователь и единую архитектуру трансформера, аналогичную стандартным языковым моделям. Исследование показывает, что увеличение вычислительных ресурсов при обучении улучшает естественность синтезированной речи и позволяет генерировать более сложные просодические паттерны. Масштабирование вычислений во время вывода с использованием моделей понимания речи в качестве верификаторов улучшает эмоциональную выразительность, согласованность тембра и точность содержания. Авторы опубликовали контрольные точки и код обучения для своей модели синтеза речи.'}, 'en': {'title': 'Simplifying Speech Synthesis with Scalable LLMs', 'desc': 'This paper discusses advancements in text-to-speech (TTS) systems that utilize large language models (LLMs) like GPT and o1. It introduces Llasa, a new framework that simplifies speech synthesis by using a single-layer vector quantizer and a Transformer architecture, aligning it with standard LLMs. The research shows that increasing training compute enhances the naturalness and complexity of the generated speech. Additionally, it highlights how scaling inference compute can improve emotional expressiveness and accuracy by using speech understanding models as verifiers during the synthesis process.'}, 'zh': {'title': '简化语音合成，提升自然性与情感表达', 'desc': '本文探讨了文本基础的大型语言模型（LLMs）在语音合成中的应用，特别是GPT系列和o1模型的最新进展。我们提出了一种名为Llasa的简单框架，使用单层向量量化（VQ）编解码器和单一Transformer架构，旨在简化多阶段的语音合成过程。实验结果表明，增加训练时间的计算资源可以显著提高合成语音的自然性，并生成更复杂的韵律模式。此外，我们还发现，在推理时间增加计算资源可以改善情感表现、音色一致性和内容准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.04299', 'title': 'MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.04299', 'abstract': 'This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications.', 'score': 10, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '91b39568cf3793e2', 'authors': ['Jinbo Xing', 'Long Mai', 'Cusuh Ham', 'Jiahui Huang', 'Aniruddha Mahapatra', 'Chi-Wing Fu', 'Tien-Tsin Wong', 'Feng Liu'], 'affiliations': ['Adobe Research', 'Monash University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04299.jpg', 'data': {'categories': ['#multimodal', '#video', '#diffusion', '#3d', '#games'], 'emoji': '🎬', 'ru': {'title': 'MotionCanvas: Интуитивное проектирование видеокадров с контролем движения', 'desc': 'Эта статья представляет метод MotionCanvas, позволяющий пользователям проектировать кинематографические видеокадры в контексте генерации видео из изображений. Метод интегрирует пользовательское управление в модели I2V, позволяя контролировать движения объектов и камеры с учетом сцены. MotionCanvas соединяет классическую компьютерную графику и современные методы генерации видео, обеспечивая 3D-осведомленное управление движением без необходимости в дорогостоящих 3D-данных для обучения. Метод позволяет интуитивно описывать намерения движения в пространстве сцены и преобразовывать их в пространственно-временные сигналы для обусловливания движения в диффузионных моделях видео.'}, 'en': {'title': 'Empowering Cinematic Creativity with MotionCanvas', 'desc': 'This paper introduces MotionCanvas, a novel method for designing cinematic video shots in image-to-video generation. It addresses two main challenges: capturing user intentions for both camera and object motions, and effectively representing this motion information for video diffusion models. By integrating user-driven controls, MotionCanvas allows for intuitive scene-aware motion design without the need for expensive 3D training data. The method enhances creative workflows in digital content creation, making it easier for users to specify and visualize their motion intentions in video synthesis.'}, 'zh': {'title': '直观设计电影镜头，提升视频生成体验', 'desc': '本文提出了一种方法，使用户能够在图像到视频生成的背景下设计电影镜头。镜头设计是电影制作中的关键环节，涉及到对相机运动和场景中物体运动的精心规划。我们介绍了MotionCanvas，这是一种将用户驱动的控制集成到图像到视频生成模型中的方法，允许用户以场景感知的方式控制物体和相机的运动。通过将经典计算机图形学的见解与现代视频生成技术相结合，我们展示了在不需要昂贵的3D训练数据的情况下，实现I2V合成中的3D感知运动控制的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.04270', 'title': 'PILAF: Optimal Human Preference Sampling for Reward Modeling', 'url': 'https://huggingface.co/papers/2502.04270', 'abstract': 'As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.', 'score': 7, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '5c0dd6afed760a2b', 'authors': ['Yunzhen Feng', 'Ariel Kwiatkowski', 'Kunhao Zheng', 'Julia Kempe', 'Yaqi Duan'], 'affiliations': ['Meta FAIR', 'NYU'], 'pdf_title_img': 'assets/pdf/title_img/2502.04270.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'PILAF: точное согласование ИИ с человеческими ценностями', 'desc': 'Статья представляет новый метод обучения языковых моделей с учетом человеческих ценностей - PILAF (Policy-Interpolated Learning for Aligned Feedback). Этот подход улучшает существующую технику обучения с подкреплением на основе обратной связи от человека (RLHF). PILAF предлагает стратегию выборки ответов для маркировки предпочтений, которая явно согласует обучение предпочтениям с максимизацией базовой награды. Метод демонстрирует сильную производительность в итеративных и онлайн-настройках RLHF, где критически важна курация обратной связи.'}, 'en': {'title': 'Aligning AI with Human Values through PILAF', 'desc': 'This paper introduces Policy-Interpolated Learning for Aligned Feedback (PILAF), a new method for improving Reinforcement Learning from Human Feedback (RLHF). PILAF focuses on aligning preference learning with the true human values by using a response sampling strategy that enhances reward model accuracy. The approach is theoretically sound, showing optimal performance in both optimization and statistical contexts. It is easy to implement and performs well in scenarios where feedback curation is essential, making it a valuable tool for aligning large language models with human values.'}, 'zh': {'title': '政策插值学习：对齐人类反馈的新策略', 'desc': '随着大型语言模型在实际应用中的广泛使用，使其与人类价值观保持一致变得至关重要。强化学习与人类反馈（RLHF）成为了一种关键技术，它将偏好数据转化为奖励模型，尤其是在无法获取人类价值观的情况下。我们提出了一种新的响应采样策略，称为政策插值学习（PILAF），它明确将偏好学习与最大化基础奖励对齐。PILAF在理论上是有依据的，从优化和统计的角度都展示了其最优性，并且在反馈策划至关重要的迭代和在线RLHF环境中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.04295', 'title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'url': 'https://huggingface.co/papers/2502.04295', 'abstract': 'Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code will be available at https://github.com/HenryLau7/CFPO.', 'score': 7, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'f72f46ad2c1b9853', 'authors': ['Yuanye Liu', 'Jiahang Xu', 'Li Lyna Zhang', 'Qi Chen', 'Xuan Feng', 'Yang Chen', 'Zhongxin Guo', 'Yuqing Yang', 'Cheng Peng'], 'affiliations': ['Fudan University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04295.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training'], 'emoji': '🧬', 'ru': {'title': 'Интегрированная оптимизация формы и содержания промптов повышает эффективность LLM', 'desc': 'Статья представляет новую методологию оптимизации промптов для больших языковых моделей (LLM), называемую CFPO. Этот подход оптимизирует как содержание, так и форматирование промптов через итеративный процесс уточнения. CFPO использует мутации естественного языка для исследования вариаций содержания и применяет динамическую стратегию исследования форматов. Результаты показывают значительное улучшение производительности по сравнению с методами оптимизации, ориентированными только на содержание.'}, 'en': {'title': 'Enhancing LLMs with Integrated Content and Format Optimization', 'desc': 'This paper presents a new method called Content-Format Integrated Prompt Optimization (CFPO) that improves the performance of Large Language Models (LLMs) by optimizing both the content and formatting of prompts. The authors argue that while prompt content has been the focus of recent research, the formatting aspect is equally important and has not been thoroughly explored. CFPO uses natural language mutations to create variations in content and a dynamic strategy to test different formatting options. The results show that this integrated approach leads to better performance in various tasks compared to methods that only optimize content.'}, 'zh': {'title': '内容与格式的完美结合，提升LLM性能！', 'desc': '大型语言模型（LLMs）在各种任务中表现出色，其实际效果往往依赖于提示设计。尽管最近的研究主要集中在优化提示内容上，但提示格式的作用却被忽视，缺乏系统性的研究。本文提出了一种新的方法——内容格式集成提示优化（CFPO），通过迭代优化过程同时优化提示内容和格式。我们的评估表明，CFPO在多个任务和开源LLMs上相较于仅优化内容的方法，显示出显著的性能提升，强调了内容与格式集成优化的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.00989', 'title': 'ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution', 'url': 'https://huggingface.co/papers/2502.00989', 'abstract': 'Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.', 'score': 6, 'issue_id': 2092, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '4d26f9419f20aadb', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['ACM, New York, NY, USA', 'IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00989.jpg', 'data': {'categories': ['#interpretability', '#hallucinations', '#agents', '#cv', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'Точные ответы по графикам с доказательствами от ИИ', 'desc': 'ChartCitor - это мультиагентная система для ответов на вопросы по графикам с использованием больших языковых моделей (LLM). Она решает проблему галлюцинаций и необоснованных ответов путем предоставления точных ссылок на области изображения графика. Система включает в себя извлечение данных из графика в таблицу, переформулировку вопроса, дополнение таблицы, поиск доказательств и сопоставление таблицы с графиком. ChartCitor превосходит существующие методы и повышает доверие пользователей к генеративному ИИ.'}, 'en': {'title': 'ChartCitor: Enhancing Trust in AI with Accurate Chart Question-Answering', 'desc': "This paper introduces ChartCitor, a multi-agent framework designed to improve the accuracy of question-answering tasks involving charts by addressing the issue of hallucinated responses from Large Language Models (LLMs). It enhances answer attribution by providing precise bounding box citations that link responses to specific parts of chart images, overcoming challenges related to visual-semantic context and complex layouts. The framework includes processes for chart-to-table extraction, answer reformulation, and evidence retrieval, which collectively improve the reliability of the generated answers. User studies indicate that ChartCitor not only boosts the performance of LLMs in chart QA tasks but also increases user trust and productivity by offering clearer explanations of the AI's reasoning."}, 'zh': {'title': 'ChartCitor：提升图表问答的可信度与效率', 'desc': '大型语言模型（LLMs）可以执行图表问答任务，但常常生成未经验证的虚假回答。现有的答案归属方法由于视觉语义上下文有限、复杂的视觉文本对齐要求以及在复杂布局中进行边界框预测的困难，难以将回答与源图表关联。我们提出了ChartCitor，一个多代理框架，通过识别图表图像中的支持证据来提供细粒度的边界框引用。ChartCitor在不同图表类型上超越了现有基准，增强了用户对生成式人工智能的信任，并提高了专业人士的工作效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03639', 'title': 'Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach', 'url': 'https://huggingface.co/papers/2502.03639', 'abstract': 'We present a novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune a latent diffusion model, enabling it to track 2D objects with 3D Cartesian coordinates. Building on this, we regularize the shape and motion of objects in the video to eliminate undesired artifacts, \\eg, nonphysical deformation. Consequently, we enhance the quality of generated RGB videos and alleviate common issues like object morphing, which are prevalent in current video models due to a lack of shape awareness. With our 3D augmentation and regularization, our model is capable of handling contact-rich scenarios such as task-oriented videos. These videos involve complex interactions of solids, where 3D information is essential for perceiving deformation and contact. Furthermore, our model improves the overall quality of video generation by promoting the 3D consistency of moving objects and reducing abrupt changes in shape and motion.', 'score': 6, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '7875c341693f4d1e', 'authors': ['Yunuo Chen', 'Junli Cao', 'Anil Kag', 'Vidit Goel', 'Sergei Korolev', 'Chenfanfu Jiang', 'Sergey Tulyakov', 'Jian Ren'], 'affiliations': ['Snap Inc.', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2502.03639.jpg', 'data': {'categories': ['#diffusion', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': '3D-осведомленная генерация видео с улучшенной геометрией и динамикой', 'desc': 'Представлена новая система генерации видео, объединяющая трехмерную геометрию и динамическое восприятие. Двумерные видео дополняются трехмерными траекториями точек и выравниваются в пиксельном пространстве. Полученный набор данных PointVid используется для дообучения модели латентной диффузии, что позволяет отслеживать 2D объекты с помощью 3D координат. Регуляризация формы и движения объектов улучшает качество генерируемых RGB видео и устраняет нежелательные артефакты.'}, 'en': {'title': 'Enhancing Video Generation with 3D Awareness', 'desc': 'This paper introduces a new framework for generating videos that combines 3D geometry with an understanding of dynamic movements. It creates a dataset called PointVid by enhancing 2D videos with 3D point trajectories, which helps the model learn to track objects in three dimensions. The framework uses a latent diffusion model that is fine-tuned to ensure that the shapes and motions of objects are realistic, reducing issues like nonphysical deformation. By focusing on 3D consistency, the model improves the quality of generated videos, especially in scenarios where objects interact closely, such as in task-oriented videos.'}, 'zh': {'title': '三维感知，提升视频生成质量', 'desc': '我们提出了一种新的视频生成框架，结合了三维几何和动态感知。通过在像素空间中对齐三维点轨迹，我们增强了二维视频，创建了一个三维感知的视频数据集PointVid。利用这个数据集，我们对潜在扩散模型进行微调，使其能够跟踪具有三维坐标的二维物体。我们的模型通过正则化物体的形状和运动，消除了不必要的伪影，提高了生成RGB视频的质量，特别是在处理复杂的接触场景时。'}}}, {'id': 'https://huggingface.co/papers/2502.00988', 'title': 'PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback', 'url': 'https://huggingface.co/papers/2502.00988', 'abstract': 'Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.', 'score': 5, 'issue_id': 2094, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '9548b306478edf6d', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00988.jpg', 'data': {'categories': ['#cv', '#agents', '#science', '#dataset', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'PlotGen: ИИ-помощник для создания точных научных визуализаций', 'desc': 'Статья представляет PlotGen - новую мультиагентную систему для автоматического создания научных визуализаций данных. Система использует несколько агентов на основе больших языковых моделей (LLM) для планирования, генерации кода и итеративного улучшения графиков. PlotGen включает агентов для разбиения сложных запросов, генерации Python-кода и многомодальной обратной связи для уточнения точности данных, текстовых меток и визуальной корректности. Эксперименты показывают, что PlotGen превосходит базовые методы на 4-6% на датасете MatPlotBench, повышая доверие пользователей к визуализациям, созданным с помощью LLM.'}, 'en': {'title': 'Automating Scientific Visualization with PlotGen', 'desc': 'This paper introduces PlotGen, a multi-agent framework designed to automate the creation of accurate scientific visualizations. It utilizes several Large Language Model (LLM)-based agents to streamline the process, including a Query Planning Agent for breaking down user requests and a Code Generation Agent for converting pseudocode into Python code. Additionally, it employs retrieval feedback agents that enhance the quality of visualizations by refining data accuracy and visual elements through iterative self-reflection. Experimental results demonstrate that PlotGen significantly improves visualization accuracy and reduces debugging time, thereby increasing user trust and productivity for novice users.'}, 'zh': {'title': '自动化科学可视化的未来', 'desc': '科学数据可视化对于将原始数据转化为易于理解的视觉表示至关重要，能够帮助识别模式和预测结果。新手用户在选择合适工具和掌握可视化技术时常常面临困难。本文提出了一种名为PlotGen的新型多代理框架，旨在自动化创建精确的科学可视化。通过多个基于大语言模型的代理，PlotGen能够有效地分解用户请求并生成高质量的可视化图表。'}}}, {'id': 'https://huggingface.co/papers/2501.19085', 'title': 'Enhancing Code Generation for Low-Resource Languages: No Silver Bullet', 'url': 'https://huggingface.co/papers/2501.19085', 'abstract': "The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights.", 'score': 4, 'issue_id': 2094, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '8c0ab784750b1038', 'authors': ['Alessandro Giagnorio', 'Alberto Martin-Lopez', 'Gabriele Bavota'], 'affiliations': ['Software Institute USI Università della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2501.19085.jpg', 'data': {'categories': ['#training', '#low_resource', '#plp', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'Повышение эффективности языковых моделей для малоресурсных языков программирования', 'desc': 'Исследование посвящено улучшению генерации кода языковыми моделями (ЯМ) для малоресурсных языков программирования. Авторы сравнивают эффективность различных подходов, включая дообучение, обучение в контексте и предобучение на задаче перевода. Результаты показывают, что для небольших ЯМ лучше всего работает дообучение, а для крупных - обучение в контексте. Очень большие ЯМ могут ухудшать производительность при дообучении из-за недостатка данных.'}, 'en': {'title': 'Boosting Code Generation for Low-Resource Languages with LLMs', 'desc': 'This paper explores how Large Language Models (LLMs) can be improved for generating code in low-resource programming languages, which have limited training data. It examines various methods such as fine-tuning, in-context learning, and a pre-training objective that translates between high- and low-resource languages. The study finds that smaller LLMs benefit most from fine-tuning, while larger models perform better with in-context learning due to their architecture. However, fine-tuning large LLMs can lead to worse performance on low-resource languages because they require more data to adjust their parameters effectively.'}, 'zh': {'title': '提升低资源语言代码生成的有效策略', 'desc': '大型语言模型（LLMs）的出现显著推动了自动代码生成领域的发展。这些模型依赖于大量多样化的数据集来学习编程语言的语法、语义和使用模式。然而，对于低资源语言（即训练数据稀缺的小众编程语言），数据的有限性限制了模型的泛化能力，导致代码生成性能较差。因此，本文研究了几种提升LLMs在低资源语言上表现的有效方法，包括经典的微调和几种上下文学习变体。'}}}, {'id': 'https://huggingface.co/papers/2502.04296', 'title': 'Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression', 'url': 'https://huggingface.co/papers/2502.04296', 'abstract': 'We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. \\ourshort achieves better visual fidelity and controllability than the previous robotic video generation models with 15 times faster speed in the real world. After post-training, this model can be used as a video simulator from low-level action inputs for evaluating policies and generating synthetic data. See this link https://liruiw.github.io/hma for more information.', 'score': 4, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'da9d11d5ea5d9d9e', 'authors': ['Lirui Wang', 'Kevin Zhao', 'Chaoqi Liu', 'Xinlei Chen'], 'affiliations': ['MIT', 'Meta, FAIR', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.04296.jpg', 'data': {'categories': ['#games', '#robotics', '#dataset', '#video', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'HMA: Быстрое и качественное моделирование видео для обучения роботов', 'desc': 'Статья представляет новый метод Heterogeneous Masked Autoregression (HMA) для моделирования динамики видео действий в робототехнике. HMA использует гетерогенное предобучение на наблюдениях и последовательностях действий из различных роботизированных воплощений, доменов и задач. Метод применяет маскированную авторегрессию для генерации квантованных или мягких токенов для предсказания видео. HMA достигает лучшего визуального качества и управляемости по сравнению с предыдущими моделями генерации видео для роботов, работая в 15 раз быстрее в реальном мире.'}, 'en': {'title': 'Revolutionizing Robot Learning with Heterogeneous Video Modeling', 'desc': 'The paper introduces Heterogeneous Masked Autoregression (HMA), a novel approach for modeling the dynamics of action videos to enhance robot learning. HMA addresses the challenges of diverse environments and the need for real-time computational efficiency by leveraging heterogeneous pre-training from various robotic tasks and domains. By employing masked autoregression, HMA generates high-quality video predictions using quantized or soft tokens. The results show that HMA significantly improves visual fidelity and controllability while operating 15 times faster than previous models, making it a valuable tool for simulating video from low-level actions and evaluating robotic policies.'}, 'zh': {'title': '异构掩蔽自回归：提升机器人学习的视频生成', 'desc': '我们提出了异构掩蔽自回归（HMA）模型，用于建模动作视频的动态，以生成高质量的数据并评估机器人学习的扩展性。构建交互式视频世界模型和机器人策略面临挑战，因为需要处理多样化的环境，同时保持实时运行的计算效率。HMA通过对不同机器人形态、领域和任务的观察和动作序列进行异构预训练，来提高模型的性能。经过后期训练，该模型可以作为视频模拟器，从低级动作输入中评估策略并生成合成数据。'}}}, {'id': 'https://huggingface.co/papers/2502.04322', 'title': 'Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions', 'url': 'https://huggingface.co/papers/2502.04322', 'abstract': 'Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.', 'score': 3, 'issue_id': 2090, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '1ad4a9febd48be28', 'authors': ['Yik Siu Chan', 'Narutatsu Ri', 'Yuxin Xiao', 'Marzyeh Ghassemi'], 'affiliations': ['Brown University', 'Columbia University', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.04322.jpg', 'data': {'categories': ['#dataset', '#ethics', '#multilingual', '#benchmark', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Простое взаимодействие - скрытая угроза безопасности языковых моделей', 'desc': 'Статья посвящена проблеме уязвимостей больших языковых моделей (LLM) к атакам, направленным на обход систем безопасности. Авторы предлагают новую метрику HarmScore для оценки эффективности вредоносных ответов LLM и разрабатывают фреймворк атаки Speak Easy, основанный на многошаговом многоязычном взаимодействии. Исследование показывает, что простые паттерны взаимодействия могут быть легко использованы злоумышленниками для вредоносных целей. Результаты демонстрируют значительное увеличение успешности атак и показателя HarmScore при применении Speak Easy к различным LLM.'}, 'en': {'title': 'Uncovering Hidden Vulnerabilities in Language Models', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks that can lead to harmful behavior. It explores whether these attacks are useful for average users and if safety issues arise in typical human-LLM interactions. The authors introduce HarmScore, a metric to evaluate how well LLM responses can facilitate harmful actions, and Speak Easy, a framework for executing multi-step, multilingual attacks. Their findings indicate that malicious users can exploit common interaction patterns, significantly increasing the success rate of harmful actions.'}, 'zh': {'title': '揭示大型语言模型的安全漏洞', 'desc': '尽管已经进行了广泛的安全对齐工作，大型语言模型（LLMs）仍然容易受到越狱攻击，这会引发有害行为。本文探讨了两个关键问题：越狱响应是否真正帮助普通用户实施有害行为，以及在更常见的人类与LLM的简单互动中是否存在安全漏洞。我们提出了HarmScore，这是一种评估LLM响应如何有效促进有害行为的指标，并介绍了Speak Easy，一个简单的多步骤、多语言攻击框架。研究表明，恶意用户可以轻松利用常见的互动模式来实现有害意图。'}}}, {'id': 'https://huggingface.co/papers/2502.02492', 'title': 'VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models', 'url': 'https://huggingface.co/papers/2502.02492', 'abstract': "Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/", 'score': 23, 'issue_id': 2042, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '33581479f8c6ed9f', 'authors': ['Hila Chefer', 'Uriel Singer', 'Amit Zohar', 'Yuval Kirstain', 'Adam Polyak', 'Yaniv Taigman', 'Lior Wolf', 'Shelly Sheynin'], 'affiliations': ['GenAI, Meta', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.02492.jpg', 'data': {'categories': ['#video'], 'emoji': '🎬', 'ru': {'title': 'VideoJAM: Реалистичное движение в генеративных видеомоделях', 'desc': 'Статья представляет VideoJAM - новый подход к генерации видео, решающий проблему недостаточной реалистичности движения в существующих моделях. Авторы предлагают обучать модель совместному представлению внешнего вида и движения, а также вводят механизм Inner-Guidance для улучшения когерентности движения при генерации. VideoJAM может быть применен к любой видеомодели без существенных изменений и показывает лучшие результаты по сравнению с существующими решениями. Исследование демонстрирует, что интеграция внешнего вида и движения улучшает как визуальное качество, так и согласованность генерируемого видео.'}, 'en': {'title': 'Enhancing Video Generation with Motion Coherence', 'desc': 'This paper addresses the challenges faced by generative video models in accurately capturing real-world motion and dynamics. The authors identify that traditional pixel reconstruction methods prioritize visual appearance over motion coherence, leading to less realistic video outputs. To overcome this, they propose VideoJAM, a framework that integrates a motion prior into video generation by learning a combined representation of appearance and motion. By extending the training objective and introducing a dynamic guidance mechanism during inference, VideoJAM significantly improves motion coherence and visual quality, outperforming existing models without requiring changes to training data or model architecture.'}, 'zh': {'title': 'VideoJAM：提升视频生成的运动一致性与视觉质量', 'desc': '尽管生成视频模型在最近取得了巨大进展，但仍然难以捕捉真实世界的运动和动态。本文提出了VideoJAM框架，通过引入有效的运动先验，帮助视频生成器学习联合的外观-运动表示。该框架在训练过程中扩展了目标，预测生成的像素及其对应的运动，并在推理阶段引入了内部引导机制，以实现一致的运动生成。VideoJAM在运动一致性方面达到了最先进的性能，同时提升了生成视频的视觉质量，表明外观和运动可以互为补充，合理整合后能增强视频生成的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.01362', 'title': 'Inverse Bridge Matching Distillation', 'url': 'https://huggingface.co/papers/2502.01362', 'abstract': 'Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose a novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in a one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on a wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup.', 'score': 22, 'issue_id': 2045, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '061049a23278b0f6', 'authors': ['Nikita Gushchin', 'David Li', 'Daniil Selikhanovych', 'Evgeny Burnaev', 'Dmitry Baranchuk', 'Alexander Korotin'], 'affiliations': ['Skolkovo Institute of Science and Technology', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01362.jpg', 'data': {'categories': ['#inference', '#optimization', '#cv', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных мостов: искусство эффективной дистилляции', 'desc': 'Статья представляет новый метод дистилляции для моделей диффузионного моста (DBM). Этот подход позволяет ускорить вывод DBM от 4 до 100 раз, сохраняя или даже улучшая качество генерации. Метод основан на формулировке обратного сопоставления моста и может применяться как к условным, так и к безусловным DBM. Техника была успешно протестирована на различных задачах, включая суперразрешение и восстановление JPEG.'}, 'en': {'title': 'Accelerating Diffusion Bridge Models with Innovative Distillation Techniques', 'desc': 'This paper introduces a new method to improve the speed and practicality of diffusion bridge models (DBMs), which are used for tasks like image translation. The authors present a distillation technique that leverages inverse bridge matching to create a more efficient training process. This method allows for the distillation of both conditional and unconditional DBMs, using only corrupted images, and enables one-step generation. The results show that their approach can significantly speed up inference times by up to 100 times while maintaining or even enhancing the quality of generated images compared to the original models.'}, 'zh': {'title': '加速扩散桥模型，提升生成质量！', 'desc': '扩散桥模型（DBMs）是一种有前景的扩展，适用于图像到图像的转换。然而，DBMs在推理时速度较慢，这是现代扩散和流模型普遍面临的问题。为了解决这个问题，我们提出了一种基于逆桥匹配的蒸馏技术，并推导出可行的目标来实际解决它。我们的蒸馏方法能够同时处理条件和无条件的DBMs，并且只使用损坏的图像进行训练，从而显著加快推理速度。'}}}, {'id': 'https://huggingface.co/papers/2502.01718', 'title': 'ACECODER: Acing Coder RL via Automated Test-Case Synthesis', 'url': 'https://huggingface.co/papers/2502.01718', 'abstract': 'Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.', 'score': 15, 'issue_id': 2041, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'b5b43fe7221df9d8', 'authors': ['Huaye Zeng', 'Dongfu Jiang', 'Haozhe Wang', 'Ping Nie', 'Xiaotong Chen', 'Wenhu Chen'], 'affiliations': ['HKUST', 'Independent Researcher', 'Netmind.AI', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2502.01718.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Революция в обучении моделей кода: мощь RL и автоматизированных тест-кейсов', 'desc': 'В этой статье представлен новый подход к обучению моделей для генерации кода с использованием обучения с подкреплением (RL). Авторы разработали пайплайн для создания пар (вопрос, тест-кейсы) из существующих кодовых данных, которые затем используются для обучения моделей вознаграждения. Применение этого метода привело к значительным улучшениям производительности моделей на различных бенчмарках, включая HumanEval и MBPP. Результаты исследования показывают большой потенциал обучения с подкреплением в области моделей для генерации кода.'}, 'en': {'title': 'Unlocking the Power of Reinforcement Learning in Coder Models', 'desc': 'This paper explores the use of reinforcement learning (RL) to improve coder models, which have primarily relied on supervised fine-tuning (SFT). The authors introduce a method for generating large-scale test-case pairs from existing code, which helps create reliable reward signals for training. By employing a preference-based reward model using the Bradley-Terry loss, they achieve significant performance gains in various coding benchmarks. The results demonstrate that RL can substantially enhance the capabilities of coder models, showcasing its untapped potential in this domain.'}, 'zh': {'title': '强化学习提升代码模型的潜力', 'desc': '本文探讨了在代码模型训练中使用强化学习（RL）的潜力，尤其是在缺乏可靠奖励数据的情况下。我们设计了一种自动化的大规模测试用例合成管道，以生成大量（问题，测试用例）对，从而增强代码模型的训练。通过使用这些测试用例，我们构建了基于通过率的偏好对，并利用Bradley-Terry损失训练奖励模型。实验结果表明，使用强化学习后，模型在多个基准测试上均有显著提升，展示了强化学习在代码模型中的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.02584', 'title': 'QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search', 'url': 'https://huggingface.co/papers/2502.02584', 'abstract': 'Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.', 'score': 11, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'f2d3938d4ad71761', 'authors': ['Zongyu Lin', 'Yao Tang', 'Xingcheng Yao', 'Da Yin', 'Ziniu Hu', 'Yizhou Sun', 'Kai-Wei Chang'], 'affiliations': ['Shanghai Jiaotong University, Shanghai, China', 'University of California, Los Angeles, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.02584.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#inference', '#reasoning', '#agents', '#training', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'QLASS: Пошаговое обучение языковых агентов для повышения эффективности', 'desc': 'Статья представляет новый метод QLASS для обучения языковых агентов. QLASS использует пошаговую оценку Q-значений для генерации аннотаций и улучшения промежуточного обучения. Метод вводит дерево рассуждений и моделирование вознаграждений процесса для эффективного пошагового руководства. QLASS позволяет языковым агентам лучше адаптироваться к долгосрочным целям, значительно улучшая производительность при решении сложных интерактивных задач.'}, 'en': {'title': 'Enhancing Language Agents with Stepwise Q-Guidance', 'desc': 'This paper introduces QLASS, a method designed to enhance the performance of language agents in complex tasks by providing stepwise guidance through Q-value estimation. Traditional approaches rely on outcome reward models, which can result in sub-optimal decision-making due to the lack of intermediate feedback. QLASS addresses this by creating a reasoning tree that models rewards at each step, allowing agents to learn from their interactions more effectively. The results show that QLASS can maintain strong performance with less annotated data, proving its efficiency in environments with limited supervision.'}, 'zh': {'title': 'QLASS：提升语言代理的决策能力', 'desc': '本文提出了一种名为QLASS的模型，用于改进语言代理在复杂交互任务中的表现。QLASS通过逐步估计Q值来自动生成中间交互的注释，从而为语言代理提供有效的指导。与传统的结果奖励模型不同，QLASS引入了推理树和过程奖励建模，使得每一步都有明确的指导。实验结果表明，即使在标注数据减少的情况下，QLASS仍能保持良好的性能，展示了其在有限监督下的高效性。'}}}, {'id': 'https://huggingface.co/papers/2502.02508', 'title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search', 'url': 'https://huggingface.co/papers/2502.02508', 'abstract': "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.", 'score': 9, 'issue_id': 2040, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '80bd687783bd609b', 'authors': ['Maohao Shen', 'Guangtao Zeng', 'Zhenting Qi', 'Zhang-Wei Hong', 'Zhenfang Chen', 'Wei Lu', 'Gregory Wornell', 'Subhro Das', 'David Cox', 'Chuang Gan'], 'affiliations': ['Harvard', 'MIT', 'MIT-IBM Watson AI Lab, IBM Research', 'Singapore University of Technology and Design', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2502.02508.jpg', 'data': {'categories': ['#small_models', '#open_source', '#training', '#reasoning', '#math', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Satori: LLM с внутренним поиском для улучшенного рассуждения', 'desc': 'Исследователи представили новый подход к улучшению способностей больших языковых моделей (LLM) к рассуждению. Они разработали метод Chain-of-Action-Thought (COAT), который позволяет модели проводить самоанализ и исследовать новые стратегии решения задач. Процесс обучения включает два этапа: настройку формата на небольшом масштабе и масштабное самосовершенствование с использованием обучения с подкреплением. В результате была создана 7-миллиардная модель Satori, показавшая отличные результаты в задачах математического рассуждения и обобщения на новые области.'}, 'en': {'title': 'Empowering LLMs with Internalized Reasoning through COAT', 'desc': 'This paper discusses the development of Satori, a large language model (LLM) that enhances reasoning capabilities through a novel approach called Chain-of-Action-Thought (COAT). The authors propose a two-stage training process that first tunes the model to internalize COAT reasoning and then employs reinforcement learning for self-improvement. By internalizing the search capabilities typically guided by an external verifier, Satori can perform complex reasoning tasks more effectively. The results show that Satori not only excels in mathematical reasoning benchmarks but also generalizes well to other tasks, indicating its robust performance.'}, 'zh': {'title': '内化搜索能力，提升推理能力！', 'desc': '大型语言模型（LLMs）在多个领域展示了出色的推理能力。研究表明，增加测试时的计算可以提升LLMs的推理能力，通常需要在推理时进行大量采样，并由外部LLM验证器指导。本文提出了一个新问题：能否将搜索能力内化，以根本性地增强单个LLM的推理能力？我们提出了行动思维链（COAT）推理方法，并设计了一个两阶段的训练范式，以实现LLM的自我改进和自我反思。'}}}, {'id': 'https://huggingface.co/papers/2502.01941', 'title': 'Can LLMs Maintain Fundamental Abilities under KV Cache Compression?', 'url': 'https://huggingface.co/papers/2502.01941', 'abstract': "This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of 17.4%-43.3%. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only 9.67%-25.53% performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves 9%-18% performance improvements on long-context generation tasks under aggressive compression ratios.", 'score': 7, 'issue_id': 2041, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '1352d78ee18eadfa', 'authors': ['Xiang Liu', 'Zhenheng Tang', 'Hong Chen', 'Peijie Dong', 'Zeyu Li', 'Xiuze Zhou', 'Bo Li', 'Xuming Hu', 'Xiaowen Chu'], 'affiliations': ['The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China', 'The Hong Kong University of Science and Technology, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.01941.jpg', 'data': {'categories': ['#optimization', '#inference', '#long_context', '#training'], 'emoji': '🧠', 'ru': {'title': 'Сжатие KV-кэша в LLM: баланс между эффективностью и производительностью', 'desc': 'Статья исследует влияние методов сжатия KV-кэша на фундаментальные возможности больших языковых моделей (LLM). Авторы провели комплексный эмпирический анализ различных методов сжатия на разнообразных задачах, включая общие знания, здравый смысл, арифметические рассуждения и генерацию кода. Результаты показали, что сжатие KV-кэша может значительно ухудшать производительность модели, особенно в задачах арифметических рассуждений. На основе анализа авторы предложили новый метод ShotKV, который по-разному обрабатывает фазы предзаполнения и декодирования, сохраняя семантическую целостность.'}, 'en': {'title': 'Optimizing KV Cache Compression for Enhanced LLM Performance', 'desc': "This paper explores how compressing the KV cache in large language models (LLMs) affects their performance on various tasks. While compression can reduce memory usage, it may also lead to a decline in the model's ability to perform tasks like arithmetic reasoning and code generation. The study finds that different compression methods impact tasks differently, with some methods causing significant performance drops, especially in arithmetic reasoning. To address these issues, the authors introduce ShotKV, a new compression technique that improves performance on long-context tasks while preserving important semantic information."}, 'zh': {'title': 'KV缓存压缩对大型语言模型能力的影响研究', 'desc': '本文研究了大型语言模型（LLMs）中一个未被充分探讨的挑战：KV缓存压缩方法对LLMs基本能力的影响。虽然现有方法在长上下文基准测试中取得了令人印象深刻的压缩比，但它们对核心模型能力的影响仍然缺乏研究。我们的实证研究评估了多种KV缓存压缩方法在不同任务上的表现，包括世界知识、常识推理、算术推理、代码生成、安全性以及长上下文理解和生成。分析结果显示，KV缓存压缩方法在特定任务上表现出性能下降，尤其是算术推理任务对激进压缩特别敏感，性能下降幅度达到17.4%-43.3%。'}}}, {'id': 'https://huggingface.co/papers/2502.02589', 'title': 'COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.02589', 'abstract': 'This paper introduces the COCONut-PanCap dataset, created to enhance panoptic segmentation and grounded image captioning. Building upon the COCO dataset with advanced COCONut panoptic masks, this dataset aims to overcome limitations in existing image-text datasets that often lack detailed, scene-comprehensive descriptions. The COCONut-PanCap dataset incorporates fine-grained, region-level captions grounded in panoptic segmentation masks, ensuring consistency and improving the detail of generated captions. Through human-edited, densely annotated descriptions, COCONut-PanCap supports improved training of vision-language models (VLMs) for image understanding and generative models for text-to-image tasks. Experimental results demonstrate that COCONut-PanCap significantly boosts performance across understanding and generation tasks, offering complementary benefits to large-scale datasets. This dataset sets a new benchmark for evaluating models on joint panoptic segmentation and grounded captioning tasks, addressing the need for high-quality, detailed image-text annotations in multi-modal learning.', 'score': 6, 'issue_id': 2056, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '28c4625deea8ac72', 'authors': ['Xueqing Deng', 'Qihang Yu', 'Ali Athar', 'Chenglin Yang', 'Linjie Yang', 'Xiaojie Jin', 'Xiaohui Shen', 'Liang-Chieh Chen'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2502.02589.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Детальные аннотации для лучшего понимания изображений', 'desc': 'Статья представляет набор данных COCONut-PanCap для улучшения панорамной сегментации и привязки подписей к изображениям. Этот датасет расширяет COCO, добавляя детальные панорамные маски и подробные описания сцен, привязанные к регионам изображения. COCONut-PanCap направлен на преодоление ограничений существующих наборов данных изображение-текст, которые часто не содержат всесторонних описаний сцен. Экспериментальные результаты показывают, что использование COCONut-PanCap значительно улучшает производительность моделей в задачах понимания и генерации изображений.'}, 'en': {'title': 'Enhancing Image Understanding with COCONut-PanCap Dataset', 'desc': 'The COCONut-PanCap dataset is designed to improve panoptic segmentation and grounded image captioning by providing detailed scene descriptions. It builds on the existing COCO dataset by adding advanced panoptic masks and fine-grained, region-level captions. This dataset enhances the training of vision-language models (VLMs) by offering high-quality, human-edited annotations that ensure consistency and detail in generated captions. Experimental results show that COCONut-PanCap significantly enhances performance in both understanding and generation tasks, establishing a new standard for evaluating models in multi-modal learning.'}, 'zh': {'title': 'COCONut-PanCap：提升全景分割与图像描述生成的新数据集', 'desc': '本文介绍了COCONut-PanCap数据集，旨在增强全景分割和基于图像的描述生成。该数据集在COCO数据集的基础上，结合了先进的COCONut全景掩码，克服了现有图像-文本数据集中缺乏详细场景描述的局限性。COCONut-PanCap数据集包含基于全景分割掩码的细粒度区域级描述，确保了一致性并提高了生成描述的细节。实验结果表明，COCONut-PanCap在理解和生成任务中显著提升了性能，为多模态学习中的高质量图像-文本注释需求提供了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2502.00674', 'title': 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?', 'url': 'https://huggingface.co/papers/2502.00674', 'abstract': 'Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of 3.8% improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.', 'score': 5, 'issue_id': 2050, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 февраля', 'en': 'February 2', 'zh': '2月2日'}, 'hash': '34ba4144afc562aa', 'authors': ['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2502.00674.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Один лучше многих: новый подход к ансамблированию языковых моделей', 'desc': 'Исследователи предложили новый метод ансамблирования под названием Self-MoA, который агрегирует выходные данные только одной наиболее эффективной языковой модели. Эксперименты показали, что Self-MoA превосходит стандартный метод Mixture-of-Agents (MoA), который объединяет разные языковые модели, на многих бенчмарках. Авторы изучили компромисс между разнообразием и качеством выходных данных в различных конфигурациях MoA. Также была представлена последовательная версия Self-MoA, способная агрегировать большое количество выходных данных языковой модели в несколько этапов.'}, 'en': {'title': 'Self-MoA: Elevating Performance with a Single Top LLM', 'desc': 'This paper investigates the effectiveness of an ensemble method called Self-MoA, which aggregates outputs from a single top-performing Large Language Model (LLM) instead of mixing multiple LLMs. The authors find that Self-MoA significantly outperforms the traditional Mixture-of-Agents (MoA) method, achieving notable improvements on various benchmarks. Their experiments reveal that the quality of outputs is crucial, as mixing different LLMs can reduce overall performance due to lower average quality. Additionally, the paper introduces a sequential version of Self-MoA that efficiently aggregates outputs over multiple rounds, maintaining high effectiveness.'}, 'zh': {'title': '单一模型集成，超越多样性', 'desc': '本论文探讨了混合不同大型语言模型（LLMs）输出的有效性，提出了一种新的集成方法Self-MoA。Self-MoA仅聚合单一表现最佳的LLM的输出，实验结果显示其在多个基准测试中表现优于传统的Mixture-of-Agents（MoA）方法。具体而言，Self-MoA在AlpacaEval 2.0基准上提高了6.6%的性能，并在多个基准上平均提高了3.8%。此外，论文还分析了输出多样性与质量之间的权衡，确认混合不同LLMs可能会降低模型的平均质量。'}}}, {'id': 'https://huggingface.co/papers/2501.19066', 'title': 'Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations', 'url': 'https://huggingface.co/papers/2501.19066', 'abstract': 'Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lack scalability, and/or compromise generation quality. In this work, we propose a novel framework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable concept manipulation in diffusion models. Specifically, we first identify interpretable monosemantic concepts in the latent space of text embeddings and leverage them to precisely steer the generation away or towards a given concept (e.g., nudity) or to introduce a new concept (e.g., photographic style). Through extensive experiments, we demonstrate that our approach is very simple, requires no retraining of the base model nor LoRA adapters, does not compromise the generation quality, and is robust to adversarial prompt manipulations. Our method yields an improvement of 20.01% in unsafe concept removal, is effective in style manipulation, and is sim5x faster than current state-of-the-art.', 'score': 4, 'issue_id': 2050, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'a8acff84a873ecb8', 'authors': ['Dahye Kim', 'Deepti Ghadiyaram'], 'affiliations': ['Department of Computer Science, Boston U'], 'pdf_title_img': 'assets/pdf/title_img/2501.19066.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#interpretability', '#security', '#cv', '#training'], 'emoji': '🎨', 'ru': {'title': 'Безопасное и эффективное управление концепциями в генерации изображений', 'desc': 'Статья представляет новый подход к манипулированию концепциями в генеративных моделях изображений с использованием разреженных автоэнкодеров (k-SAE). Авторы предлагают метод для эффективного удаления нежелательного контента и добавления новых стилей без необходимости переобучения базовой модели. Эксперименты показывают, что предложенный подход превосходит существующие методы по скорости и эффективности удаления небезопасных концепций на 20.01%. Метод также демонстрирует устойчивость к состязательным атакам и сохраняет качество генерируемых изображений.'}, 'en': {'title': 'Efficient Concept Control in Text-to-Image Generation', 'desc': 'This paper presents a new method for improving text-to-image generative models by using k-sparse autoencoders (k-SAEs) to manipulate concepts in a more efficient and interpretable way. Instead of fine-tuning models, which can be slow and reduce quality, this approach allows for precise control over the generation of specific concepts, such as removing unsafe content or adding new styles. The authors demonstrate that their method does not require retraining the base model and is significantly faster than existing techniques, achieving a 20.01% improvement in unsafe concept removal. Overall, this framework enhances the safety and versatility of generative models while maintaining high-quality outputs.'}, 'zh': {'title': '高效操控生成模型中的概念', 'desc': '本文提出了一种新颖的框架，利用k稀疏自编码器（k-SAEs）来实现扩散模型中的概念高效且可解释的操控。我们首先在文本嵌入的潜在空间中识别可解释的单义概念，并利用这些概念精确地引导生成内容，避免或引入特定概念。通过大量实验，我们证明了该方法简单易用，无需重新训练基础模型或使用LoRA适配器，且不影响生成质量。我们的技术在去除不安全概念方面提高了20.01%，在风格操控上也表现出色，速度比当前最先进的方法快5倍。'}}}, {'id': 'https://huggingface.co/papers/2502.01720', 'title': 'Generating Multi-Image Synthetic Data for Text-to-Image Customization', 'url': 'https://huggingface.co/papers/2502.01720', 'abstract': 'Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image quality. We propose a simple approach that addresses both limitations. We first leverage existing text-to-image models and 3D datasets to create a high-quality Synthetic Customization Dataset (SynCD) consisting of multiple images of the same object in different lighting, backgrounds, and poses. We then propose a new encoder architecture based on shared attention mechanisms that better incorporate fine-grained visual details from input images. Finally, we propose a new inference technique that mitigates overexposure issues during inference by normalizing the text and image guidance vectors. Through extensive experiments, we show that our model, trained on the synthetic dataset with the proposed encoder and inference algorithm, outperforms existing tuning-free methods on standard customization benchmarks.', 'score': 2, 'issue_id': 2043, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd249f21cea90b464', 'authors': ['Nupur Kumari', 'Xi Yin', 'Jun-Yan Zhu', 'Ishan Misra', 'Samaneh Azadi'], 'affiliations': ['Carnegie Mellon University', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2502.01720.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#dataset', '#architecture', '#cv', '#inference'], 'emoji': '🎨', 'ru': {'title': 'Улучшение кастомизации моделей text-to-image с помощью синтетических данных и новой архитектуры', 'desc': 'Статья представляет новый подход к кастомизации моделей text-to-image. Авторы создали синтетический набор данных SynCD с множественными изображениями объектов в разных условиях. Они предложили новую архитектуру энкодера, основанную на механизмах разделяемого внимания, для лучшего учета визуальных деталей. Также был разработан метод вывода, нормализующий векторы текстового и изображенческого руководства для устранения проблем переэкспозиции.'}, 'en': {'title': 'Enhancing Customization in Text-to-Image Models with Synthetic Datasets', 'desc': 'This paper presents a novel approach to customize text-to-image models, allowing users to generate images of custom concepts in various settings. The authors create a Synthetic Customization Dataset (SynCD) using 3D datasets, which includes multiple images of the same object under different conditions. They introduce a new encoder architecture that utilizes shared attention mechanisms to capture detailed visual information effectively. Additionally, a new inference technique is proposed to address overexposure issues, resulting in improved image quality compared to existing methods.'}, 'zh': {'title': '高质量定制化：突破文本到图像生成的限制', 'desc': '本文提出了一种文本到图像模型的定制化方法，允许用户在未见过的环境中生成自定义概念。现有方法通常依赖于昂贵的测试时优化或在单图像数据集上训练编码器，导致图像质量较差。我们的方法利用现有的文本到图像模型和3D数据集，创建了一个高质量的合成定制数据集（SynCD），包含同一对象在不同光照、背景和姿势下的多张图像。通过新的编码器架构和推理技术，我们的模型在标准定制基准测试中表现优于现有的无调优方法。'}}}, {'id': 'https://huggingface.co/papers/2502.01839', 'title': 'Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification', 'url': 'https://huggingface.co/papers/2502.01839', 'abstract': "Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one -- typically by verifying each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation that uses only random sampling and direct self-verification results in sustained performance improvements that, for example, elevate the Gemini v1.5 Pro model's reasoning capabilities past that of o1-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts -- chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies.", 'score': 1, 'issue_id': 2055, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd5c87eae437c7bec', 'authors': ['Eric Zhao', 'Pranjal Awasthi', 'Sreenivas Gollapudi'], 'affiliations': ['Google Research', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.01839.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#reasoning', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Масштабирование поиска: простота ведет к силе', 'desc': 'Статья исследует масштабирование поиска на основе выборки в моделях машинного обучения. Авторы обнаружили, что простое увеличение количества случайных выборок и прямая самопроверка могут значительно улучшить производительность моделей, например Gemini v1.5 Pro. Выявлено явление неявного масштабирования, когда увеличение пула ответов повышает точность верификации. Исследование также подчеркивает важность сравнения ответов и адаптации стилей вывода модели для улучшения самопроверки.'}, 'en': {'title': 'Boosting Model Performance with Smart Sampling and Verification', 'desc': 'This paper explores how sampling-based search can enhance the performance of machine learning models during testing. By generating multiple candidate responses and verifying them, the authors show that scaling up this approach leads to better reasoning capabilities in models like Gemini v1.5 Pro. They discover that larger pools of sampled responses improve the accuracy of verification, a concept they call implicit scaling. Additionally, the paper highlights two principles for enhancing self-verification: comparing responses to identify errors and using different output styles for various tasks.'}, 'zh': {'title': '基于采样的搜索：提升模型推理能力的关键', 'desc': '本文研究了基于采样的搜索方法，这是一种在测试时利用计算资源的简单策略。我们发现，通过扩展一个仅使用随机采样和自我验证的简化实现，可以显著提高模型的推理能力。我们还提出了两个原则来改善自我验证能力：比较不同响应可以帮助识别错误位置，而不同的模型输出风格在不同上下文中有不同的效果。尽管可以实现准确的验证，但当前的前沿模型在自我验证能力上仍然表现较弱。'}}}, {'id': 'https://huggingface.co/papers/2501.19389', 'title': 'Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2501.19389', 'abstract': "Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines.", 'score': 0, 'issue_id': 2058, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'c106990f1f2dc4d8', 'authors': ['Wenzhi Fang', 'Dong-Jun Han', 'Liangqi Yuan', 'Seyyedali Hosseinalipour', 'Christopher G. Brinton'], 'affiliations': ['Department of Computer Science and Engineering, Yonsei University', 'Department of Electrical Engineering, University at Buffalo-SUNY', 'Department of Electrical and Computer Engineering, Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2501.19389.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#transfer_learning'], 'emoji': '📱', 'ru': {'title': 'FSLoRA: эффективная федеративная настройка больших языковых моделей на устройствах', 'desc': 'Данная статья представляет новый метод под названием FSLoRA (Federated Sketching LoRA) для федеративной настройки больших языковых моделей на устройствах с ограниченными ресурсами. FSLoRA использует механизм скетчинга, позволяющий устройствам выборочно обновлять подматрицы глобальных LoRA-модулей на сервере. Метод адаптируется к вычислительным ограничениям конкретных устройств путем настройки коэффициентов скетчинга. Авторы проводят строгий анализ сходимости FSLoRA и демонстрируют превосходную производительность метода в экспериментах на различных наборах данных и моделях.'}, 'en': {'title': 'Adaptive Fine-Tuning for Diverse Devices with FSLoRA', 'desc': "This paper introduces federated sketching LoRA (FSLoRA), a method for fine-tuning large language models (LLMs) on devices with varying computational resources. FSLoRA uses a sketching mechanism that allows devices to update only specific parts of the global LoRA modules, making it adaptable to different device capabilities. The method addresses the challenges of data scarcity and model size by adjusting sketching ratios, which control the ranks of the updates based on device constraints. The authors provide a detailed analysis of FSLoRA's convergence properties and demonstrate its effectiveness through experiments, showing it outperforms existing methods."}, 'zh': {'title': '灵活适应设备限制的联邦草图LoRA', 'desc': '本文提出了一种新的方法，称为联邦草图LoRA（FSLoRA），旨在解决在设备上微调大型语言模型（LLMs）时的计算资源异质性问题。FSLoRA利用草图机制，使设备能够选择性地更新由服务器维护的全局LoRA模块的子矩阵。通过调整草图比例，FSLoRA能够灵活适应设备特定的通信和计算限制。实验结果表明，FSLoRA在多个数据集和LLM模型上表现优于现有的基线方法。'}}}, {'id': 'https://huggingface.co/papers/2502.07346', 'title': 'BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models', 'url': 'https://huggingface.co/papers/2502.07346', 'abstract': 'Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.', 'score': 30, 'issue_id': 2192, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '8a6e122fc0804618', 'authors': ['Xu Huang', 'Wenhao Zhu', 'Hanxu Hu', 'Conghui He', 'Lei Li', 'Shujian Huang', 'Fei Yuan'], 'affiliations': ['Carnegie Mellon University', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2502.07346.jpg', 'data': {'categories': ['#long_context', '#low_resource', '#machine_translation', '#multilingual', '#dataset', '#open_source', '#benchmark'], 'emoji': '🌐', 'ru': {'title': 'BenchMAX: Многоязычная оценка продвинутых возможностей языковых моделей', 'desc': 'BenchMAX - это новый многоязычный бенчмарк для оценки продвинутых возможностей больших языковых моделей (LLM) в 17 языках. Он фокусируется на таких задачах, как следование инструкциям, рассуждение, понимание длинного контекста и генерация кода. Каждый пример аннотируется тремя носителями языка для обеспечения высокого качества. Эксперименты показали различную эффективность ключевых возможностей LLM в разных языках, что нельзя преодолеть простым увеличением размера модели.'}, 'en': {'title': 'BenchMAX: Bridging Multilingual Gaps in Advanced Language Model Evaluation', 'desc': 'This paper introduces BenchMAX, a new multilingual evaluation benchmark designed to assess advanced capabilities of large language models (LLMs) such as instruction following, reasoning, and code generation across multiple languages. Unlike previous benchmarks that focused on simpler tasks, BenchMAX allows for a fair comparison of these complex abilities by using machine-translated data and independent annotations from native speakers. The study reveals significant performance gaps in LLMs when evaluated on these advanced tasks, indicating that merely increasing model size is not sufficient to improve multilingual proficiency. BenchMAX aims to enhance the development of multilingual models by providing a robust platform for evaluation and research.'}, 'zh': {'title': 'BenchMAX：多语言能力评估的新基准', 'desc': '本文介绍了BenchMAX，这是一个多语言评估基准，旨在比较大型语言模型在指令遵循、推理、长文本理解和代码生成等高级能力上的表现。以往的多语言基准主要关注简单理解任务，而BenchMAX则填补了这一空白，允许对不同语言的能力进行公平比较。为了确保数据质量，三位母语评审员独立对每个样本进行标注，确保评估的准确性。实验结果显示，不同语言在核心能力上的表现差异，表明仅仅增加模型规模无法解决这些性能差距。'}}}, {'id': 'https://huggingface.co/papers/2502.07870', 'title': 'TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation', 'url': 'https://huggingface.co/papers/2502.07870', 'abstract': 'Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models.', 'score': 29, 'issue_id': 2188, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '5b552b320e2e69f0', 'authors': ['Alex Jinpeng Wang', 'Dongxing Mao', 'Jiawei Zhang', 'Weiming Han', 'Zhuobai Dong', 'Linjie Li', 'Yiqi Lin', 'Zhengyuan Yang', 'Libo Qin', 'Fuwei Zhang', 'Lijuan Wang', 'Min Li'], 'affiliations': ['Central South University', 'Microsoft', 'National University of Singapore', 'North University of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07870.jpg', 'data': {'categories': ['#dataset', '#open_source', '#long_context', '#benchmark', '#cv'], 'emoji': '📚', 'ru': {'title': 'TextAtlas5M: Новый горизонт в генерации изображений с длинным текстом', 'desc': 'Статья представляет новый датасет TextAtlas5M для оценки генерации изображений с длинным текстом. Датасет содержит 5 миллионов изображений с разнообразными типами данных, что позволяет всесторонне оценивать крупномасштабные генеративные модели. Авторы также создали тестовый набор TextAtlasEval из 3000 улучшенных человеком изображений. Оценка показала, что даже самые продвинутые проприетарные модели (например, GPT4o с DallE-3) сталкиваются со значительными трудностями при работе с этим набором данных.'}, 'en': {'title': 'Empowering Long-Text Image Generation with TextAtlas5M', 'desc': 'This paper introduces TextAtlas5M, a new dataset aimed at improving text-conditioned image generation, particularly for long-form text. Current models struggle with generating images that include dense and intricate text, as existing datasets primarily focus on shorter text. TextAtlas5M contains 5 million images with long text, allowing for better evaluation of generative models. The paper also presents TextAtlasEval, a curated test set that highlights the challenges faced by both proprietary and open-source models in this area.'}, 'zh': {'title': '长文本图像生成的新突破', 'desc': '本文介绍了一个新的数据集TextAtlas5M，旨在解决文本条件下图像生成中长文本渲染的挑战。现有的数据集通常只关注短文本，限制了生成模型的能力。TextAtlas5M包含500万张长文本生成的图像，覆盖多种数据类型，为大规模生成模型的评估提供了基础。通过建立3000个经过人工改进的测试集TextAtlasEval，本文为文本条件生成提供了一个广泛的基准，帮助未来的研究和模型训练。'}}}, {'id': 'https://huggingface.co/papers/2502.08590', 'title': 'Light-A-Video: Training-free Video Relighting via Progressive Light Fusion', 'url': 'https://huggingface.co/papers/2502.08590', 'abstract': "Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source video's appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi.github.io/light-a-video.github.io/.", 'score': 29, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'dcc03282320e88b1', 'authors': ['Yujie Zhou', 'Jiazi Bu', 'Pengyang Ling', 'Pan Zhang', 'Tong Wu', 'Qidong Huang', 'Jinsong Li', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Anyi Rao', 'Jiaqi Wang', 'Li Niu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.08590.jpg', 'data': {'categories': ['#video', '#diffusion', '#cv'], 'emoji': '💡', 'ru': {'title': 'Плавное переосвещение видео без обучения', 'desc': 'Эта статья представляет Light-A-Video - подход к переосвещению видео без дополнительного обучения. Авторы предлагают два ключевых метода для улучшения согласованности освещения: модуль Consistent Light Attention (CLA) для стабилизации генерации фонового источника света и стратегию Progressive Light Fusion (PLF) для плавных переходов освещения между кадрами. Light-A-Video адаптирует модели переосвещения изображений для видео, решая проблемы несогласованности источника света и мерцания. Эксперименты показывают, что метод улучшает временную согласованность переосвещенного видео, сохраняя качество изображения.'}, 'en': {'title': 'Achieving Smooth Video Relighting with Light-A-Video', 'desc': 'This paper presents Light-A-Video, a novel approach for video relighting that addresses the challenges of lighting consistency and flickering in generated videos. Unlike traditional methods that apply image relighting on a frame-by-frame basis, Light-A-Video utilizes a training-free strategy to enhance temporal smoothness. It introduces a Consistent Light Attention (CLA) module to improve interactions between frames and stabilize background lighting. Additionally, the Progressive Light Fusion (PLF) technique is employed to blend the original and relighted appearances, ensuring coherent lighting transitions across video frames.'}, 'zh': {'title': '实现视频重光的一致性与平滑性', 'desc': '最近，图像重光模型的进展得益于大规模数据集和预训练的扩散模型，使得一致的光照效果得以实现。然而，视频重光仍然滞后，主要是由于训练成本高和缺乏多样化的高质量视频重光数据集。简单地将图像重光模型逐帧应用会导致光源不一致和重光外观不一致，从而在生成的视频中产生闪烁现象。我们提出了Light-A-Video，这是一种无训练的方法，旨在实现时间上平滑的视频重光。'}}}, {'id': 'https://huggingface.co/papers/2502.08639', 'title': 'CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.08639', 'abstract': 'In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.', 'score': 27, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '3d3890b6b6bf7904', 'authors': ['Qinghe Wang', 'Yawen Luo', 'Xiaoyu Shi', 'Xu Jia', 'Huchuan Lu', 'Tianfan Xue', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai'], 'affiliations': ['Dalian University of Technology', 'Kuaishou Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.08639.jpg', 'data': {'categories': ['#dataset', '#video', '#diffusion', '#3d', '#games'], 'emoji': '🎬', 'ru': {'title': 'CineMaster: Режиссируйте свое видео в 3D', 'desc': 'CineMaster - это новая система для создания 3D-ориентированного и контролируемого видео на основе текста. Она позволяет пользователям точно размещать объекты в сцене, гибко манипулировать объектами и камерой в 3D-пространстве. Система работает в два этапа: сначала пользователь интерактивно создает 3D-сигналы управления, затем эти сигналы используются для управления диффузионной моделью генерации видео. Для обучения была разработана автоматизированная система аннотации видеоданных с извлечением 3D-ограничивающих рамок и траекторий камеры.'}, 'en': {'title': 'Empowering Video Creation with 3D Control', 'desc': 'CineMaster is a new framework designed for generating videos from text while allowing users to control 3D elements like a film director. It operates in two stages: first, users create 3D-aware signals by placing objects and defining camera movements, and second, these signals guide a text-to-video diffusion model to produce the desired video. The framework also includes an automated data annotation system to gather necessary 3D motion and camera data from existing videos. Experiments show that CineMaster outperforms current methods in generating 3D-aware videos from text descriptions.'}, 'zh': {'title': 'CineMaster：让视频生成如导演般可控', 'desc': 'CineMaster是一个新颖的框架，用于生成具有3D感知和可控性的文本到视频。它使用户能够像专业电影导演一样精确控制场景中的物体位置、灵活操作3D空间中的物体和相机，并直观地布局渲染帧。该框架分为两个阶段：第一阶段通过交互式工作流程构建3D感知的条件信号，第二阶段利用这些信号指导文本到视频的扩散模型生成用户所需的视频内容。此外，CineMaster还建立了一个自动化数据注释管道，以解决缺乏3D物体运动和相机姿态标注的数据集问题。'}}}, {'id': 'https://huggingface.co/papers/2502.08127', 'title': 'Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance', 'url': 'https://huggingface.co/papers/2502.08127', 'abstract': 'Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.', 'score': 23, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': 'fa3f08993ba529cd', 'authors': ['Lingfei Qian', 'Weipeng Zhou', 'Yan Wang', 'Xueqing Peng', 'Jimin Huang', 'Qianqian Xie'], 'affiliations': ['TheFinAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.08127.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#rl', '#open_source', '#training', '#long_context'], 'emoji': '💹', 'ru': {'title': 'Специализация языковых моделей - ключ к успеху в финансовом анализе', 'desc': 'В этом исследовании оценивается эффективность 16 мощных языковых моделей в решении сложных финансовых задач. Авторы обнаружили, что улучшение наборов данных и предварительное обучение повышают способности моделей к финансовым рассуждениям. Они разработали специализированную модель на основе Llama-3.1-8B-Instruct, которая превзошла даже более крупные модели в финансовых задачах. Исследование подчеркивает необходимость адаптации моделей к специфике финансовой области.'}, 'en': {'title': 'Enhancing Financial Reasoning in LLMs with Domain-Specific Adaptations', 'desc': 'This paper investigates the performance of large language models (LLMs) in financial reasoning tasks, which include interpreting financial text, analyzing tabular data, and solving equations. The authors evaluate 16 advanced LLMs and find that while improved datasets and pretraining enhance financial reasoning, general techniques like Chain-of-Thought (CoT) fine-tuning do not consistently improve results. They propose a new model, enhanced with CoT fine-tuning and reinforcement learning, which shows a significant performance boost of 10% across various financial tasks. The study emphasizes the importance of domain-specific adaptations for better performance in financial reasoning and introduces a leaderboard for future benchmarking.'}, 'zh': {'title': '金融推理模型的创新与提升', 'desc': '本研究评估了16种强大的语言模型在金融推理任务中的表现。这些任务包括金融文本、表格数据和方程式，涉及数值推理、表格解读和金融术语理解等方面。研究结果表明，尽管更好的数据集和预训练可以提升金融推理能力，但通用的增强方法如链式推理微调并不总是有效。为了解决这些问题，我们开发了一种基于Llama-3.1-8B-Instruct的金融推理增强模型，经过微调和强化学习后，在多个任务上实现了10%的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2502.08047', 'title': 'WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation', 'url': 'https://huggingface.co/papers/2502.08047', 'abstract': 'Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation.', 'score': 20, 'issue_id': 2190, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '0f83dccb05181f21', 'authors': ['Henry Hengyuan Zhao', 'Difei Gao', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.08047.jpg', 'data': {'categories': ['#agents', '#benchmark'], 'emoji': '🖥️', 'ru': {'title': 'Критическое мышление для улучшения автоматизации GUI', 'desc': 'Статья представляет WorldGUI - новый бенчмарк для оценки графических интерфейсов, который симулирует реальные взаимодействия пользователей с компьютером в различных начальных состояниях. Авторы также предлагают фреймворк GUI-Thinker, использующий механизм критического мышления для эффективного управления непредсказуемостью и сложностью взаимодействий с GUI. Эксперименты показывают, что GUI-Thinker значительно превосходит Claude-3.5 (Computer Use) по показателю успешности выполнения задач в WorldGUI. Это исследование подчеркивает эффективность подхода, основанного на критическом мышлении, для улучшения автоматизации GUI.'}, 'en': {'title': 'Enhancing GUI Automation with WorldGUI and GUI-Thinker', 'desc': 'This paper addresses the challenges faced by current GUI agents in planning tasks due to their sensitivity to the initial state of the environment. It introduces WorldGUI, a new benchmark that simulates real user interactions by incorporating various initial states across multiple software applications. To tackle the complexities of dynamic GUI automation, the authors propose GUI-Thinker, a framework that utilizes a critique mechanism to improve decision-making in unpredictable scenarios. Experimental results show that GUI-Thinker outperforms existing models, highlighting its effectiveness in enhancing the success rate of GUI automation tasks.'}, 'zh': {'title': '提升GUI自动化的关键思维框架', 'desc': '当前的图形用户界面（GUI）代理在元素定位方面表现出色，但在规划方面仍然面临挑战，尤其是对环境初始状态的敏感性。初始状态的微小差异，例如目标软件未打开或界面不在默认状态，常常导致规划错误。为了解决这个问题，本文提出了WorldGUI，一个新颖的GUI基准，设计了具有多种初始状态的GUI任务，以模拟真实的计算机用户交互。我们还提出了GUI-Thinker，一个全面的框架，通过批判机制有效管理GUI交互的不可预测性和复杂性，实验结果显示其在WorldGUI任务上的成功率比Claude-3.5高出14.9%。'}}}, {'id': 'https://huggingface.co/papers/2502.07864', 'title': 'TransMLA: Multi-head Latent Attention Is All You Need', 'url': 'https://huggingface.co/papers/2502.07864', 'abstract': 'Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.', 'score': 19, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'dbff84dafe8c2312', 'authors': ['Fanxu Meng', 'Zengwei Yao', 'Muhan Zhang'], 'affiliations': ['Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, Peking University', 'Xiaomi Corp., Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07864.jpg', 'data': {'categories': ['#optimization', '#architecture', '#inference', '#training', '#long_context'], 'emoji': '🚀', 'ru': {'title': 'Революция в архитектуре внимания: MLA для эффективных языковых моделей', 'desc': 'Статья представляет новый метод под названием Multi-head Latent Attention (MLA), который решает проблему коммуникационных узких мест в больших языковых моделях. MLA использует матрицы низкого ранга в слоях ключ-значение, что позволяет сжимать и кэшировать латентные состояния KV. Авторы также предлагают метод TransMLA для преобразования предобученных моделей на основе Group Query Attention (GQA) в модели на основе MLA. Этот подход позволяет значительно уменьшить размер KV-кэша и ускорить вывод, сохраняя при этом выразительность модели.'}, 'en': {'title': 'Transforming Attention: From GQA to Efficient MLA', 'desc': 'This paper introduces Multi-head Latent Attention (MLA) as a solution to communication bottlenecks in large language models (LLMs) caused by hardware limitations. MLA utilizes low-rank matrices in the key-value (KV) layers to compress KV states, significantly reducing cache size and improving inference speed. The authors demonstrate that Group Query Attention (GQA) can be represented by MLA without increasing KV cache overhead, while MLA offers greater flexibility. To facilitate the adoption of MLA, they propose TransMLA, a method for converting GQA-based models into MLA-based ones, allowing for enhanced expressiveness without additional cache size.'}, 'zh': {'title': '提升语言模型效率的关键：多头潜在注意力', 'desc': '现代大型语言模型（LLMs）在当前硬件上常常面临通信瓶颈，而不仅仅是计算限制。多头潜在注意力（MLA）通过在键值（KV）层中使用低秩矩阵来解决这个问题，从而允许压缩的潜在KV状态被缓存。这种方法显著减少了KV缓存的大小，相比传统的多头注意力，推理速度更快。此外，MLA使用上投影矩阵来增加表达能力，以额外的计算换取减少的通信开销。'}}}, {'id': 'https://huggingface.co/papers/2502.07563', 'title': 'LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid', 'url': 'https://huggingface.co/papers/2502.07563', 'abstract': 'Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 18, 'issue_id': 2188, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'f5a4cfd0a0d018ae', 'authors': ['Weigao Sun', 'Disen Lan', 'Yiran Zhong', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.07563.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение обучения трансформеров с линейным вниманием на сверхдлинных последовательностях', 'desc': 'Статья представляет новый метод параллелизма последовательностей LASP-2 для обучения моделей трансформеров с линейным вниманием на очень длинных входных последовательностях. LASP-2 оптимизирует коммуникацию и вычислительный параллелизм, требуя только одну операцию AllGather для промежуточных состояний памяти, размер которых не зависит от длины последовательности. Авторы также предлагают расширение LASP-2H для гибридных моделей, сочетающих линейное и стандартное внимание. Эксперименты на модели Linear-Llama3 показывают, что LASP-2 превосходит предыдущие методы по скорости обучения на длинных последовательностях при распределенных вычислениях.'}, 'en': {'title': 'Boosting Efficiency in Linear Attention with LASP-2', 'desc': 'This paper presents LASP-2, a new sequence parallelism (SP) method designed to improve the efficiency of training linear attention transformer models with very long input sequences. LASP-2 optimizes communication and computation parallelism by minimizing the communication requirements for linear attention layers, allowing for a single AllGather operation on intermediate memory states. This approach leads to significant enhancements in both communication and computation parallelism, enabling better scalability in distributed systems. Additionally, LASP-2 is extended to LASP-2H, which applies similar optimizations to standard attention modules, providing an efficient solution for hybrid models that utilize both linear and standard attention.'}, 'zh': {'title': 'LASP-2：提升线性注意力模型的并行性', 'desc': '本文介绍了一种新的序列并行方法LASP-2，旨在提高线性注意力变换器模型在处理非常长输入序列时的通信和计算并行性。与之前的LASP方法相比，LASP-2重新思考了线性注意力层的最小通信需求，并重新组织了通信和计算的工作流程。通过这种方式，LASP-2只需在中间内存状态上进行一次AllGather集体通信，显著提高了通信和计算的并行性及其重叠。我们的评估表明，LASP-2在训练速度上比LASP提高了15.2%，比环形注意力提高了36.6%。'}}}, {'id': 'https://huggingface.co/papers/2502.08606', 'title': 'Distillation Scaling Laws', 'url': 'https://huggingface.co/papers/2502.08606', 'abstract': 'We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.', 'score': 12, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '774eeded4c92c597', 'authors': ['Dan Busbridge', 'Amitis Shidani', 'Floris Weers', 'Jason Ramapuram', 'Etai Littwin', 'Russ Webb'], 'affiliations': ['Apple', 'University of Oxford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.08606.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Масштабируемая дистилляция: оптимизация обучения моделей', 'desc': 'Эта статья представляет закон масштабирования дистилляции, который оценивает производительность дистиллированной модели на основе вычислительного бюджета и его распределения между студентом и учителем. Авторы предоставляют оптимальные рецепты дистилляции для различных сценариев, включая случаи с существующим учителем или необходимостью его обучения. Исследование показывает, что дистилляция превосходит обычное предобучение с учителем до определенного уровня вычислений, который предсказуемо растет с размером модели-студента. Работа также предоставляет ценные insights о процессе дистилляции, улучшающие понимание этого метода и информирующие дизайн экспериментов.'}, 'en': {'title': 'Maximizing Student Performance through Optimal Distillation Strategies', 'desc': 'This paper introduces a distillation scaling law that helps predict how well a distilled model will perform based on the available computing resources and how those resources are divided between the teacher and student models. The authors demonstrate that by optimizing the compute allocation, the performance of the student model can be significantly improved, especially when multiple students are distilled from a pre-trained teacher. They also provide guidelines for when to use distillation versus supervised learning, depending on whether a teacher model is available and whether it requires training. Overall, the study enhances our understanding of model distillation and offers practical strategies for effective implementation in machine learning tasks.'}, 'zh': {'title': '优化蒸馏模型性能的计算法则', 'desc': '本文提出了一种蒸馏缩放法则，用于根据计算预算和在学生与教师之间的分配来估计蒸馏模型的性能。研究结果降低了大规模使用蒸馏的风险，能够优化教师和学生模型的计算分配，以最大化学生的表现。我们提供了计算最优的蒸馏方案，适用于已有教师或需要训练教师的情况。通过大规模研究，我们增加了对蒸馏的理解，并为实验设计提供了指导。'}}}, {'id': 'https://huggingface.co/papers/2502.08168', 'title': 'SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation', 'url': 'https://huggingface.co/papers/2502.08168', 'abstract': "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.", 'score': 10, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '1ca2b8d38e35b203', 'authors': ['Zhiming Ma', 'Xiayang Xiao', 'Sihao Dong', 'Peidong Wang', 'HaiPeng Wang', 'Qingyun Pan'], 'affiliations': ['China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China', 'China Mobile Internet Company Ltd., Guangzhou, China', 'School of Computer Science and Engineering, Northeastern University, Shenyang, China', 'The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China', 'The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.08168.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#open_source', '#synthetic'], 'emoji': '🛰️', 'ru': {'title': 'SARChat-2M: Революция в интерпретации изображений SAR с помощью мультимодальных диалоговых моделей', 'desc': 'Статья представляет первый крупномасштабный мультимодальный диалоговый датасет для изображений SAR, названный SARChat-2M. Он содержит около 2 миллионов пар изображение-текст высокого качества и охватывает различные сценарии с детальными аннотациями целей. Датасет поддерживает ключевые задачи, такие как визуальное понимание и обнаружение объектов, а также предоставляет основу для создания мультимодальных датасетов в различных областях дистанционного зондирования. Эффективность датасета была подтверждена экспериментами на 16 основных VLM, что позволило создать первый многозадачный диалоговый бенчмарк в области SAR.'}, 'en': {'title': 'Empowering SAR Image Interpretation with SARChat-2M!', 'desc': 'This paper introduces SARChat-2M, a large-scale multimodal dialogue dataset specifically designed for synthetic aperture radar (SAR) images. It contains around 2 million image-text pairs that cover various scenarios and include detailed annotations for effective visual understanding and object detection. The dataset serves as a benchmark for evaluating vision language models (VLMs) in the SAR domain, demonstrating their capabilities in interpreting SAR images. By conducting experiments on 16 popular VLMs, the study establishes a new multi-task dialogue benchmark, paving the way for advancements in remote sensing applications.'}, 'zh': {'title': '推动SAR图像解读的多模态对话数据集', 'desc': '在合成孔径雷达（SAR）遥感图像解读领域，尽管视觉语言模型（VLMs）在自然语言处理和图像理解方面取得了显著进展，但由于缺乏专业领域的知识，其应用仍然有限。本文创新性地提出了第一个大规模的SAR图像多模态对话数据集SARChat-2M，包含约200万对高质量的图像-文本配对，涵盖了多种场景和详细的目标注释。该数据集不仅支持视觉理解和目标检测等关键任务，还开发了SAR领域的视觉语言数据集和基准，评估VLMs在SAR图像解读中的能力。通过对16个主流VLM的实验验证，该数据集的有效性得到了充分证明，并成功建立了SAR领域的第一个多任务对话基准。'}}}, {'id': 'https://huggingface.co/papers/2502.06533', 'title': 'Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.06533', 'abstract': 'The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of "critical tokens" which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage.', 'score': 9, 'issue_id': 2192, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '178e4717b984d64d', 'authors': ['Jean Vassoyan', 'Nathanaël Beau', 'Roman Plaud'], 'affiliations': ['Institut Polytechnique de Paris', 'Université Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, France', 'Université de Paris, LLF, CNRS, France', 'onepoint, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.06533.jpg', 'data': {'categories': ['#rl', '#training', '#long_context', '#small_models', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное исследование для LLM через оптимизацию критических токенов', 'desc': "В статье рассматривается проблема достижения долгосрочных целей большими языковыми моделями (LLM). Авторы предлагают использовать обучение с подкреплением для настройки предобученных LLM, чтобы оптимизировать заданную цель. Они исследуют динамику исследования на простой арифметической задаче, показывая влияние предварительного обучения и важность 'критических токенов'. Предлагается модификация KL-штрафа для более эффективного исследования критических токенов при обучении с подкреплением."}, 'en': {'title': 'Enhancing Exploration in Language Models with Critical Tokens', 'desc': "This paper addresses the challenge of enabling large language models (LLMs) to achieve long-term goals through reinforcement learning (RL). It highlights the difficulty of exploration in LLMs, where a balance must be maintained between discovering new solutions and preserving the model's foundational capabilities. The authors investigate how pre-training affects exploration dynamics in a small language model tasked with simple arithmetic. They introduce a modified Kullback-Leibler (KL) penalty that enhances exploration of 'critical tokens', leading to more efficient RL fine-tuning."}, 'zh': {'title': '优化长期目标的探索策略', 'desc': '本文探讨了大型语言模型（LLMs）在实现长期目标时面临的挑战。我们提出通过强化学习（RL）对预训练的LLMs进行微调，以优化特定目标的解决方案。研究表明，预训练的程度对探索过程有显著影响，尤其是“关键标记”在最终结果中起着重要作用。我们还引入了一种对KL惩罚的简单修改，以促进对关键标记的探索，从而提高RL微调阶段的效率。'}}}, {'id': 'https://huggingface.co/papers/2502.07599', 'title': 'DPO-Shift: Shifting the Distribution of Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2502.07599', 'abstract': 'Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.', 'score': 9, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '85d178e03a57421f', 'authors': ['Xiliang Yang', 'Feng Jiang', 'Qianen Zhang', 'Lei Zhao', 'Xiao Li'], 'affiliations': ['Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University', 'School of Data Science, The Chinese University of Hong Kong, Shenzhen', 'School of Mathematics, South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.07599.jpg', 'data': {'categories': ['#alignment', '#training', '#rlhf'], 'emoji': '🔀', 'ru': {'title': 'Контролируемое смещение вероятностей для улучшения обучения языковых моделей', 'desc': 'Статья представляет новый метод под названием DPO-Shift для улучшения обучения языковых моделей с учетом человеческих предпочтений. Авторы решают проблему смещения вероятности выбранных ответов, которая возникает при использовании метода Direct Preference Optimization (DPO). DPO-Shift позволяет контролируемо смещать распределение вероятности выбранных ответов. Экспериментальные результаты показывают превосходство DPO-Shift над DPO на ряде задач, включая MT-Bench.'}, 'en': {'title': 'Mitigating Likelihood Displacement in Language Model Training', 'desc': 'This paper addresses the issue of likelihood displacement in Direct Preference Optimization (DPO) for aligning language models with human preferences. The authors introduce a new method, referred to as \textit{method}, which aims to control the distribution of chosen response probabilities during training. They highlight a trade-off between enhancing the chosen probability and the reward margin, supported by both theoretical insights and experimental results. The findings indicate that \textit{method} outperforms DPO in various downstream tasks, providing a promising solution to the challenges posed by likelihood displacement.'}, 'zh': {'title': '解决选择概率下降的有效方法', 'desc': '本文介绍了一种新的方法\textit{method}，旨在解决直接偏好优化（DPO）中出现的选择概率下降问题。研究表明，在训练过程中，模型对选择响应的概率往往会降低，这被称为似然位移。我们的方法可以控制选择概率的分布，从而改善模型的表现。通过理论分析和实验验证，我们证明了\textit{method}在下游任务中优于传统的DPO方法。'}}}, {'id': 'https://huggingface.co/papers/2502.08524', 'title': 'LLM Pretraining with Continuous Concepts', 'url': 'https://huggingface.co/papers/2502.08524', 'abstract': "Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process.", 'score': 7, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '99ad370e7cd11e3c', 'authors': ['Jihoon Tack', 'Jack Lanchantin', 'Jane Yu', 'Andrew Cohen', 'Ilia Kulikov', 'Janice Lan', 'Shibo Hao', 'Yuandong Tian', 'Jason Weston', 'Xian Li'], 'affiliations': ['FAIR at Meta', 'KAIST', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.08524.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#interpretability', '#training', '#architecture', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'CoCoMix: смешивание концепций для улучшения языковых моделей', 'desc': 'В этой статье представлен новый метод предобучения языковых моделей под названием Continuous Concept Mixing (CoCoMix). В отличие от стандартного подхода предсказания следующего токена, CoCoMix сочетает дискретное предсказание токенов с непрерывными концепциями, полученными из предобученного разреженного автоэнкодера. Эксперименты показывают, что CoCoMix превосходит стандартные методы по эффективности обучения и качеству на различных задачах. Кроме того, CoCoMix улучшает интерпретируемость и управляемость модели, позволяя напрямую анализировать и модифицировать предсказанные концепции.'}, 'en': {'title': 'Revolutionizing Language Models with Continuous Concept Mixing', 'desc': "This paper introduces Continuous Concept Mixing (CoCoMix), a new framework for pretraining large language models. Unlike traditional methods that focus solely on predicting the next token, CoCoMix integrates continuous concepts derived from a sparse autoencoder into the model's hidden states. This approach improves sample efficiency and enhances performance on various tasks, including language modeling and reasoning. Additionally, CoCoMix allows for better interpretability and control over the model's reasoning by enabling direct manipulation of the predicted concepts."}, 'zh': {'title': '连续概念混合：提升语言模型的效率与可解释性', 'desc': '本文提出了一种新的预训练框架，称为连续概念混合（CoCoMix），它结合了离散的下一个标记预测和连续概念。CoCoMix通过将从预训练稀疏自编码器中学习的连续概念与标记的隐藏表示交错混合，来优化模型的隐藏状态。实验结果表明，CoCoMix在样本效率上表现更佳，并且在语言建模和推理任务上均优于传统的下一个标记预测和知识蒸馏方法。该方法还增强了模型的可解释性和可引导性，使得用户可以直接检查和修改预测的概念，从而透明地引导模型的内部推理过程。'}}}, {'id': 'https://huggingface.co/papers/2502.05167', 'title': 'NoLiMa: Long-Context Evaluation Beyond Literal Matching', 'url': 'https://huggingface.co/papers/2502.05167', 'abstract': 'Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.', 'score': 5, 'issue_id': 2188, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '4ff0f34526efea9f', 'authors': ['Ali Modarressi', 'Hanieh Deilamsalehy', 'Franck Dernoncourt', 'Trung Bui', 'Ryan A. Rossi', 'Seunghyun Yoon', 'Hinrich Schütze'], 'affiliations': ['Adobe Research', 'Center for Information and Language Processing'], 'pdf_title_img': 'assets/pdf/title_img/2502.05167.jpg', 'data': {'categories': ['#benchmark', '#long_context'], 'emoji': '🔍', 'ru': {'title': 'NoLiMa: Новый вызов для больших языковых моделей в работе с длинным контекстом', 'desc': 'Статья представляет новый бенчмарк NoLiMa для оценки способности больших языковых моделей (LLM) работать с длинным контекстом. В отличие от существующих тестов, NoLiMa требует от моделей выявления скрытых ассоциаций между вопросом и релевантной информацией в тексте. Исследование показало, что производительность 12 популярных LLM значительно снижается при увеличении длины контекста. Результаты указывают на трудности механизма внимания в обработке длинных последовательностей при отсутствии прямых лексических совпадений.'}, 'en': {'title': 'NoLiMa: Challenging LLMs Beyond Literal Matches', 'desc': 'This paper introduces NoLiMa, a new benchmark designed to evaluate the capabilities of large language models (LLMs) in retrieving relevant information from extensive contexts. Unlike traditional methods that allow models to rely on literal matches, NoLiMa requires models to infer deeper associations between questions and answers with minimal lexical overlap. The study evaluates 12 popular LLMs, revealing that while they perform well in shorter contexts, their performance significantly declines as context length increases, particularly beyond 1K tokens. The findings indicate that the attention mechanism struggles to effectively retrieve relevant information in longer contexts when direct matches are not available.'}, 'zh': {'title': '长上下文中的信息检索挑战', 'desc': '最近的大型语言模型（LLMs）支持长达128K到1M的上下文。本文提出了一种新的基准测试NoLiMa，旨在评估模型在长上下文中检索相关信息的能力。与传统的针在干草堆（NIAH）测试不同，NoLiMa设计了最小词汇重叠的针集，要求模型推断潜在关联以找到针。我们的评估显示，尽管这些模型在短上下文中表现良好，但在长上下文中性能显著下降，尤其是在缺乏字面匹配的情况下。'}}}, {'id': 'https://huggingface.co/papers/2502.06145', 'title': 'Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance', 'url': 'https://huggingface.co/papers/2502.06145', 'abstract': 'Recent character image animation methods based on diffusion models, such as Animate Anyone, have made significant progress in generating consistent and generalizable character animations. However, these approaches fail to produce reasonable associations between characters and their environments. To address this limitation, we introduce Animate Anyone 2, aiming to animate characters with environment affordance. Beyond extracting motion signals from source video, we additionally capture environmental representations as conditional inputs. The environment is formulated as the region with the exclusion of characters and our model generates characters to populate these regions while maintaining coherence with the environmental context. We propose a shape-agnostic mask strategy that more effectively characterizes the relationship between character and environment. Furthermore, to enhance the fidelity of object interactions, we leverage an object guider to extract features of interacting objects and employ spatial blending for feature injection. We also introduce a pose modulation strategy that enables the model to handle more diverse motion patterns. Experimental results demonstrate the superior performance of the proposed method.', 'score': 4, 'issue_id': 2192, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '66fa48cd36ed02b0', 'authors': ['Li Hu', 'Guangyuan Wang', 'Zhen Shen', 'Xin Gao', 'Dechao Meng', 'Lian Zhuo', 'Peng Zhang', 'Bang Zhang', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.06145.jpg', 'data': {'categories': ['#multimodal', '#cv', '#video', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'Оживление персонажей с учетом окружающей среды', 'desc': 'Данная работа представляет Animate Anyone 2 - метод анимации персонажей с учетом окружающей среды. В отличие от предыдущих подходов, основанных на диффузионных моделях, здесь извлекаются не только сигналы движения, но и представления окружения в качестве условных входных данных. Предложена стратегия маски, не зависящая от формы, для лучшей характеристики взаимосвязи персонажа и среды. Для повышения точности взаимодействия с объектами используется объектный направляющий и пространственное смешивание признаков.'}, 'en': {'title': 'Animating Characters with Environmental Awareness', 'desc': 'This paper presents Animate Anyone 2, an advanced character animation method that improves upon previous diffusion models by integrating environmental context into the animation process. Unlike earlier methods, which struggled to connect characters with their surroundings, this approach captures environmental representations as conditional inputs, allowing for more coherent animations. The authors introduce a shape-agnostic mask strategy to better define the relationship between characters and their environments, enhancing the realism of interactions. Additionally, they implement an object guider and spatial blending techniques to refine object interactions and a pose modulation strategy to accommodate diverse motion patterns, resulting in superior animation quality.'}, 'zh': {'title': '角色与环境的完美结合', 'desc': '本文介绍了一种新的角色动画方法Animate Anyone 2，旨在解决现有基于扩散模型的动画方法在角色与环境之间的关联不足的问题。我们通过提取环境表示作为条件输入，使角色动画能够与环境的特征相一致。我们提出了一种形状无关的掩码策略，更有效地描述角色与环境之间的关系。此外，我们还引入了姿态调节策略，以处理更丰富的运动模式，实验结果表明该方法在性能上优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2502.07737', 'title': 'Next Block Prediction: Video Generation via Semi-Autoregressive Modeling', 'url': 'https://huggingface.co/papers/2502.07737', 'abstract': 'Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.', 'score': 4, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '8038af0ecacc031f', 'authors': ['Shuhuai Ren', 'Shuming Ma', 'Xu Sun', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07737.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#inference', '#training'], 'emoji': '🎬', 'ru': {'title': 'NBP: Быстрая и качественная генерация видео блоками', 'desc': 'Статья представляет новый подход к генерации видео под названием Next-Block Prediction (NBP). В отличие от традиционного метода Next-Token Prediction, NBP использует полуавтореrрессивную модель, разбивая видео на блоки и предсказывая их параллельно. Это позволяет значительно ускорить процесс генерации и улучшить качество результатов. Модель NBP превзошла базовые методы по метрике FVD на датасетах UCF101 и K600, демонстрируя масштабируемость и эффективность подхода.'}, 'en': {'title': 'Revolutionizing Video Generation with Next-Block Prediction', 'desc': 'This paper introduces a new method for generating videos called Next-Block Prediction (NBP), which improves upon the traditional Next-Token Prediction (NTP) approach. NBP changes the way video content is generated by using blocks instead of individual tokens, allowing for simultaneous predictions within each block. This method utilizes bidirectional attention to enhance the understanding of spatial relationships in the video, leading to better quality outputs. As a result, NBP not only speeds up the generation process significantly but also achieves superior performance metrics compared to the standard NTP model.'}, 'zh': {'title': '视频生成的新突破：下一块预测', 'desc': '本文提出了一种新的半自回归框架，称为下一块预测（NBP），用于视频生成。与传统的自回归方法不同，NBP通过将视频内容均匀分解为相等大小的块，使得每个块内的标记可以同时预测下一个块的对应标记，从而捕捉更强的空间依赖性。该方法通过并行预测多个标记，显著减少了生成步骤，提高了推理速度，达到了每秒生成8.89帧的效果。实验结果表明，NBP在多个数据集上表现优于传统模型，展示了其在生成质量和速度上的优势。'}}}, {'id': 'https://huggingface.co/papers/2502.04411', 'title': 'Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing', 'url': 'https://huggingface.co/papers/2502.04411', 'abstract': 'Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.', 'score': 3, 'issue_id': 2192, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '9370051a307713bf', 'authors': ['Kunfeng Lai', 'Zhenheng Tang', 'Xinglin Pan', 'Peijie Dong', 'Xiang Liu', 'Haolan Chen', 'Li Shen', 'Bo Li', 'Xiaowen Chu'], 'affiliations': ['Platform and Content Group, Tencent', 'Sun Yatsen University', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.04411.jpg', 'data': {'categories': ['#model merging', '#training', '#architecture', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Умное слияние языковых моделей: эффективность без компромиссов', 'desc': 'Статья описывает новый метод объединения языковых моделей, обученных на разных задачах. Авторы предлагают усреднять слои с минимальными конфликтами параметров и использовать маршрутизацию экспертов для слоев со значительными конфликтами. Они также вводят разделение экспертов на плотный и разреженные для уменьшения затрат памяти. Эксперименты на моделях LLaMA и Qwen показывают улучшение производительности при меньших системных затратах по сравнению с существующими методами.'}, 'en': {'title': 'Merging Models Smartly: Harnessing Layer Insights for Better Performance', 'desc': 'This paper presents a method for merging Large Language Models (LLMs) that have been fine-tuned on different tasks, aiming to create a more powerful model. The authors identify that parameter conflicts between models can degrade performance when simply averaging them. To address this, they propose a strategy that averages layers with minimal conflicts and employs task-level expert routing for layers with significant conflicts. Their approach not only reduces storage costs by decoupling fine-tuned experts but also improves performance on real-world reasoning tasks while being more efficient than existing methods.'}, 'zh': {'title': '智能合并，提升模型性能！', 'desc': '本论文提出了一种模型合并的方法，旨在将不同任务上微调的大型语言模型（LLMs）聚合成一个更强的模型。我们发现不同层之间的参数冲突程度不同，因此我们对参数冲突较小的层进行平均，而对参数冲突较大的层采用新颖的任务级专家路由。为了进一步降低存储成本，我们将多个微调的专家解耦为一个稠密专家和几个稀疏专家，并根据输入数据的任务不确定性选择和合并合适的专家。实验结果表明，我们的方法在性能上显著优于现有方法，同时减少了系统成本。'}}}, {'id': 'https://huggingface.co/papers/2502.06872', 'title': 'Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2502.06872', 'abstract': "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.", 'score': 3, 'issue_id': 2188, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': 'f454782ce3101c66', 'authors': ['Bo Ni', 'Zheyuan Liu', 'Leyao Wang', 'Yongjia Lei', 'Yuying Zhao', 'Xueqi Cheng', 'Qingkai Zeng', 'Luna Dong', 'Yinglong Xia', 'Krishnaram Kenthapadi', 'Ryan Rossi', 'Franck Dernoncourt', 'Md Mehrab Tanjim', 'Nesreen Ahmed', 'Xiaorui Liu', 'Wenqi Fan', 'Erik Blasch', 'Yu Wang', 'Meng Jiang', 'Tyler Derr'], 'affiliations': ['Adobe Research', 'Air Force Research Lab', 'Cisco AI Research', 'Meta', 'North Carolina State University', 'Oracle Health AI', 'The Hong Kong Polytechnic University', 'University of Notre Dame', 'University of Oregon', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06872.jpg', 'data': {'categories': ['#hallucinations', '#interpretability', '#security', '#survey', '#rag', '#ethics'], 'emoji': '🧠', 'ru': {'title': 'Путь к надёжному ИИ: Преодоление рисков в генерации с дополнением извлечённой информацией', 'desc': 'Статья посвящена методу генерации с дополнением извлечённой информацией (RAG), который улучшает генерацию контента искусственным интеллектом. Авторы рассматривают риски, связанные с RAG, включая проблемы надёжности, конфиденциальности и безопасности. Предлагается комплексная дорожная карта для разработки надёжных RAG-систем с акцентом на пять ключевых аспектов. Статья также освещает применение надёжных RAG-систем в различных прикладных областях.'}, 'en': {'title': 'Building Trust in Retrieval-Augmented Generation Systems', 'desc': 'This paper discusses Retrieval-Augmented Generation (RAG), a technique that enhances AI-generated content by incorporating external knowledge for better accuracy and relevance. While RAG improves content generation, it also brings new challenges such as robustness, privacy, and accountability issues that need to be addressed. The authors propose a comprehensive framework that focuses on five key areas: reliability, privacy, safety, fairness, and explainability, to guide future research and development of trustworthy RAG systems. By providing a structured approach, the paper aims to foster innovation and highlight the importance of trust in RAG applications.'}, 'zh': {'title': '构建可信赖的检索增强生成系统', 'desc': '检索增强生成（RAG）是一种先进的技术，旨在解决人工智能生成内容（AIGC）面临的挑战。通过将上下文检索与内容生成相结合，RAG 提供可靠且最新的外部知识，减少幻觉现象，并确保在各种任务中保持相关上下文。然而，尽管 RAG 取得了成功，但最近的研究表明，该范式也引入了新的风险，包括鲁棒性问题、隐私问题、对抗性攻击和问责问题。本文旨在提供一个全面的路线图，以开发可信赖的 RAG 系统，围绕可靠性、隐私、安全性、公平性、可解释性和问责性等五个关键视角进行讨论。'}}}, {'id': 'https://huggingface.co/papers/2502.08213', 'title': 'LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention', 'url': 'https://huggingface.co/papers/2502.08213', 'abstract': 'In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources. Experimental results on the Bespoke-Stratos-17k dataset demonstrate that after 15 epochs of training, the combined model generates responses comparable in quality to those obtained by distillation. We discuss the advantages of the modular approach, provide examples of input queries and comparative analysis, and outline prospects for further extension of the method.', 'score': 1, 'issue_id': 2194, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'}, 'hash': '2623177431838d6c', 'authors': ['Konstantin Kolomeitsev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.08213.jpg', 'data': {'categories': ['#small_models', '#transfer_learning', '#optimization', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективная передача знаний между языковыми моделями разного размера', 'desc': 'В статье предлагается архитектура модулей LLM, позволяющая передавать знания от большой предобученной модели к меньшей с помощью улучшенного механизма кросс-внимания. Замороженная модель Qwen2-1.5B передает свои представления через специальные слои внимания модели GPT-Neo-125M, которая обучается на ограниченных вычислительных ресурсах. Эксперименты на наборе данных Bespoke-Stratos-17k показывают, что после 15 эпох обучения комбинированная модель генерирует ответы, сравнимые по качеству с полученными путем дистилляции. Авторы обсуждают преимущества модульного подхода и перспективы дальнейшего развития метода.'}, 'en': {'title': 'Empowering Smaller Models with Enhanced Knowledge Transfer', 'desc': 'This paper introduces a new architecture called LLM Modules that facilitates knowledge transfer from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. The Qwen2-1.5B model is kept unchanged, and its learned representations are utilized by the smaller GPT-Neo-125M model, which is optimized for limited computational resources. The results from experiments on the Bespoke-Stratos-17k dataset show that after 15 training epochs, the performance of the combined model is on par with traditional distillation methods. The authors highlight the benefits of this modular approach and provide examples and analyses to support their findings, while also discussing future enhancements.'}, 'zh': {'title': '知识转移的新方法：模块化架构与增强交叉注意力', 'desc': '本文提出了一种LLM模块架构，能够通过增强的交叉注意力机制将知识从大型预训练模型转移到较小的模型。我们将Qwen2-1.5B模型固定，并通过专门设计的注意力层将其表示传递给在有限计算资源上训练的GPT-Neo-125M模型。实验结果表明，在Bespoke-Stratos-17k数据集上经过15个训练周期后，组合模型生成的响应质量与蒸馏获得的结果相当。我们讨论了模块化方法的优势，提供了输入查询的示例和比较分析，并概述了该方法进一步扩展的前景。'}}}, {'id': 'https://huggingface.co/papers/2502.07985', 'title': 'MetaSC: Test-Time Safety Specification Optimization for Language Models', 'url': 'https://huggingface.co/papers/2502.07985', 'abstract': 'We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at https://github.com/vicgalle/meta-self-critique.git .', 'score': 1, 'issue_id': 2190, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '68244a5483bfc513', 'authors': ['Víctor Gallego'], 'affiliations': ['Komorebi AI, Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2502.07985.jpg', 'data': {'categories': ['#optimization', '#training', '#security', '#alignment', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Динамическая оптимизация безопасности языковых моделей без переобучения', 'desc': 'Авторы предлагают новую динамическую систему безопасности для оптимизации рассуждений языковых моделей во время вывода без изменения весов модели. Подход основан на механизме мета-критики, который итеративно обновляет промпты безопасности для адаптивного управления процессом критики и пересмотра. Оптимизация во время тестирования улучшает защиту от попыток обхода ограничений и повышает безопасность в различных задачах. Эмпирические оценки показывают, что динамически оптимизированные промпты безопасности дают значительно более высокие показатели безопасности по сравнению с фиксированными системными промптами.'}, 'en': {'title': 'Dynamic Safety for Language Models: Adapting Prompts for Better Protection', 'desc': "This paper introduces a dynamic safety framework designed to enhance the safety of language models during inference without altering their underlying weights. It utilizes a meta-critique mechanism that adaptively updates safety prompts, known as specifications, to improve the model's ability to handle adversarial inputs and general safety tasks. The approach shows significant improvements in safety performance, particularly in avoiding moral harm and ensuring honest responses. Empirical results indicate that the dynamically optimized safety prompts outperform traditional fixed prompts and static defenses in various scenarios."}, 'zh': {'title': '动态优化，提升语言模型安全性！', 'desc': '我们提出了一种新颖的动态安全框架，旨在优化语言模型在推理时的安全性推理，而无需修改模型权重。该方法基于自我批评方法的最新进展，利用元批评机制迭代更新安全提示（称为规范），以自适应地推动批评和修订过程。此测试时优化不仅提高了对抗性越狱请求的性能，还在避免道德伤害和追求诚实回应等多种安全相关任务中表现出色。我们的实证评估显示，动态优化的安全提示相比于固定系统提示和静态自我批评防御，显著提高了安全评分。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (11)', '#agents (16)', '#agi (8)', '#alignment (13)', '#architecture (49)', '#audio (4)', '#benchmark (64)', '#cv (27)', '#data (18)', '#dataset (58)', '#diffusion (25)', '#ethics (8)', '#games (8)', '#graphs (1)', '#hallucinations (9)', '#healthcare (4)', '#inference (35)', '#interpretability (19)', '#leakage (3)', '#long_context (19)', '#low_resource (9)', '#machine_translation (1)', '#math (14)', '#multilingual (9)', '#multimodal (38)', '#open_source (35)', '#optimization (85)', '#plp (3)', '#rag (9)', '#reasoning (46)', '#rl (18)', '#rlhf (15)', '#robotics (2)', '#science (4)', '#security (13)', '#small_models (8)', '#story_generation', '#survey (2)', '#synthetic (11)', '#training (109)', '#transfer_learning (12)', '#video (27)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-13 17:08',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-13 17:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-13 17:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    